[
    {
        "job_title": "Data Scientist/Machine Learning",
        "company_name": "Infosys Limited",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "IT/Computers - Software",
        "job_description": "Job Description:\nJob Title\nLead Analyst Data Science Intermediate Senior\nRole Summary\nWe are looking for candidates to build and implement analytics solutions to our esteemed clients\nThe incumbent should have strong aptitude for numbers experience in any domain and willingness to learn some cutting edge technologies\nKey Responsibilities:\nJob Title\nLead Analyst Data Science Intermediate Senior\nRole Summary\nWe are looking for candidates to build and implement analytics solutions to our esteemed clients\nThe incumbent should have strong aptitude for numbers experience in any domain and willingness to learn some cutting edge technologies\nTechnical Requirements:\nOther key to have Skills\nSQL knowledge and experience working with relational databases\nUnderstanding of any one of domain Eg Retail Supply chain Logistics Manufacturing\nUnderstanding of the project lifecycles waterfall and agile\nSoft Skills\nStrong verbal and written communication skills with the ability to work well in a team\nStrong customer focus ownership urgency and drive\nAbility to handle multiple competing priorities in a fast paced environment\nWork well with the team members to maintain high credibility\nWork Experience\nOverall 6 8 of years of experience in Data Analytics Data Science and Machine Learning\nEducational Requirements any of the following\nBachelor of Engineering Bachelor of Technology in any stream with consistent academic track record\nBachelor s degree in a quantitative discipline e\ng\nStatistics Economics Mathematics Marketing Analytics or significant relevant coursework with consistent academic track record\nAdditional Responsibilities:\nAdditional Academic Qualification good to have\nMasters in any area related to Science Mathematics Statistics Economy and Finance with consistent academic track record\nPhD in any stream\nLocation\nBangalore Pune Hyderabad Chennai Trivandrum Mysore\nPreferred Skills:\nTechnology->Data Science->Machine Learning",
        "skills": [
            "Relational Databases",
            "Data Science",
            "Machine Learning",
            "Data Analytics",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist/Senior Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Consulting",
        "job_description": "What You'll Do:\nAnalyse and integrate clinical and multi-omics datasets (e.g., genomics, proteomics) to extract actionable insights.\nDevelop and validate predictive models for biomedical research and biomarker discovery.\nApply Design of Experiments (DOE) and statistical methods to support research objectives.\nBuild interactive web applications using Streamlit or R Shiny for data visualization and exploration.\nEnsure reproducibility, code quality, and adherence to industrialized coding best practices using Git/GitHub.\nCollaborate with cross-functional teams including bioinformaticians, data engineers, and clinicians.\nDocument analysis workflows, methodologies, and results in a clear and structured manner.\nCommunicate findings through reports, dashboards, and presentations to technical and non-technical stakeholders.\nTechnical Skills Programming & Tools:\nR (Tidyverse), Python (pandas, NumPy, scikit-learn)\nStreamlit or R Shiny for web application development\nGit/GitHub for version control and code management\nGCP\nWhat You Need:\nData Science & Modeling:\nExploratory Data Analysis (EDA), Statistical Modeling, Predictive Modeling\nClinical data interpretation and Omics data integration\nDesign of Experiments (DOE), Reproducibility & Repeatability\nData visualization (ggplot2, matplotlib, seaborn, etc.)",
        "skills": [
            "Omics",
            "R",
            "Geonomics",
            "Clinical Analysis",
            "Data Science",
            "Predictive Modeling",
            "Eda",
            "Python"
        ]
    },
    {
        "job_title": "Associate Principal - Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Consulting",
        "job_description": "Curious about the role What your typical day would look like\nYour work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.\nDemonstrate proficiency in programming languages such as Python or R, with the ability to write clean, efficient, and maintainable code. Experience with relevant libraries and frameworks (e.g., TensorFlow, PyTorch, scikit-learn) is essential.\nUse your robust problem-solving skills to develop data-driven solutions, analyse complex datasets, and derive actionable insights that lead to impactful outcomes.\nWork closely with clients to understand their business objectives, identify opportunities for analytics-driven solutions, and communicate findings clearly and promptly.\nTake ownership of end-to-end model development, from problem definition and data exploration to model training, validation, and deployment.\nCollaborate with cross-functional teams, including data engineers, software developers, and business stakeholders, to integrate analytics solutions into business processes.\nLeverage a profound understanding of mathematical and statistical principles to guide developing and validating advanced data science models.\nStay abreast of industry trends, emerging technologies, and best practices in data science, bringing innovative ideas to the team and contributing to continuous improvement.\nDesired Skills and Experience:\n8 -10 years of total DS and model development experience\nMandatory:Minimum 4+ years of experiencein the Banking and Financial services industry\nA passion for writing high-quality code (Python), and the code should be modular, scalable, and end-end project execution while planning an active hands-on role\nHaving good problem-solving skills is essential, and it is equally important to have in-depth knowledge to solve complex problems effectively.\nComprehensive knowledge of the regression and classification concepts and mathematical backend along with SQL\nEncourage collaboration with various stakeholders and take complete ownership of deliverables.\nAdept understanding of various data science approaches, machine learning algorithms, and statistical methods.\nExcellent communication skills with presentability, articulation, storytelling capability, and ability to manage complex client situations\nEffective mentoring of a team with expertise in industry/domain/functional areas",
        "skills": [
            "Bfsi",
            "Banking And Finance",
            "Fintech",
            "model development",
            "Data Science",
            "Machine Learning Algorithms",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - AI ML Team",
        "company_name": "Customerxps Software",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "Undertaking data collection, preprocessing and analysis\nBuilding models to address business problems\nPresenting information using data visualization techniques\nIdentify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams",
        "skills": [
            "Product management",
            "Team Management",
            "Business Analysis",
            "Project management",
            "Mis",
            "Change management"
        ]
    },
    {
        "job_title": "Associate Principal - Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Consulting",
        "job_description": "Curious about the role What your typical day would look like\nYour work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.\nDemonstrate proficiency in programming languages such as Python or R, with the ability to write clean, efficient, and maintainable code. Experience with relevant libraries and frameworks (e.g., TensorFlow, PyTorch, scikit-learn) is essential.\nUse your robust problem-solving skills to develop data-driven solutions, analyse complex datasets, and derive actionable insights that lead to impactful outcomes.\nWork closely with clients to understand their business objectives, identify opportunities for analytics-driven solutions, and communicate findings clearly and promptly.\nTake ownership of end-to-end model development, from problem definition and data exploration to model training, validation, and deployment.\nCollaborate with cross-functional teams, including data engineers, software developers, and business stakeholders, to integrate analytics solutions into business processes.\nLeverage a profound understanding of mathematical and statistical principles to guide developing and validating advanced data science models.\nStay abreast of industry trends, emerging technologies, and best practices in data science, bringing innovative ideas to the team and contributing to continuous improvement.\nDesired Skills and Experience:\n8 -10 years of total DS and model development experience\nMandatory:Minimum 4+ years of experiencein the Banking and Financial services industry\nA passion for writing high-quality code (Python), and the code should be modular, scalable, and end-end project execution while planning an active hands-on role\nHaving good problem-solving skills is essential, and it is equally important to have in-depth knowledge to solve complex problems effectively.\nComprehensive knowledge of the regression and classification concepts and mathematical backend along with SQL\nEncourage collaboration with various stakeholders and take complete ownership of deliverables.\nAdept understanding of various data science approaches, machine learning algorithms, and statistical methods.\nExcellent communication skills with presentability, articulation, storytelling capability, and ability to manage complex client situations\nEffective mentoring of a team with expertise in industry/domain/functional areas",
        "skills": [
            "Bfsi",
            "Banking And Finance",
            "Fintech",
            "model development",
            "Data Science",
            "Machine Learning Algorithms",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist/Senior Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Consulting",
        "job_description": "What You'll Do:\nAnalyse and integrate clinical and multi-omics datasets (e.g., genomics, proteomics) to extract actionable insights.\nDevelop and validate predictive models for biomedical research and biomarker discovery.\nApply Design of Experiments (DOE) and statistical methods to support research objectives.\nBuild interactive web applications using Streamlit or R Shiny for data visualization and exploration.\nEnsure reproducibility, code quality, and adherence to industrialized coding best practices using Git/GitHub.\nCollaborate with cross-functional teams including bioinformaticians, data engineers, and clinicians.\nDocument analysis workflows, methodologies, and results in a clear and structured manner.\nCommunicate findings through reports, dashboards, and presentations to technical and non-technical stakeholders.\nTechnical Skills Programming & Tools:\nR (Tidyverse), Python (pandas, NumPy, scikit-learn)\nStreamlit or R Shiny for web application development\nGit/GitHub for version control and code management\nGCP\nWhat You Need:\nData Science & Modeling:\nExploratory Data Analysis (EDA), Statistical Modeling, Predictive Modeling\nClinical data interpretation and Omics data integration\nDesign of Experiments (DOE), Reproducibility & Repeatability\nData visualization (ggplot2, matplotlib, seaborn, etc.)",
        "skills": [
            "Omics",
            "R",
            "Geonomics",
            "Clinical Analysis",
            "Data Science",
            "Predictive Modeling",
            "Eda",
            "Python"
        ]
    },
    {
        "job_title": "Associate Principal - Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Consulting",
        "job_description": "Curious about the role What your typical day would look like\nYour work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.\nDemonstrate proficiency in programming languages such as Python or R, with the ability to write clean, efficient, and maintainable code. Experience with relevant libraries and frameworks (e.g., TensorFlow, PyTorch, scikit-learn) is essential.\nUse your robust problem-solving skills to develop data-driven solutions, analyse complex datasets, and derive actionable insights that lead to impactful outcomes.\nWork closely with clients to understand their business objectives, identify opportunities for analytics-driven solutions, and communicate findings clearly and promptly.\nTake ownership of end-to-end model development, from problem definition and data exploration to model training, validation, and deployment.\nCollaborate with cross-functional teams, including data engineers, software developers, and business stakeholders, to integrate analytics solutions into business processes.\nLeverage a profound understanding of mathematical and statistical principles to guide developing and validating advanced data science models.\nStay abreast of industry trends, emerging technologies, and best practices in data science, bringing innovative ideas to the team and contributing to continuous improvement.\nDesired Skills and Experience:\n8 -10 years of total DS and model development experience\nMandatory:Minimum 4+ years of experiencein the Banking and Financial services industry\nA passion for writing high-quality code (Python), and the code should be modular, scalable, and end-end project execution while planning an active hands-on role\nHaving good problem-solving skills is essential, and it is equally important to have in-depth knowledge to solve complex problems effectively.\nComprehensive knowledge of the regression and classification concepts and mathematical backend along with SQL\nEncourage collaboration with various stakeholders and take complete ownership of deliverables.\nAdept understanding of various data science approaches, machine learning algorithms, and statistical methods.\nExcellent communication skills with presentability, articulation, storytelling capability, and ability to manage complex client situations\nEffective mentoring of a team with expertise in industry/domain/functional areas",
        "skills": [
            "Bfsi",
            "Banking And Finance",
            "Fintech",
            "model development",
            "Data Science",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Data Scientist - AI ML Team",
        "company_name": "Customerxps Software",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai City, Mumbai",
        "industry": "Information Technology",
        "job_description": "Undertaking data collection, preprocessing and analysis\nBuilding models to address business problems\nPresenting information using data visualization techniques\nIdentify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams",
        "skills": [
            "Product management",
            "Team Management",
            "Business Analysis",
            "Project management",
            "Mis",
            "Change management"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Customerxps Software",
        "experience": "12-22 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "Undertaking data collection, preprocessing and analysis\nBuilding models to address business problems\nPresenting information using data visualization techniques\nIdentify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams",
        "skills": [
            "Product management",
            "Team Management",
            "Business Analysis",
            "Project management",
            "Mis",
            "Change management"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "GlaxoSmithKline Pte Ltd",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Pharmaceutical",
        "job_description": "Site Name: Bengaluru Luxor North Tower Posted Date: May 27 2025 Job Purpose: We are seeking a highly skilled and experienced Principal Agentic AI Developer and Deep Learning Scientist to join our dynamic team. The ideal candidate will have over 10 years of hands-on experience in developing and implementing advanced AI solutions, with a strong focus on Reasoning Agentic AI, EDA, Data Science models, and various AI services. This role requires a deep understanding of AI technologies, including OpenAI, Azure AI services, and Gemini models, as well as expertise in Monte Carlo Tree simulation and Graph AI. Key Responsibilities: Develop and implement advanced AI and deep learning models, focusing on Reasoning Agentic AI, EDA, and Data Science models. Utilize AI services such as OpenAI, Azure AI, and Gemini models to create innovative solutions. Generate insights and reports using AI-driven tools and techniques. Apply Monte Carlo Tree simulation and Graph AI techniques to solve complex problems. Mentor and groom junior team members, fostering a collaborative and innovative environment. Be hands-on with LangChain and Python to develop large-scale enterprise AI solutions. Leverage domain knowledge of ServiceNow and Azure AI services to enhance customer feedback mechanisms. Implement strong data engineering practices, including SLM/LLM, RAG, and Graph RAG skills. Skills and Experience: Technical Skills: Over 10 years Machine learning , deep learning and 1-2 years in GENAI , Agentic AI with Production Live projects . not POC's. Proficiency in Reasoning Agentic AI, EDA, and Data Science models. Expertise in OpenAI, Azure AI services, and GEmini models. Strong knowledge of Monte Carlo Tree simulation and Graph AI. Hands-on experience with LangChain and Python. Familiarity with ServiceNow and Azure AI services. Strong data engineering skills, including SLM/LLM, RAG, and Graph RAG. Must-Have Skills: Proven problem-solving skills, including computer vision, reasoning, causation, and correlation. Expertise in handling unstructured and structured data insights with hallucination control. Advanced knowledge of evolved Graph RAG and RAG solutions. Strong knowledge of Azure AI services. Certifications .Anyone of the following Certified Artificial Intelligence (AI) Expert Certified Generative AI Expert Applied Generative AI Certificate Certified Prompt Engineer Machine learning Skills Management skills Soft Skills: Excellent critical thinking and problem-solving abilities. Strong communication and interpersonal skills. Ability to mentor and lead teams effectively. Adaptability and a continuous learning mindset. Strong organizational and project management skills. Qualifications: Master's degree in Computer Science, Data Science, AI, or a related field. Proven track record of developing and deploying AI solutions in a large enterprise environment. Experience with customer feedback mechanisms and improving user experience through AI. Demonstrated ability to work collaboratively in a team-oriented environment. Inclusion at GSK: As an employer committed to Inclusion, we encourage you to reach out if you need any adjustments during the recruitment process. Please contact our Recruitment Team at [HIDDEN TEXT] to discuss your needs. Why GSK Uniting science, technology and talent to get ahead of disease together. GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organisation where people can thrive. We prevent and treat disease with vaccines, specialty and general medicines. We focus on the science of the immune system and the use of new platform and data technologies, investing in four core therapeutic areas (infectious diseases, HIV, respiratory/ immunology and oncology). Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves - feeling welcome, valued, and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together. Important notice to Employment businesses/ Agencies GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site. It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way. GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable. If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in gsk.com, you should disregard the same and inform us by emailing [HIDDEN TEXT], so that we can confirm to you if the job is genuine.",
        "skills": [
            "LangChain",
            "Graph AI",
            "Graph RAG",
            "Llm",
            "Data Science models",
            "Gemini models",
            "Reasoning Agentic AI",
            "Monte Carlo Tree simulation",
            "RAG",
            "Azure AI services",
            "OpenAI",
            "Servicenow",
            "Eda",
            "Deep Learning",
            "Slm",
            "Machine Learning",
            "Python"
        ]
    },
    {
        "job_title": "Staff Data Scientist",
        "company_name": "PayPal",
        "experience": "9-11 Years",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nStaff Data Scientist - Product Data Science\nJob Description:\nAbout the Role\nWe're looking for a Staff Data Scientist to join our Product Data Science team at PayPal-someone who thrives on solving strategic problems with data and delivering insights that drive action, not just reporting. In this high-impact role, you'll collaborate with senior leaders, influence product direction, and deliver decision-enabling insights grounded in data science to shape product launches, iteration cycles, and growth strategies.\n\nYou'll be a key thought partner across the organization, leveraging PayPal's vast data assets to deliver actionable insights, deepen our understanding of the PayPal consumer experience, and help shape the future of commerce.\nWhat You'll Do\nKey Roles and Responsibilities include:\nAct as a strategic partner to product and business leaders, shaping priorities, framing key questions, and identifying opportunities where data can influence decision-making and accelerate growth.\nCollaborate with software and data engineering teams to ensure scalable data pipelines, instrumentation quality, and alignment with PayPal's long-term data and experimentation strategy.\nLead end-to-end execution of A/B tests and causal analyses to evaluate feature performance, optimize user experience, and guide roadmap decisions.\nConduct deep-dive analyses on customer behavior, product usage, and key business trends to identify insights that drive growth and retention.\nDefine success metrics and develop automated monitoring frameworks that reflect experimentation standards and support scalable decision-making.\nSynthesize findings into clear, compelling data stories for executive audiences, enabling confident, data-informed decisions.\nChampion experimentation best practices and help evolve PayPal's measurement frameworks, platform capabilities, and tooling.\nMentor junior data scientists, fostering technical development, problem-solving excellence, and business acumen across the team.\nRequirements\nEducational background in a quantitative field (e.g., Statistics, Mathematics, Computer Science, Economics, or Engineering) advanced degree preferred.\n9+ years of hands-on experience in data science or analytics roles, with a strong track record of driving impact in product-oriented environments.\nExpertise in controlled experimentation-including the design, execution, and analysis of tests that guide product launches, feature iteration, and strategic decision-making.\nProficiency in SQL and comfort working with large-scale, complex datasets in modern cloud environments (e.g., Google BigQuery).\nStrong Python programming skills, with experience in data exploration, statistical analysis, and visualization familiarity with machine learning concepts is a plus.\nStrategic mindset with business fluency-able to connect the dots between data, product experience, and business outcomes.\nProven ability to influence senior stakeholders through compelling communication, structured thinking, and insight-driven storytelling.\nExperience mentoring or guiding junior data scientists and a desire to foster a collaborative, learning-oriented environment.\nDomain experience in payments, fintech, or e-commerce is a strong plus.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\n-\nPayPal is committed to fair and equitable compensation practices.\nActual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.\nThe total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit .\nThe US national annual pay range for this role is $176,500 to $262,350\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": [
            "Data exploration",
            "Machine learning concepts",
            "Controlled experimentation",
            "Google BigQuery",
            "Statistical Analysis",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PayPal",
        "experience": "6-8 Years",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWe are looking for a senior product data scientist for our Venmo Analytics team who goes above and beyond simply providing reports and analyses. This person will need to collaborate with senior leadership and take end to end ownership on key initiatives, experimentation and strategic projects. The candidate will also help generate insights by mining Venmo's rich data and support experimentation efforts driven by the consumer product organization. In the process, this person will develop a deep understanding of the Venmo consumer experience, Payment business, our site functionality, further strengthen their analytic skills, and gaining exposure to a wide variety of functional teams within Venmo.\nJob Description:\nKey Roles and Responsibilities include:\nPerform technical deep dive analyses on key business trends from multiple perspectives and package the insights into easily consumable presentations\nMake data visually appealing and simple to both navigate and comprehend for end-users\nIdentify key metrics and drive the building of exec-facing dashboards to track progress of the business and its highest priority initiatives\nIdentify key business levers, establish cause & effect, and communicate key findings to various stakeholders to facilitate data driven decision-making\nEngage in problem solving with Risk, Product, Marketing, and various other teams to determine performance trends and root causes.\nSupport new product launches by establishing test and control plans and ensuring constant monitoring is in place to track product performance.\nCreate close relationships with stakeholders to anticipate & answer questions that might be asked by executives\nPlay an active role in establishing experimentation best practices across the team\nBasic Requirements:\nBachelor's/Master's degree in a quantitative field (such as Analytics, Statistics, Mathematics, Economics or Engineering) or equivalent field experience\n6+ years professional experience in an analytical role\nStrong expertise in A/B testing, experimental design and analyzing controlled experiments (e.g. feature rollouts, multivariate testing)\nStrong foundation in causal inference techniques\nExperience in driving conversations with senior executives\nStrong written and verbal communication skills with the ability to translate complex problems into simpler terms, and effectively influence both peers and senior leadership\nAdvanced SQL experience, preferable with Teradata systems, data mining, and Big Query analytics (Google Cloud)\nExperience analyzing very large, complex, multi-dimensional data sets\nExperience with one or both will be highly desirable - Python and R\nExperience in Machine learning techniques (models for classification, regression, clustering etc) is a plus\nWork experience in the payments, ecommerce, or financial services industry is a plus\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\n-\nPayPal is committed to fair and equitable compensation practices.\nActual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.\nThe total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit .\nThe US national annual pay range for this role is $143,500 to $212,850\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": [
            "Big Query",
            "R",
            "Machine learning techniques",
            "Experimental Design",
            "causal inference techniques",
            "Teradata",
            "data mining",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "3-7 Years",
        "salary": "INR 18 - 36 LPA ",
        "location": "Ahmedabad, Bengaluru, Chennai",
        "industry": "Cloud Management, Penetration Testing, Identity Management, CMS, Data Center, Scheduling, Information Services",
        "job_description": "Description\nWe are seeking a Data Scientist to join our team in India. The ideal candidate will have a strong background in data analysis and modeling, and will play a key role in driving data-driven decision-making across the organization.\nResponsibilities\nDevelop and implement data models and algorithms to analyze complex datasets.\nCollaborate with cross-functional teams to understand business needs and translate them into analytical requirements.\nConduct data mining and analysis to extract valuable insights that drive business decisions.\nCreate data visualizations to effectively communicate findings to stakeholders.\nMaintain and optimize existing data pipelines and workflows.\nSkills and Qualifications\nStrong proficiency in programming languages such as Python or R.\nExperience with SQL and database management systems.\nFamiliarity with machine learning frameworks (e.g., TensorFlow, scikit-learn) and statistical analysis techniques.\nKnowledge of data visualization tools (e.g., Tableau, Power BI, Matplotlib) to present data in an understandable format.\nUnderstanding of big data technologies (e.g., Hadoop, Spark) is a plus.\nExcellent problem-solving skills and ability to work with large datasets.",
        "skills": [
            "Data Analysis",
            "Statistics",
            "Tensorflow",
            "Machine Learning",
            "Hadoop",
            "Data Visualization",
            "Big Data",
            "Python",
            "Sql",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist Senior Associate",
        "company_name": "Commonwealth Bank",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, tar\ngets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title:Senior Associate Data Scientist\nLocation:Bangalore\nBusiness & Team:Home Buying Decision Science\nImpact & contribution:\nThe Senior Associate Data Scientist will use technical knowledge and understanding of business domain to deliver moderate or highly complex data science projects independently or with minimal guidance.\nYou will also engage and collaborate with business stakeholders to clearly articulate findings to solve business problems.\nRoles & Responsibilities:\nAnalyse complex data sets to extract insights and identify trends.\nDevelop predictive models and algorithms to solve business problems.\nWork on deployment of models in production.\nCollaborate with cross-functional teams to understand requirements and deliver data-driven solutions.\nClean, preprocess, and manipulate data for analysis through programming.\nCommunicate findings and recommendations to stakeholders through reports and presentations.\nStay updated with industry trends and best practices in data science.\nContribute to the development and improvement of data infrastructure and processes.\nDesign experiments and statistical analysis to validate hypotheses and improve models.\nContinuously learn and enhance skills in data science techniques and tools.\nStrongly support the adoption of data science across the organization.\nIdentify problems in the products, services and operations of the bank and solve those with innovative research driven solutions.\nEssential Skills:\nStrong hands-on programming experience in Python (mandatory), R, SQL, Hive and Spark.\nMore than 3 years of relevant experience.\nAbility to write well designed, modular and optimized code.\nKnowledge of H2O.ai, GitHub, Big Data and ML Engineering.\nKnowledge of commonly used data structures and algorithms.\nGood to have: Knowledge of Time Series, NLP and Deep Learning and Generative AI is preferred.\nGood to have: Knowledge and hands-on experience in developing solutions with Large Language Models.\nMust have been part of projects building and deploying predictive models in production (financial services domain preferred) involving large and complex data sets.\nStrong problem solving and critical thinking skills.\nCurious, fast learning capability and team player attitude is a must.\nAbility to communicate clearly and effectively.\nDemonstrated expertise through blogposts, research, participation in competitions, speaking opportunities, patents and paper publications.\nMost importantly - ability to identify and translate theories into real applications to solve practical problems.\nPreferred Skills:\nGood to have: Knowledge and hands-on data engineering or model deployment\nExperience in Data Science in either of Credit Risk, Pricing, Sales and Marketing, Campaign Analytics, Ecommerce Retail or banking products for retail or business banking is preferred.\nSolid foundation of Statistics and core ML algorithms at a mathematical (under the hood) level.\nEducation Qualifications : Bachelor's degree in Engineering in Computer Science/Information Technology.\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 22/06/2025",
        "skills": [
            "Large Language Models",
            "R",
            "ML Engineering",
            "H2O.ai",
            "Generative AI",
            "Spark",
            "Sql",
            "Deep Learning",
            "Github",
            "Time Series",
            "Hive",
            "Python",
            "Big Data",
            "Nlp"
        ]
    },
    {
        "job_title": "Principal Applied Data Scientist (OCI) - Product Development Gen AI and ML Solutions",
        "company_name": "Oracle",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "IT/Computers - Hardware & Networking",
        "job_description": "Job Description:\nOracle Cloud Infrastructure (OCI) is a pioneering force in cloud technology, merging the agility of startups with the robustness of an enterprise software leader. Within OCI, the Oracle Generative AI Service team spearheads innovative solutions at the convergence of artificial intelligence and cloud infrastructure. As part of this team, you'll contribute to large-scale cloud solutions utilizing cutting-edge machine learning technologies, aimed at addressing complex global challenges.\nJoin us to create innovative solutions using top-notch machine learning technologies to solve global challenges. We're looking for an experienced Principal Applied Data Scientist to join our OCI Gen-AI Solutions team for strategic customers. In this role, you'll collaborate with applied scientists and product managers to design, develop, and deploy tailored Gen-AI solutions with an emphasis on Large Language Models (LLMs), Agents, MPC and Retrieval Augmented Generation (RAG) with large OpenSearch clusters.\nAs part of theOCI Gen AI and Data Solutionsfor strategic customersteam, you will be responsible for developing innovative Gen AI and data services for our strategic customers.As a Principal Applied Data Scientist, you'll lead the development of advanced Gen AI solutions using the latest ML technologies combined with Oracle's cloud expertise. Your work will significantly impact sectors like financial services, telecom, healthcare, and code generation by creating distributed, scalable, high-performance solutions for strategic customers.\nWork directly with key customers and accompany them on their Gen AI journey - understanding their requirements, help them envision and design and build the right solutions and work together with their ML engineering to remove blockers.\nYou will dive deep into model structure to optimize model performance and scalability.\nYou will build state of art solutions with brand new technologies in this fast-evolving area.\nYou will configure large scale OpenSearch clusters, setting up ingestion pipelines to get the data into the OpenSearch.\nYou will diagnose, troubleshoot, and resolve issues in AI model training and serving. You may also perform other duties as assigned.\nBuild re-usable solution patterns and reference solutions / showcases that can apply across multiple customers.\nBe an enthusiastic, self-motivated, and a great collaborator.\nBe our product evangelist - engage directly with customers and partners, participate and present in external events and conferences, etc.\nQualifications and experience\nBachelors or master's in computer science or equivalent technical field with 10+ years of experience\nAble to optimally communicate technical ideas verbally and in writing (technical proposals, design specs, architecture diagrams and presentations).\nDemonstrated experience in designing and implementing scalable AI models and solutions for production,relevant professional experience as end-to-end solutions engineer or architect (data engineering, data science and ML engineering is a plus), with evidence of close collaborations with PM and Dev teams.\nExperience with OpenSearch, Vector databases, PostgreSQL and Kafka Streaming.\nPractical experience with setting up and finetuning large OpenSearch Clusters.\nExperience in setting up data ingestion pipelines with OpenSearch.\nExperience with search algorithms, indexing, optimizing latency and response times.\nPractical experience with the latest technologies in LLM and generative AI, such as parameter-efficient fine-tuning, instruction fine-tuning, and advanced prompt engineering techniques like Tree-of-Thoughts.\nFamiliarity with Agents and Agent frameworks and Model Predictive Control (MPC)\nHands-on experience with emerging LLM frameworks and plugins, such as LangChain, LlamaIndex, VectorStores and Retrievers, LLM Cache, LLMOps (MLFlow), LMQL, Guidance, etc.\nStrong publication record, including as a lead author or reviewer, in top-tier journals or conferences.\nAbility and passion to mentor and develop junior machine learning engineers.\nProficient in Python and shell scripting tools.\nPreferred Qualifications:\nMasters or Bachelor's in related field with 5+ years relevant experience\nExperience with RAG based solutions architecture. Familiarity in OpenSearch and Vector stores as a knowledge store\nKnowledge of LLM and experience delivering, Generative AI And Agent models are a significant plus.\nFamiliarity and experience with the latest advancements in computer vision and multimodal modeling is a plus.\nExperience with semantic search, multi-modal search and conversational search.\nExperience in working on a public cloud environment, and in-depth knowledge of IaaS/PaaS industry and competitive capabilities.Experience with popular model training and serving frameworks like KServe, KubeFlow, Triton etc.\nExperience with LLM fine-tuning, especially the latest parameter efficient fine-tuning technologies and multi-task serving technologies.\nDeep technical understanding of Machine Learning, Deep Learning architectures like Transformers, training methods, and optimizers.\nExperience with deep learning frameworks (such as PyTorch, JAX, or TensorFlow) and deep learning architectures (especially Transformers).\nExperience in diagnosing, fixing, and resolving issues in AI model training and serving.\nCareer Level - IC4",
        "skills": [
            "Data ingestion pipelines",
            "Retrievers",
            "LLM Cache",
            "LlamaIndex",
            "VectorStores",
            "Tree-of-Thoughts",
            "OpenSearch",
            "search algorithms",
            "Vector databases",
            "LLMOps",
            "Kafka Streaming",
            "MLFlow",
            "LangChain",
            "LMQL",
            "Generative AI",
            "Shell scripting",
            "Python",
            "PostgreSQL"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "IDFC FIRST Bank",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Requirements\n\nJob Requirements\n\nRole/ Job Title: Senior Data Scientist\n\nFunction/ Department: Data & Analytics\n\nJob Purpose\n\nIn this specialized role, you will leverage your expertise in machine learning and statistics to derive valuable insights from data. Your role will include developing predictive models, interpreting data and working closely with out ML engineers to ensure the effective deployment and functioning of these models.\n\nKey / Primary Responsibilities\n\nLead cross-functional teams in the design, development, and deployment of Generative AI solutions, with a strong focus on Large Language Models (LLMs).\nArchitect, train, and fine-tune state-of-the-art LLMs (e.g., GPT, BERT, T5) for various business applications, ensuring alignment with project goals.\nDeploy and scale LLM-based solutions, integrating them seamlessly into production environments and optimizing for performance and efficiency.\nDevelop and maintain machine learning workflows and pipelines for training, evaluating, and deploying Generative AI models, using Python or R, and leveraging libraries like Hugging Face Transformers, TensorFlow, and PyTorch.\nCollaborate with product, data, and engineering teams to define and refine use cases for LLM applications such as conversational agents, content generation, and semantic search.\nDesign and implement fine-tuning strategies to adapt pre-trained models to domain-specific tasks, ensuring high relevance and accuracy.\nEvaluate and optimize LLM performance, including handling challenges such as prompt engineering, inference time, and model bias.\nManage and process large, unstructured datasets using SQL and NoSQL databases, ensuring smooth integration with AI models.\nBuild and deploy AI-driven APIs and services, providing scalable access to LLM-based solutions.\nUse data visualization tools (e.g., Matplotlib, Seaborn, Tableau) to communicate AI model performance, insights, and results to non-technical stakeholders.\n\nSecondary Responsibilities\n\nContribute to data analysis projects, with a strong emphasis on text analytics, natural language understanding, and Generative AI applications.\nBuild, validate, and deploy predictive models specifically tailored to text data, including models for text generation, classification, and entity recognition.\nHandle large, unstructured text datasets, performing essential preprocessing and data cleaning steps, such as tokenization, lemmatization, and noise removal, for machine learning and NLP tasks.\nWork with cutting-edge text data processing techniques, ensuring high-quality input for training and fine-tuning Large Language Models (LLMs).\nCollaborate with cross-functional teams to develop and deploy scalable AI-powered solutions that process and analyze textual data at scale.\n\nKey Success Metrics\n\nEnsure timely deliverables. Spot Training Infrastructure fixes. Lead technical aspects of the projects. Error free deliverables.\n\nEducation Qualification\n\nGraduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)\n\nPost-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA\n\nExperience: 5-10 years of relevant experience",
        "skills": [
            "T5",
            "Generative AI",
            "Hugging Face Transformers",
            "GPT",
            "R",
            "Statistics",
            "BERT",
            "Matplotlib",
            "Machine Learning",
            "Tableau",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Pytorch",
            "Data Visualization",
            "Seaborn",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "HDFC Bank",
        "experience": "8-12 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Responsibilities:\nMachine Learning Engineer with strong knowledge of Python and cloud based deployments\nHands on experience in Java, Springboot, Quarks and database technologies\nYou will be part of hands-on team developing ML based application for the bank using various ML techniques\nYou will be required to build and deply ML applications in cloud using NLP, LLMs and other supervised learning techniques.\nYou will be required to efficiently understand the data domain of the bank to perform EDA and MI report. And eventually designing and developing ML application on the data for multiple programs.\nUse data to change the economic ecosystem of the bank and Indian Financial space.\nThe logo of the team is data can create magic and revenue streams for the bank and ML+MLOps is just a way achieve the goal.\nKeyskills Required:\n8-12 years proven experience as a Data Scientist or Data Analyst\nMachine Learning Engineer with strong knowledge of Python and cloud based deployments\nHands-on development experience and in-depth knowledge of Java, Golang, Spring boot and front-end technologies\nHave working knowledge of LLMs via APIs like Open AI and deploying open source LLMs like LLAMA2 and FLAN T5 etc\nWorking experience of Cloud preferably Azure. Understanding of MLOps frameworks is a big plus",
        "skills": [
            "LLAMA2",
            "FLAN T5",
            "Open AI",
            "LLMs",
            "Java",
            "Golang",
            "Nlp",
            "MLops",
            "Spring Boot",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Fiserv",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Thane, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist Fraud & Credit Risk - Decision Science\nLocation: Thane\nWhat does a successful Data Scientist, Fraud & Credit Risk - Decision Science do at Fiserv\nAs a Data Scientist in Fraud & Credit Risk - Decision Science at Fiserv, you will play a pivotal role in our Global Business Solutions (Merchant business) team. You will be responsible for developing and deploying predictive ML models that automate risk controls, reducing fraud and credit risk losses while driving top-line growth. Your efforts will generate insightful analytics, build models/rules, and create data-driven solutions to manage risk and identify new opportunities. This role interfaces with internal and external stakeholders to deliver best-in-class analytical solutions and supports functions including New Merchant On-boarding & Underwriting, Existing Merchant Risk Monitoring, and more.\nWhat will you do\n- Develop and deploy ML/predictive models using internal and external data.\n- Track and monitor model performance and provide analytical support for business decisions.\n- Conduct complex analysis using statistical and quantitative techniques.\n- Evaluate and integrate data from various sources for modeling, analytics, and reporting.\n- Generate insights from data to assess key performance indicators/trends.\n- Partner with business and technical SMEs to analyze and solve business problems.\n- Support the transformation of risk data capabilities through advanced technology and real-time decision-making.\n- Implement models and decision rules in production with IT/deployment teams.\n- Develop documentation to meet internal and external stakeholder requirements.\n- Coach junior team members and oversee project delivery.\nWhat you will need to have\n- Bachelor's degree in Mathematics, Statistics, Computer Science, Engineering, or related field.\n- 7+ years of experience in risk/marketing data analytics or predictive modeling.\n- Proficiency in SQL, Python, SAS, or other analytical tools/open-source programming languages.\n- Strong technical skills and problem-solving ability.\n- Excellent communication and interpersonal skills.\nWhat would be great to have\n- Master's degree in Mathematics, Statistics, Computer Science, Engineering, or related field.\n- 10+ years of relevant experience.\n- Experience in statistical/financial modelling in the FinTech/Payment's domain.\n- Experience with credit bureaus and other external data sources.\n-Hands-on experience with AI/Money Laundering techniques.",
        "skills": [
            "Analytics",
            "AI Money Laundering techniques",
            "quantitative techniques",
            "Predictive Modeling",
            "SAS",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Jr. Data Scientist (Analyst III), Fraud & Credit Risk - Decision Science",
        "company_name": "Fiserv India Private Limited",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Thane, India",
        "industry": "IT/Computers - Software",
        "job_description": "Responsibilities\nCalling all innovators - find your future at Fiserv.\nWe're Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day - quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we're involved. If you want to make an impact on a global scale, come make a difference at Fiserv.\nJob Title\nJr. Data Scientist (Analyst III), Fraud & Credit Risk - Decision Science\nWhat does a successful Data Scientist, Fraud & Credit Risk - Decision Science do at Fiserv\nAs a Data Scientist in Fraud & Credit Risk - Decision Science at Fiserv, you will play a pivotal role in our Global Business Solutions (Merchant business) team. You will be responsible for developing and deploying predictive ML models that automate risk controls, reducing fraud and credit risk losses while driving top-line growth. Your efforts will generate insightful analytics, build models/rules, and create data-driven solutions to manage risk and identify new opportunities. This role interfaces with internal and external stakeholders to deliver best-in class analytical solutions and supports functions including New Merchant On-boarding & Underwriting, Existing Merchant Risk Monitoring, and more.\nWhat will you do\n- Develop and deploy ML/predictive models using internal and external data.\n- Track and monitor model performance and provide analytical support for business decisions.\n- Conduct complex analysis using statistical and quantitative techniques.\n- Evaluate and integrate data from various sources for modeling, analytics, and reporting.\n- Generate insights from data to assess key performance indicators/trends.\n- Partner with business and technical SMEs to analyze and solve business problems.\n- Support the transformation of risk data capabilities through advanced technology and real-time decision-making.\n- Implement models and decision rules in production with IT/deployment teams. - Develop documentation to meet internal and external stakeholder requirements.\n- Coach junior team members and oversee project delivery.\nWhat you will need to have\n- Bachelor's degree in Mathematics, Statistics, Computer Science, Engineering, or related field.\n- 7+ years of experience in risk/marketing data analytics or predictive modeling. - Proficiency in SQL, Python, SAS, or other analytical tools/open-source programming languages.\n- Strong technical skills and problem-solving ability.\n- Excellent communication and interpersonal skills.\nWhat would be great to have\n- Master's degree in Mathematics, Statistics, Computer Science, Engineering, or related field.\n- 10+ years of relevant experience.\n- Experience in statistical/financial modelling in the FinTech/Payment's domain. - Experience with credit bureaus and other external data sources.\n-Hands-on experience with AI/Money Laundering techniques\nThank you for considering employment with Fiserv. Please:\nApply using your legal name\nComplete the step-by-step profile and attach your resume (either is acceptable, both are preferable).\nOur commitment to Diversity and Inclusion:\nFiserv is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, gender, gender identity, sexual orientation, age, disability, protected veteran status, or any other category protected by law.\nNote to agencies:\nFiserv does not accept resume submissions from agencies outside of existing agreements. Please do not send resumes to Fiserv associates. Fiserv is not responsible for any fees associated with unsolicited resume submissions.\nWarning about fake job posts:\nPlease be aware of fraudulent job postings that are not affiliated with Fiserv. Fraudulent job postings may be used by cyber criminals to target your personally identifiable information and/or to steal money or financial information. Any communications from a Fiserv representative will come from a legitimate Fiserv email address.",
        "skills": [
            "quantitative techniques",
            "Sql",
            "Python",
            "SAS",
            "Predictive Modeling"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Hitachi Energy",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Share this job\n\nBusiness Information\n\n\nOur Indian Operations Centre (INOPC) is a competence center with around 2600+ skilled engineers who focus on tendering, engineering, planning, procurement, functional system testing, installation supervision, and commissioning. Over the last decade, INOPC has evolved to become the largest engineering hub serving more than 40 countries across different energy sectors. The team caters to the four business unitsTransformers, Grid Integration, Grid Automation, High Voltage and has successfully executed engineering and commissioning for projects in more than 80 countries.\n\nMission Statement\n\nFull responsibility of driving marketing and sales function initiatives along with Pricing specialist to support business for improving profitability, identifying the demand market, behavior analysis of products. Strong technical leadership to the team in analyzing the business by developing models which allow to make efficient decision making to improve business opportunities & profitability. You are responsible for supporting the Sales and Strategic Marketing functions of the global Business Unit (BU) High Voltage in understanding market, competitor trends and Price performance by developing and implementing state-of-the-art data Analytics systems as well as process and tool support.\n\nYour Responsibilities\n\nResponsible for supporting the Sales and Strategic Marketing functions of the global Business Unit (BU) High Voltage in understanding market, competitor trends and Price performance by developing and implementing state-of-the-art data Analytics model as well as process and tool support.\nUnderstand the product & market segmentation of portfolio and build know-how of data eco systems in various process involved in Business.\nPrepare reports with clear information understanding the raw information and interpret the date in optimized KPIs (target achievement, Price performance, Order trend, pipeline development, etc), profitability.\nCollaborate with Global, HUB & Pricing specialist to develop and implement competitive strategies, etc.\nBe in charge of the development and updation of models using Python as well as related user training in order to increase the analytical capabilities of our sales management around the globe.\nResponsible to continuously improve process performance and data quality\nDocument and follow the guidelines for the efficient business\nAssess data from different sources to gain insights into pricing strategies and market trends.\nAnalyze historical sales data to identify trends, patterns, and insights that can drive business decisions.\nDevelop and implement predictive models to forecast future sales and demand for power system products.\nCollaborate with CFT to understand business requirements and translate them into analytical solutions.\nPerform data cleaning, preprocessing, and validation to ensure data quality and accuracy.\nUtilize machine learning algorithms and statistical techniques to build robust models.\nContinuously monitor and improve model performance based on feedback and new data.\nStay updated with the latest industry trends and advancements in data science and machine learning.\nLiving Hitachi Energy's core values of safety and integrity, which means taking responsibility for your own actions while caring for your colleagues and the business\n\nYour Background\n\nBachelor's or master's degree in data science, Statistics, Computer Science, or a related field.\nProven experience in data analysis, modeling, and machine learning.\nProficiency in programming languages such as Python, R, or SQL.\nStrong understanding of statistical methods and machine learning algorithms.\nExperience with data visualization tools like Tableau, Power BI, or similar.\nExcellent problem-solving skills and attention to detail.\nAbility to work independently and as part of a team.\nKnowledge of big data technologies and cloud platforms.\nStrong communication skills to effectively convey complex analytical concepts to non-technical stakeholders.\nAny graduation (Science, Statistics, Computer applications, Engineering) 4-7 years relevant experience\nStrong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.\nAttention to detail and a commitment to delivering high-quality digital solutions\n\nApply now\n\nLocation Chennai, Tamil Nadu, India Job type Full time Experience Experienced Job function Sales, Marketing & Product Management Contract Regular Publication date 2025-04-29 Reference number R0090714",
        "skills": [
            "Modeling",
            "Data Analysis",
            "cloud platforms",
            "data visualization tools",
            "R",
            "statistical methods",
            "Machine Learning",
            "Power Bi",
            "Big Data Technologies",
            "Tableau",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST (MLE)",
        "company_name": "Walmart Global Tech India",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout the Team:\n\nSpark Driver Platform operates across all 50 U.S. states, empowering millions of drivers to provide local delivery and making it Walmarts largest local delivery service provider. The Spark Driver Applied AI team is dedicated to enhancing efficiency and profitability by pushing the boundaries of machine learning, data science, and economics. We build highly reliable and scalable production systems that drive the growth of the Spark Driver Platform. Our key focus areas include Capacity Planning, Driver Search, Driver Matching, Driver Offer Pricing, Driver Surge Pricing, Incentive Strategy, ETA Prediction, and GenAI Automation for Driver Support.\n\nAbout the Role:\n\nWalmarts Spark Platform is revolutionizing last-mile delivery by leveraging advanced AI and ML technologies. We are looking for a Senior Machine Learning Engineer to develop and optimize predictive systems that estimate delivery times with high accuracy. This role offers an opportunity to work on large-scale, real-time data pipelines and deploy ML solutions that directly impact millions of customers and drivers.\n\nJob Description\n\nWhat youll Do:\n\nCollaborate with data scientists, software engineers, and product teams to scale models into production systems.\nDesign, deploy, and scale machine learning systems to predict estimated delivery times (ETAs) based on dynamic real-world conditions.\nWork with large-scale datasets, including GPS tracking, order details, and traffic patterns.\nDesign and optimize ML pipelines, ensuring scalability, efficiency, and real-time processing capabilities.\nDeploy and Scale end-to-end models, balancing accuracy and computational efficiency.\nEnsure models meet business and operational requirements, optimizing for accuracy, latency, and reliability.\nExplore and integrate the latest AI advancements, ensuring Walmart remains a leader in AI-driven delivery solutions.\n\nAdditional Job Description\n\nWhat youll bring:\n\nPh.D. or equivalent advanced degree in Computer Science, Statistics, Operations Research, Economics, or a related field.\nProven track record of deploying ML models in production, preferably in high-scale environments.\nStrong proficiency in Python, TensorFlow, PyTorch, or other ML frameworks. Experience with time-series forecasting, real-time data processing, or predictive modeling.\nHands-on experience with cloud platforms such as GCP, or Azure.\nKnowledge of big data technologies like Spark, Kafka, and SQL.\nProficiency in MLOps practices, including CI/CD for ML pipelines and model monitoring.\nKnowledge of reinforcement learning or deep learning techniques for sequential decision-making.\nStrong understanding of DevOps tools and infrastructure automation.\nAbility to drive projects from conception to production in a fast-paced, dynamic environment.\nExcellent communication and leadership skills, with the ability to influence and inspire cross-functional teams.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionals within the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2172509",
        "skills": [
            "infrastructure automation",
            "time-series forecasting",
            "reinforcement learning",
            "MLOps practices",
            "deep learning techniques",
            "ML frameworks",
            "CI CD for ML pipelines",
            "real-time data processing",
            "model monitoring",
            "Kafka",
            "Big Data Technologies",
            "Sql",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "Predictive Modeling",
            "Spark",
            "Azure",
            "Devops Tools",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - Advanced Analytics",
        "company_name": "Ericsson",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Opportunity :\n\nEricsson is looking for a highly skilled Data Scientist with a minimum of 7 years of hands-on experience in Power BI, Tableau, Python, SQL, AI/ML, and Data Analytics. This role demands strong analytical thinking, data storytelling skills, and a deep understanding of machine learning models and data visualization techniques to drive strategic business decisions.\n\nWhat You will do :\n\nDesign and develop interactive dashboards and reports using Power BI and Tableau.\nBuild and deploy predictive models using Python and AI/ML frameworks.\nPerform advanced data analytics using SQL to derive actionable insights.\nCollaborate with cross-functional teams to define data requirements and deliver solutions.\nTranslate complex datasets into strategic insights and compelling visual stories.\nOptimize data workflows and model performance for scalability and efficiency.\n\nThe Skills you bring :\n\nMinimum 7 years of experience in data science and analytics.\nStrong proficiency in Power BI, Tableau, Python, and SQL.\nProven track record in building and deploying AI/ML models.\nSolid understanding of statistical methods and data preprocessing techniques.\nExperience in telecom or related domains is a plus.\nExcellent communication and stakeholder management skills.",
        "skills": [
            "Ai",
            "Ml",
            "Power Bi",
            "Tableau",
            "Data Analytics",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST (MLE)",
        "company_name": "Walmart Global Tech India",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout the Team:\n\nSpark Driver Platform operates across all 50 U.S. states, empowering millions of drivers to provide local delivery and making it Walmart's largest local delivery service provider. The Spark Driver Applied AI team is dedicated to enhancing efficiency and profitability by pushing the boundaries of machine learning, data science, and economics. We build highly reliable and scalable production systems that drive the growth of the Spark Driver Platform. Our key focus areas include Capacity Planning, Driver Search, Driver Matching, Driver Offer Pricing, Driver Surge Pricing, Incentive Strategy, ETA Prediction, and GenAI Automation for Driver Support.\n\nAbout the Role:\n\nWalmart's Spark Platform is revolutionizing last-mile delivery by leveraging advanced AI and ML technologies. We are looking for a Senior Machine Learning Engineer to develop and optimize predictive systems that estimate delivery times with high accuracy. This role offers an opportunity to work on large-scale, real-time data pipelines and deploy ML solutions that directly impact millions of customers and drivers.\n\nJob Description\n\nWhat you'll Do:\n\nCollaborate with data scientists, software engineers, and product teams to scale models into production systems.\nDesign, deploy, and scale machine learning systems to predict estimated delivery times (ETAs) based on dynamic real-world conditions.\nWork with large-scale datasets, including GPS tracking, order details, and traffic patterns.\nDesign and optimize ML pipelines, ensuring scalability, efficiency, and real-time processing capabilities.\nDeploy and Scale end-to-end models, balancing accuracy and computational efficiency.\nEnsure models meet business and operational requirements, optimizing for accuracy, latency, and reliability.\nExplore and integrate the latest AI advancements, ensuring Walmart remains a leader in AI-driven delivery solutions.\n\nAdditional Job Description\n\nWhat you'll bring:\n\nPh.D. or equivalent advanced degree in Computer Science, Statistics, Operations Research, Economics, or a related field.\nProven track record of deploying ML models in production, preferably in high-scale environments.\nStrong proficiency in Python, TensorFlow, PyTorch, or other ML frameworks. Experience with time-series forecasting, real-time data processing, or predictive modeling.\nHands-on experience with cloud platforms such as GCP, or Azure.\nKnowledge of big data technologies like Spark, Kafka, and SQL.\nProficiency in MLOps practices, including CI/CD for ML pipelines and model monitoring.\nKnowledge of reinforcement learning or deep learning techniques for sequential decision-making.\nStrong understanding of DevOps tools and infrastructure automation.\nAbility to drive projects from conception to production in a fast-paced, dynamic environment.\nExcellent communication and leadership skills, with the ability to influence and inspire cross-functional teams.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2172509",
        "skills": [
            "Model Monitoring",
            "reinforcement learning",
            "MLOps Practices",
            "Deep Learning Techniques",
            "Tensorflow",
            "Machine Learning",
            "Pytorch",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "S&P Global Market Intelligence",
        "experience": "7-10 Years",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "About the Role:\n11\nThe Team:\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\nThe Impact:\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\nWhat's in it for you:\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\nKey Responsibilities\nBuild production ready data acquisition and transformation pipelines from ideation to deployment\nBeing a hands-on problem solver and developer helping to extend and manage the data platforms\nArchitect and lead the development of end-to-end data ingestion and processing pipelines to support downstream ML workflows\nApply best practices in data modeling and building ETL pipelines (streaming and batch) using cloud-native solutions\nMentor junior and mid-level data engineers and provide technical guidance and best practices\nWhat We're Looking For:\n7-10 years of professional software work experience\nExpertise in Python and Apache Spark\nOOP Design patterns, Test-Driven Development and Enterprise System design\nSQL (any variant, bonus if this is a big data variant)\nProficient in optimizing data flows for performance, storage, and cost efficiency\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\nNice to have\nCore Java 17+, preferably Java 21+, and associated toolchain\nDevOps with a keen interest in automation\nApache Avro\nApache Kafka\nKubernetes\nCloud expertise (AWS and GCP preferably)\nOther JVM based languages - e.g. Kotlin, Scala\nC# - in particular .NET Core\nWhat's In It For You\nOur Purpose:\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology-the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\nWe're more than 35,000 strong worldwide-so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\nOur Values:\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything you-and your career-need to thrive at S&P Global.\n\nOur benefits include:\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awards-small perks can make a big difference.\nFor more information on benefits by country visit:\nGlobal Hiring and Opportunity at S&P Global:\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n-----------------------------------------------------------\nEqual Opportunity Employer\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\nIf you need an accommodation during the application process due to a disability, please send an email to: and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -\n-----------------------------------------------------------\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority - Ratings - (Strategic Workforce Planning)",
        "skills": [
            "Enterprise System design",
            "Test-Driven Development",
            "ETL pipelines",
            "Cloud-native solutions",
            "OOP Design patterns",
            "Sql",
            "Linux Os",
            "Python",
            "Github",
            "Apache Spark",
            "Git",
            "Azure DevOps"
        ]
    },
    {
        "job_title": "Data Scientist II",
        "company_name": "Worley",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Building on our past. Ready for the future\n\nWorley is a global professional services company of energy, chemicals and resources experts. We partner with customers to deliver projects and create value over the life of their assets. We're bridging two worlds, moving towards more sustainable energy sources, while helping to provide the energy, chemicals and resources needed now.\n\nAt Worley, our Digital team collaborates closely with the business to deliver efficient, technology-enabled sustainable solutions, that will be transformational for Worley. This team, aptly named Worley Digital, is currently seeking talented individuals who would be working on a wide range of latest technologies, including solutions based on Automation, Generative AI.\n\nWhat drives us at Worley Digital It's our shared passion for pushing the boundaries of technological innovation, embracing best practices, and propelling Worley to the forefront of industry advancements. If you're naturally curious, open-minded, and a self-motivated learner - one who's ready to invest time and effort to stay future-ready - then Worley could be your ideal workplace.\n\nPosition Title (Global): Sr ML Engineer\n\nMAJOR ACCOUNTABILITIES OF POSITION:\n\nUnderstanding business objectives and developing models that help to achieve them, along with metrics to track their progress Utilize the existing frameworks, standards, patterns to create architectural foundation and services necessary for AI applications that scale from multi-user to enterprise class\nManaging Data Science project life cycle from exploratory data analysis to productization (Alpha/Beta Release) .Manage small team to collaborate with Architecture, Data Warehouse, Data Governance teams for providing analytics as service.\nMentor team member for AI/ML development\nVerifying data quality, and/or ensuring it via data cleaning Supervising the data acquisition process if more data is needed Finding available datasets online that could be used for training Defining validation strategies Defining the preprocessing or feature engineering to be done on a given dataset.\nDefining data augmentation pipelines Training models and tuning their hyperparameters Analyzing the errors of the model and designing strategies to overcome them Deploying models to production.\nDevelopment of the ML algorithms that could be used to solve a given problem and ranking them by their success probability.\nExploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world\n\nKnowledge / Experience / Competencies Required\n\nIT Skills & Experience (Priority wise):\n\nProven experience as a Data Scientist AI/ML or similar role\nAbility to write robust code in Python.\nExperience in the Generative AI components like LLMs, LangChain, LlamaIndex, OpenAI, Mistral, Llama etc.\nExperience in supervised/semi-supervised and unsupervised machine learning algorithms.\nExperience using the cognitive APIs machine learning studios on cloud.\nUp to speed on NLP (Summarization, Translation models, Named Entity Recognition)\nHands-on knowledge of image processing with deep learning (CNN, RNN, LSTM, GAN)\nUnderstanding of complete AI/ML project life cycle.\nUnderstanding of data structures, data modelling and software architecture.\n\nPeople Skills:\n\nAbility to communicate clearly and concisely and a flexible mindset to handle a quickly changing culture\nAbility to work independently and/or as part of cross-domain big team\nProfessional and open communication to all internal and external interfaces.\nAccurately report to management in a timely and effective manner.\n\nOther Skills:\n\nOutstanding analytical and problem-solving skills\nTaking ownership of the tasks in hand and being accountable for deliverables.\n\nEducation Qualifications, Accreditation, Training:\n\nMinimum 47 years experience as a Data Scientist on AI and ML projects.\nMaster's in information technology / Big Data/Data Science/AI/Computer Science or related field\n\nMoving forward together\n\nWe're committed to building a diverse, inclusive and respectful workplace where everyone feels they belong, can bring themselves, and are heard. We provide equal employment opportunities to all qualified applicants and employees without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by law.\n\nWe want our people to be energized and empowered to drive sustainable impact. So, our focus is on a values-inspired culture that unlocks brilliance through belonging, connection and innovation.\n\nAnd we're not just talking about it; we're doing it. We're reskilling our people, leveraging transferable skills, and supporting the transition of our workforce to become experts in today's low carbon energy infrastructure and technology.\n\nWhatever your ambition, there's a path for you here. And there's no barrier to your potential career success. Join us to broaden your horizons, explore diverse opportunities, and be part of delivering sustainable change.\n\nEducation Qualifications, Accreditation, Training:\n\nMaster's in Information Technology / Big Data/Data Science/AI/Computer Science\n\nMoving forward together\n\nWe're committed to building a diverse, inclusive and respectful workplace where everyone feels they belong, can bring themselves, and are heard. We provide equal employment opportunities to all qualified applicants and employees without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by law.\n\nWe want our people to be energized and empowered to drive sustainable impact. So, our focus is on a values-inspired culture that unlocks brilliance through belonging, connection and innovation.\n\nAnd we're not just talking about it; we're doing it. We're reskilling our people, leveraging transferable skills, and supporting the transition of our workforce to become experts in today's low carbon energy infrastructure and technology.\n\nWhatever your ambition, there's a path for you here. And there's no barrier to your potential career success. Join us to broaden your horizons, explore diverse opportunities, and be part of delivering sustainable change.\n\nCompany\n\nWorley\n\nPrimary Location\n\nIND-MM-Navi Mumbai\n\nOther Locations\n\nIND-KR-Bangalore, IND-MM-Mumbai, IND-MM-Pune, IND-TN-Chennai, IND-GJ-Vadodara, IND-AP-Hyderabad, IND-WB-Kolkata\n\nJob\n\nDigital Platforms & Data Science\n\nSchedule\n\nFull-time\n\nEmployment Type\n\nEmployee\n\nJob Level\n\nExperienced\n\nJob Posting\n\nMay 8, 2025\n\nUnposting Date\n\nJun 7, 2025\n\nReporting Manager Title\n\nSenior Manager",
        "skills": [
            "Translation models",
            "Mistral",
            "LangChain",
            "Named Entity Recognition",
            "LSTM",
            "Generative AI",
            "unsupervised machine learning algorithms",
            "LLMs",
            "supervised machine learning algorithms",
            "cognitive APIs",
            "GAN",
            "OpenAI",
            "LlamaIndex",
            "machine learning studios on cloud",
            "Cnn",
            "Deep Learning",
            "Nlp",
            "Rnn",
            "Software Architecture",
            "data structures",
            "Summarization",
            "Python",
            "Image Processing",
            "Data Modelling"
        ]
    },
    {
        "job_title": "Data Scientist- Across PAN India",
        "company_name": "Capgemini Engineering",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Title: Data Scientist\nCompany Overview: Capgemini Engineering is a global leader in engineering services, bringing together a worldwide team of engineers, scientists, and architects to assist the most innovative companies in unleashing their potential.\nPosition Overview: We are seeking a skilled Data Scientist with expertise in Cognite Data Fusion, data modelling, Unified Namespace (UNS), ontologies, and the identification of data products and datasets. The ideal candidate will have a strong background in developing and implementing data science projects, analyzing large and complex data sets, and driving data-driven decision-making across the organization\nKey Responsibilities:\nSolution Development: Design, implement, and deploy scalable data solutions utilizing Cognite Data Fusion, focusing on data modeling, UNS, and ontologies to address industry-specific challenges.\nData Analysis: Analyze large and complex data sets to identify trends, insights, and opportunities, supporting solution development and business strategies.\nCollaboration: Collaborate with cross-functional teams to understand data needs and translate them into data science solutions, ensuring seamless integration and operationalization of digital solutions across various domains.\nClient Engagement: Engage with clients to understand their business objectives, lead discovery workshops, and provide expert guidance on data-driven strategies and potential challenges.\nVisualization: Develop dashboards and visualizations using tools such as Power BI, Grafana, or web development frameworks like Plotly Dash and Streamlit to effectively communicate data insights.\nMentorship: Provide guidance and mentorship to junior team members, promoting best practices in data science and software development.\nQualifications:\nEducational Background: Master's or PhD degree in a quantitative field.\nExperience: Minimum of 2 years of experience in data science, with a strong background in developing analytical solutions within domains such as pharma, oil and gas, manufacturing, or power & utilities.\nTechnical Skills: Proficiency in Python and its data ecosystem (pandas, numpy), machine learning libraries (scikit-learn, keras), and experience with SQL.\nVisualization Tools: Experience with data visualization tools like Power BI, Grafana, Tableau, or web development frameworks such as Plotly Dash and Streamlit.\nSoftware Practices: Strong understanding of software development practices, including version control (e.g., Git), automated testing, and documentation.\nCloud Platforms: Experience with cloud services such as GCP, Azure, or AWS is advantageous.\nDomain Knowledge: Familiarity with industrial data management concepts, including Unified Namespace (UNS), ontologies, and data product identification.\nCommunication Skills: Excellent communication and collaboration skills, with the ability to work with cross-functional teams and stakeholders.\nLeadership: Demonstrated ability to lead projects and mentor junior team members.\nPreferred Qualifications:\nIndustry Expertise: Experience serving as a domain expert on internal or customer projects within relevant industries.\nCloud Deployment: Experience deploying models and solutions in production environments using cloud infrastructure.\nContinuous Learning: Willingness to stay updated with the latest developments in data science and related technologies.",
        "skills": [
            "scikit-learn",
            "Plotly",
            "Documentation",
            "Cognite Data Fusion",
            "Dash",
            "Streamlit",
            "Power Bi",
            "Data Modelling",
            "Tableau",
            "Automated Testing",
            "Grafana",
            "Sql",
            "Git",
            "Pandas",
            "Numpy",
            "Gcp",
            "Keras",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Specialist Data Scientist (Python with OpenAI)",
        "company_name": "AT&T",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description:\n\nThe candidate should have expertise in Hands-on experience in developing and deploying machine learning models into production, ensuring scalability and reliability, handling data, financial/risk modeling, anomaly detection, and cloud environments (preferably Azure). Strong programming skills in Python, and experience with SQL for data manipulation.\n\nExcellent oral and written communication skills to effectively engage with team members, stakeholders, and end-users.\n\nRoles and Responsibilities:\n\nDevelop, enhance, and support assigned applications, ensuring seamless functionality and high performance.\nTake full ownership of assigned projects or modules, providing end-to-end solutions from development to deployment.\nParticipate in client engagements to enhance existing ML models, develop advanced models for business needs, and design experiments for test/control analysis and business process trials.\nCollaborate with cross-functional teams on AI/ML, knowledge discovery, data modeling, and analytics to address business challenges.\nBuild, deploy, and maintain predictive ML models, ensuring production readiness and optimal performance.\nExpertise in building multi class classification models, handling high dimensional data.\nSkilled in working within big data environments utilizing the capabilities of PySpark.\nExpertise in leveraging frameworks such as OpenAI, LLama, Langchain, and Langraph to develop robust, scalable AI solutions.\nProficient in text embeddings, document parsing, and advanced natural language processing (NLP) techniques.\nExperience in designing and implementing agent-based systems for workflow automation.\nWell-versed in autonomous agents and orchestration frameworks.\nDesign and execute experiments such as A/B testing or DOE (Design of Experiments) to validate models and hypotheses.\nWrite and execute data extraction algorithms to collect data from primary or secondary sources and define data requirements for analysis.\nTroubleshoot issues, monitor performance, and optimize deployed models and processes for continuous improvement.\nGain expertise in the organization's data framework and its alignment with business processes. Recommend any additional data needs and coordinate with data engineers to ensure data suitability.\nApply advanced data analytics and strategies to optimize statistical efficiency and model quality.\nInterpret data and analyze results using analytical techniques, providing actionable insights and regular reports to stakeholders.\nIdentify, analyze, and interpret trends or patterns in complex data sets to uncover actionable insights.\nTrain and develop machine learning models, run evaluation experiments, refine/test/validate models, and ensure robust deployment in production environments using FasAPI framework.\nWork with management and stakeholders to prioritize business and informational needs for data-driven solutions.\nStay current with industry trends, share best practices, and provide subject matter expertise within the team.\nIdentify new opportunities for process improvement using statistical testing, predictive modeling, and data-driven methodologies.\nAdhere to company policies, procedures, and security requirements while maintaining compliance standards.\n\nQualifications Required / Desired Skills:\n\nDegree in one or more quantitative discipline Operations Research, Stats, Math, Comp Sci, Engineering, Economics or similar\nExperience in developing a variety of machine learning models algorithms in a commercial environment with a track record of creating meaningful business impact.\nProficient with PySpark, NoSQL and Python and distributed programming\nExpertise working in MongoDB, Databricks and cloud computing platforms (Azure), or equivalent on-premise platform and deployment\nprovide hands-on technical guidance; conduct code reviews.\nAbility to simultaneously coordinate and track multiple deliverables, tasks and dependencies across multiple stakeholders / business areas.\nExperience in client engagements, interpreting client's business challenges, and recommendations for statistical analysis solutions (i.e. analytical consulting and solution design)\nExperience in presentation design, development, delivery, and communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nExperience in Gen AI, LLM Workflow, Graph RAG etc. is an added advantage.\nFamiliarity with visualization tools like PowerBI and Tableau is an added advantage.\nFlexible to work from office 3 days(in a week) from 12:30pm to 9:30pm\n\nWeekly Hours:\n\n\n40\n\nTime Type:\n\nRegular\n\nLocation:\n\nIND:KA:Banglaore / Intl Tech Park, Whitefield Rd - Storage: Innovator Building, Itpb, Whitefield Rd\n\nIt is the policy of AT&T to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, AT&T will provide reasonable accommodations for qualified individuals with disabilities. AT&T is a fair chance employer and does not initiate a background check until an offer is made.\n\nJobCategory:BigData",
        "skills": [
            "FasAPI",
            "Langchain",
            "Document parsing",
            "OpenAI LLama",
            "Text embeddings",
            "Langraph",
            "Pyspark",
            "Sql",
            "Nosql",
            "Databricks",
            "MongoDB",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Policybazaar.com",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Insurance",
        "job_description": "As a Data Scientist, you'll design, develop, and deploy natural language processing models to drive business value. You'll collaborate with cross-functional teams to identify opportunities, collect and pre-process data, and implement scalable NLP solutions.\nResponsibilities :\nDevelop and fine-tune Transformer models (e.g., BERT, RoBERTa, XLNet) for NLP tasks (e.g., text classification, sentiment analysis, named entity recognition)\nCollect, pre-process, and annotate datasets for NLP model training\nConduct research and stay up-to-date with advancements in Transformer architectures and NLP\nDevelop and fine-tune conversational AI models (e.g., intent recognition, sentiment analysis,dialogue management)\nRequirements :\n3+ years of experience in NLP, machine learning, and data science\nMaster's or Ph.D. in Computer Science/Data Science (Preferable)\nExperience with deep learning frameworks\nClarity on fundamentals of NLP and ML algorithms\nExcellent problem-solving and communication skills",
        "skills": [
            "Transformer models",
            "XLNet",
            "dialogue management",
            "conversational AI",
            "deep learning frameworks",
            "text classification",
            "intent recognition",
            "BERT",
            "named entity recognition",
            "RoBERTa",
            "Sentiment Analysis"
        ]
    },
    {
        "job_title": "Data Scientist Frontend Architect",
        "company_name": "Ericsson",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About this opportunity:\n\nTeam is now looking for an experienced Front-End Architect and Front -End developer with strong technical expertise to design and lead the development of scalable, high-performance front-end applications of AI/ML. The ideal candidate should possess a deep understanding of modern front-end technologies, a passion for UX, and experience in integrating various third-party libraries and APIs. The role requires excellent problem-solving skills and the ability to collaborate with cross-functional teams, including back-end developers, DevOps, , UI/UX designers and AI Knowledge.\n\nWhat you will do:\n\nArchitect, design, and implement front-end solutions using Angular (12+), TypeScript, JavaScript, and related frameworks.\n\nDevelop and maintain scalable and reusable front-end components, ensuring a seamless user experience.\n\nIntegrate CSS3, HTML5, and frameworks like Bootstrap or Angular Material to enhance UI functionality and aesthetics.\n\nWork with RxJS, MomentJS, UnderscoreJS, ReactJS, NodeJS, or other third-party libraries as needed to meet project requirements.\n\nDesign testable and maintainable code, incorporating Jasmine and Karma for testing when appropriate.\n\nCollaborate with the DevOps team to manage builds and CI/CD pipelines using Jira, GitLab, and other tools.\n\nFacilitate seamless communication between front-end and back-end teams using REST, JSON, and SOAP for API integration.\n\nLeverage cloud-native development practices and contribute to architecture discussions.\n\nCommunicate effectively with a diverse set of technical audiences to convey complex concepts.\n\nWhat you will Bring:\n\nAngular (12+), TypeScript, JavaScript expertise.\n\nProficiency in RxJS, CSS3, HTML5, and Bootstrap/Angular Material.\n\nExperience with third-party library integration such as MomentJS, UnderscoreJS, ReactJS, NodeJS, or similar.\n\nFamiliarity with REST, JSON, and SOAP integration.\n\nSolid understanding of cloud-native development and CI/CD tools, especially Jira and GitLab.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 766743",
        "skills": [
            "MomentJS",
            "Underscorejs",
            "Nodejs",
            "Jasmine",
            "Soap",
            "Json",
            "Jira",
            "Css3",
            "Typescript",
            "REST",
            "Javascript",
            "Angular 12",
            "Html5",
            "Reactjs",
            "Rxjs",
            "Bootstrap",
            "Gitlab",
            "Angular Material",
            "Karma"
        ]
    },
    {
        "job_title": "Data Scientist with Devops",
        "company_name": "Ericsson",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About this opportunity:\n\nA&AI (SL IT & ADM) team is currently seeking a versatile and motivated DevOps Engineer (with expertise in Kubernetes and Cloud Infrastructure) to join the AI/ML team. This role will be pivotal in managing multiple platforms and systems, focusing on Kubernetes, ELK/Opensearch, and various DevOps tools to ensure seamless data flow for our machine learning and data science initiatives. The ideal candidate should have a strong foundation in Python programming, experience with Elasticsearch, Logstash, and Kibana (ELK), proficiency in MLOps, and expertise in machine learning model development and deployment. Additionally, familiarity with basic Spark concepts and visualization tools like Grafana and Kibana is desirable.\n\nWhat you will do:\n\nDesign and implement robust AI/ML infrastructure using cloud services and Kubernetes to support machine learning operations (MLOps) and data processing workflows.\n\nDeploy, manage, and optimize Kubernetes clusters specifically tailored for AI/ML workloads, ensuring optimal resource allocation and scalability across different network configurations.\n\nDevelop and maintain CI/CD pipelines tailored for continuous training and deployment of machine learning models, integrating tools like Kubeflow, MLflow, ArgoFlow or TensorFlow Extended (TFX).\n\nCollaborate with data scientists to oversee the deployment of machine learning models and set up monitoring systems to track their performance and health in production.\n\nDesign and implement data pipelines for large-scale data ingestion, processing, and analytics essential for machine learning models, utilizing distributed storage and processing technologies such as Hadoop, Spark, and Kafka.\n\nThe skills you bring:\n\nExtensive experience with Kubernetes and cloud services (AWS, Azure, GCP, private cloud) with a focus on deploying and managing AI/ML environments.\n\nStrong proficiency in scripting and automation using languages like Python, Bash, or Perl.\n\nExperience with AI/ML tools and frameworks (TensorFlow, PyTorch, Scikit-learn) and MLOps tools (Kubeflow, MLflow, TFX).\n\nIn-depth knowledge of data pipeline and workflow management tools, distributed data processing (Hadoop, Spark), and messaging systems (Kafka, RabbitMQ).\n\nExpertise in implementing CI/CD pipelines, infrastructure as code (IaC), and configuration management tools.\n\nFamiliarity with security standards and data protection regulations relevant to AI/ML projects.\n\nProven ability to design and maintain reliable and scalable infrastructure tailored for AI/ML workloads.\n\nExcellent analytical, problem-solving, and communication skills.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 766746",
        "skills": [
            "TFX",
            "Scikit-learn",
            "MLflow",
            "Kubeflow",
            "Kibana",
            "Hadoop",
            "Logstash",
            "Kafka",
            "Cloud Infrastructure",
            "Tensorflow",
            "Rabbitmq",
            "Configuration Management",
            "Pytorch",
            "MLops",
            "Elasticsearch",
            "Spark",
            "Python",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Data Scientist - Periscope",
        "company_name": "McKinsey & Company",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who You'll Work With\n\nYou will be based in one of our offices in Bengaluru or Gurugram as part of our Growth, Marketing & Sales practice.\n\nThe Growth, Marketing & Sales practice strives to help client in both consumer and business-to-business environments on a wide variety of marketing and sales topics. The mission of this practice is to help clients achieve marketing-driven profit growth. Our clients benefit from our experience in core areas of marketing such as customer insights, marketing ROI, digital marketing, Contract Lifecycle Management pricing, and sales and channel management.\n\nYour Impact\n\nYou are a highly collaborative individual and enjoy solving problems that focus on adding business value. You have a sense of ownership and enjoy hands-on technical work. Our values resonate with yours.\n\nCollaboration with business stakeholders, engineers, and internal teams to build and implement extraordinary retail-focused data products (reusable assets) and solutions and delivering them right to the client will be of utmost importance.\n\nYou will also be responsible for developing deep retail domain understanding in at least one of the following areas Customer Analytics, Pricing Optimization, Inventory Management, E-commerce, or Digital Transformation.\n\nYou will have a chance to work with teams on cutting edge generative AI and advanced AI initiatives solving customer, marketing & sales and pricing use-cases.\n\nOther Key Responsibilities Will Include\n\nBuild real-world scalable machine learning pipelines and deploy them to production\nOperate at the intersection of data science and software engineering to create analytics solutions\nProduce high-quality code that allows us to put solutions into production\nAutomate ML workflows and deploy models into production environments\nLead the thinking on choosing and using the right analytical libraries, programming languages, and frameworks\nBuild analytics libraries and tooling based on project experience and the latest research, refactor code into reusable libraries, APIs, and tools\nDevelop and execute comprehensive QA tests for data science pipelines\nAutomate testing processes to improve efficiency and consistency\nPlay an active role in leading team meetings and workshops to inform product development and process evolution\n\nWhat You'll Learn\n\nSuccessful projections on real-world problems across Retail use cases are achieved by referencing past deliveries of end-to-end pipelines.\nBuild products alongside the Core engineering team and evolve the engineering process to scale with data, handling complex problems and advanced client situations.\nBe focused on the wrangling, clean-up, and transformation of data by working alongside the Data Science team which focuses on modeling the data.\nUsing new technologies and problem-solving skills in a multicultural and creative environment.\n\nYou will work on the frameworks and libraries that our teams of Data Scientists and Data Engineers use to progress from data to impact. You will guide global companies through data science solutions to transform their businesses and enhance performance across industries including E-commerce, Grocery, F&B, and CPG.\n\nReal-World Impact We provide unique learning and development opportunities.\nFusing Tech & Leadership We work with the latest technologies and methodologies and offer first-class learning programs at all levels.\nMultidisciplinary Teamwork - Our teams include data scientists, engineers, project managers, UX, and visual designers who work collaboratively to enhance performance.\nInnovative Work Culture Creativity, insight, and passion come from being balanced. We cultivate a modern work environment through an emphasis on wellness, insightful talks, and training sessions.\nStriving for Diversity With a multicultural team representing diverse nationalities, we recognize the benefits of working with people from all walks of life.\n\nYour Qualifications and Skills\n\nBachelor's degree in computer science, engineering, mathematics, or equivalent experience\n3+ years of relevant experience with strong foundations of statistics and machine learning techniques\nStrong understanding of machine learning algorithms, model training, validation, and deployment; proficiency in popular ML frameworks such as TensorFlow, PyTorch, and scikit-learn\nProven experience writing production-grade code (Python / PySpark) for machine learning in a professional setting and building scalable machine learning pipelines\nProficiency in software development practices, version control (e.g., Git), CI/CD pipelines, containerization (Docker), and orchestration tools (Kubernetes)\nFamiliarity with cloud platforms (AWS, GCP, Azure, Databricks) and infrastructure as code (Terraform, CloudFormation)\nFamiliarity or hands-on experience with data preprocessing, feature engineering, and building scalable data pipelines using tools like Spark, Kafka, and ETL processes.\nKnowledge of database management (SQL, NoSQL) and data warehousing solutions\nFamiliarity or hands-on experience with data visualization tools (PowerBI, Tableau, etc.)",
        "skills": [
            "scikit-learn",
            "Machine Learning",
            "Cloudformation",
            "Pyspark",
            "Kafka",
            "Tableau",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Gcp",
            "Pytorch",
            "Terraform",
            "Powerbi",
            "Docker",
            "Spark",
            "Databricks",
            "Azure",
            "Python",
            "Kubernetes",
            "Etl",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist- Pytorch/ TensorFlow",
        "company_name": "Ups Supply Chain Solutions",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Logistics",
        "job_description": "RESPONSIBILITIES\nDefines key data sources from UPS and external sources to deliver models.\nDevelops and implements pipelines that facilitates data cleansing, data transformations, data enrichments from multiple sources (internal and external) that serve as inputs for data and analytics systems.\nFor larger teams, works with data engineering teams to validate and test data and model pipelines identified during proof of concepts\nDevelops data design based on the exploratory analysis of large amounts of data to discover trends and patterns that meet stated business needs.\nDefines model key performance indicator (KPI) expectations and validation, testing, and re-training of existing models to meet business objectives.\nReviews and creates repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling.\nSynthesizes insights and documents findings through clear and concise presentations and reports to stakeholders.\nPresents operationalized analytic findings and provides recommendations.\nIncorporates best practices on the use of statistical modeling, machine learning algorithms, distributed computing, cloud-based AI technologies, and run time performance tuning with the goal of deployment and market introduction\nLeverages emerging tools and technologies together with the use of open-source or vendor products in the creation and delivery of insights that support predictive and prescriptive solutions.\nQUALIFICATIONS\nRequirements:\nAbility to take a data science problem from start to finish, use pytorch/tensorflow to build the full model product.\nStrong analytical skills and attention to detail.\nAble to engage key business and executive-level stakeholders to translate business problems to high level analytics solution approach.\nExpertise with statistical techniques, machine learning or operations research and their application in business applications.\nExpertise in R, SQL, Python.\nDeep understanding of data management pipelines and experience in launching moderate scale advanced analytics projects in production at scale.\nDemonstrated experience in Cloud-AI technologies and knowledge of environments both in Linux/Unix and Windows.\nExperience implementing open-source technologies and cloud services; with or without the use of enterprise data science platforms.\nSolid oral and written communication skills, especially around analytical concepts and methods.\nAbility to communicate data through a story framework to convey data-driven results to technical and non-technical audience.\nMasters Degree in a quantitative field of mathematics, computer science, physics, economics, engineering, statistics (operations research, quantitative social science, etc.), international equivalent, or equivalent job experience.Preferences:\nFamiliarity with Java or C++\nEmployee Type:PermanentUPS is committed to providing a workplace free of discrimination, harassment, and retaliation.",
        "skills": [
            "Pipeline",
            "Tensorflow",
            "Java",
            "Data Science",
            "Data Management",
            "Artificial Intelligence",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "R Systems International",
        "experience": "4-8 Years",
        "salary": null,
        "location": "Noida",
        "industry": "Information Technology",
        "job_description": "Leveraging knowledge in analytical and statistical algorithms to assist stakeholders in improving their business\nPartnering on the design and implementation of statistical data quality procedures for existing and new data sources\nCommunicating complex data science solutions, concepts, and analyses to team members and business leaders\nPresenting data insights & recommendations to key stakeholders\nEstablishing links across existing data sources and finding new, interesting data correlations\nEnsuring testing and validation are components of all analytics solutions\nSkills:\nStrong proficiency in predictive analytics, prescriptive analytics, AI, machine learning, and statistical modeling\nProficiency in programming languages such as Python and SQL for data analysis and model development.\nWorking with text data including Natural Language Processing (NLP) techniques such as tokenization, lemmatization, sentiment analysis, text classification, topic modeling, and entity recognition\nHands-on experience with advanced AI techniques such as Retrieval-Augmented Generation (RAG), vector databases, and LLM integration for enhanced information retrieval and generation, GPT, and BERT\nStrong understanding of Machine Learning concepts including supervised and unsupervised learning, model training, evaluation, and deployment",
        "skills": [
            "bert",
            "Llm",
            "Nlp",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist Lead",
        "company_name": "R Systems International",
        "experience": "6-11 Years",
        "salary": null,
        "location": "Chennai, Noida, Pune",
        "industry": "Information Technology",
        "job_description": "The Lead Data Scientistis responsible for designing, developing, and deploying advanced analytics solutions, including predictive and prescriptive outputs, insights, and visualizations to enable business users to make better data-driven decisions. The lead data scientist identifies and applies best analytical techniques to achieve business objectives, including data gathering, cleansing, and integration, statistical transformation and modeling, and feature engineering and selection.\nThe role requires a master's degree in a quantitative field plus 5+ years of experience of relevant work experience, expert knowledge of and experience using advanced statistics and machine learning methods, and a demonstrated aptitude distilling complex business problems into clear data science and advanced analytics models and solutions that can and will be adopted and implemented.",
        "skills": [
            "Data Analysis",
            "Statistical Modeling",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python",
            "Sql",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ups Supply Chain Solutions",
        "experience": "4-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Industrial Manufacturing",
        "job_description": "RESPONSIBILITIES\nDefines key data sources from UPS and external sources to deliver models.\nDevelops and implements pipelines that facilitates data cleansing, data transformations, data enrichments from multiple sources (internal and external) that serve as inputs for data and analytics systems.\nFor larger teams, works with data engineering teams to validate and test data and model pipelines identified during proof of concepts.\nDevelops data design based on the exploratory analysis of large amounts of data to discover trends and patterns that meet stated business needs.\nDefines model key performance indicator (KPI) expectations and validation, testing, and re-training of existing models to meet business objectives.\nReviews and creates repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling.\nSynthesizes insights and documents findings through clear and concise presentations and reports to stakeholders.\nPresents operationalized analytic findings and provides recommendations.\nIncorporates best practices on the use of statistical modeling, machine learning algorithms, distributed computing, cloud-based AI technologies, and run time performance tuning with the goal of deployment and market introduction.\nLeverages emerging tools and technologies together with the use of open-source or vendor products in the creation and delivery of insights that support predictive and prescriptive solutions.\nQUALIFICATIONS\nExpertise in R, SQL, Python and/or any other high-level languages.\nExploratory data analysis (EDA), data engineering and development of advanced analytics models.\nExperience in development of AI and ML using platforms like VertexAI, Databricks or Sagemaker, and familiarity with available frameworks like PyTorch, Tensorflow and Keras.\nExperience applying models from small to medium scaled problems.\nStrong analytical skills and attention to detail.\nAble to engage key business and executive-level stakeholders to translate business problems to a high-level analytics solution approach.\nExpertise with statistical techniques, machine learning, and/or operations research and their application in business.\nDeep understanding of data management pipelines and experience in launching moderate scale advanced analytics projects in production.\nDemonstrated experience in Cloud-AI technologies and knowledge of environments both in Linux/Unix and Windows.\nExperience implementing open-source technologies and cloud services; with or without the use of enterprise data science platforms.\nCore AI / Machine Learning knowledge and application in supervised and unsupervised learning domains.\nFamiliarity with Java or C++ is a plus.\nSolid oral and written communication skills, especially around analytical concepts and methods.\nAbility to communicate data through a story framework to convey data-driven results to technical and non-technical audiences.\nMaster s Degree in a quantitative field of mathematics, computer science, physics, economics, engineering, statistics (operations research, quantitative social science, etc.), international equivalent, or equivalent job experience.",
        "skills": [
            "R",
            "machine learning frameworks",
            "Apache Spark",
            "Microsoft Azure",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "CONTUS TECH",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Services",
        "job_description": "Required Skills:\nPython Programming:Strong proficiency in Python.\nStatistics and Operations Research:Practical knowledge and working experience.\nAI/ML Tools and Frameworks:Experience with Flask, PySpark, PyTorch, TensorFlow, Keras, Databricks, OpenCV, Pillow/PIL, Streamlit, d3js, DashPlotly, and Neo4j.\nAWS Services:Hands-on experience with AWS AI/ML services like Sagemaker, Canvas, and Bedrock.\nMachine Learning Techniques:Understanding of predictive and ML techniques like regression models, XGBoost, random forest, GBM, neural networks, SVM, etc.\nNLP Techniques:Proficient with RNN, LSTM, and attention-based models; experience with models from Stanford, IBM, Azure, OpenAI.\nSQL:Good understanding of SQL for efficient data querying.\nVersion Control:Hands-on experience with version control tools like GitHub or Bitbucket.\nMLOps:Experience deploying ML models into production on platforms like Azure and AWS.\nBusiness Analysis:Ability to understand business needs and map them to business processes.\nAgile Methodology:Experience with agile project delivery.\nVisualization:Good at conceptualizing and visualizing end-to-end business needs both at a high level and in detail.\nCommunication:Good communication, listening, and probing skills.\nAnalytical Skills:Strong analytical and problem-solving skills.\nInterpersonal Skills:Strong interpersonal skills, collaboration with team members, and ability to work effectively in a team.\nKey Responsibilities:\nUnderstand and address business issues with valuable solutions.\nDesign factual, AI, and deep learning models to solve business problems.\nDevelop and deploy statistical, ML, and DL models into production.\nIdentify and augment accessible information sources.\nCreate innovative data visualization graphs using tools like d3js, DashPlotly, and Neo4j.\nPreferred Certifications:\nAWS Specialty Certification in Data Analytics\nMachine Learning",
        "skills": [
            "Machine Learning",
            "Aws",
            "Pyspark",
            "Pytorch",
            "Python Programming",
            "Tensorflow",
            "Data Scientist",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ups Supply Chain Solutions",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Supply Chain Management",
        "job_description": "ThisData Scientistcreates and implements advanced analytics models and solutions to yield predictive and prescriptive insights from large volumes of structured and unstructured data\nThis position works with a team responsible for the research and implementation of predictive requirements by leveraging industry standard machine learning and data visualization tools to draw insights that empower confident decisions and product creation\nThis position leverages emerging tools and technologies available in On-prem and Cloud environments\nThis position utilizes industry standard machine learning and data visualization tools to transform data and analytics requirements into predictive solutions and provide data literacy on a range of machine learning systems at UPS\nThis position identifies opportunities for driving descriptive to predictive and prescriptive solutions, which become inputs to department and project teams on their decisions supporting projects\nRESPONSIBILITIES\nDefines key data sources from UPS and external sources to deliver models.\nDevelops and implements pipelines that facilitates data cleansing, data transformations, data enrichments from multiple sources (internal and external) that serve as inputs for data and analytics systems.\nFor larger teams, works with data engineering teams to validate and test data and model pipelines identified during proof of concepts.\nDevelops data design based on the exploratory analysis of large amounts of data to discover trends and patterns that meet stated business needs.\nDefines model key performance indicator (KPI) expectations and validation, testing, and re-training of existing models to meet business objectives.\nReviews and creates repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling.\nSynthesizes insights and documents findings through clear and concise presentations and reports to stakeholders.\nPresents operationalized analytic findings and provides recommendations.\nIncorporates best practices on the use of statistical modeling, machine learning algorithms, distributed computing, cloud-based AI technologies, and run time performance tuning with the goal of deployment and market introduction.\nLeverages emerging tools and technologies together with the use of open-source or vendor products in the creation and delivery of insights that support predictive and prescriptive solutions.\nQUALIFICATIONS\nExpertise in R, SQL, Python and/or any other high-level languages.\nExploratory data analysis (EDA), data engineering and development of advanced analytics models.\nExperience in development of AI and ML using platforms like VertexAI, Databricks or Sagemaker, and familiarity with available frameworks like PyTorch, Tensorflow and Keras.\nExperience applying models from small to medium scaled problems.\nStrong analytical skills and attention to detail.\nAble to engage key business and executive-level stakeholders to translate business problems to a high-level analytics solution approach.\nExpertise with statistical techniques, machine learning, and/or operations research and their application in business.\nDeep understanding of data management pipelines and experience in launching moderate scale advanced analytics projects in production.\nDemonstrated experience in Cloud-AI technologies and knowledge of environments both in Linux/Unix and Windows.\nExperience implementing open-source technologies and cloud services; with or without the use of enterprise data science platforms.\nCore AI / Machine Learning knowledge and application in supervised and unsupervised learning domains.\nFamiliarity with Java or C++ is a plus.\nSolid oral and written communication skills, especially around analytical concepts and methods.\nAbility to communicate data through a story framework to convey data-driven results to technical and non-technical audiences.\nMaster s Degree in a quantitative field of mathematics, computer science, physics, economics, engineering, statistics (operations research, quantitative social science, etc.), international equivalent, or equivalent job experience.",
        "skills": [
            "Data Analysis",
            "Unix",
            "Performance Tuning",
            "C++",
            "Linux",
            "Data Management",
            "Windows",
            "Sql",
            "Python",
            "Open Source"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PhonePe",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Mobile Payments",
        "job_description": "Key deliverables:\nDevelop and deploy scalable machine learning models including offline, real-time, and edge compute solutions\nAnalyze complex datasets to extract actionable insights supporting business decisions\nCollaborate with cross-functional teams to solve business challenges using data-driven approaches\nCommunicate findings and technical recommendations effectively to stakeholders\nRole responsibilities:\nImplement and optimize ML models focused on NLP and computer vision use cases\nWork with big data frameworks like PySpark to process and manage large datasets\nCreate visualizations and reports to convey analytical results clearly\nContinuously learn and apply new data science technologies and methodologies",
        "skills": [
            "Machine Learning Algorithms",
            "Pyspark",
            "Natural Language Processing",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist- Pytorch/ TensorFlow",
        "company_name": "Ups Supply Chain Solutions",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Logistics",
        "job_description": "RESPONSIBILITIES\nDefines key data sources from UPS and external sources to deliver models.\nDevelops and implements pipelines that facilitates data cleansing, data transformations, data enrichments from multiple sources (internal and external) that serve as inputs for data and analytics systems.\nFor larger teams, works with data engineering teams to validate and test data and model pipelines identified during proof of concepts\nDevelops data design based on the exploratory analysis of large amounts of data to discover trends and patterns that meet stated business needs.\nDefines model key performance indicator (KPI) expectations and validation, testing, and re-training of existing models to meet business objectives.\nReviews and creates repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling.\nSynthesizes insights and documents findings through clear and concise presentations and reports to stakeholders.\nPresents operationalized analytic findings and provides recommendations.\nIncorporates best practices on the use of statistical modeling, machine learning algorithms, distributed computing, cloud-based AI technologies, and run time performance tuning with the goal of deployment and market introduction\nLeverages emerging tools and technologies together with the use of open-source or vendor products in the creation and delivery of insights that support predictive and prescriptive solutions.\nQUALIFICATIONS\nRequirements:\nAbility to take a data science problem from start to finish, use pytorch/tensorflow to build the full model product.\nStrong analytical skills and attention to detail.\nAble to engage key business and executive-level stakeholders to translate business problems to high level analytics solution approach.\nExpertise with statistical techniques, machine learning or operations research and their application in business applications.\nExpertise in R, SQL, Python.\nDeep understanding of data management pipelines and experience in launching moderate scale advanced analytics projects in production at scale.\nDemonstrated experience in Cloud-AI technologies and knowledge of environments both in Linux/Unix and Windows.\nExperience implementing open-source technologies and cloud services; with or without the use of enterprise data science platforms.\nSolid oral and written communication skills, especially around analytical concepts and methods.\nAbility to communicate data through a story framework to convey data-driven results to technical and non-technical audience.\nMasters Degree in a quantitative field of mathematics, computer science, physics, economics, engineering, statistics (operations research, quantitative social science, etc.), international equivalent, or equivalent job experience.Preferences:\nFamiliarity with Java or C++\nEmployee Type:PermanentUPS is committed to providing a workplace free of discrimination, harassment, and retaliation.",
        "skills": [
            "Pipeline",
            "Tensorflow",
            "Java",
            "Data Science",
            "Data Management",
            "Artificial Intelligence",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Tokopedia",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "E-Commerce Platforms",
        "job_description": "Understand business concerns and formulate them as technical problems that can be solved using data and math/stats/ML\nWork with Product managers, Machine Learning Engineers, and Business Users to build, deploy, and scale Data Science solutions on Driver Experience and Earning related initiatives at Gojek that impact the driver s livelihood\nUse your experience in Data Science, Machine Learning, Software Engineering, distributed systems to rapidly prototype and deploy the systems to production\nWork with Business teams to continuously refine, improve the systems to cater to ever-changing Gojek need\nsAnalyze large volume data and generate insights that will be actionable\nDesign and conduct experiments to measure the impact of ML solutions or product features launched by Gojek Marketplace Team (Pricing and Matchmaking)\nWhat You Will Need\nBachelors/Master s Degree in Computer Science, Statistics, Operation research or other quantitative fields, with at least 5 years of relevant experience\nHands-on experience in Statistics/ML fundamentals and demonstrated experience in Python, SQL, and basic visualization tools\nSolid knowledge of Data Science and Machine Learning fundamentals, with proven experience to formulate Data Science solutions to business problems\nAbility to write clear and concise technical documentation\nExperience in taking Data Science models to production\nExpertise to conduct analysis independently for a business problem through data manipulation, formulation of hypotheses, design and statistical analysis of experiments to test them",
        "skills": [
            "experimentation",
            "Statistical Modeling",
            "Business Analysis",
            "Machine Learning",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Forecasting Data Scientist",
        "company_name": "Mondelez",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Remote",
        "industry": "Food and Beverage",
        "job_description": "How you will contribute\nAnalyze and derive value from data through the application methods such as mathematics, statistics, computer science, machine learning and data visualization. In this role you will also formulate hypotheses and test them using math, statistics, visualization and predictive modeling\nUnderstand business challenges, create valuable actionable insights about the data, and communicate your findings to the business. After that you will work with stakeholders to determine how to use business data for business solutions/insights\nEnable data-driven decision making by creating custom models or prototypes from trends or patterns discerned and by underscoring implications. Coordinate with other technical/functional teams to implement models and monitor results\nApply mathematical, statistical, predictive modelling or machine-learning techniques and with sensitivity to the limitations of the techniques. Select, acquire and integrate data for analysis. Develop data hypotheses and methods, train and evaluate analytics models, share insights and findings and continues to iterate with additional data\nDevelop processes, techniques, and tools to analyze and monitor model performance while ensuring data accuracy\nEvaluate the need for analytics, assess the problems to be solved and what internal or external data sources to use or acquire. Specify and apply appropriate mathematical, statistical, predictive modelling or machine-learning techniques to analyze data, generate insights, create value and support decision making\nContribute to exploration and experimentation in data visualization and you will manage reviews of the benefits and value of analytics techniques and tools and recommend improvements\nWhat you will bring\nStrong quantitative skillset with experience in statistics and linear algebra.\nA natural inclination toward solving complex problems\nKnowledge/experience with statistical programming languages including R, Python, SQL, etc, to process data and gain insights from it\nKnowledge of machine learning techniques including decision-tree learning, clustering, artificial neural networks, etc, and their pros and cons\nKnowledge and experience in advanced statistical techniques and concepts including, regression, distribution properties, statistical testing, etc\nGood communication skills to promote cross-team collaboration\nMultilingual coding knowledge/experience: Java, JavaScript, C, C++, etc\nExperience/knowledge in statistics and data mining techniques including random forest, GLM/regression, social network analysis, text mining, etc Ability to use data visualization tools to showcase data for stakeholders\nMore about this role\nYou willrepresentand communicate data requirements to help us understand our data assets and the relationships among them. You will investigate,analyzeand scope data requirements to support the development of data integration, data retrieval and reusable data sets.\nWhat you need to know about this position:\nThe Data Scientist willbe responsible foradvanced forecasting methodologies for demand forecasting to generate better forecasting results in terms of accuracy and bias\nDetermine,createandmaintainthe best Statistical models be to be used by considering SKU demand behavior using segmentation strategy to generate high-quality demand statistical forecast with low forecast error and bias\nRefine forecasting models by reviewing forecast performance and incorporating feedback from the Demand Planner to improve forecast error and bias metrics\nAnalyze the model performance every month/week where MAPE (Main Absolute Percentage Error) is deteriorating and post process the output and ifrequiredfinetune the output\nProposeadditionalstrategies to have a better code (efficient or moreaccurate) to execute in Databricks.\nCollaborate with Demand Planners to provideexplainabilityof models and gather feedback to improve the models\nLead, develop, and deploysome algorithms built ondatabricksthat help to make the predictive process more efficient andaccurate.\nWhat extra ingredients you will bring:\nLead statistical forecasting process to provide business a statistical base forecast for its demand planning process.\nLead some continuous improvement projects to get a more efficient, robust, andaccuratemodels to predict.\nAnalyze business requirements as a guide for data modeling and apply data analysis, design, modeling, and quality assurance techniques, based on a detailed understanding of business processes, toestablish, modify ormaintaindata structures and associated components (entity descriptions, relationship descriptions, attribute definitions\nManage the iteration, review and maintenance of data requirements and data models and you willassistin creating the sematic layer of data\nChoose a suitable data modeling approach for each project, such as by assessing the suitability of existing data models and build data models with the flexibility to change when business requirements change\nReconcile multiple logical source models into a single, logically consistent model and you will ensure that the proposed model follows data architecture guidelines and best practices\nDesign, build,implementand/ormaintainPython/PySparkalgorithms to execute predictive analysis which help to improve planning processes.\nAnalyse code, define and implement strategies to have moreaccurateand efficient results\nEducation / Certifications:\nBachelors Degreerequired.Masters in quantitativefield of Statistics, Applied Mathematics or Engineering, with specific full-time courses in Analytics preferred\nCertificationsGoogle CloudPlatformas Data Scientist, or DataAnalyst, or in Databricks(nice to have)\nExperience in any Data Science language like Python (preferred),PySpark(preferred),R, Julia.\nExperience developing and deploying Statisticaland/or machine learningsolutions.\nKnowledge and experience in data science cloud platforms like Databricks (preferred) orsimilar.\n+5 yearsof applied knowledge of analytical techniques in statistical modelling, machine learning with exposure to forecasting domain especiallydriver-basedforecasting\nExperience on working with FMCG, Food & Beverages,Retailor similar industry data with understanding the business process will be an advantage(nice to have)\nShould be able to articulate data science outcome into business understandable language\nExperience leading projects and/or processes\nFluent English isa must\nJob specific requirements:\nA desire to drive your future and accelerate your career and the following experience and knowledge:\nExtensive experience in data modeling in a large complex business with multiple systems\nAbility to simplify complex problems and communicate them to a broad audience\nDesign, build,implementand/ormaintainPython/PySparkalgorithms to execute predictive analysis which help to improve planning processes.\nAnalyze code,define,and implement strategies to have moreaccurateand efficient results\nWorking in cloud computing environments with tools likedatabricksand help MDLZ to have better practices on it.\nAligning and coordinating activities with different areas andstakeholderto guarantee a good execution of projects and processes.",
        "skills": [
            "Python/ PySpark",
            "Data Analyst",
            "Google Cloud Platform",
            "Databricks",
            "Java",
            "Javascript"
        ]
    },
    {
        "job_title": "Data Scientist - II",
        "company_name": "63ideas Infolabs Private Limited",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Agriculture, E-Commerce, E-Commerce Platforms, Information Technology",
        "job_description": "About the job\nNinjacart - Pioneer. Challenge Yourself. Create Better Lives.\nNinjacart is a pioneer in agritech. We believe in fostering a culture of innovation. We develop professionals who pioneer new ideas, simplify customer experiences, and impact the lives of millions. Our employees take complete ownership and truly belong.\nNinjacart is India's leading agri-platform, which leverages technology and data to organize the agri-commerce ecosystem. Over the last 9 years, our made-for-India technology and India-centric solutions have disrupted the way fruits and vegetables move from farms to consumers plates. We empower farmers and other players in the ecosystem in ways never done before.\nNinjacart aims to be the digital network for global agri-commerce that solves structural problems such as information asymmetry, payment hassles, distribution inefficiency, and discovering new buyers and sellers with tech-first solutions.\nNinjacart has raised over $350mn from Walmart Group, Tiger Global, Accel US, Accel India, Syngenta, and others. We are one of the top 25 startups to work for, according to Linkedin.\nWe are a highly motivated, results driven community, who focus on proactive approaches to problem solving. We cherish entrepreneurial spirit and provide significant operating autonomy to senior leaders. There are ample opportunities and challenges to be solved at scale and in real-time.\nRead More:\n#BetterLives For Every Agri Citizen\nhttps://www.linkedin.com/feed/update/urn:li:activity:7120720715909115905/\nHow Ninjacart has evolved in the last 8 years: https://www.youtube.com/watchv=J9Kts-O7tv4\nNinjacart Blog : https://www.ninjacart.com/blog/\nNandan Nilekani on Ninjacart : https://www.linkedin.com/posts/ninja-cart_tech-pioneersatwork-ninjacart-activity-7027281166617505792-pciW\nNinjacart Culture CODE: https://www.linkedin.com/posts/ninja-cart_ninja-code-our-values-activity-7076821402548387840-KuL1\nLocation: Bangalore, KA, IN\nAbout the Team\nThe Ninjacart technical team consists of professionals from various backgrounds, including programming, design, architecture, databases, DevOps, security, analytics & machine learning. We work to create the first and biggest agricultural ecosystem in the world.\nResponsibilities:\nMachine Learning and AI\nDevelop and deploy machine learning models, including classical ML, deep learning, and GenAI applications.\nImplement NLP solutions, computer vision models, and generative AI techniques.\nCollaborate on integrating AI models into web-based applications.\nParticipate in fine-tuning large language models (LLMs) for various generative AI tasks, including:\nAdapting pre-trained LLMs to domain-specific agricultural tasks.\nImplementing different fine-tuning strategies (e.g., full fine-tuning, LoRA, prefix tuning).\nOptimizing LLMs for specific downstream tasks while maintaining general capabilities.\nData Analytics & Feature Engineering\nConduct exploratory data analysis to uncover patterns and trends in complex datasets.\nLead feature engineering processes to improve model performance and interpretability.\nUtilize advanced analytical techniques such as clustering, segmentation, and time-series analysis.\nMLOps and DevOps Integration\nImplement MLOps practices to streamline the machine learning lifecycle.\nUse tools like DVC and MLflow for experiment tracking and model management.\nCollaborate with DevOps teams to integrate ML models into production environments.\nFull Stack ML Engineering\nDevelop user-friendly and interactive machine learning applications using FastAPI and Streamlit.\nIntegrate ML models into end-to-end solutions.\nModel Evaluation and Optimization\nEvaluate models for effectiveness and efficiency in real-world applications.\nOptimize models for performance, scalability, and resource utilization.\nAssess and compare the performance of fine-tuned LLMs using appropriate metrics and benchmarks.\nCollaboration and Continuous Learning\nWork with cross-functional teams to translate business requirements into ML solutions.\nStay updated on the latest developments in ML, NLP, computer vision, and GenAI.\nContribute to the company's innovation pipeline.\nA Ninja is resilient, smart, and ambitious. Sounds like you Here's what you will need to have to join the Ninja Clan\nBachelor's or Master's in Computer Science, Machine Learning, or a related field.\nMinimum of 3 years of proven experience in developing and deploying ML solutions, including NLP, computer vision, and classical ML models.\nExperience with GenAI techniques and applications, including LLM fine-tuning.\nStrong understanding of classical ML techniques, mathematical modeling, and statistical analysis.\nProficiency in Python and relevant libraries (TensorFlow, PyTorch, Hugging Face transformers, OpenCV, FastAPI, Streamlit).\nExperience with MLOps practices and DevOps integration.\nExcellent problem-solving skills and ability to work independently or collaboratively.\nAdditional Information\nAt Ninjacart, we are creating a workplace that enables everyone to find their true potential, purpose, and passion irrespective of their background, gender, race, sexual orientation, religion and ethnicity. We are committed to providing equal opportunity for all and believe that diversity in the workplace creates a more vibrant, richer work environment that advances the goals of our employees, communities and the business. Check out what Life at Ninjacart looks like!",
        "skills": [
            "Statistics",
            "Cloud Computing",
            "Machine Learning",
            "Natural Language Processing",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Python",
            "Sql",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "UPL Limited",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Farming",
        "job_description": "Job Responsibilities:\nDesign, develop, and implement generative AI models, algorithms and applications.\nDevelop and maintain AI pipelines, including data preprocessing, feature extraction, model training, and evaluation.\nApply quality software engineering practices throughout the software development lifecycle.\nREQUIREMENTS:\nProficiency and hands-on experience in object-oriented programming with Python.\nProficiency in designing and building scalable APIs\nExperience with cloud platforms such as Azure or AWS\nExperience working in Agile development teams with excellent collaboration skills.\nExperience with Langchain and/or agentic workflows.\nExperience working with data cleaning , data preprocessing and fine-tuning LLMs.\nUnderstanding of prompt engineering and prompt tuning.\nExperience building applications using LLM frameworks such as LangChain, Llama Index, and Semantic Kernel.\nExperience with vector databases.\nKnowledge of ML model evaluation to ensure consistent performance with changing data.\nFamiliarity with MLOps and ML model lifecycle pipelines.\nExperience with natural language processing (NLP) techniques and tools, such as SpaCy, NLTK, or Hugging Face.\nREQUIRED EDUCATION AND EXPERIENCE:\nB.E./B.Tech.\n5+ years of relevant work experience",
        "skills": [
            "AI models",
            "Algorithms",
            "Agile Development",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist Mahindra AI",
        "company_name": "Mahindra Finance",
        "experience": "1-5 Years",
        "salary": null,
        "location": "Mumbai",
        "industry": "Financial Services",
        "job_description": "Responsibilities & Key Deliverables\nResponsibilities\nDeploying AI/ML solution to productions and generating value from it\nDeveloping End-to-End solutions with teammates and working with stakeholders, data team, business team and product team\nHelping dev-ops team to deploy and integrate solution with existing systems\nData Quality and Availability: Ensuring the quality and availability of data for model training and validation can be a significant challenge\nThis includes dealing with missing, unstructured, or inconsistent data\nModel Performance: Developing models that perform well in real-world scenarios is a complex task\nIt requires rigorous testing, validation, and continuous optimization\nScalability: Scaling AI solutions to handle large volumes of data and integrate with various business functions can be technically challenging\nKeeping Up with Rapid Advances: The field of AI is advancing rapidly\nKeeping up with the latest research, techniques, and tools can be demanding\nEthical and Regulatory Compliance: Ensuring that AI models are transparent, fair, and comply with all relevant regulations and ethical guidelines is a critical challenge\nThis includes understanding and mitigating potential biases in AI models\nExperience\n2 4 years\nQualifications\nIndustry Preferred\nMasters in science, Stats, ML, DS/ Bachelors in Engg, Tech DS, Comp Sci\nGeneral Requirements\nFunctional Skills\nWorking knowledge of Data Science toolbox like: Python, SQL, Jupyter Notebook, Azure/AWS cloud, PySpark, TensorFlow, PyTorch, Keras, and other Big Data toolkit\nDeep understanding of AI/ML models with experience in LLM, Reinforcement Learning, Computer Vision, and Deep Learning\nExpertise in NLP, Computer Vision, Speech technology(STT, TTS)\nBehavioural Skills\nAbility to connect technology with business goals and outcomes\nA hustler who constantly likes to go into design-build-deploy cycle for innovative new idea\nA Problem solving mindset for trying out new ideasResult orientation with execution excellence\nCustomer Focus\nWeaving passion and energy at work\nCan Do and Will Do attitude",
        "skills": [
            "reinforcement learning",
            "Python",
            "Sql",
            "Pyspark",
            "Data Science",
            "Nlp",
            "Tensorflow",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Stigen Martech Private Limited",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Gurugram",
        "industry": "Login to check your skill match score",
        "job_description": "Job Overview\nWe are looking for a dynamic and innovative Full Stack Data Scientist with 45 years of experience who excels in end-to-end data science solutions. The ideal candidate is a tech-savvy professional passionate about leveraging data to solve complex problems, develop predictive models, and drive business impact in the MarTech domain.\nKey Responsibilities\n1. Data Engineering & Preprocessing\nCollect, clean, and preprocess structured and unstructured data from various sources.\nPerform advanced feature engineering, outlier detection, and data transformation.\nCollaborate with data engineers to ensure seamless data pipeline development.\n2. Machine Learning Model Development\nDesign, train, and validate machine learning models (supervised, unsupervised, deep learning).\nOptimize models for business KPIs such as accuracy, recall, and precision.\nInnovate with advanced algorithms tailored to marketing technologies.\n3. Full Stack Development\nBuild production-grade APIs for model deployment using frameworks like Flask, FastAPI, or Django.\nDevelop scalable and modular code for data processing and ML integration.\n4. Deployment & Operationalization\nDeploy models on cloud platforms (AWS, Azure, or GCP) using tools like Docker and Kubernetes.\nImplement continuous monitoring, logging, and retraining strategies for deployed models.\n5. Insight Visualization & Communication\nCreate visually compelling dashboards and reports using Tableau, Power BI, or similar tools.\nPresent insights and actionable recommendations to stakeholders effectively.\n6. Collaboration & Teamwork\nWork closely with marketing analysts, product managers, and engineering teams to solve business challenges.\nFoster a collaborative environment that encourages innovation and shared learning.\n7. Continuous Learning & Innovation\nStay updated on the latest trends in AI/ML, especially in marketing automation and analytics.\nIdentify new opportunities for leveraging data science in MarTech solutions.\nQualifications\nEducational Background\nBachelor's or Master's degree in Computer Science, Data Science, Statistics, Mathematics, or a related field.\nTechnical Skills\nProgramming Languages: Python (must-have), R, or Julia; familiarity with Java or C++ is a plus.\nML Frameworks: TensorFlow, PyTorch, Scikit-learn, or XGBoost.\nBig Data Tools: Spark, Hadoop, or Kafka.\nCloud Platforms: AWS, Azure, or GCP for model deployment and data pipelines.\nDatabases: Expertise in SQL and NoSQL (e.g., MongoDB, Cassandra).\nVisualization: Mastery of Tableau, Power BI, Plotly, or D3.js.\nVersion Control: Proficiency with Git for collaborative coding.\nExperience\n45 years of hands-on experience in data science, machine learning, and software engineering.\nProven expertise in deploying machine learning models in production environments.\nExperience in handling large datasets and implementing big data technologies.\nSoft Skills\nStrong problem-solving and analytical thinking.\nExcellent communication and storytelling skills for technical and non-technical audiences.\nAbility to work collaboratively in diverse and cross-functional teams.\nPreferred Qualifications\nExperience with Natural Language Processing (NLP) and Computer Vision (CV).\nFamiliarity with CI/CD pipelines and DevOps for ML workflows.\nExposure to Agile project management methodologies.",
        "skills": [
            "Visualization",
            "Machine Learning",
            "Cloud",
            "Gcp",
            "Data Engineer",
            "Devops",
            "Aws"
        ]
    },
    {
        "job_title": "Data Scientist (Marketing Mix Model)",
        "company_name": "Ara Resources Private Limited",
        "experience": "2-5 Years",
        "salary": "INR 18 - 20 LPA ",
        "location": "Bengaluru",
        "industry": "Login to check your skill match score",
        "job_description": "About The Company:\nAra's client is a fast-growing analytics software company headquartered in New Jersey, with a development center in Bangalore. Focused on the healthcare industry, they build cutting-edge data science products that deliver powerful insights from big data, helping clients achieve their business goals and make a global impact.\nThe Role:\nThis role calls for strong knowledge of analytical techniques to support data-driven decision-making. The candidate will work on sales and marketing projects like campaign effectiveness, marketing mix modeling, attribution, segmentation, response modeling, and A/B testing. A solid understanding of business context and the ability to apply statistical and AI/ML methods effectively is essential.\nSkills Required:\nStrong Experience in Statistical Modeling (Marketing mix model).\nHands-on experience with SQL, R or Python is mandatory.\nExperience working with pharma companies and pharma datasets is a must.\nProficiency with Adobe Analytics, Google AdWords, Google Analytics, Google Data Studio or similar tools which capture and present campaign, web & app data\nExperience building reports, dashboards\nExperience with marketing automation tools and techniques including but not limited to audience development, configuration of journeys/workflows, campaign deployment and tracking\nProficient in Microsoft Excel & PowerPoint. Ability to work with visualization tools e.g. Tableau, Power BI etc. will be considered good to have skill-set\nExperience working with pharma companies and pharma datasets is a must\nQualifications & Experience:\nData Scientist with approximately 2 to 4 years of experience, preferably in Pharma or Healthcare analytics. Hands-on experience of AI/ML techniques will be preferred\nEducational background in Engineering, Operations Research, Econometrics and/or Statistics",
        "skills": [
            "marketing mix modeling",
            "Data Science",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Consultant - Data Scientist",
        "company_name": "Thryve Digital",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "AI Engineer (Mid-Level)\nResponsibilities:\nDesign, develop, and implement AI/ML models and algorithms.\nBuild Proof of Concept (POC) applications to showcase feasibility and value of AI solutions.\nWrite clean, efficient, and well-documented code.\nCollaborate with data engineers to ensure data quality for model training and evaluation.\nWork with senior team members to understand requirements and contribute to technical solutions.\nTroubleshoot and debug AI/ML models and applications.\nStay current with AI/ML advancements.\nUse ML frameworks (TensorFlow, PyTorch, Scikit-learn) for model development and deployment.\nDeploy AI solutions on Google Cloud Platform (GCP).\nImplement data preprocessing and feature engineering with Pandas and NumPy.\nUtilize Vertex AI for training, deployment, and management.\nIntegrate Google Gemini for specific AI functionalities.\nQualifications:\nBachelor's degree in Computer Science, AI, or related field.\n3+ years of experience in AI/ML model development.\nStrong Python programming skills.\nExperience with ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nSolid understanding of machine learning concepts.\nAbility to work both independently and in teams.\nStrong problem-solving and communication skills.\nExperience with GCP preferred.\nFamiliarity with Vertex AI is a plus.",
        "skills": [
            "Vertex AI",
            "Tensorflow",
            "Machine Learning",
            "Pytorch",
            "Google Cloud Platform",
            "Python"
        ]
    },
    {
        "job_title": "Consultant - Data Scientist",
        "company_name": "Thryve Digital",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "AI Engineer (Mid-Level)\nResponsibilities:\nDesign, develop, and implement AI/ML models and algorithms.\nBuild Proof of Concept (POC) applications to showcase feasibility and value of AI solutions.\nWrite clean, efficient, and well-documented code.\nCollaborate with data engineers to ensure data quality for model training and evaluation.\nWork with senior team members to understand requirements and contribute to technical solutions.\nTroubleshoot and debug AI/ML models and applications.\nStay current with AI/ML advancements.\nUse ML frameworks (TensorFlow, PyTorch, Scikit-learn) for model development and deployment.\nDeploy AI solutions on Google Cloud Platform (GCP).\nImplement data preprocessing and feature engineering with Pandas and NumPy.\nUtilize Vertex AI for training, deployment, and management.\nIntegrate Google Gemini for specific AI functionalities.\nQualifications:\nBachelor's degree in Computer Science, AI, or related field.\n3+ years of experience in AI/ML model development.\nStrong Python programming skills.\nExperience with ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nSolid understanding of machine learning concepts.\nAbility to work both independently and in teams.\nStrong problem-solving and communication skills.\nExperience with GCP preferred.\nFamiliarity with Vertex AI is a plus.",
        "skills": [
            "Vertex AI",
            "Tensorflow",
            "Machine Learning",
            "Pytorch",
            "Google Cloud Platform",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Reflections Info Systems",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Thiruvananthapuram / Trivandrum, Bengaluru",
        "industry": "IT Management",
        "job_description": "Job Description\nA minimum of 8 years of professional experience, with at least 6 years in a data science role.\nStrong knowledge of statistical modeling, machine learning, deep learning and GenAI.\nProficiency in Python and hands on experience optimizing code for performance.\nExperience with data preprocessing, feature engineering, data visualization and hyperparameter tuning.\nSolid understanding of database concepts and experience working with large datasets.\nExperience deploying and scaling machine learning models in a production environment.\nFamiliarity with machine learning operations (MLOps) and related tools.\nGood understanding of Generative AI concepts and LLM finetuning.\nExcellent communication and collaboration skills.\nResponsibilities include:\nLead a high performance team, guide and mentor them on the latest technology landscape, patterns and design standards and prepare them to take on new roles and responsibilities.\nProvide strategic direction and technical leadership for AI initiatives, guiding the team in designing and implementing state-of-the-art AI solutions.\nLead the design and architecture of complex AI systems, ensuring scalability, reliability, and performance.\nLead the development and deployment of machine learning/deep learning models to address key business challenges.\nApply statistical modeling, data preprocessing, feature engineering, machine learning, and deep learning techniques to build and improve models.\nUtilize expertise in at least two of the following areas: computer vision, predictive analytics, natural language processing, time series analysis, recommendation systems.\nDesign, implement, and optimize data pipelines for model training and deployment.\nExperience with model serving frameworks (e.g., TensorFlow Serving, TorchServe, KServe, or similar).\nDesign and implement APIs for model serving and integration with other systems.\nCollaborate with cross-functional teams to define project requirements, develop solutions, and communicate results.\nMentor junior data scientists, providing guidance on technical skills and project execution.\nStay up-to-date with the latest advancements in data science and machine learning, particularly in generative AI, and evaluate their potential applications.\nCommunicate complex technical concepts and analytical findings to both technical and non-technical audiences.\nServes as a primary point of contact for client managers and liaises frequently with internal stakeholders to gather data or inputs needed for project work\nCertifications :\nBachelors or Masters degree in a quantitative field such as statistics, mathematics, computer science, or a related area.\nPrimary Skills :\nPython\nData Science concepts\nPandas, NumPy, Matplotlib\nArtificial Intelligence\nStatistical Modeling\nMachine Learning, Natural Language Processing (NLP), Deep Learning\nModel Serving Frameworks (e.g., TensorFlow Serving, TorchServe)\nMLOps(e.g; MLflow, Tensorboard, Kubeflow etc)\nComputer Vision, Predictive Analytics, Time Series Analysis, Anomaly\nDetection, Recommendation Systems (Atleast 2)\nGenerative AI, RAG, Finetuning(LoRa, QLoRa)\nProficent in any of Cloud Computing Platforms (e.g., AWS, Azure, GCP)\nSecondary Skills :\nExpertise in designing scalable and efficient model architectures is crucial for developing robust AI solutions.\nAbility to assess and forecast the financial requirements of data science projects ensures alignment with budgetary constraints and organizational goals.\nStrong communication skills are vital for conveying complex technical concepts to both technical and non-technical stakeholders.",
        "skills": [
            "Analytical",
            "Cloud Computing",
            "Machine Learning",
            "Data Science",
            "Natural Language Processing",
            "Python"
        ]
    },
    {
        "job_title": "Principle Data Scientist",
        "company_name": "Nalco Water",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Description\nRequired Skills:\nOverall 10+ years of SAP experience with atleast 4+ years as HANA XSA developer\nStrong understanding and experience with SAP HANA 2.0.\nIn-depth knowledge of the SAP HANA XSA Engine for building and deploying applications.\nExpertise in HANA Modelling & HANA SQL Scripting ( Sql scripts, Sql Procedures, Calculation views, Function calls, CDS views, flowcharts etc.)\nProficient in HANA XSA development ( HANA XSA, HANA CAP, Node JS, XSA Cockpit, HANA Cockpit, JavaScript, Job Scheduling )\nProficient in HANA XSA & DB Security concepts (Roles, Privileges etc)\nFamiliarity with HANA CLI for administrative tasks and script automation.\nExperience working with SLT ( Data Load & Replication)\nMust know CI/CD and GIT\nKnowledge of tools & IDE like Web IDE, VS Code, HANA Studio, Postman, App D\nPerformance analysis and optimization\nMust have good troubleshooting skills\nWorking experience with SAP ECC & knowledge of SAP tables\nExperience working with OData Services\nExcellent verbal and written communication skills\nWillingness to work on new technologies with on-the-job learning.\nGood to have:\nABAP Programming experience\nSAP ODATA Gateway objects development\nUI development skills - SAP UI5\nResponsibilities:\nCoordinate with the onsite/offshore functional team & other technical teams\nProvide technical expertise for new business requirements and also suggest value additions to existing landscape.\nConfigure, Maintain & Monitor data replication objects and transformation processes in SLT.\nDevelop, implement, and maintain applications using HANA XSA or HANA CAP.\nUse Node.js for backend development and server-side scripting.\nEmploy HANA CLI for administrative tasks and script automation.\nPerform data modeling and SQL scripting on SAP HANA.\nBuild and deploy applications using SAP HANA XS/XSA Engine.\nDevelop front-end interfaces using SAPUI5, JavaScript, and Node.js.\nLeverage expertise in SAP HANA 2.0 for development projects.\nResponsible for Trouble shooting,Performance analysis and optimization",
        "skills": [
            "Machine Learning",
            "Deep Learning",
            "Python Programming",
            "Ui Development"
        ]
    },
    {
        "job_title": "Data Scientist - Internal Audit",
        "company_name": "Applause",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "Five to eight years of experience in data analysis or data science or a related field, preferably or a technology company with a focus on crowd-based platforms\nStrong understanding of internal audit processes and methodologies and ability to analyze and cross reference multiple databases and run scripts to validate data points\nTechnical Skills:\nPython proficiency: Advanced knowledge of Python for data manipulation, analysis, and visualization.\nSQL expertise: Ability to write complex SQL queries to extract and transform data from databases.\nAPI interaction: Comfortable working with APIs to extract data from third-party software tools and integrate them into the analysis process.\nexperience with threat/audit investigation on internal proprietary systems\nExperience with LexisNexis software\nIndustry Knowledge:\nCrowd-based platforms: Understanding of the unique business models and data structures associated with crowd-based companies.\nInternal audit: Experienced in having managed and executed internal control frameworks and risk assessment methodologies.\nTechnology industry: Knowledgeable in the technology landscape and relevant regulations.\nOther Skills:\nProblem-solving: Ability to identify and map complex data discrepancies and inconsistencies.\nCommunication: Excellent written and verbal communication skills to present findings and recommendations to both technical and non-technical stakeholders.\nAttention to detail: Meticulous approach to data analysis to ensure accuracy and completeness.\nCollaboration: Ability to work effectively with cross-functional teams, including auditors, engineers, delivery and customer operations, human resources and other business stakeholders.\nResponsibilities:\nData extraction and Analysis: Retrieve data from internal databases and third-party software tools, to audit data quality, consistency and compliance with company policies.\nData reconciliation: Develop and implement processes to compare data sets and identify discrepancies.\nRoot cause analysis: Investigate and analyze the root causes of data mismatches, providing actionable insights.\nReporting: Prepare clear and concise reports summarizing audit findings and recommendations.\nContinuous improvement: Collaborate with teams to enhance data integrity and streamline audit processes.\nOther duties as assigned.\nAdditional Considerations:\nExperience with specific third-party software tools used by your company can be a significant advantage.\nCertifications in data science or auditing can demonstrate the candidates commitment to professional development",
        "skills": [
            "SQL expertise",
            "Data extraction and Analysis",
            "LexisNexis software",
            "API interaction",
            "data reconciliation",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Coditas Technologies",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "Coditas Solutions is seeking a highly skilled and motivated Data Scientist to join our dynamic team. As a Data Scientist, you will play a key role in designing, implementing, and optimizing machine learning models and algorithms to solve complex business challenges. If you have a passion for leveraging AI and ML technologies to drive innovation, this is an exciting opportunity to contribute to groundbreaking projects.\nRoles and Responsibilities\nDesign, implement, and optimize machine learning algorithms using R and Python.\nWork on developing predictive models and decision-making systems.\nConduct exploratory data analysis to understand data patterns and insights.\nCollaborate with data engineers to ensure the availability and quality of data for model training.\nDeploy machine learning models into production environments.\nCollaborate with cross-functional teams to integrate models into existing systems.\nContinuously optimize and improve the performance of machine learning models.\nStay updated on the latest advancements in ML algorithms and technologies.\nWork closely with software engineers to ensure seamless integration of AI/ML solutions.\nCollaborate with clients to understand their business requirements and customize solutions accordingly.\nTechnical Skills\nExcellent programming skills with the ability to implement complex algorithms in Python or R.\nExperience with cloud-based platforms (AWS, Azure, GCP) for deploying machine learning models.\nStrong experience of minimum 3 years in developing and implementing machine learning algorithms.\nExperience with model deployment and integration into production systems.\nHands-on experience with use of standard classical machine learning libraries such as Scikit learn, NLTK, OpenCV as well as deep learning libraries Tensorflow, PyTorch, Keras.\nUnderstanding of machine learning algorithms, techniques, and concepts (linear regression, logistic regression, decision trees, random forests, neural networks, etc.).\nExperience with data preprocessing, feature engineering, and model evaluation techniques of structured and unstructured data. Proven experience with identifying, creating and selecting relevant features or variables to enhance model performance.\nAbility to collaborate effectively with cross-functional teams.\nPrevious experience working on real-world AI/ML projects.\nShould be focused on linear algebra, machine learning, and statistics & probability are preferred.\nAbility to have a basic knowledge of the LLMs and optimal use of the GenAI models.\nStrong problem-solving and critical-thinking skills.\nExcellent communication and collaboration skills.\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Sc in Any Specialization",
        "skills": [
            "Deploying Models",
            "ML Deployment",
            "Machine Learning",
            "Azure Cloud",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Mastermind Network",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Mumbai",
        "industry": "Recruiting",
        "job_description": "Developing and implementing machine learning models / NLP/ AI and algorithms.\nPerform advance Statistical and Machine learning modelling exercise to develop descriptive/ predictive/ prescriptive models\nConduct statistical analyses of models, and adjust models where possible\nExtend machine learning frameworks and libraries for NLP projects\nBuild, Improve and extend NLP capabilities\nSelect the proper annotated datasets for supervised learning techniques\nDevelop NLP projects in accordance with prescribed requirements\nTrain developed NLP models and evaluated their effectiveness\nHands-on experience with R and/or Python for hypothesis testing, statistical modelling and Machine learning.",
        "skills": [
            "R",
            "AI Algorithms",
            "Statistical Modeling",
            "Machine Learning",
            "Nlp",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Ciklum",
        "experience": "5-9 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "About the Role\nAs a Senior Data Scientist, become a part of a cross-functional development team engineering experiences of tomorrow.\nResponsibilities\nPrototype Solutions: Develop prototype solutions, mathematical models, algorithms, machine learning techniques, and robust analytics to support analytic insights and visualization of complex data sets.\nExploratory Data Analysis: Work on exploratory data analysis to navigate a dataset and draw broad conclusions based on initial appraisals.\nOptimization Recommendations: Provide optimization recommendations that drive KPIs established by product, marketing, operations, PR teams, and others.\nCollaboration: Interact with engineering teams to ensure that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability.\nBusiness Collaboration: Work directly with business analysts and data engineers to understand and support their use cases.\nData-Driven Business Solutions: Collaborate with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.\nDrive Innovation: Drive innovation by exploring new experimentation methods and statistical techniques that could sharpen or speed up our product decision-making processes.\nCross-Training: Cross-train other team members on technologies being developed, while also continuously learning new technologies from other team members.\nCommunity Contribution: Contribute to unit activities and community building, participate in conferences, and provide excellence in exercise and best practices.\nSales & Marketing Support: Support marketing & sales activities, customer meetings, and digital services through direct support for sales opportunities and providing thought leadership & content creation for the service.\nRequirements\nEducation: BSc, MSc, or PhD in Mathematics, Statistics, Computer Science, Engineering, Operations Research, Econometrics, or related fields.\nMathematics & Statistics: Strong knowledge of Probability Theory, Statistics, and a deep understanding of the mathematics behind Machine Learning.\nMethodologies: Proficiency with CRISP-ML(Q) or TDSP methodologies for addressing commercial problems through data science solutions.\nMachine Learning Techniques: Hands-on experience with various machine learning techniques, including:\nRegression\nClassification\nClustering\nDimensionality reduction\nProgramming: Proficiency in Python for developing machine learning models and conducting statistical analyses.\nData Visualization: Strong understanding of data visualization tools and techniques (e.g., Python libraries such as Matplotlib, Seaborn, Plotly) and the ability to present data effectively.\nSQL Proficiency: Proficiency in SQL for data processing, manipulation, sampling, and reporting.\nData Challenges: Experience working with imbalanced datasets and applying appropriate techniques.\nTime Series Data: Experience with time series data, including preprocessing, feature engineering, and forecasting.\nAnomaly Detection: Experience with outlier detection and anomaly detection.\nData Types: Experience working with various data types: text, image, and video data.\nCloud Platforms: Familiarity with AI/ML cloud implementations (AWS, Azure, GCP) and cloud-based AI/ML services (e.g., Amazon SageMaker, Azure ML).\nDomain Experience\nMedical Signals & Images: Experience with analyzing medical signals and images.\nPredictive Models: Expertise in building predictive models for patient outcomes, disease progression, readmissions, and population health risks.\nNLP & Text Mining: Experience extracting insights from clinical notes, medical literature, and patient-reported data using NLP and text mining techniques.\nSurvival Analysis: Familiarity with survival or time-to-event analysis.\nClinical Trials: Expertise in designing and analyzing data from clinical trials or research studies.\nCausal Relationships: Experience identifying causal relationships between treatments and outcomes, such as propensity score matching or instrumental variable techniques.\nHealthcare Regulations: Understanding of healthcare regulations and standards like HIPAA, GDPR (for healthcare data), and FDA regulations for medical devices and AI in healthcare.\nHealthcare Data Security: Expertise in handling sensitive healthcare data in a secure, compliant way, understanding the complexities of patient consent, de-identification, and data sharing.\nDecentralized Data Models: Familiarity with decentralized data models such as federated learning to build models without transferring patient data across institutions.\nInteroperability Standards: Knowledge of interoperability standards such as HL7, SNOMED, FHIR, or DICOM.\nStakeholder Collaboration: Ability to work with clinicians, researchers, health administrators, and policymakers to understand problems and translate data into actionable healthcare insights.\nGood to Have Skills\nMLOps: Experience with MLOps, including integration of machine learning pipelines into production environments, Docker, and containerization/orchestration (e.g., Kubernetes).\nDeep Learning: Experience in deep learning development using TensorFlow or PyTorch libraries.\nLarge Language Models: Experience with Large Language Models (LLMs) and Generative AI applications.\nSQL: Advanced SQL proficiency, with experience in MS SQL Server or PostgreSQL.\nData Engineering: Familiarity with platforms like Databricks and Snowflake for data engineering and analytics.\nBig Data: Experience working with Big Data technologies (e.g., Hadoop, Apache Spark).\nNoSQL: Familiarity with NoSQL databases (e.g., columnar or graph databases like Cassandra, Neo4j).\nBusiness-Related Requirements\nData Science Solutions: Proven experience in developing data science solutions that drive measurable business impact, with a strong track record of end-to-end project execution.\nBusiness Problem Translation: Ability to effectively translate business problems into data science problems and create solutions from scratch using machine learning and statistical methods.\nProject Management: Excellent project management and time management skills, with the ability to manage complex, detailed work and effectively communicate progress and results to stakeholders at all levels.\nDesirable\nResearch: Research experience with peer-reviewed publications.\nCompetitions: Recognized achievements in data science competitions, such as Kaggle.\nCertifications: Certifications in cloud-based machine learning services (AWS, Azure, GCP).",
        "skills": [
            "PyTorch libraries",
            "Data Scientist",
            "Python",
            "Tensorflow",
            "MLops",
            "Ms Sql Server",
            "Databricks"
        ]
    },
    {
        "job_title": "Lead/Staff Data Scientist",
        "company_name": "Tableau Software",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "About Salesforce\nWe re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good - you ve come to the right place.\nJob Details\nWe re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good - you ve come to the right place.\nWe re looking for an experienced Lead Data Scientist who will help us build predictive models and recommender systems using machine learning and statistical techniques to drive personalized marketing and customer experience. This Lead Data Scientist brings significant experience in designing, developing, and delivering statistical models and machine learning algorithms for targeting and digital optimization use cases on large-scale data sets in a cloud environment. They show rigor in how they prototype, test, and evaluate algorithm performance both in the testing phase of algorithm development and in managing production algorithms. They demonstrate advanced knowledge of machine learning and statistical techniques along with ensuring the ethical use of data in the algorithm design process. At Salesforce, Trust is our number one value and we expect all applications of statistical and machine learning models to adhere to our values and policies to ensure we balance business needs with responsible uses of technology.\nResponsibilities\nAs part of the Customer Targeting Algorithms team within the Marketing AI/ML Algorithms & Applications organization, develop machine learning algorithms and statistical models to drive effective marketing and personalized customer experience - e.g., propensity models, uplift models, next-best recommender systems, customer lifetime value, etc.\nOwn the full lifecycle of model development from ideation and data exploration, algorithm design, validation, and testing. Work closely with data engineers to develop modeling data sets and pipelines; deploy models in production, setup model monitoring and in-production tuning processes.\nBe a master in cross-functional collaboration by developing deep relationships with key partners across the company and coordinating with working teams.\nCollaborate with stakeholders to translate business requirements into technical specifications, and present data science solutions to technical and non-technical audiences technical and non-technical across the organization.\nConstantly learn, have a clear pulse on innovation across the enterprise SaaS, AdTech, paid media, data science, customer data, and analytics communities.\nAssume leadership responsibilities and cover the end-to-end data science solution outside of model development. This includes driving projects to completion with minimal supervision, engaging with stakeholders to quantify impact, and planning roadmaps for future enhancements.\nWork independently to manage stakeholder expectations and explore alternative use cases to get better return on investment from the suite of AI/ML models.\nRequired Skills\n8+ years of experience using advanced statistical and machine learning techniques such as clustering, linear and logistic regressions, PCA, gradient boosting machines (GBM), support vector machines (SVM), neural networks (e.g., ANN, RNN, CNN), and other deep learning algorithms (e.g., Wide & Deep). Must have multiple robust examples of using these techniques to support marketing efforts and to solve business problems on large-scale data sets.\n8+ years of proficiency with one or more programming languages such as Python, R, PySpark, Java.\nExpert-level knowledge of SQL with strong data exploration and manipulation skills.\nExperience using cloud platforms such as GCP and AWS for model development and operationalization is preferred.\nExperience developing production-ready feature engineering scripts for model scoring deployment.\nExperience transforming semi-structured and unstructured data into features for model development.\nExperience creating model monitoring and model re-training frameworks to validate and optimize in-production performance.\nMust have superb quantitative reasoning and interpretation skills with strong ability to provide analysis-driven business insight and recommendations.\nExcellent written and verbal communication skills; ability to work well with peers and leaders across data science, marketing, and engineering organizations.\nExcellent presentation skills; ability to articulate data science solutions to a wide audience to drive model use and implementation adoption.\nCreative problem-solver who simplifies problems to their core elements.\nExperience with setting up endpoints, lambda functions, and API gateways is a plus.\nB2B customer data experience a big plus. Advanced Salesforce product knowledge is also a plus.\nAccommodations\nIf you require assistance due to a disability applying for open positions please submit a request via thisAccommodations Request Form .",
        "skills": [
            "Salesforce",
            "Business Strategy",
            "CRM",
            "Analytics",
            "Monitoring",
            "Predictive Modeling",
            "Data Modeling",
            "Gcp",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Nalco Water",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Job description\nDesign and Develop AI-Powered Solutions:Lead the design, development, and deployment of AI-powered solutions using Azure Databricks, and other components or our technology stack.\nTechnical Leadership:Provide technical leadership and guidance to junior data scientists and engineers on Databricks, Mosaic AI, and AI agent development. Mentor and coach team members to improve their skills and expertise.\nDatabricks and Mosaic AI Expertise:Develop and maintain expertise in Databricks and Mosaic AI, staying up-to-date with the latest features and best practices. Apply this expertise to drive the adoption of these technologies within the organization.\nAI Agent Development:Design and develop AI agents using Mosaic AI and other relevant technologies. Collaborate with stakeholders to identify opportunities for AI agents to drive business value.\nData Science Innovation:Stay abreast of the latest advancements in data science and AI, identifying opportunities to apply new techniques and technologies to drive business innovation.\nCollaboration and Communication:Collaborate with stakeholders across the organization to identify business problems and develop data-driven solutions. Communicate complex technical concepts to non-technical stakeholders, driving adoption and understanding of data science solutions.\nRequirements:\nEducation/Experience:Degree or advanced degree in data science, physics, mathematics, statistics, computer science or related quantitative field. BS and 10+ years related experience or MS and 7+ years related experience or PhD and less than 2 years experience. 1-3 Years Supervisory experience preferred.\nTechnical Skills:\nProficiency in Databricks, or a similar platform like AWS SageMaker, Azure Machine Learning, Vertex AI, and others.\nStrong programming skills in languages such as Python, Scala, SQL, etc.\nFamiliarity with data engineering, data warehousing, and data governance.\nSoft Skills:\nExcellent communication and collaboration skills.\nStrong leadership and mentoring skills.\nAbility to drive innovation and stay up-to-date with the latest advancements in data science and AI.\nNice to Have:\nExperience with Cloud Platforms:Experience working with cloud platforms such as AWS, Azure, or Google Cloud.Experience with AI agents:Experience developing AI agents using Mosaic AI or similar code-first platforms.Certifications:Databricks or Mosaic AI certifications are a plus.Open-Source Contributions:Contributions to open-source projects related to data science, machine learning, or AI.\nWhat We Offer:\nOpportunity to take on some of the world s most meaningful challenges, helping customers achieve clean water, safe food, abundant energy, and healthy environments.\nAbility to make an impact and shape your career with a company that is passionate about your growth.\nThe opportunity to work with the latest technologies and techniques in the data science and machine learning engineering field, acting as the subject matter expert in a growing organization within the company.\nSupport of an organization that believes it is vital to include and engage diverse people, perspectives, and ideas to achieve our best.",
        "skills": [
            "Data Analysis",
            "Machine Learning",
            "Sql Database",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Varite India Private Limited",
        "experience": "2-5 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Cloud Computing",
        "job_description": "VARITE is a global staffing and IT consulting company providing technical consulting and team augmentation services to Fortune 500 Companies across the USA, UK, Canada, and India.\nAt VARITE, we collaborate with leading organizations to connect top talent with exceptional opportunities. This role is on behalf of one of our prestigious clients.\nAbout the Client\nOur client is a pioneering cloud computing company offering a platform for digital workflows. They specialize in IT service management, HR solutions, customer service, and security operations. Headquartered in the United States, this globally recognized innovator was named Forbes Most Innovative Company in 2018 for its revolutionary approach to workflow automation.\nRole Overview\nJoin the EWF Data Science Team to work on AI-driven Talent/HR products, contributing to product enhancements and improving algorithm quality. Collaborate with cross-functional teams to explore data, develop models, and solve technical challenges that drive product success.\nLocation: Hyderabad, Telangana (On-Site)\nContract Duration: 12 Months\nStart Date: 1st January 2025\nKey Responsibilities\nBuild and enhance algorithms for product and feature requirements.\nDesign and optimize NLP models (e.g., named entity extraction, classification, parsing).\nAnalyze large datasets to extract actionable insights.\nDevelop metrics to support projects and team initiatives.\nWork on advanced AI solutions for HR/Talent domains.\nWhat You'll Need\nQualifications:\nBachelor's or Master's degree in a technical, mathematical, or engineering field.\nExperience:\n2-4 years of experience in machine learning and deep learning model development.\nEssential Skills:\nProficiency in Python, particularly for machine learning and text processing.\nExpertise in NLP and handling large datasets.\nKnowledge of SQL and Regex for data manipulation.\nStrong problem-solving skills for technical and statistical challenges.\nBackground in HR/Talent/Learning products (preferred).\nExcellent interpersonal and communication skills.\nHow to Apply\nExcited to shape the future of AI-driven solutions Share your updated resume with us today. Explore this opportunity and more through VARITE Indiayour gateway to impactful career paths.\nApply now to make your mark!",
        "skills": [
            "Nlp",
            "Regex",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Dynpro",
        "experience": "1-3 Years",
        "salary": null,
        "location": "Kolkata",
        "industry": "Information Technology",
        "job_description": "Job Description\nWe are seeking a motivated Data Scientist with 1-3 years of experience to join our analytics team. The ideal candidate will leverage their expertise in data analysis and machine learning to drive business insights and support decision-making processes.\nResponsibilities\nAnalyze complex datasets to derive actionable insights.\nDevelop predictive models and machine learning algorithms to solve business problems.\nVisualize data and present findings to stakeholders in a clear and concise manner.\nCollaborate with cross-functional teams to integrate data-driven solutions into business processes.\nConduct experiments to improve data quality and model performance.\nSkills and Qualifications\nProficiency in programming languages such as Python and R.\nStrong knowledge of statistical analysis and machine learning techniques.\nExperience with data manipulation and analysis tools like SQL and Pandas.\nFamiliarity with data visualization tools such as Tableau or Matplotlib.\nUnderstanding of big data technologies like Hadoop or Spark is a plus.\nBachelor's degree in Computer Science, Data Science, Statistics, or a related field.",
        "skills": [
            "Tensorflow",
            "Machine Learning",
            "Pandas",
            "Data Visualization",
            "Python",
            "Sql",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kanini Software Solutions",
        "experience": "1-6 Years",
        "salary": null,
        "location": "Coimbatore, Pune",
        "industry": "Software Engineering",
        "job_description": "About the Role:\nWe are seeking a highly skilled Senior Data Scientist with expertise in Python, Machine Learning (ML), Natural Language Processing (NLP), Generative AI (GenAI), and Azure Cloud Services. The ideal candidate will be responsible for designing, developing, and deploying advanced AI/ML models to drive data-driven decision-making. This role requires strong analytical skills, proficiency in AI/ML technologies, and experience with cloud-based solutions.\nKey Responsibilities:\nDesign and develop ML, NLP, and GenAI models to solve complex business problems.\nBuild, train, and optimize AI models using Python and relevant ML frameworks.\nImplement Azure AI/ML services for scalable deployment of models.\nDevelop and integrate APIs for real-time model inference and decision-making.\nWork with large-scale data to extract insights and drive strategic initiatives.\nCollaborate with cross-functional teams, including Data Engineers, Software Engineers, and Product Teams, to integrate AI/ML solutions into applications.\nImplement CI/CD pipelines to automate model training, deployment, and monitoring.\nEnsure adherence to software engineering best practices and Agile methodologies in AI/ML projects.\nStay updated on cutting-edge AI/ML advancements and continuously enhance models and algorithms.\nConduct research on emerging AI/ML trends and contribute to the development of innovative solutions.\nProvide technical mentorship and guidance to junior data scientists.\nOptimize model performance and scalability in a production environment.\n---\nRequired Skills & Qualifications:\nProficiency in Python and ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nHands-on experience in NLP techniques, including transformers, embeddings, and text processing.\nExpertise in Generative AI models (GPT, BERT, LLMs, etc.).\nStrong knowledge of Azure AI/ML services, including Azure Machine Learning, Azure Cognitive Services, and Azure Databricks.\nExperience in developing APIs for model deployment and integration.\nFamiliarity with CI/CD pipelines for AI/ML models.\nStrong understanding of software engineering principles and best practices.\nExperience working in an Agile development environment.\nExcellent problem-solving skills and ability to work in a fast-paced, dynamic environment.\nStrong background in statistical analysis, data mining, and data visualization.\nLocation:\nChennai, Coimbatore, Bangalore, Pune\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Natural Language Processing",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kanini Software Solutions",
        "experience": "1-6 Years",
        "salary": null,
        "location": "Chennai, Bengaluru, Noida",
        "industry": "Software Engineering",
        "job_description": "About the Role:\nWe are seeking a highly skilled Senior Data Scientist with expertise in Python, Machine Learning (ML), Natural Language Processing (NLP), Generative AI (GenAI), and Azure Cloud Services. The ideal candidate will be responsible for designing, developing, and deploying advanced AI/ML models to drive data-driven decision-making. This role requires strong analytical skills, proficiency in AI/ML technologies, and experience with cloud-based solutions.\nKey Responsibilities:\nDesign and develop ML, NLP, and GenAI models to solve complex business problems.\nBuild, train, and optimize AI models using Python and relevant ML frameworks.\nImplement Azure AI/ML services for scalable deployment of models.\nDevelop and integrate APIs for real-time model inference and decision-making.\nWork with large-scale data to extract insights and drive strategic initiatives.\nCollaborate with cross-functional teams, including Data Engineers, Software Engineers, and Product Teams, to integrate AI/ML solutions into applications.\nImplement CI/CD pipelines to automate model training, deployment, and monitoring.\nEnsure adherence to software engineering best practices and Agile methodologies in AI/ML projects.\nStay updated on cutting-edge AI/ML advancements and continuously enhance models and algorithms.\nConduct research on emerging AI/ML trends and contribute to the development of innovative solutions.\nProvide technical mentorship and guidance to junior data scientists.\nOptimize model performance and scalability in a production environment.\n---\nRequired Skills & Qualifications:\nProficiency in Python and ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nHands-on experience in NLP techniques, including transformers, embeddings, and text processing.\nExpertise in Generative AI models (GPT, BERT, LLMs, etc.).\nStrong knowledge of Azure AI/ML services, including Azure Machine Learning, Azure Cognitive Services, and Azure Databricks.\nExperience in developing APIs for model deployment and integration.\nFamiliarity with CI/CD pipelines for AI/ML models.\nStrong understanding of software engineering principles and best practices.\nExperience working in an Agile development environment.\nExcellent problem-solving skills and ability to work in a fast-paced, dynamic environment.\nStrong background in statistical analysis, data mining, and data visualization.\nLocation:\nChennai, Coimbatore, Bangalore, Pune\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Natural Language Processing",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Expert Data Scientist",
        "company_name": "Ciklum",
        "experience": "1-2 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "About the Role\nAs an Expert Data Scientist, you'll become a key member of a cross-functional development team engineering the experiences of tomorrow. You'll develop advanced models, derive actionable insights from complex datasets, and work closely with stakeholders to create meaningful business and healthcare outcomes.\nResponsibilities\nDevelop prototype solutions, mathematical models, algorithms, machine learning techniques, and robust analytics to support insight generation and data visualization.\nConduct exploratory data analysis to uncover trends, patterns, and high-level insights.\nProvide optimization recommendations to support KPIs across product, marketing, operations, PR, and other business units.\nCollaborate with engineering teams to ensure developed solutions meet standards for functionality, scalability, performance, and reliability.\nWork with business analysts and data engineers to understand their use cases and support implementation.\nIdentify opportunities for leveraging data to solve business problems and improve outcomes.\nDrive innovation by researching and applying new methods, tools, and statistical techniques to improve decision-making processes.\nMentor teammates and promote knowledge sharing across the team.\nParticipate in community-building activities, internal knowledge exchange, and conferences.\nSupport marketing and sales teams through technical input, content creation, and customer meetings.\nRequirements\nGeneral Technical Requirements\nBSc, MSc, or PhD in Mathematics, Statistics, Computer Science, Engineering, Operations Research, Econometrics, or related fields.\nStrong knowledge of probability theory, statistics, and the mathematics behind machine learning.\nExperience using CRISP-ML(Q) or TDSP methodologies to solve business problems.\nProficient in machine learning techniques including:\nRegression\nClassification\nClustering\nDimensionality reduction\nStrong proficiency in Python for modeling and statistical analysis.\nSkilled in data visualization with libraries such as Matplotlib, Seaborn, or Plotly.\nSpecific Technical Skills\nAdvanced SQL skills for data manipulation, sampling, and reporting.\nExperience with:\nImbalanced datasets\nTime series data (preprocessing, feature engineering, forecasting)\nOutlier and anomaly detection\nHandling various data types (text, image, video)\nFamiliarity with cloud-based ML services: AWS SageMaker, Azure ML, or Google AI Platform.\nDomain Experience (Healthcare)\nAnalyzing medical signals and images.\nPredictive modeling for outcomes, disease progression, readmissions, and population health risks.\nNLP/text mining on clinical notes, medical literature, or patient-reported data.\nExperience with survival analysis and time-to-event modeling.\nDesigning and analyzing clinical trials or research studies.\nCausal inference methods (e.g., propensity score matching, instrumental variable techniques).\nKnowledge of healthcare regulations like HIPAA, GDPR, and FDA compliance.\nSecure handling of healthcare data, including de-identification and patient consent.\nFamiliarity with federated learning and decentralized models.\nUnderstanding of healthcare interoperability standards: HL7, SNOMED, FHIR, DICOM.\nAbility to work with clinicians, researchers, and policymakers to extract actionable insights.\nGood to Have Skills\nMLOps experience: integrating ML into production, using Docker, Kubernetes.\nExperience in deep learning with TensorFlow or PyTorch.\nKnowledge of LLMs and Generative AI.\nExperience with MS SQL Server, PostgreSQL, Databricks, Snowflake.\nFamiliarity with Big Data technologies (e.g., Hadoop, Apache Spark).\nExperience with NoSQL databases (e.g., Cassandra, Neo4j).\nBusiness-Related Requirements\nDemonstrated success in delivering data science projects that drive measurable business impact.\nAbility to translate business problems into data science use cases and execute them end-to-end.\nExcellent project and time management skills.\nStrong communication and storytelling skills for conveying complex technical concepts to stakeholders.\nDesirable\nPublished research or peer-reviewed journal articles.\nRecognized achievements in data science competitions (e.g., Kaggle).\nCertifications in cloud-based ML platforms (AWS, Azure, GCP).",
        "skills": [
            "Plotly",
            "AWS SageMaker",
            "Matplotlib",
            "Azure ML",
            "Seaborn",
            "Data Visualization",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Machine Learning / AI)",
        "company_name": "Som Imaging Informatics",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Kolkata",
        "industry": "Health Care",
        "job_description": "Fundamentals of data science and programming.\nNeural networks and deep learning.\nExperience :\n5- 8 years in Data Scientist/ Data Science role with Good working knowledge in Machine Learning / Python with Team Management experience",
        "skills": [
            "Data Science",
            "Neural Networks",
            "Deep Learning",
            "Data Science - ML",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Innovaccer Analytics Private Limited",
        "experience": "7-12 Years",
        "salary": null,
        "location": "Noida, Delhi NCR",
        "industry": "Health Care",
        "job_description": "Design and lead the development of various artificial intelligence initiatives to help improve health and we'llness of patients\nWork with the business leaders and customers to understand their pain-points and build large-scale solutions for them.\nDefine technical architecture to productize Innovaccer s machine-learning algorithms and take them to market with partnerships with different organizations\nProven ability to break down complex business problems into machine learning problems and design solution workflows.\nWork with our data platform and applications team to help them successfully integrate the data science capability or algorithms in their product/workflows.\nWork with development teams to build tools for repeatable data tasks that will accelerate and automate development cycle.\nDefine and execute on the quarterly roadmap\nWhat You Need\nMasters in Computer Science, Computer Engineering or other relevant fields (PhD Preferred)\n7+ years of experience in Data Science (healthcare experience will be a plus)\nStrong written and spoken communication skills\nStrong hands-on experience in Python - building enterprise applications alongwith optimization techniques.\nStrong experience with deep learning techniques to build NLP/Computer vision models as we'll as state of art GenAI pipelines - knowledge of implementing agentic workflows is a plus.\nHas demonstrable experience deploying deep learning models in production at scale with interactive improvements- would require hands-on expertise with at least 1 deep learning frameworks like Pytorch or Tensorflow.\nHas keen interest in research and stays updated with key advancements in the area of AI and ML in the industry.\nDeep understanding of classical ML techniques - Random Forests, SVM, Boosting, Bagging - and building training and evaluation pipelines.\nDemonstrate experience with global and local model explainability using LIME, SHAP and associated techniques.\nHands on experience with at least one ML platform among Databricks, Azure ML, Sagemaker s\nExperience in developing and deploying production ready models\nKnowledge of implementing an MLOps framework.\nPossess a customer-focused attitude through conversations and documentation",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Artificial Intelligence, Machine Learning)",
        "company_name": "Franklin Templeton",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Financial Services",
        "job_description": "What are the ongoing responsibilities of a Data ScientistData Collection and Preprocessing:\nImplement data collection strategies to gather relevant data from different sources.\nClean, preprocess, and validate data to ensure accuracy and reliability for subsequent analysis.\nAssist in maintaining and enhancing data pipelines in collaboration with data engineering teams.\nStatistical Analysis:\nPerform exploratory data analysis to identify trends, patterns, and insights within datasets.\nApply basic statistical techniques to test hypotheses, validate assumptions, and draw conclusions from data.\nMachine Learning and AI Model Development:\nDevelop and optimize machine learning models to address business problems and improve processes.\nExplore and apply Generative AI techniques under the supervision of senior team members to contribute to innovative solutions.\nAssist in model evaluation, validation, and deployment, monitoring performance and making adjustments as needed.\nUnderstanding Human Behavior for AI Applications:\nAnalyze data related to human behavior to help develop models that predict and influence outcomes.\nWork with domain experts to incorporate insights into AI models, enhancing their relevance and accuracy.\nData Engineering Collaboration:\nCollaborate with data engineering teams to ensure smooth integration of models into existing data systems.\nContribute to designing and implementing scalable data storage solutions.\nCross-functional Collaboration:\nWork with product management, marketing, and business stakeholders to understand requirements and provide data-driven insights.\nCommunicate analytical concepts and insights effectively to non-technical stakeholders through reports and visualizations.\nContinuous Learning and Innovation:\nStay updated on the latest developments in data science, machine learning, and AI technologies.\nExperiment with new methodologies and tools to enhance project outcomes and expand your skill set.\nWhat ideal qualifications, skills & experience would help someone to be Successful\nMaster s or Bachelor s degree in Statistics, Mathematics, Econometrics, Computer Science, Engineering, or related disciplines\n2-4 years of experience in data science, predictive modeling, and machine learning.\nProficiency in Python, R, or SQL, with hands-on experience in data analysis and model development.\nBasic knowledge of Generative AI models and their applications\nAbility to translate business problems into analytical tasks.\nSkill in explaining statistical and machine learning techniques to business partners.\nExperience in creating data-driven stories and insights.\nProficiency in handling large datasets, including data cleansing, manipulation, and mining.\nCapability to tackle ambiguous challenges with a problem-solving mindset.\nCuriosity and willingness to learn independently.\nStrong written and verbal communication skills.\nEffective organizational and planning abilities\nAbility to work well under pressure and adapt in a dynamic environment.\nTeam-oriented approach with a capacity to work independently when needed.\nStrong interpersonal skills and the ability to build relationships with colleagues and stakeholders",
        "skills": [
            "Data Analysis",
            "R",
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Manager - Data Scientist",
        "company_name": "Innovaccer Analytics Private Limited",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Noida, Delhi NCR",
        "industry": "Health Care",
        "job_description": "Data is the foundation of our innovation.\nWe are seeking a Manager, Data Science with expertise in NLP and Generative AI to lead the development of cutting-edge AI-driven solutions in healthcare.\nThis role requires a deep understanding of healthcare data and the ability to design and implement advanced language models that extract insights, automate workflows, and enhance clinical decision-making.\nWe're looking for a visionary leader who can define and build the next generation of AI-driven tools, leveraging LLMs, deep learning, and predictive analytics to personalize care based on patients clinical and behavioral history.\nIf you're passionate about pushing the boundaries of AI in healthcare, we'd love to hear from you!\nA Day in the Life\nTeam Leadership & Development: Build, mentor, and manage a team of data scientists, and machine learning engineers. Foster a culture of collaboration, innovation, and technical excellence.\nRoadmap Execution: Define and execute on the quarterly AI/ML roadmap, setting clear goals, priorities, and deliverables for the team.\nWork with the business leaders and customers to understand their pain-points and build large-scale solutions for them.\nDefine technical architecture to productize Innovaccer s machine-learning algorithms and take them to market with partnerships with different organization.\nWork with our data platform and applications team to help them successfully integrate the data science capability or algorithms in their product/workflows.\nProject & Stakeholder Management: Work closely with cross-functional teams, including product managers, engineers, and business leaders, to align AI/ML initiatives with company objectives.\nWhat You Need\nMasters in Computer Science, Computer Engineering or other relevant fields (PhD Preferred)\n8+ years of experience in Data Science (healthcare experience will be a plus)\nStrong experience with deep learning techniques to build NLP/Computer vision models as well as state of art GenAI pipelines - Has demonstrable experience deploying deep\nlearning models in production at scale with interactive improvements- would require\nhands-on expertise with at least 1 deep learning frameworks like Pytorch or Tensorflow.\nStrong hands-on experience in building GenAI applications - building LLM based workflows along with optimization techniques - knowledge of implementing agentic\nworkflows is a plus.\nHas keen interest in research and stays updated with key advancements in the area of AI and ML in the industry. Having patents/publications in any area of AI/ML is a great add on.\nHands on experience with at least one ML platform among Databricks, Azure ML, Sagemaker s\nStrong written and spoken communication skills",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Python",
            "Data Visualization",
            "Big Data"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Innovaccer Analytics Private Limited",
        "experience": "7-12 Years",
        "salary": null,
        "location": "Noida, Delhi NCR",
        "industry": "Health Care",
        "job_description": "Design and lead the development of various artificial intelligence initiatives to help improve health and we'llness of patients\nWork with the business leaders and customers to understand their pain-points and build large-scale solutions for them.\nDefine technical architecture to productize Innovaccer s machine-learning algorithms and take them to market with partnerships with different organizations\nProven ability to break down complex business problems into machine learning problems and design solution workflows.\nWork with our data platform and applications team to help them successfully integrate the data science capability or algorithms in their product/workflows.\nWork with development teams to build tools for repeatable data tasks that will accelerate and automate development cycle.\nDefine and execute on the quarterly roadmap\nWhat You Need\nMasters in Computer Science, Computer Engineering or other relevant fields (PhD Preferred)\n7+ years of experience in Data Science (healthcare experience will be a plus)\nStrong written and spoken communication skills\nStrong hands-on experience in Python - building enterprise applications alongwith optimization techniques.\nStrong experience with deep learning techniques to build NLP/Computer vision models as we'll as state of art GenAI pipelines - knowledge of implementing agentic workflows is a plus.\nHas demonstrable experience deploying deep learning models in production at scale with interactive improvements- would require hands-on expertise with at least 1 deep learning frameworks like Pytorch or Tensorflow.\nHas keen interest in research and stays updated with key advancements in the area of AI and ML in the industry.\nDeep understanding of classical ML techniques - Random Forests, SVM, Boosting, Bagging - and building training and evaluation pipelines.\nDemonstrate experience with global and local model explainability using LIME, SHAP and associated techniques.\nHands on experience with at least one ML platform among Databricks, Azure ML, Sagemaker s\nExperience in developing and deploying production ready models\nKnowledge of implementing an MLOps framework.\nPossess a customer-focused attitude through conversations and documentation",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Python",
            "Data Visualization",
            "Big Data"
        ]
    },
    {
        "job_title": "Principle Data Scientist",
        "company_name": "Nalco Water",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Description\nDesign and Develop AI-Powered Solutions:Lead the design, development, and deployment of AI-powered solutions using Azure Databricks, and other components or our technology stack.Technical Leadership:Provide technical leadership and guidance to junior data scientists and engineers on Databricks, Mosaic AI, and AI agent development. Mentor and coach team members to improve their skills and expertise.Databricks and Mosaic AI Expertise:Develop and maintain expertise in Databricks and Mosaic AI, staying up-to-date with the latest features and best practices. Apply this expertise to drive the adoption of these technologies within the organization.AI Agent Development:Design and develop AI agents using Mosaic AI and other relevant technologies. Collaborate with stakeholders to identify opportunities for AI agents to drive business value.Data Science Innovation:Stay abreast of the latest advancements in data science and AI, identifying opportunities to apply new techniques and technologies to drive business innovation.Collaboration and Communication:Collaborate with stakeholders across the organization to identify business problems and develop data-driven solutions. Communicate complex technical concepts to non-technical stakeholders, driving adoption and understanding of data science solutions.\nRequirements:\nEducation/Experience:Degree or advanced degree in data science, physics, mathematics, statistics, computer science or related quantitative field. BS and 10+ years related experience or MS and 7+ years related experience or PhD and less than 2 years experience. 1-3 Years Supervisory experience preferred.Technical Skills:\nProficiency in Databricks, or a similar platform like AWS Sage Maker, Azure Machine Learning, Vertex AI, and others.\nStrong programming skills in languages such as Python, Scala, SQL, etc.\nFamiliarity with data engineering, data warehousing, and data governance.\nSoft Skills:\nExcellent communication and collaboration skills.\nStrong leadership and mentoring skills.\nAbility to drive innovation and stay up-to-date with the latest advancements in data science and AI.\nNice to Have:\nExperience with Cloud Platforms:Experience working with cloud platforms such as AWS, Azure, or Google Cloud.Experience with AI agents:Experience developing AI agents using Mosaic AI or similar code-first platforms.Certifications:Databricks or Mosaic AI certifications are a plus.Open-Source Contributions:Contributions to open-source projects related to data science, machine learning, or AI.",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Machine Learning",
            "Python Programming"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Cyber Infrastructure",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Indore",
        "industry": "Cyber Security",
        "job_description": "4+ years with hands on experience with building language models, machine learning and AI models leveraging industry tools, and products. Proficient in large language models.\nRequirements :\n- Proven experience as a Senior Lead, with expertise in AI/ML, Python and Web Technologies.\n- Experience with Python, Spacy, SciKit, NLP, Text Processing, LLMs etc.,\n- Strong knowledge of AI/ML techniques for various activities from Data Wrangling to AI/ML Model developments.\n- Strong Knowledge of the AI/ML algorithms, generative AI Space and both commercial and open -source libraries.\n- Machine Learning and AI - TensorFlow, Pattern, Opencv, Artificial intelligence, Machine learning, Deep learning, Natural Language Processing, sentiment analysis, Artificial Neural network, Generative AI, LLM & Computer Vision\n- Expertise in dealing and setting up the AI/ML Models with experience in ML/Ops\n- Familiarity with database systems (SQL, Oracle, MySQL) and proficiency in SQL queries.\n- Design, develop, and maintain web applications, both front-end and back-end, using modern web technologies.\n- Experience with cloud platforms and deploying applications to the cloud.\n- Ensure the responsiveness and scalability of the applications.\n- Prior experience in leading development projects is a plus.\n- Should have good knowledge of Deployment, as well as on project estimation.\n- Excellent leadership and communication skills, with the ability to guide and motivate a team.\n- Strong problem-solving and analytical abilities.",
        "skills": [
            "Deployment",
            "leading development teams",
            "Database Systems",
            "oracle",
            "MLops"
        ]
    },
    {
        "job_title": "Sr. Data Scientist",
        "company_name": "Nihilent",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Kolkata",
        "industry": "Business Information Systems",
        "job_description": "5+ years of DS and AI/ML project implementation experience in one of the cloud environments (AWS/Azure/GCP). Advanced / intermediate level certification is a plus\nHands-on experience in data handling using SQL and advanced analytics tools such as Python\nHands on Experience in analyzing and visualizing data using tools such as Tableau, PowerBI\nExperience in scoping, development, validation & deployment of explanatory, predictive, and prescriptive analytic solutions\nExperience in stakeholder management, driving project governance, and linking analytics to business value delivered\nKnowledge of Deep Learning models and its applications in Natural Language Processing (NLP), AI/GenAI and Computer Vision(CV) in solving business problems\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate",
        "skills": [
            "Project Implementation",
            "Project Governance",
            "Stakeholder Management",
            "Deep Learning",
            "Advanced Analytics",
            "Computer Vision",
            "Gcp",
            "Natural Language Processing",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Spectrum Talent Management",
        "experience": "6-11 Years",
        "salary": null,
        "location": "Mumbai",
        "industry": "Recruiting, Staffing Agency",
        "job_description": "As a Senior Data Scientist/Manager specializing in Generative AI, you will:\nShape Next-Generation Solutions: Partner with cross-functional teams to design and implement AI-driven systems that transform user experiences and streamline business processes.\nEnhance AI Through Data: Employ data-centric methodologies to improve model accuracy and responsiveness by curating high-quality datasets and optimizing real-time performance.\nFinetune Industry-Specific Gen AI Models: Adapt and fine-tune large language models for domain-specific applications using advanced techniques like instruction tuning, domain adaptation, and reinforcement learning from human feedback (RLHF).\nDevelop Intelligent AI Agents: Architect and refine AI agents that solve complex business challenges, leveraging LLMs to deliver personalized, user-centric solutions.\nAdvance Generative AI Applications: Innovate with cutting-edge generative AI models such as LLM, VLM, GANs, and VAEs to create tailored applications for dynamic content creation, predictive analytics, and enhanced automation.\nScale AI with Cloud Technology: Deploy and scale LLM-based solutions on platforms like GCP, AWS and Azure to address real-world business problems with precision and efficiency.\nKey Responsibilities\nDesign AI Systems: Build AI agents for tasks such as content compliance, asset decomposition, and contextual personalization.\nDevelop NLP Pipelines: Implement advanced NLP solutions for search relevance, intent detection, and dynamic content generation.\nIntegrate Multi-Modal Systems: Combine data modalities such as text, images, and metadata for enriched user interactions and insights.\nOptimize AI Pipelines: Innovate in latency reduction, scalability, and real-time responsiveness for AI systems in production.\nCollaborate on AI Innovation: Work with business stakeholders to identify opportunities and deliver impactful AI-driven solutions.\nEXPERIENCE\nBS/MS in Computer Science, Math, Physics, Engineering, Statistics, or another quantitative or computational field. Advanced degrees preferred.\nFine tuning experience of Large Language models (LLMs, VLLMs, or Vision model)\nDistributed training or inference experience with frameworks like Ray, vllm, openllm, bentoML etc.\nExperience with frameworks like like LangChain, Llamaindex for building maintainable, scalable Generative AI applications.\nDeployment experience or optimized hosting experience of Large Language models (LLMs, VLLMs, or Vision model)\nExperience working with any Vector database like Milvus, FAISS, ChromaDB etc\nExperience developing agents with frameworks like LangGraph, CrewAI, Autogen etc\nExperience with prompt engineering\nKeeping up with latest market trends\nExperience working with open source large language models from HuggingFace\nExperience working with atleast one public cloud provider such as Azure, AWS or GCP\nExperience working with container technology like Docker, ECS etc\nExperience with DevOps practices and CI/CD pipelines for data solutions\nExperience in deploying solutions to production with Kubernetes or OpenShift\nExperience with managing ML workflows with MLFlow or KubeFlow",
        "skills": [
            "Generative AI",
            "ML Workflows",
            "Artificial Intelligence",
            "Natural Language Processing",
            "Azure",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Principal Data Scientist - Credit Risk/Lending",
        "company_name": "Toast",
        "experience": "10-11 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Lead the development and implementation of advanced predictive models, to assess credit risk, optimize lending growth, and improve overall portfolio performance.\nWill actively engage in code reviews and comprehensive data quality assessments, ensuring that all development data pipelines and code meet the highest standards of reliability, accuracy, and readiness for seamless production deployment.Collaborate closely with cross-functional teams, including Risk Management, Product, Engineering, and Business Strategy, to identify opportunities for leveraging data-driven insights to support growth and risk mitigation initiatives.\nDesign, develop, and maintain innovative credit risk and lending growth strategies that align with the organizations objectives, while ensuring compliance with regulatory requirements and industry best practices.\nMonitor and analyze the performance of existing models and strategies, identifying areas for improvement and implementing data-driven refinements as needed.\nPresent complex data-driven insights and recommendations to stakeholders in a clear, concise, and actionable manner, while fostering a data-driven culture within the organization.\nMentor and guide junior data scientists, fostering a collaborative environment and promoting the growth and development of the data science team.\nStay current with the latest trends and developments in data science, fintech, and credit risk management, continually exploring new methods and technologies to enhance the organizations capabilities.\nDo you have the rightingredients*\nAdvanced degree in quantitative field such as Data Science, Statistics, Mathematics, Financial Engineering or related discipline\n10+ years of experience in data science, with a focus on credit risk modeling in retail lending space (consumer or small business) and lending growth strategies, preferably within the fintech lending space.\nProficiency in machine learning, AI, and statistical modeling techniques, with a strong track record of developing and deploying successful predictive models in a business setting.\nFamiliarity with the US model risk management policies such as SR 11-7 that govern the lending models is strongly preferred.\nStrong proficiency in Python and SQL and experience with data science libraries and tools such as: Spark, Scala, scikit-learn, Tensorflow, PyTorch, XGBoost etc.\nFamiliarity with standard software engineering practices and tools including object-oriented programming, test-driven development, CI/CD, git, task orchestration (Airflow) and AWS tooling\nStrong understanding of credit risk management, lending practices, and regulatory requirements within the financial industry.\nExcellent communication and presentation skills, with the ability to articulate complex data-driven insights to both technical and non-technical stakeholders.\nStrong leadership skills, with experience mentoring and guiding junior data scientists",
        "skills": [
            "Machine Learning",
            "Data Modeling",
            "Sql Queries",
            "Statistical Analysis",
            "Python Programming"
        ]
    },
    {
        "job_title": "Lead Data Scientist (Agentic AI)",
        "company_name": "Nalco Water",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Job description\nDesign, develop, and deploy conversational AI agents using Microsoft Copilot Studio and Power Virtual Agents.\nLeverage Power Platform tools (Power Apps, Power Automate, Dataverse) to build end-to-end intelligent agent solutions.\nApply data science techniques including machine learning and predictive modelling to enrich agent functionality.\nIntegrate the agents with enterprise systems, data sources, and APIs for contextual, real-time responses.\nImplement custom workflows, triggers, and connectors to extend agent capabilities\nOptimize conversation flows, implement prompt engineering best practices, and utilize analytics for continuous improvement.",
        "skills": [
            "Machine Learning",
            "Big Data",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Covalense Global",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Bengaluru, Hyderabad, Uttar Pradesh",
        "industry": "IT Management",
        "job_description": "Undertaking data collection, preprocessing and analysis\nBuilding models to address business problems\nPresenting information using data visualization techniques\nIdentify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams",
        "skills": [
            "Data Analysis",
            "Data preprocessing",
            "Data Collection",
            "Data Visualization",
            "Machine Learning",
            "Predictive Modeling"
        ]
    },
    {
        "job_title": "Data Scientist - APM",
        "company_name": "Smiths Detection",
        "experience": "2-7 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software Engineering",
        "job_description": "Smiths Detection is seeking an experienced Senior Data Scientist to join the company on a permanent basis. The successful candidate will bring a strong background of using AI/ML/DL algorithms and techniques. You are expected to contribute your proven experience as a Machine Learning Engineer.\nSmiths Detection's Digital Platform is an innovative global security platform designed to transform the way government agencies and organizations secure and manage their ports, airports, infrastructure and border checkpoints. It brings all security operation information together onto a single, integrated and secure platform to achieve 100% data inspection of all cases. Using insights produced through machine learning, artificial intelligence (AI) and data science, our platform enables people to make informed decisions quickly to protect against any border security threat.\nSmiths Detection is looking for dynamic, innovative and motivated software architects to be the part of its global platform and applications development team in Technology group.\nThe purpose of this role is to develop, sustain and enhance technical architecture of Smith's cloud (and on-premise) based Platform and its applications.\nThe Senior Data Scientist is part of the overall Platform and Application Architecture group, working closely with Sales, Products & Technology, Program Management, Service and overall business management to deliver successful pursuit outcomes and grow customer engagement, intimacy and our opportunity pipeline.\nThis person uses professional concepts and company objectives to resolve complex issues in creative and effective ways. The role brings the capabilities of solution architecture and design using latest tech stack and in house developed platform. He/she applies extensive technical expertise and has full knowledge of other related disciplines. The role requires person to be hands-on software developer.\nResponsibilities:\nStudy and transform data science prototypes\nDesign machine learning systems\nResearch and implement appropriate ML algorithms and tools\nDevelop machine learning applications according to requirements\nSelect appropriate datasets and data representation methods\nRun machine learning tests and experiments\nPerform statistical analysis and fine-tuning using test results\nTrain and retrain systems when necessary\nExtend existing ML libraries and frameworks\nKeep abreast of developments in the field\nCollaborate with engineering and product development teams",
        "skills": [
            "billing",
            "Data Modelling",
            "Http",
            "Mvc",
            "Opensource",
            "Data Structure"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Systechcorp Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "We are inviting applications for the role of Lead Consultant Data Scientist with experience in Artificial Intelligence and Generative Models. We're seeking a talented and experienced professional to join our team and play a key role in developing cutting-edge, AI-powered solutions using large language models.\nKey Responsibilities\nCollaborate with cross-functional teams to identify, analyze, and interpret complex datasets to generate actionable insights.\nDesign, develop, and implement:\nAdvanced statistical models\nMachine learning algorithms\nAI applications and generative models (using LLMs such as GPT-3, BERT, etc.)\nFrameworks like Retrieval-Augmented Generation (RAG), Knowledge Graphs, etc.\nCommunicate insights clearly to both technical and non-technical stakeholders through presentations, reports, and data visualizations.\nMonitor and optimize the performance of AI and generative models on an ongoing basis.\nStay updated with the latest trends, tools, and advancements in data science, AI, and generative modeling.\nMentor and guide junior team members to foster growth and collaboration within the team.\nMinimum Qualifications\nBachelor's or Master's degree in Data Science, Computer Science, Statistics, or a related field.\nProven experience in:\nData science and machine learning\nAI applications and generative AI modeling\nStrong programming skills in Python, R, or equivalent languages.\nHands-on experience with large language models (e.g., GPT-3, BERT) and frameworks such as RAG and Knowledge Graphs.\nExperience working with large datasets using SQL, NoSQL, Hadoop, Spark, etc.\nExcellent problem-solving, analytical, and critical thinking abilities.\nStrong communication and interpersonal skills for cross-functional collaboration.\nPreferred Qualifications\nExperience deploying AI and generative models in production environments on AWS, Azure, or Google Cloud Platform (GCP).\nFamiliarity with domain-specific data sources and challengesespecially in the Insurance industry.\nProven experience leading data science projects end-to-end, with demonstrated project management and team collaboration skills.",
        "skills": [
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Python",
            "Sql",
            "Aws"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Systechcorp Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Bengaluru, Chennai",
        "industry": "Software",
        "job_description": "We are inviting applications for the role of Lead Consultant Data Scientist with experience in Artificial Intelligence and Generative Models. We're seeking a talented and experienced professional to join our team and play a key role in developing cutting-edge, AI-powered solutions using large language models.\nKey Responsibilities\nCollaborate with cross-functional teams to identify, analyze, and interpret complex datasets to generate actionable insights.\nDesign, develop, and implement:\nAdvanced statistical models\nMachine learning algorithms\nAI applications and generative models (using LLMs such as GPT-3, BERT, etc.)\nFrameworks like Retrieval-Augmented Generation (RAG), Knowledge Graphs, etc.\nCommunicate insights clearly to both technical and non-technical stakeholders through presentations, reports, and data visualizations.\nMonitor and optimize the performance of AI and generative models on an ongoing basis.\nStay updated with the latest trends, tools, and advancements in data science, AI, and generative modeling.\nMentor and guide junior team members to foster growth and collaboration within the team.\nMinimum Qualifications\nBachelor's or Master's degree in Data Science, Computer Science, Statistics, or a related field.\nProven experience in:\nData science and machine learning\nAI applications and generative AI modeling\nStrong programming skills in Python, R, or equivalent languages.\nHands-on experience with large language models (e.g., GPT-3, BERT) and frameworks such as RAG and Knowledge Graphs.\nExperience working with large datasets using SQL, NoSQL, Hadoop, Spark, etc.\nExcellent problem-solving, analytical, and critical thinking abilities.\nStrong communication and interpersonal skills for cross-functional collaboration.\nPreferred Qualifications\nExperience deploying AI and generative models in production environments on AWS, Azure, or Google Cloud Platform (GCP).\nFamiliarity with domain-specific data sources and challengesespecially in the Insurance industry.\nProven experience leading data science projects end-to-end, with demonstrated project management and team collaboration skills.",
        "skills": [
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Python",
            "Sql",
            "Aws"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Coditas Technologies",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "Coditas Solutions is seeking a highly skilled and motivated Data Scientist to join our dynamic team. As a Data Scientist, you will play a key role in designing, implementing, and optimizing machine learning models and algorithms to solve complex business challenges. If you have a passion for leveraging AI and ML technologies to drive innovation, this is an exciting opportunity to contribute to groundbreaking projects.\nRoles and Responsibilities\nDesign, implement, and optimize machine learning algorithms using R and Python.\nWork on developing predictive models and decision-making systems.\nConduct exploratory data analysis to understand data patterns and insights.\nCollaborate with data engineers to ensure the availability and quality of data for model training.\nDeploy machine learning models into production environments.\nCollaborate with cross-functional teams to integrate models into existing systems.\nContinuously optimize and improve the performance of machine learning models.\nStay updated on the latest advancements in ML algorithms and technologies.\nWork closely with software engineers to ensure seamless integration of AI/ML solutions.\nCollaborate with clients to understand their business requirements and customize solutions accordingly.\nTechnical Skills\nExcellent programming skills with the ability to implement complex algorithms in Python or R.\nExperience with cloud-based platforms (AWS, Azure, GCP) for deploying machine learning models.\nStrong experience of minimum 3 years in developing and implementing machine learning algorithms.\nExperience with model deployment and integration into production systems.\nHands-on experience with use of standard classical machine learning libraries such as Scikit learn, NLTK, OpenCV as well as deep learning libraries Tensorflow, PyTorch, Keras.\nUnderstanding of machine learning algorithms, techniques, and concepts (linear regression, logistic regression, decision trees, random forests, neural networks, etc.).\nExperience with data preprocessing, feature engineering, and model evaluation techniques of structured and unstructured data. Proven experience with identifying, creating and selecting relevant features or variables to enhance model performance.\nAbility to collaborate effectively with cross-functional teams.\nPrevious experience working on real-world AI/ML projects.\nShould be focused on linear algebra, machine learning, and statistics & probability are preferred.\nAbility to have a basic knowledge of the LLMs and optimal use of the GenAI models.\nStrong problem-solving and critical-thinking skills.\nExcellent communication and collaboration skills.\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Sc in Any Specialization",
        "skills": [
            "Deploying Models",
            "ML Deployment",
            "Machine Learning",
            "Azure Cloud",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Principle Data Scientist",
        "company_name": "Nalco Water",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Description\nDesign and Develop AI-Powered Solutions:Lead the design, development, and deployment of AI-powered solutions using Azure Databricks, and other components or our technology stack.Technical Leadership:Provide technical leadership and guidance to junior data scientists and engineers on Databricks, Mosaic AI, and AI agent development. Mentor and coach team members to improve their skills and expertise.Databricks and Mosaic AI Expertise:Develop and maintain expertise in Databricks and Mosaic AI, staying up-to-date with the latest features and best practices. Apply this expertise to drive the adoption of these technologies within the organization.AI Agent Development:Design and develop AI agents using Mosaic AI and other relevant technologies. Collaborate with stakeholders to identify opportunities for AI agents to drive business value.Data Science Innovation:Stay abreast of the latest advancements in data science and AI, identifying opportunities to apply new techniques and technologies to drive business innovation.Collaboration and Communication:Collaborate with stakeholders across the organization to identify business problems and develop data-driven solutions. Communicate complex technical concepts to non-technical stakeholders, driving adoption and understanding of data science solutions.\nRequirements:\nEducation/Experience:Degree or advanced degree in data science, physics, mathematics, statistics, computer science or related quantitative field. BS and 10+ years related experience or MS and 7+ years related experience or PhD and less than 2 years experience. 1-3 Years Supervisory experience preferred.Technical Skills:\nProficiency in Databricks, or a similar platform like AWS Sage Maker, Azure Machine Learning, Vertex AI, and others.\nStrong programming skills in languages such as Python, Scala, SQL, etc.\nFamiliarity with data engineering, data warehousing, and data governance.\nSoft Skills:\nExcellent communication and collaboration skills.\nStrong leadership and mentoring skills.\nAbility to drive innovation and stay up-to-date with the latest advancements in data science and AI.\nNice to Have:\nExperience with Cloud Platforms:Experience working with cloud platforms such as AWS, Azure, or Google Cloud.Experience with AI agents:Experience developing AI agents using Mosaic AI or similar code-first platforms.Certifications:Databricks or Mosaic AI certifications are a plus.Open-Source Contributions:Contributions to open-source projects related to data science, machine learning, or AI.",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Machine Learning",
            "Python Programming"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Reflections Info Systems",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Thiruvananthapuram / Trivandrum, Bengaluru",
        "industry": "IT Management",
        "job_description": "Job Description\nA minimum of 8 years of professional experience, with at least 6 years in a data science role.\nStrong knowledge of statistical modeling, machine learning, deep learning and GenAI.\nProficiency in Python and hands on experience optimizing code for performance.\nExperience with data preprocessing, feature engineering, data visualization and hyperparameter tuning.\nSolid understanding of database concepts and experience working with large datasets.\nExperience deploying and scaling machine learning models in a production environment.\nFamiliarity with machine learning operations (MLOps) and related tools.\nGood understanding of Generative AI concepts and LLM finetuning.\nExcellent communication and collaboration skills.\nResponsibilities include:\nLead a high performance team, guide and mentor them on the latest technology landscape, patterns and design standards and prepare them to take on new roles and responsibilities.\nProvide strategic direction and technical leadership for AI initiatives, guiding the team in designing and implementing state-of-the-art AI solutions.\nLead the design and architecture of complex AI systems, ensuring scalability, reliability, and performance.\nLead the development and deployment of machine learning/deep learning models to address key business challenges.\nApply statistical modeling, data preprocessing, feature engineering, machine learning, and deep learning techniques to build and improve models.\nUtilize expertise in at least two of the following areas: computer vision, predictive analytics, natural language processing, time series analysis, recommendation systems.\nDesign, implement, and optimize data pipelines for model training and deployment.\nExperience with model serving frameworks (e.g., TensorFlow Serving, TorchServe, KServe, or similar).\nDesign and implement APIs for model serving and integration with other systems.\nCollaborate with cross-functional teams to define project requirements, develop solutions, and communicate results.\nMentor junior data scientists, providing guidance on technical skills and project execution.\nStay up-to-date with the latest advancements in data science and machine learning, particularly in generative AI, and evaluate their potential applications.\nCommunicate complex technical concepts and analytical findings to both technical and non-technical audiences.\nServes as a primary point of contact for client managers and liaises frequently with internal stakeholders to gather data or inputs needed for project work\nCertifications :\nBachelors or Masters degree in a quantitative field such as statistics, mathematics, computer science, or a related area.\nPrimary Skills :\nPython\nData Science concepts\nPandas, NumPy, Matplotlib\nArtificial Intelligence\nStatistical Modeling\nMachine Learning, Natural Language Processing (NLP), Deep Learning\nModel Serving Frameworks (e.g., TensorFlow Serving, TorchServe)\nMLOps(e.g; MLflow, Tensorboard, Kubeflow etc)\nComputer Vision, Predictive Analytics, Time Series Analysis, Anomaly\nDetection, Recommendation Systems (Atleast 2)\nGenerative AI, RAG, Finetuning(LoRa, QLoRa)\nProficent in any of Cloud Computing Platforms (e.g., AWS, Azure, GCP)\nSecondary Skills :\nExpertise in designing scalable and efficient model architectures is crucial for developing robust AI solutions.\nAbility to assess and forecast the financial requirements of data science projects ensures alignment with budgetary constraints and organizational goals.\nStrong communication skills are vital for conveying complex technical concepts to both technical and non-technical stakeholders.",
        "skills": [
            "Analytical",
            "Cloud Computing",
            "Machine Learning",
            "Data Science",
            "Natural Language Processing",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Systechcorp Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Delhi, Hyderabad",
        "industry": "Software",
        "job_description": "We are inviting applications for the role of Lead Consultant Data Scientist with experience in Artificial Intelligence and Generative Models. We're seeking a talented and experienced professional to join our team and play a key role in developing cutting-edge, AI-powered solutions using large language models.\nKey Responsibilities\nCollaborate with cross-functional teams to identify, analyze, and interpret complex datasets to generate actionable insights.\nDesign, develop, and implement:\nAdvanced statistical models\nMachine learning algorithms\nAI applications and generative models (using LLMs such as GPT-3, BERT, etc.)\nFrameworks like Retrieval-Augmented Generation (RAG), Knowledge Graphs, etc.\nCommunicate insights clearly to both technical and non-technical stakeholders through presentations, reports, and data visualizations.\nMonitor and optimize the performance of AI and generative models on an ongoing basis.\nStay updated with the latest trends, tools, and advancements in data science, AI, and generative modeling.\nMentor and guide junior team members to foster growth and collaboration within the team.\nMinimum Qualifications\nBachelor's or Master's degree in Data Science, Computer Science, Statistics, or a related field.\nProven experience in:\nData science and machine learning\nAI applications and generative AI modeling\nStrong programming skills in Python, R, or equivalent languages.\nHands-on experience with large language models (e.g., GPT-3, BERT) and frameworks such as RAG and Knowledge Graphs.\nExperience working with large datasets using SQL, NoSQL, Hadoop, Spark, etc.\nExcellent problem-solving, analytical, and critical thinking abilities.\nStrong communication and interpersonal skills for cross-functional collaboration.\nPreferred Qualifications\nExperience deploying AI and generative models in production environments on AWS, Azure, or Google Cloud Platform (GCP).\nFamiliarity with domain-specific data sources and challengesespecially in the Insurance industry.\nProven experience leading data science projects end-to-end, with demonstrated project management and team collaboration skills.",
        "skills": [
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Python",
            "Sql",
            "Aws"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Netcore Cloud",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Bengaluru, Mumbai",
        "industry": "Marketing Automation",
        "job_description": "Keyskills :\nProgramming, Machine Learning, Artificial Intelligence, Data Science, Business Analytics, Product engineering, Requirement gathering, Problem formulation, quick POCs\nResponsibilities\nTo help designing, innovating and building our next generation ML architecture\nIdentify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams\nTeach and mentor others in the use of AI/Machine Learning\nMust-Have :\nExperience in data mining\nStrong math skills (e.g. statistics, algebra)\nStrong programming skills in R, Python and familiarity with Java, Scala, C++\nDB/NoSql MongoDB, Neo4J, MySql. Cassandra, DynamoDB, Redshift\nExperience on Hadoop Map Reduce, Pig, Hive, Mahout and Apache Spark, H20\nStrong experience in Data warehousing, ETL, BI (e.g. Tableau, Power BI) and Data Visualization tools (matplotlib, D3.js, Plotly.js, Shiny etc)\nExperience in neural networks, regression, classification and clustering\nThink big & scale\nGood to have :\nExperience with Deep Learning tools Tensorflow, Theano, Caffe etc.\nElastic Search, NLP background and Machine Learning Platforms\nExperienced in deployment of High performance, Scalable Big Data Hadoop clusters and Web applications on cloud infrastructure (Azure, AWS, Bluemix etc)",
        "skills": [
            "moodelling",
            "ml architecture",
            "quick pcos",
            "Machine Learning",
            "Artificial Intelligence",
            "Programming"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Future Focus Infotech",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai City, Mumbai",
        "industry": "Information Technology",
        "job_description": "Overview:\nWe are seeking a highly motivated and detail-oriented individual to join our team as a Data Scientist. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.\nKey Responsibilities:\nProvide support and expertise in the domain of Data Scientist.\nCollaborate with cross-functional teams to achieve business goals.\nEnsure timely delivery of services and maintain high-quality standards.\nRequired Qualifications:\nProven experience in a relevant field or position.\nStrong understanding of the responsibilities and tools associated with the role.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nCertifications or training relevant to Data Scientist.\nExperience working in a fast-paced environment or large organizations.",
        "skills": [
            "Random Forests",
            "Data Science",
            "Machine Learning",
            "SAS",
            "Linear Regression",
            "Decision Trees",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Systechcorp Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Kolkata, Mumbai",
        "industry": "Software",
        "job_description": "We are inviting applications for the role of Lead Consultant Data Scientist with experience in Artificial Intelligence and Generative Models. We're seeking a talented and experienced professional to join our team and play a key role in developing cutting-edge, AI-powered solutions using large language models.\nKey Responsibilities\nCollaborate with cross-functional teams to identify, analyze, and interpret complex datasets to generate actionable insights.\nDesign, develop, and implement:\nAdvanced statistical models\nMachine learning algorithms\nAI applications and generative models (using LLMs such as GPT-3, BERT, etc.)\nFrameworks like Retrieval-Augmented Generation (RAG), Knowledge Graphs, etc.\nCommunicate insights clearly to both technical and non-technical stakeholders through presentations, reports, and data visualizations.\nMonitor and optimize the performance of AI and generative models on an ongoing basis.\nStay updated with the latest trends, tools, and advancements in data science, AI, and generative modeling.\nMentor and guide junior team members to foster growth and collaboration within the team.\nMinimum Qualifications\nBachelor's or Master's degree in Data Science, Computer Science, Statistics, or a related field.\nProven experience in:\nData science and machine learning\nAI applications and generative AI modeling\nStrong programming skills in Python, R, or equivalent languages.\nHands-on experience with large language models (e.g., GPT-3, BERT) and frameworks such as RAG and Knowledge Graphs.\nExperience working with large datasets using SQL, NoSQL, Hadoop, Spark, etc.\nExcellent problem-solving, analytical, and critical thinking abilities.\nStrong communication and interpersonal skills for cross-functional collaboration.\nPreferred Qualifications\nExperience deploying AI and generative models in production environments on AWS, Azure, or Google Cloud Platform (GCP).\nFamiliarity with domain-specific data sources and challengesespecially in the Insurance industry.\nProven experience leading data science projects end-to-end, with demonstrated project management and team collaboration skills.",
        "skills": [
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Python",
            "Sql",
            "Aws"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Systechcorp Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Hyderabad, Kolkata, Mumbai",
        "industry": "Software",
        "job_description": "We are currently looking for a talented and experienced Data Scientist with a strong background in AI, specifically in building generative AI models using large language models, to join our team.\nThis individual will play a crucial role in developing and implementing data-driven solutions, AI-powered applications, and generative models that will help us stay ahead of the competition and achieve our ambitious goals.\nResponsibilities:\nCollaborate with cross-functional teams to identify, analyze, and interpret complex datasets to develop actionable insights and drive data-driven decision-making.\nDesign, develop, and implement advanced statistical models, machine learning algorithms, AI applications, and generative models using large language models such as GPT-3, BERT and also frameworks like RAG, Knowledge Graphs etc Communicate findings and insights to both technical and non-technical stakeholders through clear and concise presentations, reports, and visualizations.\nContinuously monitor and assess the performance of AI models, generative models, and data-driven solutions, refining and optimizing them as needed.\nStay up-to-date with the latest industry trends, tools, and technologies in data science, AI, and generative models, and apply this knowledge to improve existing solutions and develop new ones.\nMentor and guide junior team members, helping to develop their skills and contribute to their professional growth.\nQualifications we seek in you:\nMinimum Qualifications Bachelors or Masters degree in Data Science, Computer Science, Statistics, or a related field.\nexperience in data science, machine learning, AI applications, and generative AI modeling.\nStrong expertise in Python, R, or other programming languages commonly used in data science and AI, with experience in implementing large language models and generative AI frameworks.\nProficient in statistical modeling, machine learning techniques, AI algorithms, and generative model development using large language models such as GPT-3, BERT, or similar frameworks like RAG, Knowledge Graphs etc\nExperience working with large datasets and using various data storage and processing technologies such as SQL, NoSQL, Hadoop, and Spark.\nStrong analytical, problem-solving, and critical thinking skills, with the ability to draw insights from complex data and develop actionable recommendations.\nExcellent communication and collaboration skills, with the ability to work effectively with cross-functional teams and explain complex concepts to non-technical stakeholders.\nPreferred Qualifications/ skills:\nExperience in deploying AI models, generative models, and applications in a production environment using cloud platforms such as AWS, Azure, or GCP.\nKnowledge of industry-specific data sources, challenges, and opportunities relevant to Insurance\nDemonstrated experience in leading data science projects from inception to completion, including project management and team collaboration skills.",
        "skills": [
            "Project management",
            "model development",
            "Gcp",
            "Machine Learning",
            "Data Science",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist - AI/ML Engineer",
        "company_name": "Mindera",
        "experience": "7-12 Years",
        "salary": null,
        "location": "Remote",
        "industry": "Software",
        "job_description": "Job Description\nWe are seeking a Lead Data Scientist - AI/ML Engineer to join our dynamic team in India. The ideal candidate will have a deep understanding of artificial intelligence and machine learning principles, along with a proven track record of delivering innovative data-driven solutions.\nResponsibilities\nLead the design and development of AI/ML models and algorithms.\nCollaborate with cross-functional teams to identify business opportunities for AI/ML applications.\nMentor junior data scientists and provide guidance on best practices in data science and machine learning.\nConduct research and development to innovate and improve existing AI/ML solutions.\nAnalyze large datasets to derive actionable insights and drive data-driven decision-making.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nSkills and Qualifications\n7-12 years of experience in data science, machine learning, or related fields.\nStrong programming skills in Python, R, or Java.\nProficiency in machine learning frameworks such as TensorFlow, Keras, or PyTorch.\nExperience with data manipulation and analysis libraries like Pandas and NumPy.\nSolid understanding of statistical modeling and data analysis techniques.\nExperience with big data technologies such as Hadoop, Spark, or similar tools.\nFamiliarity with cloud services like AWS, Azure, or Google Cloud for deploying machine learning models.\nExcellent problem-solving skills and ability to work in a fast-paced environment.",
        "skills": [
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python",
            "Sql",
            "Statistical Analysis",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Systechcorp Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Delhi, Bengaluru, Chennai",
        "industry": "Software",
        "job_description": "We are currently looking for a talented and experienced Data Scientist with a strong background in AI, specifically in building generative AI models using large language models, to join our team.\nThis individual will play a crucial role in developing and implementing data-driven solutions, AI-powered applications, and generative models that will help us stay ahead of the competition and achieve our ambitious goals.\nResponsibilities:\nCollaborate with cross-functional teams to identify, analyze, and interpret complex datasets to develop actionable insights and drive data-driven decision-making.\nDesign, develop, and implement advanced statistical models, machine learning algorithms, AI applications, and generative models using large language models such as GPT-3, BERT and also frameworks like RAG, Knowledge Graphs etc Communicate findings and insights to both technical and non-technical stakeholders through clear and concise presentations, reports, and visualizations.\nContinuously monitor and assess the performance of AI models, generative models, and data-driven solutions, refining and optimizing them as needed.\nStay up-to-date with the latest industry trends, tools, and technologies in data science, AI, and generative models, and apply this knowledge to improve existing solutions and develop new ones.\nMentor and guide junior team members, helping to develop their skills and contribute to their professional growth.\nQualifications we seek in you:\nMinimum Qualifications Bachelors or Masters degree in Data Science, Computer Science, Statistics, or a related field.\nexperience in data science, machine learning, AI applications, and generative AI modeling.\nStrong expertise in Python, R, or other programming languages commonly used in data science and AI, with experience in implementing large language models and generative AI frameworks.\nProficient in statistical modeling, machine learning techniques, AI algorithms, and generative model development using large language models such as GPT-3, BERT, or similar frameworks like RAG, Knowledge Graphs etc\nExperience working with large datasets and using various data storage and processing technologies such as SQL, NoSQL, Hadoop, and Spark.\nStrong analytical, problem-solving, and critical thinking skills, with the ability to draw insights from complex data and develop actionable recommendations.\nExcellent communication and collaboration skills, with the ability to work effectively with cross-functional teams and explain complex concepts to non-technical stakeholders.\nPreferred Qualifications/ skills:\nExperience in deploying AI models, generative models, and applications in a production environment using cloud platforms such as AWS, Azure, or GCP.\nKnowledge of industry-specific data sources, challenges, and opportunities relevant to Insurance\nDemonstrated experience in leading data science projects from inception to completion, including project management and team collaboration skills.",
        "skills": [
            "Project management",
            "model development",
            "Gcp",
            "Machine Learning",
            "Data Science",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kanini Software Solutions",
        "experience": "1-6 Years",
        "salary": null,
        "location": "Coimbatore, Pune",
        "industry": "Software Engineering",
        "job_description": "About the Role:\nWe are seeking a highly skilled Senior Data Scientist with expertise in Python, Machine Learning (ML), Natural Language Processing (NLP), Generative AI (GenAI), and Azure Cloud Services. The ideal candidate will be responsible for designing, developing, and deploying advanced AI/ML models to drive data-driven decision-making. This role requires strong analytical skills, proficiency in AI/ML technologies, and experience with cloud-based solutions.\nKey Responsibilities:\nDesign and develop ML, NLP, and GenAI models to solve complex business problems.\nBuild, train, and optimize AI models using Python and relevant ML frameworks.\nImplement Azure AI/ML services for scalable deployment of models.\nDevelop and integrate APIs for real-time model inference and decision-making.\nWork with large-scale data to extract insights and drive strategic initiatives.\nCollaborate with cross-functional teams, including Data Engineers, Software Engineers, and Product Teams, to integrate AI/ML solutions into applications.\nImplement CI/CD pipelines to automate model training, deployment, and monitoring.\nEnsure adherence to software engineering best practices and Agile methodologies in AI/ML projects.\nStay updated on cutting-edge AI/ML advancements and continuously enhance models and algorithms.\nConduct research on emerging AI/ML trends and contribute to the development of innovative solutions.\nProvide technical mentorship and guidance to junior data scientists.\nOptimize model performance and scalability in a production environment.\n---\nRequired Skills & Qualifications:\nProficiency in Python and ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nHands-on experience in NLP techniques, including transformers, embeddings, and text processing.\nExpertise in Generative AI models (GPT, BERT, LLMs, etc.).\nStrong knowledge of Azure AI/ML services, including Azure Machine Learning, Azure Cognitive Services, and Azure Databricks.\nExperience in developing APIs for model deployment and integration.\nFamiliarity with CI/CD pipelines for AI/ML models.\nStrong understanding of software engineering principles and best practices.\nExperience working in an Agile development environment.\nExcellent problem-solving skills and ability to work in a fast-paced, dynamic environment.\nStrong background in statistical analysis, data mining, and data visualization.\nLocation:\nChennai, Coimbatore, Bangalore, Pune\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Natural Language Processing",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kanini Software Solutions",
        "experience": "1-6 Years",
        "salary": null,
        "location": "Chennai, Bengaluru, Noida",
        "industry": "Software Engineering",
        "job_description": "About the Role:\nWe are seeking a highly skilled Senior Data Scientist with expertise in Python, Machine Learning (ML), Natural Language Processing (NLP), Generative AI (GenAI), and Azure Cloud Services. The ideal candidate will be responsible for designing, developing, and deploying advanced AI/ML models to drive data-driven decision-making. This role requires strong analytical skills, proficiency in AI/ML technologies, and experience with cloud-based solutions.\nKey Responsibilities:\nDesign and develop ML, NLP, and GenAI models to solve complex business problems.\nBuild, train, and optimize AI models using Python and relevant ML frameworks.\nImplement Azure AI/ML services for scalable deployment of models.\nDevelop and integrate APIs for real-time model inference and decision-making.\nWork with large-scale data to extract insights and drive strategic initiatives.\nCollaborate with cross-functional teams, including Data Engineers, Software Engineers, and Product Teams, to integrate AI/ML solutions into applications.\nImplement CI/CD pipelines to automate model training, deployment, and monitoring.\nEnsure adherence to software engineering best practices and Agile methodologies in AI/ML projects.\nStay updated on cutting-edge AI/ML advancements and continuously enhance models and algorithms.\nConduct research on emerging AI/ML trends and contribute to the development of innovative solutions.\nProvide technical mentorship and guidance to junior data scientists.\nOptimize model performance and scalability in a production environment.\n---\nRequired Skills & Qualifications:\nProficiency in Python and ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nHands-on experience in NLP techniques, including transformers, embeddings, and text processing.\nExpertise in Generative AI models (GPT, BERT, LLMs, etc.).\nStrong knowledge of Azure AI/ML services, including Azure Machine Learning, Azure Cognitive Services, and Azure Databricks.\nExperience in developing APIs for model deployment and integration.\nFamiliarity with CI/CD pipelines for AI/ML models.\nStrong understanding of software engineering principles and best practices.\nExperience working in an Agile development environment.\nExcellent problem-solving skills and ability to work in a fast-paced, dynamic environment.\nStrong background in statistical analysis, data mining, and data visualization.\nLocation:\nChennai, Coimbatore, Bangalore, Pune\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Natural Language Processing",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kanini Software Solutions",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Chennai, Bengaluru, Pune",
        "industry": "Software Engineering",
        "job_description": "About the Role:\nWe are seeking a highly skilled Senior Data Scientist with expertise in Python, Machine Learning (ML), Natural Language Processing (NLP), Generative AI (GenAI), and Azure Cloud Services. The ideal candidate will be responsible for designing, developing, and deploying advanced AI/ML models to drive data-driven decision-making. This role requires strong analytical skills, proficiency in AI/ML technologies, and experience with cloud-based solutions.\nKey Responsibilities:\nDesign and develop ML, NLP, and GenAI models to solve complex business problems.\nBuild, train, and optimize AI models using Python and relevant ML frameworks.\nImplement Azure AI/ML services for scalable deployment of models.\nDevelop and integrate APIs for real-time model inference and decision-making.\nWork with large-scale data to extract insights and drive strategic initiatives.\nCollaborate with cross-functional teams, including Data Engineers, Software Engineers, and Product Teams, to integrate AI/ML solutions into applications.\nImplement CI/CD pipelines to automate model training, deployment, and monitoring.\nEnsure adherence to software engineering best practices and Agile methodologies in AI/ML projects.\nStay updated on cutting-edge AI/ML advancements and continuously enhance models and algorithms.\nConduct research on emerging AI/ML trends and contribute to the development of innovative solutions.\nProvide technical mentorship and guidance to junior data scientists.\nOptimize model performance and scalability in a production environment.\n---\nRequired Skills & Qualifications:\nProficiency in Python and ML frameworks like TensorFlow, PyTorch, or Scikit-learn.\nHands-on experience in NLP techniques, including transformers, embeddings, and text processing.\nExpertise in Generative AI models (GPT, BERT, LLMs, etc.).\nStrong knowledge of Azure AI/ML services, including Azure Machine Learning, Azure Cognitive Services, and Azure Databricks.\nExperience in developing APIs for model deployment and integration.\nFamiliarity with CI/CD pipelines for AI/ML models.\nStrong understanding of software engineering principles and best practices.\nExperience working in an Agile development environment.\nExcellent problem-solving skills and ability to work in a fast-paced, dynamic environment.\nStrong background in statistical analysis, data mining, and data visualization.\nLocation:\nChennai, Coimbatore, Bangalore, Pune\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Natural Language Processing",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Expert Data Scientist",
        "company_name": "Ciklum",
        "experience": "1-2 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "About the Role\nAs an Expert Data Scientist, you'll become a key member of a cross-functional development team engineering the experiences of tomorrow. You'll develop advanced models, derive actionable insights from complex datasets, and work closely with stakeholders to create meaningful business and healthcare outcomes.\nResponsibilities\nDevelop prototype solutions, mathematical models, algorithms, machine learning techniques, and robust analytics to support insight generation and data visualization.\nConduct exploratory data analysis to uncover trends, patterns, and high-level insights.\nProvide optimization recommendations to support KPIs across product, marketing, operations, PR, and other business units.\nCollaborate with engineering teams to ensure developed solutions meet standards for functionality, scalability, performance, and reliability.\nWork with business analysts and data engineers to understand their use cases and support implementation.\nIdentify opportunities for leveraging data to solve business problems and improve outcomes.\nDrive innovation by researching and applying new methods, tools, and statistical techniques to improve decision-making processes.\nMentor teammates and promote knowledge sharing across the team.\nParticipate in community-building activities, internal knowledge exchange, and conferences.\nSupport marketing and sales teams through technical input, content creation, and customer meetings.\nRequirements\nGeneral Technical Requirements\nBSc, MSc, or PhD in Mathematics, Statistics, Computer Science, Engineering, Operations Research, Econometrics, or related fields.\nStrong knowledge of probability theory, statistics, and the mathematics behind machine learning.\nExperience using CRISP-ML(Q) or TDSP methodologies to solve business problems.\nProficient in machine learning techniques including:\nRegression\nClassification\nClustering\nDimensionality reduction\nStrong proficiency in Python for modeling and statistical analysis.\nSkilled in data visualization with libraries such as Matplotlib, Seaborn, or Plotly.\nSpecific Technical Skills\nAdvanced SQL skills for data manipulation, sampling, and reporting.\nExperience with:\nImbalanced datasets\nTime series data (preprocessing, feature engineering, forecasting)\nOutlier and anomaly detection\nHandling various data types (text, image, video)\nFamiliarity with cloud-based ML services: AWS SageMaker, Azure ML, or Google AI Platform.\nDomain Experience (Healthcare)\nAnalyzing medical signals and images.\nPredictive modeling for outcomes, disease progression, readmissions, and population health risks.\nNLP/text mining on clinical notes, medical literature, or patient-reported data.\nExperience with survival analysis and time-to-event modeling.\nDesigning and analyzing clinical trials or research studies.\nCausal inference methods (e.g., propensity score matching, instrumental variable techniques).\nKnowledge of healthcare regulations like HIPAA, GDPR, and FDA compliance.\nSecure handling of healthcare data, including de-identification and patient consent.\nFamiliarity with federated learning and decentralized models.\nUnderstanding of healthcare interoperability standards: HL7, SNOMED, FHIR, DICOM.\nAbility to work with clinicians, researchers, and policymakers to extract actionable insights.\nGood to Have Skills\nMLOps experience: integrating ML into production, using Docker, Kubernetes.\nExperience in deep learning with TensorFlow or PyTorch.\nKnowledge of LLMs and Generative AI.\nExperience with MS SQL Server, PostgreSQL, Databricks, Snowflake.\nFamiliarity with Big Data technologies (e.g., Hadoop, Apache Spark).\nExperience with NoSQL databases (e.g., Cassandra, Neo4j).\nBusiness-Related Requirements\nDemonstrated success in delivering data science projects that drive measurable business impact.\nAbility to translate business problems into data science use cases and execute them end-to-end.\nExcellent project and time management skills.\nStrong communication and storytelling skills for conveying complex technical concepts to stakeholders.\nDesirable\nPublished research or peer-reviewed journal articles.\nRecognized achievements in data science competitions (e.g., Kaggle).\nCertifications in cloud-based ML platforms (AWS, Azure, GCP).",
        "skills": [
            "Plotly",
            "AWS SageMaker",
            "Matplotlib",
            "Azure ML",
            "Seaborn",
            "Data Visualization",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Innovaccer Analytics Private Limited",
        "experience": "7-12 Years",
        "salary": null,
        "location": "Noida, Delhi NCR",
        "industry": "Health Care",
        "job_description": "Design and lead the development of various artificial intelligence initiatives to help improve health and we'llness of patients\nWork with the business leaders and customers to understand their pain-points and build large-scale solutions for them.\nDefine technical architecture to productize Innovaccer s machine-learning algorithms and take them to market with partnerships with different organizations\nProven ability to break down complex business problems into machine learning problems and design solution workflows.\nWork with our data platform and applications team to help them successfully integrate the data science capability or algorithms in their product/workflows.\nWork with development teams to build tools for repeatable data tasks that will accelerate and automate development cycle.\nDefine and execute on the quarterly roadmap\nWhat You Need\nMasters in Computer Science, Computer Engineering or other relevant fields (PhD Preferred)\n7+ years of experience in Data Science (healthcare experience will be a plus)\nStrong written and spoken communication skills\nStrong hands-on experience in Python - building enterprise applications alongwith optimization techniques.\nStrong experience with deep learning techniques to build NLP/Computer vision models as we'll as state of art GenAI pipelines - knowledge of implementing agentic workflows is a plus.\nHas demonstrable experience deploying deep learning models in production at scale with interactive improvements- would require hands-on expertise with at least 1 deep learning frameworks like Pytorch or Tensorflow.\nHas keen interest in research and stays updated with key advancements in the area of AI and ML in the industry.\nDeep understanding of classical ML techniques - Random Forests, SVM, Boosting, Bagging - and building training and evaluation pipelines.\nDemonstrate experience with global and local model explainability using LIME, SHAP and associated techniques.\nHands on experience with at least one ML platform among Databricks, Azure ML, Sagemaker s\nExperience in developing and deploying production ready models\nKnowledge of implementing an MLOps framework.\nPossess a customer-focused attitude through conversations and documentation",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Artificial Intelligence, Machine Learning)",
        "company_name": "Franklin Templeton",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Financial Services",
        "job_description": "What are the ongoing responsibilities of a Data ScientistData Collection and Preprocessing:\nImplement data collection strategies to gather relevant data from different sources.\nClean, preprocess, and validate data to ensure accuracy and reliability for subsequent analysis.\nAssist in maintaining and enhancing data pipelines in collaboration with data engineering teams.\nStatistical Analysis:\nPerform exploratory data analysis to identify trends, patterns, and insights within datasets.\nApply basic statistical techniques to test hypotheses, validate assumptions, and draw conclusions from data.\nMachine Learning and AI Model Development:\nDevelop and optimize machine learning models to address business problems and improve processes.\nExplore and apply Generative AI techniques under the supervision of senior team members to contribute to innovative solutions.\nAssist in model evaluation, validation, and deployment, monitoring performance and making adjustments as needed.\nUnderstanding Human Behavior for AI Applications:\nAnalyze data related to human behavior to help develop models that predict and influence outcomes.\nWork with domain experts to incorporate insights into AI models, enhancing their relevance and accuracy.\nData Engineering Collaboration:\nCollaborate with data engineering teams to ensure smooth integration of models into existing data systems.\nContribute to designing and implementing scalable data storage solutions.\nCross-functional Collaboration:\nWork with product management, marketing, and business stakeholders to understand requirements and provide data-driven insights.\nCommunicate analytical concepts and insights effectively to non-technical stakeholders through reports and visualizations.\nContinuous Learning and Innovation:\nStay updated on the latest developments in data science, machine learning, and AI technologies.\nExperiment with new methodologies and tools to enhance project outcomes and expand your skill set.\nWhat ideal qualifications, skills & experience would help someone to be Successful\nMaster s or Bachelor s degree in Statistics, Mathematics, Econometrics, Computer Science, Engineering, or related disciplines\n2-4 years of experience in data science, predictive modeling, and machine learning.\nProficiency in Python, R, or SQL, with hands-on experience in data analysis and model development.\nBasic knowledge of Generative AI models and their applications\nAbility to translate business problems into analytical tasks.\nSkill in explaining statistical and machine learning techniques to business partners.\nExperience in creating data-driven stories and insights.\nProficiency in handling large datasets, including data cleansing, manipulation, and mining.\nCapability to tackle ambiguous challenges with a problem-solving mindset.\nCuriosity and willingness to learn independently.\nStrong written and verbal communication skills.\nEffective organizational and planning abilities\nAbility to work well under pressure and adapt in a dynamic environment.\nTeam-oriented approach with a capacity to work independently when needed.\nStrong interpersonal skills and the ability to build relationships with colleagues and stakeholders",
        "skills": [
            "Data Analysis",
            "R",
            "Data Science",
            "Machine Learning",
            "Artificial Intelligence",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Manager - Data Scientist",
        "company_name": "Innovaccer Analytics Private Limited",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Noida, Delhi NCR",
        "industry": "Health Care",
        "job_description": "Data is the foundation of our innovation.\nWe are seeking a Manager, Data Science with expertise in NLP and Generative AI to lead the development of cutting-edge AI-driven solutions in healthcare.\nThis role requires a deep understanding of healthcare data and the ability to design and implement advanced language models that extract insights, automate workflows, and enhance clinical decision-making.\nWe're looking for a visionary leader who can define and build the next generation of AI-driven tools, leveraging LLMs, deep learning, and predictive analytics to personalize care based on patients clinical and behavioral history.\nIf you're passionate about pushing the boundaries of AI in healthcare, we'd love to hear from you!\nA Day in the Life\nTeam Leadership & Development: Build, mentor, and manage a team of data scientists, and machine learning engineers. Foster a culture of collaboration, innovation, and technical excellence.\nRoadmap Execution: Define and execute on the quarterly AI/ML roadmap, setting clear goals, priorities, and deliverables for the team.\nWork with the business leaders and customers to understand their pain-points and build large-scale solutions for them.\nDefine technical architecture to productize Innovaccer s machine-learning algorithms and take them to market with partnerships with different organization.\nWork with our data platform and applications team to help them successfully integrate the data science capability or algorithms in their product/workflows.\nProject & Stakeholder Management: Work closely with cross-functional teams, including product managers, engineers, and business leaders, to align AI/ML initiatives with company objectives.\nWhat You Need\nMasters in Computer Science, Computer Engineering or other relevant fields (PhD Preferred)\n8+ years of experience in Data Science (healthcare experience will be a plus)\nStrong experience with deep learning techniques to build NLP/Computer vision models as well as state of art GenAI pipelines - Has demonstrable experience deploying deep\nlearning models in production at scale with interactive improvements- would require\nhands-on expertise with at least 1 deep learning frameworks like Pytorch or Tensorflow.\nStrong hands-on experience in building GenAI applications - building LLM based workflows along with optimization techniques - knowledge of implementing agentic\nworkflows is a plus.\nHas keen interest in research and stays updated with key advancements in the area of AI and ML in the industry. Having patents/publications in any area of AI/ML is a great add on.\nHands on experience with at least one ML platform among Databricks, Azure ML, Sagemaker s\nStrong written and spoken communication skills",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Python",
            "Data Visualization",
            "Big Data"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Innovaccer Analytics Private Limited",
        "experience": "7-12 Years",
        "salary": null,
        "location": "Noida, Delhi NCR",
        "industry": "Health Care",
        "job_description": "Design and lead the development of various artificial intelligence initiatives to help improve health and we'llness of patients\nWork with the business leaders and customers to understand their pain-points and build large-scale solutions for them.\nDefine technical architecture to productize Innovaccer s machine-learning algorithms and take them to market with partnerships with different organizations\nProven ability to break down complex business problems into machine learning problems and design solution workflows.\nWork with our data platform and applications team to help them successfully integrate the data science capability or algorithms in their product/workflows.\nWork with development teams to build tools for repeatable data tasks that will accelerate and automate development cycle.\nDefine and execute on the quarterly roadmap\nWhat You Need\nMasters in Computer Science, Computer Engineering or other relevant fields (PhD Preferred)\n7+ years of experience in Data Science (healthcare experience will be a plus)\nStrong written and spoken communication skills\nStrong hands-on experience in Python - building enterprise applications alongwith optimization techniques.\nStrong experience with deep learning techniques to build NLP/Computer vision models as we'll as state of art GenAI pipelines - knowledge of implementing agentic workflows is a plus.\nHas demonstrable experience deploying deep learning models in production at scale with interactive improvements- would require hands-on expertise with at least 1 deep learning frameworks like Pytorch or Tensorflow.\nHas keen interest in research and stays updated with key advancements in the area of AI and ML in the industry.\nDeep understanding of classical ML techniques - Random Forests, SVM, Boosting, Bagging - and building training and evaluation pipelines.\nDemonstrate experience with global and local model explainability using LIME, SHAP and associated techniques.\nHands on experience with at least one ML platform among Databricks, Azure ML, Sagemaker s\nExperience in developing and deploying production ready models\nKnowledge of implementing an MLOps framework.\nPossess a customer-focused attitude through conversations and documentation",
        "skills": [
            "Data Analysis",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Python",
            "Data Visualization",
            "Big Data"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PepsiCo",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Food Processing & Packaged Food",
        "job_description": "Overview\n\nWe are PepsiCo\nPepsiCo is one of the world's leading food and beverage companies with more than $79 Billion in Net Revenue and a global portfolio of diverse and beloved brands. We have a complementary food and beverage portfolio that includes 22 brands that each generate more than $1 Billion in annual retail sales. PepsiCo's products are sold in more than 200 countries and territories around the world. PepsiCo's strength is its people. We are over 250,000 game changers, mountain movers and history makers, located around the world, and united by a shared set of values and goals.\nWe believe that acting ethically and responsibly is not only the right thing to do, but also the right thing to do for our business. At PepsiCo, we aim to deliver top-tier financial performance over the long term by integrating sustainability into our business strategy, leaving a positive imprint on society and the environment. We call thisWinning with Pep+ Positive. For more information on PepsiCo and the opportunities it holds, visit.\nPepsiCo Data Analytics & AI Overview:\nWith data deeply embedded in our DNA, PepsiCo Data, Analytics and AI (DA&AI) transforms data into consumer delight. We build and organize business-ready data that allows PepsiCo's leaders to solve their problems with the highest degree of confidence. Our platform of data products and services ensures data is activated at scale. This enables new revenue streams, deeper partner relationships, new consumer experiences, and innovation across the enterprise.\nThe Data Science Pillar in DA&AI will be the organization where Data Scientist and ML Engineers report to in the broader D+A Organization. Also DS will lead, facilitate and collaborate on the larger DS community in PepsiCo. DS will provide the talent for the development and support of DS component and its life cycle within DA&AI Products. And will support pre-engagement activities as requested and validated by the prioritization framework of DA&AI.\nData Scientist-Gurugram and Hyderabad\nThe role will work in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Machine Learning Services and Pipelines.\n\nResponsibilities\n\nDelivery of key Advanced Analytics/Data Science projects within time and budget, particularly around DevOps/MLOps and Machine Learning models in scope\nCollaborate with data engineers and ML engineers to understand data and models and leverage various advanced analytics capabilities\nEnsure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards\nUse big data technologies to help process data and build scaled data pipelines (batch to real time)\nAutomate the end-to-end ML lifecycle with Azure Machine Learning and Azure/AWS/GCP Pipelines.\nSetup cloud alerts, monitors, dashboards, and logging and troubleshoot machine learning infrastructure\nAutomate ML models deployments\n\nQualifications\n\nMinimum 3years of hands-on work experience in data science / Machine learning\nMinimum 3year of SQL experience\nExperience in DevOps and Machine Learning (ML) with hands-on experience with one or more cloud service providers\nBE/BS in Computer Science, Math, Physics, or other technical fields.\nDataScience - Hands on experience and strong knowledge of building machine learning models - supervised and unsupervised models\nProgramming Skills - Hands-on experience in statistical programming languages likePythonand database query languages like SQL\nStatistics - Good applied statistical skills, including knowledge of statistical tests, distributions, regression, maximum likelihood estimators\nAny Cloud - Experience in Databricks and ADF is desirable\nFamiliarity with Spark, Hive, Pig is an added advantage\nModel deployment experience will be a plus\nExperience with version control systems like GitHub and CI/CD tools\nExperience in Exploratory data Analysis\nKnowledge of ML Ops / DevOps and deploying ML models is required\nExperience using MLFlow, Kubeflow etc. will be preferred\nExperience executing and contributing to ML OPS automation infrastructure is good to have\nExceptional analytical and problem-solving skills",
        "skills": [
            "Kubeflow",
            "MLFlow",
            "ML Ops",
            "AWS",
            "Databricks",
            "Sql",
            "Pig",
            "Github",
            "Devops",
            "Azure Machine Learning",
            "Machine Learning",
            "Artificial Intelligence",
            "Hive",
            "Python",
            "Gcp",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist 1",
        "company_name": "PayPal",
        "experience": "3-6 Years",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWe are seeking a highly skilled Credit Risk Strategies Specialist to join our team at PayPal and Venmo. The ideal candidate will be responsible for developing, implementing, and managing credit risk strategies across account life cycle, including Acquisition underwriting/line assignment/pricing and Portfolio credit line increase/credit line decrease/authorization/account closure.\nJob Description:\nDevelop and implement credit risk strategies for various products, including PayPal and Venmo credit card programs.\nMake sound decisions that balance risk and reward, user experience and financial performance.\nEnsure strategies are reviewed and approved within the approved governance committee structures.\nPartner with operational teams to implement credit risk strategies. Perform operational monitoring to ensure that the strategy executions align perfectly with intents.\nDevelop and automate tracking functionality that monitors volume and strategy performance. Provide PayPal and Venmo Card program insights via monitoring and reporting leveraging internal/external data.\nConduct ongoing performance reviews with senior leadership team.\nTest new data sources to enhance credit risk decisions.\nPreferred Qualification:\nQualifications:\nMaster's degree with 3-6 years professional experiences in related fields. Job level may be adjusted to match the experiences of candidates.\nData driven mindset with degree in a quantitative discipline such as Data Science, Statistics, Mathematics, Analytics.\nProven experience in employing mathematical, statistical and data mining methods on real world problems\nProficient in data mining and familiar with SQL, BigQuery, SAS, Python, Tableau and other big data tools\nMust be an intuitive, organized analytical thinker, with the ability to drive analysis end to end\nIndustry experience in Consumer Lending is a plus.\nExceptional written and verbal communication skills to influence cross-functional teams.\nSubsidiary:\nPayPal\nTravel Percent:\n0\n-\nPayPal is committed to fair and equitable compensation practices.\nActual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.\nThe total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit .\nThe US national annual pay range for this role is $143,500 to $212,850\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": [
            "Python",
            "Sql",
            "BigQuery",
            "data mining",
            "SAS",
            "Tableau"
        ]
    },
    {
        "job_title": "Data Scientist 1",
        "company_name": "PayPal",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWhat you need to know about the role\nWe are looking for a Senior Risk Analyst or Senior Decision Scientist with experience in credit cards, seller risk, or deposit risk developing acquisition or portfolio management risk strategies to balance user experience and risk (fraud, abuse, and settlement). The position is part of a larger Small Medium Business (SMB) Seller Risk team that manages both Seller Risk and different payment solutions both physical and digital. Qualified individual exhibit ability to think creatively while balancing multiple constraints to execution on the team objective.\n\nMeet Your Team\nPayPal's SMB payment product solutions Risk team is responsible for assessing and managing buyer and seller side financial risk exposures which includes identity theft, stolen financials, account takeover, and merchant fraud risk. To manage these risks, we leverage analytics that drives insights into policies and strategies that we implement and continuously test for optimization that balances risk and rewards. The team is also responsible for partnering with the corresponding Business Units to align with and influence their strategic priorities and product roadmap, educate business partners about Risk management principles, and collaboratively optimize the Risk treatments and experiences for these unique products and partners.\nJob Description:\n\nYour way to impact\nThis role will be part of a team that focuses on managing Zettle, PayPal's Point of Sales solutions. This team manages the risk lifecycle from onboarding to transactions. The individual is expected to lead and execute the daily management of this product to balance authorization rate to optimize user experience while mitigating losses within defined risk tolerance. .\nThe work is fast paced and intellectually challenges to solve complex business problems that cover various markets across the world. If you're interested in working with PayPal's most interesting payments experiences, then this is the right team to join!\nYour day-to-day\nDeveloping strategy through analytics that include understanding of the swap-in and swap-out trade-offs in losses and end user experience.\nPresentation to senior management within Risk and Business Unit on core strategy changes\nMonitoring and management fraud events\nWorking with both Risk Product and Business Product organizations on risk control requirements from feature development for strategy implementation to platform orchestration on step-up authentication\nPartnering with data scientist to develop statistical models as part of tools kits in managing different risks\nWork is generally self-directed.\nWorks on assignments that are of intermediate complexity with multiple steps in execution, and guided by generally defined processes and project requirements\nManaging approval rate, complaint rate, and loss target.\nWhat Do You Need To Bring\nStrong analytical skills -- ability to build quick estimates using back-of-the-envelope analysis, structure (and, if needed, execute) more complex analyses, pull together business cases with financial support to navigate through multi-dimensional sets of tradeoffs.\nEnthusiasm for data-driven problem solving within a fast-paced environment is a must. In addition, experience with statistical optimization software, working knowledge of SQL or other relational database languages, and hands-on experience in data analysis involving large data sets are strongly desired.\nPolished communication and influence skills to collaborate cross-functionally across different global markets with product managers, data scientists, business owners, and customers to learn from various subject-matter experts. Presenting in a clear and concise manner and reach alignment on how to execute risk strategies.\nDemonstrated ability to influence groups and effectivelyfinding solutions that enable business growth while balancing risk controls.\nAn innate intellectual curiosity, and a willingness to build awareness of current payments industry and risk management best practices. PayPal is constantly innovating by introducing new products and entering new markets, so successful risk analysts on this team must quickly get up to speed on new content areas. You will be expected to become an expert in your specific domain\nCan-do attitude, team player, energetic personality, ability to work well under pressure in a fast-paced and constantly changing environment to meet deadlines. The successful risk analyst is a self-starter who has the resilience to learn from their mistakes and reach their true potential.\nIdentify glitches in processes and tools and develop and execute solutions to overcome general issues and obstacles with little supervision.\nLearning in-depth analysis of alternatives and applying specialized knowledge\nImpact of decision has moderate reach\nSeeks improvement within defined tasks. Understands, evaluates, and executes improvement ideas from supervisors\nBS/BA degree with 5+ years of experience or master's degree with 3+ years of experience.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at.\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": [
            "Credit Cards",
            "Analytics",
            "risk control requirements",
            "statistical models",
            "portfolio management risk strategies",
            "Data Analysis",
            "deposit risk",
            "seller risk",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Cadbury",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Remote, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\nAre You Ready to Make It Happen at Mondelz International\nJoin our Mission to Lead the Future of Snacking. Make It With Pride.\nYou will help our businessreact to changing businessconditionswith excellent data analysis. You will define the requirements, perform the analysis and identify patterns in large amounts of data.\nHow you will contribute\nYou will:\nDefine requirements for analysis in a given business area and perform detailed analysis and identify trends defined in the requirements\nIdentify patterns and help the business react to changing business conditions\nPerform root-cause analysis and interpret data\nWork with large amounts of data such as facts, figures, and mathematics/formulas and undertake analytical activities and delivers analysis outputs in accordance with customer needs andconforming to established standards\nUnderstand and be involved with aspects of the data science process\nAs the investigator on the data science team, you will apply your knowledge of languages and pull data out of SQL databases, using Tableau/Power BI type tools, and producing basic datavisualizations and reporting dashboards\nWhat you will bring\nA desire to drive your future and accelerate your career and the following experience and knowledge:\nKnowledge of Microsoft Excel, Power BI, SQL,R, Python, SAS software, Google Analytics, Google Tag Manager, Tableau, GoogleAdWords, statistical software\nModerate knowledge in math and statistical skills, strong business acumen, moderate computer science/coding skills\nDevelop key performance indicators and create visualizations of the data\nUtilize business intelligence and analytics tool\nKey responsibilities:\nGenerate monthly forecast for the markets in scope as per IBP/Achilles calendar\nPerform forecast validation checks and handover stats forecast to market\nDriver CI activities including Experimentation, automation to increase value of the product to improve SFA, FVA and efficiency of forecast generation process\nDriver market monthly RCA call to drive actionable insights in collaboration with Demand Planners to improve SFA and drive No Touch Adoption\nParticipate in Monthly BU leadership meetings and drive agenda for you markets in scope\nDeliver zero defect mindset in collaboration with Data Engineering and MDS D&A Team\nBuild up Statistics SFA target achievement strategy in coordination with Product Owner & Demand Planner\nDocumentation of modelling rules, experimentation for markets in scope\nLead data discovery sessions from data science, guide MDS D&A and DE team members for o9 modelling needs and design validation rules for source data.\nParticipate in data discovery sessions from data science, in collaboration with MDS D&A and DE team members for o9 modelling needs and design validation rules for source data.\nJob specific requirements:\nExperience in any Data Science language like Python (preferred), PySpark (preferred), R\nExperience with Cloud solutions (GCP) and deployment of data science models in production environment.\nKnowledge and experience in data science cloud platforms like Databricks (preferred) or similar.\nKnowledge of analytical techniques in statistical modelling, machine learning with exposure to forecasting domain especially driver-based forecasting.\nBasic understanding of supply chain processes.\nShould be able to articulate data science outcome into business understandable language.\nExperience developing and deploying high standard Statistical and/or machine learning solution.\n4+ years of experience in data science or analytics roles.\nWithin Country Relocation support available and for candidates voluntarily moving internationally some minimal support is offered through our Volunteer International Transfer Policy\nBusiness Unit Summary\nHeadquartered in Singapore, Mondelz International's Asia, Middle East and Africa (AMEA) region is comprised of six business units, has more than 21,000 employees and operates in more than 27 countries including Australia, China, Indonesia, Ghana, India, Japan, Malaysia, New Zealand, Nigeria, Philippines, Saudi Arabia, South Africa, Thailand, United Arab Emirates and Vietnam. Seventy-six nationalities work across a network of more than 35 manufacturing plants, three global research and development technical centers and in offices stretching from Auckland, New Zealand to Casablanca, Morocco. Mondelz International in the AMEA region is the proud maker of global and local iconic brands such as and biscuits, mooncakes, and chocolate, candy, gum, powdered beverage and cheese. We are also proud to be named a Top Employer in many of our markets.\nMondelz International is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation or preference, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.\nJob Type\nRegular\nData Science\nAnalytics & Data Science",
        "skills": [
            "R",
            "Tableau",
            "Databricks",
            "Sql",
            "Power Bi",
            "Statistical Modelling",
            "Google Tag Manager",
            "Python",
            "Google Analytics",
            "SAS",
            "Gcp",
            "Machine Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PepsiCo",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Food Processing & Packaged Food",
        "job_description": "Overview\n\nWe are PepsiCo\nPepsiCo is one of the world's leading food and beverage companies with more than $79 Billion in Net Revenue and a global portfolio of diverse and beloved brands. We have a complementary food and beverage portfolio that includes 22 brands that each generate more than $1 Billion in annual retail sales. PepsiCo's products are sold in more than 200 countries and territories around the world. PepsiCo's strength is its people. We are over 250,000 game changers, mountain movers and history makers, located around the world, and united by a shared set of values and goals.\nWe believe that acting ethically and responsibly is not only the right thing to do, but also the right thing to do for our business. At PepsiCo, we aim to deliver top-tier financial performance over the long term by integrating sustainability into our business strategy, leaving a positive imprint on society and the environment. We call thisWinning with Pep+ Positive. For more information on PepsiCo and the opportunities it holds, visit.\nPepsiCo Data Analytics & AI Overview:\nWith data deeply embedded in our DNA, PepsiCo Data, Analytics and AI (DA&AI) transforms data into consumer delight. We build and organize business-ready data that allows PepsiCo's leaders to solve their problems with the highest degree of confidence. Our platform of data products and services ensures data is activated at scale. This enables new revenue streams, deeper partner relationships, new consumer experiences, and innovation across the enterprise.\nThe Data Science Pillar in DA&AI will be the organization where Data Scientist and ML Engineers report to in the broader D+A Organization. Also DS will lead, facilitate and collaborate on the larger DS community in PepsiCo. DS will provide the talent for the development and support of DS component and its life cycle within DA&AI Products. And will support pre-engagement activities as requested and validated by the prioritization framework of DA&AI.\nData Scientist-Gurugram and Hyderabad\nThe role will work in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Machine Learning Services and Pipelines.\n\nResponsibilities\n\nDelivery of key Advanced Analytics/Data Science projects within time and budget, particularly around DevOps/MLOps and Machine Learning models in scope\nCollaborate with data engineers and ML engineers to understand data and models and leverage various advanced analytics capabilities\nEnsure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards\nUse big data technologies to help process data and build scaled data pipelines (batch to real time)\nAutomate the end-to-end ML lifecycle with Azure Machine Learning and Azure/AWS/GCP Pipelines.\nSetup cloud alerts, monitors, dashboards, and logging and troubleshoot machine learning infrastructure\nAutomate ML models deployments\n\nQualifications\n\nMinimum 3years of hands-on work experience in data science / Machine learning\nMinimum 3year of SQL experience\nExperience in DevOps and Machine Learning (ML) with hands-on experience with one or more cloud service providers\nBE/BS in Computer Science, Math, Physics, or other technical fields.\nDataScience - Hands on experience and strong knowledge of building machine learning models - supervised and unsupervised models\nProgramming Skills - Hands-on experience in statistical programming languages likePythonand database query languages like SQL\nStatistics - Good applied statistical skills, including knowledge of statistical tests, distributions, regression, maximum likelihood estimators\nAny Cloud - Experience in Databricks and ADF is desirable\nFamiliarity with Spark, Hive, Pig is an added advantage\nModel deployment experience will be a plus\nExperience with version control systems like GitHub and CI/CD tools\nExperience in Exploratory data Analysis\nKnowledge of ML Ops / DevOps and deploying ML models is required\nExperience using MLFlow, Kubeflow etc. will be preferred\nExperience executing and contributing to ML OPS automation infrastructure is good to have\nExceptional analytical and problem-solving skills",
        "skills": [
            "Kubeflow",
            "MLFlow",
            "ML Ops",
            "AWS",
            "Databricks",
            "Sql",
            "Pig",
            "Github",
            "Devops",
            "Azure Machine Learning",
            "Machine Learning",
            "Artificial Intelligence",
            "Hive",
            "Python",
            "Gcp",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PepsiCo",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Food Processing & Packaged Food",
        "job_description": "Overview\n\nWe are PepsiCo\nPepsiCo is one of the world's leading food and beverage companies with more than $79 Billion in Net Revenue and a global portfolio of diverse and beloved brands. We have a complementary food and beverage portfolio that includes 22 brands that each generate more than $1 Billion in annual retail sales. PepsiCo's products are sold in more than 200 countries and territories around the world. PepsiCo's strength is its people. We are over 250,000 game changers, mountain movers and history makers, located around the world, and united by a shared set of values and goals.\nWe believe that acting ethically and responsibly is not only the right thing to do, but also the right thing to do for our business. At PepsiCo, we aim to deliver top-tier financial performance over the long term by integrating sustainability into our business strategy, leaving a positive imprint on society and the environment. We call thisWinning with Pep+ Positive. For more information on PepsiCo and the opportunities it holds, visit.\nPepsiCo Data Analytics & AI Overview:\nWith data deeply embedded in our DNA, PepsiCo Data, Analytics and AI (DA&AI) transforms data into consumer delight. We build and organize business-ready data that allows PepsiCo's leaders to solve their problems with the highest degree of confidence. Our platform of data products and services ensures data is activated at scale. This enables new revenue streams, deeper partner relationships, new consumer experiences, and innovation across the enterprise.\nThe Data Science Pillar in DA&AI will be the organization where Data Scientist and ML Engineers report to in the broader D+A Organization. Also DS will lead, facilitate and collaborate on the larger DS community in PepsiCo. DS will provide the talent for the development and support of DS component and its life cycle within DA&AI Products. And will support pre-engagement activities as requested and validated by the prioritization framework of DA&AI.\nData Scientist-Gurugram and Hyderabad\nThe role will work in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Machine Learning Services and Pipelines.\n\nResponsibilities\n\nDelivery of key Advanced Analytics/Data Science projects within time and budget, particularly around DevOps/MLOps and Machine Learning models in scope\nCollaborate with data engineers and ML engineers to understand data and models and leverage various advanced analytics capabilities\nEnsure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards\nUse big data technologies to help process data and build scaled data pipelines (batch to real time)\nAutomate the end-to-end ML lifecycle with Azure Machine Learning and Azure/AWS/GCP Pipelines.\nSetup cloud alerts, monitors, dashboards, and logging and troubleshoot machine learning infrastructure\nAutomate ML models deployments\n\nQualifications\n\nMinimum 3years of hands-on work experience in data science / Machine learning\nMinimum 3year of SQL experience\nExperience in DevOps and Machine Learning (ML) with hands-on experience with one or more cloud service providers.\nBE/BS in Computer Science, Math, Physics, or other technical fields.\nDataScience - Hands on experience and strong knowledge of building machine learning models - supervised and unsupervised models\nProgramming Skills - Hands-on experience in statistical programming languages likePythonand database query languages like SQL\nStatistics - Good applied statistical skills, including knowledge of statistical tests, distributions, regression, maximum likelihood estimators\nAny Cloud - Experience in Databricks and ADF is desirable\nFamiliarity with Spark, Hive, Pig is an added advantage\nModel deployment experience will be a plus\nExperience with version control systems like GitHub and CI/CD tools\nExperience in Exploratory data Analysis\nKnowledge of ML Ops / DevOps and deploying ML models is required\nExperience using MLFlow, Kubeflow etc. will be preferred\nExperience executing and contributing to ML OPS automation infrastructure is good to have\nExceptional analytical and problem-solving skills",
        "skills": [
            "Kubeflow",
            "CI CD",
            "MLFlow",
            "ML Ops",
            "Spark",
            "Pig",
            "Sql",
            "Databricks",
            "Github",
            "Devops",
            "Azure Machine Learning",
            "Machine Learning",
            "AWS",
            "Artificial Intelligence",
            "Hive",
            "Python",
            "Gcp"
        ]
    },
    {
        "job_title": "!! Urgent hiring for Data Scientist in Australia and Canada !!",
        "company_name": "Fanishwar Kumar (Proprietor of Sparkup solutions)",
        "experience": "2-10 Years",
        "salary": "INR 35 - 45 LPA ",
        "location": "Canada, Australia",
        "industry": "Login to check your skill match score",
        "job_description": "Job highlights\nAt least a Bachelor / Diploma\nMin 2 Years of Experience\nJob match score\nEarly Applicant\nKey skills\nLocation\nWork Experience\nJob description\nCall for more details: +91 8527364486\nData scientists use advanced analytics technologies, including machine learning and predictive modelling, to support the identification of trends, scrape information from unstructured data sources and provide automated recommendations. They are employed by consulting firms, universities, banks and information technology departments in the private and public sectors.\nImplement cutting-edge techniques and tools in machine learning, deep learning and artificial intelligence to make data analysis more efficient\nPerform large-scale experimentation to identify hidden relationships between variables in large datasets\nCreate advanced machine learning algorithms such as regression, simulation, scenario analysis, modeling, clustering, decision trees and , neural networks\nPrepare and extract data using programming language\nImplement new statistical, machine learning, or other mathematical methodologies to solve specific business problems\nVisualize data in a way that allows a business to quickly draw conclusions and make decisions\nDevelop artificial intelligence models and algorithms and implement them to meet the needs of the organization\nCoordinate research and analysis activities using unstructured and structured data and use programming to clean and organize data.\nKey Requirements:\nPossess a solid foundation in statistics and practical experience with statistical software (such as Excel, SPSS, SAS) and mastery in data analysis languages including SQL, Python, and R.\nExhibit exceptional analytical abilities to compile, structure, examine, and present substantial data sets with precision and thoroughness.\nCapable of critically evaluating data to derive meaningful, actionable insights.\nDemonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background.\nA bachelor's degree inComputer Science, Information Management, Statistics, or a comparable discipline is required, with prior experience in data analysis or a related field being advantageous.\nPreferred Qualifications:\nA bachelor's degree in statistics, mathematics, computer science, computer systems engineering or a related discipline or completion of a college program in computer science is usually required.\nA master's or doctoral degree in machine learning, data science, or a related quantitative field is usually required.\nExperience in programming is usually required.\nExperience in statistical modelling or machine learning is usually required.\nRequired Candidate profile\nAge below 50 years\nMin 2 Years of Experience\nAt least a Bachelor/Diploma\nExcellent communication skills\nMedically Fit\nPermanent settle with family\nCan work in any occupation\nRole:Data Scientist\nIndustry Type:Private and public sector\nDepartment:Information technology department\nEmployment Type:Full Time, Permanent\nRole Category:Data Scientist\nSalary 35 -55 LPA\nEducation\nUG:Any Graduate\nKey Skills\nPossess a solid foundation in statistics and practical experience with statistical software (such as Excel, SPSS, SAS) and mastery in data analysis languages including SQL, Python, and R.\nExhibit exceptional analytical abilities to compile, structure, examine, and present substantial data sets with precision and thoroughness.\nCapable of critically evaluating data to derive meaningful, actionable insights.\nDemonstrate superior communication and presentation capabilities, adept at simplifying complex data insights for audiences without a technical background.\nA bachelor's degree inComputer Science, Information Management, Statistics, or a comparable discipline is required, with prior experience in data analysis or a related field being advantageous.",
        "skills": [
            "Data Scientist"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "7-13 Years",
        "salary": "INR 37 - 61 LPA ",
        "location": "Gurugram, Delhi, Chennai",
        "industry": "Spam Filtering, Document Management, VoIP, Cloud Data Services, Reputation, Data Center",
        "job_description": "Description\nWe are seeking an experienced Data Scientist to join our team in India. The ideal candidate will have a strong background in data analysis, machine learning, and statistical modeling, with a passion for leveraging data to drive business decisions.\nResponsibilities\nAnalyze large datasets to derive actionable insights.\nDevelop and implement predictive models using statistical and machine learning techniques.\nCollaborate with cross-functional teams to understand business needs and requirements.\nVisualize data and present findings to stakeholders in a clear and concise manner.\nConduct experiments and A/B testing to optimize processes and outcomes.\nStay updated with the latest industry trends and technologies in data science.\nSkills and Qualifications\n7-13 years of experience in Data Science or related field.\nProficiency in programming languages such as Python, R, or Scala.\nStrong knowledge of machine learning algorithms and statistical analysis.\nExperience with data manipulation and analysis libraries (e.g., Pandas, NumPy).\nFamiliarity with data visualization tools like Tableau, Power BI, or Matplotlib.\nKnowledge of big data technologies such as Hadoop, Spark, or similar frameworks.\nStrong problem-solving skills and ability to work with complex datasets.\nExcellent communication skills to convey data-driven insights to non-technical stakeholders.",
        "skills": [
            "Data Analysis",
            "Statistical Modeling",
            "Cloud Computing",
            "Machine Learning",
            "Sql Queries",
            "Data Visualization",
            "Big Data",
            "Predictive Analytics",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Bengaluru, Chennai, Pune",
        "industry": "Information Technology",
        "job_description": "We are looking for a highly skilled Data Scientist with 5-10 years of experience to join our team in India. The ideal candidate will have a strong background in data analysis and machine learning, and will be responsible for turning data into actionable insights to guide our business strategies.\nResponsibilities\nAnalyze large and complex datasets to extract actionable insights.\nDevelop predictive models and machine learning algorithms to drive business solutions.\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions.\nDesign and implement data collection systems and other strategies to optimize statistical efficiency and data quality.\nVisualize and present data findings to stakeholders using data visualization tools.\nStay up-to-date with the latest data science trends and technologies.\nSkills and Qualifications\nMaster's degree in Computer Science, Statistics, Mathematics, or a related field.\nProficiency in programming languages such as Python, R, or SQL.\nExperience with machine learning frameworks such as TensorFlow or PyTorch.\nStrong understanding of statistical analysis and modeling techniques.\nFamiliarity with data visualization tools like Tableau, Power BI, or Matplotlib.\nExperience with big data technologies such as Hadoop, Spark, or similar.\nExcellent problem-solving skills and ability to work with unstructured data.\nStrong communication skills to present complex information clearly.",
        "skills": [
            "SQL Databases",
            "Cloud Computing",
            "Predictive Modeling",
            "Machine Learning",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Statistical Analysis",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "10-15 Years",
        "salary": "INR 12 - 16 LPA ",
        "location": "Gurugram, Hyderabad, Bengaluru",
        "industry": "Contact Management, Application Performance Management, Augmented Reality, File Sharing, Scheduling",
        "job_description": "We are seeking a highly skilled Data Scientist with 10-15 years of experience to join our dynamic team in India. The ideal candidate will have a strong background in data analysis, statistical modeling, and machine learning, along with excellent problem-solving skills.\nResponsibilities\nAnalyze and interpret complex data sets to inform business decisions.\nDevelop predictive models and machine learning algorithms to enhance product offerings.\nCollaborate with cross-functional teams to understand data needs and provide insights.\nCreate data visualizations and dashboards to communicate findings effectively.\nConduct statistical analysis and testing to validate models and results.\nSkills and Qualifications\nProficiency in programming languages such as Python and R.\nExperience with data manipulation and analysis using SQL.\nStrong understanding of machine learning techniques and algorithms.\nFamiliarity with big data technologies like Hadoop and Spark.\nExperience in data visualization tools such as Tableau or Power BI.\nSolid foundation in statistics and mathematical concepts.\nAbility to work with large datasets and perform data wrangling effectively.",
        "skills": [
            "Data Analysis",
            "Statistical Modeling",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Predictive Analytics",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Best infosystems Ltd.",
        "experience": "8-10 Years",
        "salary": "INR 10 - 25 LPA ",
        "location": "Hyderabad, Chennai, Bengaluru",
        "industry": "Information Technology",
        "job_description": "Databricks with Data Scientist_Full-Time_Pan India\nJob Title: Data Scientist\nJob Type: Full-Time\nLocation: Bangalore/Pune/Navi Mumbai/Noida/Hyderabad/Chennai\nExperience: 8-10 Years\nJob Description:\nDatabricks with Data Scientist:\n*4 years of relevant work experience as a data scientist\n*Minimum 2 years of experience in Azure Cloud using Databricks Services, PySpark, Natural Language API, MLflow\n*Experience designing and building statistical forecasting models.\n*Experience in Ingesting data from API and Databases\n*Experience in Data Transformation using PySpark and SQL\n*Experience designing and building machine learning models.\n*Experience designing and building optimization models., including expertise with statistical data analysis\n*Experience articulating and translating business questions and using statistical techniques to arrive at an answer using available data.\n*Demonstrated skills in selecting the right statistical tools given a data analysis problem. Effective written and verbal communication skills.\n*Skillset: Python, Pyspark, Databricks, MLflow, ADF\n=======\nIf you are interested, please share your updated resume along with the following details for the next steps:\n# Your full name ( First : Middle : Last ) ( All expanded ):\n# Present Employer Name & Work Location:\n# Permanent / Contract Employee:\n# Current Location:\n# Preferred Location (Bengaluru, Chennai, Hyderabad, Mumbai, Noida, Pune, Coimbatore):\n# Highest Qualification (University Name and Passing year):\n# Total experience:\n# Relevant experience as a Data Scientist in years:\n# Relevant experience in Databricks in years:\n# Relevant experience in Python in years:\n# Relevant experience in Pyspark in years:\n# Relevant experience in MLflow in years:\n# Relevant experience in ADF in years:\n# Current CTC and take home:\n# Expected CTC and take home:\n# Official Notice Period:\n# Are you serving notice period if yes then mention LWD (Last Working Day):\n# Any offer you are holding (if yes please share the offer amount):\n# Date of Birth(DOB):\n# PAN Card Number (To upload your profile in client's ATS):",
        "skills": [
            "Data Scientist",
            "Databricks",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Dayworks Private Limited",
        "experience": "5-8 Years",
        "salary": "INR 15 - 25 LPA ",
        "location": "Bengaluru",
        "industry": "Login to check your skill match score",
        "job_description": "This role is for one of the Weekday's clients\nSalary range: Rs 1500000 - Rs 2500000 (ie INR 15-25 LPA)\nMin Experience: 5 years\nLocation: Bengaluru\nJobType: full-time\nWe are looking for an experienced Data Scientist with 58 years of hands-on expertise, ideally within the Retail or Consumer Packaged Goods (CPG) domain. The ideal candidate will possess a strong analytical foundation and a proven ability to derive actionable insights from complex datasets, particularly in areas like product analytics, geospatial data, and sales performance.\nRequirements\nKey Responsibilities\nFunctional Expertise:\nCategory/Product Analytics (Must Have):Analyze product assortments, identify emerging trends, and evaluate new product performance.\nGeo-Spatial Analytics (Must Have):Work with GIS tools such as ArcGIS or QGIS for spatial matching, trade area analysis, and spatial data visualization.\nSales Data Analysis (Must Have):Analyze historical sales trends, forecast future performance, and identify key sales drivers.\nCustomer Analytics (Good to Have):Utilize loyalty and campaign data to improve targeting, segmentation, and performance measurement.\nTechnical Skills & Tools\nStrong storytelling and data presentation skills, with the ability to communicate insights clearly to non-technical stakeholders.\nProficiency inAzure Databricks,Python,SQL, andR.\nSolid understanding of machine learning algorithms, model evaluation, and principles of explainable AI.\nExperience with data visualization tools such asPower BIorSpotfireis a plus.\nPreferred Skills\nGeo-spatial analytics\nGIS tools: ArcGIS, QGIS\nSpatial matching & trade area analysis\nProduct assortment & performance analysis\nSales forecasting & trend analysis\nCustomer behavior and loyalty analytics\nCloud and big data tools: Azure Databricks\nProgramming: Python, SQL, R\nData storytelling and stakeholder communication",
        "skills": [
            "Product assortment analysis",
            "Spatial matching",
            "Geo spatial analytics",
            "customer analytics",
            "Sales Forecasting",
            "Gis",
            "Spatial data visualization",
            "Trade area analysis",
            "Arcgis",
            "Qgis",
            "Power Bi",
            "Azure Databricks",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Bengaluru, Chennai, Pune",
        "industry": "Information Technology",
        "job_description": "We are seeking a skilled Data Scientist with 5-10 years of experience to join our team in India. The ideal candidate will have a strong background in data analysis, machine learning, and statistical modeling, and will be responsible for deriving insights from complex datasets to support business decisions.\nResponsibilities\nAnalyze large datasets to derive actionable insights.\nDevelop and implement predictive models using machine learning techniques.\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions.\nVisualize data and present findings to stakeholders using appropriate tools and techniques.\nContinuously improve processes and methodologies for data analysis and reporting.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field.\nStrong proficiency in programming languages such as Python or R.\nExperience with data manipulation and analysis libraries such as Pandas, NumPy, and Scikit-learn.\nFamiliarity with SQL and database management systems.\nKnowledge of machine learning algorithms and techniques.\nExperience with data visualization tools such as Tableau, Power BI, or Matplotlib.\nStrong analytical and problem-solving skills.\nExcellent communication and presentation skills.",
        "skills": [
            "ETL Processes",
            "Data Analysis",
            "Statistical Modeling",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Python Programming"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Bengaluru, Chennai, Noida",
        "industry": "Information Technology",
        "job_description": "We are seeking an experienced Data Scientist to join our team in India. The ideal candidate will have a strong background in data analysis and machine learning, with the ability to translate complex data into actionable insights.\nResponsibilities\nAnalyze large datasets to identify trends, patterns, and insights.\nDevelop and implement predictive models and algorithms to solve business problems.\nCollaborate with cross-functional teams to understand data requirements and deliver actionable solutions.\nCommunicate findings and recommendations to stakeholders effectively.\nContinuously improve data collection and analysis processes.\nSkills and Qualifications\n5-10 years of experience in data science or a related field.\nProficiency in programming languages such as Python or R.\nStrong understanding of statistical analysis and machine learning techniques.\nExperience with data visualization tools such as Tableau or Power BI.\nFamiliarity with SQL and database management systems.\nExcellent problem-solving skills and ability to work with large datasets.",
        "skills": [
            "SQL Databases",
            "Tensorflow",
            "Cloud Computing",
            "R Programming",
            "Machine Learning",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Statistical Analysis",
            "Python Programming"
        ]
    },
    {
        "job_title": "Data Scientist jobs in Abroad Countries",
        "company_name": "Vijay Sabarwal (Proprietorship of Advance Immigration Solutions)",
        "experience": "1-11 Years",
        "salary": "INR 38 - 65 LPA ",
        "location": "New Zealand, Canada, Australia",
        "industry": "Information Technology",
        "job_description": "URGENT HIRING !!!\nlocation's : Canada , Australia , New Zealand ( Not In India )\nBenefits : Medical Insurances , Travel allowances , Flight Tickets , Meals , etc\nFor more information call or whatsapp +91 9220850077\nKey Responsibilities:\nData Collection and Processing: Gathering data from diverse sources and ensuring its quality through cleaning and preprocessing.\nExploratory Data Analysis (EDA): Analyzing datasets to summarize their main characteristics, often employing visual methods to identify patterns, anomalies, or trends.\nModel Development: Building predictive or descriptive models using machine learning and statistical techniques to address specific business challenges.\nData Visualization: Creating visual representations of data findings to effectively communicate insights to stakeholders.\nCollaboration: Working alongside cross-functional teams, including business analysts, engineers, and domain experts, to implement data-driven solutions.",
        "skills": [
            "Data Analysis",
            "Software Quality Assurance",
            "Data Science",
            "data engineering",
            "Data Management",
            "Datastage",
            "Software Services",
            "Database Testing"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Bengaluru, Chennai, Pune",
        "industry": "Information Technology",
        "job_description": "We are seeking a highly skilled Data Scientist to join our team in India. The ideal candidate will have a strong background in data analysis, machine learning, and statistical modeling to help drive business decisions through data-driven insights.\nResponsibilities\nAnalyze large datasets to derive actionable insights\nDevelop predictive models and machine learning algorithms\nCollaborate with cross-functional teams to understand business needs\nVisualize data and present findings to stakeholders\nImplement data-driven solutions to improve business processes\nConduct A/B testing and statistical analysis to validate hypotheses\nSkills and Qualifications\n5-10 years of experience in data science or related field\nProficiency in programming languages such as Python or R\nStrong understanding of statistics and machine learning algorithms\nExperience with data visualization tools like Tableau or Power BI\nFamiliarity with SQL and database management\nAbility to work with big data technologies such as Hadoop or Spark\nExcellent problem-solving skills and attention to detail\nStrong communication skills to present complex data insights",
        "skills": [
            "Data Analysis",
            "Statistical Modeling",
            "SQL Databases",
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Big Data Technologies",
            "Predictive Analytics",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist jobs in Abroad Countries",
        "company_name": "Vijay Sabarwal (Proprietorship of Advance Immigration Solutions)",
        "experience": "1-11 Years",
        "salary": "INR 45 - 55 LPA ",
        "location": "New Zealand, Canada, Australia",
        "industry": "Information Technology",
        "job_description": "URGENT HIRING !!!\nFor more information call or whatsapp +919220850077\nlocation's : Canada , Australia , New Zealand ( Not In India )\nBenefits : Medical Insurances , Travel allowances , Flight Tickets , Meals , etc\nData mining or extracting usable data from valuable data sources\nUsing machine learning tools to select features, create and optimize classifiers\nCarrying out the preprocessing of structured and unstructured data\nEnhancingdata collectionprocedures to include all relevant information for developing analytic systems\nProcessing, cleansing, and validating the integrity of data to be used for analysis\nAnalyzing large amounts of information to find patterns and solutions\nDeveloping prediction systems and machine learning algorithms\nPresenting results in a clear manner\nPropose solutions and strategies to tackle business challenges\nCollaborate with Business and IT teams",
        "skills": [
            "Machine Learning",
            "Power Bi",
            "Tableau",
            "AI ML",
            "Data Science",
            "Nlp",
            "Software Development",
            "MLops",
            "Docker",
            "Data Scientist",
            "Big Data Technologies",
            "Data Science - ML",
            "Azure",
            "Kubernetes",
            "Aws",
            "Data Scraping"
        ]
    },
    {
        "job_title": "DATA SCIENTIST -Ml INTERNSHIP",
        "company_name": "Maxgen Technologies Private Limited",
        "experience": "Fresher",
        "salary": null,
        "location": "Kolhapur, Solapur, Pune",
        "industry": "Information Technology",
        "job_description": "Maxgen Technologies Pvt ltd is based in pune offering software development internship. internship we are providing in data science and machine learning program.\nwhy we are important\nSkill development Program.\nhybrid internships.\nprofessional team work .\nadvance skill development .\njob placement program.\ncall us 9099039845, 9607007009, 9975064558.\nvisit us www.maxgentechnologies.com.\nAddress 509 pride icon , near atithi veg restaurant , kharadi pune.",
        "skills": [
            "Statistics",
            "Internship",
            "R",
            "Machine Learning",
            "Apis",
            "Big Data",
            "Sql",
            "Data Science",
            "Data Scientist",
            "Data Visualization",
            "data wrangling",
            "Excel",
            "Python"
        ]
    },
    {
        "job_title": "Principal Data Scientist with AWS SageMaker, Pyhton & SQL",
        "company_name": "Mastek",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "As a member of the Pricing team, this Principal Data Scientist will support the development of innovative solutions for our dealer-facing software products. This work will include optimizing our dealer platform to improve market analysis capabilities.\nWhat you'll do\nCollect, clean, and analyze large datasets to identify trends and patterns.\nWork in a multi-functional team with software engineers, data analysts, and product managers\nTest out new insights with novel and quick experiments\nTrain and monitor machine learning systems and models\nEvaluating performance of production-deployed models\nWrite clean, well-tested, and efficient code\nCollaborate with and be mentored and mentor experienced engineers\nKnowledge of data pipelines, ETL processes, and data warehousing\nBe able to establish best practices for the team What you'll bring\nPh.D or Master's in Data Science , Machine Learning, Statistics, or a related fi eld\n7+ years of experience building ML models\nWell versed with EDA and can explain complex problems in simple terms\nProfi cient knowledge of Python\nProfi cient knowledge of SQL\nProfi cient knowledge in AWS SagerMaker preferred\nProfi cient knowledge in AWS / AWS Certifi cation preferred\nBeing able to balance speed with quality\nAble to work across teams with both technical and non-technical partners.\nWillingness to work outside your comfort zone, to evaluate and assess new technologies.",
        "skills": [
            "ETL processes",
            "Machine Learning",
            "Data Warehousing",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "STAFF, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team\n\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart's product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\n\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\n\nWhat you'll do:\n\nAs a Staff Data Scientist for Walmart Global Tech, you'll have the opportunity to\n\nMentor peers and junior members and handle multiple projects at the same time.\nConsult with business stakeholders across stores and e-commerce businesses regarding algorithm-based recommendations and be a thought-leader to develop these into business actions.\nDesign, develop, and deploy AI/ML, NLP and LLM models into production environments with a focus on reliability and scalability\nDevelop efficient and scalable models at Walmart scale.\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nEngage and partner with universities, institutes, and vendor partners to bring in ideas and innovation into the lab's environment.\nPeer review and publish work in top tier ML/AI conferences such as NIPS, ICML, AAAI and COLT\nParticipate and speak at various external forums such as research conferences and technical summits.\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\n\nWhat you'll bring:\n\nQualifications\nPhD with >5 years of relevant experience / 4-year bachelor's degree with > 10 years of experience / Master's degree with > 8 years of experience. Educational qualifications should be preferably in Computer Science or a strongly quantitative discipline.\nDemonstrated history of strong hands-on experience in AI/ML modeling\nProven records of scientific publications or intellectual property generation\nPast experience in solving complex business problems using Vision Models\nPast experience in strong programming skills across data science, big data and ML engineering stack\nStrong communication skills with inclination to high ownership and commitment\n\nGood to have:\n\nExperience with Ecommerce domain.\n\nAbout Walmart Global Tech:\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work:\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits:\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nEqual Opportunity Employer:\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions while being inclusive of all people\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2128208",
        "skills": [
            "Llm",
            "ML Engineering",
            "Generative AI",
            "Big Data",
            "Data Science",
            "Computer Vision",
            "Artificial Intelligence",
            "Deep Learning",
            "Nlp",
            "Machine Learning"
        ]
    },
    {
        "job_title": "STAFF, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "6-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nPersonalization team at Walmart is dedicated to the mission of knowing our customers better and making it frictionless and faster for them to get what they want while building a trusted partnership with Walmart. We help bring a seamless experience to our customers irrespective of how and where they engage with us - while shopping online and picking groceries at stores. We thrive to provide each customer with a journey tailored to their individual needs, preferences and routines. We operate at the intersection of huge Walmart assortment, millions of customers, thousands of stores and Walmart associates - a multi-layer multi-objective problem space which has a unique impact on Global consumer base and powers assisted AI for our associates.\n\nThe Personalization team consists of data scientists, application engineers, big data geeks and product visionaries all working together to design, prototype and build technology-driven products and experiences that will change the future landscape of e-commerce. We are focussed on building an intelligent system that will enable 1:1 personalised customer experience, from finding the product to delivering it to the customer. We are building state-of-the-art machine learnt systems using cutting edge technology that has ability to A/B test in fast iterations, use GPU/CPU based online inferencing of deep learnt and traditional models, event driven architecture that serves personalized experience for customers. This will span many markets, business models and form factors, therefore, we are looking for engineers and data scientists who will being not only an abundance of experiences in technologies but an abundant curiosity to innovate.\n\nWhat you will do:\n\nAs a Staff Data Scientist for Walmart Labs, you'll have the opportunity to\n\nApply and/or develop ML solutions to develop efficient and scalable models at Walmart scale.\nPlay a key role to solve complex problems, pivotal to Walmart's business and drive actionable insights from terabytes of data.\nLeverage data science tools and techniques, keeping abreast with the latest in the community to solve problems for Walmart.\nCollaborate with counterparts in business, engineering, and science to find impactful solutions to business problems.\nDefine and/or own the model goodness metrics and track the business impact over time.\nCreate recommendation models for personalizing user experience.\nDevelop PoC, present lucidly to the business and evolve the solutions.\nTake forward the solutions into Pipelines/APIs as needed by the business.\nResearch, learn/disseminate & adapt new technologies to solve problems & improve upon existing solutions.\nAdopt Wal-Mart's quality standards and develop and recommend process standards and best practices across the retail industry.\nManage the continuous improvement of data science and machine learning by following industry best practices and staying up to date with and extending the state-of-the-art in Machine Learning research.\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nMentor peers and junior members and handle multiple projects at the same time.\nPeer review and publish work in top tier ML/AI conferences\nParticipate and speak at various external forums such as research conferences and technical summits.\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\n\nWhat you will bring:\n\nPhD with > 6 years of experience / master's degree with > 8 years of experience / bachelor's degree with > 9 years of experience. Educational qualifications should be preferably in STEM. Experience should be relevant to the role.\nExperience in analysing complex problems and translating them to data science algorithms with due attention to computational efficiency and testing at scale.\nExpertise in machine learning, supervised and unsupervised: Recommendation models for ranking, latest technique in NLP Deep Learning Algorithms and Reinforcement Learning\nExperience working with big data - identifying trends, patterns, and outliers in large volumes of data.\nWorked with at least one mainstream machine learning framework such as Tensor Flow, Torch\nExperience with SQL, relational databases and data warehouse\nExperience with big data platforms - Hadoop(Hive, Pig, Map Reduce, HQL) / Spark\nDomain Knowledge : Personalization\nExperience with multiple stakeholder management, data based story-telling, mentoring peers and juniors, multiple project handling at the same time.\n\nAbout Global Tech.\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That's what we do at Walmart Global Tech. We're a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world's largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail\n\nFlexible, hybrid work:\n\nWe use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.\n\nBenefits:\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer:\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas, and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2129123",
        "skills": [
            "reinforcement learning",
            "Torch",
            "Map Reduce",
            "Deep Learning Algorithms",
            "Recommendation Models",
            "Machine Learning",
            "Hadoop",
            "Hql",
            "Big Data",
            "Sql",
            "Pig",
            "Nlp",
            "Hive",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Bosch Global Software Technologies",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Roles & Responsibilities :\nMachine Learning & Data Science:\nDevelop and implement advanced statistical models, predictive analytics, and optimization frameworks.\nResearch, develop, and apply ML, Deep Learning, and AI techniques on large datasets.\nConduct exploratory data analysis (EDA) and apply feature engineering techniques.\nOptimize and tune ML models for performance, scalability, and accuracy.\nLarge Language Models (LLMs) & NLP:\nFine-tune and customize LLMs (GPT, LLaMA, Falcon, etc.) for enterprise use cases.\nImplement Retrieval Augmented Generation (RAG) pipelines for chatbots and knowledge retrieval.\nWork with vector databases for semantic search.\nDeploy LLM-powered applications for document summarization, sentiment analysis, and automation.\nAI & Data-Driven Business Insights:\nTranslate complex AI/ML models into actionable business insights.\nPartner with stakeholders to define data requirements and ensure the right data is captured.\nDevelop proof-of-concept solutions demonstrating AI and ML capabilities.\nDeployment & MLOps:\nDeploy AI/ML models using cloud technologies (Azure, AWS, GCP).\nWork with Docker, Kubernetes, and CI/CD pipelines for scalable ML deployment.\nContinuous Learning & Innovation:\nStay updated with the latest trends in AI, LLMs, GenAI, and cloud-based ML platforms.\nQualifications & Skills\n4 - 6 years of experience working as a data scientist\nTechnical Skills:\nStrong experience in Python (NumPy, Pandas, Scikit-learn, TensorFlow, PyTorch, Keras, SciPy).\nHands-on expertise with LLMs, NLP techniques, and vector search (LangChain, Hugging Face).\nSolid understanding of statistics, probability, and data science methodologies.\nExperience with data pipelines (ETL, Spark, Hadoop, SQL, NoSQL, Data Lakes).\nKnowledge of Cloud AI services (Azure ML, AWS SageMaker, GCP Vertex AI).\nStrong software engineering principles with experience in Jupyter Notebooks, API development, and workflow automation.\nSolid understanding of Cloud Technologies (ideally Microsoft Azure) and SQL are beneficial\nSoft Skills:\nExcellent communication, presentation, and collaboration skills.\nAbility to translate AI/ML insights into business impact.\nSelf-motivated with a continuous learning mindset",
        "skills": [
            "vector search",
            "data pipelines",
            "Hugging Face",
            "GCP Vertex AI",
            "workflow automation",
            "NLP techniques",
            "LangChain",
            "Data Lakes",
            "LLMs",
            "probability",
            "Scikit-learn",
            "data science methodologies",
            "Cloud AI services",
            "Jupyter Notebooks",
            "AWS SageMaker",
            "Statistics",
            "Nosql",
            "Tensorflow",
            "Numpy",
            "Pytorch",
            "Python",
            "Scipy",
            "Hadoop",
            "Api Development",
            "Sql",
            "Cloud Technologies",
            "Azure ML",
            "Pandas",
            "Spark",
            "Keras",
            "Etl"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout The Team\n\nCentroid team at Walmart serves as the backbone of Walmart's end-to-end supply chain strategy. They are entrusted with the task of designing and implementing a long-term supply chain strategy that uses advanced data analytics and data science. Their primary objective is to ensure that Walmart provides top-tier customer service while supporting the increasing demand over time and simultaneously operating at low and efficient costs.\n\nThe team utilizes sophisticated data analysis methods to understand patterns, identify potential bottlenecks, and predict future trends. This enables them to optimize processes, make informed business decisions, and enhance overall operational efficiency.\n\nOne of Centroid's key responsibilities also includes the creation of a Digital Twin Simulation platform for Walmart's supply chain. This innovative tool allows the team to test and validate all future strategies and tactical decisions before they are launched operationally. It also enables a deep assessment of long-term strategic sensitivity.\n\nIn essence, the Centroid team's work is integral to ensuring Walmart's supply chain is robust, flexible, and capable of adapting to ever-changing market demands. Their work helps to keep Walmart at the forefront of retail supply chain management, delivering exceptional service to customers while maintaining efficient operational costs.\n\nWhat You'll Do -\n\nDevelop and manage advanced data analytics models to optimize supply chain strategies, balancing customer satisfaction with operational cost and asset efficiency.\nLeverage data analytics to identify opportunities for improvement and drive impactful results through collaboration with cross-functional teams.\nEstablish relationships across Walmart functional areas to identify best practices, solicit data/input, coordinate interdisciplinary initiatives, and rally support for data-driven recommendations.\nSecure alignment and support from relevant business partners and management for data-centric projects, leading discussions to drive necessary change.\nUtilize all available data resources effectively to ensure successful project outcomes.\nCommunicate data insights clearly and persuasively through emails, verbal discussions, and presentations, tailoring communication methods to the audience for maximum impact.\nCollaborate with multiple supply chain business teams to proactively identify, assess, and leverage cost-saving and service improvement opportunities through advanced data analytics.\nUtilize advanced analytics models to derive insights that will inform policy design across various supply chain areas, laying out multiple scenarios and performing sensitivity analysis.\nCollaborate with Data Scientists and Engineers to productionize and scale advanced analytics models as needed.\nDevelop and present compelling data-driven narratives/documents/visuals to influence key stakeholders in their decision-making.\nProvide coaching and training support to other team members in the supply chain area, leveraging your expertise in advanced data analytics.\n\nWhat You'll Bring -\n\nStrong analytical acumen with technical expertise in Advanced Data Analytics and modelling\nExpert in SQL, - BigQuery like cloud data platforms.\nExpert in programming in Python, (or R)\nExperience in using data visualization tools like Tableau and Looker and be able to drive powerful insights.\nExperience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, and/or Spark)\nExperience in operating from a cloud environment such as Google Could Platform or Microsoft Azure.\nAbility to work in a fast-paced, iterative development environment.\nStrong communication skills, both written and verbal, plus ability to work with cross functional teams of technical and non-technical members.\nStrong ability to understand the business and have good stakeholder management capabilities.\nExperience of working in cross-functional environment and leading or mentoring teams.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2153626",
        "skills": [
            "R",
            "Map Reduce",
            "SQL - BigQuery",
            "Looker",
            "Advanced Data Analytics",
            "Hive",
            "Hadoop",
            "Spark",
            "Tableau",
            "Microsoft Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist Gen AI",
        "company_name": "Zensar Technologies",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Hi Applicants,\nWe have an urgent demand for Data Scientist Gen AI. Please go through the Job Description below and share your resume if you feel interested.\nRequirement for Data Scientist Gen AI\nExperience-5+ Years\nLocation-Hyderabad,Pune,Bangalore\nNotice Period-Immediate to15 days\nJob Description\nDevelop and deploy ML and Gen AI applications using LangChain, Llama Index, and OpenAI.\nConduct comprehensive EDA to uncover insights and inform ML model development.\nCollaborate with cross-functional teams to integrate AI solutions into business processes.\nContinuously monitor and improve the performance of AI models.\nStay updated with the latest advancements in ML and Gen AI technologies.\nMandatory Skillsets:\nHighly proficient in ML and Gen AI application development with LangChain, Llama Index, OpenAI\nStrong industry exposure in Data Science with Machine Learning modelling techniques and Exploratory Data Analysis\nAtleast 5+ years of experience in data science with 2 years of experience in Gen AI\nGood-to-Have Skillsets:\nExperience with other AI frameworks and tools.\nStrong programming skills in Python or R.\nFamiliarity with cloud platforms such as AWS, Azure, or Google Cloud.\nExcellent problem-solving and analytical skills.\nStrong communication and teamwork abilities.",
        "skills": [
            "LangChain",
            "R",
            "Llama Index",
            "OpenAI",
            "Machine Learning",
            "Exploratory Data Analysis",
            "Azure",
            "Google Cloud",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Fidelity Investments",
        "experience": "Fresher",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nJob Title Data Scientist\n\nThe Purpose of This Role\n\nThis role involves working with senior stakeholders in understanding and identifying the business requirements and needs in the areas of Machine Learning and Artificial Intelligence. Scanning the fast paced external and internal environments in these areas, developing futuristic ideas that would benefit the firm and leading implementation of mathematical/AI models/algorithms/solutions to bring those ideas to life. Specific experience in AI areas like Recommendation systems, deep learning, Reinforcement learning etc. will be an advantage.\n\nHow Will Your Work Impact The Organization\n\nIndividual in this role is expected to work with multiple stakeholders and teams, have excellent knowledge and experience of recent developments and work in a self-directed manner in ambiguously defined need areas.\n\nThe Value You Deliver\n\nImprove customer experience, top line and bottom line of the business by identifying growth and efficiency opportunities\nAnalyze structured and unstructured data using data mining/statistical/machine learning/deep learning tools, Data visualization, Digital/ Web analytics tools and techniques and storytelling.\nManaging business partner / stakeholder engagement\nParticipating in / leading interactions with middle and senior level management\n\nThe Skills That Are Key To This Role\n\nTechnical / Behavioral\n\nExpertise in Machine Learning (ML) and Artificial Intelligence (AI) :\nGiven a business problem, the ability to translate it into an ML problem and then\nconceptualize and design solutions from end-to-end\nPossess extensive knowledge of and experience in applying data mining and machine learning, deep learning and Reinforcement learning techniques\nExpert in handling various data types and structures: structured, unstructured, voice, static versus streaming data. Extensive prior experience in integrating data\nFamiliarity with recommender system algorithms such as multi-armed bandit, neural collaborative filtering, graph based recommendation etc.\nShould have extensive experience with relational, NO-SQL and Graph databases\nInterpret, criticize, and debug neural networks (not just implement and evaluate accuracy)\nProficiency in the entire Data Modeling life cycle\nComfortable with TensorFlow/Keras/Theano/PyTorch\nFamiliarity with foundational model reasoning: bias-variance tradeoff, shrinkage/partial pooling\nAble to independently research and apply state-of-the-art deep learning and Bayesian modeling literature\nCommunicate design decisions, tradeoffs, and findings to technical and non-technical stakeholders\nVery good communication skills\nSelf-motivated individual with a strong sense of purpose\nIndependent execution abilities with no hand-holding\nAbility to work in teams\nMust be able to mentor and guide junior team members\n\nAdditional Skills\n\nExpert in Python Programming\nExperience in building and deploying models on the cloud (AWS SageMaker, Glue)\nComfort with Git source control and repo-driven development - continuous integration / continuous deployment\nAble to organize code using functional programming concepts, experience with statically typed functional programming a plus\nComfortable working with data platforms independently able to write REST clients, SQL, PySpark, as needed.\nExposure to containerization/DockerThe Skills that are Good To Have for this role\nAdvanced theory - optimization, differential geometry, statistical physics, category theory\nAdvanced functional programming languages - Haskell, Idris and exposure to declarative programming concepts\n\nThe Expertise We're Looking For\n\nRelevant experience with organizations known for cutting edge/ best-in-class applications of Advanced Analytics, Predictive Modeling and Artificial Intelligence.\nMaster's Degree / PhD in a quantitative field (e.g., Computer Science, Economics, Engineering, Statistics, Mathematics, Finance, Operations Research)\n\nLocation: Chennai\n\nShift timings: 11:00 AM 8:00PM\n\nCertifications\n\nCategory:\n\nData Analytics and Insights",
        "skills": [
            "reinforcement learning",
            "Recommendation Systems",
            "SageMaker",
            "theano",
            "Glue",
            "Pyspark",
            "Data Mining",
            "Sql",
            "Deep Learning",
            "Python Programming",
            "Tensorflow",
            "Git",
            "Pytorch",
            "Docker",
            "graph databases",
            "Keras",
            "AWS"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmarts product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust ; safety, and catalog optimization, empowering next-generation retail use cases.\n\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations ; our associates, helping Customers Save Money to Live Better.\n\nWhat you'll do:\n\nAs a Senior Data Scientist for Walmart Global Tech, you'll have the opportunity to\n\nDesign, develop, and deploy AI/ML, NLP, LLM models into production environments with a focus on reliability and scalability\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nSpearhead collaborations with other senior team members and stakeholders, leveraging your data science expertise to drive strategic decision-making and optimize business operations\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\n\nWhat you'll bring:\n\nQualifications\nPhD with >3 years of relevant experience / 4-year bachelors degree with > 8 years of experience / Masters degree with > 6 years of experience. Educational qualifications should be preferably in Computer Science or a strongly quantitative discipline.\nDemonstrated history of strong hands-on experience in AI/ML modelling\nPast experience in building Vision-based models\nPast experience in programming skills across data science, big data and ML engineering stack\nStrong communication skills with inclination to high ownership and commitment\nProven track record of delivering high-impact AI/ML solutions\n\nGood to have:\n\nExperience with Ecommerce domain.\n\nAbout Walmart Global Tech:\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionals within the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work:\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits:\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nEqual Opportunity Employer:\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2079766",
        "skills": [
            "Ai",
            "ML engineering",
            "Llm",
            "Ml",
            "Nlp",
            "Computer Vision",
            "Data Science",
            "Big Data"
        ]
    },
    {
        "job_title": "STAFF, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "6-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nThe role is for Staff Datascientist in Search datascience team based out of Bangalore.\n\nWhat you'll do...\n\nAbout Team:\n\nOur Search datascience team is building a world class search system for Walmart e-comm customers. We use cutting edge machine learning, data mining and optimization algorithms to ingest, model, and analyze Walmart's proprietary online and in-store data, encompassing 95% of American households. Importantly, we build smart data systems that deliver relevant search results and experiences that connect our customers with the brands and products they love.\n\nWhat you will do:\n\nAs a Staff Data scientist, Search for Walmart Global Tech, you'll have the opportunity to\n\nApply and/or develop ML solutions to develop efficient and scalable models at Walmart scale.\nPlay a key role to solve complex problems, pivotal to Walmart's business and drive actionable insights from terabytes of data.\nLeverage data science tools and techniques, keeping abreast with the latest in the community to solve problems for Walmart.\nCollaborate with counterparts in business, engineering, and science to find impactful solutions to business problems.\nDefine and/or own the model goodness metrics and track the business impact over time.\nPresent recommendations from complex analysis to business partners in clear and actionable form, influencing the future.\nDevelop PoC, present lucidly to the business and evolve the solutions.\nTake forward the solutions into Pipelines/APIs as needed by the business.\nResearch, learn/disseminate & adapt new technologies to solve problems & improve upon existing solutions.\nAdopt Wal-Mart's quality standards and develop and recommend process standards and best practices across the retail industry.\nManage the continuous improvement of data science and machine learning by following industry best practices and staying up to date with and extending the state-of-the-art in Machine Learning research.\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nMentor peers and junior members and handle multiple projects at the same time.\nPeer review and publish work in top tier ML/AI conferences such as NIPS, ICML, AAAI and COLT\nParticipate and speak at various external forums such as research conferences and technical summits.\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\n\nWhat you will bring:\n\nPhD with > 6 years of experience / master's degree with > 8 years of experience / bachelor's degree with > 9 years of experience. Educational qualifications should be preferably in STEM. Experience should be relevant to the role.\nExperience in analysing complex problems and translating them to data science algorithms with due attention to computational efficiency and testing at scale.\nExpertise in machine learning, supervised and unsupervised: Time Series Forecasting, latest technique in NLP Deep Learning Algorithms and Reinforcement Learning\nExperience in statistical learning: Predictive & Prescriptive Analytics, Web Analytics, Parametric and Non-parametric models, Regression, Time Series, Dynamic / Causal Model, Statistical Learning, Guided Decisions, Topic Modelling\nExperience working with big data - identifying trends, patterns, and outliers in large volumes of data.\nWorked with at least one mainstream machine learning framework such as caffe, convNet, Tensor Flow and Torch\nExperience with SQL, relational databases and data warehouse\nExperience with big data platforms - Hadoop(Hive, Pig, Map Reduce, HQL) / Spark / H20\nDomain Knowledge : Search, Recommendation Engine\nExperience with multiple stakeholder management, data based story-telling, mentoring peers and juniors, multiple project handling at the same time.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2113786",
        "skills": [
            "Prescriptive Analytics",
            "Deep Learning Algorithms",
            "reinforcement learning",
            "Search Recommendation Engine",
            "Time Series Forecasting",
            "statistical learning",
            "Data Mining",
            "Sql",
            "Web Analytics",
            "Optimization Algorithms",
            "Machine Learning"
        ]
    },
    {
        "job_title": "Principal Data Scientist with AWS SageMaker, Pyhton & SQL",
        "company_name": "Mastek",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "As a member of the Pricing team, this Principal Data Scientist will support the development of innovative solutions for our dealer-facing software products. This work will include optimizing our dealer platform to improve market analysis capabilities.\nWhat you'll do\nCollect, clean, and analyze large datasets to identify trends and patterns.\nWork in a multi-functional team with software engineers, data analysts, and product managers\nTest out new insights with novel and quick experiments\nTrain and monitor machine learning systems and models\nEvaluating performance of production-deployed models\nWrite clean, well-tested, and efficient code\nCollaborate with and be mentored and mentor experienced engineers\nKnowledge of data pipelines, ETL processes, and data warehousing\nBe able to establish best practices for the team What you'll bring\nPh.D or Master's in Data Science , Machine Learning, Statistics, or a related fi eld\n7+ years of experience building ML models\nWell versed with EDA and can explain complex problems in simple terms\nProfi cient knowledge of Python\nProfi cient knowledge of SQL\nProfi cient knowledge in AWS SagerMaker preferred\nProfi cient knowledge in AWS / AWS Certifi cation preferred\nBeing able to balance speed with quality\nAble to work across teams with both technical and non-technical partners.\nWillingness to work outside your comfort zone, to evaluate and assess new technologies.",
        "skills": [
            "ETL processes",
            "Machine Learning",
            "Data Warehousing",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nOur Team\n\nInternational Data Science team at Walmart Global Tech is focused on using the latest research in machine learning, statistics and optimization to solve business problems in assortment, pricing, sourcing, customer, supply chain, and replenishment areas for multiple countries within Walmart's global operations. We mine data, distill insights, extract information, build analytical models, deploy Machine Learning algorithms, and use the latest algorithms and technology to empower business decision-making. We work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions. Advanced analytical algorithms driven by our team will help Walmart to optimize business operations, business practices and change the way our customers shop.\n\nThe data science community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Labs will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\n\nYour Opportunity\n\nAs an Senior Data Scientist for Walmart Global Tech, you'll have the opportunity to\n\nDrive data-derived insights across the wide range of retail divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assessing data validity and synthesizing data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train statistical models and machine learning algorithms for replication for future projects\nCommunicate recommendations to business partners and influencing future plans based on insights\n\nWhat You Will Do\n\nPlay a key role to solve complex problems, pivotal to Walmart's business and drive actionable insights from petabytes of data\nUtilize product mindset to build, scale and deploy holistic data science products after successful prototyping\nDemonstrate incremental solution approach with agile and flexible ability to overcome practical problems\nLead small teams and participate in data science project teams by serving as the technical lead for project\nPartner with senior team members to assess customer needs and define business questions\nClearly articulate and present recommendations to business partners, and influence future plans based on insights\nPartner and engage with associates in other regions for delivering best services to the customers around the globe\nWork with customer centric mindset to deliver high quality business driven analytic solution\nMentor peers and analysts across the division in analytical best practices\nDrive innovation in approach, method, practices, process, outcome, delivery, or any component of end-to-end problem solving\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity\n\nOur Ideal Candidate\n\nYou are a technically strong and high performing individual with excellent communication skills, proven analytical skill set and strong customer focus. You stay updated with latest research and technology ideas, and have a passion to utilize innovative ways to solve problems. You are a good story-teller and able to simply articulate the intricacies of your models as well as explain your results clearly to stakeholders. You have industry knowledge of the retail space, with keen interest in keeping up to date on the latest happenings in this space.\n\nWhat You Will Bring\n\nBachelor's with > 7 years of relevant experience OR Masters with > 5 years of relevant experience OR PHD in Comp Science/Statistics/Mathematics with > 3 years of relevant experience\nExperience in Analyzing the Complex Problems and translate it into data science algorithms\nExperience in machine learning, supervised and unsupervised: NLP, Classification, Data/Text Mining, Multi-modal supervised and unsupervised models, Neural Networks, Deep Learning Algorithms\nExperience in statistical learning: Predictive & Prescriptive Analytics, Web Analytics, Parametric and Non-parametric models, Regression, Time Series, Dynamic/Causal Model, Statistical Learning, Guided Decisions, Topic Modeling\nExperience with big data analytics - identifying trends, patterns, and outliers in large volumes of data\nEmbedding generation from training materials, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nLead role mentoring multiple Jr. Analysts on approach and results.\nStrong Experience in Python, PySpark\nGoogle Cloud platform ,Vertex AI, Kubeflow, model deployment\nStrong Experience with big data platforms Hadoop (Hive, Map Reduce, HQL, Scala)\n\nAdditional Preferred Qualifications:\n\nDomain Knowledge of one or more divisions in Retail\nPublished papers or given talks in leading academic and research journals\nPublished papers or given talks in Data Science Forums\nHold data science related patents\nExperience with big data platforms - Hadoop(Hive, Pig, Map Reduce, HQL) / Spark\nExperience in deep learning and worked in TensorFlow and Torch\nExperience with GPU/CUDA for computational efficiency\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\nPardhanani Wilshire Ii, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2133546",
        "skills": [
            "Computational Algorithms",
            "Torch",
            "GPU CUDA",
            "Statistical Models",
            "Map Reduce",
            "Kubeflow",
            "Vertex AI",
            "Machine Learning",
            "Big Data Analytics",
            "Google Cloud Platform",
            "Hadoop",
            "Hql",
            "Scala",
            "Pyspark",
            "Deep Learning",
            "Tensorflow",
            "Hive",
            "Python"
        ]
    },
    {
        "job_title": "(IND) SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThis is the team which builds reusable technologies that aid in acquiring customers, onboarding and empowering merchants besides ensuring a seamless experience for both these stakeholders. We also optimize tariffs and assortment, adhering to the Walmart philosophy - Everyday Low Cost. In addition to ushering in affordability, we also create personalized experiences for customers the omnichannel way, across all channels - in-store, on the mobile app and websites. Marketplace is the gateway to domestic and international Third-Party sellers; we enable them to manage their end-to-end onboarding, catalog management, order fulfilment, return & refund management. Our team is responsible for design, development, and operations of large-scale distributed systems by leveraging cutting-edge technologies in web/mobile, cloud, big data & AI/ML. We interact with multiple teams across the company to provide scalable robust technical solutions.\n\nWhat you'll do:\n\nAs a Data Scientist for Walmart , you'll have the opportunity to\n\nDrive data-derived insights across the wide range of retail divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assessing data validity and synthesizing data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train statistical models and machine learning algorithms for replication for future projects\nCommunicate recommendations to business partners and influencing future plans based on insights\n\nWhat you'll bring:\n\nVery good knowledge of the foundations of machine learning and statistics\nHand on Experience in building and maintaining Gen AI powered solutions in production\nExperience in Analyzing the Complex Problems and translate it into data science algorithms\nExperience in machine learning, supervised and unsupervised and deep learning.\nHands on experience in Computer Visions and NLP.\nExperience with big data analytics - identifying trends, patterns, and outliers in large volumes of data\nStrong Experience in Python with excellent knowledge of Data Structures\nStrong Experience with big data platforms Hadoop (Hive, Pig, Map Reduce, HQL, Scala, Spark)\nHands on experience with Git\nExperience with SQL and relational databases, data warehouse\nQualifications\nBachelors with > 7 years of experience / Master's degree with > 5 years of experience. Educational qualifications should be preferably in Computer Science/Mathematics/Statistics or a related area. Experience should be relevant to the role.\nGood to have:\nExperience in ecommerce domain.\nExperience in R and Julia\nDemonstrated success in data science platforms like Kaggle.\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2146190",
        "skills": [
            "Data Science Techniques",
            "Relational Databases",
            "Computational Algorithms",
            "Statistical Models",
            "Map Reduce",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Data Structures",
            "Scala",
            "Data Warehouse",
            "Pig",
            "Sql",
            "Nlp",
            "Git",
            "Hive",
            "Spark",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "IN-Senior Associate_Data Scientist_Clients & Industries_IFS_ Gurgaon",
        "company_name": "PwC India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Line of Service\n\nInternal Firm Services\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nOperations\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in brand management, marketing and sales focus on collaboration to develop and execute strategic sales and marketing initiatives. These individuals focus on driving revenue growth, promoting the Firm's services, enhancing brand visibility, and capturing new business opportunities. They utilise market research, digital marketing, creative campaigns, and effective sales strategies to engage clients, enhance the firm's brand and market presence, and achieve organisational targets.\n\nAs a marketing generalist at PwC, you will focus on a wide range of marketing activities aimed at promoting the Firm's products or services. You will conduct market research, develop marketing strategies, create and execute marketing campaigns, and analyse campaign performance. You will leverage a versatile skill set and knowledge in various marketing channels, holding responsibility for validating brand consistency, reaching target audiences, and driving customer engagement and loyalty.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary:\n\nWe are seeking a highly skilled and motivated individual to join our research unit, The Research and Insights Hub (RIH), as a Data Analyst and Visualization Specialist. In this role, you will be responsible for conducting data analysis and creating visually appealing and insightful data visualization for our internal clients. Your expertise in statistical tools and programming languages, particularly R, will be crucial in delivering accurate and meaningful outcomes.\n\nResponsibilities:\n\nConduct data analysis using statistical techniques to extract valuable insights from survey data.\n- Collaborate with research teams to understand the objectives and requirements of each project.\n- Develop and implement data visualization strategies to effectively communicate research findings.\n- Utilize R and other relevant tools to clean, transform, and analyze large datasets.\n- Create visually appealing and interactive dashboards, charts, and graphs to present data in a clear and concise manner.\n- Collaborate with cross-functional teams to ensure data accuracy and consistency.\n- Provide guidance on survey design and question formulation to ensure the desired outcomes are achieved.\n-Set up various databases and ensure that they are up to date\n- Stay up to date with the latest trends and advancements in data analysis and visualization techniques.\n\n\nMandatory Skill Sets:\n\n\nFamiliarity with survey design and questionnaire development.\n\nExcellent analytical and problem-solving skills.\nStrong attention to detail and ability to work with large datasets.\nEffective communication skills to present complex data findings to non-technical stakeholders.\nAbility to work independently and manage multiple projects simultaneously.\nBasic familiarity with GenAI and ability to explore its use cases in data analysis\nPrior experience in a research or consulting environment is a plus.\n\nStatistical Data\n\nStatistics\n\nData Visualization\n\nPreferred Skill Sets:\n\nData Modeling\n\nYears Of Experience Required:\n\n\n5+ Years\n\nEducation Qualification:\n\nBa Statistics\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor Degree\n\nDegrees/Field Of Study Preferred:\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nData Modeling\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Brand Management, Brand Marketing, Business Development, Campaign Performance Analysis, Channel Marketing, Communication, Consumer Behavior, Content Marketing, Content Strategy, Creativity, CRM Software, Customer Insight, Developing Communication Strategies, Digital Marketing, Embracing Change, Emotional Regulation, Empathy, Entertainment Management, Inclusion, Intellectual Curiosity, Learning Agility + 26 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date",
        "skills": [
            "R",
            "survey design",
            "Data Analysis",
            "Statistical Techniques",
            "Data Modeling",
            "Data Visualization"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "S&P Global Market Intelligence",
        "experience": "3-5 Years",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "About the Role:\n10\nThe Team:\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\nThe Impact:\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\nWhat's in it for you:\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\nKey Responsibilities\nDesign, Develop and Deploy ML powered products and pipelines\nPlay a central role in all stages of the data science project life cycle, including:\nIdentification of suitable data science project opportunities\nPartnering with business leaders, domain experts, and end-users to gain business understanding, data understanding, and collect requirements\nEvaluation/interpretation of results and presentation to business leaders\nPerforming exploratory data analysis, proof-of-concept modelling, model benchmarking and setup model validation experiments\nTraining large models both for experimentation and production\nDevelop production ready pipelines for enterprise scale projects\nPerform code reviews & optimization for your projects and team\nSpearhead deployment and model scaling strategies\nStakeholder management and representing the team in front of our leadership\nLeading and mentoring by example including project scrums\nWhat We're Looking For:\n3+ years of professional experience in Data Science domain\nExpertise in Python (Numpy, Pandas, Spacy, Sklearn, Pytorch/TF2, HuggingFace etc.)\nExperience with SOTA models related to NLP and expertise in text matching techniques, including sentence transformers, word embeddings, and similarity measures\nExpertise in probabilistic machine learning model for classification, regression & clustering\nStrong experience in feature engineering, data preprocessing, and building machine learning models for large datasets.\nExposure to Information Retrieval, Web scraping and Data Extraction at scale\nOOP Design patterns, Test-Driven Development and Enterprise System design\nSQL (any variant, bonus if this is a big data variant)\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\nNice to have\nPrior work to show on Github, Kaggle, StackOverflow etc.\nCloud expertise (AWS and GCP preferably)\nExpertise in deploying machine learning models in cloud environments\nFamiliarity in working with LLMs\nWhat's In It For You\nOur Purpose:\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology-the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\nWe're more than 35,000 strong worldwide-so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\nOur Values:\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything you-and your career-need to thrive at S&P Global.\n\nOur benefits include:\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awards-small perks can make a big difference.\nFor more information on benefits by country visit:\nGlobal Hiring and Opportunity at S&P Global:\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n-----------------------------------------------------------\nEqual Opportunity Employer\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\nIf you need an accommodation during the application process due to a disability, please send an email to: and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -\n-----------------------------------------------------------\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority - Ratings - (Strategic Workforce Planning)",
        "skills": [
            "TF2",
            "sentence transformers",
            "feature engineering",
            "word embeddings",
            "SOTA models related to NLP",
            "data preprocessing",
            "similarity measures",
            "Regression",
            "Classification",
            "Spacy",
            "HuggingFace",
            "probabilistic machine learning",
            "text matching techniques",
            "Sql",
            "Clustering",
            "Linux Os",
            "Github",
            "Numpy",
            "Sklearn",
            "Pandas",
            "Python",
            "Git",
            "Pytorch",
            "Azure DevOps"
        ]
    },
    {
        "job_title": "Lead Data Scientist - Demand Forecasting",
        "company_name": "Asper.ai",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Asper.ai:\nAt Asper.ai, we are transforming organizations into adaptive, intelligent enterprises by enabling autonomic decisioning at the intersection of demand and supply. Powered by cutting-edge AI and machine learning, our platform automates complex decision-making processes, unlocking growth and optimizing workflows across industries like CPG, retail, and supply chain management. As part of the Fractal family, we aim to revolutionize business decision-making by delivering data-driven insights that drive measurable results.\nAbout the Role:\nWe are seeking an experienced Lead Data Scientist to join our growing team. In this pivotal role, you will harness your expertise in machine learning, deep learning, and data science to solve complex business challenges, particularly in demand forecasting, supply chain optimization, and price recommendation.\nJob Title: Lead Data Scientist\nLocation: Bangalore | Hybrid\nReporting to: Director of Data Science\nPosition Type: Senior Individual Contributor\nWhat you are going to do\nConduct exploratory data analysis to uncover trends, patterns, and insights from complex\ndatasets\nDesign, train, and deploy machine learning and deep learning models for time series use cases\nsuch as demand forecasting, inventory management, and pricing\nExtract data insights, create visualizations, and communicate findings to technical and non-\ntechnical stakeholders\nBuild and maintain data science pipelines for model development, deployment, monitoring,\nand updates\nCollaborate with engineering, product, and business teams to integrate models into\nproduction systems and align with client goals\nResearch and prototype new algorithms and modelling techniques to improve solution\nperformance\nMonitor and evaluate model performance and iterate based on finding.\nWhat we are looking for\n6+ years of experience in data science, machine learning, or analytics roles\nExperience with time series analysis, forecasting, and predictive modelling.\nProficient in Python (NumPy, pandas, Scikit-learn, TensorFlow, or PyTorch), SQL, and Excel\nFamiliarity with deploying machine learning solutions and managing model lifecycle\nAbility to work with engineering and product teams to scope and deliver projects\nAbility to explain modelling choices, data assumptions, and outcomes to stakeholders\nExposure to statistical methods, optimization, and model monitoring frameworks\nPrior experience working in CPG, FMCG, or retail industries is a plus.\nWhy Join Asper.ai\nPlay a crucial role in transforming how businesses in the CPG, retail, and supply chain industries make data-driven decisions.\nWork on cutting-edge AI/ML projects that push the boundaries of decision-making automation.\nCollaborative and inclusive work culture that encourages innovation and personal growth.\nBe part of a fast-growing, innovative AI company with opportunities for career development and leadership.\nWhat We Offer:\nCompetitive salary and benefits package.\nA dynamic, inclusive work environment that promotes continuous learning and growth.\nOpportunities to work on impactful projects that will shape the future of decision-making at scale\nReady to make a difference and be part of an AI-driven revolution If you're passionate about data science and solving complex challenges in the CPG, FMCG, or retail industries, we'd love to hear from you!",
        "skills": [
            "Scikit-learn",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Pandas",
            "Machine Learning",
            "Pytorch",
            "Data Science",
            "Excel",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Roche Pharmacutical Holding",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Pharmaceutical",
        "job_description": "At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nAs a Data Scientist you will join the data science cluster in the Roche Informatics Data and Analytics Chapter (DnA). You will be part of one or several multi-disciplinary agile teams where you'll actively shape the future of healthcare by using data science methods and principles to generate deeper insights from a great variety of data sources. To achieve this, you will proactively identify needs, design and implement analytical solutions, provide advice and consulting support to our key stakeholders and show impact by executing proof-of-value initiatives, or contributing to existing products.\nAs a Data Scientist you will:\nApply your expertise in NLP/LLM to develop and refine models that address Roche business needs. Involved in building and fine-tuning models and optimising their performance to provide valuable insights and solutions to business stakeholders\nSupport prioritisation efforts, understand feasibility and business impact, take smart risks to make informed decisions in a fast-paced, evolving environment to deliver patient benefits faster.\nCollaborate within global agile teams in the Roche Informatics business and foundational domains to develop products that provide the highest value to both Roche Pharma and Diagnostics business stakeholders.\nProvide methodical and implementation guidance as well as hands-on support around analytical LLM/NLP use cases. Evaluate the pros & cons of different NLP approaches and Generative AI platforms with comprehensive quantitative and qualitative analysis\nCommunicate findings and market the value of use cases to key stakeholders.\nContribute to positioning data science as a key competency within the enterprise\nContinuously look for opportunities to broaden knowledge, capabilities and skill set to enable talent to flow into different specialties.\nBe a role model for knowledge sharing within the DnA chapter.\nAct as a coach, mentor, or buddy to help colleagues grow and develop.\nQualifications\nM.Sc. or PhD in Computer Science, Physics, Statistics, Mathematics or equivalent degree and experience with machine learning/data mining/artificial intelligence.\nExperience of working as a hands-on data scientist in pharmaceutical industry is preferred.\nHands-on experience with Python programming and common NLP libraries (e.g., transformers, gensim, spaCy, etc.)\nFamiliarity with essential frameworks (e.g. PyTorch) and infrastructure components (Docker, GPU) for training, fine-tuning and evaluating NLP tasks\nExperience in using both open source (e.g. HuggingFace) and closed source LLM models with different deep learning architectures\nExperience implementing RAG, working with knowledge databases and using LLM through APIs\nGood knowledge of effective training and optimising language models to fit for internal infrastructure and ensure seamless integration\nFamiliarity with best practices for code generation, code documentation, data security, and compliance in cloud-based data science workflows.\nProven experience to add value and insight by providing advanced analytical solutions.\nData storytelling skills and using visualisation tools to communicate data and results with a non-technical audience.\nInternational, goal oriented mindset with can do attitude.\nFluency in written and spoken English.\nWho we are\nA healthier future drives us to innovate. Together, more than 100'000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet's build a healthier future, together.\nRoche is an Equal Opportunity Employer.",
        "skills": []
    },
    {
        "job_title": "Lead Data Scientist - Computer Vision & OCR",
        "company_name": "Yubi",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nYubi stands for ubiquitous. But Yubi will also stand for transparency, collaboration, and the power of possibility.\n\nFrom being a disruptor in India's debt market to marching towards global corporate markets from one product to one holistic product suite with seven products\n\nYubi is the place to unleash potential. Freedom, not fear. Avenues, not roadblocks. Opportunity, not obstacles.\n\nAbout Yubi\n\nYubi, formerly known as CredAvenue, is re-defining global debt markets by freeing the flow of finance between borrowers, lenders, and investors. We are the world's possibility platform for the discovery, investment, fulfillment, and collection of any debt solution. At Yubi, opportunities are plenty and we equip you with tools to seize it.\n\nIn March 2022, we became India's fastest fintech and most impactful startup to join the unicorn club with a Series B fundraising round of $137 million.\n\nIn 2020, we began our journey with a vision of transforming and deepening the global institutional debt market through technology. Our two-sided debt marketplace helps institutional and HNI investors find the widest network of corporate borrowers and debt products on one side and helps corporates to discover investors and access debt capital efficiently on the other side. Switching between platforms is easy, which means investors can lend, invest and trade bonds - all in one place. All of our platforms shake up the traditional debt ecosystem and offer new ways of digital finance.\n\nYubi Credit Marketplace - With the largest selection of lenders on one platform, our credit marketplace helps enterprises partner with lenders of their choice for any and all capital requirements.\n\nYubi Invest - Fixed income securities platform for wealth managers & financial advisors to channel client investments in fixed income\n\nFinancial Services Platform - Designed for financial institutions to manage co-lending partnerships & asset based securitization\n\nSpocto - Debt recovery & risk mitigation platform\n\nCorpository - Dedicated SaaS solutions platform powered by Decision-grade data, Analytics, Pattern Identifications, Early Warning Signals and Predictions to Lenders, Investors and Business Enterprises\n\nSo far, we have on-boarded over 17000+ enterprises, 6200+ investors & lenders and have facilitated debt volumes of over INR 1,40,000 crore.\n\nBacked by marquee investors like Insight Partners, B Capital Group, Dragoneer, Sequoia Capital, LightSpeed and Lightrock, we are the only-of-its-kind debt platform globally, revolutionizing the segment.\n\nAt Yubi, People are at the core of the business and our most valuable assets. Yubi is constantly growing, with 1000+ like-minded individuals today, who are changing the way people perceive debt. We are a fun bunch who are highly motivated and driven to create a purposeful impact. Come, join the club to be a part of our epic growth story.\n\nResponsibilities\n\nThis particular role is within our Yubi Invest vertical, and you would get to work on building our bonds platform, called Aspero, for retail users.\nBe able to operate in ambiguous situations and define clear objectives by breaking down the narratives independently.\nWork closely with business, research, data and engineering teams to understand the user goals, market dynamics and ship products.\nAligning product strategy, proposition and roadmap with measurable metrics with all stakeholders.\nDrive PRDs, product planning, and product design of new features and enhancements.\nClearly communicate product and platform benefits to our users and internal stakeholders\n\nAbout The Role-\n\nWe're looking for a highly skilled, results-driven AI engineer who thrives in fast-paced, high-impact environments. If you are passionate about pushing the boundaries of Computer Vision, OCR, and Large Language Models (LLMs) and have a strong foundation in building and deploying AI solutions, this role is for you.\n\nAs a Senior Data Scientist, you will take ownership of designing and implementing state-of-the-art OCR and Computer Vision systems. This role demands deep technical expertise, the ability to work autonomously, and a mindset that embraces complex challenges head-on.\n\nHere, you won't just fine-tune pre-trained modelsyou'll be architecting, optimizing, and scaling AI solutions that power real-world applications.\n\nKey Responsibilities-\n\nArchitect, develop, and deploy high-performance Computer Vision and OCR models for real-world applications.\n\nImplement and optimize state-of-the-art OCR models such as Donut, TrOCR, LayoutLM, and DocFormer for document processing and information extraction.\n\nFine-tune and integrate LLMs (GPT, LLaMA, Mistral, etc.) to enhance text understanding and automation.\n\nDevelop custom deep learning models for large-scale image and document processing.\n\nBuild and optimize end-to-end AI pipelines, ensuring efficient data processing and model deployment.\n\nWork closely with engineers to operationalize AI models in production (Docker, FastAPI, TensorRT, ONNX).\n\nEnhance GPU performance and model inference efficiency, applying techniques such as quantization and pruning.\n\nStay ahead of industry advancements, continuously experimenting with new AI architectures and training techniques.\n\nWork in a highly dynamic, startup-like environment, balancing rapid experimentation with production-grade robustness.\n\nRequirements\n\n5-10 years experience p roven technical expertise Strong programming skills in Python, PyTorch, TensorFlow with deep experience in Computer Vision and OCR.\n\nHands-on experience in developing, training, and deploying OCR and document AI models.\n\nDeep understanding of Transformer-based architectures for vision and text processing.\n\nExperience working with Hugging Face, OpenCV, TensorRT, and NVIDIA GPUs for model acceleration.\n\nAutonomous problem solver You take initiative, work independently, and drive projects from research to production.\n\nStrong experience in scaling AI solutions, including model optimization and deployment on cloud platforms (AWS/GCP/Azure).\n\nThrives in fast-paced environments You embrace challenges, pivot quickly, and execute effectively.\n\nFamiliarity with MLOps tools (Docker, FastAPI, Kubernetes) for seamless model deployment.\n\nExperience in multi-modal models (Vision + Text).\n\nNice to Have-\n\nStrong background in vector databases, RAG pipelines, and fine-tuning LLMs for document intelligence.\n\nContributions to open-source AI projects.",
        "skills": [
            "Hugging Face",
            "TensorRT",
            "NVIDIA GPUs",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "Docker",
            "Opencv",
            "FastAPI",
            "Azure",
            "Python",
            "Kubernetes",
            "Ocr",
            "AWS",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Senior Applied Scientist / Data Scientist",
        "company_name": "Zscaler",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Zscaler\n\nServing thousands of enterprise customers around the world including 40% of Fortune 500 companies, Zscaler (NASDAQ: ZS) was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. As the operator of the world's largest security cloud, Zscaler accelerates digital transformation so enterprises can be more agile, efficient, resilient, and secure. The pioneering, AI-powered Zscaler Zero Trust Exchange platform, which is found in our SASE and SSE offerings, protects thousands of enterprise customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.\n\nNamed a Best Workplace in Technology by Fortune and others, Zscaler fosters an inclusive and supportive culture that is home to some of the brightest minds in the industry. If you thrive in an environment that is fast-paced and collaborative, and you are passionate about building and innovating for the greater good, come make your next move with Zscaler.\n\nOur Engineering team built the world's largest cloud security platform from the ground up, and we keep building. With more than 100 patents and big plans for enhancing services and increasing our global footprint, the team has made us and our multitenant architecture today's cloud security leader, with more than 15 million users in 185 countries. Bring your vision and passion to our team of cloud architects, software engineers, security experts, and more who are enabling organizations worldwide to harness speed and agility with a cloud-first strategy.\n\nResponsibilities\n\nWe are Seeking an Applied/Data Scientist with expertise in machine learning, AI, and deploying large language models. Focus on transforming SaaS business models using traditional and Generative AI. This role drives innovation, efficiency, and data-driven decisions. You will be reporting to Senior Manager, Data Science. You'll be responsible for:\n\nDeveloping and implementing machine learning models, including LLMs, to improve Zscaler's tools and selecting suitable statistical methods for data analysis\nConducting extensive research and development in advanced areas such as generative AI, predictive analytics, anomaly detection, and other emerging technologies\nCollaborating with engineering and product teams to seamlessly integrate AI/ML technologies into Zscaler's solutions\nAnalyzing large datasets to extract actionable insights, improve model accuracy, and enhance overall performance\nStaying updated with the latest advancements in machine learning, AI, and cybersecurity to ensure Zscaler remains a leader in innovation\n\nWhat We're Looking For (Minimum Qualifications)\n\n\nPhD (preferred) or advanced degree in CS, ML, Statistics, with experience in applied data science models like Regression, Decision Trees, and K-Means\nProcessing and analyzing large volumes of natural language data with over 5 years of experience in NLP\nProgramming and developing machine learning solutions using Python, SQL, and ML frameworks like TensorFlow and PyTorch\nStrong understanding of ML algorithms, deep learning, neural networks, and their applications in cybersecurity\n\nWhat Will Make You Stand Out (Preferred Qualifications)\n\n\nStrong background in mathematics, statistics, and their application in ML and AI, with experience in text and image generation models\nExperience with cloud computing platforms (AWS, Azure, GCP) and big data technologies (Hadoop, Spark)\nFamiliarity with SQL, NoSQL databases, and data engineering principles\n\nAt Zscaler, we are committed to building a team that reflects the communities we serve and the customers we work with. We foster an inclusive environment that values all backgrounds and perspectives, emphasizing collaboration and belonging. Join us in our mission to make doing business seamless and secure.\n\nBenefits\n\nOur Benefits program is one of the most important ways we support our employees. Zscaler proudly offers comprehensive and inclusive benefits to meet the diverse needs of our employees and their families throughout their life stages, including:\n\nVarious health plans\nTime off plans for vacation and sick time\nParental leave options\nRetirement options\nEducation reimbursement\nIn-office perks, and more!\n\nBy applying for this role, you adhere to applicable laws, regulations, and Zscaler policies, including those related to security and privacy standards and guidelines.\n\nZscaler is committed to providing equal employment opportunities to all individuals. We strive to create a workplace where employees are treated with respect and have the chance to succeed. All qualified applicants will be considered for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status, or any other characteristic protected by federal, state, or local laws.\n\nSee more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link.\n\nPay Transparency\n\nZscaler complies with all applicable federal, state, and local pay transparency rules.\n\nZscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support.",
        "skills": [
            "Ai",
            "Tensorflow",
            "Machine Learning",
            "Pytorch",
            "Hadoop",
            "Spark",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Supply Chain - Data Scientist, GCP-Senior Manager",
        "company_name": "PwC Acceleration Centers in India",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals. In data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\n\nGrowing as a strategic advisor, you leverage your influence, expertise, and network to deliver quality results. You motivate and coach others, coming together to solve complex problems. As you increase in autonomy, you apply sound judgment, recognising when to take action and when to escalate. You are expected to solve through complexity, ask thoughtful questions, and clearly communicate how things fit together. Your ability to develop and sustain high performing, diverse, and inclusive teams, and your commitment to excellence, contributes to the success of our Firm.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\nCraft and convey clear, impactful and engaging messages that tell a holistic story.\nApply systems thinking to identify underlying problems and/or opportunities.\nValidate outcomes with clients, share alternative perspectives, and act on client feedback.\nDirect the team through complexity, demonstrating composure through ambiguous, challenging and uncertain situations.\nDeepen and evolve your expertise with a focus on staying relevant.\nInitiate open and honest coaching conversations at all levels.\nMake difficult decisions and take action to resolve issues hindering team effectiveness.\nModel and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.\n\nPosition: Senior Associate\n\nIndustry: CPG\n\nDomain: Range of Analytics (Descriptive to Advanced) depending on the client problem\n\nAbout Acceleration Center Bangalore\n\nAt PwC, we connect people with diverse backgrounds and skill sets to solve important problems together and lead with purposefor our clients, our communities and for the world at large. It is no surprise therefore that 429 of 500 Fortune global companies engage with PwC. Acceleration Centers (ACs) are PwC's diverse, global talent hubs focused on enabling growth for the organization and value creation for our clients.The PwC Advisory Acceleration Center in Bangalore is part of our Advisory business in the US. The team is focused on developing a broader portfolio with solutions for Risk Consulting, Management Consulting, Technology Consulting,Strategy Consulting, Forensics as well as vertical specific solutions. PwC's high-performance culture is based on passion for excellence with focus on diversity and inclusion. You will collaborate with and receive support from a network of people to achieve your goals. We will also provide you with global leadership development frameworks and the latest in digital technologies to learn and excel in your career. At the core of our firm's philosophy is a simple construct: We care for our people. Globally PwC is ranked the 3rd most attractive employer according to Universum. Our commitment to Responsible Business Leadership, Diversity & Inclusion, work-life flexibility, career coaching and learning & development makes our firm one of the best places to work, learn and excel\n\nWe are looking for experienced leaders with a strong analytical background (and overall professional experience of 10+ years) to work in our Analytics Consulting practice in Mumbai, Bangalore.\n\nSenior Associate's will work as an integral part of business analytics teams in India alongside clients and consultants in the U.S., leading teams for high-end analytics consulting engagements and providing business recommendations to project teams.\n\nEducation: Advanced Degree in a quantitative discipline such as Computer Science, Engineering, Econometrics, Statistics, Operations Research or Information Sciences such as business analytics or informatics\n\nRequired Skills: Successful candidates will have demonstrated the following skills and characteristics:\n\nMust Have\n\nProven expertise in supply chain analytics across domains such as demand forecasting, inventory optimization, logistics, segmentation, and network design\nWell versed and hands-on experience of working on optimization methods like linear programming, mixed integer programming, scheduling optimization. Having understanding of working on third party optimization solvers like Gurobi will be an added advantage\nProficiency in forecasting techniques (e.g., Holt-Winters, ARIMA, ARIMAX, SARIMA, SARIMAX, FBProphet, NBeats) and machine learning techniques (supervised and unsupervised)\nStrong command of statistical modeling, testing, and inference\nProficient in using GCP tools: BigQuery, Vertex AI, Dataflow, Looker\nBuilding data pipelines and models for forecasting, optimization, and scenario planning\nStrong SQL and Python programming skills; experience deploying models in GCP environment\nKnowledge of orchestration tools like Cloud Composer (Airflow)\n\nNice To Have\n\nFamiliarity with MLOps, containerization (Docker, Kubernetes), and orchestration tools (e.g., Cloud composer)\nStrong communication and stakeholder engagement skills at the executive level\n\nRoles And Responsibilities\n\nAssist analytics projects within the supply chain domain, driving design, development, and delivery of data science solutions\nDevelop and execute on project & analysis plans under the guidance of Project Manager\nInteract with and advise consultants/clients in US as a subject matter expert to formalize data sources to be used, datasets to be acquired, data & use case clarifications that are needed to get a strong hold on data and the business problem to be solved\nDrive and Conduct analysis using advanced analytics tools and coach the junior team members\nImplement necessary quality control measures in place to ensure the deliverable integrity like data quality, model robustness, and explainability for deployments.\nValidate analysis outcomes, recommendations with all stakeholders including the client team\nBuild storylines and make presentations to the client team and/or PwC project leadership team\nContribute to the knowledge and firm building activities",
        "skills": [
            "SARIMA",
            "Supply Chain Analytics",
            "forecasting techniques",
            "NBeats",
            "Airflow",
            "Looker",
            "machine learning techniques",
            "Demand Forecasting",
            "optimization methods",
            "scheduling optimization",
            "Statistical Modeling",
            "SARIMAX",
            "Linear Programming",
            "FBProphet",
            "Vertex AI",
            "Cloud Composer",
            "GCP tools",
            "logistics segmentation",
            "data pipelines",
            "Holt-Winters",
            "inventory optimization",
            "ARIMAX",
            "mixed integer programming",
            "Sql",
            "Network Design",
            "DataFlow",
            "BigQuery",
            "Python",
            "Arima"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Rapido",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Rapido\nWe are India's largest bike-taxi platform, steadily venturing into Auto, Delivery, Rental, and more.\nCurrently, present in 100 cities, we are growing close to 50% year-on-year with steady funding. We have changed the concept of intra-city travel and made last-mile connectivity affordable to all. Along with being the #1 choice of 40 million people, we have also built a solid captain base of over 5 million registered captains, who have bettered their lives with Rapido. As an employer, we provide a lot of ownership to our team members providing multiple avenues for them to grow within the company. You will only grow with us with the right balance of ambition, fun, and transparent work culture!\nJob Summary:\nAs a Senior Data Scientist your will be responsible for understanding product/business priorities,\nconducting opportunity analysis, defining product level metrics, setting metric targets, building data-driven hypothesis, and running high-quality experiments to hit those targets.A Senior Data Scientist should have an experienced of solving complex data analysis and machine learning problems in a real-world product setting. The role requires someone who is very comfortable working across teams, good at communicating and able to drive analysis and manage multiple streams. Your role would also require you to mentor junior Data Scientists in their learning and development. You will be part of a dynamic team and regularly work with colleagues from other verticals to co-ordinate and collaborate within an increasingly interconnected Data Science team here @ Rapido.\nJob Responsibilities:\nTranslating business requirements into analytical solutions\nDo hands-on data analysis, using statistical techniques\nPre-processing of structured and unstructured data\nShould be able to steer direction related to improving data collection for analytical systems\nShould be proficient in model development, validation and then roll-out\nUnderstand business challenges and goals to formulate the approach for data analysis and\nmodel creation\nTune the model(s) to improve results provided over time\nCreate an analytics roadmap for the project\nCollaborate with a technology/data engineering team to transfer the business understanding, get\nthe model productionized and validate the output\nWork in highly collaborative teams that strive to build quality systems and provide business value\nArticulate insights and story telling for the senior management\nMentor Junior Data team members in their learning and development\nJob Requirements:\nWork experience - 4 years as a Data Scientist or equivalent position and overall 6 years\nMasters or Ph.D. in Statistics, Math, CS, Econ, Physics, Engineering or related scientific\ndisciplines;\nPassion for understanding business problems and trying to address them by leveraging data -\ncharacterized by high-volume, high dimensionality from multiple sources\nExperience with building predictive statistical, behavioral or other models via supervised and\nunsupervised machine learning, statistical analysis, and other predictive modeling techniques.\nWorked in a big data environment before alongside a data engineering team (and data\nvisualization team, data and business analysts)\nStatistical knowledge - Proven experience in statistical methods like and not limited to Markov\nModels, Stochastic models, Bayesian Models, Classification Models, Cluster Analysis,\nMultivariate Stats, Regression Models, Ensemble Techniques\nMachine Learning - Prior work experience in one or more of these knowledge areas (domain\nagnostic): Price Modeling, Demand Forecasting, Recommender Systems, User Profiling, Fraud\nDetectors\nExperience in SQL is must\nTechnologies - Proficiency in Python (must have) and any other prog. language; Specific libraries\nmay include - Scikit-learn, pandas, TensorFlow, Keras, Torch, Caffe, Theano, etc;\nExperience of Pyspark is a plus\nExperience with A/B Testing framework\nExperience in managing a small team of data scientists is a plus\nWhat's there for you\nIf you are excited to solve Complex Business problems at scale where you will cater to 10 million\nrides in a day across 100 cities, no better place to be.\nYou can go swimming in our 100s of TBs of data on self-managed databases\nOpportunity to contribute in solving challenging problems like supply-demand matching, demand forecasting, ROI optimization, Segmentation, Causal Inference and many more.\nWe have a very flat hierarchy and collaboration is our success mantra\nChance to work with a lean team of highly talented and motivated people across all levels\nYou will be working with cross functional teams and have a great learning curve and business\nexposure.",
        "skills": [
            "Scikit-learn",
            "Torch",
            "theano",
            "Caffe",
            "Pyspark",
            "Machine Learning",
            "Sql",
            "Tensorflow",
            "Pandas",
            "Keras",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Data Scientist Lead - Focused Analytics Solutions Vice President",
        "company_name": "JPMorganChase",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nWe have an exciting opportunity for you to lead impactful data science projects and advance your career.\n\nAs a Data Scientist Lead within the Focused Analytics Solutions Team, you will design, build, and deliver analytical solutions that drive business outcomes. You will be part of a dynamic team that values creativity, problem-solving, and client interaction.\n\nJob Responsibilities\n\nLead teams of data scientists to deliver end-to-end analytical projects.\nServe as a trusted advisor throughout project lifecycles.\nGuide teams in synthesizing findings for clients and executives.\nEstablish and manage relationships with internal clients.\nRecruit, develop, and retain talent through open communication.\nSet standards of excellence for the team.\nMaintain a rigorous controls environment for accurate results.\n\nRequired Qualifications, Capabilities, And Skills\n\n10+ years of industry experience in data science or business analytics.\nExperience leading project teams and mentoring talent.\nKnowledge of statistical, data science, and machine learning methodologies.\nExcellent communication skills for conveying complex information.\nFamiliarity with various analytical project types.\nProficiency in SQL, Python, Tableau, and big data technologies.\nMaster's degree in a relevant quantitative field.\n\nPreferred Qualifications, Capabilities, And Skills\n\nAdvanced degree in a relevant quantitative field.\nDirect people management experience.\nFinancial services experience.\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We're proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions all while ranking first in customer satisfaction.\n\nThe CCB Data & Analytics team responsibly leverages data across Chase to build competitive advantages for the businesses while providing value and protection for customers. The team encompasses a variety of disciplines from data governance and strategy to reporting, data science and machine learning. We have a strong partnership with Technology, which provides cutting edge data and analytics infrastructure. The team powers Chase with insights to create the best customer and business outcomes.",
        "skills": [
            "machine learning methodologies",
            "Big Data Technologies",
            "Tableau",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "NielsenIQ",
        "experience": "3-6 Years",
        "salary": null,
        "location": "India",
        "industry": "Market Research",
        "job_description": "Job Description\n\nPlans and completes assignments independently within an established framework, breaking down complex tasks, making reasonable decisions. Work is reviewed for overall technical soundness\nParticipates in data experiments and PoCs, setting measurable goals, timelines and reproducible outcomes. Applies critical thinking and takes initiative\nConsistently challenges and analyzes data to ensure accuracy\nEnsures documentation and establishes process flows,RACI, specifications for Technology for smooth deployment in Data Science\nWorks with direct manager, colleagues, and other stakeholders such as Technology and Operations\nTakes direction from manager and senior colleagues\nCommunicates progress, identifies blocking issues & actively collaborates with team\nAble to design Dashboards of KPIs with minimal supervision as well as the associated process to generate them\n\nQualifications\n\nProfessionals with degrees in Maths, Data Science, Statistics, or related fields involving statistical analysis of large data sets\n3-6 years of experience in market research or relevant field\nAbility to manipulate, analyze and interpret large data sources\nExperienced in high-level programming languages (f.e. Python, SQL, ), as well as with data visualization tools (e.g. Power BI)\nAble to work with Planning tools as Jira or similar\nKnowledge of cloud computation and storage\nDomain expertise in Data Science related processes\n\nAdditional Information\n\nOur Benefits\n\nFlexible working environment\nVolunteer time off\nLinkedIn Learning\nEmployee-Assistance-Program (EAP)\n\nAbout NIQ\n\nNIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insightsdelivered with advanced analytics through state-of-the-art platformsNIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.\n\nFor more information, visit NIQ.com\n\nWant to keep up with our latest updates\n\nFollow us on: LinkedIn | Instagram | Twitter | Facebook\n\nOur commitment to Diversity, Equity, and Inclusion\n\nNIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center: https://nielseniq.com/global/en/news-center/diversity-inclusion",
        "skills": [
            "Cloud computation and storage",
            "Power Bi",
            "Jira",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Ray Business Technologies (A CMMI Level 3 Company)",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Ray Business Technologies is Hiring for Senior Data Scientist Role.\nRoles and Responsibilities:\nTranslating business requirements into analytical solutions\nDo hands-on data analysis, using statistical techniques\nDesign and implement AI/ML solutions using advanced frameworks and technologies, ensuring scalability, efficiency, and alignment with business goals.\nDevelop and optimize GenAI frameworks like AutoGen, and NLP models tailored to specific use cases.\nBuild and deploy production-ready RAG (Retrieval-Augmented Generation) systems, chatbots, and other AI-driven applications using OpenAI APIs, Ollama, Llama, and llamaparse.\nLeverage Azure cloud services along with event-driven architecture in Python, to deliver high-performing AI solutions.\nApply advanced prompt engineering techniques, including Chain of Thought (CoT) prompting, for enhancing AI model interactions.\nUse Docker effectively, including executing Docker commands for containerization and deployments in cloud environments.\nEnsure solutions adhere to best practices in system design, addressing trade-offs, security, performance, and efficiency.\nPerforms code reviews and regression tests as well as triages and fixes issues to ensure the quality of code.\nCollaborates with others inside the project team to accomplish project objectives.\nLead a lean team of senior and associate software engineers and data scientists.\nCommunicate complex technical concepts effectively to both technical and non-technical stakeholders.\nQualifications:\nBachelor's/Master's degree in Engineering, Information Systems, Statistics, Math, Computer Science or related field and 6+ years of engineering work experience.\nProficiency in Python (mandatory).\nSkill Set; NLP, NLU, NLI\nExperience working with both structured and unstructured data.\n4+ years of hands-on experience with AI/ML frameworks (e.g., PyTorch, TensorFlow) and programming in Python.\nDemonstrated experience with RAG systems, chatbot development, and working with GenAI technologies like LLM fine-tuning and OpenAI APIs.\nDeep understanding of Autogen components and advanced NLP techniques.\nFamiliarity with best practices in system design, including security, performance optimization, and scalability.\nLevels of Responsibility:\nWork to be under supervision.\nCollaborate with founders and CTO towards the vision and direction of the product.\nRequires verbal and written communication skills to convey information. May require basic negotiation, influence, tact, etc.\nTasks do not have defined steps; planning, problem-solving, and prioritization must occur to complete the tasks effectively.\nIf Interested share your profile on [HIDDEN TEXT]",
        "skills": [
            "RAG systems",
            "GenAI technologies",
            "OpenAI APIs",
            "NLU",
            "NLI",
            "AI ML frameworks",
            "Tensorflow",
            "Nlp",
            "Pytorch",
            "Docker",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Poshmark",
        "experience": "6-9 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Senior Data Scientist\nAre you passionate about the power of data and excited to leverage cutting-edge AI/ML to drive business impact At Poshmark, we tackle complex challenges in personalization, trust & safety, marketing optimization, product experience, and more.\nWhy Poshmark\nAs a leader in Social Commerce, Poshmark offers an unparalleled opportunity to work with extensive multi-platform social and commerce data. With over 130 million users generating billions of daily events and petabytes of rapidly growing data, you'll be at the forefront of data science innovation. If building impactful, data-driven AI solutions for millions excites you, this is your place.\nWhat You'll Do\nDrive end-to-end data science initiatives, from ideation to deployment, delivering measurable business impact through projects such as feed personalization, product recommendation systems, and attribute extraction using computer vision.\nCollaborate with cross-functional teams, including ML engineers, product managers, and business stakeholders, to design and deploy high-impact models.\nDevelop scalable solutions for key areas like product, marketing, operations, and community functions.\nOwn the entire ML Development lifecycle: data exploration, model development, deployment, and performance optimization.\nApply best practices for managing and maintaining machine learning models in production environments.\nExplore and experiment with emerging AI trends, technologies, and methodologies to keep Poshmark at the cutting edge.\nYour Experience & Skills\nExperience: 6-9 years of building scalable data science solutions in a big data environment. Experience with personalization algorithms, recommendation systems, or user behavior modeling is a big plus.\nMachine Learning Knowledge: Hands-on experience with key ML algorithms, including CNNs, Transformers, and Vision Transformers. Familiarity with Large Language Models (LLMs) and techniques like RAG or PEFT is a bonus.\nTechnical Expertise: Proficiency in Python, SQL, and Spark (Scala or PySpark), with hands-on experience in deep learning frameworks like PyTorch or TensorFlow. Familiarity with ML engineering tools like Flask, Docker, and MLOps practices.\nMathematical Foundations: Solid grasp of linear algebra, statistics, probability, calculus, and A/B testing concepts.\nCollaboration & Communication: Strong problem-solving skills and ability to communicate complex technical ideas to diverse audiences, including executives and engineers.",
        "skills": [
            "Large Language Models",
            "PEFT",
            "CNNs",
            "RAG",
            "Vision Transformers",
            "Transformers",
            "Flask",
            "Sql",
            "Tensorflow",
            "Pyspark",
            "Machine Learning",
            "MLops",
            "Pytorch",
            "Python",
            "Docker",
            "Scala",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist (freelancer)",
        "company_name": "Soul AI",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are looking for a Data Scientist to join and build intelligent, data-driven solutions for our client that enable impactful decisions. This role requires contributions across the data science lifecycle from data wrangling and exploratory analysis to building and deploying machine learning models.\nWhether you're just getting started or have years of experience, we're looking for individuals who are curious, analytical, and driven to make a difference with data.\nResponsibilities:\nDesign, develop, and deploy machine learning models and analytical solutions.\nConduct exploratory data analysis and feature engineering.\nOwn or contribute to the end-to-end data science pipeline: data cleaning, modeling, validation, and deployment.\nCollaborate with cross-functional teams (engineering, product, business) to define problems and deliver measurable impact.\nTranslate business challenges into data science problems and communicate findings clearly.\nImplement A/B tests, statistical tests, and experimentation strategies.\nSupport model monitoring, versioning, and continuous improvement in production environments.\nEvaluate new tools, frameworks, and best practices to improve model accuracy and scalability.\nRequired Skills:\nStrong programming skills in Python including libraries such as pandas, NumPy, scikit-learn, matplotlib, seaborn.\nProficient in SQL, comfortable querying large, complex datasets.\nSound understanding of statistics, machine learning algorithms, and data modeling.\nExperience building end-to-end ML pipelines.\nExposure to or hands-on experience with model deployment tools like FastAPI, Flask, MLflow.\nExperience with data visualization and insight communication.\nFamiliarity with version control tools (e.g., Git) and collaborative workflows.\nAbility to write clean, modular code and document processes clearly.\nNice to Have:\nExperience with deep learning frameworks like TensorFlow or PyTorch.\nFamiliarity with data engineering tools like Apache Spark, Kafka, Airflow, dbt.\nExposure to MLOps practices and managing models in production environments.\nWorking knowledge of cloud platforms like AWS, GCP, or Azure (e.g., SageMaker, BigQuery, Vertex AI).\nExperience designing and interpreting A/B tests or causal inference models.\nPrior experience in high-growth startups or cross-functional leadership roles.\nEducational Qualifications:\nBachelor's or Master's degree in Computer Science, Data Science, Mathematics, Engineering, or a related field.\nPh.D. holders or candidates with demonstrated applied research contributions are a plus.",
        "skills": [
            "scikit-learn",
            "MLflow",
            "end-to-end ML pipelines",
            "Statistics",
            "Data Modeling",
            "Sql",
            "Numpy",
            "Seaborn",
            "Git",
            "Pandas",
            "Matplotlib",
            "Flask",
            "Data Visualization",
            "FastAPI",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Gen AI Data Scientist",
        "company_name": "Kyndryl India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who We Are\n\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\n\nAs a Data Scientist at Kyndryl you are the bridge between business problems and innovative solutions, using a powerful blend of well-defined methodologies, statistics, mathematics, domain expertise, consulting, and software engineering. You'll wear many hats, and each day will present a new puzzle to solve, a new challenge to conquer.\n\nYou will dive deep into the heart of our business, understanding its objectives and requirements viewing them through the lens of business acumen, and converting this knowledge into a data problem. You'll collect and explore data, seeking underlying patterns and initial insights that will guide the creation of hypotheses.\n\nIn this role, you will embark on a transformative process of business understanding, data understanding, and data preparation. Utilizing statistical and mathematical modeling techniques, you'll have the opportunity to create models that defy convention models that hold the key to solving intricate business challenges. With an acute eye for accuracy and generalization, you'll evaluate these models to ensure they not only solve business problems but do so optimally.\n\nAdditionally, you're not just building and validating models you're deploying them as code to applications and processes, ensuring that the model(s) you've selected sustains its business value throughout its lifecycle.\n\nYour expertise doesn't stop at data; you'll become intimately familiar with our business processes and have the ability to navigate their complexities, identifying issues and crafting solutions that drive meaningful change in these domains. You will develop and apply standards and policies that protect our organization's most valuable asset ensuring that data is secure, private, accurate, available, and, most importantly, usable. Your mastery extends to data management, migration, strategy, change management, and policy and regulation.\n\nIf you're ready to embrace the power of data to transform our business and embark on an epic data adventure, then join us at Kyndryl. Together, let's redefine what's possible and unleash your potential.\n\nYour Future at Kyndryl\n\nEvery position at Kyndryl offers a way forward to grow your career. We have opportunities that you won't find anywhere else, including hands-on experience, learning opportunities, and the chance to certify in all four major platforms. Whether you want to broaden your knowledge base or narrow your scope and specialize in a specific sector, you can find your opportunity here.\n\nWho You Are\n\nYou're good at what you do and possess the required experience to prove it. However, equally as important you have a growth mindset; keen to drive your own personal and professional development. You are customer-focused someone who prioritizes customer success in their work. And finally, you're open and borderless naturally inclusive in how you work with others.\n\nRequired Technical And Professional Expertise\n\nMinimum 5+ years of AI and Data consulting experience, or other relevant experience in AI & Analytics domain, with a proven track record of building and maintaining client relationships\nCollaborate with customers and account partners to identify new AI opportunities, and build proposals and pitch materials to position Kyndryl as a trusted partner for AI & Data transformation\nExperience in organizing and lead educational and ideation AI and Generative AI workshops for customers\nEvaluate and recommend appropriate AI frameworks, tools and platforms\nTo Develop end to end AI PoC projects using Python, Flask, FastAPI and Streamlit\nHave experience in cloud services of AWS / GCP / Azure to deploy AI products\nIn-depth understanding of AI technologies, including machine learning, deep learning, and generative AI techniques (such as generative adversial networks (GANs), variational autoencoders (VAEs), etc.)\nGood knowledge of AI frameworks, tools, and platform such as TensorFlow, PyTorch\nAffinity of GenAI libraries and tool sets included HuggingFace, LangChain, RAGAS and more\n\nPreferred Technical And Professional Expertise\n\nBachelor's degree in computer science, Information Security, or a related field Skilled in planning, organization, analytics, and problem-solving.\nExcellent communication and interpersonal skills to work collaboratively with clients and team members.\nComfortable working with statistics.\nAdvanced skills in PowerPoint and Excel.\nStrong Business acumen and understanding of industry trends and challenges in AI and Generative AI .\nStrong leadership skills and proactive and self-driven attitude.\nAdvanced communication and storytelling skills.\nHighly adaptable and flexible through shifting priorities.\n\nBeing You\n\nDiversity is a whole lot more than what we look like or where we come from, it's how we think and who we are. We welcome people of all cultures, backgrounds, and experiences. But we're not doing it single-handily: Our Kyndryl Inclusion Networks are only one of many ways we create a workplace where all Kyndryls can find and provide support and advice. This dedication to welcoming everyone into our company means that Kyndryl gives you and everyone next to you the ability to bring your whole self to work, individually and collectively, and support the activation of our equitable culture. That's the Kyndryl Way.\n\nWhat You Can Expect\n\nWith state-of-the-art resources and Fortune 100 clients, every day is an opportunity to innovate, build new capabilities, new relationships, new processes, and new value. Kyndryl cares about your well-being and prides itself on offering benefits that give you choice, reflect the diversity of our employees and support you and your family through the moments that matter wherever you are in your life journey. Our employee learning programs give you access to the best learning in the industry to receive certifications, including Microsoft, Google, Amazon, Skillsoft, and many more. Through our company-wide volunteering and giving platform, you can donate, start fundraisers, volunteer, and search over 2 million non-profit organizations. At Kyndryl, we invest heavily in you, we want you to succeed so that together, we will all succeed.\n\nGet Referred!\n\nIf you know someone that works at Kyndryl, when asked How Did You Hear About Us during the application process, select Employee Referral and enter your contact's Kyndryl email address.",
        "skills": [
            "LangChain",
            "AI and Data consulting",
            "generative AI",
            "HuggingFace",
            "RAGAS",
            "Streamlit",
            "Machine Learning",
            "Tensorflow",
            "Deep Learning",
            "Gcp",
            "Pytorch",
            "Flask",
            "FastAPI",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist - III",
        "company_name": "Accelon Inc.",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist (10 Positions)\nExperience: 68 Years\nLocation: Bangalore & Chennai (Hybrid)\nKey Responsibilities:\nDevelop and implement data models and algorithms using Python and SQL.\nAnalyze complex datasets to drive business insights and decisions.\nDesign and execute A/B tests and statistical experiments.\nBuild and maintain BI dashboards (Tableau preferred) for data visualization and reporting.\nCollaborate with cross-functional teams to understand data needs and deliver actionable solutions.\nPrimary Skills:\nStrong proficiency in Python and SQL\nExpertise in Statistics and A/B Testing\nHands-on experience with Business Intelligence tools, especially Tableau\nInterview Process:\nShort Screening Test for Python, SQL\nHackerRank Assessment\nTechnical Interview\nHiring Manager Discussion",
        "skills": [
            "Statistics",
            "Business Intelligence Tools",
            "Tableau",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Circle K",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nAlimentation Couche-Tard Inc., (ACT) is a global Fortune 200 company. A leader in the convenience store and fuel space with over 16,700 stores in 31 countries, serving more than 9 million customers each day. The India Data & Analytics Global Capability Centre is an integral part of ACT's Global Data & Analytics Team and the Data Scientist will be a key player on this team that will help grow analytics globally at ACT.\n\nAbout The Role\n\nThe hired candidate will partner with multiple departments, including Global Marketing, Merchandising, Global Technology, and Business Units. The incumbent will be responsible for delivering advanced analytics projects that drive business results including interpreting business, selecting the appropriate methodology, data cleaning, exploratory data analysis, model building, and creation of polished deliverables.\n\nResponsibilities\n\nAnalyze large-scale structured and unstructured data; develop deep-dive analyses and machine learning models in retail, marketing, merchandising, and other areas of the business\nUtilize data mining, statistical and machine learning techniques to derive business value from store, product, operations, financial, and customer transactional data\nApply multiple algorithms or architectures and recommend the best model with in-depth description to evangelize data-driven business decisions\nUtilize cloud setup to extract processed data for statistical modelling and big data analysis, and visualization tools to represent large sets of time series/cross-sectional data\nStructure hypothesis, build thoughtful analyses, develop underlying data models, and bring clarity to previously undefined problems\nPartner with Data Engineering to build, design and maintain core data infrastructure, pipelines, and data workflows to automate dashboards and analyses\nArticulate complex data science models to business teams and present the insights in easily understandable and innovative formats\n\nQualifications And Experience\n\nBachelor's degree required, preferably with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.)\nMaster's degree preferred (MBA/MS Computer Science/M.Tech Computer Science, etc.)\n34 years of relevant working experience in a data science/advanced analytics role\nKnowledge of Functional Analytics (Supply chain analytics, Marketing Analytics, Customer Analytics)\nKnowledge and ability to conduct statistical modelling using Analytical tools (R, Python, KNIME, etc.) and use big data technologies\nKnowledge of business intelligence & reporting (Power BI, Tableau, Alteryx, etc.)\nKnowledge of Enterprise reporting systems, relational (MySQL, Microsoft SQL Server etc.), non-relational database management systems and Data Engineering tools\nKnowledge and ability to use Big data technologies (Hadoop, Spark, Kafka, Presto etc) and Cloud computing services in Azure/AWS/GCP for data engineering, ML Ops\nAbility to delivery, strong disposition towards business and strong interpersonal communication\nIndividual must be organized, dependable, able to multi-task and manage priorities, display initiative, and must have the ability to work independently in a demanding, fast-paced environment.",
        "skills": [
            "R",
            "Alteryx",
            "Hadoop",
            "Power Bi",
            "Microsoft Sql Server",
            "Kafka",
            "Tableau",
            "Gcp",
            "Presto",
            "MySQL",
            "Spark",
            "Knime",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "RevX",
        "experience": "5-8 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Title: Lead Data Scientist\nAbout RevX:\nMade for Growth, Built for App Marketers.\nRevX helps app businesses acquire and reengage users via programmatic to retain, monetize, and accelerate revenue. We're all about taking your app businesses to a new growth level. We rely on data science, innovative technology, and AI, and a skilled team, to create and deliver seamless ad experiences to delight your app users. That's why RevX is the ideal partner for app marketers that demand trustworthy insights, a hands-on team, and a commitment to growth. We help you build sound mobile strategies, combining programmatic UA, app re engagement, and performance branding to drive real and verifiable results so you can scale your business: with real users, high retention, and incremental revenue.\nMajor Responsibilities:\nResearch and Problem-Solving: Identify and frame business problems, conduct exploratory data analysis, and propose innovative data science solutions tailored to business needs.\nLeadership & Communication: Serve as a technical referent for the research team, driving high-impact, high-visibility initiatives. Effectively communicate complex scientific concepts to senior stakeholders, ensuring insights are actionable for both technical and non-technical audiences. Mentor and develop scientists within the team, fostering growth and technical excellence.\nAlgorithm Development: Design, optimize, and implement advanced machine learning algorithms, including neural networks, ensemble models (XGBoost, random forests), and clustering techniques.\nEnd-to-End Project Ownership: Lead the development, deployment, and monitoring of machine learning models and data pipelines for large-scale applications.\nModel Optimization and Scalability: Focus on optimizing algorithms for performance and scalability, ensuring robust, well-calibrated models suitable for real-time environments.\nA/B Testing and Validation: Design and execute experiments, including A/B testing, to validate model effectiveness and business impact.\nBig Data Handling: Leverage tools like BigQuery, advanced SQL, and cloud platforms (e.g., GCP) to process and analyze large datasets.\nCollaboration and Mentorship: Work closely with engineering, product, and campaign management teams, while mentoring junior data scientists in best practices and advanced techniques.\nData Visualization: Create impactful visualizations using tools like matplotlib, seaborn, Looker, and Grafana to communicate insights effectively to stakeholders.\nRequired Experience/Skills:\n58 years of hands-on experience in data science or machine learning roles. 2+ years leading data science projects in AdTech Strong hands-on skills in Advanced Statistics, Machine Learning, and Deep Learning.\nDemonstrated ability to implement and optimize neural networks and other advanced ML models.\nProficiency in Python for developing machine learning models, with a strong grasp of TensorFlow or PyTorch.\nExpertise handling large datasets using advanced SQL and big data tools like BigQuery.\nIn-depth knowledge of MLOps pipelines, from data preprocessing to deployment and monitoring.\nStrong background in A/B testing, statistical analysis, and experimental design.\nProven capability in clustering, segmentation, and unsupervised learning methods.\nStrong problem-solving and analytical skills with a focus on delivering business value.\nEducation:\nA Master's in Data Science, Computer Science, Mathematics, Statistics, or a related field is preferred. A Bachelor's degree with exceptional experience will also be considered.\nFor more information visit www.revx.io",
        "skills": [
            "Random Forests",
            "Looker",
            "Clustering Techniques",
            "Advanced Statistics",
            "Experimental Design",
            "Machine Learning",
            "BigQuery",
            "Neural Networks",
            "Grafana",
            "Deep Learning",
            "Tensorflow",
            "Seaborn",
            "Pytorch",
            "MLops",
            "Matplotlib",
            "XGBoost",
            "Data Visualization",
            "Advanced Sql",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Mid-Level Data Scientist",
        "company_name": "neurIOT Labs",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Location:Noida\nType:Full-Time\nExperience Level:25 years\nIndustry:Artificial Intelligence, Machine Learning, Data Science\nAbout the Role\nWe are looking for a self-motivatedMid-Level Data Scientistto join our AI team focused onGenAIapplications. We work at the intersection of multi-modal modeling, Retrieval-Augmented Generation (RAG), and real-time machine learning systems. You'll collaborate with a high-impact team to design, prototype, and deploy next-generation AI solutions, especially around document understanding and multi-modal tasks.\nKey Responsibilities\nDesign and implement state-of-the-art GenAI solutions, involving multi-modal, document understanding models and agents.\nBuild and optimizeRAG pipelines, including knowledge of various RAG architectures.\nDevelop and maintainagentic workflowsusing tools likeLangGraph, LangChain.\nWork with large-scale datasets and ensure efficient data processing pipelines.\nPerform statistical analysis, algorithm development, and performance tuning.\nWorking with opensource LLMs and deploying them on serving frameworks such as sglang and vllm.\nStay up to date with the latest developments in GenAI and ML, and actively contribute to knowledge sharing.\nRequired Qualifications\nBachelor's degree (Master's preferred) in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 3 years of experience working in machine learning, data science, or AI roles.\nStrong command ofPythonand familiarity withRor other scripting languages.\nHands-on experience withdeep learning,transformer-based models, andmulti-modal learning.\nProficiency in AI/ML frameworks and libraries (e.g., PyTorch, TensorFlow, Hugging Face Transformers).\nStrong understanding of statistics, linear algebra, and probability theory.\nExperience working withcloud environments, preferablyAzure.\nExposure toOpenAI,Anthropic,Mistral, or similar APIs and deployment ofopen-source models(LLaMA, MPT, etc.).\nDemonstrated experience indocument AI,vision-language models, orOCR-based understanding systems.\nPreferred Skills\nExperience withLangGraph,CrewAI,Autogen, or similar orchestration frameworks.\nWorking knowledge ofvector databases(e.g., Qdrant, Weaviate, Pinecone) andembedding search techniques.\nExposure toKubernetes,Docker, orML model deployment workflows.\nCuriosity-driven mindset with a passion for learning and experimenting with the latest in AI research.\nWhy Join Us\nBe part of a team working on powerful AI applications\nAccess to cutting-edge tools and open models\nFlexible working hours\nSupportive environment that encourages innovation, research, and upskilling",
        "skills": [
            "embedding search techniques",
            "Hugging Face Transformers",
            "Qdrant",
            "vision-language models",
            "vector databases",
            "Pinecone",
            "Autogen",
            "LangGraph",
            "R",
            "Mistral",
            "multi-modal learning",
            "CrewAI",
            "Anthropic",
            "OCR-based understanding systems",
            "cloud environments",
            "OpenAI",
            "document AI",
            "Weaviate",
            "transformer-based models",
            "Tensorflow",
            "Deep Learning",
            "Pytorch",
            "Docker",
            "Python",
            "Azure",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "HERE Technologies",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "What's the role\n\nWe are seeking a highly skilled and motivated Data Scientist to join our team. As a Data Scientist, you will develop and implement logic and algorithms to generate hazard events using vehicle sensor data. This role is pivotal in enhancing vehicle safety and operational efficiency by leveraging data science techniques to predict and identify potential hazards.\n\nData Analysis & Interpretation - Analyse large datasets from vehicle sensors to identify patterns, trends, and anomalies related to hazard events.\nDevelop, implement, and optimise algorithms to detect and predict hazard events. Utilise machine learning, statistical modelling, and data mining techniques to enhance prediction accuracy.\nCreate and validate predictive models to simulate various driving conditions and potential hazards.\nWork closely with cross-functional teams, including engineers, product managers, and stakeholders, to understand requirements and deliver solutions.\nStay updated with the latest advancements in data science, machine learning, and vehicle sensor technology.\n\nWho are you\n\nA data scientist with experience in real-world data analysis and algorithm development for commercially profitable products. You excel in using clustering algorithms, designing mathematical models from first principles, and staying current with state-of-the-art machine learning techniques.\n\nBachelor's or Master's or Ph.D. in computational science or engineering with 5+ years of professional experience.\nUnderstanding of statistical analysis of data and mathematical modelling\nKnowledge of Clustering, Regression, Forecasting\nProficiency in Python and Machine Learning techniques (Prediction, classification)\nExperience with the AWS AI/ML tech stack\nStrong communication skills to interact with both technical and non-technical stakeholders.\nAbility to specify, explain, and execute modelling solutions in collaboration with other data scientists and engineers.\n\nNice To Have\n\nDeep learning\nComputer vision\nAWS Sagemaker\n\nHERE is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, age, gender identity, sexual orientation, marital status, parental status, religion, sex, national origin, disability, veteran status, and other legally protected characteristics\n\nWho are we\n\nHERE Technologies is a location data and technology platform company. We empower our customers to achieve better outcomes from helping a city manage its infrastructure or a business optimize its assets to guiding drivers to their destination safely.\n\nAt HERE we take it upon ourselves to be the change we wish to see. We create solutions that fuel innovation, provide opportunity and foster inclusion to improve people's lives. If you are inspired by an open world and driven to create positive change, join us. Learn more about us on our YouTube Channel.",
        "skills": [
            "Forecasting",
            "Data Analysis Interpretation",
            "AWS AI ML tech stack",
            "Regression",
            "Machine Learning",
            "Data Mining",
            "Statistical Modelling",
            "Clustering",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "XLSCOUT",
        "experience": "Fresher",
        "salary": null,
        "location": "Mohali, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the job\nAre you passionate about AI and eager to work on groundbreaking projects\nDo you want to be part of a high-impact team in a fast-growing AI startup Do you thrive in a dynamic environment, collaborating with industry leaders and building state-of-the-art AI solutions Here's your chance!\nAbout the Role\nAs a Data Scientist at XLSCOUT, you will be working with cutting-edge AI technologies to revolutionize IP and R&D intelligence. You'll develop and deploy Natural Language Processing (NLP) solutions for analyzing large-scale text data and contribute to our AI-driven patent analytics platform.\nYour Responsibilities\nDevelop machine learning and deep learning models for NLP-based text analysis.\nWork on sequence models such as RNN, LSTM, GRU, and transformers (BERT, GPT).\nImplement text preprocessing techniques (tokenization, POS tagging, parsing, embeddings).\nResearch and fine-tune open-source LLMs like LLaMA 2, Mistral, and Falcon for AI applications.\nDeploy models on AWS/GCP and integrate with cloud-based AI solutions.\nCollaborate with engineers, data scientists, and domain experts to improve AI-driven patent research.\nAbout XLSCOUT\nXLSCOUT headquartered in Toronto, Canada, is a leading AI-powered IP intelligence platform. We leverage Artificial Intelligence, Machine Learning, and Big Data Analytics to analyze 100M+ technology documents across 90+ countries. Our mission is to empower IP and R&D teams worldwide through cutting-edge NLP technologies.\nWho You'll Work With\nJoin a team of experts, innovators, and AI enthusiasts dedicated to transforming intellectual property research with cutting-edge AI solutions.\nJitin Talwar Founder & AI/IP Expert\nJitin is a visionary leader in AI-driven IP intelligence. He is a Chevening Gurukul Fellow from the University of Oxford - where he gained advanced insights into business strategy, innovation, and emerging technologies. With a diverse academic background spanning engineering, law, and business, he began his journey in engineering at Punjab Engineering College. He later pursued an LL.B. from Panjab University, specializing in patent law and technology commercialization. Jitin's passion for leveraging AI in the legal and IP domain led him to establish XLSCOUT, where he has pioneered the integration of AI and NLP in patent analytics empowering businesses globally to accelerate innovation and streamline IP research.\nSandeep Singh Kohli AI & Data Science Leader\nSandeep is a seasoned AI researcher and data scientist specializing in Natural Language Processing (NLP) and Deep Learning. He completed a program in AI from the University of Toronto, gaining expertise in Machine Learning, NLP, Reinforcement Learning, and LLMs, and holds a Design Thinking certificate from the Indian School of Business (ISB). He also holds a B.Tech degree from Kurukshetra University and a PG Diploma in Patent Law from NALSAR University.\nAt XLSCOUT, Sandeep leverages this diverse background to build AI-driven patent intelligence solutions that support global R&D teams in making faster, data-informed innovation decisions.\nHongyi Ding AI Research & Innovation\nHongyi is an AI researcher specializing in large language models (LLMs), deep learning, and computational linguistics. He earned his Master of Science in Applied Computing (Data Science concentration) from the University of Toronto and holds an Honours degree in Mathematics and Statistics Co-op from McMaster University, where he received accolades including the French Government Book Prize and the Provost's Honour Roll Medal for academic excellence.\nAt XLSCOUT, he leads the research and development of advanced NLP models, driving innovation in patent search and IP analytics. His work in LLM fine-tuning and AI ethics ensures the delivery of high-performance, responsible AI solutions.\nWho You Are\nWe are looking for a driven, high-energy AI professional who thrives in fast-paced environments. If you're passionate about AI, NLP, and deep learning, this role is for you.\nYou love innovation You are always eager to experiment with new AI/ML models.\nYou thrive in fast-paced environments You enjoy building and shipping AI products at scale.\nYou take ownership You're an independent problem solver who owns projects from start to finish.\nWhat You Bring\nMust Haves:\nStrong programming skills in Python.\nSolid understanding of machine learning algorithms.\nHands-on experience in NLP techniques, including document classification, topic modeling, clustering, keyword extraction.\nExperience with deep learning models (GRU, RNN, LSTM) and familiarity with advanced NLP models like BERT, GPT.\nKnowledge of text preprocessing techniques (Tokenization, POS tagging, etc.).\nFamiliarity with PyTorch and/or TensorFlow for deep learning.\nExposure to model deployment on cloud platforms like AWS/GCP.\nProficiency in Flask for developing software applications.\nKnowledge of open-source LLMs (LLaMA 2, Mistral, etc.), with fine-tuning expertise for specific applications.\nPreferred Haves:\nBE/B.Tech in any discipline.\nAbility to solve problems related to patent searches and construction.\nStrong communication skills.\nUnderstanding of technical invention disclosures.\nNice To Haves:\nExperience in working with AI-based products and startups.\nExposure to advanced AI technologies in NLP and deep learning.\nPrevious experience with cloud infrastructure and model deployment.\nWhat We Offer\nCompetitive total compensation (Base + Performance incentives).\nEquity ownership Take part in XLSCOUT's success and growth.\nOpportunity to work with a world-class AI team building industry-defining products.\nA fast-growing, dynamic environment where you can make a real impact.\nReady to be part of the future of AI-driven IP research Apply now and join the revolution!",
        "skills": [
            "LSTM",
            "Text Preprocessing Techniques",
            "GRU",
            "LLaMA 2",
            "GPT",
            "POS Tagging",
            "Mistral",
            "BERT",
            "Open-source LLMs",
            "tokenization",
            "Machine Learning",
            "Deep Learning",
            "Tensorflow",
            "Rnn",
            "Pytorch",
            "Gcp",
            "Flask",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Commonwealth Bank",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Organization:\n\nAt CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\n\nJob Title: Senior Data Scientist\n\nLocation: Bangalore\n\nBusiness & Team: Analytics BB\n\nImpact & contribution:\n\nAs a Senior Data Scientist, you will have the opportunity to apply your quantitative and computational skills within an applied and production-oriented R&D function within the group, focusing on cutting-edge deep learning and generative AI techniques. Your role will have a significant impact on our innovation capabilities and business processes by leveraging these advanced technologies\n\nRoles & Responsibilities:\n\nAdvanced Model Development: Lead the design and implementation of advanced Generative AI solutions, including LLMs, GANs, VAEs, Diffusion Models, and multi-modal systems.\nOptimize models for deployment in multi-agent settings, ensuring agent-level scalability and robustness.\nMulti-Agent System Design: Architect and deploy multi-agent systems, enabling AI agents to collaborate, share knowledge, and solve tasks dynamically.\nDevelop frameworks for agents to interact effectively using generative AI for reasoning, task execution, and learning.\nProduction Deployment: Manage and oversee the deployment of at least one Generative AI model or multi-agent system in production, ensuring operational excellence and high availability.\nStrategic Leadership: Collaborate with stakeholders to identify opportunities for Generative AI and multi-agent systems, define use cases, and create actionable strategies for implementation.\nDrive thought leadership in multi-agent architectures and Generative AI innovation.\nMentorship: Mentor junior team members in building and deploying Generative AI models and multi-agent systems.\nResearch and Development: Drive internal research initiatives and contribute to the AI community through papers, patents, or open-source projects in Generative AI and multi-agent systems.\n\nEssential Skills:\n\n10+ years of experience\nAdvanced knowledge of multi-agent design principles, agent-based simulations, and reinforcement learning for multi-agent systems.\nExpertise in transformers, GANs, VAEs, and Diffusion Models.\nExperience with tools like Ray, DeepSpeed, and cloud-based deployment pipelines.\nStrong grasp of optimization techniques like quantization, pruning, and knowledge distillation\n\nEducation Qualifications: Bachelor's degree in Engineering ( Computer Science/Information Technology)\n\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through Sidekick to submit a valid application. We're keen to support you with the next step in your career.\n\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\n\nAdvertising End Date: 20/05/2025",
        "skills": [
            "GANs",
            "multi-agent systems",
            "DeepSpeed",
            "Generative AI",
            "Transformers",
            "Diffusion Models",
            "Ray",
            "knowledge distillation",
            "pruning",
            "cloud-based deployment pipelines",
            "VAEs",
            "Optimization Techniques",
            "quantization"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ampstek",
        "experience": "5-7 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Title: Data Scientist\nLocation: India, Remote\nFulltime.\nJob Description:\nWhat does a Data Scientist do\nAs a data scientist, you will be responsible for analyzing and interpreting complex data sets to identify patterns and insights. You will be developing data models and algorithms to support the project needs. Implement data validation strategies to ensure high data quality. Collaborate with AI/ML engineers to integrate data models into the conversion engine system. Present findings and recommendations to stakeholders and management. Not being afraid to fail fast is one of the essential tools an NL Data Scientist can have in the cutting-edge technology market.\nWhat you will do:\nDevelop AI-Driven Data Conversion Application: Utilize AI technologies to develop a comprehensive data conversion application.\nData Mapping: Implement AI algorithms for accurate data mapping from source to target.\nCode Generation: Automate code generation using AI models.\nData Validation: Apply AI-driven validation techniques to ensure data integrity and quality.\nCode Conversion: Facilitate seamless code conversion using AI algorithms.\nCollaboration: Work with data engineers, software developers, AI solution engineers, and business analysts to integrate the data conversion application.\nMonitor and Troubleshoot: Continuously monitor and troubleshoot the AI-driven application to ensure optimal functionality.\nDocumentation: Maintain detailed documentation of methodologies and processes used in the application development.\nStay Updated with Technologies: Keep up-to-date with the latest AI, data science, and data conversion technologies to enhance the application.\nWhat you will need to have:\nBachelor's or Master's Degree in Data Science, Computer Science, Artificial Intelligence, or a related field.\nProficiency in Programming Languages such as Python, SQL, and other relevant languages.\nExperience with AI/ML Frameworks like TensorFlow, PyTorch, or similar.\nKnowledge of Data Mapping Techniques: Experience with AI-driven tools and methodologies for data mapping.\nStrong Code Generation Skills: Experience in developing AI models for automating code generation.\nExperience with Data Validation: Implementing AI-based validation techniques.\nFamiliarity with Code Conversion: Understanding AI algorithms for code conversion.\nExperience with Databases like SQL Server and MongoDB.\nCollaboration Skills: Ability to work effectively with cross-functional teams.\nProblem-Solving Skills: Strong ability to identify issues and develop creative solutions.\nAttention to Detail: Ensure accuracy and reliability of data conversions.\n5+ Years of Relevant Experience in data science or a related field.\nWillingness to Fail Fast and learn from mistakes in the fast-paced technology market.\nPreferred qualifications for consideration:\nExperience in the Financial Services Industry and an understanding of compliance standards.\nCertification in Data Science or AI/ML.\nExperience with Master data management, Data Wrangling and ETL Processes.\nFamiliarity with DevOps Tools like Jira, Confluence, and BitBucket.\nExperience with data and AI/ML Technologies: Such as NLP/NLU, Azure Cognitive Services, Azure Synapse Analytics, Azure data bricks and Azure ML service.\nPrevious Experience Delivering AI Solutions for complex data or conversions: Seamless Data Schema Conversion, AI-Driven Data Validation for Migration Accuracy, Intelligent Code Generation for Data Transformation Scripts , Historical Data Transformation and Archiving, Intelligent Error Detection and Correction, AI-Augmented Data Reconciliation",
        "skills": [
            "Azure ML Service",
            "Data Mapping Techniques",
            "Code Conversion",
            "AI-driven tools and methodologies",
            "code generation",
            "NLU",
            "Azure Cognitive Services",
            "Data Validation",
            "Azure Data Bricks",
            "SQL Server",
            "Sql",
            "Tensorflow",
            "Nlp",
            "Pytorch",
            "Azure Synapse Analytics",
            "MongoDB",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Lloyd Bedford Cox, Inc.",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Insurance",
        "job_description": "Introduction\n\nWe believe that every candidate brings something special to the table, including you! So, even if you feel that you're close but not an exact match, we encourage you to apply. We'd be thrilled to receive applications from exceptional individuals like yourself.\n\nGallagher, a global industry leader in insurance, risk management, and consulting services, boasts a team of over 50,000 professionals worldwide. Our culture, known as The Gallagher Way, is driven by shared values and a passion for excellence. At the heart of our global operations, the Gallagher Center of Excellence (GCoE) in India, founded in 2006, upholds the values of quality, innovation, and teamwork. With 10,000+ professionals across five India locations, GCoE is where knowledge-driven individuals make a significant impact and build rewarding, long-term careers.\n\nOverview\n\nWe are seeking a data scientist to join our Data Analytics team to help us make better business decisions based on our data. The ideal candidate will have a working knowledge of statistics, AL / ML algorithms, mathematics, and data science programming languages (e.g., SQL, R, and Python). Your primary responsibilities will be performing statistical analyses, exploratory data analysis, data research, study the data thoroughly, running custom SQL queries, and identifying patterns and trends that can improve our products and services efficiency and usability. Should be able to work as an individual contributor and provide robust insights, analysis and analytical reporting to interpret results and inform key business areas on trends and business insights. Maintain good relationship with stakeholders. Should be proactive to learn new skills per business requirement. Familiar with extraction of relevant data, cleanse and transform data into insights that drive business value, through use of data analytics, data visualization and data modeling techniques.\n\nHow You'll Make An Impact\n\nStatistical analysis: Identify patterns in data. This includes having a keen sense of pattern detection and anomaly detection.\nArtificial Intelligence & Machine learning Techniques: Expert level knowledge on AI / ML techniques. Implement algorithms and statistical models to enable a computer to automatically learn from data.\nStrong knowledge of Python & R, MS Azure ML Studio, AWS, or Google Cloud.\nKnowledge of data science toolkits such as R, NumPy, and MatLab.\nComputer science: Apply the principles of artificial intelligence, database systems, human/computer interaction, numerical analysis, and software engineering.\nProgramming: Write computer programs and analyze large datasets to uncover answers to complex problems. Data scientists need to be comfortable writing code working in a variety of languages such as Python, Java, R, and SQL.\nData storytelling: Communicate actionable insights using data, often for a non-technical audience.\nAnalytical thinking. Find analytical solutions to abstract business issues.\nBusiness intuition: Connect with stakeholders to gain a full understanding of the problems they're looking to solve.\nInterpersonal skills: Communicate across a diverse audience across all levels of an organization.\nCritical thinking: Apply objective analysis of facts before coming to a conclusion.\nInquisitiveness: Look beyond what's on the surface to discover patterns and solutions within the data.\nPerform data mining, exploration, and analysis.\nAbility to store and process unstructured data with NoSQL databases and machine learning models\nDesign, train, and implement machine learning algorithms.\nStrong attention to detail and accuracy of output.\nAdvanced knowledge of SQL and Excel.\nStrong Data Visualization skills preferably MS PowerBI.\nKeeping up to date with the latest tools and trends.\nExcellent verbal and written Communication skills\nLeverage predictive models to optimize customer experiences.\nCreating automated anomaly detection.\n\nAbout You\n\nTechnical Bachelor's Degree. Preferably in statistics, computer science, mathematics, or engineering.\n3+ year of relevant experience.\nStrong domain knowledge.\nShould be ready work in shifts.\n\nAdditional Information\n\nWe value inclusion and diversity\n\nInclusion and diversity (I&D) is a core part of our business, and it's embedded into the fabric of our organization. For more than 95 years, Gallagher has led with a commitment to sustainability and to support the communities where we live and work.\n\nGallagher embraces our employees diverse identities, experiences and talents, allowing us to better serve our clients and communities. We see inclusion as a conscious commitment and diversity as a vital strength. By embracing diversity in all its forms, we live out The Gallagher Way to its fullest.\n\nGallagher believes that all persons are entitled to equal employment opportunity and prohibits any form of discrimination by its managers, employees, vendors or customers based on race, color, religion, creed, gender (including pregnancy status), sexual orientation, gender identity (which includes transgender and other gender non-conforming individuals), gender expression, hair expression, marital status, parental status, age, national origin, ancestry, disability, medical condition, genetic information, veteran or military status, citizenship status, or any other characteristic protected (herein referred to as protected characteristics) by applicable federal, state, or local laws.\n\nEqual employment opportunity will be extended in all aspects of the employer-employee relationship, including, but not limited to, recruitment, hiring, training, promotion, transfer, demotion, compensation, benefits, layoff, and termination. In addition, Gallagher will make reasonable accommodations to known physical or mental limitations of an otherwise qualified person with a disability, unless the accommodation would impose an undue hardship on the operation of our business.",
        "skills": [
            "MS Azure ML Studio",
            "NoSQL databases",
            "Statistics",
            "R",
            "MS PowerBI",
            "Data Modeling",
            "Google Cloud",
            "Sql",
            "Numpy",
            "Data Visualization",
            "Excel",
            "Python",
            "AWS",
            "MatLab"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "McCormick & Company",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "At McCormick, we bring our passion for flavor to work each day. We encourage growth, respect everyone's contributions and do what's right for our business, our people, our communities and our planet. Join us on our quest to make every meal and moment better.\nFounded in Baltimore, MD in 1889 in a room and a cellar by 25-year-old Willoughby McCormick with three employees, McCormick is a global leader in flavour. With over 14,000 employees around the world and more than $6 Billion in annual sales, the Company manufactures, markets, and distributes spices, seasoning mixes, condiments and other flavourful products to the entire food industry, retail outlets, food manufactures, food service businesses and consumers.\nWhile our global headquarters are in the Baltimore, Maryland, USA area, McCormick operates and serves customers from nearly 60 locations in 25 countries and 170 markets in Asia-Pacific, China, Europe, Middle East and Africa, and the Americas.\nAt McCormick, we have over a 100-year legacy based on our Power of People principle. This principle fosters an unusually dedicated workforce requiring a culture of respect, recognition, inclusion and collaboration based on the highest ethical values.\nPosition Overview\nTo provide the business with data insights that will help increase revenue and/or lower costs. ^ Provide technical leadership and focus on generating insights for business customers. ^ Individual should help shape strategies for our consumer units globally.\nKey Responsibilities\nLeadership, Project Lead, management of key stakeholders for key data science projects and presentation of results.\nBringing new insights to the business, leads data mining and analytics, interpreting and reporting of large integrated data sets built with structured and unstructured data; develops tools to leverage new proprietary data sources (SAP Analytics Cloud SAC, S/4Hana LeoConnect Fiori, Analysis for Office A40). Statistical model building and deployment in the areas of forecasting, marketing mix and consumer insights.\nContinuously seeks out industry best practice and skills development to create new capabilities for data analytics at McCormick to drive marketing strategy.\nRequired Qualifications & Experience\nBachelor's Degree in Statistics (or Statistics Major) or Data Analytics. Minimum of 8 years of experience in analytics.\nExtensive experience analyzing complex datasets, generating insights and building robust statistical models.\nComfortable working with structured and unstructured data. Expertise in R, MS-Office required. Familiarity with brands (McCormick and competition) and their metrices.\nInterpersonal Skills\nAble to influence and navigate across multiple functions and regions (Procurement, Marketing, Sales, R&D, Finance etc.) to leverage enterprise data seamlessly for insights that can be leveraged across the business. Sound timely decisions grounded in data but also intuition, welldeveloped emotional intelligence. This role will interact with subordinates and superiors in the global analytics organization as well as with the internal customers/sponsors of the work. He/she will most likely join a senior colleague in meetings with the business and on occasion, present findings from the analyses.\nMcCormick & Company is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, colour, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.",
        "skills": [
            "Analysis for Office",
            "R",
            "LeoConnect",
            "SAP Analytics Cloud",
            "MS-Office",
            "Data Analytics",
            "Fiori"
        ]
    },
    {
        "job_title": "Telecom- Data Scientist- Manager",
        "company_name": "PwC Acceleration Centers in India",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals. In data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\n\nYears of Experience: Candidates with 8+ years of hands on experience\n\nPosition: Manager\n\nIndustry: Telecom / Network Analytics / Customer Analytics\n\nRequired Skills: Successful candidates will have demonstrated the following skills and characteristics:\n\nMust Have\n\nExperience working in the telecom industry across domains such as customer churn prediction, ARPU modeling, pricing optimization, and network performance analytics\nStrong understanding of telecom KPIs and datasets, such as CDRs, service usage data, network QoS, and customer lifecycle metrics\nProficiency in machine learning techniques, including classification, regression, clustering, and time-series forecasting\nStrong command of statistical techniques (e.g., logistic regression, hypothesis testing, segmentation models)\nHands-on experience in Python, SQL, and PySpark for large-scale data handling and modeling\nExperience with ML libraries such as scikit-learn, XGBoost, TensorFlow, PyTorch\nKnowledge of data wrangling, feature engineering, and preparing telecom datasets for modeling\nProficiency in data visualization tools such as Power BI, Tableau, or Looker\n\nNice To Have\n\nExposure to cloud platforms (Azure ML, AWS SageMaker, GCP Vertex AI)\nExperience working with telecom OSS/BSS systems or customer segmentation tools\nFamiliarity with network performance analytics, anomaly detection, or real-time data processing\nStrong client communication and presentation skills\n\nRoles And Responsibilities\n\nLead and manage analytics projects within the telecom domain, driving design, development, and delivery of data science solutions\nWork closely with US-based clients and consultants to understand telecom-specific use cases and define analytical approaches\nGuide teams in building and validating predictive models that address key business needs like churn, upsell, ARPU, and service quality\nTranslate business problems into data-driven solutions, and ensure alignment with KPIs and expected outcomes\nReview and manage data quality, model robustness, and explainability for telecom-specific deployments\nPresent insights and analytical recommendations to internal stakeholders and client leadership teams\nMentor and upskill junior team members while fostering a culture of collaboration and continuous learning\nSupport practice development activities including IP creation, proposal support, and client engagement planning\n\nProfessional And Educational Background\n\nBE / B.Tech / MCA / M.Sc / M.E / M.Tech /Master's Degree /MBA from reputed institute",
        "skills": [
            "feature engineering",
            "machine learning techniques",
            "Regression",
            "Classification",
            "Statistical Techniques",
            "segmentation models",
            "time-series forecasting",
            "Clustering",
            "Sql",
            "Pyspark",
            "data wrangling",
            "Logistic Regression",
            "Python",
            "Hypothesis Testing"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "MiQ",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Principal Data Scientist\n\nLocation: Bengaluru\n\nWHAT YOU'LL DO\n\nWe're MiQ, a global programmatic media partner for marketers and agencies. Our people are at the heart of everything we do, so you will be too. No matter the role or the location, we're all united in the vision to lead the programmatic industry and make it better.\n\nAs a Principal Data Scientist in our data science department, you'll have the chance to:\n\nProvide technical leadership and strategic direction for the data science practice across multiple product teams.\nFocus on creating new business opportunities leveraging innovative data science technologies and approaches to solve complex business problems\nDrive end-user usage and consumption of Data Science solutions while also striving for technical excellence.\nMentor and guide senior data scientists and engineers, fostering technical growth and excellence within the team.\nDriving best practices across teams for - code development, code versioning, unit tests, testing strategies, pipeline architecture, model development & deployment, use of cloud applications, use of AI/ML platforms.\nLead the evaluation and implementation of new data science models, platforms, and technologies to enhance the organization's capabilities.\n\nWho are your stakeholders\n\nInternal Consumer/Stakeholders:\n\nEnd Users (Account Managers / Traders / Sales / Analysts): Gather insights into business challenges and user feedback to propose solutions and features for enhanced user experience.\nDevelopment Team: Collaborate with Technology Managers to foster technical excellence in product development, ensuring the creation of robust, proven, scalable, data science-driven, and high-impact solutions.\n\nWhat You'll Bring\n\nBachelor's degree in Statistics, Mathematics, Data Science, Engineering, Physics, Economics, or a related quantitative field.\n10 years of hands-on work experience in data science modelling to solve product or business problems.\nExperience translating product and business questions into clearly framed investigative projects, performing statistical analysis, and coding (e.g., Python, R, SQL)\nAbility to present to business leaders and communicate effectively with highly technical leadership teams.\nUnderstanding of AI Models, Large Language Models, and AI specialized infrastructure especially as it relates to AI trends and enablers within businesses.\nCandidates with the ability to set-up and ensure continuous improvement of MLOps & CI/CD processes will be preferred.\nExperience in the advertising or ad-tech industry will be a plus.\n\nWe've highlighted some key skills, experience and requirements for this role. But please don't worry if you don't meet every single one. Our talent team strives to find the best people. They might see something in your background that's a fit for this role or another opportunity at MiQ.\n\nIf you have a passion for the role, please still apply.\n\nWhat impact will you create\n\nLead the technical strategy and execution of data science initiatives that deliver significant impact across interdependent product lines.\nElevate the technical capabilities of the data science team by driving the adoption of advanced methodologies and best practices.\nEnsure the rigor, quality, and ethical integrity of data science outputs across all product domains\nAccelerate the experimentation of data science applications by developing rapid POC/MVP to drive business impact.\n\nWhat's in it for you\n\nOur Center of Excellence is the very heart of MiQ, and it's where the magic happens. It means everything you do and everything you create will have a huge impact on our entire global business.\n\nMiQ is incredibly proud to foster a welcoming culture. We do everything possible to make sure everyone feels valued for what they bring. With global teams committed to diversity, equity, and inclusion, we're always moving towards becoming an even better place to work.\n\nValues\n\nOur values are so much more than statements. They unite MiQers in every corner of the world. They shape the way we work and the decisions we make. And they inspire us to stay true to ourselves and to aim for better. Our values are there to be embraced by everyone so that we naturally live and breathe them. Just like inclusivity, our values flow through everything we do - no matter how big or small.\n\nWe do what we love - Passion\nWe figure it out - Determination\nWe anticipate the unexpected - Agility\nWe always unite - Unite\nWe dare to be unconventional - Courage\n\nBenefits\n\nEvery region and office has specific perks and benefits, but every person joining MiQ can expect:\n\nA hybrid work environment\nNew hire orientation with job-specific onboarding and training\nInternal and global mobility opportunities\nCompetitive healthcare benefits\nBonus and performance incentives\nGenerous annual PTO paid parental leave, with two additional paid days to acknowledge holidays, cultural events, or inclusion initiatives.\nEmployee resource groups are designed to connect people across all MiQ regions, drive action, and support our communities.\n\nApply today!\n\nEqual Opportunity Employer",
        "skills": [
            "R",
            "CI CD processes",
            "Large Language Models",
            "AI Models",
            "MLops",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "AppZen",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nAppZen is the leader in autonomous spend-to-pay software. Its patented artificial intelligence accurately and efficiently processes information from thousands of data sources so that organizations can better understand enterprise spend at scale to make smarter business decisions. It seamlessly integrates with existing accounts payable, expense, and card workflows to read, understand, and make real-time decisions based on your unique spend profile, leading to faster processing times and fewer instances of fraud or wasteful spend. Global enterprises, including one-third of the Fortune 500, use AppZen's invoice, expense, and card transaction solutions to replace manual finance processes and accelerate the speed and agility of their businesses. To learn more, visit us at www.appzen.com.\nAbout the Role:\nWe are looking for a Senior Data Scientist to come and work on our growing AI stack. You will be working with a team of highly skilled and motivated data scientists and machine learning engineers. If you are excited about natural language understanding and machine translation, AppZen is the right place for you to apply and grow your skills.\nMust haves:\nSolid understanding of machine learning fundamentals, and familiar with standard algorithms and techniques.\nExpert knowledge of a statistical computing language such as Python, Knowledge of probability and statistics, including experimental design, predictive modeling, optimization, and causal inference.\nLead the design, development, and implementation of state-of-the-art NLP algorithms and models using Transformers and similar architectures.\nGood Understanding of MLOps tools/processes like ElasticSearch, Jenkins, Docker is a plus.\nGood knowledge of Deep Learning frameworks like PyTorch, Tensorflow is a must.\nEnsure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, cross-lingual alignment/mapping, etc.\nManage your own process: identify and execute on high impact projects, triage external requests, and make sure you bring projects to conclusion in time for the results to be useful.\nExcellent written and verbal technical communication skills; communicate proposals and results in a clear manner backed by data and coupled with actionable conclusions to drive business decisions.\nM.Tech/B.Tech. or equivalent experience in Computer Science, Engineering, Statistics, or other relevant technical field.\nMust have 4+ years of industry experience.\nYou are a team player.\nNice-to-Have:\nTrack-record of having developed novel algorithms, e.g. publications in one or more of the following: KDD, WWW, NIPS, ISWC, NAACL, ACL, SIGIR, EMNLP, ICML etc.\nExpertise in building and fine-tuning LLM models using Transformers and RAG systems.",
        "skills": [
            "Transformers",
            "Elasticsearch",
            "Tensorflow",
            "Jenkins",
            "Pytorch",
            "MLops",
            "Docker",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "TVARIT",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Senior Data Scientist\n\nIndustry: Manufacturing (Steel/Metal Industry) or similar Industry.\n\nLocation: Pune\n\nJob Type: Full-Time\n\nAbout TVARIT\n\nTVARIT GmbH specializes in developing and delivering cutting-edge artificial intelligence (AI) solutions for the metal industry, including steel, aluminum, copper, cast iron, and more. Our software products empower customers to make intelligent, data-driven decisions, driving advancements in Predictive Quality (PsQ), Predictive Maintenance (PdM), and Energy Consumption Reduction (PsE), etc.\n\nWith a strong portfolio of renowned reference customers, state-of-the-art technology, a talented research team from prestigious universities, and recognition through esteemed awards such as the EU Horizon 2020 AI Prize, TVARIT is recognized as one of the most innovative AI companies in Germany and Europe.\n\nWe are looking for a self-motivated person with a positive can-do attitude and excellent oral and written communication skills in English.\n\nAbout The Role\n\nWe are looking for a highly skilled and motivated Senior Data Scientist who is passionate about solving complex data problems and developing cutting-edge solutions. This role requires someone with a strong technical background in data science, with full-stack development exposure from requirement gathering to solution deployment. The ideal candidate will be comfortable working as an individual contributor and as an active team member.\n\nKey Responsibilities\n\nBuild, optimize, and deploy scalable data models and machine learning pipelines.\nCollaborate with cross-functional teams to design, develop, and deploy data-driven solutions.\nUnderstands business problems and processes based on direct conversations with customers, can see the big picture, and translate that into specific solutions.\nConduct exploratory data analysis and feature engineering for large datasets.\nWrite clean, efficient, and maintainable Python code for data processing and analysis.\nDevelop APIs and integrate machine learning models into production systems.\nWork on cloud platforms (AWS, Azure, or GCP) to deploy and monitor models.\nActively participate in team discussions, code reviews, and knowledge sharing sessions.\nManage client discussions actively and Present findings and recommendations to stakeholders in a clear and compelling manner.\n\nQualifications\n\nBachelor's or master's degree in Metallurgy, mechanical, or a related field from IIT, NIT or top university.\n5+ years of experience in data science, machine learning.\nExperience in metal industry, manufacturing industry or similar industry exposure\n\nTechnical Skills\n\nStrong Python programming skills with experience in libraries like NumPy, Pandas, Scikit-learn, TensorFlow, or PyTorch.\nFull-stack development experience (e.g., backend development, REST APIs, front-end integration).\nProficiency in cloud platforms such as AWS, Azure, or GCP.\nHands-on experience with SQL and NoSQL databases.\nUnderstanding of MLOps principles and tools for model versioning, monitoring, and deployment\nFamiliarity with Kubernetes core concepts like pods, deployments, services, and ingress\nKnowledge of deep learning architectures and techniques for computer vision, NLP, and generative AI\nExcellent problem-solving and analytical skills.\nStrong communication skills and the ability to collaborate in a team environment.\nAbility to work in a fast-paced, agile environment and adapt to changing priorities.",
        "skills": [
            "Full-stack development",
            "generative AI",
            "Scikit-learn",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Deep Learning",
            "Numpy",
            "Nlp",
            "Pytorch",
            "Computer Vision",
            "Pandas",
            "Gcp",
            "MLops",
            "Rest Apis",
            "Azure",
            "Python",
            "Kubernetes",
            "AWS"
        ]
    },
    {
        "job_title": "Staff, Data Scientist",
        "company_name": "Walmart Global Tech",
        "experience": "8-14 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Staff, Data Scientist\nExperience: 8 - 14 years\nLocation: Chennai\nAbout EBS team:\nEnterprise Business Services is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, Associate Digital Experience. Our team is responsible for design and development of solution that knows our consumer's needs better than ever by predicting what they want based on unconstrained demand, and efficiently unlock strategic growth, economic profit, and wallet share by orchestrating intelligent, connected planning and decisioning across all functions. We interact with multiple teams across the company to provide scalable robust technical solutions. This role will play crucial role in overseeing the planning, execution and delivery of complex projects within team.\nAbout Team\nThe data science team at Enterprise Business Services Pillar at Walmart Global Tech focuses on using the latest research in machine learning, statistics, and optimization to solve business problems. We mine data, distill insights, extract information, build analytical models, deploy Machine Learning algorithms, and use the latest algorithms and technology to empower business decision-making. In addition, we work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions. Advanced analytical algorithms driven by our team will help Walmart to optimize business operations, business practices and change the way our customers shop.\nThe data science community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Labs will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat You Will Do\nAs a Staff Data Scientist for Walmart Global Tech, you'll have the opportunity to Drive data-derived insights across a wide range of retail & Finance divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assess data validity and synthesize data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train AI/ML models for replication for future projects\nGuides. data scientists, senior data scientists & staff data scientists across multiple sub-domains to ensure on-time delivery of ML products\nDrive efficiency across the domain in terms of DS and ML best practices, ML Ops practices, resource utilization, reusability and multi-tenancy.\nLead multiple complex ML products and guide senior tech leads in the domain in efficiently leading their products.\nDrive synergies across different products in terms of algorithmic innovation and sharing of best practices.\nWhat You Will Bring\nMaster's with > 9 years OR Ph.D. with > 8 years of relevant experience. Educational qualifications should be Computer Science/Statistics/Mathematics or a related area.\nMinimum 6 years of experience as a data science technical lead\nAbility to lead multiple data science projects end to end.\nDeep experience in building data science solution in areas like fraud prevention, forecasting, shrink and waste reduction, inventory management, recommendation, assortment and price optimization\nDeep experience in simultaneously leading multiple data science initiatives end to end from translating business needs to analytical asks, leading the process of building solutions and the eventual act of deployment and maintenance of them Strong experience in machine learning: Classification models, regression models, NLP, Forecasting, Unsupervised models, Optimization, Graph ML, Causal inference, Causal ML, Statistical Learning, experimentation & Gen-AI\nIn Gen-AI, it is desirable to have experience in embedding generation from training materials, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nAbility to scale and deploy data science solutions.\nStrong Experience with one or more of Python and R.\nExperience in GCP/Azure\nStrong Experience in Python, PySpark\nGoogle Cloud platform, Vertex AI, Kubeflow, model deployment\nStrong Experience with big data platforms Hadoop (Hive, Map Reduce, HQL, Scala)\nExperience with GPU/CUDA for computational efficiency\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.",
        "skills": [
            "Computational Algorithms",
            "GPU CUDA",
            "Statistical Models",
            "AI ML Models",
            "R",
            "Vertex AI",
            "Kubeflow",
            "Map Reduce",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Scala",
            "Pyspark",
            "Hive",
            "Gcp",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Grab",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Description\n\nAbout Grab and Our Workplace\n\nGrab is Southeast Asia's leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, we've got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.\n\nJob Description\n\nGet to know the team\n\nGrabFin is an aggregate of FinTech businesses spread across 6 countries in S.E. Asia, in the Lending, Payments and Insurance domains. We are excited to provide innovative financial services to all participants of the Grab Ecosystem be it our Drivers, Consumers or Merchants. Our products are built on fundamental market insights combined with data science and engineering to bring the best product market fit across the cross section of our user base. This understanding of our ecosystem combined with world class engineering execution continues to create tremendous value for our customers.\n\nThe data scientist will work in a relatively flat team structure with an independent goal of building and manage critical data science models daily. You can expect to solve hard technical problems and grow into an expert on both batch and real-time Data Science use cases. You will have experience with technology and data science.\n\nYou will be reporting to Senior Manager, Data Science.\n\nThis role is onsite based in Bangalore.\n\nGet to know the role\n\nYou'll develop credit risk scoring models for consumer loans, including PD, LGD, and collection models. You'll work with alternative data sources to boost model signal and accuracy. Your role will involve full ownership of the end-to-end model lifecyclefrom building and validation to deployment and maintenance. You'll collaborate with business, risk, and operations teams to shape solutions and influence product strategy with your insights. This is an individual contributor role suited for professionals with 8+ years of experience.\n\nThe Critical Tasks You Will Perform\n\nBuild predictive models using a mix of machine learning and traditional analytics methods to segregate between Good vs Bad borrowers\nBuild Machine learning & Deep learning models to estimate losses from of a given portfolio.\nValidate models on new datasets, based on in-market performance.\nEngineer predictive features from internal data assets to build refined customer profiles. Identify external data assets to bring into the model mix.\nDrive model governance by collaborating with risk policy, compliance, and audit teams to ensure adherence to regulatory expectations.\nIdentify model gaps or performance drifts and lead model refresh cycles.\nPresent findings to senior leadership with clear articulation of risk trade-offs and growth.\nTranslate model insights into strategic recommendations (e.g., policy changes, pricing levers, customer targeting strategies).\nSolve previously unsolved analytics problems using best in class data analytics and machine learning methodologies.\n\nQualifications\n\nThe Essential Skills You Need\n\n8+ years of experience.\nStrong understanding of credit business lifecycle of a loan, collections process, and credit KPIs like NPL, ECL.\nExpert in building machine learning and predictive models in Python and Spark is an absolute must.\nSQL, Presto, Hive proficiency.\nSound knowledge of machine learning concepts. Illustrative machine learning concepts/methods are: Bagging, Boosting, Regularisation, Online Learning, Recommendation Engines\nExperience with LLMs, and Generative AI\nExperience with model deployment pipelines using MLFlow, Airflow, or other MLOps tools.\nDemonstrated experience building machine learning models\nUnderstand the trade-offs between model performance and our needs.\nStrong problem-solving mindset is critical for success in this role.\n\nAdditional Information\n\nLife at Grab\n\nWe care about your well-being at Grab, here are some of the global benefits we offer:\n\nWe have your back with Term Life Insurance and comprehensive Medical Insurance.\nWith GrabFlex, create a benefits package that suits your needs and aspirations.\nCelebrate moments that matter in life with loved ones through Parental and Birthday leave, and give back to your communities through Love-all-Serve-all (LASA) volunteering leave\nWe have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through life's challenges.\n\nWhat We Stand For At Grab\n\nWe are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique.",
        "skills": [
            "Airflow",
            "Generative AI",
            "MLFlow",
            "Machine Learning",
            "Hive",
            "Presto",
            "Spark",
            "Python",
            "Sql",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "ClickPost",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Lead Data Scientist - Drive the AI Vision at ClickPost\nLocation: Hybrid (Bengaluru preferred)\nExperience: 8+ years in Data Science & AI | 3+ years in Leadership\nAbout Us:\nEstablished in 2015, ClickPost is Asia's largest Logistics intelligence platform, working with companies such as Walmart, Nykaa, Meesho, Adidas etc to help them improve post purchase customer experience. We have been profitable for past 5 years, are backed by some of the biggest investors around and are growing 2x year on year.We are one of the rare startups working at cutting edge technology, with enormous scales and a brand name that is well respected in Indian Saas and e-commerce world. Our tenets of transparency, learning, ownership and velocity have ensured a culture where multiple ex startup founders are part of the team making it a fun and learning filled place to work at.\nAbout the Role:\nAre you passionate about building scalable AI systems that solve real-world problems Do you want to lead the data strategy for one of India's fastest-growing logistics tech companies\nAt ClickPost, we're redefining the post-purchase experience for the largest e-commerce brands in India and beyond. As our Lead Data Scientist, you'll own the entire AI charter from setting the vision to hands-on execution. You'll work on cutting-edge problems in logistics intelligence, last-mile automation, LLM-powered agents, and predictive modeling that directly impact millions of shipments every day.\nThis is not just a leadership role it's a builder's role. We're looking for someone who loves to roll up their sleeves, code alongside the team, and drive measurable outcomes with AI.\nWhat You'll Do:\nLead End-to-End Projects: Own the complete lifecycle of AI/ML initiatives - from problem definition and data gathering to model deployment and monitoring in production.\nCode. Innovate. Scale: Build state-of-the-art models using ML, deep learning, and LLMs to power logistics automation. Evaluate and deploy models in real-world settings with massive data volumes.\nBuild & Nurture the Team: Grow and lead a high-performing team of data scientists and ML engineers. Mentor talent, shape culture, and lead by example.\nCross-Functional Impact: Partner with engineering, product, and ops to ship data science features that move business metrics at scale.\nShape AI Strategy: Drive innovation and influence company-wide decisions with data. You'll be a key voice in strategic planning for ClickPost's AI roadmap.\nChampion Best Practices: Evangelize modern AI/ML tooling, experimentation pipelines, and MLOps standards.\nWhat We're Looking For:\n8+ years of deep experience in applied data science, AI, and ML, including LLMs and predictive modeling.\nProven success in taking AI projects from idea to impact, with a strong portfolio of shipped initiatives.\nFluency in Python (or R), and familiarity with modern ML frameworks (TensorFlow, PyTorch, Scikit-learn, Hugging Face).\nHands-on leadership experience of 3+ years, with the ability to inspire and upskill teams.\nA strategic thinker and excellent communicator - you can explain models to CXOs and junior devs alike.\nSelf-starter mindset with strong problem-solving skills and a bias for action.\nWhy ClickPost\nWe're a Series A-funded company powering logistics for top e-commerce giants. With over 100 million shipments processed monthly, we're scaling fast and building world-class AI products in India for the world.\nWe offer:\nCompetitive compensation + performance bonuses\nHealth insurance for you and your family\nHybrid work flexibility\nGenerous paid time off\nLearning budget for professional growth\nA collaborative, inclusive, and high-ownership culture",
        "skills": [
            "R",
            "LLMs",
            "Scikit-learn",
            "Hugging Face",
            "Deep Learning",
            "Ml",
            "Tensorflow",
            "Pytorch",
            "Predictive Modeling",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Insight Global",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "***$10-$20/hr USD***\nMust haves:\n5+ years in data science, statistical modeling/ machine learning\nStrong background in statistics and Machine learning models (Clustering, Market basket/apriori, supervised machine learning, Keras, boosting and bagging)\nWell versed in SQL, Python\nExperience with Keras, Tensorflow deep learning models (To predict price impact on product demands)\nKnowledge on Optimizations (Dual Annealing/ Mixed Objective/ Goal based optimization)\nExperience working on large data sets, linear/ non-linear optimizations, ML applications\nPluses:\nPricing analytics and price elasticity\nExperience with Deep Learning Framework\nDay to day:\nAn employer is looking for a Data Scientist to sit remotely. You will primarily be tasked with analyzing data and generate analytical outputs to deliver insights, evaluate hypothesis and complete root cause analysis of the business problem. The focus for this team will be AI based solutions to determine consumer insights and marketing performance analytics to help brands understand consumer behavior for promotions and pricing decisions. You will design end to end solutions by leveraging the latest ML techniques, including data preparation, exploratory data analysis, feature engineering, model selection, model development, and deployment. Additional tasks will include:\nSolve business problems which involve:\nBrainstorm with clients and internal teams to define a problem\nTranslate the business problem into an analytical problem\nIdentify internal and external data requirements for solving the analytical problem\nSolving the analytical problem using concepts from mathematics, statistics, Artificial Intelligence and Machine learning\nCreate, maintain and enhance artefacts that can help communicate the solution to clients like dashboards, power point decks, excel sheets etc.\nDevelop algorithms based on the problem statement & domain area. Design & develop new algorithms for optimization, recommendation, attribution, and emerging real-time consumer analytics.\nEstablish mathematical models to represent specific business functions or consumer behavior.\nProgramming of algorithms, and integration into the data science framework.\nProvide Support and add new features in a marketing performance analytics automation platform. Work on the scalability and performance of the automation platform.\nCreate proof of concepts to vet out new technologies.\nInfluence design and architecture principles for new technology products.",
        "skills": [
            "Statistical Modeling",
            "linear optimizations",
            "Boosting",
            "Mixed Objective Goal based optimization",
            "bagging",
            "Dual Annealing",
            "optimizations",
            "non-linear optimizations",
            "Keras",
            "Sql",
            "Tensorflow",
            "Deep Learning",
            "Machine Learning",
            "Data Science",
            "Python"
        ]
    },
    {
        "job_title": "Global Data Scientist - Manager",
        "company_name": "Boston Consulting Group (BCG)",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nBCG is on an ambitious and accelerated AI journey, leveraging the power of Generative AI to radically transform the speed and scale of business delivery and growth. At the heart of this transformation is a commitment to rapidly developing and deploying cutting-edge GenAI solutions that unlock new levels of efficiency and innovation across the enterprise.\n\nThis role will drive the development of high impact prioritized Generative AI solutions within the Legal Product Portfolio, accelerating the realization of our ambitious functional AI and GenAI vision.\n\nKey Responsibilities\n\nDevelop AI/Gen AI projects as individual coding contributors (IC), from ideation to ensuring they deliver impactful insights\nDrive end-to-end design, development & deployment of Gen AI solutions at scale to deliver high-quality outcomes\nFine-tune large language models (LLMs) to meet specific Generative AI business outcomes\nExtract and analyze data from various sources to inform modeling and solution development.\nAnalyze large and complex datasets to extract meaningful trends, patterns, and insights\nShare expertise and perspectives on alternate approaches & tools to improve end solution\nCollaborate closely with data engineers, LLM engineers, Full stack AI engineers, QA, product owners and analysts to create and operationalize data & AI pipelines, ensuring compliance with data governance. security and responsible AI requirements\nBuild strong relationships with business stakeholders, ensuring alignment on project goals and successful integration of data-driven insights into business strategies\n\nWhat You'll Bring\n\nAI/ML & LLM Engineering Skills\n\nOverall 7+ years of experience in technology, software engineering or data science/ML\n5+ years of hands-on coding experience in python development\n2+ years of experience in building Generative AI products at scale\nDemonstrated experience in developing Generative AI based solutions using frameworks (e.g. Langchain, Llamaindex, etc.) to build retrieval augmented generation (RAG), multi-agent-architecture driven solutions\nHighly curious and fast learner who is keenly keeping pace with latest and greatest happening in Gen AI and agentic AI space\nExperience in working with variety of LLM models and fine tuning\nExperience in applying MLOps/LLMOps principles to deploy, monitor models and implement automated CI/CD pipelines\nExperience with LLM evaluation frameworks, guardrails to evaluate and monitor outputs quality, control hallucinations\nAwareness of ethical AI frameworks to ensure responsible AI deployment\nAbility to work iteratively on AI/ML/Gen AI solutions to improve performance over time and in response to user feedback\nExperience in deploying solutions on a cloud platforms AWS (preferred), Azure, GCP\nExperience with microservices & serverless architecture, API development\nAn MVP mindset that emphasizes rapid prototyping and continuous improvement.\n\nCommunication, Interpersonal And Teaming Skills\n\nExcellent communication skills, with the ability to explain complex technical concepts to various audiences\nExperience working in a fast-paced global company, with a diverse team of both technical and functional backgrounds located in multiple time zones\nProven ability to operate with a transparent mindset, communicating openly with stakeholders at various levels of the organization\n\nWho You'll Work With\n\nJoin a dynamic, cross-functional team of professionals from diverse backgrounds. At BCG, we value drive, curiosity, and collaborative leadership. You'll tackle challenging projects that address complex business problems, working closely with sharp, analytical stakeholders who make decisions with a global outlook\n\nAdditional info\n\nYOU'RE GOOD AT\n\nDriving rapid delivery through an MVP approach, enabling fast prototyping, iterative development, and impactful features.\nProblem-solving and innovating to overcome technical challenges, learning new tools quickly, and adapting to a fast-paced environment.\nWorking independently with minimal direction, showing strong initiative and drive.\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.",
        "skills": [
            "Generative AI",
            "Langchain",
            "LLMOps",
            "Llamaindex",
            "Ethical AI Frameworks",
            "MLops",
            "Data Governance",
            "Python",
            "Api Development",
            "Microservices"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "GyanSys Inc.",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "The ideal candidate's favorite words are learning, data, scale, and agility. You will leverage your strong collaboration skills and ability to extract valuable insights from highly complex data sets to ask the right questions and find the right answers.\nWe are seeking a 4-7 years skilled Data Scientist proficient in Python and R to join our team. The ideal candidate will have extensive experience developing machine learning and statistical models for regression, classification, and time series problems. Additionally, they will have a strong understanding of software engineering principles and the ability to write clear, well-structured, and testable code.\nKey Responsibilities:\nDevelop machine learning and statistical models for various business needs, including regression, classification, and time series analysis.\nImplement models using Python and relevant libraries such as scikit-learn, pandas, and numpy.\nApply machine learning techniques such as Linear Regression, Random Forest, Classification, and Ensemble methods.\nConvert R code to Python and ensure seamless integration between the two languages.\nUtilize R for data manipulation and analysis using tidyverse, dplyr, and data.table.\nOptimize and enhance existing R code for performance and scalability.\nCollaborate with cross-functional teams to understand business requirements and deliver data-driven solutions.\nFollow best practices in software engineering to produce clean, manageable, and testable code.\nStay updated with the latest industry trends and advancements in data science and machine learning.\nRequired Skills and Qualifications:\nBachelor's or Master's degree in Computer Science, Statistics, Data Science, or a related field.\n6+ years of IT experience with a min of 3+ years in Data Science (AI/ML)\nProven experience as a Data Scientist or similar role with hands-on experience in Python and R.\nStrong knowledge of machine learning algorithms and statistical modeling techniques.\nProficiency in Python libraries such as scikit-learn, pandas, numpy, and matplotlib.\nExpertise in R programming, particularly with tidyverse, dplyr, tidymodels, and data.table.\nExperience with relational databases and SQL.\nAbility to convert R code to Python efficiently.\nFamiliarity with software engineering principles and best practices.\nStrong problem-solving skills and attention to detail.\nExcellent communication and collaboration skills.\nAbility to manage multiple projects and meet deadlines.\nPlease Share your Updated resume to [HIDDEN TEXT]",
        "skills": [
            "R",
            "scikit-learn",
            "tidyverse",
            "data.table",
            "tidymodels",
            "dplyr",
            "Pandas",
            "Numpy",
            "Matplotlib",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Inxite Out",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Senior Data Scientist\n\nLocation: Kolkata/Bangalore (Remote/Hybrid options available)\n\nCompany: Inxite-out\n\nJob Overview: We are seeking a highly skilled and experienced Senior Data Scientist to join our dynamic team. This role will involve leading data science initiatives, developing advanced machine learning models, and providing actionable insights for business strategies. The ideal candidate will have a strong background in data science, statistical analysis, and business problem-solving, with a passion for delivering high-quality solutions.\n\nKey Responsibilities & Skills\n\nLead the development of advanced predictive and prescriptive models using machine learning, forecasting and other statistical methods\nWorking knowledge of regression, classification, clustering, association algorithms and model optimization techniques\nWork closely with business stakeholders to understand requirements and translate them into data science solutions\nDesign and implement end-to-end data science workflows, including data collection, preprocessing, feature engineering, model building, and evaluation\nCollaborate with cross-functional teams to deploy and integrate models into production environments\nPerform model validation, fine-tuning, and optimization to ensure accuracy and scalability\nIdentify and investigate anomalies or unexpected behaviours in model outputs\nBuild REST APIs using Fast/Flask, move models into production, ensure expected accuracies and throughput for the model\nMentor and guide junior data scientists, providing technical expertise and leadership\n\nQualifications\n\nBachelor's or master's in computer science, Statistics, Mathematics, Engineering, or a related field\n4+ years of experience in data science or a similar analytical role, with a proven track record of delivering impactful projects\nStrong proficiency in python with experience in libraries like pandas, NumPy, Spacy, TensorFlow, Scikit-learn etc.\nExperience in working with cloud platforms (Azure), Databricks, PySpark\nStrong theoretical and practical knowledge of ML model development, hyper-parameter tuning, and production deployment\nSolid understanding of statistical analysis, data mining, feature engineering\nStrong problem-solving skills with the ability to think critically and adapt to complex business challenges\nExcellent communication skills with the ability to explain complex data-driven concepts to a non-technical audience\nExperience in leading and mentoring junior team members.\nStrong understanding of MLOps- lifecycle of a machine learning model\n\nAdditional Skills (good To Have)\n\nKnowledge of deep learning, CNN, MLP, OpenCV etc.\nWorking knowledge of NLP/GenAI based solutions\nExperience working with SAP (S/4 HANA, BW) is a plus",
        "skills": [
            "Scikit-learn",
            "Fast",
            "Spacy",
            "Pyspark",
            "Tensorflow",
            "Numpy",
            "Pandas",
            "MLops",
            "Flask",
            "Databricks",
            "Rest Apis",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - II",
        "company_name": "G2",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About G2 - Our People\n\nG2 was founded to create a place where people will love to work. We strive to create meaning in work and provide more than just a job: a true calling. At the heart of our community and culture are our people. Our global G2 team comes from a wide range of backgrounds and experiences, and that's what makes our G2 community strong and vibrant. We want everyone to bring their authentic selves to work, and we do this through our company and team events, our G2 Gives charitable initiatives, and our Employee Resource Groups (ERGs).\n\nOur employee-led, leadership-supported ERGs celebrate the diversity of our team, foster inclusivity and belonging, and create a space to connect to each other. Through connections and understanding, we build a stronger and more dynamic global team and help every person reach their personal peak.\n\nWe support our employees well-being by providing extensive benefits, including flexible work, aligned time off, and various leave options such as maternity, paternity, and sabbatical leaves. Click here to learn more about our benefits.\n\nAbout G2 - The Company\n\nWhen you join G2, you join the global team behind the largest and most trusted software marketplace. Every month, 5.5 million people come to G2 to inform smarter software decisions based on honest peer reviews. Authenticity is our focus, and every day we help thousands of companies, and hundreds of employees, propel their potential. Ready for meaningful work that starts and ends with compassion and heart You've come to the right place.\n\nG2 is going through exciting growth! We've recently secured our Series D funding of $157 million, which will further allow us to grow and develop our product and people. Read about it here!\n\nAbout The Role\n\nG2 is looking for a Data Scientist - II, your role encompasses leading model development and contributing to machine learning product development. You'll own end-to-end data science workflows, experiment with advanced algorithms, mentor junior team members, and drive innovation within the data science domain. You will work on Improving G2's intent scoring, content moderation, and other AI-driven features through the use of machine learning.This is a hybrid position, with the team meeting in person 2-3 days a week at our Bengaluru office.\n\nIn This Role, You Will\n\nModeling and Statistical Analysis:\n\nIndependently lead the development of machine learning models, owning feature engineering, extraction, model selection, and optimization\nDesign experiments by formulating statistical hypotheses, defining data requirements, pre-processing and cleaning the data, and performing the hypothesis testing.\nOperationalise models at scale applying AI and engineering best practices by working with the ML engineers.\nExperiment with various algorithms and techniques to advance model performance.\nDefine feedback and evaluation methods for the business problems.\nDemonstrate excellent coding and debugging skills.\n\nBusiness, Data Understanding, And Impact\n\nMake impactful contributions by leveraging AI and Machine Learning expertise to address pressing business challenges.\nCollaborate with cross functional teams to understand the business requirements and data architecture.\nTranslate business requirements into technical solutions by working with the business and senior data scientists.\nIdentify and document the data requirements and manage data collection and preparation for projects.\nDesign and document training and testing strategy.\nDocument methodologies, findings, and outcomes of model experiments and present it to the team and key stakeholders.\n\nMentorship And Collaboration\n\nMentor junior team members, providing technical support, guidance on model development, and best practices implementation.\nCoach junior team members, helping them understand complex datasets, models, and business requirements by giving clear and actionable feedback.\nEncourage the development of best practices and innovative approaches in data analysis and modeling.\n\nQualifications\n\n4+ years of experience as a data scientist involved in data extraction, analysis, and modeling.\n4+ years of experience in Python and SQL or related tools for machine learning.\nUnderstanding of statistics and linear algebra.\nProficiency in machine learning algorithms and all stages of machine learning.\nFamiliarity with neural networks and deep learning.\nBasic knowledge about AWS services and cloud databases.\nProficiency in handling structured and unstructured data.\n\nYou would be successful in this role if you describe yourself as:\n\nSuccessful end-to-end delivery of data science products.\nExposure to MLOps tools like MLFlow, KubeFlow, DVC,AWS Sagemaker, Seldon etc\nExperience deploying models in a AWS cloud environment - with specific experience with AWS tools such as Sagemaker and Step Functions.\nExpertise with Natural Language Processing and Understanding.\nExperience with libraries and frameworks for training ML and DL models (PySpark, Tensorflow).\nExperience and expertise in ML Operations best practices.\n\nOur Commitment to Inclusivity and Diversity\n\nAt G2, we are committed to creating an inclusive and diverse environment where people of every background can thrive and feel welcome. We consider applicants without regard to race, color, creed, religion, national origin, genetic information, gender identity or expression, sexual orientation, pregnancy, age, or marital, veteran, or physical or mental disability status.\n\nLearn more about our commitments here Commitments",
        "skills": [
            "Tensorflow",
            "Machine Learning",
            "MLops",
            "Natural Language Processing",
            "Pyspark",
            "Neural Networks",
            "Statistical Analysis",
            "Python",
            "Sql",
            "Deep Learning",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Recro",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Major Duties & Responsibilities\nWork with business stakeholders and cross-functional SMEs to deeply understand business context and key business\nquestions\nCreate Proof of concepts (POCs) / Minimum Viable Products (MVPs), then guide them through to production deployment\nand operationalization of projects\nInfluence machine learning strategy for Digital programs and projects\nMake solution recommendations that appropriately balance speed to market and analytical soundness\nExplore design options to assess efficiency and impact, develop approaches to improve robustness and rigor\nDevelop analytical / modelling solutions using a variety of commercial and open-source tools (e.g., Python, R,\nTensorFlow)\nFormulate model-based solutions by combining machine learning algorithms with other techniques such as simulations.\nDesign, adapt, and visualize solutions based on evolving requirements and communicate them through presentations,\nscenarios, and stories.\nCreate algorithms to extract information from large, multiparametric data sets.\nDeploy algorithms to production to identify actionable insights from large databases.\nCompare results from various methodologies and recommend optimal techniques.\nDesign, adapt, and visualize solutions based on evolving requirements and communicate them through presentations,\nscenarios, and stories.\nDevelop and embed automated processes for predictive model validation, deployment, and implementation\nWork on multiple pillars of AI including cognitive engineering, conversational bots, and data science\nEnsure that solutions exhibit high levels of performance, security, scalability, maintainability, repeatability, appropriate\nreusability, and reliability upon deployment\nLead discussions at peer review and use interpersonal skills to positively influence decision making\nProvide thought leadership and subject matter expertise in machine learning techniques, tools, and concepts; make\nimpactful contributions to internal discussions on emerging practices\nFacilitate cross-geography sharing of new ideas, learnings, and best-practices\nRequired Qualifications\nBachelor of Science or Bachelor of Engineering at a minimum.\n4+ years of work experience as a Data Scientist\nA combination of business focus, strong analytical and problem-solving skills, and programming knowledge to be able to\nquickly cycle hypothesis through the discovery phase of a project\nAdvanced skills with statistical/programming software (e.g., R, Python) and data querying languages (e.g., SQL,\nHadoop/Hive, Scala)\nGood hands-on skills in both feature engineering and hyperparameter optimization\nExperience producing high-quality code, tests, documentation\nExperience with Microsoft Azure or AWS data management tools such as Azure Data factory, data lake, Azure ML,\nSynapse, Databricks\nUnderstanding of descriptive and exploratory statistics, predictive modelling, evaluation metrics, decision trees, machine\nlearning algorithms, optimization & forecasting techniques, and / or deep learning methodologies\nProficiency in statistical concepts and ML algorithms\nGood knowledge of Agile principles and process\nAbility to lead, manage, build, and deliver customer business results through data scientists or professional services team\nAbility to share ideas in a compelling manner, to clearly summarize and communicate data analysis assumptions and\nresults\nSelf-motivated and a proactive problem solver who can work independently and in teams",
        "skills": [
            "Synapse",
            "R",
            "Tensorflow",
            "Azure ML",
            "Azure Data Factory",
            "Hive",
            "Hadoop",
            "Scala",
            "Databricks",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Enlyft",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Enlyft\n\nData and AI are at the core of the Enlyft platform. We are looking for creative, customer and detail-obsessed machine learning engineers who can contribute to our strong engineering culture. Our big data engine indexes billions of structured / unstructured documents and leverages data science to accurately infer the footprint of thousands of technologies and products across millions of businesses worldwide. The complex and evolving relationships between products and companies form a technological graph that is core to our predictive modeling solutions. Our machine learning based models work by combining data from our customer's CRM with our proprietary technological graph and firmographic data, and reliably predict an account's propensity to buy.\n\nAbout The Role\n\nAs part of our team, you'll be tasked with handling substantial datasets to develop machine learning models catering to our enterprise clients. Your role will also involve contributing to the development of foundational models for our product. To excel in this position, you should possess a strong analytical aptitude, with a deep understanding of data analysis, mathematics, and statistics. Critical thinking and problem-solving abilities are imperative for the interpretation of data. Furthermore, we value a genuine enthusiasm for machine learning and a commitment to research.\n\nResponsibilities\n\nDevelop and Deploy predictive models in production and conduct advanced analytics, data mining, and data visualization to influence strategic decisions\nArchitect and build data models to transform data into insights at scale\nEvaluate model performance and conduct iterative model training to maximize predictive and forecast accuracy on an on-going basis.\n\nRequirements\n\nBachelor or above degree in Computer Science, Applied Mathematics, Statistics, Econometrics, or related field\n4-6 years of work experience depending on educational level and relevance\nSharp analytical abilities and problem solving skills\nWorking knowledge of Statistics, programming and predictive modeling\nExpert knowledge of Python or a related scripting language, familiarity with Python packages such as pandas, numpy, scipy, nltk is a must. Databricks & Spark are good to have\nComfort manipulating and analyzing complex, high-volume, high dimensionality data coming from various data sources\nExperience working with databases like MySQL, NoSQL, MongoDB, SQL Server, Oracle or Teradata.\n\nWhy join Enlyft\n\nA top-notch culture that is customer obsessed, transparent and constantly strives for excellence\nA top-notch team with colleagues that will help you learn and grow in a collaborative environment\nCompetitive pay and great benefits\n\nEnlyft is an equal opportunity employer and values diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",
        "skills": [
            "Teradata",
            "Statistics programming",
            "Scipy",
            "Nltk",
            "SQL Server",
            "Nosql",
            "Pandas",
            "Numpy",
            "MySQL",
            "Predictive Modeling",
            "Spark",
            "Databricks",
            "MongoDB",
            "Oracle",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - 2",
        "company_name": "Navi",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Navi\nNavi is one of the fastest-growing financial services companies in India providing Personal & Home Loans, UPI, Insurance, Mutual Funds, and Gold. Navi's mission is to deliver digital-first financial products that are simple, accessible, and affordable. Drawing on our in-house AI/ML capabilities, technology, and product expertise, Navi is dedicated to building delightful customer experiences.\nFounders: Sachin Bansal & Ankit Agarwal\nKnow what makes you a Navi_ite :\n1.Perseverance, Passion and Commitment\nPassionate about Navi's mission and vision\nDemonstrates dedication, perseverance and high ownership\nGoes above and beyond by taking on additional responsibilities\n2.Obsession with high quality results\nConsistently creates value for the customers and stakeholders through high quality outcomes\nEnsuring excellence in all aspects of work\nEfficiently manages time, prioritizes tasks, and achieves higher standards\n3.Resilience and Adaptability\nAdapts quickly to new roles, responsibilities, and changing circumstances, showing resilience and agility\nAbout the role:\nData Science @Navi:\nAt Navi, our Data Science team is the powerhouse behind scalable and efficient solutions that span across a broad spectrum of fintech sectorsbe it lending, insurance, investments, or UPI. We're not just a team; we're the architects of the future of fintech\nWe're not just keeping up with innovation; we're setting the pace. Our team is constantly pushing the boundaries, introducing groundbreaking methods that amplify business growth, enhance customer experiences, and streamline operational processes.\nOur work isn't confined to a single domain. We tackle a diverse set of problem statements, from computer vision and tabular data to natural language processing, speech recognition, and even Generative AI. Each day brings a new challenge and a new opportunity for breakthroughs.\nWhen you join us, you're not just taking a job; you're becoming a part of a movement. A movement that's making a tangible difference in the fintech landscape, one innovative solution at a time.\nReady for a transformative career journey Join us at Navi and be a part of a team that's shaping the future of fintech.\nWhat you gain by working with the Data Science team at Navi:\nJoin the Navi Data Science team for:\nOwn Your Journey from Start to Finish : Take pride in having full-cycle ownership of your projects. You won't just be a cog in the machine; you'll be the architect, designer, and implementer of cutting-edge Data Science solutions that drive our business forward.\nImmerse Yourself in a Data Wonderland : Welcome to a playground where data is abundant and limitations are few. Our high-growth, agile environment offers a treasure trove of data, giving you the freedom to experiment, innovate, and make data-driven decisions that matter.\nBe Part of a Synergistic Dream Team : Collaborate with a diverse group of high-performing professionals who are as passionate about data science as you are. Our culture fosters continuous learning and upskilling, setting you up for ongoing success and career growth.\nMake a Tangible Impact : Your work won't just sit on a shelf; it will make waves. By closely collaborating with stakeholders across departments, you'll have the opportunity to design and develop data science solutions that not only solve complex problems but also make a meaningful impact on our company and the fintech industry at large\nWhat we are looking for:\nBachelor's or Master's in Engineering or equivalent.\n2+ years of Data Science/Machine Learning experience.\nStrong knowledge in statistics, tree-based techniques (e.g., Random Forests, XGBoost), machine learning (e.g., MLP, SVM), inference, hypothesis testing, simulations, and optimizations.\nBonus: Experience with deep learning techniques; experience in working Ad domain/reinforcement learning.\nStrong Python programming skills and experience in building Data Pipelines in PySpark, along with feature engineering.\nProficiency in pandas, scikit-learn, Scala, SQL, and familiarity with TensorFlow/PyTorch.\nUnderstanding of DevOps/MLOps, including creating Docker containers and deploying to production (using platforms like Databricks or Kubernetes).",
        "skills": [
            "Data Pipelines",
            "scikit-learn",
            "Random Forests",
            "Feature engineering",
            "MLP",
            "simulations",
            "Tree-based techniques",
            "Statistics",
            "Inference",
            "Optimizations",
            "Machine Learning",
            "Pyspark",
            "Svm",
            "Tensorflow",
            "Data Science",
            "Pytorch",
            "Docker",
            "XGBoost",
            "Python",
            "Hypothesis Testing",
            "Scala",
            "Sql",
            "Devops",
            "Pandas",
            "MLops",
            "Databricks",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Ideas2IT Technologies",
        "experience": "Fresher",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nIdeas2IT is a high-end Product Development firm where Technology, Business, and Product engineering intersect. Ideas2IT started as a CTO consulting firm to help early-stage startups execute their technology vision. Our value proposition is developing thought leadership and competency in cutting-edge technologies like Generative AI and helping our customers adapt to them.\n\nWe are strong at applying technology to solve business problems for our clients. We are the innovative tech partners that developed AI-driven products for Facebook, Bloomberg, Siemens, Roche, and more.\n\nLeveraging our product mindset, we have launched multiple AI-based products and spun them off as separate start-ups - Pipecandy, Element5, IdeaRx, and Carefi.in. All these firms have become successful VC-funded startups.\n\nWe have clocked phenomenal growth in the last fourteen years and are marching towards lofty goals.\n\nKey Responsibility:Be the visionary behind our AI and Gen AI products, shaping their direction and driving success in a dynamic, forward-thinking environment.\n\nWhy Choose Ideas2IT\n\nAGI is going to change the world. Big companies like Microsoft are betting heavily on this (see here and here). We are following suit. As a product owner/solution consultant exclusively focussing on AGI, you will be at the forefront of this revolution.\n\nYour Role: Pioneering the Future of Products with Innovative Expertise\n\nProduct Innovation and Strategy:Understand the business context and unlock product ideas to solve real enterprise problems. Strategize on the full spectrum of taking the product from concept to market.\n\nMarket Insights and Opportunity:Analyze market data, customer feedback, and industry reports for ideal product-market fit.\n\nStrategic Roadmap Development\n\nPartner closely with engineering and development teams to convert concepts into actionable roadmaps.\n\nPrioritize features based on market demand, customer feedback, and strategic objectives.\n\nStakeholder Engagement:Interface with key stakeholders for input and validation, fostering alignment between business and technical teams.\n\nCompetitive Analysis:Monitor competitors to identify areas of differentiation and advise product enhancements.\n\nCross-Functional Collaboration:Cultivate innovation by collaborating with engineering, design, marketing, and sales teams. Ensure customer-centric product development that exceeds expectations.\n\nP.S. We're all about diversity, and our doors are wide open to everyone. Join us in celebrating the awesomeness of differences!",
        "skills": [
            "Stakeholder Engagement",
            "product innovation",
            "Generative AI",
            "Strategic Roadmap Development",
            "AI-driven products",
            "market insights",
            "Cross-Functional Collaboration",
            "Competitive Analysis"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Solytics Partners",
        "experience": "Fresher",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Expertise in developing Machine Learning (ML), Deep Learning (DL), and Generative AI (GenAI) solutions, including experience with transformer models (e.g., BERT, GPT, T5). Proficiency in developing Natural Language Processing (NLP) solutions, leveraging state-of-the-art deep learning architectures. Expert-level knowledge of Python programming, along with experience in production-level Python scripting, API development, and coding. Experience with Python-based frameworks such as FastAPI, Streamlit, and others. Experience in working on Linux environments, including shell scripting. Hands-on experience with SQL ecosystems and proficiency in working with data retrieval systems like Solr in Python. Strong knowledge of statistical modeling and the ability to apply statistical techniques to analyze datasets. Experience with Cloud Native technologies from Azure and AWS, including services such as Cognitive Search, Form Recognizer, and related AI offerings. Solid understanding of computer science fundamentals (data structures, algorithms, operating systems, databases). Good communication and organizational skills with a focus on detail, especially when working across teams of engineers, data scientists, and product designers. Excellent problem-solving skills, with a strong attention to detail and the ability to thrive in a fast-paced, distributed technical environment. Passionate about emerging technologies and AI trends, with a strong drive for continuous innovation and improvement. Self-motivated, able to manage priorities independently while working on multiple projects concurrently",
        "skills": [
            "Form Recognizer",
            "Cognitive Search",
            "Statistical Modeling",
            "Streamlit",
            "Solr",
            "Sql",
            "Linux",
            "Shell scripting",
            "Python Programming",
            "FastAPI",
            "Azure",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Dexian India",
        "experience": "2-4 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Title : Data Scientist\nWork Model : Remote\nWork Shift: 10:30 AM - 7:30 PM IST\nWork Experience: 2-3 years\nNotice period: Immediate Joiner\nAWS (Lambda, Glue, RCS, ECS, Data Pipeline, EMR), Databricks, Spark, PySpark, Python, SQL\nPython for Data Science (XGBoost, SciKit Learn, NLTK, SciPy, Shap, Prophet)\nDevelop promotion and pricing algorithms with client team.\nMaintain optimization applications by detecting data drift, model retraining, enhancements.\nDomain experience: Retail, Pricing, and Promotions.\nLooking for Forecasting, prophet, Multi arm bandit, Reinforcement learning, etc ML skills.\nLooking more at forecasting with FB Prophet, bandits, and ML development. Traditional DS with more experience in recommender systems and forecasting will be a better fit.",
        "skills": [
            "Shap",
            "Glue",
            "prophet",
            "Aws Lambda",
            "Rcs",
            "Scipy",
            "Pyspark",
            "Emr",
            "Sql",
            "Nltk",
            "Data Pipeline",
            "ECS",
            "XGBoost",
            "Spark",
            "Databricks",
            "Python",
            "Scikit Learn"
        ]
    },
    {
        "job_title": "Data Scientist ML",
        "company_name": "Team Geek Solutions",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nTeam Geek Solutions is a forward-thinking technology firm dedicated to empowering businesses through innovative data solutions. Our mission is to harness the power of data analytics and machine learning to drive strategic insights and informed decision-making. We pride ourselves on our collaborative culture, where diverse talents come together to solve complex problems and deliver exceptional value to our clients.\n\nKey Responsibilities\n\nExtract and analyze data from company databases to drive the optimization and\n\nenhancement of product development and marketing strategies.\n\nAnalyze large datasets to uncover trends, patterns, and insights that can influence business decisions.\n\nLeverage predictive and AI/ML modeling techniques to enhance and optimize\n\ncustomer experience, boost revenue generation, improve ad targeting, and more.\n\nDesign, implement, and optimize machine learning models for a wide range of applications such as predictive analytics, natural language processing, recommendation systems, and more.\n\nConduct experiments to fine-tune machine learning models and evaluate their performance using appropriate metrics.\n\nDeploy machine learning models into production environments, ensuring scalability\n\nQualifications\n\nBachelor's, Master's or Ph.D in Computer Science, Data Science, Mathematics, Statistics, or a related field.\n\n2+ years of experience in Analytics, Machine learning, Deep learning.\n\nProficiency in programming languages such as Python, and familiarity with machine learning libraries (e.g., Numpy, Pandas, TensorFlow, Keras, PyTorch, Scikit-learn).\n\nStrong experience with data wrangling, cleaning, and transforming raw data into structured, usable formats.\n\nHands-on experience in developing, training, and deploying machine learning models for various applications (e.g., predictive analytics, recommendation systems, anomaly detection).\n\nIn-depth understanding of machine learning algorithms (supervised, unsupervised,\n\nreinforcement learning) and their appropriate use cases.\n\nExperience with model evaluation techniques (e.g., cross-validation, A/B testing,\n\nperformance metrics).\n\nExperience with cloud platforms (AWS, GCP, Azure) for model deployment and scalal Proficiency in data processing and manipulation techniques.\n\nExcellent problem-solving skills and the ability to work effectively in a collabor environment.\n\nStrong communication skills to convey complex technical concepts to non-tech stakeholders.\n\nGood To Have\n\nExperience in the [banking/financial services/industry-specific] sector.\n\nFamiliarity with cloud-based machine learning platforms such as Azure, AWS, or GCP.\n\nSkills: natural language processing,numpy,azure,data wrangling,tensorflow,data analysis,data science,big data technologies,pandas,pytorch,cloud platforms,gcp,python,model evaluation,recommendation systems,keras,scikit-learn,data scientist,data visualization,interpersonal skills,aws,predictive analytics,statistical modeling,machine learning,deep learning,model deployment",
        "skills": [
            "Scikit-learn",
            "Model Evaluation",
            "Recommendation Systems",
            "Cloud Platforms",
            "Machine Learning",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Pandas",
            "Pytorch",
            "Gcp",
            "Keras",
            "data wrangling",
            "Predictive Analytics",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Talentica Software",
        "experience": "2-4 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "The Company\nTalentica Software is a boutique software development company started by ex-IITB grads and industry veterans. It is a privately held company. The company is 18 years old and 500+ employees work exclusively for startups as Tech Partners taking them from a Series-A position to Series-B position and possible acquisition (ex- Citrus Pay, Mist).\nWe have built products for over 150 startups, most of them are based in the Bay Area or Europe. These startups come to us primarily because we know the issues that plague startup product development and the solutions for the same, thereby improving their success chances. Owing to the unique space we are in, we deal extensively with cutting edge technology.\nData Science Team\nThe data science team works under the purview of the Technology Excellence Group at Talentica Software. The goal of this team is to solve problems and build algorithms that are typically data driven. Hence, problems involving statistics, optimization, computer vision, machine learning, and natural language processing are of interest to this group.\nScope for Hiring\nWe are looking to hire a Data Scientist with experience in different areas of Machine learning and is flexible to work in wide areas of ML.\nHere is what we are looking for in prospective candidates: Mandatory\nExcellent programming skills and must be able to implement complex algorithms in Python\nExtensive understanding of ML and statistical algorithms\nHands-on experience with use of standard machine learning libraries such as OpenCV, Tensorflow, Keras etc.\nBig data experience and should have worked in SQl/NoSQL DBs like Mongo/Cassandra/PostgreSQL/Neo4J\nCapability of converting PoC to production quality code and subsequent, deployment\nHere is what we are looking for in prospective candidates: Good to have\nCredited courses focused on linear algebra, stochastic models, pattern recognition, design and analysis of algorithms, machine learning\nInterest in applications of Deep learning like computer vision algorithms for video, shape recognition, matching & retrieval\nSome exposure to statistical modeling\nExperience:\nShould have worked in the industry for at least 2+ years.\nShould like to work in a startup environment.\nShould be capable of converting theory to practice by reading relevant papers.\nShould be capable of conceiving original ideas and coding them as working algorithms.\nShould have taken at least 2 products to production end-to-end.\nBE/B.Tech from IIT, NIT, BITS Pilani, COEP (Pune), VJTI, IIT-BHU or ISM.",
        "skills": [
            "Machine Learning",
            "Cassandra",
            "PostgreSQL",
            "Big Data",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Neo4j",
            "Opencv",
            "Keras",
            "Statistical Algorithms",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Ford Motor Company",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Automotive/Automobile/Ancillaries",
        "job_description": "JOB DESCRIPTION\nPotential candidates should have excellent depth and breadth of knowledge in machine learning, data mining, statistical modeling and Generative AI. They should possess the ability to translate a business problem into an analytical problem, identify the relevant data sets needed for addressing the analytical problem, recommend, implement, and validate the best suited analytical algorithm(s), and generate/deliver insights to stakeholders. Candidates are expected to be at the cutting-edge with respect to algorithms, tools, and techniques and have excellent business acumen and communication, focusing on delivering business value. The role is that of an individual contributor however, the candidate is expected to work in project teams of 2 to 3 people and interact with business partners on regular basis.\nRESPONSIBILITIES\nUnderstand business requirements and analyze datasets to determine suitable approaches to meet analytic business needs and support data-driven decision-making\nDesign and implement data analysis and ML models, hypotheses, algorithms and experiments to support data-driven decision-making\nApply various analytics techniques like data mining, predictive modeling, prescriptive modeling, math, statistics, advanced analytics, machine learning models and algorithms, etc. to analyze data and uncover meaningful patterns, relationships, and trends\nDesign efficient data loading, data augmentation and data analysis techniques to enhance the accuracy and robustness of data science and machine learning models, including scalable models suitable for automation\nResearch, study and stay updated in the domain of data science, machine learning, analytics tools and techniques etc. and continuously identify avenues for enhancing analysis efficiency, accuracy and robustness\nQUALIFICATIONS\nBachelor's degree in computer science, Operational research, Statistics, Applied mathematics, or in any other engineering discipline.\n2+ years of hands-on experience in Python programming for data analysis, machine learning, and with libraries such as NumPy, Pandas, Matplotlib, Scikit-learn, TensorFlow, PyTorch, NLTK, spaCy, and Gensim.\n2+ years of experience with both supervised and unsupervised machine learning techniques.\n2+ years of experience with data analysis and visualization using Python packages such as Pandas, NumPy, Matplotlib, Seaborn, or data visualization tools like Dash or QlikSense.\n1+ years experience in SQL programming language and relational databases.\n2+ years experience in any dashboarding tool such as Tableau, Power BI, DOMO and QlikSense\nPreferred Qualifications\nAn MS/PhD in Computer Science, Operational research, Statistics, Applied mathematics, or in any other engineering discipline. PhD strongly preferred.\nExperience working with Google Cloud Platform (GCP) services, leveraging its capabilities for ML model development and deployment.\nExperience with Git and GitHub for version control and collaboration.\nBesides Python, familiarity with one more additional programming language (e.g., C/C++/Java)\nStrong background and understanding of mathematical concepts relating to probabilistic models, conditional probability, numerical methods, linear algebra, neural network under the hood detail.\nExperience working with large language models such GPT-4, Google, Palm, Llama-2, etc.\nExcellent problem solving, communication, and data presentation skills.",
        "skills": [
            "Gensim",
            "Scikit-learn",
            "spaCy",
            "Matplotlib",
            "Github",
            "Google Cloud Platform",
            "Power Bi",
            "Tableau",
            "Sql",
            "Domo",
            "Tensorflow",
            "Numpy",
            "Git",
            "Nltk",
            "Pytorch",
            "Pandas",
            "Qliksense",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "FREED",
        "experience": "Fresher",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Role\nYou'll work at the intersection of Generative AI, Machine Learning, and personal finance. You'll partner with Product, Engineering, and Ops to:\nDrive smarter decision-making using AI.\nImprove customer outcomes by personalising experiences.\nLaunch agentic AI systems to streamline sales, support, and credit evaluation.\nKey Responsibilities\nDesign, build, and deploy machine learning and generative AI models.\nTranslate real-world business problems into data science solutions from ideation to production.\nCollaborate with engineers to set up scalable data pipelines and APIs.\nConstantly monitor model performance and retrain as needed.\nBuild solutions that leverage LLMs for tasks like summarization, sentiment detection, classification, and recommendations.\nStay up-to-date with developments in GenAI and actively experiment with new techniques.\nRequired Qualifications\nStrong foundation in machine learning, feature engineering, and model deployment.\nHands-on experience implementing GenAI use cases (e.g., prompt engineering, RAG, embeddings).\nProficiency in Python, SQL, and ML libraries like scikit-learn, XGBoost, TensorFlow, or PyTorch.\nFamiliarity with LLM frameworks (LangChain, OpenAI, HuggingFace) and cloud services (AWS/GCP/Azure).\nDemonstrated ability to take AI models from notebook to production.\nExcellent communication skills and ability to collaborate cross-functionally.\nPreferred Qualification\nPrior experience in Fintech, BFSI, or working with credit and customer data.\nUnderstanding of ethical AI, data privacy, and model fairness.\nFamiliarity with customer-facing AI products (chatbots, agentic workflows).\nWhat We Offer\nA high-impact role at a company solving a real societal problem.\nThe opportunity to shape AI strategy and product direction from the ground up.\nA culture that values curiosity, ownership, and bold thinking.",
        "skills": [
            "LangChain",
            "scikit-learn",
            "generative AI",
            "HuggingFace",
            "OpenAI",
            "Machine Learning",
            "Sql",
            "Tensorflow",
            "Gcp",
            "Pytorch",
            "XGBoost",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist ML",
        "company_name": "Team Geek Solutions",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nTeam Geek Solutions is a forward-thinking technology firm dedicated to empowering businesses through innovative data solutions. Our mission is to harness the power of data analytics and machine learning to drive strategic insights and informed decision-making. We pride ourselves on our collaborative culture, where diverse talents come together to solve complex problems and deliver exceptional value to our clients.\n\nKey Responsibilities\n\nExtract and analyze data from company databases to drive the optimization and\n\nenhancement of product development and marketing strategies.\n\nAnalyze large datasets to uncover trends, patterns, and insights that can influence business decisions.\n\nLeverage predictive and AI/ML modeling techniques to enhance and optimize\n\ncustomer experience, boost revenue generation, improve ad targeting, and more.\n\nDesign, implement, and optimize machine learning models for a wide range of applications such as predictive analytics, natural language processing, recommendation systems, and more.\n\nConduct experiments to fine-tune machine learning models and evaluate their performance using appropriate metrics.\n\nDeploy machine learning models into production environments, ensuring scalability\n\nQualifications\n\nBachelor's, Master's or Ph.D in Computer Science, Data Science, Mathematics, Statistics, or a related field.\n\n2+ years of experience in Analytics, Machine learning, Deep learning.\n\nProficiency in programming languages such as Python, and familiarity with machine learning libraries (e.g., Numpy, Pandas, TensorFlow, Keras, PyTorch, Scikit-learn).\n\nStrong experience with data wrangling, cleaning, and transforming raw data into structured, usable formats.\n\nHands-on experience in developing, training, and deploying machine learning models for various applications (e.g., predictive analytics, recommendation systems, anomaly detection).\n\nIn-depth understanding of machine learning algorithms (supervised, unsupervised,\n\nreinforcement learning) and their appropriate use cases.\n\nExperience with model evaluation techniques (e.g., cross-validation, A/B testing,\n\nperformance metrics).\n\nExperience with cloud platforms (AWS, GCP, Azure) for model deployment and scalal Proficiency in data processing and manipulation techniques.\n\nExcellent problem-solving skills and the ability to work effectively in a collabor environment.\n\nStrong communication skills to convey complex technical concepts to non-tech stakeholders.\n\nGood To Have\n\nExperience in the [banking/financial services/industry-specific] sector.\n\nFamiliarity with cloud-based machine learning platforms such as Azure, AWS, or GCP.\n\nSkills: natural language processing,numpy,azure,data wrangling,tensorflow,data analysis,data science,big data technologies,pandas,pytorch,cloud platforms,gcp,python,model evaluation,recommendation systems,keras,scikit-learn,data scientist,data visualization,interpersonal skills,aws,predictive analytics,statistical modeling,machine learning,deep learning,model deployment",
        "skills": [
            "Scikit-learn",
            "Model Evaluation",
            "Recommendation Systems",
            "Machine Learning",
            "Natural Language Processing",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Gcp",
            "Pytorch",
            "Pandas",
            "Keras",
            "data wrangling",
            "Predictive Analytics",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Haber",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Role:\nAt Haber, we are redefining how industries operate by blending automation with intelligent decision making. As a Data Scientist, you will play a pivotal role in designing machine learning systems that power our suite of innovative productsincluding Elixa, Kaiznn, Mount Fuji, and upcoming platforms like our Fiber Morphology solution. You'll work extensively with time-series data, sensor feeds, and computer vision pipelines to uncover actionable insights in real-time. This is a high-impact, high-ownership role where your work will directly influence operational efficiency and sustainability across complex industrial environments. We value independent thinkers who are not just technical experts but also strategic problem-solvers.\nKey Responsibilities:\nLead the full machine learning system development lifecycle: data collection, cleaning, feature engineering, modeling, deployment, and evaluation\nWork with time-series sensor data and computer vision systems to build predictive and anomaly detection models\nDesign and develop scalable recommendation platforms for real-time industrial decision-making Apply machine learning, data mining, and statistical techniques (e.g., regression, clustering, collaborative filtering, PCA) to a wide range of problems\nDesign novel approaches to large-scale data analysis, including semi-structured and unstructured data\nCollaborate with engineering, QA, product, and operations teams to deliver end-to-end solutions Contribute to and stay up-to-date with research in areas such as machine learning, signal processing, and domain-specific optimization\nMake independent technical decisions and guide strategic direction based on data insights Mentor junior team members and foster a collaborative learning culture\nQualification:\nHaving 3-6 years of experience as a Data Scientist, preferably in a client facing role taking complete ownership of the Data Science lifecycle\nStrong programming skills in Python or R\nProven experience working with time-series data and real-time data pipelines\nSolid foundation in supervised and unsupervised ML methods\nExposure to sensor data, image processing, or computer vision techniques is a strong plus Experience with cloud environments, preferably AWS, for model deployment and monitoring Sound understanding of statistics, optimization, and data visualization\nAbility to work independently and drive projects from idea to execution\nStrong communication skills and the ability to collaborate across multidisciplinary teams\nA passion for solving tough, real-world problems using data\nWhat We Offer:\nReal-World Impact: Your models will be deployed in operational environments, not just experimental labs\nOwnership & Autonomy: Freedom to explore, decide, and build in a high-trust environment Diverse Tech Stack: Work on ML pipelines that combine time-series, sensor, and vision data\nFast-Track Learning: Be part of a startup where learning never stops and innovation is constant Collaborative Culture: A team that supports initiative, experimentation, and personal growth Product-Minded Thinking: Your work will be tightly integrated with product and business decisions",
        "skills": [
            "supervised ML methods",
            "cloud environments",
            "time-series data",
            "real-time data pipelines",
            "R",
            "sensor data",
            "Statistical Techniques",
            "Statistics",
            "Optimization",
            "unsupervised ML methods",
            "Image Processing",
            "Data Visualization",
            "Machine Learning",
            "AWS",
            "data mining",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Blackcoffer",
        "experience": "Fresher",
        "salary": null,
        "location": "Delhi, India",
        "industry": "Login to check your skill match score",
        "job_description": "Data Scientist Associate Responsibilities\nAs a selected intern, your daily tasks will include:\nEngaging in data science projects and analytics.\nDeveloping and implementing data models, AI, ML, deep learning, NLP, GenAI, LangChain, LLM, LLAMA, OpenAI, and GPT-based solutions.\nManaging data pipelines, ETL/ELT processes, and data warehousing.\nUtilizing Python and its libraries for advanced programming tasks.\nHandling data collection, management, cleaning, and transformation.\nCreating data visualizations using BI tools such as Power BI, Kibana, and Google Data Studio.\nWorking with databases like MongoDB, Neo4j, Dgraph, and SQL.\nLeveraging cloud platforms, including GCP, AWS, Azure, Linode, and Heroku.\nRequired Skills\nPython\nFlask\nDjango\nMongoDB\nAPI Development\nElasticsearch\nMachine Learning\nArtificial Intelligence\nJob Details\nWork Mode: Remote (Work From Home)\nStart Date: Immediate\nDuration: 6 months\nStipend: 10,000 12,000 per month\nIndustry: Information Technology & Services\nEmployment Type: Probation of 6 Months followed by Full-time Position based on performance",
        "skills": [
            "GenAI",
            "Google Data Studio",
            "LLAMA",
            "Linode",
            "Llm",
            "GPT-based solutions",
            "LangChain",
            "Ai",
            "OpenAI",
            "Kibana",
            "Machine Learning",
            "Data Warehousing",
            "ELT",
            "Deep Learning",
            "Nlp",
            "Elasticsearch",
            "Flask",
            "Data Visualization",
            "Python",
            "AWS",
            "Ml",
            "Heroku",
            "Artificial Intelligence",
            "Power Bi",
            "Api Development",
            "Django",
            "Gcp",
            "MongoDB",
            "Azure",
            "Etl"
        ]
    },
    {
        "job_title": "Gen AI Data Scientist - Wholesale Credit Risk",
        "company_name": "JPMorganChase",
        "experience": "Fresher",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nPosition\n\nThis role will be part of the Wholesale Credit Risk Quantitative Research Applied AI/ML team that has been tasked to develop Generative AI solutions on top of the firm's big data resources. In particular, the role will focus on building tools based on LLM to enhance the current End-to-End credit risk process across all of Wholesale.\n\nResponsibilities Include But Not Limited To\n\nHave deep understanding in modern Machine Learning methodologies, LLM and NLP techniques, and apply thoughtful data science and analytical skills to solve complex business problems.\nDevelop risk strategies that improve risk monitoring capabilities through the use of data from various source.\nAnalyze structured/unstructured data from internal and external data sources to drive actionable insights in credit risk.\nLead development and rapid deployment AI solutions based on macro-economic factors and current events on the Wholesale portfolio.\nDevelop data visualization and summarization techniques to convey key findings in dashboards and presentations to senior management.\n\nQualifications\n\nAdvanced degree in analytical field (e.g., Data Science, Computer Science, Engineering, Mathematics, Statistics)\nDeep understanding and practical expertise and/or work experience with Machine Learning. LLM/NLP expertise or experience is strongly preferred\nExperience across broad range of modern analytic and data tools, particularly Python/Anaconda, Tensorflow and/or Keras/PyTorch, Spark, SQL etc. Experience working on Cloud is preferred\nExperience with model implementation/production deployment is preferred\nExcellent problem solving, communications, and teamwork skills\nFinancial service background preferred, but not required\nDesire to use modern technologies as a disruptive influence within Banking\n\nAbout Us\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nJ.P. Morgan's Commercial & Investment Bank is a global leader across banking, markets, securities services and payments. Corporations, governments and institutions throughout the world entrust us with their business in more than 100 countries. The Commercial & Investment Bank provides strategic advice, raises capital, manages risk and extends liquidity in markets around the world.",
        "skills": [
            "Anaconda",
            "Llm",
            "Tensorflow",
            "Machine Learning",
            "Nlp",
            "Cloud",
            "Pytorch",
            "Spark",
            "Keras",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "ZS",
        "experience": "1-3 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Role:\nWe are seeking a highly analytical and technically skilled ADS Associate to join our team. This role is ideal for individuals with a strong academic foundation in quantitative fields and experience working with large datasets and modern analytical tools. As an ADS Associate, you will leverage advanced analytics, statistical modeling, and machine learning to uncover insights and help solve complex business challenges.\nKey Responsibilities:\nDesign and develop advanced, statistically effective algorithms for solving high-dimensional problems.\nApply statistical and data mining techniques such as hypothesis testing, machine learning, text mining, and predictive modeling to analyze trends and generate insights.\nCreate visualizations and figures to clearly communicate analytical findings.\nCollaborate closely with clients and stakeholders to understand business needs and deliver actionable insights.\nEvaluate and integrate new datasets and technologies into the existing analytical platform.\nContribute to the continuous improvement of analytical processes and tools.\nRequired Qualifications:\n12 years of relevant industry experience in data analytics, statistical modeling, or related fields.\nStrong academic background with coursework emphasizing analytical and quantitative skills.\nProficiency in at least one programming language such as Java, Python, or R.\nHands-on experience with platforms and tools like the Hadoop ecosystem, Amazon Web Services (AWS), or other database systems.\nFamiliarity with big data concepts and advanced analytical methods (e.g., recommender systems, social listening, etc.).\nExcellent communication skills and fluency in English.\nPreferred Skills:\nExperience working in cross-functional teams.\nStrong problem-solving and critical-thinking abilities.\nAbility to work in a fast-paced, dynamic environment.",
        "skills": [
            "R",
            "Statistical Modeling",
            "Java",
            "Machine Learning",
            "Hypothesis Testing",
            "Hadoop Ecosystem",
            "data mining",
            "Predictive Modeling",
            "Text Mining",
            "Python"
        ]
    },
    {
        "job_title": "Risk Analytics Data Scientist",
        "company_name": "PayPal",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWe are seeking a highly motivated and experienced Risk Analytics professional to join our Enterprise Risk Management Organization.\n\nThis role is responsible for providing risk analytics support to Financial and Model Risk oversight groups, including but not limited to Credit, Collection, Fraud, Seller, Acquiring, Treasury oversight. This role connects the dots between business, model, and data by providing insights through in-depth data analysis. This role also provides critical support in ad hoc analysis required for business enablement and new risk initiatives to support PP's growth.\n\nThis role is part of the broader AI/Model/Quant Risk Oversight function.\nThe AI/Model/Quant Risk Oversight team:\n. Oversees and proactively challenges our model use and AI application.\n. Supports effective model development/deployment and responsible AI innovation\n. Ensure model/AI practice meets regulatory compliance requirement, strengthening decision-making, protecting customers and safeguarding the company from financial and reputational damage\n. Partners with financial risk oversight groups in conducting in-depth, thematic reviews of various products and risk functions, providing data insights and expertise around strategy development and risk mitigations\nJob Description:\nYou will make an impact by ensuring that\nSupport Credit & Collection, Fraud, Seller, Merchant acquiring, Treasury oversight with data insights to enable effective challenges\nProvide critical support in ad hoc analysis required for business enablement and risk initiatives\nLeverage AI to enhance risk analytics functionality to bring out more data insights\nContribute to the major uplift of data infrastructure and other requirement per PPEU ECB supervision readiness project\nIn your day-to-day role you will\nOversee and challenge risk analytics practices across various groups within the company, primarily focused on Fraud, Seller and Credit risks, including use of machine learning and advanced analytics to detect and mitigate risks\nConduct root cause analysis to identify opportunities and gaps in existing strategies and controls.\nDevelop and analyze multiple reports and dashboards, looking at key risk indicators and relevant business metrics to uncover emerging trends and areas of opportunities\nPartner with financial risk oversight groups in conducting in-depth, thematic reviews of various products and risk functions, providing data insights and expertise around strategy development and analytics.\nEducation:\nA master's or advanced degree in a quantitative field such as statistics, mathematics, finance, or economics.\nExperience:\nGood understanding of financial and banking risk\nRobust coding skills for big data and AI systems\nAble to builds relationships across multiple teams and at various levels\nHas a flexible and curious mindset. Understands the business needs and adapts risk controls accordingly.\nExceptional communication, writing, and presentation skills to convey technical concepts to stakeholders at all seniority levels (executives, regulators, auditors, etc.).\nAbility to work effectively both independently and collaboratively in a high-pressure, fast-paced environment\nExperience in risk (Model, Fraud, Credit, Market, Liquidity, etc.) or similar highly quantitative roles is a plus\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\n-\nPayPal is committed to fair and equitable compensation practices.\nActual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.\nThe total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit .\nThe US national annual pay range for this role is $116,500 to $173,250\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": []
    },
    {
        "job_title": "Data Scientist",
        "company_name": "v4c.ai",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Description\n\nv4c.ai is a premier IT services consultancy specializing in Dataiku, the Universal AI platform, to drive strategic business transformation. The company partners with organizations to accelerate their journey towards AI-driven success by offering Dataiku and generative AI services. v4c.ai's expertise in implementation, optimization, and enablement empowers clients to harness the full potential of their data, unlocking significant competitive advantages and fostering innovation.\n\nRole Description\n\nThis is a full-time remote role for a Data Scientist at v4c.ai. The Data Scientist will be responsible for tasks such as data science, statistics, data analytics, data visualization, and data analysis to drive strategic business transformation and innovation.\n\nQualifications\n\nData Science skills\nStatistics skills\nData Analytics skill\nData Visualization and Data Analysis skills\nStrong problem-solving and analytical skills\nExperience with AI platforms like Dataiku is a plus\nAdvanced degree in Data Science, Statistics, Computer Science, or related field",
        "skills": [
            "Data Analysis",
            "Statistics",
            "Data Science",
            "Data Visualization",
            "Data Analytics"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Dimensionless Technologies",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "We're seeking a skilled Data Scientist with expertise in SQL, Python, AWS SageMaker, and Commercial Analytics to contribute to Team. You'll design predictive models, uncover actionable insights, and deploy scalable solutions to recommend optimal customer interactions. This role is ideal for a problem-solver passionate about turning data into strategic value.\nKey Responsibilities\nModel Development: Build, validate, and deploy machine learning models (e.g., recommendation engines, propensity models) using Python and AWS SageMaker to drive next-best-action decisions.\nData Pipeline Design: Develop efficient SQL queries and ETL pipelines to process large-scale commercial datasets (e.g., customer behavior, transactional data).\nCommercial Analytics: Analyze customer segmentation, lifetime value (CLV), and campaign performance to identify high-impact NBA opportunities.\nCross-functional Collaboration: Partner with marketing, sales, and product teams to align models with business objectives and operational workflows.\nCloud Integration: Optimize model deployment on AWS, ensuring scalability, monitoring, and performance tuning.\nInsight Communication: Translate technical outcomes into actionable recommendations for non-technical stakeholders through visualizations and presentations.\nContinuous Improvement: Stay updated on advancements in AI/ML, cloud technologies, and commercial analytics trends.\nQualifications\nEducation: Bachelor's/Master's in Data Science, Computer Science, Statistics, or a related field.\nExperience: 3-4 years in data science, with a focus on commercial/customer analytics (e.g., pharma, retail, healthcare, e-commerce, or B2B sectors).\nTechnical Skills:\nProficiency in SQL (complex queries, optimization) and Python (Pandas, NumPy, Scikit-learn).\nHands-on experience with AWS SageMaker (model training, deployment) and cloud services (S3, Lambda, EC2).\nFamiliarity with ML frameworks (XGBoost, TensorFlow/PyTorch) and A/B testing methodologies.\nAnalytical Mindset: Strong problem-solving skills with the ability to derive insights from ambiguous data.\nCommunication: Ability to articulate technical concepts to business stakeholders.\nPreferred Qualifications\nAWS Certified Machine Learning Specialty or similar certifications.\nExperience with big data tools (Spark, Redshift) or ML Ops practices.\nKnowledge of NLP, reinforcement learning, or real-time recommendation systems.\nExposure to BI tools (Tableau, Power BI) for dashboarding.",
        "skills": [
            "Scikit-learn",
            "AWS SageMaker",
            "S3",
            "Sql",
            "Pytorch",
            "Tensorflow",
            "Python",
            "Lambda",
            "Ec2",
            "Numpy",
            "XGBoost",
            "Pandas"
        ]
    },
    {
        "job_title": "Senior Data Scientist Credit Risk",
        "company_name": "PayPal",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nAs PayPal continues its mission to revolutionize commerce, we are looking for a skilled Senior Data Scientist to join our Buy Now Pay Later (BNPL) Risk Team, focusing on the France BNPL products. In this role, you will use data to drive key business decisions, optimize credit risk strategies, and help shape effective credit risk management at PayPal. If this intro sparks your interest, read on - the best is yet to come!\nJob Description:\nYour Way to Impact\nAs a Senior Data Scientist for the France BNPL products, you will be an integral part of the European and Australian Buy Now Pay Later Risk Team. You will analyze consumer risk trends, recommend optimal credit risk strategies, and help optimize the BNPL funnel. This is a highly data-driven role in a high-impact credit risk organization, where your insights will drive business outcomes and improve the customer experience.\nYour Day-to-Day\nCollaborate with P&L owners and stakeholdersto analyze, propose, and implement effective strategies across the entire BNPL lifecycle.\nAssess the P&L impact of credit strategies, ensuring the optimal balance between risk and revenue.\nProvide leadership with insights on book quality, offering recommendations on risks and opportunities.\nExplore internaland external data sourcesto evaluate their potential contributions to the credit underwriting process.\nPartner with compliance and legal teamsto ensure all credit strategies comply with applicable regulatory guidelines.\nWorkcloselywithoperationsteamsto assess the impact of credit strategies on customer communications and collections efforts.\nWhat you'll need to succeed:\nExpertise in Credit Risk Management: In-depth knowledge of credit performance, data providers, scoring models, and industry best practices.\nExperience in Credit & Fraud Risk: Strong background in risk management and underwriting for consumer credit products (e.g., credit cards, installment loans, revolving credit), with a preference for fintech experience.\nAdvanced Analytics Expertise: Proficiency in SQL, Python, Advanced Excel, Tableau, and other analytics tools, with a proven track record of using them to solve real-world problems.\nData-Driven Decision-Making: Hands-on experience working with credit bureaus and other consumer data sources to drive strategic decision-making.\nExceptional Communication Skills: Outstanding written, verbal, and interpersonal communication abilities, capable of translating complex technical concepts into clear, actionable insights for diverse audiences.\nStrategic & Creative Problem-Solving: Strong judgment and the ability to think strategically, creatively, and practically to address complex challenges.\nCollaboration and Influence: Strong ability to collaborate across teams, build relationships, and drive results through influence and teamwork.\nWe Believe in You\nReady to join our ride Click apply or/and - We can't wait to hear from you!\n_\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at.\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": []
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Apex Tigre",
        "experience": "Fresher",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Real Estate",
        "job_description": "Skills Required\n\nData Science\n\nDuties & Responsibilities\n\nAdvanced statistics, predictive analytics.\nAdvanced object-oriented programming\nHadoop, MySQL, TensorFlow, Spark\nMachine learning, data modeling\nGood aptitude and communication\n\nEducation & Training\n\nIT Education",
        "skills": [
            "Advanced statistics",
            "Advanced object-oriented programming",
            "Tensorflow",
            "Hadoop",
            "MySQL",
            "Spark",
            "Machine Learning",
            "Data Modeling",
            "Predictive Analytics"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "SLB",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About us\nWe are a global technology company, driving energy innovation for a balanced planet.\nTogether, we create amazing technology that unlocks access to energy for the benefit of all.\nOur inclusive culture is the key to our success. We collaborate with our internal community of colleagues, alumni, and our valued external partners to support each other and achieve our goals. We aim to raise the bar high. We look for people who are committed to innovation and success and act with integrity to become and be a part of one of the most diverse group of experts in our industry, anywhere around the globe.\nGlobal in outlook, local in practice and with a united, shared passion for discovering solutions, we hire talented, driven people and support them to succeed, personally and professionally.\nJob Description\nJob Title: Data Scientist\nLocation: PuTC, Pune\nJob Summary:\nWe are looking for data scientists to join the PuTC Data Science team. The Data Scientists will be responsible for designing and executing undirected research and tackling open-ended data problems related to oil and gas domain using advanced techniques in AI (Artificial Intelligence), ML (Machine learning), DL (deep learning) etc. The data scientists will be working with very complex and large data sets. The individual will be responsible for collaborating with various cross functional teams and providing periodic updates through presentations and prototype demonstrations. The role will require working on multiple projects simultaneously.\nResponsibilities\nAssimilate next-generation technologies for data-driven solutions in oil and gas domain\nGenerate innovative ideas, establish new research directions, and shape and execute on research projects\nApply engineering knowledge in developing data-driven algorithms and prototypes for classification, regression, anomaly detection, failure prediction, and optimization\nCollaborate with business and engineering teams to implement robust and scalable industrial software solutions\nContribute to technical discussions and undertake reviews of different projects as an expert\nCommunicate ideas, plans, and results effectively to different stake holders\nParticipate in technical forums and other appropriate events and conferences\nQualifications and Requirements\nM.S/MTech/PhD in computer science, mathematics, physics, applied sciences, engineering or similar disciplines with demonstrated capability in field of algorithm development using General AI, Generative AI with 3-5 years of experience,\nDeep understanding of Generative AI, deep learning algorithms and their applications to text analytics/NLP, computer vision etc\nStrong background in retrieval augmented generation (RAG), agentic frameworks, etc. would be preferred.\nStrong background in linear algebra, statistics, and probability\nStrong background in Python programming\nHand-on experience in developing deep learning models using TensorFlow, Keras, Pytorch, etc. will be a plus\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nSLB is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or other characteristics protected by law.",
        "skills": [
            "Generative AI",
            "Statistics",
            "probability",
            "linear algebra",
            "Text Analytics",
            "Machine Learning",
            "Artificial Intelligence",
            "Deep Learning",
            "Tensorflow",
            "Pytorch",
            "Keras",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Gameskraft",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Gameskraft\nEstablished in 2017, Gameskraft has become one of India's fastest-growing companies. We are building the world's most-loved online gaming ecosystem, one game at a time. Started by a group of passionate gamers, we have grown from a small team of 5-6 members to a large family of 600+ Krafters, working out of our office in Prestige Tech Park, Bangalore.\nOur short-term success lies in the fact that we strive to focus on building a safe, secure, and responsible gaming environment for everyone. Our vision is to create unmatched experiences every day and everywhere. We set the highest benchmarks in the industry in terms of design, technology, and intuitiveness. We are the industry's only ISO 27001 and ISO 9001 certified company.\nSenior Data Scientist\nAs the Senior Data Scientist, you will be working hands on day to day, driving the development and implementation of advanced analytics and machine learning solutions. You will play a critical role in shaping the data strategy, guiding the team in extracting meaningful insights from large datasets, and ensuring the successful deployment of data-driven solutions to meet business objectives.\nAs a Senior Data Scientist, you will :\nData Strategy:\nDevelop and execute a comprehensive data strategy that aligns with the organization's objectives.\nIdentify and prioritize data science opportunities that drive business value.\nAdvanced Analytics and Machine Learning:\nOversee the design, development, and implementation of advanced analytics and machine learning models.\nEnsure the team is utilizing state-of-the-art techniques and technologies to solve complex business problems.\nData Governance and Quality:\nEstablish and enforce data governance practices to ensure the accuracy and reliability of data used in analytics.\nCollaborate with data engineering teams to ensure data quality and integrity.\nCollaboration:\nCollaborate with business stakeholders to understand their needs and translate them into data science initiatives.\nWork closely with IT and business leaders to integrate data science solutions into existing business processes.\nInnovation:\nStay abreast of industry trends and emerging technologies in data science and analytics.\nFoster a culture of innovation within the data science team.\nPerformance Measurement:\nDefine and monitor key performance indicators (KPIs) to assess the impact of data science initiatives.\nContinuously evaluate and improve the effectiveness of data science solutions.\nCommunication:\nEffectively communicate complex technical concepts to non-technical stakeholders.\nPrepare and present regular updates and reports to senior management.\nWhat we expect you will bring to the table :\nLooking for a Senior Data Scientist role with an experience in the range of 2-4 years. Here are some of the responsibilities expected from the role:\n1. Responsible for solving open-ended, non-trivial, high-impact business problems related to overall user experience, fraud detection, personalisation, responsible game play (to name a few) etc. using machine/deep learning, optimization, statistical analysis and related techniques. 2. Drive ML/DS projects independently end-to-end right from engaging with different stakeholders and partner teams (product; engineering; business; marketing etc) across Gameskraft; understanding the problem statement clearly; coming up with appropriate design and solution in a fast, iterative and time bound manner. 3. Develop a strong understanding of key business metrics and concepts and perform data analysis to get useful insights and develop different hypotheses and validate with sound rigorous methodologies as a precursor to formulating the problems and modeling with ML. 4. Carry out the literature review (understand ML/DS related scientific papers around the given problem domain); exploratory data analysis; feature engineering; model building; integrate them in product and work on continuously improving them. 5. Work with and mentor the fellow data scientists on the team.\nIf you are passionate about creating exceptional user experiences, possess strong leadership skills, and have a track record of delivering successful data science, we encourage you to apply for this exciting opportunity.\nWe are committed to providing equal opportunity in employment and creating an inclusive work environment.\nRemember, together, we can achieve more!",
        "skills": [
            "Optimization",
            "Statistical Analysis",
            "Advanced Analytics",
            "Deep Learning",
            "Machine Learning"
        ]
    },
    {
        "job_title": "Data Scientist - Voice",
        "company_name": "Spydra",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "We are looking for an exceptional Data Scientist with deep expertise in speech technologies, advanced NLP, and LLM fine-tuning to join our cutting-edge AI research team. In this pivotal role, you will be responsible for building and optimizing state-of-the-art machine learning pipelines that drive intelligent audio and language-based products. Your work will directly contribute to the development of next-generation AI solutions that are privacy-focused, high-performance, and built for scale.\nKey Responsibilities:\nDevelop and deploy real-time ASR pipelines, leveraging models like Whisper, wav2vec2, or custom speech models.\nDesign and implement robust intent detection and entity extraction systems, utilizing transcribed speech, keyword spotting, and semantic pattern recognition.\nFine-tune LLMs and transformer architectures (BERT, RoBERTa, etc.) for tasks including intent classification, entity recognition, and contextual comprehension.\nOptimize end-to-end pipelines for mobile and on-device inference, employing tools like TFLite, ONNX, quantization, and pruning to achieve low-latency performance.\nCollaborate closely with AI product teams and MLOps engineers to ensure seamless deployment, continuous iteration, and performance monitoring.\nRequired Technical Skills:\nHands-on experience with ASR models (Whisper, wav2vec2, DeepSpeech, Kaldi, Silero), with a focus on fine-tuning for Indian languages and multilingual scenarios.\nStrong command of NLP techniques such as keyword spotting, sequence labeling, masked token prediction, and rule-based classification.\nProven track record in LLM and transformer fine-tuning for NER, intent detection, and domain-specific adaptation.\nExpertise in speech metadata extraction, feature engineering, and signal enrichment.\nProficiency in model optimization methods like quantization-aware training (QAT), pruning, and efficient runtime deployment for edge devices.\nExcellent Python skills with proficiency in PyTorch or TensorFlow, along with solid experience in NumPy, pandas, and real-time data processing frameworks.\nQualifications:\nBachelor's or Master's degree in Computer Science, Electrical Engineering, Data Science, or a related technical field.\nAcademic or industry background in speech processing, ASR, telecom analytics, or applied NLP is highly desirable.\nPortfolio showcasing real-world speech/NLP projects, open-source contributions, or published research will be a strong advantage.\nExperience:\n3 to 6+ years of applied experience in speech AI, NLP for intent detection, or machine learning model development.\nProven success in building, deploying, and optimizing ML models for real-time, low-latency environments.\nContributions to leading open-source projects like openai/whisper, mozilla/DeepSpeech, or facebook/wav2vec2 are highly valued.",
        "skills": [
            "speech metadata extraction",
            "ASR models",
            "NLP techniques",
            "real-time data processing frameworks",
            "LLM and transformer fine-tuning",
            "Pytorch",
            "Tensorflow",
            "Python",
            "Pandas",
            "Numpy"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "MiQ",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Principal Data Scientist\n\nLocation: Bengaluru\n\nWHAT YOU'LL DO\n\nWe're MiQ, a global programmatic media partner for marketers and agencies. Our people are at the heart of everything we do, so you will be too. No matter the role or the location, we're all united in the vision to lead the programmatic industry and make it better.\n\nAs a Principal Data Scientist in our data science department, you'll have the chance to:\n\nProvide technical leadership and strategic direction for the data science practice across multiple product teams.\nFocus on creating new business opportunities leveraging innovative data science technologies and approaches to solve complex business problems\nDrive end-user usage and consumption of Data Science solutions while also striving for technical excellence.\nMentor and guide senior data scientists and engineers, fostering technical growth and excellence within the team.\nDriving best practices across teams for - code development, code versioning, unit tests, testing strategies, pipeline architecture, model development & deployment, use of cloud applications, use of AI/ML platforms.\nLead the evaluation and implementation of new data science models, platforms, and technologies to enhance the organization's capabilities.\n\nWho are your stakeholders\n\nInternal Consumer/Stakeholders:\n\nEnd Users (Account Managers / Traders / Sales / Analysts): Gather insights into business challenges and user feedback to propose solutions and features for enhanced user experience.\nDevelopment Team: Collaborate with Technology Managers to foster technical excellence in product development, ensuring the creation of robust, proven, scalable, data science-driven, and high-impact solutions.\n\nWhat You'll Bring\n\nBachelor's degree in Statistics, Mathematics, Data Science, Engineering, Physics, Economics, or a related quantitative field.\n10 years of hands-on work experience in data science modelling to solve product or business problems.\nExperience translating product and business questions into clearly framed investigative projects, performing statistical analysis, and coding (e.g., Python, R, SQL)\nAbility to present to business leaders and communicate effectively with highly technical leadership teams.\nUnderstanding of AI Models, Large Language Models, and AI specialized infrastructure especially as it relates to AI trends and enablers within businesses.\nCandidates with the ability to set-up and ensure continuous improvement of MLOps & CI/CD processes will be preferred.\nExperience in the advertising or ad-tech industry will be a plus.\n\nWe've highlighted some key skills, experience and requirements for this role. But please don't worry if you don't meet every single one. Our talent team strives to find the best people. They might see something in your background that's a fit for this role or another opportunity at MiQ.\n\nIf you have a passion for the role, please still apply.\n\nWhat impact will you create\n\nLead the technical strategy and execution of data science initiatives that deliver significant impact across interdependent product lines.\nElevate the technical capabilities of the data science team by driving the adoption of advanced methodologies and best practices.\nEnsure the rigor, quality, and ethical integrity of data science outputs across all product domains\nAccelerate the experimentation of data science applications by developing rapid POC/MVP to drive business impact.\n\nWhat's in it for you\n\nOur Center of Excellence is the very heart of MiQ, and it's where the magic happens. It means everything you do and everything you create will have a huge impact on our entire global business.\n\nMiQ is incredibly proud to foster a welcoming culture. We do everything possible to make sure everyone feels valued for what they bring. With global teams committed to diversity, equity, and inclusion, we're always moving towards becoming an even better place to work.\n\nValues\n\nOur values are so much more than statements. They unite MiQers in every corner of the world. They shape the way we work and the decisions we make. And they inspire us to stay true to ourselves and to aim for better. Our values are there to be embraced by everyone so that we naturally live and breathe them. Just like inclusivity, our values flow through everything we do - no matter how big or small.\n\nWe do what we love - Passion\nWe figure it out - Determination\nWe anticipate the unexpected - Agility\nWe always unite - Unite\nWe dare to be unconventional - Courage\n\nBenefits\n\nEvery region and office has specific perks and benefits, but every person joining MiQ can expect:\n\nA hybrid work environment\nNew hire orientation with job-specific onboarding and training\nInternal and global mobility opportunities\nCompetitive healthcare benefits\nBonus and performance incentives\nGenerous annual PTO paid parental leave, with two additional paid days to acknowledge holidays, cultural events, or inclusion initiatives.\nEmployee resource groups are designed to connect people across all MiQ regions, drive action, and support our communities.\n\nApply today!\n\nEqual Opportunity Employer",
        "skills": [
            "R",
            "CI CD processes",
            "Large Language Models",
            "AI Models",
            "MLops",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ampstek",
        "experience": "5-7 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Title: Data Scientist\nLocation: India, Remote\nFulltime.\nJob Description:\nWhat does a Data Scientist do\nAs a data scientist, you will be responsible for analyzing and interpreting complex data sets to identify patterns and insights. You will be developing data models and algorithms to support the project needs. Implement data validation strategies to ensure high data quality. Collaborate with AI/ML engineers to integrate data models into the conversion engine system. Present findings and recommendations to stakeholders and management. Not being afraid to fail fast is one of the essential tools an NL Data Scientist can have in the cutting-edge technology market.\nWhat you will do:\nDevelop AI-Driven Data Conversion Application: Utilize AI technologies to develop a comprehensive data conversion application.\nData Mapping: Implement AI algorithms for accurate data mapping from source to target.\nCode Generation: Automate code generation using AI models.\nData Validation: Apply AI-driven validation techniques to ensure data integrity and quality.\nCode Conversion: Facilitate seamless code conversion using AI algorithms.\nCollaboration: Work with data engineers, software developers, AI solution engineers, and business analysts to integrate the data conversion application.\nMonitor and Troubleshoot: Continuously monitor and troubleshoot the AI-driven application to ensure optimal functionality.\nDocumentation: Maintain detailed documentation of methodologies and processes used in the application development.\nStay Updated with Technologies: Keep up-to-date with the latest AI, data science, and data conversion technologies to enhance the application.\nWhat you will need to have:\nBachelor's or Master's Degree in Data Science, Computer Science, Artificial Intelligence, or a related field.\nProficiency in Programming Languages such as Python, SQL, and other relevant languages.\nExperience with AI/ML Frameworks like TensorFlow, PyTorch, or similar.\nKnowledge of Data Mapping Techniques: Experience with AI-driven tools and methodologies for data mapping.\nStrong Code Generation Skills: Experience in developing AI models for automating code generation.\nExperience with Data Validation: Implementing AI-based validation techniques.\nFamiliarity with Code Conversion: Understanding AI algorithms for code conversion.\nExperience with Databases like SQL Server and MongoDB.\nCollaboration Skills: Ability to work effectively with cross-functional teams.\nProblem-Solving Skills: Strong ability to identify issues and develop creative solutions.\nAttention to Detail: Ensure accuracy and reliability of data conversions.\n5+ Years of Relevant Experience in data science or a related field.\nWillingness to Fail Fast and learn from mistakes in the fast-paced technology market.\nPreferred qualifications for consideration:\nExperience in the Financial Services Industry and an understanding of compliance standards.\nCertification in Data Science or AI/ML.\nExperience with Master data management, Data Wrangling and ETL Processes.\nFamiliarity with DevOps Tools like Jira, Confluence, and BitBucket.\nExperience with data and AI/ML Technologies: Such as NLP/NLU, Azure Cognitive Services, Azure Synapse Analytics, Azure data bricks and Azure ML service.\nPrevious Experience Delivering AI Solutions for complex data or conversions: Seamless Data Schema Conversion, AI-Driven Data Validation for Migration Accuracy, Intelligent Code Generation for Data Transformation Scripts , Historical Data Transformation and Archiving, Intelligent Error Detection and Correction, AI-Augmented Data Reconciliation",
        "skills": [
            "Azure ML Service",
            "Data Mapping Techniques",
            "Code Conversion",
            "AI-driven tools and methodologies",
            "code generation",
            "NLU",
            "Azure Cognitive Services",
            "Data Validation",
            "Azure Data Bricks",
            "SQL Server",
            "Sql",
            "Tensorflow",
            "Nlp",
            "Pytorch",
            "Azure Synapse Analytics",
            "MongoDB",
            "Python"
        ]
    },
    {
        "job_title": "Staff, Data Scientist",
        "company_name": "Walmart Global Tech",
        "experience": "8-14 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Staff, Data Scientist\nExperience: 8 - 14 years\nLocation: Chennai\nAbout EBS team:\nEnterprise Business Services is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, Associate Digital Experience. Our team is responsible for design and development of solution that knows our consumer's needs better than ever by predicting what they want based on unconstrained demand, and efficiently unlock strategic growth, economic profit, and wallet share by orchestrating intelligent, connected planning and decisioning across all functions. We interact with multiple teams across the company to provide scalable robust technical solutions. This role will play crucial role in overseeing the planning, execution and delivery of complex projects within team.\nAbout Team\nThe data science team at Enterprise Business Services Pillar at Walmart Global Tech focuses on using the latest research in machine learning, statistics, and optimization to solve business problems. We mine data, distill insights, extract information, build analytical models, deploy Machine Learning algorithms, and use the latest algorithms and technology to empower business decision-making. In addition, we work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions. Advanced analytical algorithms driven by our team will help Walmart to optimize business operations, business practices and change the way our customers shop.\nThe data science community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Labs will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat You Will Do\nAs a Staff Data Scientist for Walmart Global Tech, you'll have the opportunity to Drive data-derived insights across a wide range of retail & Finance divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assess data validity and synthesize data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train AI/ML models for replication for future projects\nGuides. data scientists, senior data scientists & staff data scientists across multiple sub-domains to ensure on-time delivery of ML products\nDrive efficiency across the domain in terms of DS and ML best practices, ML Ops practices, resource utilization, reusability and multi-tenancy.\nLead multiple complex ML products and guide senior tech leads in the domain in efficiently leading their products.\nDrive synergies across different products in terms of algorithmic innovation and sharing of best practices.\nWhat You Will Bring\nMaster's with > 9 years OR Ph.D. with > 8 years of relevant experience. Educational qualifications should be Computer Science/Statistics/Mathematics or a related area.\nMinimum 6 years of experience as a data science technical lead\nAbility to lead multiple data science projects end to end.\nDeep experience in building data science solution in areas like fraud prevention, forecasting, shrink and waste reduction, inventory management, recommendation, assortment and price optimization\nDeep experience in simultaneously leading multiple data science initiatives end to end from translating business needs to analytical asks, leading the process of building solutions and the eventual act of deployment and maintenance of them Strong experience in machine learning: Classification models, regression models, NLP, Forecasting, Unsupervised models, Optimization, Graph ML, Causal inference, Causal ML, Statistical Learning, experimentation & Gen-AI\nIn Gen-AI, it is desirable to have experience in embedding generation from training materials, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nAbility to scale and deploy data science solutions.\nStrong Experience with one or more of Python and R.\nExperience in GCP/Azure\nStrong Experience in Python, PySpark\nGoogle Cloud platform, Vertex AI, Kubeflow, model deployment\nStrong Experience with big data platforms Hadoop (Hive, Map Reduce, HQL, Scala)\nExperience with GPU/CUDA for computational efficiency\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.",
        "skills": [
            "Computational Algorithms",
            "GPU CUDA",
            "Statistical Models",
            "AI ML Models",
            "R",
            "Vertex AI",
            "Kubeflow",
            "Map Reduce",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Scala",
            "Pyspark",
            "Hive",
            "Gcp",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "McCormick & Company",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "At McCormick, we bring our passion for flavor to work each day. We encourage growth, respect everyone's contributions and do what's right for our business, our people, our communities and our planet. Join us on our quest to make every meal and moment better.\nFounded in Baltimore, MD in 1889 in a room and a cellar by 25-year-old Willoughby McCormick with three employees, McCormick is a global leader in flavour. With over 14,000 employees around the world and more than $6 Billion in annual sales, the Company manufactures, markets, and distributes spices, seasoning mixes, condiments and other flavourful products to the entire food industry, retail outlets, food manufactures, food service businesses and consumers.\nWhile our global headquarters are in the Baltimore, Maryland, USA area, McCormick operates and serves customers from nearly 60 locations in 25 countries and 170 markets in Asia-Pacific, China, Europe, Middle East and Africa, and the Americas.\nAt McCormick, we have over a 100-year legacy based on our Power of People principle. This principle fosters an unusually dedicated workforce requiring a culture of respect, recognition, inclusion and collaboration based on the highest ethical values.\nPosition Overview\nTo provide the business with data insights that will help increase revenue and/or lower costs. ^ Provide technical leadership and focus on generating insights for business customers. ^ Individual should help shape strategies for our consumer units globally.\nKey Responsibilities\nLeadership, Project Lead, management of key stakeholders for key data science projects and presentation of results.\nBringing new insights to the business, leads data mining and analytics, interpreting and reporting of large integrated data sets built with structured and unstructured data; develops tools to leverage new proprietary data sources (SAP Analytics Cloud SAC, S/4Hana LeoConnect Fiori, Analysis for Office A40). Statistical model building and deployment in the areas of forecasting, marketing mix and consumer insights.\nContinuously seeks out industry best practice and skills development to create new capabilities for data analytics at McCormick to drive marketing strategy.\nRequired Qualifications & Experience\nBachelor's Degree in Statistics (or Statistics Major) or Data Analytics. Minimum of 8 years of experience in analytics.\nExtensive experience analyzing complex datasets, generating insights and building robust statistical models.\nComfortable working with structured and unstructured data. Expertise in R, MS-Office required. Familiarity with brands (McCormick and competition) and their metrices.\nInterpersonal Skills\nAble to influence and navigate across multiple functions and regions (Procurement, Marketing, Sales, R&D, Finance etc.) to leverage enterprise data seamlessly for insights that can be leveraged across the business. Sound timely decisions grounded in data but also intuition, welldeveloped emotional intelligence. This role will interact with subordinates and superiors in the global analytics organization as well as with the internal customers/sponsors of the work. He/she will most likely join a senior colleague in meetings with the business and on occasion, present findings from the analyses.\nMcCormick & Company is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, colour, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.",
        "skills": [
            "Analysis for Office",
            "R",
            "LeoConnect",
            "SAP Analytics Cloud",
            "MS-Office",
            "Data Analytics",
            "Fiori"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "TVARIT",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Senior Data Scientist\n\nIndustry: Manufacturing (Steel/Metal Industry) or similar Industry.\n\nLocation: Pune\n\nJob Type: Full-Time\n\nAbout TVARIT\n\nTVARIT GmbH specializes in developing and delivering cutting-edge artificial intelligence (AI) solutions for the metal industry, including steel, aluminum, copper, cast iron, and more. Our software products empower customers to make intelligent, data-driven decisions, driving advancements in Predictive Quality (PsQ), Predictive Maintenance (PdM), and Energy Consumption Reduction (PsE), etc.\n\nWith a strong portfolio of renowned reference customers, state-of-the-art technology, a talented research team from prestigious universities, and recognition through esteemed awards such as the EU Horizon 2020 AI Prize, TVARIT is recognized as one of the most innovative AI companies in Germany and Europe.\n\nWe are looking for a self-motivated person with a positive can-do attitude and excellent oral and written communication skills in English.\n\nAbout The Role\n\nWe are looking for a highly skilled and motivated Senior Data Scientist who is passionate about solving complex data problems and developing cutting-edge solutions. This role requires someone with a strong technical background in data science, with full-stack development exposure from requirement gathering to solution deployment. The ideal candidate will be comfortable working as an individual contributor and as an active team member.\n\nKey Responsibilities\n\nBuild, optimize, and deploy scalable data models and machine learning pipelines.\nCollaborate with cross-functional teams to design, develop, and deploy data-driven solutions.\nUnderstands business problems and processes based on direct conversations with customers, can see the big picture, and translate that into specific solutions.\nConduct exploratory data analysis and feature engineering for large datasets.\nWrite clean, efficient, and maintainable Python code for data processing and analysis.\nDevelop APIs and integrate machine learning models into production systems.\nWork on cloud platforms (AWS, Azure, or GCP) to deploy and monitor models.\nActively participate in team discussions, code reviews, and knowledge sharing sessions.\nManage client discussions actively and Present findings and recommendations to stakeholders in a clear and compelling manner.\n\nQualifications\n\nBachelor's or master's degree in Metallurgy, mechanical, or a related field from IIT, NIT or top university.\n5+ years of experience in data science, machine learning.\nExperience in metal industry, manufacturing industry or similar industry exposure\n\nTechnical Skills\n\nStrong Python programming skills with experience in libraries like NumPy, Pandas, Scikit-learn, TensorFlow, or PyTorch.\nFull-stack development experience (e.g., backend development, REST APIs, front-end integration).\nProficiency in cloud platforms such as AWS, Azure, or GCP.\nHands-on experience with SQL and NoSQL databases.\nUnderstanding of MLOps principles and tools for model versioning, monitoring, and deployment\nFamiliarity with Kubernetes core concepts like pods, deployments, services, and ingress\nKnowledge of deep learning architectures and techniques for computer vision, NLP, and generative AI\nExcellent problem-solving and analytical skills.\nStrong communication skills and the ability to collaborate in a team environment.\nAbility to work in a fast-paced, agile environment and adapt to changing priorities.",
        "skills": [
            "Full-stack development",
            "generative AI",
            "Scikit-learn",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Deep Learning",
            "Numpy",
            "Nlp",
            "Pytorch",
            "Computer Vision",
            "Pandas",
            "Gcp",
            "MLops",
            "Rest Apis",
            "Azure",
            "Python",
            "Kubernetes",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "AppZen",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nAppZen is the leader in autonomous spend-to-pay software. Its patented artificial intelligence accurately and efficiently processes information from thousands of data sources so that organizations can better understand enterprise spend at scale to make smarter business decisions. It seamlessly integrates with existing accounts payable, expense, and card workflows to read, understand, and make real-time decisions based on your unique spend profile, leading to faster processing times and fewer instances of fraud or wasteful spend. Global enterprises, including one-third of the Fortune 500, use AppZen's invoice, expense, and card transaction solutions to replace manual finance processes and accelerate the speed and agility of their businesses. To learn more, visit us at www.appzen.com.\nAbout the Role:\nWe are looking for a Senior Data Scientist to come and work on our growing AI stack. You will be working with a team of highly skilled and motivated data scientists and machine learning engineers. If you are excited about natural language understanding and machine translation, AppZen is the right place for you to apply and grow your skills.\nMust haves:\nSolid understanding of machine learning fundamentals, and familiar with standard algorithms and techniques.\nExpert knowledge of a statistical computing language such as Python, Knowledge of probability and statistics, including experimental design, predictive modeling, optimization, and causal inference.\nLead the design, development, and implementation of state-of-the-art NLP algorithms and models using Transformers and similar architectures.\nGood Understanding of MLOps tools/processes like ElasticSearch, Jenkins, Docker is a plus.\nGood knowledge of Deep Learning frameworks like PyTorch, Tensorflow is a must.\nEnsure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, cross-lingual alignment/mapping, etc.\nManage your own process: identify and execute on high impact projects, triage external requests, and make sure you bring projects to conclusion in time for the results to be useful.\nExcellent written and verbal technical communication skills; communicate proposals and results in a clear manner backed by data and coupled with actionable conclusions to drive business decisions.\nM.Tech/B.Tech. or equivalent experience in Computer Science, Engineering, Statistics, or other relevant technical field.\nMust have 4+ years of industry experience.\nYou are a team player.\nNice-to-Have:\nTrack-record of having developed novel algorithms, e.g. publications in one or more of the following: KDD, WWW, NIPS, ISWC, NAACL, ACL, SIGIR, EMNLP, ICML etc.\nExpertise in building and fine-tuning LLM models using Transformers and RAG systems.",
        "skills": [
            "Transformers",
            "Elasticsearch",
            "Tensorflow",
            "Jenkins",
            "Pytorch",
            "MLops",
            "Docker",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "AiFA Labs",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Need 5-8 Years of experience\n\nRole Overview\n\nWe are looking for a highly skilled Data /AI/ML/Gen AI Scientist to join our dynamic team. This role is crucial in developing and implementing cutting-edge AI products and solutions, enabling businesses to unlock the full potential of AI and ML. The ideal candidate will have a strong background in Python, experience with cloud platforms, and a solid understanding of various AI concepts.\n\nKey Responsibilities\n\nDevelop and implement innovative AI and ML models to solve complex business problems.\nDesign and build scalable AI solutions using Python and relevant frameworks/libraries (Flask API, Fast API, Pandas, Scikit Learn, PyTorch, Transformer, OpenAI, Huggingface).\nCollaborate with cross-functional teams to understand business requirements and translate them into technical solutions.\nConduct research and development to stay updated with the latest advancements in AI and ML.\nEngage with clients to understand their needs and provide tailored AI solutions.\nLead and mentor junior team members, fostering a collaborative and innovative work environment.\nEnsure continuous learning and development to keep up with the fast-paced AI industry.\nRapidly prototype and iterate on AI models and solutions to meet business needs.\n\nKey Skills And Qualifications\n\nProficiency in Python\nExperience with cloud platforms such as AWS, Azure, or GCP for AI/ML/DS project development.\nStrong understanding of ML models, Deep Learning, NLP/CV, and Generative AI concepts.\nComfortable working in a fast-paced environment with rapid prototyping.\nExperience with client engagement and leading teams is a plus.\nProven track record in building products/solutions using cutting-edge AI models and tools.\n\nSkills: nlp,concepts,transformer,flask api,genai,azure,ml,openai,models,pandas,huggingface,fast api,platforms,learning,api,cloud,generative ai,aws,machine learning,gcp,python,ai,pytorch,data,edge,scikit learn",
        "skills": [
            "Transformer",
            "Fast API",
            "Huggingface",
            "OpenAI",
            "Scikit Learn",
            "Gcp",
            "Pytorch",
            "Pandas",
            "Flask",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Lead Data Scientist (Gen AI)",
        "company_name": "Blend",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Blend is hiring a Lead Data Scientist (Generative AI) to spearhead the development of advanced AI-powered classification and matching systems on Databricks. You will contribute to flagship programs like the Diageo AI POC by building RAG pipelines, deploying agentic AI workflows, and scaling LLM-based solutions for high-precision entity matching and MDM modernization.\n\nKey Responsibilitie\ns\nDesign and implement end-to-end AI pipelines for product classification, fuzzy matching, and deduplication using LLMs, RAG, and Databricks-native workflow\ns.Develop scalable, reproducible AI solutions within Databricks notebooks and job clusters, leveraging Delta Lake, MLflow, and Unity Catalo\ng.Engineer Retrieval-Augmented Generation (RAG) workflows using vector search and integrate with Python-based matching logi\nc.Build agent-based automation pipelines (rule-driven + GenAI agents) for anomaly detection, compliance validation, and harmonization logi\nc.Implement explainability, audit trails, and governance-first AI workflows aligned with enterprise-grade MDM need\ns.Collaborate with data engineers, BI teams, and product owners to integrate GenAI outputs into downstream system\ns.Contribute to modular system design and documentation for long-term scalability and maintainabilit\ny.\nQualificati\nons\nBachelor's/Master's in Computer Science, Artificial Intelligence, or related fi\neld.7+ years of overall Data Science experience with 2+ years in Generative AI / LLM-based applicati\nons.Deep experience with Databricks ecosystem: Delta Lake, MLflow, DBFS, Databricks Jobs & Workfl\nows.Strong Python and PySpark skills with ability to build scalable data pipelines and AI workflows in Databri\ncks.Experience with LLMs (e.g., OpenAI, LLaMA, Mistral) and frameworks like LangChain or LlamaIn\ndex.Working knowledge of vector databases (e.g., FAISS, Chroma) and prompt engineering for classification/retrie\nval.Exposure to MDM platforms (e.g., Stibo STEP) and familiarity with data harmonization challen\nges.Experience with explainability frameworks (e.g., SHAP, LIME) and AI audit tool\ning.\nPreferred S\nkills\nKnowledge of agentic AI architectures and multi-agent orchestr\nation.Familiarity with Azure Data Hub and enterprise data ingestion frame\nworks.Understanding of data governance, lineage, and regulatory compliance in AI sy\nstems.\nThrive & Grow w\nith Us:\nCompetitive Salary: Your skills and contributions are highly valued here, and we make sure your salary reflects that, rewarding you fairly for the knowledge and experience you bring to th\ne table.Dynamic Career Growth: Our vibrant environment offers you the opportunity to grow rapidly, providing the right tools, mentorship, and experiences to fast-track your\ncareer.Idea Tanks: Innovation lives here. Our Idea Tanks are your playground to pitch, experiment, and collaborate on ideas that can shape the\nfuture.Growth Chats: Dive into our casual Growth Chats where you can learn from the bestwhether it's over lunch or during a laid-back session with peers, it's the perfect space to grow your\nskills.Snack Zone: Stay fuelled and inspired! In our Snack Zone, you'll find a variety of snacks to keep your energy high and ideas\nflowing.Recognition & Rewards: We believe great work deserves to be recognized. Expect regular Hive-Fives, shoutouts and the chance to see your ideas come to life as part of our reward\nprogram.Fuel Your Growth Journey with Certifications: We're all about your growth groove! Level up your skills with our support as we cover the cost of your certifi\ncations.",
        "skills": [
            "LLMs",
            "OpenAI",
            "MLflow",
            "SHAP",
            "Delta Lake",
            "LIME",
            "LangChain",
            "LLaMA",
            "FAISS",
            "Mistral",
            "Chroma",
            "Databricks",
            "Pyspark",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - II",
        "company_name": "G2",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About G2 - Our People\n\nG2 was founded to create a place where people will love to work. We strive to create meaning in work and provide more than just a job: a true calling. At the heart of our community and culture are our people. Our global G2 team comes from a wide range of backgrounds and experiences, and that's what makes our G2 community strong and vibrant. We want everyone to bring their authentic selves to work, and we do this through our company and team events, our G2 Gives charitable initiatives, and our Employee Resource Groups (ERGs).\n\nOur employee-led, leadership-supported ERGs celebrate the diversity of our team, foster inclusivity and belonging, and create a space to connect to each other. Through connections and understanding, we build a stronger and more dynamic global team and help every person reach their personal peak.\n\nWe support our employees well-being by providing extensive benefits, including flexible work, aligned time off, and various leave options such as maternity, paternity, and sabbatical leaves. Click here to learn more about our benefits.\n\nAbout G2 - The Company\n\nWhen you join G2, you join the global team behind the largest and most trusted software marketplace. Every month, 5.5 million people come to G2 to inform smarter software decisions based on honest peer reviews. Authenticity is our focus, and every day we help thousands of companies, and hundreds of employees, propel their potential. Ready for meaningful work that starts and ends with compassion and heart You've come to the right place.\n\nG2 is going through exciting growth! We've recently secured our Series D funding of $157 million, which will further allow us to grow and develop our product and people. Read about it here!\n\nAbout The Role\n\nG2 is looking for a Data Scientist - II, your role encompasses leading model development and contributing to machine learning product development. You'll own end-to-end data science workflows, experiment with advanced algorithms, mentor junior team members, and drive innovation within the data science domain. You will work on Improving G2's intent scoring, content moderation, and other AI-driven features through the use of machine learning.This is a hybrid position, with the team meeting in person 2-3 days a week at our Bengaluru office.\n\nIn This Role, You Will\n\nModeling and Statistical Analysis:\n\nIndependently lead the development of machine learning models, owning feature engineering, extraction, model selection, and optimization\nDesign experiments by formulating statistical hypotheses, defining data requirements, pre-processing and cleaning the data, and performing the hypothesis testing.\nOperationalise models at scale applying AI and engineering best practices by working with the ML engineers.\nExperiment with various algorithms and techniques to advance model performance.\nDefine feedback and evaluation methods for the business problems.\nDemonstrate excellent coding and debugging skills.\n\nBusiness, Data Understanding, And Impact\n\nMake impactful contributions by leveraging AI and Machine Learning expertise to address pressing business challenges.\nCollaborate with cross functional teams to understand the business requirements and data architecture.\nTranslate business requirements into technical solutions by working with the business and senior data scientists.\nIdentify and document the data requirements and manage data collection and preparation for projects.\nDesign and document training and testing strategy.\nDocument methodologies, findings, and outcomes of model experiments and present it to the team and key stakeholders.\n\nMentorship And Collaboration\n\nMentor junior team members, providing technical support, guidance on model development, and best practices implementation.\nCoach junior team members, helping them understand complex datasets, models, and business requirements by giving clear and actionable feedback.\nEncourage the development of best practices and innovative approaches in data analysis and modeling.\n\nQualifications\n\n4+ years of experience as a data scientist involved in data extraction, analysis, and modeling.\n4+ years of experience in Python and SQL or related tools for machine learning.\nUnderstanding of statistics and linear algebra.\nProficiency in machine learning algorithms and all stages of machine learning.\nFamiliarity with neural networks and deep learning.\nBasic knowledge about AWS services and cloud databases.\nProficiency in handling structured and unstructured data.\n\nYou would be successful in this role if you describe yourself as:\n\nSuccessful end-to-end delivery of data science products.\nExposure to MLOps tools like MLFlow, KubeFlow, DVC,AWS Sagemaker, Seldon etc\nExperience deploying models in a AWS cloud environment - with specific experience with AWS tools such as Sagemaker and Step Functions.\nExpertise with Natural Language Processing and Understanding.\nExperience with libraries and frameworks for training ML and DL models (PySpark, Tensorflow).\nExperience and expertise in ML Operations best practices.\n\nOur Commitment to Inclusivity and Diversity\n\nAt G2, we are committed to creating an inclusive and diverse environment where people of every background can thrive and feel welcome. We consider applicants without regard to race, color, creed, religion, national origin, genetic information, gender identity or expression, sexual orientation, pregnancy, age, or marital, veteran, or physical or mental disability status.\n\nLearn more about our commitments here Commitments",
        "skills": [
            "Tensorflow",
            "Machine Learning",
            "MLops",
            "Natural Language Processing",
            "Pyspark",
            "Neural Networks",
            "Statistical Analysis",
            "Python",
            "Sql",
            "Deep Learning",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist/ Senior Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Chennai, Mumbai",
        "industry": "Internet/E-commerce",
        "job_description": "Job Description\nData Scientist\nTiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 3900+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you'll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.\n\nWe're Great Place to Work-Certified. Working at Tiger Analytics, you'll be at the heart of an AI revolution. You'll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.\n\n\nAbout the Project:\nWe are seeking a highly motivated and skilled Data Scientist with 4-8 years of experience to\njoin our research and development team. The ideal candidate will work at the intersection of\nclinical studies, omics data analysis, and predictive modelling, contributing to data-driven\nstrategies for biomarker discovery, product innovation, and application development.\nWhat You'll Do:\nAnalyse and integrate clinical and multi-omics datasets (e.g., genomics, proteomics) to extract actionable insights.\nDevelop and validate predictive models for biomedical research and biomarker discovery.\nApply Design of Experiments (DOE) and statistical methods to support research objectives.\nBuild interactive web applications using Streamlit or R Shiny for data visualization and exploration.\nEnsure reproducibility, code quality, and adherence to industrialized coding best practices using Git/GitHub.\nCollaborate with cross-functional teams including bioinformaticians, data engineers, and clinicians.\nDocument analysis workflows, methodologies, and results in a clear and structured manner.\nCommunicate findings through reports, dashboards, and presentations to technical and non-technical stakeholders.\n\nTechnical Skills Programming & Tools:\nR (Tidyverse), Python (pandas, NumPy, scikit-learn)\nStreamlit or R Shiny for web application development\nGit/GitHub for version control and code management\nGCP\n\nWhat You Need:\nData Science & Modeling:\nExploratory Data Analysis (EDA), Statistical Modeling, Predictive Modeling\nClinical data interpretation and Omics data integration\nDesign of Experiments (DOE), Reproducibility & Repeatability\nData visualization (ggplot2, matplotlib, seaborn, etc.)\n\nYou are important to us, let's stay connected!\nEvery individual comes with a different set of skills and qualities so even if you don't tick all the boxes for the role today, we urge you to apply as there might be a suitable/unique role for you tomorrow.\nWe are an equal-opportunity employer. Our diverse and inclusive culture and values guide us to listen, trust, respect, and encourage people to grow the way they desire.\n\nNote: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.\nAdditional Benefits: Health insurance (self & family), virtual wellness platform, and knowledge communities.",
        "skills": [
            "Omics data integration",
            "scikit-learn",
            "Streamlit",
            "Modeling",
            "R Shiny",
            "R",
            "Clinical data interpretation",
            "ggplot2",
            "Tidyverse",
            "Statistical Modeling",
            "Seaborn",
            "Github",
            "Pandas",
            "Data Visualization",
            "Matplotlib",
            "Numpy",
            "Predictive Modeling",
            "Python",
            "Gcp",
            "Git"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "HiLabs",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "The HiLabs Story\n\nHiLabs is a leading provider of AI-powered solutions to clean dirty data, unlocking its hidden potential for healthcare transformation. HiLabs is committed to transforming the healthcare industry through innovation, collaboration, and a relentless focus on improving patient outcomes.\n\nHiLabs Team\n\nMultidisciplinary industry leaders\nHealthcare domain experts\nAI/ML and data science experts\nProfessionals hailing from the worlds best universities, business schools, and engineering institutes including Harvard, Yale, Carnegie Mellon, Duke, Georgia Tech, Indian Institute of Management (IIM), and Indian Institute of Technology (IIT).\n\nJob Title: Lead Data Scientist\n\nJob Location: Pune\n\nJob summary: HiLabs is looking for highly motivated and skilled Lead/Sr. Data Scientist focused on the application of emerging technologies. The candidates must be well versed with Python, Scala, Spark, SQL and AWS platform. The individuals who will join the new Evolutionary Platform team should be continually striving to advance AI/ML excellence and technology innovation. The mission is to power the next generation of the digital product and services through innovation, collaboration, and transparency. You will be a technology leader and doer who enjoys working in a dynamic, fast- paced environment.\n\nResponsibilities\n\nLeverage AI/ML techniques and solutions to identify and mathematically interpret complex healthcare problems.\nFull-stack development of data pipelines involving Big Data.\nDesign and development of robust application/data pipelines using Python, Scala, Spark, and SQL\nLead a team of Data Scientists, developers as well as clinicians to strategize, design and evaluate AI based solutions to healthcare problems.\nIncrease efficiency and improve the quality of solutions offered.\nManaging the complete ETL pipeline development process from conception to deployment\nCollaborating with and guiding the team on writing, building, and deployment of data software\nFollowing best design and development practices to ensure high quality code.\nDesign, build and maintain efficient, secure, reusable, and reliable code\nPerform code reviews, testing, and debugging\n\nDesired Profile\n\nBachelor's or Master's degrees in computer science, Mathematics, or any other quantitative discipline from Premium/Tier 1 institutions\n5 to 7 years of experience in developing robust ETL data pipelines and implementing advanced AI/ML algorithms (GenAI is a plus).\nStrong experience working with technologies like Python, Scala, Spark, Apache Solr, MySQL, Airflow, AWS etc.\nExperience working with Relational databases like MySQL, SQLServer, Oracle etc.\nGood understanding of large system architecture and design\nUnderstands the core concepts of Machine Learning and the math behind it.\nExperience working in AWS/Azure cloud environment\nExperience using Version Control tools such as Bitbucket/GIT code repository\nExperience using tools like Maven/Jenkins, JIRA\nExperience working in an Agile software delivery environment, with exposure to continuous integration and continuous delivery tools\nGreat collaboration and interpersonal skills\nAbility to work with team members and lead by example in code, feature development, and knowledge sharing\n\nHiLabs is an equal opportunity employer (EOE). No job applicant or employee shall receive less favorable treatment or be disadvantaged because of their gender, marital or family status, color, race, ethnic origin, religion, disability, or age; nor be subject to less favorable treatment or be disadvantaged on any other basis prohibited by applicable law.\n\nHiLabs is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse and inclusive workforce to support individual growth and superior business results.\n\nThank you for reviewing this opportunity with HiLabs! If this position appears to be a good fit for your skillset, we welcome your application.\n\nHiLabs Total Rewards\n\nCompetitive Salary, Accelerated Incentive Policies, H1B sponsorship, Comprehensive benefits package that includes ESOPs, financial contribution for your ongoing professional and personal development, medical coverage for you and your loved ones, 401k, PTOs & a collaborative working environment, Smart mentorship, and highly qualified multidisciplinary, incredibly talented professionals from highly renowned and accredited medical schools, business schools, and engineering institutes.\n\nCCPA disclosure notice - https://www.hilabs.com/privacy",
        "skills": [
            "Airflow",
            "Jenkins",
            "Apache Solr",
            "Maven",
            "MySQL",
            "Scala",
            "Spark",
            "JIRA",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist - 2",
        "company_name": "Navi",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Navi\nNavi is one of the fastest-growing financial services companies in India providing Personal & Home Loans, UPI, Insurance, Mutual Funds, and Gold. Navi's mission is to deliver digital-first financial products that are simple, accessible, and affordable. Drawing on our in-house AI/ML capabilities, technology, and product expertise, Navi is dedicated to building delightful customer experiences.\nFounders: Sachin Bansal & Ankit Agarwal\nKnow what makes you a Navi_ite :\n1.Perseverance, Passion and Commitment\nPassionate about Navi's mission and vision\nDemonstrates dedication, perseverance and high ownership\nGoes above and beyond by taking on additional responsibilities\n2.Obsession with high quality results\nConsistently creates value for the customers and stakeholders through high quality outcomes\nEnsuring excellence in all aspects of work\nEfficiently manages time, prioritizes tasks, and achieves higher standards\n3.Resilience and Adaptability\nAdapts quickly to new roles, responsibilities, and changing circumstances, showing resilience and agility\nAbout the role:\nData Science @Navi:\nAt Navi, our Data Science team is the powerhouse behind scalable and efficient solutions that span across a broad spectrum of fintech sectorsbe it lending, insurance, investments, or UPI. We're not just a team; we're the architects of the future of fintech\nWe're not just keeping up with innovation; we're setting the pace. Our team is constantly pushing the boundaries, introducing groundbreaking methods that amplify business growth, enhance customer experiences, and streamline operational processes.\nOur work isn't confined to a single domain. We tackle a diverse set of problem statements, from computer vision and tabular data to natural language processing, speech recognition, and even Generative AI. Each day brings a new challenge and a new opportunity for breakthroughs.\nWhen you join us, you're not just taking a job; you're becoming a part of a movement. A movement that's making a tangible difference in the fintech landscape, one innovative solution at a time.\nReady for a transformative career journey Join us at Navi and be a part of a team that's shaping the future of fintech.\nWhat you gain by working with the Data Science team at Navi:\nJoin the Navi Data Science team for:\nOwn Your Journey from Start to Finish : Take pride in having full-cycle ownership of your projects. You won't just be a cog in the machine; you'll be the architect, designer, and implementer of cutting-edge Data Science solutions that drive our business forward.\nImmerse Yourself in a Data Wonderland : Welcome to a playground where data is abundant and limitations are few. Our high-growth, agile environment offers a treasure trove of data, giving you the freedom to experiment, innovate, and make data-driven decisions that matter.\nBe Part of a Synergistic Dream Team : Collaborate with a diverse group of high-performing professionals who are as passionate about data science as you are. Our culture fosters continuous learning and upskilling, setting you up for ongoing success and career growth.\nMake a Tangible Impact : Your work won't just sit on a shelf; it will make waves. By closely collaborating with stakeholders across departments, you'll have the opportunity to design and develop data science solutions that not only solve complex problems but also make a meaningful impact on our company and the fintech industry at large\nWhat we are looking for:\nBachelor's or Master's in Engineering or equivalent.\n2+ years of Data Science/Machine Learning experience.\nStrong knowledge in statistics, tree-based techniques (e.g., Random Forests, XGBoost), machine learning (e.g., MLP, SVM), inference, hypothesis testing, simulations, and optimizations.\nBonus: Experience with deep learning techniques; experience in working Ad domain/reinforcement learning.\nStrong Python programming skills and experience in building Data Pipelines in PySpark, along with feature engineering.\nProficiency in pandas, scikit-learn, Scala, SQL, and familiarity with TensorFlow/PyTorch.\nUnderstanding of DevOps/MLOps, including creating Docker containers and deploying to production (using platforms like Databricks or Kubernetes).",
        "skills": [
            "Data Pipelines",
            "scikit-learn",
            "Random Forests",
            "Feature engineering",
            "MLP",
            "simulations",
            "Tree-based techniques",
            "Statistics",
            "Inference",
            "Optimizations",
            "Machine Learning",
            "Pyspark",
            "Svm",
            "Tensorflow",
            "Data Science",
            "Pytorch",
            "Docker",
            "XGBoost",
            "Python",
            "Hypothesis Testing",
            "Scala",
            "Sql",
            "Devops",
            "Pandas",
            "MLops",
            "Databricks",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "XLSCOUT",
        "experience": "Fresher",
        "salary": null,
        "location": "Mohali, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title - Data Scientist\nSpecialization: Data Science - Natural Language Processing\nLocation - Chennai/ Mohali/ Hybrid\nAbout Xlscout.ai:\nXlscout.ai, headquartered in Toronto, Canada, is a pioneering cloud-based platform leveraging Artificial Intelligence, cognitive computing, machine learning, and big data technologies. We specialize in extracting intelligence from over 100 million technology documents across 90 countries, utilizing a technical corpus derived from 100 GB of processed data and developed through advanced machine learning models. Our vision is to empower IP and R&D departments globally to accelerate innovation with our cutting-edge NLP technologies.\nRole:\nIn this role, you will utilize your expertise in Natural Language Processing to analyze extensive datasets, contributing to the development of AI-driven solutions. You will play a key role in a dynamic team, pushing the boundaries of NLP and machine learning technologies.\nSkills required:\nProficient programming and scripting skills in Python\nSolid foundation in machine learning algorithms\nPractical experience applying NLP techniques in data analysis\nKnowledge in topic modeling, clustering, NLP embeddings, and keyword extraction.\nExperience in deep learning (Sequence models, GRU, RNN, LSTM), with a preference for knowledge of advanced NLP models like Bert and GPT\nStrong understanding of text pre-processing and normalization techniques (Tokenization, POS tagging, parsing)\nExperience with PyTorch and/or TensorFlow, and model deployment on platforms like AWS/GCP/server\nFamiliarity with Flask and software development frameworks is a plus\nKnowledge of Open Source LLMs: Understanding of open source large language models such as LLaMA 2, Mistral, Flacon, etc.\nFine-tuning Expertise: Ability to fine-tune open source LLMs and embedding models for specific applications.\nPreferred Qualifications\nBachelor's or Master's degree in Computer Science (BE/B.Tech/M.Tech)\nCertifications in relevant fields, like the Deep Learning Specialization from Coursera by deeplearning.ai\nGraduates or postgraduates from top-tier colleges are highly desirable\nWhat We Offer:\nA chance to work with ground-breaking technology in a field that's shaping the future\nA collaborative, inclusive, and dynamic work culture\nOpportunities for professional growth and learning\nA competitive benefits package and a flexible working environment",
        "skills": [
            "Open Source LLMs",
            "GRU",
            "NLP Embeddings",
            "LLaMA 2",
            "GPT",
            "POS Tagging",
            "Fine-tuning Expertise",
            "Sequence Models",
            "Model Deployment",
            "Mistral",
            "LSTM",
            "Parsing",
            "Text Pre-processing",
            "Topic Modeling",
            "Keyword Extraction",
            "Normalization Techniques",
            "Bert",
            "Flacon",
            "tokenization",
            "Tensorflow",
            "Rnn",
            "Pytorch",
            "Flask",
            "Machine Learning Algorithms",
            "Python",
            "AWS",
            "Clustering",
            "Deep Learning",
            "Gcp"
        ]
    },
    {
        "job_title": "Data Scientist ML",
        "company_name": "Team Geek Solutions",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nTeam Geek Solutions is a forward-thinking technology firm dedicated to empowering businesses through innovative data solutions. Our mission is to harness the power of data analytics and machine learning to drive strategic insights and informed decision-making. We pride ourselves on our collaborative culture, where diverse talents come together to solve complex problems and deliver exceptional value to our clients.\n\nKey Responsibilities\n\nExtract and analyze data from company databases to drive the optimization and\n\nenhancement of product development and marketing strategies.\n\nAnalyze large datasets to uncover trends, patterns, and insights that can influence business decisions.\n\nLeverage predictive and AI/ML modeling techniques to enhance and optimize\n\ncustomer experience, boost revenue generation, improve ad targeting, and more.\n\nDesign, implement, and optimize machine learning models for a wide range of applications such as predictive analytics, natural language processing, recommendation systems, and more.\n\nConduct experiments to fine-tune machine learning models and evaluate their performance using appropriate metrics.\n\nDeploy machine learning models into production environments, ensuring scalability\n\nQualifications\n\nBachelor's, Master's or Ph.D in Computer Science, Data Science, Mathematics, Statistics, or a related field.\n\n2+ years of experience in Analytics, Machine learning, Deep learning.\n\nProficiency in programming languages such as Python, and familiarity with machine learning libraries (e.g., Numpy, Pandas, TensorFlow, Keras, PyTorch, Scikit-learn).\n\nStrong experience with data wrangling, cleaning, and transforming raw data into structured, usable formats.\n\nHands-on experience in developing, training, and deploying machine learning models for various applications (e.g., predictive analytics, recommendation systems, anomaly detection).\n\nIn-depth understanding of machine learning algorithms (supervised, unsupervised,\n\nreinforcement learning) and their appropriate use cases.\n\nExperience with model evaluation techniques (e.g., cross-validation, A/B testing,\n\nperformance metrics).\n\nExperience with cloud platforms (AWS, GCP, Azure) for model deployment and scalal Proficiency in data processing and manipulation techniques.\n\nExcellent problem-solving skills and the ability to work effectively in a collabor environment.\n\nStrong communication skills to convey complex technical concepts to non-tech stakeholders.\n\nGood To Have\n\nExperience in the [banking/financial services/industry-specific] sector.\n\nFamiliarity with cloud-based machine learning platforms such as Azure, AWS, or GCP.\n\nSkills: natural language processing,numpy,azure,data wrangling,tensorflow,data analysis,data science,big data technologies,pandas,pytorch,cloud platforms,gcp,python,model evaluation,recommendation systems,keras,scikit-learn,data scientist,data visualization,interpersonal skills,aws,predictive analytics,statistical modeling,machine learning,deep learning,model deployment",
        "skills": [
            "Scikit-learn",
            "Model Evaluation",
            "Cloud Platforms",
            "Machine Learning",
            "Natural Language Processing",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Pytorch",
            "Gcp",
            "Pandas",
            "data wrangling",
            "Keras",
            "Predictive Analytics",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist - Automotive Research",
        "company_name": "S&P Global",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Role\n\nGrade Level (for internal use):\n\n09\n\nS&P Global Mobility\n\nTheRole: Senior Data Scientist\n\nThe Team\n\nWe are the Research and Modeling team, driving innovation by building robust models and tools to support the Vehicle & Powertrain Forecast team. Our work includes all aspects of development of, and ongoing support for, our business line data flows, analyst modelling solutions and forecasts, new apps, new client-facing products, and many other work areas besides. We value ownership, adaptability, and a passion for learning, while fostering an environment where diverse perspectives and mentorship fuel continuous growth.\n\nThe Impact\n\nWe are seeking a motivated and talented Senior Data Scientist to create new AI algorithms and/or improve our current machine learning algorithms, while exploring the potential of GenAI and LLMs to enhance our data collect, data source mapping, and information inference capabilities. This role will directly contribute to our ability to deliver more accurate and timely forecasts to our clients, solidifying S&P Mobility's position as the leader in automotive market intelligence.\n\nWhat's In It For You\n\nEngage with cutting-edge AI and ML technologies to solve complex forecasting challenges and drive innovation in automotive market intelligence.\nLead the development and implementation of advanced analytical models (pricing, demand, specifications), enhancing your expertise and contributing significantly to the product's value.\nDirectly influence forecasts provided to market-leading clients in the automotive sector, gaining exposure to their critical data and business challenges.\nBenefit from a collaborative environment with opportunities to mentor talent, fostering both your technical leadership and the growth of the team.\n\nResponsibilities\n\nAdvanced Pricing Analytics:\nDevelop and implement machine learning models to analyse vehicle and powertrain pricing trends, predicting future price points with high accuracy.\nCreate sophisticated algorithms to assess the impact of various factors (e.g., market conditions, competitor pricing, etc.) on vehicle pricing.\nRefine our forecast system to incorporate these advanced pricing analytics, enhancing the overall forecast precision considering price elasticity.\nForecasting Modeling:\nImprove the performance of current machine learning models (XGBoost)\nDevelop and optimize algorithms for predicting vehicle demand based on historical data, market trends, and pricing information.\nDevelop algorithms for predicting vehicle specifications, like dimension or mass.\nGenAI and LLM Exploration:\nInvestigate and implement LLMs for efficient data collecting from diverse sources, improving the breadth and depth of our data collection\nDevelop solutions for better mapping, filling, and structuring our data with source information, ensuring data integrity and consistency, ultimately streamlining our data acquisition and analysis processes.\nCollaboration & Mentorship:\nCollaborate closely with product teams, business stakeholders, ML Engineers, and Data Engineers to ensure smooth integration of models into production systems.\nMentor talent ML Engineer and Data Engineer in fostering their professional growth and development.\nCommunication:\nCommunicate complex technical concepts to non-technical stakeholders.\nContribute to the documentation of ML models and processes.\nWhat We're Looking For\n\nMaster's in Data Science, Computer Science, Statistics, or a related field.\nMinimum 7+ years of professional experience in data science, with a proven track record of developing and deploying machine learning models.\nStrong proficiency in Python and relevant machine learning libraries (e.g., scikit-learn, TensorFlow, PyTorch).\nHands-on experience with Generative AI and Large Language Models (LLMs), including data collection, text analysis, and information retrieval.\nExperience with cloud platforms (e.g., AWS, Azure, GCP) and data engineering tools\nStrong communication and collaboration skills.\nExperience in the automotive sector is a plus.\n\nAbout Company Statement\n\nS&P Global delivers essential intelligence that powers decision making. We provide the world's leading organizations with the right data, connected technologies and expertise they need to move ahead. As part of our team, you'll help solve complex challenges that equip businesses, governments and individuals with the knowledge to adapt to a changing economic landscape.\n\nS&P Global Mobility turns invaluable insights captured from automotive data to help our clients understand today's market, reach more customers, and shape the future of automotive mobility.\n\nAbout S&P Global Mobility\n\nAt S&P Global Mobility, we provide invaluable insights derived from unmatched automotive data, enabling our customers to anticipate change and make decisions with conviction. Our expertise helps them to optimize their businesses, reach the right consumers, and shape the future of mobility. We open the door to automotive innovation, revealing the buying patterns of today and helping customers plan for the emerging technologies of tomorrow.\n\nFor more information, visit www.spglobal.com/mobility.\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)\n\nJob ID: 314773\n\nPosted On: 2025-05-13\n\nLocation: Gurgaon, Haryana, India",
        "skills": [
            "Generative AI",
            "scikit-learn",
            "Large Language Models",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "XGBoost",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "S&P Global",
        "experience": "7-9 Years",
        "salary": "INR 26.5 - 33 LPA ",
        "location": "Ahmedabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Role\n\nGrade Level (for internal use):\n\n11\n\nThe Team\n\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\n\nThe Impact\n\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\n\nWhat's In It For You\n\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\n\nKey Responsibilities\n\nDesign, Develop and Deploy ML powered products and pipelines\nPlay a central role in all stages of the data science project life cycle, including:\nIdentification of suitable data science project opportunities\nPartnering with business leaders, domain experts, and end-users to gain business understanding, data understanding, and collect requirements\nEvaluation/interpretation of results and presentation to business leaders\nPerforming exploratory data analysis, proof-of-concept modelling, model benchmarking and setup model validation experiments\nTraining large models both for experimentation and production\nDevelop production ready pipelines for enterprise scale projects\nPerform code reviews & optimization for your projects and team\nSpearhead deployment and model scaling strategies\nStakeholder management and representing the team in front of our leadership\nLeading and mentoring by example including project scrums\nWhat We're Looking For\n\n7+ years of professional experience in Data Science domain\nExpertise in Python (Numpy, Pandas, Spacy, Sklearn, Pytorch/TF2, HuggingFace etc.)\nExperience with SOTA models related to NLP and expertise in text matching techniques, including sentence transformers, word embeddings, and similarity measures\nExpertise in probabilistic machine learning model for classification, regression & clustering\nStrong experience in feature engineering, data preprocessing, and building machine learning models for large datasets.\nExposure to Information Retrieval, Web scraping and Data Extraction at scale\nOOP Design patterns, Test-Driven Development and Enterprise System design\nSQL (any variant, bonus if this is a big data variant)\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\n\nNice to have\n\nPrior work to show on Github, Kaggle, StackOverflow etc.\nCloud expertise (AWS and GCP preferably)\nExpertise in deploying machine learning models in cloud environments\nFamiliarity in working with LLMs\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)\n\nJob ID: 315680\n\nPosted On: 2025-05-20\n\nLocation: Gurgaon, Haryana, India",
        "skills": [
            "TF2",
            "HuggingFace",
            "data preprocessing",
            "word embeddings",
            "Classification",
            "similarity measures",
            "text matching techniques",
            "Regression",
            "probabilistic machine learning",
            "feature engineering",
            "Spacy",
            "sentence transformers",
            "Github",
            "Sklearn",
            "Nlp",
            "Linux Os",
            "Python",
            "Azure DevOps",
            "Sql",
            "Numpy",
            "Git",
            "Pandas",
            "Pytorch",
            "Clustering"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "S&P Global",
        "experience": "7-10 Years",
        "salary": "INR 26.5 - 33 LPA ",
        "location": "Ahmedabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Role\n\nGrade Level (for internal use):\n\n11\n\nThe Team\n\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\n\nThe Impact\n\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\n\nWhat's In It For You\n\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\n\nKey Responsibilities\n\nBuild production ready data acquisition and transformation pipelines from ideation to deployment\nBeing a hands-on problem solver and developer helping to extend and manage the data platforms\nArchitect and lead the development of end-to-end data ingestion and processing pipelines to support downstream ML workflows\nApply best practices in data modeling and building ETL pipelines (streaming and batch) using cloud-native solutions\nMentor junior and mid-level data engineers and provide technical guidance and best practices\n\nWhat We're Looking For\n\n7-10 years of professional software work experience\nExpertise in Python and Apache Spark\nOOP Design patterns, Test-Driven Development and Enterprise System design\nSQL (any variant, bonus if this is a big data variant)\nProficient in optimizing data flows for performance, storage, and cost efficiency\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\n\nNice to have\n\nCore Java 17+, preferably Java 21+, and associated toolchain\nDevOps with a keen interest in automation\nApache Avro\nApache Kafka\nKubernetes\nCloud expertise (AWS and GCP preferably)\nOther JVM based languages - e.g. Kotlin, Scala\nC# - in particular .NET Core\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)\n\nJob ID: 315684\n\nPosted On: 2025-05-20\n\nLocation: Gurgaon, Haryana, India",
        "skills": [
            "OOP Design patterns",
            "Enterprise System design",
            "Test-Driven Development",
            "Cloud-native solutions",
            "ETL pipelines",
            "Github",
            "Apache Spark",
            "Data Modeling",
            "Sql",
            "Git",
            "Linux Os",
            "Python",
            "Azure DevOps"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "S&P Global",
        "experience": "7-10 Years",
        "salary": "INR 26.5 - 33 LPA ",
        "location": "Ahmedabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Role\n\nGrade Level (for internal use):\n\n11\n\nThe Team\n\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\n\nThe Impact\n\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\n\nWhat's In It For You\n\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\n\nKey Responsibilities\n\nBuild production ready data acquisition and transformation pipelines from ideation to deployment\nBeing a hands-on problem solver and developer helping to extend and manage the data platforms\nArchitect and lead the development of end-to-end data ingestion and processing pipelines to support downstream ML workflows\nApply best practices in data modeling and building ETL pipelines (streaming and batch) using cloud-native solutions\nMentor junior and mid-level data engineers and provide technical guidance and best practices\n\nWhat We're Looking For\n\n7-10 years of professional software work experience\nExpertise in Python and Apache Spark\nOOP Design patterns, Test-Driven Development and Enterprise System design\nSQL (any variant, bonus if this is a big data variant)\nProficient in optimizing data flows for performance, storage, and cost efficiency\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\n\nNice to have\n\nCore Java 17+, preferably Java 21+, and associated toolchain\nDevOps with a keen interest in automation\nApache Avro\nApache Kafka\nKubernetes\nCloud expertise (AWS and GCP preferably)\nOther JVM based languages - e.g. Kotlin, Scala\nC# - in particular .NET Core\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)\n\nJob ID: 315683\n\nPosted On: 2025-05-20\n\nLocation: Gurgaon, Haryana, India",
        "skills": [
            "OOP Design patterns",
            "Enterprise System design",
            "Test-Driven Development",
            "Github",
            "Apache Spark",
            "Sql",
            "Git",
            "Gcp",
            "Linux Os",
            "Core Java",
            "Apache Kafka",
            "apache avro",
            "Kubernetes",
            "Python",
            "Azure DevOps",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "S&P Global",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Ahmedabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Role\n\nGrade Level (for internal use):\n\n10\n\nThe Team\n\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\n\nThe Impact\n\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\n\nWhat's In It For You\n\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\n\nKey Responsibilities\n\nBuild production ready data acquisition and transformation pipelines from ideation to deployment\nBeing a hands-on problem solver and developer helping to extend and manage the data platforms\nApply best practices in data modeling and building ETL pipelines (streaming and batch) using cloud-native solutions\n\nWhat We're Looking For\n\n3-5 years of professional software work experience\nExpertise in Python and Apache Spark\nOOP Design patterns, Test-Driven Development and Enterprise System design\nExperience building data processing workflows and APIs using frameworks such as FastAPI, Flask etc.\nProficiency in API integration, experience working with REST APIs and integrating external & internal data sources\nSQL (any variant, bonus if this is a big data variant)\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\n\nNice to have\n\nCore Java 17+, preferably Java 21+, and associated toolchain\nDevOps with a keen interest in automation\nApache Avro\nApache Kafka\nKubernetes\nCloud expertise (AWS and GCP preferably)\nOther JVM based languages - e.g. Kotlin, Scala\nC# - in particular .NET Core\nData warehouses (e.g., Redshift, Snowflake, BigQuery)\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)\n\nJob ID: 315685\n\nPosted On: 2025-05-20\n\nLocation: Gurgaon, Haryana, India",
        "skills": [
            "OOP Design patterns",
            "Enterprise System design",
            "Test-Driven Development",
            "Github",
            "Apache Spark",
            "Api Integration",
            "Sql",
            "Git",
            "Linux Os",
            "Flask",
            "FastAPI",
            "Rest Apis",
            "Python",
            "Azure DevOps"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Recro",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "As a Manager - Data Science, you wont just manage, you'll lead by doing. This role demands strong hands-on expertise in Machine Learning, Generative AI, Python, and SQL, and any cloud environment (GCP, Azure or AWS) ensuring that you stay deeply engaged in the technical side while mentoring and growing a high-performing team. You'll spearhead end-to-end AI/ML project execution, collaborate with cross-functional teams, and drive innovation within Blends Data Science practice. If you love solving complex problems, thrive in a fast-paced environment, and can translate business challenges into cutting-edge AI solutions, wed love to have you on board!\nWhat You'll Tackle Each Day:\nDelivery & Project Management:\nDevelop and implement ML models, Gen AI solutions, and predictive analytics.\nPerform data mining, feature engineering, and statistical analysis.\nOwn project roadmaps, quality control, and timely delivery.\nCollaborate with Data Engineering teams to deploy and operationalize ML models.\nAutomate and optimize workflows for efficiency.\nPractice Development:\nContribute to scaling Blends Data Science practice by building new capabilities.\nDesign industry-specific AI/ML solutions and contribute to thought leadership.\nEvaluate emerging AI trends and tools and integrate them into our ecosystem.\nLead innovation initiatives, research, and internal AI development.\nPeople & Leadership:\nMentor and develop a high-performance data science team.\nGuide career development and set performance benchmarks.\nCollaborate with cross-functional teams to drive seamless execution.\nQualifications\n8+ years of experience in Data Science & AI, with hands-on expertise in ML, Python, and SQL.\nExposure to Generative AI Should have worked on at least a few POCs or pilot projects leveraging Gen AI capabilities\nStrong knowledge of ML algorithms (Classification, Regression, Forecasting, NLP, LLMs, Optimization, etc.).\nExperience in end-to-end ML deployment, including working with either Azure or AWS or GCP or Databricks.\nProven ability to solve complex business challenges in Retail, CPG, BFSI, Healthcare, or eCommerce.\nDeep expertise in statistics, probability, stochastic processes, and causal inference.\nStrong communicator who can explain AI concepts to non-technical stakeholders.\nExperience with big data tools (Hadoop, Hive, PySpark) and ML pipelines.\nBonus: Experience in Google Analytics, Adobe Analytics, or digital marketing analytics.\nBachelors/Master's degree in Computer Science, Statistics, Math, Operations Research, or a related field.\nWhat do you get in return\nCompetitive Salary: Your skills and contributions are highly valued here, and we make sure your salary reflects that, rewarding you fairly for the knowledge and experience you bring to the table.\nDynamic Career Growth: Our vibrant environment offers you the opportunity to grow rapidly, providing the right tools, mentorship, and experiences to fast-track your career.\nIdea Tanks: Innovation lives here. Our Idea Tanks are your playground to pitch, experiment, and collaborate on ideas that can shape the future.\nGrowth Chats: Dive into our casual Growth Chats where you can learn from the best whether it's over lunch or during a laid-back session with peers, it's the perfect space to grow your skills.\nSnack Zone: Stay fueled and inspired! In our Snack Zone, you'll find a variety of snacks to keep your energy high and ideas flowing.\nRecognition & Rewards: We believe great work deserves to be recognized. Expect regular Hive-Fives, shoutouts and the chance to see your ideas come to life as part of our reward program.\nFuel Your Growth Journey with Certifications: We're all about your growth groove! Level up your skills with our support as we cover the cost of your certifications.",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Hive",
            "Hadoop",
            "Gcp",
            "Pyspark",
            "Azure",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Staff Data Scientist",
        "company_name": "Proofpoint",
        "experience": "10-12 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "It's fun to work in a company where people truly BELIEVE in what they're doing!\n\nWe're committed to bringing passion and customer focus to the business.\n\nProofpoint is hiring a Staff Data Scientist / ML Engineer to lead multiple Data Science, GenAI, and AI Engineering initiatives. The ideal candidate will drive the development and deployment of innovative machine learning and generative AI solutions, working cross-functionally with DevOps, product, and data engineering teams. This leadership role requires deep technical expertise, hands-on implementation experience, and a strong vision for scalable AI systems that power real-world applications in cybersecurity.\n\nResponsibilities\n\nLead the development of machine learning models and advanced analytics solutions to solve complex business problems.\nDesign and implement generative AI and large language model (LLM) applications, including fine-tuning and domain adaptation for cybersecurity use cases.\nCollaborate with engineering teams to build scalable and secure LLM-based systems (retrieval-augmented generation, prompt engineering, evaluation pipelines).\nArchitect and lead AI solutions across full lifecyclefrom experimentation to MLOps pipelines and production deployment.\nDesign experiments and use statistical analysis to measure the impact of various business strategies.\nOversee the deployment of machine learning and LLM models in production, ensuring performance, scalability, and responsible AI practices.\nLead the development of model performance monitoring, observability, and continuous learning pipelines.\nDefine technical direction for LLM and GenAI adoption, including benchmarking open-source and commercial models.\nChampion AI/ML best practices including model governance, reproducibility, and ethical AI considerations.\nPromote a data-driven and AI-forward culture within the organization and advocate for cutting-edge AI adoption across teams.\nStay current with advancements in LLMs, GenAI, AI engineering, and emerging AI regulations.\n\nQualifications\n\nEducation\n\nPhD or Master's degree in Computer Science, Data Science, Machine Learning, Statistics, or related discipline.\n\nExperience\n\n10+ years of experience in data science or applied machine learning, with 3+ years in a technical leadership or managerial role.\nProven track record of designing, developing, and deploying ML and GenAI solutions at scale.\nHands-on experience working with LLMs (e.g., OpenAI, Anthropic, LLaMA, Mistral) and GenAI frameworks (e.g., LangChain, LlamaIndex, Hugging Face).\nExperience in cybersecurity or enterprise-scale threat detection systems is a strong plus.\n\nTechnical Skills\n\nProficiency in Python and relevant ML/AI libraries (e.g., PyTorch, TensorFlow, Transformers, Scikit-learn).\nStrong grasp of LLM fine-tuning, prompt engineering, RAG pipelines, vector databases (e.g., FAISS, Pinecone), and inference optimization.\nExperience with cloud platforms (AWS, GCP, Azure) and containerization tools (Docker, Kubernetes).\nSolid understanding of MLOps principles including CI/CD for ML, feature stores, model versioning, and monitoring.\nFamiliarity with privacy, security, and compliance considerations in deploying AI solutions.\n\nSoft Skills\n\nExcellent leadership and mentorship skills, with a collaborative approach to cross-functional problem solving.\nAbility to communicate complex technical ideas to both technical and non-technical stakeholders.\nStrong innovation mindset, strategic thinking, and a passion for applying AI to impactful real-world problems\n\nIf you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!",
        "skills": [
            "model versioning",
            "Scikit-learn",
            "Pinecone",
            "Monitoring",
            "prompt engineering",
            "LLM fine-tuning",
            "FAISS",
            "Transformers",
            "RAG pipelines",
            "MLOps principles",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "Docker",
            "Azure",
            "Kubernetes",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Thoucentric",
        "experience": "3-5 Years",
        "salary": "INR 10.88 - 25 LPA ",
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nThoucentric is the Consulting arm of Xoriant, a prominent digital engineering services company with 5000+ employees. We are headquartered in Bangalore with presence across multiple locations in India, US, UK, Singapore & Australia Globally.\n\nAs the Consulting business of Xoriant, We help clients with Business Consulting, Program & Project Management, Digital Transformation, Product Management, Process & Technology Solutioning and Execution including Analytics & Emerging Tech areas cutting across functional areas such as Supply Chain, Finance & HR, Sales & Distribution across US, UK, Singapore and Australia. Our unique consulting framework allows us to focus on execution rather than pure advisory. We are working closely with marquee names in the global consumer & packaged goods (CPG) industry, new age tech and start-up ecosystem. Xoriant (Parent entity) started in 1990 and is a Sunnyvale, CA headquartered digital engineering firm with offices in the USA, Europe, and Asia. Xoriant is backed by ChrysCapital, a leading private equity firm. Our strengths are now combined with Xoriant's capabilities in AI & Data, cloud, security and operations services proven for 30 years.\n\nWe have been certified as Great Place to Work by AIM and have been ranked as 50 Best Firms for Data Scientists to Work For.\n\nWe have an experienced consulting team of over 450+ world-class business and technology consultants based across six global locations, supporting clients through their expert insights, entrepreneurial approach and focus on delivery excellence. We have also built point solutions and products through Thoucentric labs using AI/ML in the supply chain space.\n\nJob Description\n\nJob Title: Data Scientist (3-5 Years Experience)\n\nLocation: Bangalore\n\nAbout Us:\n\nThoucentric is a forward-thinking organization at the forefront of leveraging data-driven insights to solve complex business challenges. We are seeking a passionate and skilled Data Scientist to join our dynamic team and help us drive innovation through advanced analytics and machine learning.\n\nKey Responsibilities:\n\nDevelop and implement machine learning and deep learning models for various business problems, with a strong focus on time series forecasting.\nAnalyze large, complex datasets to extract actionable insights and identify trends, patterns, and opportunities for improvement.\nDesign, build, and validate predictive models using state-of-the-art techniques, ensuring scalability and robustness.\nCollaborate with cross-functional teams (Product, Engineering, Business) to translate business requirements into data science solutions.\nCommunicate findings and recommendations clearly to both technical and non-technical stakeholders.\nStay updated with the latest research and advancements in machine learning, deep learning, and time series analysis, and proactively apply new techniques as appropriate.\nMentor junior team members and contribute to a culture of continuous learning and innovation.\n\nRequirements\n\nRequired Skills & Qualifications:\n\n3-5 years of hands-on experience in data science, machine learning, and statistical modeling.\nStrong expertise in time series forecasting (ARIMA, XGBoost, RandomForest, TFT, NHITS, etc.) and familiarity with deep learning frameworks (TensorFlow, PyTorch).\nExcellent programming skills in Python (preferred), with proficiency in libraries such as NumPy, Pandas, scikit-learn, and visualization tools (Matplotlib, Seaborn, Plotly).\nSolid conceptual understanding of machine learning algorithms, deep learning architectures, and statistical methods.\nExperience with data preprocessing, feature engineering, and model evaluation.\nAbility to learn quickly and adapt to new technologies, tools, and methodologies.\nStrong problem-solving skills and a keen attention to detail.\nExcellent communication and presentation skills.\n\nPreferred Qualifications:\n\nExperience with cloud platforms and MLOps tools.\nExposure to big data technologies (Spark, Hadoop) is a plus.\nMaster's degree in Computer Science, Statistics, Mathematics, or a related field.\n\nBenefits\n\nWhat a Consulting role at Thoucentric will offer you\n\nOpportunity to define your career path and not as enforced by a manager\nA great consulting environment with a chance to work with Fortune 500 companies and startups alike.\nA dynamic but relaxed and supportive working environment that encourages personal development.\nBe part of One Extended Family. We bond beyond work - sports, get-togethers, common interests etc. Work in a very enriching environment with Open Culture, Flat Organization and Excellent Peer Group.\nBe part of the exciting Growth Story of Thoucentric!",
        "skills": [
            "TFT",
            "NHITS",
            "scikit-learn",
            "RandomForest",
            "Plotly",
            "Time Series Forecasting",
            "Matplotlib",
            "Machine Learning",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Pandas",
            "Pytorch",
            "Arima",
            "XGBoost",
            "Seaborn",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Recro",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "We're Hiring: Senior Data Scientist\nRemote | Full-Time | 4+ Years Experience\nJoin us as a Senior Data Scientist and help build scalable, real-world ML solutions that power business decisions. You'll work closely with engineers and stakeholders to turn complex data into actionable insights.\nWhat You'll Do:\nDesign & deploy ML/statistical models\nAnalyze data to uncover key insights\nCollaborate across tech & business teams\nWhat You Bring:\n4+ years in data science\nStrong Python & SQL skills\nSolid grasp of ML algorithms\nCloud experience (AWS/GCP/Azure) is a plus\nLet's build something impactful together.\nApply now or DM me to learn more!\n#RemoteJobs #DataScience #MachineLearning #SeniorDataScientist #Hiring #Python #TechCareers",
        "skills": [
            "ML algorithms",
            "Gcp",
            "Azure",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Recro",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Senior Data Scientist\nLocation: Onsite Bangalore\nExperience: 7+ years\nRole Overview\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning,\ndeep learning, and statistical modeling, with the ability to translate complex\noperational problems into scalable AI/ML solutions. In addition to core data science\nresponsibilities, the role involves building production-ready backends in Python and\ncontributing to end-to-end model lifecycle management. Exposure to computer vision\nis a plus, especially for industrial use cases like identification, intrusion detection,\nand anomaly detection.\nKey Responsibilities\nDevelop, validate, and deploy machine learning and deep learning models for\nforecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in\nproduction environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to\nobject detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science\nsolutions\nWork with data engineering and DevOps teams to operationalize and monitor\nmodels at scale\nCollaborate with product, domain experts, and engineering teams to iterate on\nsolution design\nContribute to technical documentation, model explainability, and\nreproducibility practices\nRequired Skills\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and\nanomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and\nperformance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\nGood to Have\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\nQualifications\nMaster's or PhD in Data Science, Computer Science, Applied Mathematics, or\nrelated field\n7+ years of experience in machine learning and data science, including end-\nto-end deployment of models in production",
        "skills": [
            "Airflow",
            "scikit-learn",
            "time-series modeling",
            "MLflow",
            "anomaly detection",
            "forecasting techniques",
            "Statistical Modeling",
            "Machine Learning",
            "Kafka",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Deep Learning",
            "Pytorch",
            "Docker",
            "Spark",
            "Flask",
            "FastAPI",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist - AI/ML Job",
        "company_name": "YASH Technologies",
        "experience": "8-11 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "YASH Technologies is a leading technology integrator specializing in helping clients reimagine operating models, enhance competitiveness, optimize costs, foster exceptional stakeholder experiences, and drive business transformation.\n\nAt YASH, we're a cluster of the brightest stars working with cutting-edge technologies. Our purpose is anchored in a single truth bringing real positive changes in an increasingly virtual world and it drives us beyond generational gaps and disruptions of the future.\n\nWe are looking forward to hire AI/ML Professionals in the following areas :\n\nDesignation: Lead Data Scientist\n\nWe are seeking a highly motivated and experienced Lead Data Scientist to join our growing team. In this role, you will be responsible for leading complex data science projects, mentoring junior data scientists, and driving the development of innovative solutions that leverage data to achieve business objectives. You will apply your deep expertise in machine learning, statistical modeling, and data analysis to extract actionable insights and drive strategic decision-making.\n\nExperience: 811 Years\n\nJob Type: Full-time\n\nResponsibilities\n\nEnd-to-end delivery ownership of ML and GenAI use cases.\nArchitect RAG pipelines and build enterprise-scale GenAI accelerators.\nCollaborate with architects and presales teams.\nEnsure code modularity, testability, and governance.\n\nRequired Skills\n\nLangChain, LangGraph, embedding techniques, prompt engineering.\nClassical ML: XGBoost, Random Forest, time-series forecasting.\nKnowledge of Python ML stack and FastAPI.\nStrong understanding of security and cloud optimization.\n\nAt YASH, you are empowered to create a career that will take you to where you want to go while working in an inclusive team environment. We leverage career-oriented skilling models and optimize our collective intelligence aided with technology for continuous learning, unlearning, and relearning at a rapid pace and scale.\n\nOur Hyperlearning workplace is grounded upon four principles\n\nFlexible work arrangements, Free spirit, and emotional positivity\nAgile self-determination, trust, transparency, and open collaboration\nAll Support needed for the realization of business goals,\nStable employment with a great atmosphere and ethical corporate culture",
        "skills": [
            "LangChain",
            "Python ML stack",
            "time-series forecasting",
            "cloud optimization",
            "prompt engineering",
            "LangGraph",
            "embedding techniques",
            "Security",
            "Classical ML",
            "Random Forest",
            "XGBoost",
            "FastAPI"
        ]
    },
    {
        "job_title": "Sr. Data Scientist",
        "company_name": "Cognitio Analytics Inc",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Sr. Data Scientist (3 to 6 years)\nAbout Us:\nWebsite:https://www.cognitioanalytics.com/\nCognitio Analytics, founded in 2013, aims to be the preferred provider of AI / ML driven productivity solutions for large enterprises. The company has received awards for its Smart Operations and Total Rewards Analytics Solutions and is dedicated to innovation, R&D, and creating sustained value for clients. Cognitio Analytics has been recognized as a Great Place to Work for its commitment to fostering an innovative work environment and employee satisfaction.\nOur solutions include Total Rewards Analytics powered by Cognitio's Total Rewards Data Factory, The Total Rewards Analytics solutions help our clients achieve better outcomes and higher ROI on investments in all kinds of Total Rewards programs.\nOur smart operations solutions drive productivity in complex operations, such as claims processing, commercial underwriting etc. These solutions, based on proprietary capabilities based on AI, advanced process and task mining, and deep understanding of operations drive effective digital transformation for our clients.\nIdeal qualifications, skills and experiences we are looking for are:\n- We are actively seeking a talented and results-driven Data Scientist to join our team and take ownership of deliverables through the power of data analytics and insights.\n- Your contributions will be instrumental in making data-informed decisions, identifying growth opportunities, and propelling our organization to new levels of success.\n- Doctorate/Master's/bachelor's degree in data science, Statistics, Computer Science, Mathematics, Economics, commerce or a related field.\n- Minimum of 3 years of experience working as a Data Scientist or in a similar analytical role, with experience leading data science projects and teams.\nExperience in Healthcare domain with exposure to clinical operations, financial, risk rating, fraud, digital, sales and marketing, and wellness, e-commerce or the ed tech industry is a plus.\n- Expertise in programming languages such as SQL, Python/PySpark and proficiency with data manipulation, analysis, and visualization libraries (e.g., pandas, NumPy, Matplotlib, seaborn).\nVery strong python and exceptional with pandas, NumPy, advanced python (pytest, class, inheritance, docstrings).\n- Deep understanding of machine learning algorithms, model evaluation, and feature engineering. Experience with frameworks like scikit-learn, TensorFlow, or Py torch.\nDeep understanding of ML and Deep Learning is a must\nBasis NLP experience is highly valuable.\nPyspark experience is highly valuable.\nCompetitive coding experience (Leet Code) is highly valuable.\n- Strong expertise in statistical modelling techniques such as regression, clustering, time series analysis, and hypothesis testing.\n- Experience of building & deploying machine learning models in cloud environment: Microsoft Azure preferred (Databricks, Synapse, Data Factory, etc.)\n- Basic MLOPs experience with FastAPIs and experience of docker is highly valuable and AI governance\n- Ability to understand business objectives, market dynamics, and strategic priorities. Demonstrated experience translating data insights into tangible business outcomes and driving data-informed decision-making.\n- Excellent verbal and written communication skills\n- Proven experience leading data science projects, managing timelines, and delivering results within deadlines.\n- Strong collaboration skills with the ability to work effectively in cross-functional teams, build relationships, and foster a culture of knowledge sharing and continuous learning.\nCognitio Analytics is an equal-opportunity employer. We are committed to a work environment that celebrates diversity. We do not discriminate against any individual based on race, color, sex, national origin, age, religion, marital status, sexual orientation, gender identity, gender expression, military or veteran status, disability, or any factors protected by applicable law. All Cognitio employees are expected to understand and adhere to all Cognitio Security and Privacy related policies in order to protect Cognitio data and our client's data. Our salary ranges are based on paying competitively for our size and industry and are one part of the total compensation package that also includes a bonus plan, equity, benefits, and other opportunities at Cognitio. Individual pay decisions are based on a number of factors, including qualifications for the role, experience level, and skillset.",
        "skills": [
            "scikit-learn",
            "Tensorflow",
            "Matplotlib",
            "Numpy",
            "Seaborn",
            "Pandas",
            "Docker",
            "Pyspark",
            "FastAPI",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Insight Global",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "***$10-$20/hr USD***\nMust haves:\n5+ years in data science, statistical modeling/ machine learning\nStrong background in statistics and Machine learning models (Clustering, Market basket/apriori, supervised machine learning, Keras, boosting and bagging)\nWell versed in SQL, Python\nExperience with Keras, Tensorflow deep learning models (To predict price impact on product demands)\nKnowledge on Optimizations (Dual Annealing/ Mixed Objective/ Goal based optimization)\nExperience working on large data sets, linear/ non-linear optimizations, ML applications\nPluses:\nPricing analytics and price elasticity\nExperience with Deep Learning Framework\nDay to day:\nAn employer is looking for a Data Scientist to sit remotely. You will primarily be tasked with analyzing data and generate analytical outputs to deliver insights, evaluate hypothesis and complete root cause analysis of the business problem. The focus for this team will be AI based solutions to determine consumer insights and marketing performance analytics to help brands understand consumer behavior for promotions and pricing decisions. You will design end to end solutions by leveraging the latest ML techniques, including data preparation, exploratory data analysis, feature engineering, model selection, model development, and deployment. Additional tasks will include:\nSolve business problems which involve:\nBrainstorm with clients and internal teams to define a problem\nTranslate the business problem into an analytical problem\nIdentify internal and external data requirements for solving the analytical problem\nSolving the analytical problem using concepts from mathematics, statistics, Artificial Intelligence and Machine learning\nCreate, maintain and enhance artefacts that can help communicate the solution to clients like dashboards, power point decks, excel sheets etc.\nDevelop algorithms based on the problem statement & domain area. Design & develop new algorithms for optimization, recommendation, attribution, and emerging real-time consumer analytics.\nEstablish mathematical models to represent specific business functions or consumer behavior.\nProgramming of algorithms, and integration into the data science framework.\nProvide Support and add new features in a marketing performance analytics automation platform. Work on the scalability and performance of the automation platform.\nCreate proof of concepts to vet out new technologies.\nInfluence design and architecture principles for new technology products.",
        "skills": [
            "Statistical Modeling",
            "linear optimizations",
            "Boosting",
            "Mixed Objective Goal based optimization",
            "bagging",
            "Dual Annealing",
            "optimizations",
            "non-linear optimizations",
            "Keras",
            "Sql",
            "Tensorflow",
            "Deep Learning",
            "Machine Learning",
            "Data Science",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Inxite Out",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Senior Data Scientist\n\nLocation: Kolkata/Bangalore (Remote/Hybrid options available)\n\nCompany: Inxite-out\n\nJob Overview: We are seeking a highly skilled and experienced Senior Data Scientist to join our dynamic team. This role will involve leading data science initiatives, developing advanced machine learning models, and providing actionable insights for business strategies. The ideal candidate will have a strong background in data science, statistical analysis, and business problem-solving, with a passion for delivering high-quality solutions.\n\nKey Responsibilities & Skills\n\nLead the development of advanced predictive and prescriptive models using machine learning, forecasting and other statistical methods\nWorking knowledge of regression, classification, clustering, association algorithms and model optimization techniques\nWork closely with business stakeholders to understand requirements and translate them into data science solutions\nDesign and implement end-to-end data science workflows, including data collection, preprocessing, feature engineering, model building, and evaluation\nCollaborate with cross-functional teams to deploy and integrate models into production environments\nPerform model validation, fine-tuning, and optimization to ensure accuracy and scalability\nIdentify and investigate anomalies or unexpected behaviours in model outputs\nBuild REST APIs using Fast/Flask, move models into production, ensure expected accuracies and throughput for the model\nMentor and guide junior data scientists, providing technical expertise and leadership\n\nQualifications\n\nBachelor's or master's in computer science, Statistics, Mathematics, Engineering, or a related field\n4+ years of experience in data science or a similar analytical role, with a proven track record of delivering impactful projects\nStrong proficiency in python with experience in libraries like pandas, NumPy, Spacy, TensorFlow, Scikit-learn etc.\nExperience in working with cloud platforms (Azure), Databricks, PySpark\nStrong theoretical and practical knowledge of ML model development, hyper-parameter tuning, and production deployment\nSolid understanding of statistical analysis, data mining, feature engineering\nStrong problem-solving skills with the ability to think critically and adapt to complex business challenges\nExcellent communication skills with the ability to explain complex data-driven concepts to a non-technical audience\nExperience in leading and mentoring junior team members.\nStrong understanding of MLOps- lifecycle of a machine learning model\n\nAdditional Skills (good To Have)\n\nKnowledge of deep learning, CNN, MLP, OpenCV etc.\nWorking knowledge of NLP/GenAI based solutions\nExperience working with SAP (S/4 HANA, BW) is a plus",
        "skills": [
            "Scikit-learn",
            "Fast",
            "Spacy",
            "Pyspark",
            "Tensorflow",
            "Numpy",
            "Pandas",
            "MLops",
            "Flask",
            "Databricks",
            "Rest Apis",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Staff Data Scientist",
        "company_name": "Proofpoint",
        "experience": "10-12 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "It's fun to work in a company where people truly BELIEVE in what they're doing!\n\nWe're committed to bringing passion and customer focus to the business.\n\nProofpoint is hiring a Staff Data Scientist / ML Engineer to lead multiple Data Science, GenAI, and AI Engineering initiatives. The ideal candidate will drive the development and deployment of innovative machine learning and generative AI solutions, working cross-functionally with DevOps, product, and data engineering teams. This leadership role requires deep technical expertise, hands-on implementation experience, and a strong vision for scalable AI systems that power real-world applications in cybersecurity.\n\nResponsibilities\n\nLead the development of machine learning models and advanced analytics solutions to solve complex business problems.\nDesign and implement generative AI and large language model (LLM) applications, including fine-tuning and domain adaptation for cybersecurity use cases.\nCollaborate with engineering teams to build scalable and secure LLM-based systems (retrieval-augmented generation, prompt engineering, evaluation pipelines).\nArchitect and lead AI solutions across full lifecyclefrom experimentation to MLOps pipelines and production deployment.\nDesign experiments and use statistical analysis to measure the impact of various business strategies.\nOversee the deployment of machine learning and LLM models in production, ensuring performance, scalability, and responsible AI practices.\nLead the development of model performance monitoring, observability, and continuous learning pipelines.\nDefine technical direction for LLM and GenAI adoption, including benchmarking open-source and commercial models.\nChampion AI/ML best practices including model governance, reproducibility, and ethical AI considerations.\nPromote a data-driven and AI-forward culture within the organization and advocate for cutting-edge AI adoption across teams.\nStay current with advancements in LLMs, GenAI, AI engineering, and emerging AI regulations.\n\nQualifications\n\nEducation\n\nPhD or Master's degree in Computer Science, Data Science, Machine Learning, Statistics, or related discipline.\n\nExperience\n\n10+ years of experience in data science or applied machine learning, with 3+ years in a technical leadership or managerial role.\nProven track record of designing, developing, and deploying ML and GenAI solutions at scale.\nHands-on experience working with LLMs (e.g., OpenAI, Anthropic, LLaMA, Mistral) and GenAI frameworks (e.g., LangChain, LlamaIndex, Hugging Face).\nExperience in cybersecurity or enterprise-scale threat detection systems is a strong plus.\n\nTechnical Skills\n\nProficiency in Python and relevant ML/AI libraries (e.g., PyTorch, TensorFlow, Transformers, Scikit-learn).\nStrong grasp of LLM fine-tuning, prompt engineering, RAG pipelines, vector databases (e.g., FAISS, Pinecone), and inference optimization.\nExperience with cloud platforms (AWS, GCP, Azure) and containerization tools (Docker, Kubernetes).\nSolid understanding of MLOps principles including CI/CD for ML, feature stores, model versioning, and monitoring.\nFamiliarity with privacy, security, and compliance considerations in deploying AI solutions.\n\nSoft Skills\n\nExcellent leadership and mentorship skills, with a collaborative approach to cross-functional problem solving.\nAbility to communicate complex technical ideas to both technical and non-technical stakeholders.\nStrong innovation mindset, strategic thinking, and a passion for applying AI to impactful real-world problems\n\nIf you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!",
        "skills": [
            "model versioning",
            "Scikit-learn",
            "Pinecone",
            "Monitoring",
            "prompt engineering",
            "LLM fine-tuning",
            "FAISS",
            "Transformers",
            "RAG pipelines",
            "MLOps principles",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "Docker",
            "Azure",
            "Kubernetes",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Recro",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "We're Hiring: Senior Data Scientist\nRemote | Full-Time | 4+ Years Experience\nJoin us as a Senior Data Scientist and help build scalable, real-world ML solutions that power business decisions. You'll work closely with engineers and stakeholders to turn complex data into actionable insights.\nWhat You'll Do:\nDesign & deploy ML/statistical models\nAnalyze data to uncover key insights\nCollaborate across tech & business teams\nWhat You Bring:\n4+ years in data science\nStrong Python & SQL skills\nSolid grasp of ML algorithms\nCloud experience (AWS/GCP/Azure) is a plus\nLet's build something impactful together.\nApply now or DM me to learn more!\n#RemoteJobs #DataScience #MachineLearning #SeniorDataScientist #Hiring #Python #TechCareers",
        "skills": [
            "ML algorithms",
            "Gcp",
            "Azure",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist (freelancer)",
        "company_name": "Soul AI",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are looking for a Data Scientist to join and build intelligent, data-driven solutions for our client that enable impactful decisions. This role requires contributions across the data science lifecycle from data wrangling and exploratory analysis to building and deploying machine learning models.\nWhether you're just getting started or have years of experience, we're looking for individuals who are curious, analytical, and driven to make a difference with data.\nResponsibilities:\nDesign, develop, and deploy machine learning models and analytical solutions.\nConduct exploratory data analysis and feature engineering.\nOwn or contribute to the end-to-end data science pipeline: data cleaning, modeling, validation, and deployment.\nCollaborate with cross-functional teams (engineering, product, business) to define problems and deliver measurable impact.\nTranslate business challenges into data science problems and communicate findings clearly.\nImplement A/B tests, statistical tests, and experimentation strategies.\nSupport model monitoring, versioning, and continuous improvement in production environments.\nEvaluate new tools, frameworks, and best practices to improve model accuracy and scalability.\nRequired Skills:\nStrong programming skills in Python including libraries such as pandas, NumPy, scikit-learn, matplotlib, seaborn.\nProficient in SQL, comfortable querying large, complex datasets.\nSound understanding of statistics, machine learning algorithms, and data modeling.\nExperience building end-to-end ML pipelines.\nExposure to or hands-on experience with model deployment tools like FastAPI, Flask, MLflow.\nExperience with data visualization and insight communication.\nFamiliarity with version control tools (e.g., Git) and collaborative workflows.\nAbility to write clean, modular code and document processes clearly.\nNice to Have:\nExperience with deep learning frameworks like TensorFlow or PyTorch.\nFamiliarity with data engineering tools like Apache Spark, Kafka, Airflow, dbt.\nExposure to MLOps practices and managing models in production environments.\nWorking knowledge of cloud platforms like AWS, GCP, or Azure (e.g., SageMaker, BigQuery, Vertex AI).\nExperience designing and interpreting A/B tests or causal inference models.\nPrior experience in high-growth startups or cross-functional leadership roles.\nEducational Qualifications:\nBachelor's or Master's degree in Computer Science, Data Science, Mathematics, Engineering, or a related field.\nPh.D. holders or candidates with demonstrated applied research contributions are a plus.",
        "skills": [
            "scikit-learn",
            "MLflow",
            "end-to-end ML pipelines",
            "Statistics",
            "Data Modeling",
            "Sql",
            "Numpy",
            "Seaborn",
            "Git",
            "Pandas",
            "Matplotlib",
            "Flask",
            "Data Visualization",
            "FastAPI",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Nirvana Insurance",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Insurance",
        "job_description": "Who we are\n\nNirvana is on a mission to harness the power of data to revolutionize commercial insurance and enable a safer world. We are bringing much-needed innovation into the legacy, trillion-dollar commercial insurance industry. We have developed cutting-edge predictive models that use real-time IoT data from billions of connected devices, allowing us to better understand and price risk. Our AI-driven platform fundamentally changes the way an insurance company operates with personalized risk scoring, faster underwriting, modernized claims, and proactive, data-driven insights to help customers prevent accidents.\n\nWe've already proven the scalereaching well over $100 million in premiums and more than doubling year over year. Our data moat is growing exponentially with more than 20 billion miles of telematics data, leading to more predictive models and new insights into how we can better understand and reduce risk. Altogether, our loss ratio, efficiency, and customer experience are redefining what can be done in the industry.\n\nWith $170+ million raised, including an industry-leading Series C round in January 2025, we're only accelerating our growth, with strong support from top-tier VCs including Lightspeed, General Catalyst, and Valor. Nirvana's leadership team has previously helped scale multi-billion-dollar companies from scratch, including Samsara, Rubrik, and Flexport, and includes industry veterans from Hiscox, The Hartford, and RLI.\n\nAbout The Role\n\nAt Nirvana, we work with heterogeneous datasets and mine predictive signals to accurately identify risks for businesses. Some of the data we work with includes:\n\n100s of millions of GPS and sensor data points per vehicle, ingested and stored in our proprietary data platform.\nSafety events like harsh braking, distracted driving, etc computed from sensors in commercial vehicles.\nHigh-dimensional datasets containing, among others, decades of vehicle inspection, crash, and violation data for all commercial vehicles in the US.\n\nAs a Senior Data Scientist, not only will you execute the data science team's projects and move the agenda forward, you will also work closely with our data engineering and insurance teams and use these large datasets to help build data-driven insurance and safety products in a fast-paced startup environment. You must enjoy working with large data and finding interesting patterns in the data through analytics experiments in a methodical and data-driven scientific way.\n\nThe insurance / risk business has many unsolved problems where sophisticated use of information plays a critical role. Join us in making an impact on the industry!\n\nWhat you'll do\n\nServes as the lead modeler on larger projects or works on concurrent projects E2E\nWork closely with the insurance team to build and validate complex risk models\nExtract actionable insights and recommendations from time series data to help our customers improve their safety\nUnderstand data characteristics, prepare and clean large high-dimensional data sets, and define and automate robust data quality measures\nIdentifies creative solutions to complex business problems of broad scope and business impact\nInterprets data analysis and model results for business and technical teams\nStandardize analysis methodology and building automation frameworks\nAccurately performs peer reviews across a broad spectrum of work\nWork in cross functional teams\nSupports recruitment needs\n\nAbout You\n\nGraduate degree in a quantitative or technical discipline and/or 4+ years of applying advanced quantitative techniques to problems in industry\nStrong demonstrable knowledge of topics such as statistical inference, predictive analytics, probability theory, machine learning, etc\nProven track record with design, development, implementation, and monitoring of statistical and machine learning models independently\nStrong technical (written and verbal) communication, prioritization, and time management skills\nStrong programming skills with experience using modern packages in Python\nDemonstrated experience building, validating, and applying statistical machine learning methods to real world problems\nDemonstrated experience in co-creating data science platforms for efficient scaling of research and outputs; from early stages to mature processes\n\nWhat you'll get from us\n\nCompetitive compensation & meaningful equity\nHealth insurance for you and your family\nMonthly wellness stipend\nHybrid culture and reimbursement for home office equipment\nA flexible vacation policy and a team that understands building a company is a marathon, not a sprint\nA culture that gives you the autonomy you need to do great work, and the transparency you need to make good decisions",
        "skills": [
            "Data Analysis",
            "data quality measures",
            "probability theory",
            "Statistical Inference",
            "Predictive Analytics",
            "Python",
            "Automation Frameworks",
            "Machine Learning"
        ]
    },
    {
        "job_title": "Fraud Data scientist",
        "company_name": "Xpetize",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "We have opening for Fraud Data scientist at Bangalore ,Hyderabad Location\nposition- Fraud Data Scientist\nExperience-5+ years\nLocation- Bangalore,Hyderabad,Gurgaon\nNotice- Max 15 Days\nInterested candidates share your resume to [HIDDEN TEXT]\nKeyskills:\nA FraudData Scientistfocuses on analyzing large datasets and building predictive models to detect fraudulentactivities. They use statistical and machine learning techniques to identify patterns in fraudulenttransactions.\nKey Responsibilities:\nWork with large datasets to identify fraudulentpatterns.\nUse statistical models like regression (linear, logistic) and clustering (K-means).\nDevelop machine learning models like XGBoost, Gradient Boosting Machines (GBM)to predict fraudbased on historical data.\nBuild clustering models(grouping similar behaviors), scoring models(assigning fraudrisk scores), and propensity models(predicting the likelihood of fraud).\nUtilize Python, SQL, and modeling techniquesto process and analyze fraud-related data.",
        "skills": [
            "XGBoost",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist AI/ML",
        "company_name": "UNIADROITECH INNOVATION",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist\nLocation: Bangalore (Hybrid 2 days onsite, rest remote)\nCTC: Up to 12 to 14 LPA\nStart Date: Immediate Joiners Preferred\nExperience: 4 to 6 years (Minimum 3 years in Data Science AI/ML)\nJob Description:\nWe are seeking a highly skilled and motivated Data Scientist with a strong foundation in AI/ML and experience handling complex datasets and real-world modeling problems. The ideal candidate should possess both deep technical expertise and excellent communication skills to effectively collaborate with technical and non-technical stakeholders.\nKey Responsibilities:\nDevelop, train, and optimize machine learning and deep learning models.\nDesign and implement NLP systems and work with open-source LLMs to build RAG (Retrieval-Augmented Generation) applications.\nApply AI/ML techniques to solve complex business problems with a research-oriented mindset.\nWork with large-scale datasets, conduct data cleaning, preprocessing, and exploratory data analysis.\nCollaborate with cross-functional teams and stakeholders to understand business requirements and deliver actionable AI/ML solutions.\nDeploy AI/ML models in cloud environments and optimize for scalability and performance.\nStay updated with the latest developments in AI/ML and apply them to current challenges.\nRequired Skills:\nB.E./ B. Tech / M. Tech / MCA in Computer Science, Artificial Intelligence, or a related field.\n6+ years of total IT experience with a minimum of 3 years in Data Science (AI/ML).\nStrong programming skills in Python.\nExperience with deep learning frameworks such as TensorFlow or PyTorch.\nHands-on experience in building and optimizing LLMs and RAG systems.\nProficiency in supervised & unsupervised learning, reinforcement learning, federated learning, time series forecasting, Bayesian statistics, and optimization techniques.\nExperience working in cloud environments like AWS, Azure, GCP, or Databricks.\nAbility to communicate effectively with technical and business stakeholders.\nPreferred Candidate Location:\nBangalore or Chennai preferred.\nOpen to candidates willing to relocate to Bangalore.\nIdeal for professionals with 45 years of experience and a strong foundation in AI/ML.\nIf you're passionate about building impactful AI/ML solutions and enjoy working in a collaborative, stakeholder-facing role, we'd love to hear from you.\nApply Now: Send your updated resume to [HIDDEN TEXT] & [HIDDEN TEXT]\nLet's build the future, together. Suraj Biswas, Dave G - (D. Goswami), MBA Kusuma Teppala",
        "skills": [
            "NLP systems",
            "RAG systems",
            "LLMs",
            "supervised learning",
            "reinforcement learning",
            "Optimization Techniques",
            "Bayesian Statistics",
            "time series forecasting",
            "federated learning",
            "Unsupervised Learning",
            "Tensorflow",
            "Gcp",
            "Pytorch",
            "Databricks",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist AI/ML",
        "company_name": "UNIADROITECH INNOVATION",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist\nLocation: Bangalore (Hybrid 2 days onsite, rest remote)\nCTC: Up to 12 to 14 LPA\nStart Date: Immediate Joiners Preferred\nExperience: 4 to 6 years (Minimum 3 years in Data Science AI/ML)\nJob Description:\nWe are seeking a highly skilled and motivated Data Scientist with a strong foundation in AI/ML and experience handling complex datasets and real-world modeling problems. The ideal candidate should possess both deep technical expertise and excellent communication skills to effectively collaborate with technical and non-technical stakeholders.\nKey Responsibilities:\nDevelop, train, and optimize machine learning and deep learning models.\nDesign and implement NLP systems and work with open-source LLMs to build RAG (Retrieval-Augmented Generation) applications.\nApply AI/ML techniques to solve complex business problems with a research-oriented mindset.\nWork with large-scale datasets, conduct data cleaning, preprocessing, and exploratory data analysis.\nCollaborate with cross-functional teams and stakeholders to understand business requirements and deliver actionable AI/ML solutions.\nDeploy AI/ML models in cloud environments and optimize for scalability and performance.\nStay updated with the latest developments in AI/ML and apply them to current challenges.\nRequired Skills:\nB.E./ B. Tech / M. Tech / MCA in Computer Science, Artificial Intelligence, or a related field.\n6+ years of total IT experience with a minimum of 3 years in Data Science (AI/ML).\nStrong programming skills in Python.\nExperience with deep learning frameworks such as TensorFlow or PyTorch.\nHands-on experience in building and optimizing LLMs and RAG systems.\nProficiency in supervised & unsupervised learning, reinforcement learning, federated learning, time series forecasting, Bayesian statistics, and optimization techniques.\nExperience working in cloud environments like AWS, Azure, GCP, or Databricks.\nAbility to communicate effectively with technical and business stakeholders.\nPreferred Candidate Location:\nBangalore or Chennai preferred.\nOpen to candidates willing to relocate to Bangalore.\nIdeal for professionals with 45 years of experience and a strong foundation in AI/ML.\nIf you're passionate about building impactful AI/ML solutions and enjoy working in a collaborative, stakeholder-facing role, we'd love to hear from you.\nApply Now: Send your updated resume to [HIDDEN TEXT] & [HIDDEN TEXT]\nLet's build the future, together. Suraj Biswas, Dave G - (D. Goswami), MBA Kusuma Teppala",
        "skills": [
            "NLP systems",
            "RAG systems",
            "LLMs",
            "supervised learning",
            "reinforcement learning",
            "Optimization Techniques",
            "Bayesian Statistics",
            "time series forecasting",
            "federated learning",
            "Unsupervised Learning",
            "Tensorflow",
            "Gcp",
            "Pytorch",
            "Databricks",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist Forecasting",
        "company_name": "Blend",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nWe are looking for an experienced Data Scientist with a specialization in forecasting to join our data science team. The ideal candidate will have a deep understanding of statistical and machine learning techniques for time series forecasting and demand prediction. In this role, you will drive business value by developing accurate forecasting models and delivering actionable insights to optimize decision-making processes across the organization\n.\nKey Responsibilitie\ns:\nDevelop and implement advanced time series forecasting models to predict key business metrics (e.g., sales, demand, inventory, etc\n.).Analyze historical data to identify trends, seasonality, and other patterns that can be leveraged for accurate forecasti\nng.Collaborate closely with client's cross-functional teams such as Supply Chain, Finance, and Marketing to understand business needs and deliver tailored forecasting solutio\nns.Build and optimize machine learning models for forecasting using state-of-the-art techniques (e.g., ARIMA, Prophet, LSTM, etc\n.).Design and run experiments to validate forecasting models and continuously improve their accuracy and reliabili\nty.Communicate complex data-driven insights and forecasting results to both technical and non-technical stakeholde\nrs.Ensure high-quality data inputs for forecasting models by working with data engineering teams to improve data collection, transformation, and integration process\nes.Stay current with the latest advancements in forecasting methodologies, tools, and techniques, and apply them to enhance existing mode\nls.Provide thought leadership in forecasting and predictive analytics, driving innovation within the data science te\nam.Mentor junior team members and help establish best practices for forecasting across the organizati\non.\nQualificat\nions\nMaster's or Ph.D. in Data Science, Statistics, Economics, Applied Mathematics, or a related quantitative f\nield.4+ years of experience in developing and deploying forecasting models in a business environ\nment.Expertise in time series analysis and forecasting techniques, including ARIMA, SARIMA, Holt-Winters, and advanced machine learning methods like gradient boosting, neural networks (e.g., LSTM, GRU),\netc.Proficiency in programming languages such as Python or R, with strong experience in data manipulation, model building, and statistical anal\nysis.Hands-on experience with forecasting tools and libraries, such as Prophet, scikit-learn, TensorFlow, PyTorch,\netc.Strong understanding of data structures, data wrangling, and working with large data\nsets.Experience with cloud computing platforms (e.g., AWS, Azure, Google Cloud) and working in a distributed data environment is a\nplus.Excellent problem-solving skills and ability to translate complex data into actionable insi\nghts.Strong communication and presentation skills, with the ability to convey complex concepts to both technical and non-technical audie\nnces.Experience with visualization tools like Tableau, Power BI, or similar to present forecasting outc\nomes.Ability to work independently and collaboratively in a fast-paced environ\nment.\nPreferred Qualifica\ntions:Experience with demand forecasting, sales forecasting, or supply chain optimization in industries such as retail, manufacturing, or fi\nnance.Familiarity with statistical testing and experimentation (e.g., A/B tes\nting).Experience deploying models in production environments and collaborating with engineering teams to build scalable solu\ntions.Previous experience in a consulting or client-facing role is a\nplus.",
        "skills": [
            "scikit-learn",
            "SARIMA",
            "GRU",
            "R",
            "demand prediction",
            "time series forecasting",
            "LSTM",
            "gradient boosting",
            "Holt-Winters",
            "prophet",
            "Neural Networks",
            "Tableau",
            "Tensorflow",
            "Pytorch",
            "data structures",
            "Python",
            "Cloud Computing",
            "AWS",
            "data wrangling",
            "Data Manipulation",
            "Power Bi",
            "Model Building",
            "Google Cloud",
            "Arima",
            "Azure"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Zinnia",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who We Are\n\nZinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.\n\nWho You Are\n\nWe are seeking a highly motivated Senior Data Scientist with strong technical expertise, business acumen, and strategic problem-solving abilities. In this role, you will independently own and build forecasting models. You will work closely with stakeholders across Product, Data Engineering & Analytics, and Business Strategy to identify opportunities to resolve business problems. This is a high-impact individual contributor role.\n\nWhat You'll Do\n\nWork in a dynamic and innovative company to develop cutting-edge solutions.\nPerform advanced data analytics to influence decision makers.\nLeverage data from diverse sources to provide insights and build new data driven tools.\nPartner with engineering teams to continuously improve our data quality.\nDevelop machine learning based applications using supervised and unsupervised models using Python that optimize and personalize customer experiences or reduce manual effort on our internal teams through automated decision making.\nDevelop and support models to enable things such as prescriptive insights, automated decisioning, and insights.\nDevelop experiments to understand model impact, monitor live model analytics, and manage training and retraining pipelines.\nWork with stakeholders to intake complicated business problems and translate them into solvable data science projects.\nPartner with data engineering to take your model from development to deployed infrastructure.\nBrainstorm future use cases and contribute to the learning culture of the data science team.\nPartner with and mentor other data scientists and data analysts.\nPartner with multiple marketing teams, manage multiple projects and help conceptualize applications that directly drive company growth and strategy.\nYou will connect machine learning applications to business needs and help facilitate process changes based on algorithmic solution implementation.\n\nWhat You'll Need\n\nYou have Data Science experience building and validating machine learning and forecasting models in Python for a minimum of 5 years.\nYou love performing advanced analytics to find insights and patterns.\nYou have experience in supervised and unsupervised model techniques such as random forest, gradient boosting, support vector machines, k-means and hierarchical clustering, causal models, mixture models and experience in advanced modeling techniques such as reinforcement learning, neural networks, and natural language modeling.\nYou have experience in delivering natural language projects utilizing techniques such as text summarization, topic modeling, entity extraction, semantic encoding, and valence analysis\nYou have experience working in an agile business setting.\nYou have experience with relational cloud databases like BigQuery and Snowflake and are comfortable working with unstructured datasets such as unstructured text.\n\nWHAT'S IN IT FOR YOU\n\nAt Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability.",
        "skills": [
            "Entity Extraction",
            "Unsupervised Models",
            "Hierarchical Clustering",
            "Causal Models",
            "Supervised Models",
            "Text Summarization",
            "Mixture Models",
            "Gradient Boosting",
            "reinforcement learning",
            "Support Vector Machines",
            "Topic Modeling",
            "Valence Analysis",
            "Semantic Encoding",
            "snowflake",
            "k-means",
            "BigQuery",
            "Machine Learning",
            "Neural Networks",
            "Random Forest",
            "Python"
        ]
    },
    {
        "job_title": "Mid-Level Data Scientist",
        "company_name": "neurIOT Labs",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Location:Noida\nType:Full-Time\nExperience Level:25 years\nIndustry:Artificial Intelligence, Machine Learning, Data Science\nAbout the Role\nWe are looking for a self-motivatedMid-Level Data Scientistto join our AI team focused onGenAIapplications. We work at the intersection of multi-modal modeling, Retrieval-Augmented Generation (RAG), and real-time machine learning systems. You'll collaborate with a high-impact team to design, prototype, and deploy next-generation AI solutions, especially around document understanding and multi-modal tasks.\nKey Responsibilities\nDesign and implement state-of-the-art GenAI solutions, involving multi-modal, document understanding models and agents.\nBuild and optimizeRAG pipelines, including knowledge of various RAG architectures.\nDevelop and maintainagentic workflowsusing tools likeLangGraph, LangChain.\nWork with large-scale datasets and ensure efficient data processing pipelines.\nPerform statistical analysis, algorithm development, and performance tuning.\nWorking with opensource LLMs and deploying them on serving frameworks such as sglang and vllm.\nStay up to date with the latest developments in GenAI and ML, and actively contribute to knowledge sharing.\nRequired Qualifications\nBachelor's degree (Master's preferred) in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 3 years of experience working in machine learning, data science, or AI roles.\nStrong command ofPythonand familiarity withRor other scripting languages.\nHands-on experience withdeep learning,transformer-based models, andmulti-modal learning.\nProficiency in AI/ML frameworks and libraries (e.g., PyTorch, TensorFlow, Hugging Face Transformers).\nStrong understanding of statistics, linear algebra, and probability theory.\nExperience working withcloud environments, preferablyAzure.\nExposure toOpenAI,Anthropic,Mistral, or similar APIs and deployment ofopen-source models(LLaMA, MPT, etc.).\nDemonstrated experience indocument AI,vision-language models, orOCR-based understanding systems.\nPreferred Skills\nExperience withLangGraph,CrewAI,Autogen, or similar orchestration frameworks.\nWorking knowledge ofvector databases(e.g., Qdrant, Weaviate, Pinecone) andembedding search techniques.\nExposure toKubernetes,Docker, orML model deployment workflows.\nCuriosity-driven mindset with a passion for learning and experimenting with the latest in AI research.\nWhy Join Us\nBe part of a team working on powerful AI applications\nAccess to cutting-edge tools and open models\nFlexible working hours\nSupportive environment that encourages innovation, research, and upskilling",
        "skills": [
            "embedding search techniques",
            "Hugging Face Transformers",
            "Qdrant",
            "vision-language models",
            "vector databases",
            "Pinecone",
            "Autogen",
            "LangGraph",
            "R",
            "Mistral",
            "multi-modal learning",
            "CrewAI",
            "Anthropic",
            "OCR-based understanding systems",
            "cloud environments",
            "OpenAI",
            "document AI",
            "Weaviate",
            "transformer-based models",
            "Tensorflow",
            "Deep Learning",
            "Pytorch",
            "Docker",
            "Python",
            "Azure",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Senior Data Scientist (Gen AI)",
        "company_name": "Blend",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Description\n\nBlend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n\nJob Description\n\nBlend is hiring a Senior Data Scientist (Generative AI) to spearhead the development of advanced AI-powered classification and matching systems on Databricks. You will contribute to flagship programs like the Diageo AI POC by building RAG pipelines, deploying agentic AI workflows, and scaling LLM-based solutions for high-precision entity matching and MDM modernization.\n\nKey Responsibilities\n\nDesign and implement end-to-end AI pipelines for product classification, fuzzy matching, and deduplication using LLMs, RAG, and Databricks-native workflows.\nDevelop scalable, reproducible AI solutions within Databricks notebooks and job clusters, leveraging Delta Lake, MLflow, and Unity Catalog.\nEngineer Retrieval-Augmented Generation (RAG) workflows using vector search and integrate with Python-based matching logic.\nBuild agent-based automation pipelines (rule-driven + GenAI agents) for anomaly detection, compliance validation, and harmonization logic.\nImplement explainability, audit trails, and governance-first AI workflows aligned with enterprise-grade MDM needs.\nCollaborate with data engineers, BI teams, and product owners to integrate GenAI outputs into downstream systems.\nContribute to modular system design and documentation for long-term scalability and maintainability.\n\nQualifications\n\nBachelor's/Master's in Computer Science, Artificial Intelligence, or related field.\n5+ years of overall Data Science experience with 2+ years in Generative AI / LLM-based applications.\nDeep experience with Databricks ecosystem: Delta Lake, MLflow, DBFS, Databricks Jobs & Workflows.\nStrong Python and PySpark skills with ability to build scalable data pipelines and AI workflows in Databricks.\nExperience with LLMs (e.g., OpenAI, LLaMA, Mistral) and frameworks like LangChain or LlamaIndex.\nWorking knowledge of vector databases (e.g., FAISS, Chroma) and prompt engineering for classification/retrieval.\nExposure to MDM platforms (e.g., Stibo STEP) and familiarity with data harmonization challenges.\nExperience with explainability frameworks (e.g., SHAP, LIME) and AI audit tooling.\n\nPreferred Skills\n\nKnowledge of agentic AI architectures and multi-agent orchestration.\nFamiliarity with Azure Data Hub and enterprise data ingestion frameworks.\nUnderstanding of data governance, lineage, and regulatory compliance in AI systems.\n\nAdditional Information\n\nThrive & Grow with Us:\n\nCompetitive Salary: Your skills and contributions are highly valued here, and we make sure your salary reflects that, rewarding you fairly for the knowledge and experience you bring to the table.\nDynamic Career Growth: Our vibrant environment offers you the opportunity to grow rapidly, providing the right tools, mentorship, and experiences to fast-track your career.\nIdea Tanks: Innovation lives here. Our Idea Tanks are your playground to pitch, experiment, and collaborate on ideas that can shape the future.\nGrowth Chats: Dive into our casual Growth Chats where you can learn from the bestwhether it's over lunch or during a laid-back session with peers, it's the perfect space to grow your skills.\nSnack Zone: Stay fuelled and inspired! In our Snack Zone, you'll find a variety of snacks to keep your energy high and ideas flowing.\nRecognition & Rewards: We believe great work deserves to be recognized. Expect regular Hive-Fives, shoutouts and the chance to see your ideas come to life as part of our reward program.\nFuel Your Growth Journey with Certifications: We're all about your growth groove! Level up your skills with our support as we cover the cost of your certifications.",
        "skills": [
            "LangChain",
            "SHAP",
            "LLMs",
            "LLaMA",
            "MLflow",
            "Chroma",
            "LIME",
            "FAISS",
            "Mistral",
            "Delta Lake",
            "OpenAI",
            "LlamaIndex",
            "Pyspark",
            "Databricks",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Insight Global",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "***$10-$20/hr USD***\nMust haves:\n5+ years in data science, statistical modeling/ machine learning\nStrong background in statistics and Machine learning models (Clustering, Market basket/apriori, supervised machine learning, Keras, boosting and bagging)\nWell versed in SQL, Python\nExperience with Keras, Tensorflow deep learning models (To predict price impact on product demands)\nKnowledge on Optimizations (Dual Annealing/ Mixed Objective/ Goal based optimization)\nExperience working on large data sets, linear/ non-linear optimizations, ML applications\nPluses:\nPricing analytics and price elasticity\nExperience with Deep Learning Framework\nDay to day:\nAn employer is looking for a Data Scientist to sit remotely. You will primarily be tasked with analyzing data and generate analytical outputs to deliver insights, evaluate hypothesis and complete root cause analysis of the business problem. The focus for this team will be AI based solutions to determine consumer insights and marketing performance analytics to help brands understand consumer behavior for promotions and pricing decisions. You will design end to end solutions by leveraging the latest ML techniques, including data preparation, exploratory data analysis, feature engineering, model selection, model development, and deployment. Additional tasks will include:\nSolve business problems which involve:\nBrainstorm with clients and internal teams to define a problem\nTranslate the business problem into an analytical problem\nIdentify internal and external data requirements for solving the analytical problem\nSolving the analytical problem using concepts from mathematics, statistics, Artificial Intelligence and Machine learning\nCreate, maintain and enhance artefacts that can help communicate the solution to clients like dashboards, power point decks, excel sheets etc.\nDevelop algorithms based on the problem statement & domain area. Design & develop new algorithms for optimization, recommendation, attribution, and emerging real-time consumer analytics.\nEstablish mathematical models to represent specific business functions or consumer behavior.\nProgramming of algorithms, and integration into the data science framework.\nProvide Support and add new features in a marketing performance analytics automation platform. Work on the scalability and performance of the automation platform.\nCreate proof of concepts to vet out new technologies.\nInfluence design and architecture principles for new technology products.",
        "skills": [
            "Statistical Modeling",
            "linear optimizations",
            "Boosting",
            "Mixed Objective Goal based optimization",
            "bagging",
            "Dual Annealing",
            "optimizations",
            "non-linear optimizations",
            "Keras",
            "Sql",
            "Tensorflow",
            "Deep Learning",
            "Machine Learning",
            "Data Science",
            "Python"
        ]
    },
    {
        "job_title": "Global Data Scientist - Manager",
        "company_name": "Boston Consulting Group (BCG)",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nBCG is on an ambitious and accelerated AI journey, leveraging the power of Generative AI to radically transform the speed and scale of business delivery and growth. At the heart of this transformation is a commitment to rapidly developing and deploying cutting-edge GenAI solutions that unlock new levels of efficiency and innovation across the enterprise.\n\nThis role will drive the development of high impact prioritized Generative AI solutions within the Legal Product Portfolio, accelerating the realization of our ambitious functional AI and GenAI vision.\n\nKey Responsibilities\n\nDevelop AI/Gen AI projects as individual coding contributors (IC), from ideation to ensuring they deliver impactful insights\nDrive end-to-end design, development & deployment of Gen AI solutions at scale to deliver high-quality outcomes\nFine-tune large language models (LLMs) to meet specific Generative AI business outcomes\nExtract and analyze data from various sources to inform modeling and solution development.\nAnalyze large and complex datasets to extract meaningful trends, patterns, and insights\nShare expertise and perspectives on alternate approaches & tools to improve end solution\nCollaborate closely with data engineers, LLM engineers, Full stack AI engineers, QA, product owners and analysts to create and operationalize data & AI pipelines, ensuring compliance with data governance. security and responsible AI requirements\nBuild strong relationships with business stakeholders, ensuring alignment on project goals and successful integration of data-driven insights into business strategies\n\nWhat You'll Bring\n\nAI/ML & LLM Engineering Skills\n\nOverall 7+ years of experience in technology, software engineering or data science/ML\n5+ years of hands-on coding experience in python development\n2+ years of experience in building Generative AI products at scale\nDemonstrated experience in developing Generative AI based solutions using frameworks (e.g. Langchain, Llamaindex, etc.) to build retrieval augmented generation (RAG), multi-agent-architecture driven solutions\nHighly curious and fast learner who is keenly keeping pace with latest and greatest happening in Gen AI and agentic AI space\nExperience in working with variety of LLM models and fine tuning\nExperience in applying MLOps/LLMOps principles to deploy, monitor models and implement automated CI/CD pipelines\nExperience with LLM evaluation frameworks, guardrails to evaluate and monitor outputs quality, control hallucinations\nAwareness of ethical AI frameworks to ensure responsible AI deployment\nAbility to work iteratively on AI/ML/Gen AI solutions to improve performance over time and in response to user feedback\nExperience in deploying solutions on a cloud platforms AWS (preferred), Azure, GCP\nExperience with microservices & serverless architecture, API development\nAn MVP mindset that emphasizes rapid prototyping and continuous improvement.\n\nCommunication, Interpersonal And Teaming Skills\n\nExcellent communication skills, with the ability to explain complex technical concepts to various audiences\nExperience working in a fast-paced global company, with a diverse team of both technical and functional backgrounds located in multiple time zones\nProven ability to operate with a transparent mindset, communicating openly with stakeholders at various levels of the organization\n\nWho You'll Work With\n\nJoin a dynamic, cross-functional team of professionals from diverse backgrounds. At BCG, we value drive, curiosity, and collaborative leadership. You'll tackle challenging projects that address complex business problems, working closely with sharp, analytical stakeholders who make decisions with a global outlook\n\nAdditional info\n\nYOU'RE GOOD AT\n\nDriving rapid delivery through an MVP approach, enabling fast prototyping, iterative development, and impactful features.\nProblem-solving and innovating to overcome technical challenges, learning new tools quickly, and adapting to a fast-paced environment.\nWorking independently with minimal direction, showing strong initiative and drive.\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.",
        "skills": [
            "Generative AI",
            "Langchain",
            "LLMOps",
            "Llamaindex",
            "Ethical AI Frameworks",
            "MLops",
            "Data Governance",
            "Python",
            "Api Development",
            "Microservices"
        ]
    },
    {
        "job_title": "Global Data Scientist - Manager",
        "company_name": "Boston Consulting Group (BCG)",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nBCG is on an ambitious and accelerated AI journey, leveraging the power of Generative AI to radically transform the speed and scale of business delivery and growth. At the heart of this transformation is a commitment to rapidly developing and deploying cutting-edge GenAI solutions that unlock new levels of efficiency and innovation across the enterprise.\n\nThis role will drive the development of high impact prioritized Generative AI solutions within the Legal Product Portfolio, accelerating the realization of our ambitious functional AI and GenAI vision.\n\nKey Responsibilities\n\nDevelop AI/Gen AI projects as individual coding contributors (IC), from ideation to ensuring they deliver impactful insights\nDrive end-to-end design, development & deployment of Gen AI solutions at scale to deliver high-quality outcomes\nFine-tune large language models (LLMs) to meet specific Generative AI business outcomes\nExtract and analyze data from various sources to inform modeling and solution development.\nAnalyze large and complex datasets to extract meaningful trends, patterns, and insights\nShare expertise and perspectives on alternate approaches & tools to improve end solution\nCollaborate closely with data engineers, LLM engineers, Full stack AI engineers, QA, product owners and analysts to create and operationalize data & AI pipelines, ensuring compliance with data governance. security and responsible AI requirements\nBuild strong relationships with business stakeholders, ensuring alignment on project goals and successful integration of data-driven insights into business strategies\n\nWhat You'll Bring\n\nAI/ML & LLM Engineering Skills\n\nOverall 7+ years of experience in technology, software engineering or data science/ML\n5+ years of hands-on coding experience in python development\n2+ years of experience in building Generative AI products at scale\nDemonstrated experience in developing Generative AI based solutions using frameworks (e.g. Langchain, Llamaindex, etc.) to build retrieval augmented generation (RAG), multi-agent-architecture driven solutions\nHighly curious and fast learner who is keenly keeping pace with latest and greatest happening in Gen AI and agentic AI space\nExperience in working with variety of LLM models and fine tuning\nExperience in applying MLOps/LLMOps principles to deploy, monitor models and implement automated CI/CD pipelines\nExperience with LLM evaluation frameworks, guardrails to evaluate and monitor outputs quality, control hallucinations\nAwareness of ethical AI frameworks to ensure responsible AI deployment\nAbility to work iteratively on AI/ML/Gen AI solutions to improve performance over time and in response to user feedback\nExperience in deploying solutions on a cloud platforms AWS (preferred), Azure, GCP\nExperience with microservices & serverless architecture, API development\nAn MVP mindset that emphasizes rapid prototyping and continuous improvement.\n\nCommunication, Interpersonal And Teaming Skills\n\nExcellent communication skills, with the ability to explain complex technical concepts to various audiences\nExperience working in a fast-paced global company, with a diverse team of both technical and functional backgrounds located in multiple time zones\nProven ability to operate with a transparent mindset, communicating openly with stakeholders at various levels of the organization\n\nWho You'll Work With\n\nJoin a dynamic, cross-functional team of professionals from diverse backgrounds. At BCG, we value drive, curiosity, and collaborative leadership. You'll tackle challenging projects that address complex business problems, working closely with sharp, analytical stakeholders who make decisions with a global outlook\n\nAdditional info\n\nYOU'RE GOOD AT\n\nDriving rapid delivery through an MVP approach, enabling fast prototyping, iterative development, and impactful features.\nProblem-solving and innovating to overcome technical challenges, learning new tools quickly, and adapting to a fast-paced environment.\nWorking independently with minimal direction, showing strong initiative and drive.\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.",
        "skills": [
            "Generative AI",
            "LLMOps",
            "Serverless Architecture",
            "Ethical AI frameworks",
            "MLops",
            "Api Development",
            "Python development",
            "Data Governance",
            "Microservices"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Qualys",
        "experience": "2-5 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!\n\nJob Description\n\nWe are seeking a Data Scientist to develop next-generation Security Analytics products. You will work closely with engineers and product managers to prototype, design, develop, and optimize data-driven security solutions.\n\nAs a Data Scientist, you will focus on consolidating and analyzing diverse data sources to extract meaningful insights that drive product innovation and process optimization. The ideal candidate has a strong background in quantitative analysis, machine learning, and data-driven decision-making, with experience handling large datasets.\n\nResponsibilities:\n\nConduct exploratory data analysis (EDA) and apply statistical methods to extract actionable insights from complex datasets.\nCollaborate with Product Management and cross-functional stakeholders to define problem statements, develop solution strategies, and design scalable ML systems.\nDesign, develop, and deploy Machine Learning models, including traditional ML, Deep Learning, and Large Language Models (LLMs), to drive business impact.\nCreate insightful data visualizations, technical reports, and presentations to communicate findings to technical and non-technical audiences.\nDeploy ML models in production and implement monitoring frameworks to ensure model performance, stability, and continuous improvement.\n\nRequirements:\n\n\n2-5 years of experience in Machine Learning projects, including model development, deployment, and optimization.\nBS, MS, or Ph.D. in Computer Science, Statistics, or a related field.\nStrong communication, problem-solving, and analytical skills; a collaborative team player.\nDeep understanding of ML algorithms, their mathematical foundations, and real-world trade-offs.\nExpertise in at least two or more ML domains, including Linear Models, Tree-based Models, Traditional NLP, LLMs, Numerical optimization and Reinforcement Learning.\nHands-on experience with ML frameworks such as Scikit-Learn, TensorFlow, PyTorch, LangChain etc.\nStrong programming skills in Python and/or Java, R, Go, Scala.\nExperience in SQL, Pandas, and PySpark for efficient data manipulation.\nFamiliarity with microservice architectures, CI/CD, MLOps best practices.\n\nNice to Have:\n\n\nGenerative AI: Experience with fine-tuning and optimizing LLMs.\nFamiliarity with distributed computing frameworks such as Hadoop, Spark, and OpenSearch.\nPublished research in AI/ML in peer-reviewed journals or top conferences (e.g., NeurIPS, ICML, CVPR).\nPrior experience applying AI/ML to cybersecurity use cases.\nBasic proficiency in Unix/Linux environments for scripting and automation.",
        "skills": [
            "LangChain",
            "microservice architectures",
            "Go",
            "Scikit-Learn",
            "R",
            "Java",
            "Machine Learning",
            "Pyspark",
            "Scala",
            "Sql",
            "Deep Learning",
            "Tensorflow",
            "Pandas",
            "Pytorch",
            "MLops",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Haber",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Role:\nAt Haber, we are redefining how industries operate by blending automation with intelligent decision making. As a Data Scientist, you will play a pivotal role in designing machine learning systems that power our suite of innovative productsincluding Elixa, Kaiznn, Mount Fuji, and upcoming platforms like our Fiber Morphology solution. You'll work extensively with time-series data, sensor feeds, and computer vision pipelines to uncover actionable insights in real-time. This is a high-impact, high-ownership role where your work will directly influence operational efficiency and sustainability across complex industrial environments. We value independent thinkers who are not just technical experts but also strategic problem-solvers.\nKey Responsibilities:\nLead the full machine learning system development lifecycle: data collection, cleaning, feature engineering, modeling, deployment, and evaluation\nWork with time-series sensor data and computer vision systems to build predictive and anomaly detection models\nDesign and develop scalable recommendation platforms for real-time industrial decision-making Apply machine learning, data mining, and statistical techniques (e.g., regression, clustering, collaborative filtering, PCA) to a wide range of problems\nDesign novel approaches to large-scale data analysis, including semi-structured and unstructured data\nCollaborate with engineering, QA, product, and operations teams to deliver end-to-end solutions Contribute to and stay up-to-date with research in areas such as machine learning, signal processing, and domain-specific optimization\nMake independent technical decisions and guide strategic direction based on data insights Mentor junior team members and foster a collaborative learning culture\nQualification:\nHaving 3-6 years of experience as a Data Scientist, preferably in a client facing role taking complete ownership of the Data Science lifecycle\nStrong programming skills in Python or R\nProven experience working with time-series data and real-time data pipelines\nSolid foundation in supervised and unsupervised ML methods\nExposure to sensor data, image processing, or computer vision techniques is a strong plus Experience with cloud environments, preferably AWS, for model deployment and monitoring Sound understanding of statistics, optimization, and data visualization\nAbility to work independently and drive projects from idea to execution\nStrong communication skills and the ability to collaborate across multidisciplinary teams\nA passion for solving tough, real-world problems using data\nWhat We Offer:\nReal-World Impact: Your models will be deployed in operational environments, not just experimental labs\nOwnership & Autonomy: Freedom to explore, decide, and build in a high-trust environment Diverse Tech Stack: Work on ML pipelines that combine time-series, sensor, and vision data\nFast-Track Learning: Be part of a startup where learning never stops and innovation is constant Collaborative Culture: A team that supports initiative, experimentation, and personal growth Product-Minded Thinking: Your work will be tightly integrated with product and business decisions",
        "skills": [
            "supervised ML methods",
            "cloud environments",
            "time-series data",
            "real-time data pipelines",
            "R",
            "sensor data",
            "Statistical Techniques",
            "Statistics",
            "Optimization",
            "unsupervised ML methods",
            "Image Processing",
            "Data Visualization",
            "Machine Learning",
            "AWS",
            "data mining",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Risk Analytics Data Scientist",
        "company_name": "PayPal",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWe are seeking a highly motivated and experienced Risk Analytics professional to join our Enterprise Risk Management Organization.\n\nThis role is responsible for providing risk analytics support to Financial and Model Risk oversight groups, including but not limited to Credit, Collection, Fraud, Seller, Acquiring, Treasury oversight. This role connects the dots between business, model, and data by providing insights through in-depth data analysis. This role also provides critical support in ad hoc analysis required for business enablement and new risk initiatives to support PP's growth.\n\nThis role is part of the broader AI/Model/Quant Risk Oversight function.\nThe AI/Model/Quant Risk Oversight team:\n. Oversees and proactively challenges our model use and AI application.\n. Supports effective model development/deployment and responsible AI innovation\n. Ensure model/AI practice meets regulatory compliance requirement, strengthening decision-making, protecting customers and safeguarding the company from financial and reputational damage\n. Partners with financial risk oversight groups in conducting in-depth, thematic reviews of various products and risk functions, providing data insights and expertise around strategy development and risk mitigations\nJob Description:\nYou will make an impact by ensuring that\nSupport Credit & Collection, Fraud, Seller, Merchant acquiring, Treasury oversight with data insights to enable effective challenges\nProvide critical support in ad hoc analysis required for business enablement and risk initiatives\nLeverage AI to enhance risk analytics functionality to bring out more data insights\nContribute to the major uplift of data infrastructure and other requirement per PPEU ECB supervision readiness project\nIn your day-to-day role you will\nOversee and challenge risk analytics practices across various groups within the company, primarily focused on Fraud, Seller and Credit risks, including use of machine learning and advanced analytics to detect and mitigate risks\nConduct root cause analysis to identify opportunities and gaps in existing strategies and controls.\nDevelop and analyze multiple reports and dashboards, looking at key risk indicators and relevant business metrics to uncover emerging trends and areas of opportunities\nPartner with financial risk oversight groups in conducting in-depth, thematic reviews of various products and risk functions, providing data insights and expertise around strategy development and analytics.\nEducation:\nA master's or advanced degree in a quantitative field such as statistics, mathematics, finance, or economics.\nExperience:\nGood understanding of financial and banking risk\nRobust coding skills for big data and AI systems\nAble to builds relationships across multiple teams and at various levels\nHas a flexible and curious mindset. Understands the business needs and adapts risk controls accordingly.\nExceptional communication, writing, and presentation skills to convey technical concepts to stakeholders at all seniority levels (executives, regulators, auditors, etc.).\nAbility to work effectively both independently and collaboratively in a high-pressure, fast-paced environment\nExperience in risk (Model, Fraud, Credit, Market, Liquidity, etc.) or similar highly quantitative roles is a plus\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\n-\nPayPal is committed to fair and equitable compensation practices.\nActual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.\nThe total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit .\nThe US national annual pay range for this role is $116,500 to $173,250\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.",
        "skills": []
    },
    {
        "job_title": "Data Scientist",
        "company_name": "FREED",
        "experience": "Fresher",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Role\nYou'll work at the intersection of Generative AI, Machine Learning, and personal finance. You'll partner with Product, Engineering, and Ops to:\nDrive smarter decision-making using AI.\nImprove customer outcomes by personalising experiences.\nLaunch agentic AI systems to streamline sales, support, and credit evaluation.\nKey Responsibilities\nDesign, build, and deploy machine learning and generative AI models.\nTranslate real-world business problems into data science solutions from ideation to production.\nCollaborate with engineers to set up scalable data pipelines and APIs.\nConstantly monitor model performance and retrain as needed.\nBuild solutions that leverage LLMs for tasks like summarization, sentiment detection, classification, and recommendations.\nStay up-to-date with developments in GenAI and actively experiment with new techniques.\nRequired Qualifications\nStrong foundation in machine learning, feature engineering, and model deployment.\nHands-on experience implementing GenAI use cases (e.g., prompt engineering, RAG, embeddings).\nProficiency in Python, SQL, and ML libraries like scikit-learn, XGBoost, TensorFlow, or PyTorch.\nFamiliarity with LLM frameworks (LangChain, OpenAI, HuggingFace) and cloud services (AWS/GCP/Azure).\nDemonstrated ability to take AI models from notebook to production.\nExcellent communication skills and ability to collaborate cross-functionally.\nPreferred Qualification\nPrior experience in Fintech, BFSI, or working with credit and customer data.\nUnderstanding of ethical AI, data privacy, and model fairness.\nFamiliarity with customer-facing AI products (chatbots, agentic workflows).\nWhat We Offer\nA high-impact role at a company solving a real societal problem.\nThe opportunity to shape AI strategy and product direction from the ground up.\nA culture that values curiosity, ownership, and bold thinking.",
        "skills": [
            "LangChain",
            "scikit-learn",
            "generative AI",
            "HuggingFace",
            "OpenAI",
            "Machine Learning",
            "Sql",
            "Tensorflow",
            "Gcp",
            "Pytorch",
            "XGBoost",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Roche Pharmacutical Holding",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Pharmaceutical",
        "job_description": "At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nAs a Data Scientist you will join the data science cluster in the Roche Informatics Data and Analytics Chapter (DnA). You will be part of one or several multi-disciplinary agile teams where you'll actively shape the future of healthcare by using data science methods and principles to generate deeper insights from a great variety of data sources. To achieve this, you will proactively identify needs, design and implement analytical solutions, provide advice and consulting support to our key stakeholders and show impact by executing proof-of-value initiatives, or contributing to existing products.\nAs a Data Scientist you will:\nApply your expertise in NLP/LLM to develop and refine models that address Roche business needs. Involved in building and fine-tuning models and optimising their performance to provide valuable insights and solutions to business stakeholders\nSupport prioritisation efforts, understand feasibility and business impact, take smart risks to make informed decisions in a fast-paced, evolving environment to deliver patient benefits faster.\nCollaborate within global agile teams in the Roche Informatics business and foundational domains to develop products that provide the highest value to both Roche Pharma and Diagnostics business stakeholders.\nProvide methodical and implementation guidance as well as hands-on support around analytical LLM/NLP use cases. Evaluate the pros & cons of different NLP approaches and Generative AI platforms with comprehensive quantitative and qualitative analysis\nCommunicate findings and market the value of use cases to key stakeholders.\nContribute to positioning data science as a key competency within the enterprise\nContinuously look for opportunities to broaden knowledge, capabilities and skill set to enable talent to flow into different specialties.\nBe a role model for knowledge sharing within the DnA chapter.\nAct as a coach, mentor, or buddy to help colleagues grow and develop.\nQualifications\nM.Sc. or PhD in Computer Science, Physics, Statistics, Mathematics or equivalent degree and experience with machine learning/data mining/artificial intelligence.\nExperience of working as a hands-on data scientist in pharmaceutical industry is preferred.\nHands-on experience with Python programming and common NLP libraries (e.g., transformers, gensim, spaCy, etc.)\nFamiliarity with essential frameworks (e.g. PyTorch) and infrastructure components (Docker, GPU) for training, fine-tuning and evaluating NLP tasks\nExperience in using both open source (e.g. HuggingFace) and closed source LLM models with different deep learning architectures\nExperience implementing RAG, working with knowledge databases and using LLM through APIs\nGood knowledge of effective training and optimising language models to fit for internal infrastructure and ensure seamless integration\nFamiliarity with best practices for code generation, code documentation, data security, and compliance in cloud-based data science workflows.\nProven experience to add value and insight by providing advanced analytical solutions.\nData storytelling skills and using visualisation tools to communicate data and results with a non-technical audience.\nInternational, goal oriented mindset with can do attitude.\nFluency in written and spoken English.\nWho we are\nA healthier future drives us to innovate. Together, more than 100'000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet's build a healthier future, together.\nRoche is an Equal Opportunity Employer.",
        "skills": []
    },
    {
        "job_title": "DATA SCIENTIST",
        "company_name": "Ford Motor Company",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nProactively explore datasets to identify hidden patterns, emerging trends, and key insights\nEmploy data mining techniques to extract relevant features from raw data, creating new variables that improve the performance of analytical models and uncover hidden patterns.\nUse segmentation and clustering algorithms to identify distinct customer groups or market segments, enabling targeted marketing campaigns and personalized customer experiences.\nApply predictive modeling techniques to uncover patterns and trends, validate model performance, and translate complex modeling output into clear, actionable recommendations for business improvement.\nImplement rigorous data quality checks and validation processes to ensure the accuracy, completeness, and integrity of data used for analysis and reporting, maintaining the reliability of insights and recommendations.\nCreate BI/Visualization dashboards using Power BI/Qlik/other to visualize insights, trends and patterns\nCollaborate with technical stakeholders such as data architects, data modelers, enterprise architects, project managers, and platform architects to implement the large-scale data analytics and BI solution.\nDrive resolution of issues and ensure alignment across stakeholders, contributing to the overall data-driven culture of the organization.\n\nResponsibilities\n\n\nProactively explore datasets to identify hidden patterns, emerging trends, and key insights\nEmploy data mining techniques to extract relevant features from raw data, creating new variables that improve the performance of analytical models and uncover hidden patterns.\nUse segmentation and clustering algorithms to identify distinct customer groups or market segments, enabling targeted marketing campaigns and personalized customer experiences.\nApply predictive modeling techniques to uncover patterns and trends, validate model performance, and translate complex modeling output into clear, actionable recommendations for business improvement.\nImplement rigorous data quality checks and validation processes to ensure the accuracy, completeness, and integrity of data used for analysis and reporting, maintaining the reliability of insights and recommendations.\nCreate BI/Visualization dashboards using Power BI/Qlik/other to visualize insights, trends and patterns\nCollaborate with technical stakeholders such as data architects, data modelers, enterprise architects, project managers, and platform architects to implement the large-scale data analytics and BI solution.\nDrive resolution of issues and ensure alignment across stakeholders, contributing to the overall data-driven culture of the organization.\n\nQualifications\n\n\nBachelor's degree in Data Science, Data Analytics, Computer Science, Statistics, Economics or a related field.\n3+ years writing complex SQL Queries, Python, R, Dynamic queries, and database objects such as Stored Procedures, Functions, Packages, and triggers as well as working with large data sets.\n2+ years experience in Google Big Query and Google Cloud Platform.\n1+ year experience on BI solutions using Power BI/Looker /Looker Studio/Qlik\nExperience in engaging in discussions focused on analytics implementation, customization, and automation of data collection processes.\nExcellent problem-solving and analytical skills.\nDetailed oriented with strong communication and collaboration skills.\nAbility to work independently and as part of a team.\nMust be able to manage time effectively within a complex environment, and always remain composed in the event of conflicting priorities.",
        "skills": [
            "R",
            "Looker Studio",
            "Looker",
            "Google Big Query",
            "Google Cloud Platform",
            "Power Bi",
            "Qlik",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist ML",
        "company_name": "Team Geek Solutions",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nTeam Geek Solutions is a forward-thinking technology firm dedicated to empowering businesses through innovative data solutions. Our mission is to harness the power of data analytics and machine learning to drive strategic insights and informed decision-making. We pride ourselves on our collaborative culture, where diverse talents come together to solve complex problems and deliver exceptional value to our clients.\n\nKey Responsibilities\n\nExtract and analyze data from company databases to drive the optimization and\n\nenhancement of product development and marketing strategies.\n\nAnalyze large datasets to uncover trends, patterns, and insights that can influence business decisions.\n\nLeverage predictive and AI/ML modeling techniques to enhance and optimize\n\ncustomer experience, boost revenue generation, improve ad targeting, and more.\n\nDesign, implement, and optimize machine learning models for a wide range of applications such as predictive analytics, natural language processing, recommendation systems, and more.\n\nConduct experiments to fine-tune machine learning models and evaluate their performance using appropriate metrics.\n\nDeploy machine learning models into production environments, ensuring scalability\n\nQualifications\n\nBachelor's, Master's or Ph.D in Computer Science, Data Science, Mathematics, Statistics, or a related field.\n\n2+ years of experience in Analytics, Machine learning, Deep learning.\n\nProficiency in programming languages such as Python, and familiarity with machine learning libraries (e.g., Numpy, Pandas, TensorFlow, Keras, PyTorch, Scikit-learn).\n\nStrong experience with data wrangling, cleaning, and transforming raw data into structured, usable formats.\n\nHands-on experience in developing, training, and deploying machine learning models for various applications (e.g., predictive analytics, recommendation systems, anomaly detection).\n\nIn-depth understanding of machine learning algorithms (supervised, unsupervised,\n\nreinforcement learning) and their appropriate use cases.\n\nExperience with model evaluation techniques (e.g., cross-validation, A/B testing,\n\nperformance metrics).\n\nExperience with cloud platforms (AWS, GCP, Azure) for model deployment and scalal Proficiency in data processing and manipulation techniques.\n\nExcellent problem-solving skills and the ability to work effectively in a collabor environment.\n\nStrong communication skills to convey complex technical concepts to non-tech stakeholders.\n\nGood To Have\n\nExperience in the [banking/financial services/industry-specific] sector.\n\nFamiliarity with cloud-based machine learning platforms such as Azure, AWS, or GCP.\n\nSkills: natural language processing,numpy,azure,data wrangling,tensorflow,data analysis,data science,big data technologies,pandas,pytorch,cloud platforms,gcp,python,model evaluation,recommendation systems,keras,scikit-learn,data scientist,data visualization,interpersonal skills,aws,predictive analytics,statistical modeling,machine learning,deep learning,model deployment",
        "skills": [
            "Scikit-learn",
            "Model Evaluation",
            "Recommendation Systems",
            "Machine Learning",
            "Natural Language Processing",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Pytorch",
            "Gcp",
            "Pandas",
            "data wrangling",
            "Keras",
            "Predictive Analytics",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Cognida.ai",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Cognida.ai\nOur Purpose is to boost your competitive advantage using AI and Analytics.\nWe Deliver tangible business impact with data-driven insights powered by AI. Drive revenue growth, increase profitability and improve operational efficiencies.\nWe Are technologists with keen business acumen - Forever curious, always on the front lines of technological advancements. Applying our latest learnings, and tools to solve your everyday business challenges.\nWe Believe the power of AI should not be the exclusive preserve of the few. Every business, regardless of its size or sector deserves the opportunity to harness the power of AI to make better decisions and drive business value.\nWe See a world where our AI and Analytics solutions democratise decision intelligence for all businesses. With Cognida.ai, our motto is No enterprise left behind.\nOverview\nWe are seeking a Senior Data Scientist who balances exceptional technical expertise with strong business acumen. The ideal candidate will be equally proficient in technical data analysis using SQL and Python, and in understanding business contexts to deliver actionable insights that drive strategic decisions.\nKey Responsibilities\nPartner with business stakeholders to understand objectives and translate them into analytical frameworks\nDesign and execute analyses that directly address business challenges using SQL, Python, and other analytical tools\nDevelop data-driven recommendations that align with organizational goals and strategies\nCommunicate complex findings in business terms that resonate with executives and decision-makers\nIdentify business opportunities through proactive data exploration and analysis\nBuild and validate statistical models that solve real business problems\nInfluence product and business strategies through data-backed insights\nServe as a bridge between technical and business teams, effectively translating between domains\nTechnical Skills\nAdvanced SQL proficiency for complex data querying and manipulation\nStrong Python programming skills with focus on data analysis libraries\nExperience building and deploying statistical and machine learning models\nData visualization and dashboard creation for business audiences\nData cleaning, transformation, and feature engineering expertise\nBusiness Skills\nDemonstrated ability to understand business objectives and translate them into analytical questions\nExperience working directly with business stakeholders to define key metrics and success criteria\nSkill in presenting technical findings in business terms and recommending actionable strategies\nUnderstanding of business operations, market dynamics, and competitive landscapes\nAbility to prioritize analytical work based on business impact\nQualifications\n8+ years experience in data science with demonstrated impact on business outcomes\nBachelor's degree required; advanced degree in quantitative field preferred\nTrack record of successful projects that combine technical excellence with business value\nExperience in collaborative environments working across business and technical teams\nPersonal Attributes\nCurious mindset that questions assumptions and explores beyond the obvious\nStrong communicator who can adapt message to different audiences\nStrategic thinker who connects data insights to broader business context\nSelf-directed learner who stays current in both technical and business domains\nComfortable navigating ambiguity and driving clarity through data",
        "skills": [
            "Dashboard creation",
            "Statistical models",
            "Machine learning models",
            "Feature engineering",
            "Data Visualization",
            "Data Cleaning",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Art Technology and Software",
        "experience": "5-7 Years",
        "salary": null,
        "location": "India, Cochin / Kochi / Ernakulam",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description: Data Scientist\nMode of working Remote Position\nCompany Location: Kochi,Kerala (Infopark)\nOverview:\nWe are looking for a skilled and driven Data Scientist with 5-7 years of experience in data analysis and predictive modeling. The ideal candidate will have a strong technical foundation, analytical problem-solving skills, and the ability to translate complex data insights into actionable business strategies.\nKey Responsibilities\nAnalyze large and complex datasets to uncover meaningful patterns, trends, and insights.\nBuild, deploy, and validate predictive models, leveraging machine learning techniques and deep learning frameworks.\nDevelop and implement Generative AI models to solve business challenges.\nPerform data preprocessing, exploratory data analysis (EDA), and feature engineering (FE) to prepare data for modeling and optimization.\nWrite efficient and scalable code in Python, R, and SQL to manage and manipulate data.\nCollaborate with cross-functional teams to generate actionable insights that support business decision-making.\nCreate clear and impactful data visualizations to communicate findings to both technical and non-technical stakeholders.\nContinuously monitor and improve model performance using optimization techniques.\nStay up-to-date with advancements in data science, machine learning, and AI technologies.\nRequired Skills and Qualifications :\nExperience:\n5-7 years of hands-on experience in data analysis, predictive modeling, and advanced analytics.\nTechnical Proficiency:\no Strong programming skills in Python, R, and SQL.\no In-depth knowledge of machine learning techniques, deep learning frameworks, and Generative AI.\no Proficiency in data handling, pre-processing, exploratory data analysis (EDA), feature engineering (FE), and optimization techniques.\nAnalytical Skills:\no Strong problem-solving abilities and attention to detail.\no Capability to derive actionable insights and present findings in a business-friendly manner.\nCommunication:\nExcellent verbal and written communication skills to convey complex technical concepts effectively.\nTools & Technologies:\nFamiliarity with data visualization tools (e.g., Tableau, Power BI, Matplotlib, or Seaborn) is a plus.\nEducational Background\nBachelor's or Master's degree in Data Science, Computer Science, Statistics, Mathematics, or a related field.\nWhat We Offer\nA collaborative and innovative work environment.\nOpportunities to work on cutting-edge technologies, including Generative AI and advanced machine learning techniques.\nCareer growth and development opportunities through training and mentorship.\nIf you are passionate about leveraging data to drive impactful business decisions and have a strong technical background,\nwe would love to hear from you!",
        "skills": [
            "Data Preprocessing",
            "R",
            "Optimization Techniques",
            "Generative AI",
            "Sql",
            "Deep Learning",
            "Python",
            "Machine Learning"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "tax.com",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Why Ryan\n\nGlobal Award-Winning Culture\nFlexible Work Environment\nGenerous Paid Time Off\nWorld-Class Benefits and Compensation\nRapid Growth Opportunities\nCompany Sponsored Two-Way Transportation\nExponential Career Growth\n\nThe Lead Data Scientist is responsible for providing technical leadership in Data Science & AI/ML initiatives along with data-related application development leveraging cloud-based solutions. This role requires an enterprise mindset to build out robust, high-performance solutions and the ability to clearly communicate technical details within and outside of the team.\n\nDuties And Responsibilities, Aligned With Key Results\n\nPeople\n\nUse a variety of programming languages and tools to design, develop, test, and maintain Data Science & AI/ML solutions within the Platform Reference Architecture with a focus on Python, AWS and/or Azure and using best practices.\nWorking directly with management, product teams and practice personnel to understand their platform data requirements.\nMaintaining a positive work atmosphere by behaving and communicating in a manner that encourages productive interactions with customers, co-workers and supervisors.\nDeveloping and engaging with team members by creating a motivating work environment that recognizes, holds team members accountable, and rewards strong performance.\nFostering an innovative, inclusive and diverse team environment, promoting positive team culture, encouraging collaboration and self-organization while delivering high quality solutions\nClient\n\nCollaborating on an Agile team to design, develop, test, implement and support highly scalable data solutions.\nCollaborating with product teams and clients to deliver robust cloud-based data solutions that drive tax decisions and provide powerful experiences.\nAnalyzing user feedback and activity and iterate to improve the services and user experience.\n\nValue\n\nProviding effective technical leadership to design, collaborate and lead the implementation for various Data Science, AI/ML and data engineering initiatives.\nSecuring data in alignment with internal information and data security policies, best practices and client requirements.\nCreating and implementing robust cloud-based data solutions that scale effectively and provide powerful experiences for both internal teams and clients.\nPerforming unit tests and conducting reviews with other team members to make sure solutions and code are rigorously designed, elegantly coded and effectively tuned for performance.\n\nEducation And Experience\n\nBachelor's and/or master's degree in a related field\n7+ years of experience developing Data Science and/or AI/ML solutions.\n7+ years of experience with cloud-based data services, preferably in AWS or Azure.\n7+ years of experience developing Python, Scala, Java, .Net or similar solutions in a backend or data wrangling capacity.\n7+ years of experience in mixed Windows/Linux environments.\n\nAdditional Required Skills And Experience\n\nResults-proven track record of exceeding goals and evidence of the ability to consistently make good decisions through a combination of analysis, experience and judgment.\nFluency in AWS or Azure, especially with GenAI and AI/ML related services.\nFluency in one or more databases, preferably OLAP, OLTP and NoSQL is a plus.\nExperience with distributed data platforms is a plus.\nExperience with MLOps\nExperience training, deploying, monitoring, and maintaining ML models in production environments.\n\nComputer Skills\n\nTo perform this job successfully, an individual must have intermediate knowledge of Microsoft Project, Word, Excel, Access, PowerPoint, Outlook, and Internet navigation and research.\n\nSupervisory Responsibilities\n\nRequires supervisory responsibilities, including training employees, assigning work, and assuring quality throughout any deliverables.\n\nWork Environment\n\nStandard indoor working environment.\nOccasional long periods of sitting while working at computer.\nPosition requires regular interaction with employees at all levels of the Firm and interface with external vendors as necessary.\nIndependent travel requirement: As Needed\n\nEqual Opportunity Employer: disability/veteran",
        "skills": [
            "Nosql",
            "Java",
            "MLops",
            ".NET",
            "Scala",
            "OLAP",
            "Azure",
            "Python",
            "Oltp",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Manager - Data Scientist",
        "company_name": "InfoCepts",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position: Data Scientist (12-18 years)\nWe are seeking a talented and experienced Data Scientist to join our growing team. The ideal candidate will have a strong background in supply chain and operations, with expertise in developing advanced data science solutions for anomaly detection, forecasting, and revenue optimization using convolutional neural networks (CNN), Python, and PySpark.\nLocation: Chennai/Bangalore/Pune/Nagpur\nRoles & Responsibilities\nCollaborate with cross-functional teams to understand business requirements and identify opportunities for leveraging data science to drive revenue growth and improve operational efficiency in the supply chain and operations domain.\nDesign, develop, and deploy advanced data science models and algorithms for traditional and GenAI business products, with a focus on CNN anomaly detection, forecasting, and revenue optimization.\nAnalyse large and complex datasets to extract actionable insights and develop predictive models that provide valuable business intelligence and support data-driven decision-making processes.\nImplement scalable and efficient data processing pipelines using Python and PySpark to pre-process, clean, and transform raw data into a format suitable for analysis and modelling.\nEvaluate the performance of data science models using appropriate metrics and techniques, and iterate on model design and parameters to continuously improve accuracy and effectiveness.\nCollaborate with software engineers and DevOps teams to integrate data science models into production systems and ensure scalability, reliability, and performance.\nStay up-to-date with the latest advancements in data science, machine learning, and artificial intelligence technologies, and proactively identify opportunities to apply new techniques and methodologies to solve business challenges.\nEssential Skills:\nBachelor's degree or higher in Computer Science, Engineering, Mathematics, Statistics, or a related field. Advanced degree preferred.\n10+ years of experience in data science, machine learning, and predictive analytics, with a focus on supply chain and operations.\nStrong proficiency in Python and PySpark for data manipulation, analysis, and modelling. Experience with libraries such as Pandas, NumPy, TensorFlow, and PyTorch is highly desirable.\nSolid understanding of convolutional neural networks (CNN) and deep learning techniques for anomaly detection, forecasting, and revenue optimization.\nExperience working with large-scale datasets and distributed computing frameworks for processing and analysing big data, such as Hadoop, Spark, and Databricks.\nProven track record of developing and deploying data science solutions in production environments, with a focus on delivering measurable business impact and value.\nExcellent communication skills and ability to effectively collaborate with cross-functional teams, including business stakeholders, data engineers, software developers, and product managers.\nStrong analytical and problem-solving skills, with a passion for using data-driven approaches to solve complex business problems and drive innovation.\nGood to have\nThis individual will be self-directed, highly motivated, and organized with strong analytical thinking and problem-solving skills, and have an ability to work on multiple projects and function in a team environment.\nExpertise in managing complex projects, ensuring timely delivery and adherence to budget.\nQualifications:\nB.E/B.Tech/M.Tech qualification\nQualities:\nStrong leadership and team management\nExcellent project management skills\nEffective communication and collaboration\nAnalytical and strategic thinking\nAdaptability in multi-cultural environments",
        "skills": [
            "Databricks",
            "Pytorch",
            "Python",
            "Tensorflow",
            "Hadoop",
            "Pyspark",
            "Numpy",
            "Pandas",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HARMAN India",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Experience Level: 3-6 Years\n\nEducation Level: BE/B.Tech/ME/ M.Tech (CS or ECE)\n\nB.Tech/M.Tech in Electronics/Computer Science\nAt least 4 years work experience (B.Tech/MTech) in Computer Vision/Image Processing/Signal Processing/Machine Learning\n\nCandidate need to work form SRIB Bangalore with 3 days working from office is mandatory\n\nWe are seeking a highly motivated and talented Data Scientist to join our dynamic team. This role is ideal for individuals with 3-6 years of experience who are passionate about machine learning, data science, and artificial intelligence. The successful candidate will work closely with our experienced engineers and data scientists to develop, implement, and improve machine learning models and algorithms that drive our products and services.\n\nJob Requirements\n\nStrong fundamentals in Signal Processing/Computer Vision, Machine Learning, Deep Learning\nStrong proficiency in python , C++ , Linux\nFluency in data structures, algorithms\nKnowledge of embedded realization of ML models is a plus\nPrior experience with Deep learning frameworks such as Tensorflow/Pytorch\nAssist in the design, development, and implementation of machine learning models and algorithms.\nCollaborate with cross-functional teams to understand business requirements and translate them into technical solutions.\nPreprocess and analyze large datasets to extract meaningful insights and patterns.\nPerform data cleaning, feature engineering, and data augmentation to improve model performance.\nConduct experiments to test hypotheses and evaluate the performance of different machine learning approaches.\nImplement and optimize machine learning pipelines for efficiency and scalability.\nDeploy machine learning models into production environments and monitor their performance.\nDocument methodologies, processes, and results to ensure reproducibility and knowledge sharing.\nStay up to date with the latest advancements in machine learning and related technologies.\n\nJob Description\n\nUse innovative ideas to continuously improve the solution to take them to the end-user\nExtensive quality evaluation and regression testing of the developed solution to meet strict quality guidelines\nActively participate in design discussions to enhance end-to-end development of the solution\nFollow strict agile practices with regular code reviews, etc\nExcellent problem-solving skills and ability to think critically and creatively.\nStrong communication and presentation skills, with the ability to explain complex concepts to both technical and non-technical audiences.\nAbility to work independently and as part of a team in a fast-paced, dynamic environment.\nInternship or project experience related to machine learning or data science.\nA project portfolio with multiple real life projects (pet projects are welcome)",
        "skills": [
            "Signal Processing",
            "Tensorflow",
            "Machine Learning",
            "Pytorch",
            "Linux",
            "Python",
            "Computer Vision",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Roche",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\n\nThe Position\n\nWe are looking for a highly skilled Data Scientist with expertise in building AI-powered applications. We will be building GenAI solutions end-to-end: from concept, through prototyping, productization, to operations. The ideal candidate will bring technical expertise in Natural Language Processing (NLP), especially leveraging Large Language Models (LLM) and proficiency in prompt engineering techniques.\n\nKey Responsibilities:\n\nGenerative AI Application Development: Collaborate with AI engineers, product owners, business analysts and other developers in Agile teams to integrate LLMs into scalable, robust, fair and ethical end-user applications, focusing on user experience, relevance, and real-time performance\nAlgorithm Development: Design, develop, customize, optimize, and fine-tune LLM-based and other AI-infused algorithms tailored to specific use cases such as text generation, summarization, information extraction, chatbots, AI agents, code generation, document analysis, sentiment analysis, data analysis, etc\nData Curation for LLMs: Design data pipelines to curate, preprocess, and structure datasets that improve LLM-based algorithms performance and reduce biases, with a focus on data quality and diversity\nExploratory Data Analysis (EDA): Perform thorough data exploration to understand dataset characteristics, uncover patterns, detect biases, and identify data quality issues; use statistical and visualization techniques to inform feature engineering, model selection, and optimization of LLM-based applications\nSupport in Prompt Engineering: support prompt engineers, business analysts and subject matter experts in crafting and optimizing prompts to guide LLM outputs, enhancing performance for specific tasks; be ready to participate in prompt engineering when necessary\nExperimentation and Validation: Conduct rigorous experimentation, including A/B testing, to evaluate algorithm performance against benchmarks and control groups; use metrics specific to generative AI as well as pre-GenAI techniques, as required\nSoftware Development: Apply software development best practices, including writing unit test; contribute to configuring CI/CD pipelines, containerizing applications, setting up APIs, ensuring robust logging, experiment tracking, and model monitoring\nContinuous Improvement: Collaborate with other developers to monitor deployed algorithms, identify areas for improvement, and collaborate on updates to enhance performance\nStakeholder Communication: Translate complex technical results into clear, actionable insights for stakeholders, driving data-driven decision-making across the organization\nEthical AI and Bias Mitigation: Implement techniques to identify and mitigate biases in LLM outputs, ensuring responsible and ethical AI deployment\nPre-generative AI Application Development: Design and implement classical machine learning and NLP models (e.g., regression, classification, clustering, sequence modeling) when they provide a more efficient, interpretable, or cost-effective solution compared to LLMs; integrate these models into AI applications as needed\n\nRequirements:\n\nB.Sc., B.Eng., M.Sc., M.Eng., Ph.D. or D.Eng. in Computer Science, Physics, Statistics, Mathematics or equivalent degree and experience with Artificial Intelligence\nExperience: 3+ years working with advanced machine learning algorithms\n3+ years of hands-on experience working with language models, especially those based on Transformer architectures (e.g. BERT, T5, RoBERTa), and at least 1 year of experience with generative large language models (e.g. GPT, LLaMA, Claude, Cohere, etc.)\nTechnical Skills: Advanced proficiency in Python and experience with deep learning frameworks such as PyTorch or TensorFlow; expertise with Transformer architectures; hands-on experience with LangChain or similar LLM frameworks\nExperience with designing end-to-end RAG systems using state of the art orchestration frameworks (hands on experience with fine-tuning LLMs for specific tasks and use cases considered as an additional advantage)\nPractical overview and experience with AWS services to design cloud solutions, familiarity with Azure is a plus; experience with working with GenAI specific services like Azure OpenAI, Amazon Bedrock, Amazon SageMaker JumpStart, etc.\nData Skills: Strong skills in data manipulation, annotation, and crafting datasets that maximize LLM effectiveness; experience in working with data stores like vector, relational, NoSQL databases and data lakes through APIs; experience with data augmentation techniques or synthetic data generation in the context of LLMs considered as a plus\nPrompt Engineering: Hands-on experience with prompt design, zero-shot, and few-shot learning paradigms to optimize LLM performance without extensive training or fine-tuning\nEvaluation Metrics: Deep understanding of generative model and pre-GenAI evaluation techniques\nNLP Expertise: Solid foundation in natural language processing, including tokenization, embeddings, attention mechanisms, and transfer learning specific to LLMs\nStatistical Knowledge: Strong background in statistics, machine learning algorithms, and optimization techniques\nClassical Machine Learning & NLP: Experience with traditional NLP techniques and classical machine learning algorithms (e.g., decision trees, SVMs, random forests, gradient boosting) for text analysis and structured data applications\nPre-LLM Model Development: Hands-on experience developing and deploying machine learning models for tasks such as classification, clustering, regression, and sequence modeling using frameworks like Scikit-learn, XGBoost, or traditional NLP pipelines\nFeature Engineering & Data Preprocessing: Strong skills in feature engineering, dimensionality reduction, text preprocessing, and structured data transformation to improve model performance\nDeployment: Experience in deploying LLM models with cloud platforms (AWS, Azure) and machine learning workbenches for robust and scalable productization\nProficiency in best practices of software engineering\nProblem Solving: Excellent analytical skills and the ability to tackle complex challenges with innovative solutions\nCommunication: Strong verbal and written communication skills, with the ability to present complex findings clearly to both technical and non-technical audiences\n\nThe successful candidate should also:\n\nbe passionate about AI and stay up-to-date with the latest developments in LLMs, GenAI, and AI in general\nbe team-oriented, proactive, and collaborative\nbe an excellent problem solver and analytical thinker\nbe detail-oriented and highly organized\nbe willing to learn and expand their skill set\nhave the ability to work collaboratively in a fast-paced, dynamic environment\nbe able to communicate in English at the level of: C1+\nbe located near the Central European time zone, or willing to work at a time consistent with the Central European time zone\n\nWho we are\n\nA healthier future drives us to innovate. Together, more than 100'000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet's build a healthier future, together.\n\nRoche is an Equal Opportunity Employer.",
        "skills": [
            "LangChain",
            "Classical machine learning algorithms",
            "Prompt engineering",
            "Data augmentation techniques",
            "Statistical Knowledge",
            "Feature engineering",
            "Aws Services",
            "Azure",
            "Python",
            "Data Manipulation"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Crayon Data",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Sr Data Scientist\nExperience level: 5 to 7 years\nLocation: Chennai\nWho are we\nCrayon Data is a leading provider of AI-led revenue acceleration solutions, headquartered in Singapore with a presence in India and the UAE. Founded in 2012, our mission is to simplify the world's choices.\nOur flagship platform, maya.ai, helps enterprises in Banking, Fintech, and Travel unlock the value of their data to create hyperpersonalized experiences and drive sustainable revenue streams. maya.ai is powered by four as a Service components Data, Recommendation, Customer Experience, and Marketplace that work in unison to deliver tangible business outcomes.\nWhy Crayon Why now\nCrayon is transforming into an AI first company, and every Crayon (that's what we call ourselves!) is undergoing a journey of upskilling and expanding their capabilities in the AI space.\nWe're building an organization where AI is not a departmentit's a way of thinking. If you're an engineer who's passionate about building things, experimenting with models, and applying AI to solve real business problems, you'll feel right at home in our AI squads.\nOur environment is designed to be a playground for AI practitioners, with access to meaningful data, real-world challenges, and the freedom to innovate. You won't just be writing modelsyou'll be shaping Crayon's future.\nExperience: 5+ years\nIndustry: Banking, Financial Services, and AI\nTeam: Data Science\nJob Overview\nWe are seeking a Senior Data Scientist who thrives at the intersection of business and machine learning. In this role, you will develop and deploy data science models that directly drive outcomes for banking and financial institutions from boosting sales and cross-sell opportunities to reducing attrition and churn.\nIf you love solving real-world business problems using data and turning insights into action, this is the role for you.\nWhat You'll Do\nBuild and deploy machine learning and statistical models for key banking use cases: cross-sell, upsell, churn prediction, customer segmentation, lead scoring, etc.\nWork with large-scale structured and unstructured datasets to derive meaningful insights.\nTranslate business problems into analytical solutions and guide clients through data-driven decision-making.\nCollaborate closely with product, engineering, and consulting teams to deliver production-ready models.\nContinuously monitor, tune, and improve model performance post-deployment.\nMentor junior data scientists and contribute to internal knowledge sharing.\nCan you say Yes, I have! to the following\n5+ years of experience developing ML/AI solutions on large-scale datasets\nStrong academic background B.E/B.Tech/MS in Machine Learning, Computer Science, Statistics, Applied Math, or related fields; PhD is a bonus\nDeep understanding of statistical models hierarchical, stochastic, time series, survival, and econometric\nExpertise in at least one deep learning framework PyTorch, TensorFlow, or MxNet\nProficiency in Spark (Scala/PySpark) for large-scale data processing and designing scalable ML pipelines\nExperience contributing to open-source ML libraries or publishing research is a strong plus.\nCan you say Yes, I will! to the following\nInnovate with AI-first thinking\nChampion scalable, production-grade ML solutions\nCollaborate across teams to deliver outcomes aligned with business value\nStay curious, keep learning, and mentor junior team members\nBrownie points for:\nAlignment with The Crayon Box of Values because while skills can be learned, values are who we are.\nPassion for building reusable ML components and internal tools\nCome play, build, and grow with us.\n# Let's co-create the future of AI at Crayon.",
        "skills": [
            "Statistical Models",
            "Ai",
            "Machine Learning",
            "Scala",
            "Pyspark",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist, Staff",
        "company_name": "Lam Research",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Semiconductor Manufacturing",
        "job_description": "The Group You'll Be A Part Of\n\nGlobal Information Systems - Enterprise Analytics and Engineering - DataScience Hub\n\nThe Impact You'll Make\n\nAs a software lead for GenAI at Lam Research, you will lead development and architecture of various enterprise AI applications. You will play a key role in developing and guiding the technical team toward delivering business value through GenAI.\n\nWhat You'll Do\n\nLead design and development of GenAI-based applications powered by retrieval augmented generation (RAG), text-to-SQL, function-calling, and agentic architectures.\nArchitect and implement scalable solutions for ingesting, cracking, parsing, and indexing large-scale enterprise data for use in RAG-based systems.\nEstablish and implement software development life cycle (SDLC) best practices, including requirement analysis, coding standards, code reviews, source control management, build processes, testing, and operations.\nInteract with business stakeholders to gather feedback and tailor solutions to business needs.\n\nWho We're Looking For\n\n10+ years of experience in software engineering, machine learning, data science, or artificial intelligence.\nTypically requires a minimum of 12 years of related experience with a Bachelor's degree; or 8 years and a Master's degree; or a PhD with 5 years experience; or equivalent experience.\n\nPreferred Qualifications\n\n10+ years of experience in software engineering, machine learning, data science, or artificial intelligence.\nProficiency in Python, JavaScript/TypeScript, and modern frontend frameworks like Svelte.\nExperience in full-stack software engineering with a focus on API development and frontend technologies.\nExperience throughout the software development life cycle, including requirements gathering, design, architecture, development, testing, deployment, maintenance, frontend, backend, and DevOps.\nProficiency using Azure or similar cloud computing ecosystem, including AI services like Azure OpenAI and Azure AI Search.\nStrong communication skills, and ability to collaborate with cross-functional teams.\nStrong problem-solving skills and the ability to work in a fast-paced, dynamic environment.\nExperience developing GenAI applications leveraging LLMs, including chatbots using RAG, function-calling, and agentic architectures.\nExperience with GenAI development tools such as OpenAI, LangChain, and GraphRAG.\n\nOur Commitment\n\nWe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. By bringing unique individuals and viewpoints together, we achieve extraordinary results.\n\nLam Research (Lam or the Company) is an equal opportunity employer. Lam is committed to and reaffirms support of equal opportunity in employment and non-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. It is the Company's intention to comply with all applicable laws and regulations. Company policy prohibits unlawful discrimination against applicants or employees.\n\nLam offers a variety of work location models based on the needs of each role. Our hybrid roles combine the benefits of on-site collaboration with colleagues and the flexibility to work remotely and fall into two categories On-site Flex and Virtual Flex. On-site Flex you'll work 3+ days per week on-site at a Lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. Virtual Flex you'll work 1-2 days per week on-site at a Lam or customer/supplier location, and remotely the rest of the time.",
        "skills": [
            "LangChain",
            "GraphRAG",
            "OpenAI",
            "Svelte",
            "Typescript",
            "Javascript",
            "Api Development",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist II",
        "company_name": "Seismic",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nPlease be aware we have noticed an increase in hiring scams potentially targeting Seismic candidates. Read our full statement on our Careers page. Seismic is the global leader in AI-powered enablement, empowering go-to-market leaders to drive strategic growth and deliver exceptional customer experiences at scale. The Seismic Enablement Cloud is the only unified AI-powered platform that prepares customer-facing teams with the skills, content, tools, and insights needed to maximize every buyer interaction and strengthen client relationships. Trusted by more than 2,000 organizations worldwide, Seismic helps businesses achieve measurable outcomes and accelerate revenue growth. Seismic is headquartered in San Diego with offices across North America, Europe, Asia and Australia. Learn more at seismic.com. Seismic is committed to building an inclusive workplace that ignites growth for our employees and creates a culture of belonging that allows all employees to be seen and valued for who they are. Learn more about DEI at Seismic here.\n\nOverview\n\nJoin us at Seismic, a cutting-edge technology company leading the way in the SaaS industry. We specialize in delivering modern, scalable, and multi-cloud solutions that empower businesses to succeed in today's digital era. Leveraging the latest advancements in technology, including Generative AI, we are committed to driving innovation and transforming the way businesses operate. As we embark on an exciting journey of growth and expansion, we are seeking a talented Data Scientist to join our AI team in Hyderabad, India.\n\nSeismic AI\n\nAI is one of the fastest-growing product areas in Seismic. We believe that AI, particularly Generative AI, will empower and transform how Enterprise sales and marketing organizations operate and interact with customers. Seismic Aura, our leading AI engine, is powering this change in the sales enablement space and is being infused across the Seismic enablement cloud. Our focus is to leverage AI across the Seismic platform to make our customers more productive and efficient in their day-to-day tasks, and to drive more successful sales outcomes.\n\nWhy Join Us\n\nOpportunity to be a key contributor in a rapidly growing company and drive innovation in the SaaS industry.\nWork with cutting-edge technologies and be at the forefront of AI advancements.\nCompetitive compensation package, including salary, bonus, and equity options.\nA supportive, inclusive work culture.\nProfessional development opportunities and career growth potential in a dynamic and collaborative environment.\n\nWho You Are\n\nAs a Senior Data Scientist II, you will architect and develop complex AI applications, lead strategic technical initiatives, and mentor other data scientists and engineers to build next-gen AI capabilities. You will own high-impact projects and work with cross-functional teams to design, build, and maintain scalable, high-performance models and AI applications that deliver exceptional value to our customers. This position offers a unique opportunity to make an impact on our company's growth and success by bringing AI-powered features to life across our platform, including content discovery, learning and coaching, meeting intelligence and various AI capabilities. This is a hands-on, high-ownership role ideal for a senior individual contributor ready to scale AI in production.\n\nKey Responsibilities\n\nWhat you'll be doing:\n\nAI Application Development: Architect and develop robust and scalable data science and AI applications, including NLP, classification, retrieval-augmented generation (RAG), and conversational/agentic, and multi-agent systems.\nMentorship and Leadership: Mentor and coach junior and mid-level data scientists and engineers, promoting technical excellence and knowledge sharing across the team.\nModel Optimization: Design and implement high-performance machine learning models and pipelines. Continuously improve latency, robustness, accuracy, and scalability.\nInnovation and Technology Adoption: Evaluate and implement new tools, frameworks, and methodologies to improve the efficiency and quality of AI systems.\nProduct Integration: Collaborate with AI engineers, software engineers, and product managers to seamlessly integrate AI features across Seismic's products.\nCross-functional Collaboration: Partner with UX, engineering, and product teams to design end-to-end intelligent experiences for our users.\nDecision-making and Ownership: Take ownership of complex decisions within your domain and contributing to goal-setting and strategy.\n\nWhat You Bring To The Team\n\nExperience:\nBachelor's degree and 8+ years of experience, or an advanced degree (Master's or PhD) with 6+ years of industry experience in data science or AI.\nTechnical Expertise:\nDeep knowledge of AI and data science, including Generative AI, LLMs (OpenAI, Azure, Google, open-source), RAG pipelines, prompt engineering, NLP, and image models.\nStrong proficiency in Python, along with libraries such as Pandas, NumPy, Scikit-learn, PyTorch, or TensorFlow.\nHands-on experience with HuggingFace, LangChain, and cloud-native AI services.\nCloud Expertise:\nExperience with AWS, Azure, or GCP for model training, deployment, and data workflows.\nFamiliarity with MLOps and model lifecycle management in production environments.\nProduct Thinking:\nAbility to translate complex business challenges into technical solutions that scale.\nExperience in collaborating with product management and design in a product triad model.\nProven track record of delivering AI-powered features from concept to production.\nEducation:\nBachelor's or Master's degree in Computer Science, Data Science, Engineering, or a related field.\nOther Skills:\nExcellent communication and collaboration skills.\nStrong decision-making ability and a thoughtful, data-driven approach to solving problems.\nExperience working in a fast-paced SaaS or tech-driven environment.\n\nWhat We Have For You\n\nAt Seismic, we're committed to providing benefits and perks for the whole self. To explore our benefits available in each country, please visit the Global Benefits page.\n\nIf you are an individual with a disability and would like to request a reasonable accommodation as part of the application or recruiting process, please click here.\n\nHeadquartered in San Diego and with employees across the globe, Seismic is the global leader in salesenablement, backed by firms such as Permira, Ameriprise Financial, EDBI, Lightspeed Venture Partners, and T. Rowe Price. Seismic also expanded its team and product portfolio with the strategic acquisitions of SAVO, Percolate, Grapevine6, and Lessonly. Our board of directors is composed of several industry luminaries including John Thompson, former Chairman of the Board for Microsoft.\n\nSeismic is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to gender, age, race, religion, or any other classification which is protected by applicable law.\n\nPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.",
        "skills": [
            "LangChain",
            "Generative AI",
            "Google",
            "LLMs",
            "HuggingFace",
            "Scikit-learn",
            "prompt engineering",
            "image models",
            "open-source RAG pipelines",
            "OpenAI",
            "Tensorflow",
            "Numpy",
            "Nlp",
            "Pandas",
            "Pytorch",
            "Gcp",
            "MLops",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist (freelancer)",
        "company_name": "Soul AI",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are looking for a Data Scientist to join and build intelligent, data-driven solutions for our client that enable impactful decisions. This role requires contributions across the data science lifecycle from data wrangling and exploratory analysis to building and deploying machine learning models.\nWhether you're just getting started or have years of experience, we're looking for individuals who are curious, analytical, and driven to make a difference with data.\nResponsibilities:\nDesign, develop, and deploy machine learning models and analytical solutions.\nConduct exploratory data analysis and feature engineering.\nOwn or contribute to the end-to-end data science pipeline: data cleaning, modeling, validation, and deployment.\nCollaborate with cross-functional teams (engineering, product, business) to define problems and deliver measurable impact.\nTranslate business challenges into data science problems and communicate findings clearly.\nImplement A/B tests, statistical tests, and experimentation strategies.\nSupport model monitoring, versioning, and continuous improvement in production environments.\nEvaluate new tools, frameworks, and best practices to improve model accuracy and scalability.\nRequired Skills:\nStrong programming skills in Python including libraries such as pandas, NumPy, scikit-learn, matplotlib, seaborn.\nProficient in SQL, comfortable querying large, complex datasets.\nSound understanding of statistics, machine learning algorithms, and data modeling.\nExperience building end-to-end ML pipelines.\nExposure to or hands-on experience with model deployment tools like FastAPI, Flask, MLflow.\nExperience with data visualization and insight communication.\nFamiliarity with version control tools (e.g., Git) and collaborative workflows.\nAbility to write clean, modular code and document processes clearly.\nNice to Have:\nExperience with deep learning frameworks like TensorFlow or PyTorch.\nFamiliarity with data engineering tools like Apache Spark, Kafka, Airflow, dbt.\nExposure to MLOps practices and managing models in production environments.\nWorking knowledge of cloud platforms like AWS, GCP, or Azure (e.g., SageMaker, BigQuery, Vertex AI).\nExperience designing and interpreting A/B tests or causal inference models.\nPrior experience in high-growth startups or cross-functional leadership roles.\nEducational Qualifications:\nBachelor's or Master's degree in Computer Science, Data Science, Mathematics, Engineering, or a related field.\nPh.D. holders or candidates with demonstrated applied research contributions are a plus.",
        "skills": [
            "scikit-learn",
            "MLflow",
            "end-to-end ML pipelines",
            "Statistics",
            "Data Modeling",
            "Sql",
            "Numpy",
            "Seaborn",
            "Git",
            "Pandas",
            "Matplotlib",
            "Flask",
            "Data Visualization",
            "FastAPI",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Associate Principal - Data Scientist",
        "company_name": "Tiger Analytics",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title : Associate Principal\nIndia Locations: Hyderabad\nWho we are\nTiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer full-stack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow.\nOur team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence.\nWe are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine\nCurious about the role What your typical day would look like\nYour work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.\nDemonstrate proficiency in programming languages such as Python or R, with the ability to write clean, efficient, and maintainable code. Experience with relevant libraries and frameworks (e.g., TensorFlow, PyTorch, scikit-learn) is essential.\nUse your robust problem-solving skills to develop data-driven solutions, analyse complex datasets, and derive actionable insights that lead to impactful outcomes.\nWork closely with clients to understand their business objectives, identify opportunities for analytics-driven solutions, and communicate findings clearly and promptly.\nTake ownership of end-to-end model development, from problem definition and data exploration to model training, validation, and deployment.\nCollaborate with cross-functional teams, including data engineers, software developers, and business stakeholders, to integrate analytics solutions into business processes.\nLeverage a profound understanding of mathematical and statistical principles to guide developing and validating advanced data science models.\nStay abreast of industry trends, emerging technologies, and best practices in data science, bringing innovative ideas to the team and contributing to continuous improvement.\nDesired Skills and Experience:\n8 -10 years of total DS and model development experience\nMandatory : Minimum 4+ years of experience in the Banking and Financial services industry\nA passion for writing high-quality code (Python), and the code should be modular, scalable, and end-end project execution while planning an active hands-on role\nHaving good problem-solving skills is essential, and it is equally important to have in-depth knowledge to solve complex problems effectively.\nComprehensive knowledge of the regression and classification concepts and mathematical backend along with SQL\nEncourage collaboration with various stakeholders and take complete ownership of deliverables.\nAdept understanding of various data science approaches, machine learning algorithms, and statistical methods.\nExcellent communication skills with presentability, articulation, storytelling capability, and ability to manage complex client situations\nEffective mentoring of a team with expertise in industry/domain/functional areas\nYou are important to us, let's stay connected!\nEvery individual comes with a different set of skills and qualities so even if you don't tick all the boxes for the role today, we urge you to apply as there might be a suitable/unique role for you tomorrow. We are an equal opportunity employer. Our diverse and inclusive culture and values guide us to listen, trust, respect, and encourage people to grow the way they desire.\nNote: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.\nAdditional Benefits: Health insurance (self & family), virtual wellness platform, and knowledge communities",
        "skills": [
            "R",
            "scikit-learn",
            "Tensorflow",
            "Pytorch",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Comviva",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title : Data Scientist\nExperience- 4 to 6 yrs\nWe are looking for a Data Scientist who is Self-motivated, able to upskill to industry demands and has good analytical skills. The candidate should be able understand and adept at handling large data sets and develop & evaluate models to test the effectiveness of the models. Candidate must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, techniques to develop models using labelled and unlabelled data, generating simulation data. Candidate should be able to work with all types of data text, numeric, structured, unstructured, image, voice or video etc.\nResponsibilities\nAnalyse data sets from different products from production and identify patterns.\nTurn raw data into meaningful information that enterprises can use to improve their businesses\nAssess the effectiveness and accuracy of new data sources and data gathering techniques.\nDevelop custom data models and algorithms to apply to data sets.\nCoordinate with different functional teams to implement models and monitor outcomes.\nDevelop processes and tools to monitor and analyse model performance and data accuracy.\nQualifications\nStrong problem solving skills with an emphasis on product development.\nA drive to learn and master new technologies and techniques.\nExperience using statistical computer languages (R, Python etc.) to manipulate data and draw insights from large data sets.\nExperience working with different data architectures. Knowledge and experience in statistical and data mining techniques.\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\nPrior experience in working on zero touch solutions can be an advantage\nWe're looking for someone with 3-5 years of experience manipulating data sets and building statistical models, has a Bachelors or master's in computer science or another quantitative field, and is familiar with the following software:\nCoding knowledge and experience with several languages Python, R\nExperience with Python and common data science toolkits like Jupyter, Pandas, Numpy, Scikit-learn, TensorFlow, Keras etc.\nKnowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\nExperience in using different types of Databases - RDBMS, Graph, No-SQL etc.",
        "skills": [
            "data architectures",
            "Scikit-learn",
            "GLM Regression",
            "machine learning techniques",
            "R",
            "data mining techniques",
            "Boosting Trees",
            "Jupyter",
            "Random Forest",
            "Tensorflow",
            "Numpy",
            "social network analysis",
            "Pandas",
            "Keras",
            "Text Mining",
            "Python"
        ]
    },
    {
        "job_title": "Supply Chain - Data Scientist, GCP-Senior Manager",
        "company_name": "PwC Acceleration Centers in India",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals. In data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\n\nGrowing as a strategic advisor, you leverage your influence, expertise, and network to deliver quality results. You motivate and coach others, coming together to solve complex problems. As you increase in autonomy, you apply sound judgment, recognising when to take action and when to escalate. You are expected to solve through complexity, ask thoughtful questions, and clearly communicate how things fit together. Your ability to develop and sustain high performing, diverse, and inclusive teams, and your commitment to excellence, contributes to the success of our Firm.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\nCraft and convey clear, impactful and engaging messages that tell a holistic story.\nApply systems thinking to identify underlying problems and/or opportunities.\nValidate outcomes with clients, share alternative perspectives, and act on client feedback.\nDirect the team through complexity, demonstrating composure through ambiguous, challenging and uncertain situations.\nDeepen and evolve your expertise with a focus on staying relevant.\nInitiate open and honest coaching conversations at all levels.\nMake difficult decisions and take action to resolve issues hindering team effectiveness.\nModel and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.\n\nPosition: Senior Associate\n\nIndustry: CPG\n\nDomain: Range of Analytics (Descriptive to Advanced) depending on the client problem\n\nAbout Acceleration Center Bangalore\n\nAt PwC, we connect people with diverse backgrounds and skill sets to solve important problems together and lead with purposefor our clients, our communities and for the world at large. It is no surprise therefore that 429 of 500 Fortune global companies engage with PwC. Acceleration Centers (ACs) are PwC's diverse, global talent hubs focused on enabling growth for the organization and value creation for our clients.The PwC Advisory Acceleration Center in Bangalore is part of our Advisory business in the US. The team is focused on developing a broader portfolio with solutions for Risk Consulting, Management Consulting, Technology Consulting,Strategy Consulting, Forensics as well as vertical specific solutions. PwC's high-performance culture is based on passion for excellence with focus on diversity and inclusion. You will collaborate with and receive support from a network of people to achieve your goals. We will also provide you with global leadership development frameworks and the latest in digital technologies to learn and excel in your career. At the core of our firm's philosophy is a simple construct: We care for our people. Globally PwC is ranked the 3rd most attractive employer according to Universum. Our commitment to Responsible Business Leadership, Diversity & Inclusion, work-life flexibility, career coaching and learning & development makes our firm one of the best places to work, learn and excel\n\nWe are looking for experienced leaders with a strong analytical background (and overall professional experience of 10+ years) to work in our Analytics Consulting practice in Mumbai, Bangalore.\n\nSenior Associate's will work as an integral part of business analytics teams in India alongside clients and consultants in the U.S., leading teams for high-end analytics consulting engagements and providing business recommendations to project teams.\n\nEducation: Advanced Degree in a quantitative discipline such as Computer Science, Engineering, Econometrics, Statistics, Operations Research or Information Sciences such as business analytics or informatics\n\nRequired Skills: Successful candidates will have demonstrated the following skills and characteristics:\n\nMust Have\n\nProven expertise in supply chain analytics across domains such as demand forecasting, inventory optimization, logistics, segmentation, and network design\nWell versed and hands-on experience of working on optimization methods like linear programming, mixed integer programming, scheduling optimization. Having understanding of working on third party optimization solvers like Gurobi will be an added advantage\nProficiency in forecasting techniques (e.g., Holt-Winters, ARIMA, ARIMAX, SARIMA, SARIMAX, FBProphet, NBeats) and machine learning techniques (supervised and unsupervised)\nStrong command of statistical modeling, testing, and inference\nProficient in using GCP tools: BigQuery, Vertex AI, Dataflow, Looker\nBuilding data pipelines and models for forecasting, optimization, and scenario planning\nStrong SQL and Python programming skills; experience deploying models in GCP environment\nKnowledge of orchestration tools like Cloud Composer (Airflow)\n\nNice To Have\n\nFamiliarity with MLOps, containerization (Docker, Kubernetes), and orchestration tools (e.g., Cloud composer)\nStrong communication and stakeholder engagement skills at the executive level\n\nRoles And Responsibilities\n\nAssist analytics projects within the supply chain domain, driving design, development, and delivery of data science solutions\nDevelop and execute on project & analysis plans under the guidance of Project Manager\nInteract with and advise consultants/clients in US as a subject matter expert to formalize data sources to be used, datasets to be acquired, data & use case clarifications that are needed to get a strong hold on data and the business problem to be solved\nDrive and Conduct analysis using advanced analytics tools and coach the junior team members\nImplement necessary quality control measures in place to ensure the deliverable integrity like data quality, model robustness, and explainability for deployments.\nValidate analysis outcomes, recommendations with all stakeholders including the client team\nBuild storylines and make presentations to the client team and/or PwC project leadership team\nContribute to the knowledge and firm building activities",
        "skills": [
            "SARIMA",
            "Supply Chain Analytics",
            "forecasting techniques",
            "NBeats",
            "Airflow",
            "Looker",
            "machine learning techniques",
            "Demand Forecasting",
            "optimization methods",
            "scheduling optimization",
            "Statistical Modeling",
            "SARIMAX",
            "Linear Programming",
            "FBProphet",
            "Vertex AI",
            "Cloud Composer",
            "GCP tools",
            "logistics segmentation",
            "data pipelines",
            "Holt-Winters",
            "inventory optimization",
            "ARIMAX",
            "mixed integer programming",
            "Sql",
            "Network Design",
            "DataFlow",
            "BigQuery",
            "Python",
            "Arima"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "RevX",
        "experience": "5-8 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Title: Lead Data Scientist\nAbout RevX:\nMade for Growth, Built for App Marketers.\nRevX helps app businesses acquire and reengage users via programmatic to retain, monetize, and accelerate revenue. We're all about taking your app businesses to a new growth level. We rely on data science, innovative technology, and AI, and a skilled team, to create and deliver seamless ad experiences to delight your app users. That's why RevX is the ideal partner for app marketers that demand trustworthy insights, a hands-on team, and a commitment to growth. We help you build sound mobile strategies, combining programmatic UA, app re engagement, and performance branding to drive real and verifiable results so you can scale your business: with real users, high retention, and incremental revenue.\nMajor Responsibilities:\nResearch and Problem-Solving: Identify and frame business problems, conduct exploratory data analysis, and propose innovative data science solutions tailored to business needs.\nLeadership & Communication: Serve as a technical referent for the research team, driving high-impact, high-visibility initiatives. Effectively communicate complex scientific concepts to senior stakeholders, ensuring insights are actionable for both technical and non-technical audiences. Mentor and develop scientists within the team, fostering growth and technical excellence.\nAlgorithm Development: Design, optimize, and implement advanced machine learning algorithms, including neural networks, ensemble models (XGBoost, random forests), and clustering techniques.\nEnd-to-End Project Ownership: Lead the development, deployment, and monitoring of machine learning models and data pipelines for large-scale applications.\nModel Optimization and Scalability: Focus on optimizing algorithms for performance and scalability, ensuring robust, well-calibrated models suitable for real-time environments.\nA/B Testing and Validation: Design and execute experiments, including A/B testing, to validate model effectiveness and business impact.\nBig Data Handling: Leverage tools like BigQuery, advanced SQL, and cloud platforms (e.g., GCP) to process and analyze large datasets.\nCollaboration and Mentorship: Work closely with engineering, product, and campaign management teams, while mentoring junior data scientists in best practices and advanced techniques.\nData Visualization: Create impactful visualizations using tools like matplotlib, seaborn, Looker, and Grafana to communicate insights effectively to stakeholders.\nRequired Experience/Skills:\n58 years of hands-on experience in data science or machine learning roles. 2+ years leading data science projects in AdTech Strong hands-on skills in Advanced Statistics, Machine Learning, and Deep Learning.\nDemonstrated ability to implement and optimize neural networks and other advanced ML models.\nProficiency in Python for developing machine learning models, with a strong grasp of TensorFlow or PyTorch.\nExpertise handling large datasets using advanced SQL and big data tools like BigQuery.\nIn-depth knowledge of MLOps pipelines, from data preprocessing to deployment and monitoring.\nStrong background in A/B testing, statistical analysis, and experimental design.\nProven capability in clustering, segmentation, and unsupervised learning methods.\nStrong problem-solving and analytical skills with a focus on delivering business value.\nEducation:\nA Master's in Data Science, Computer Science, Mathematics, Statistics, or a related field is preferred. A Bachelor's degree with exceptional experience will also be considered.\nFor more information visit www.revx.io",
        "skills": [
            "Random Forests",
            "Looker",
            "Clustering Techniques",
            "Advanced Statistics",
            "Experimental Design",
            "Machine Learning",
            "BigQuery",
            "Neural Networks",
            "Grafana",
            "Deep Learning",
            "Tensorflow",
            "Seaborn",
            "Pytorch",
            "MLops",
            "Matplotlib",
            "XGBoost",
            "Data Visualization",
            "Advanced Sql",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Zinnia",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who We Are\n\nZinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.\n\nWho You Are\n\nWe are seeking a highly motivated Senior Data Scientist with strong technical expertise, business acumen, and strategic problem-solving abilities. In this role, you will independently own and build forecasting models. You will work closely with stakeholders across Product, Data Engineering & Analytics, and Business Strategy to identify opportunities to resolve business problems. This is a high-impact individual contributor role.\n\nWhat You'll Do\n\nWork in a dynamic and innovative company to develop cutting-edge solutions.\nPerform advanced data analytics to influence decision makers.\nLeverage data from diverse sources to provide insights and build new data driven tools.\nPartner with engineering teams to continuously improve our data quality.\nDevelop machine learning based applications using supervised and unsupervised models using Python that optimize and personalize customer experiences or reduce manual effort on our internal teams through automated decision making.\nDevelop and support models to enable things such as prescriptive insights, automated decisioning, and insights.\nDevelop experiments to understand model impact, monitor live model analytics, and manage training and retraining pipelines.\nWork with stakeholders to intake complicated business problems and translate them into solvable data science projects.\nPartner with data engineering to take your model from development to deployed infrastructure.\nBrainstorm future use cases and contribute to the learning culture of the data science team.\nPartner with and mentor other data scientists and data analysts.\nPartner with multiple marketing teams, manage multiple projects and help conceptualize applications that directly drive company growth and strategy.\nYou will connect machine learning applications to business needs and help facilitate process changes based on algorithmic solution implementation.\n\nWhat You'll Need\n\nYou have Data Science experience building and validating machine learning and forecasting models in Python for a minimum of 5 years.\nYou love performing advanced analytics to find insights and patterns.\nYou have experience in supervised and unsupervised model techniques such as random forest, gradient boosting, support vector machines, k-means and hierarchical clustering, causal models, mixture models and experience in advanced modeling techniques such as reinforcement learning, neural networks, and natural language modeling.\nYou have experience in delivering natural language projects utilizing techniques such as text summarization, topic modeling, entity extraction, semantic encoding, and valence analysis\nYou have experience working in an agile business setting.\nYou have experience with relational cloud databases like BigQuery and Snowflake and are comfortable working with unstructured datasets such as unstructured text.\n\nWHAT'S IN IT FOR YOU\n\nAt Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability.",
        "skills": [
            "Entity Extraction",
            "Unsupervised Models",
            "Hierarchical Clustering",
            "Causal Models",
            "Supervised Models",
            "Text Summarization",
            "Mixture Models",
            "Gradient Boosting",
            "reinforcement learning",
            "Support Vector Machines",
            "Topic Modeling",
            "Valence Analysis",
            "Semantic Encoding",
            "snowflake",
            "k-means",
            "BigQuery",
            "Machine Learning",
            "Neural Networks",
            "Random Forest",
            "Python"
        ]
    },
    {
        "job_title": "Mid-Level Data Scientist",
        "company_name": "neurIOT Labs",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Location:Noida\nType:Full-Time\nExperience Level:25 years\nIndustry:Artificial Intelligence, Machine Learning, Data Science\nAbout the Role\nWe are looking for a self-motivatedMid-Level Data Scientistto join our AI team focused onGenAIapplications. We work at the intersection of multi-modal modeling, Retrieval-Augmented Generation (RAG), and real-time machine learning systems. You'll collaborate with a high-impact team to design, prototype, and deploy next-generation AI solutions, especially around document understanding and multi-modal tasks.\nKey Responsibilities\nDesign and implement state-of-the-art GenAI solutions, involving multi-modal, document understanding models and agents.\nBuild and optimizeRAG pipelines, including knowledge of various RAG architectures.\nDevelop and maintainagentic workflowsusing tools likeLangGraph, LangChain.\nWork with large-scale datasets and ensure efficient data processing pipelines.\nPerform statistical analysis, algorithm development, and performance tuning.\nWorking with opensource LLMs and deploying them on serving frameworks such as sglang and vllm.\nStay up to date with the latest developments in GenAI and ML, and actively contribute to knowledge sharing.\nRequired Qualifications\nBachelor's degree (Master's preferred) in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 3 years of experience working in machine learning, data science, or AI roles.\nStrong command ofPythonand familiarity withRor other scripting languages.\nHands-on experience withdeep learning,transformer-based models, andmulti-modal learning.\nProficiency in AI/ML frameworks and libraries (e.g., PyTorch, TensorFlow, Hugging Face Transformers).\nStrong understanding of statistics, linear algebra, and probability theory.\nExperience working withcloud environments, preferablyAzure.\nExposure toOpenAI,Anthropic,Mistral, or similar APIs and deployment ofopen-source models(LLaMA, MPT, etc.).\nDemonstrated experience indocument AI,vision-language models, orOCR-based understanding systems.\nPreferred Skills\nExperience withLangGraph,CrewAI,Autogen, or similar orchestration frameworks.\nWorking knowledge ofvector databases(e.g., Qdrant, Weaviate, Pinecone) andembedding search techniques.\nExposure toKubernetes,Docker, orML model deployment workflows.\nCuriosity-driven mindset with a passion for learning and experimenting with the latest in AI research.\nWhy Join Us\nBe part of a team working on powerful AI applications\nAccess to cutting-edge tools and open models\nFlexible working hours\nSupportive environment that encourages innovation, research, and upskilling",
        "skills": [
            "embedding search techniques",
            "Hugging Face Transformers",
            "Qdrant",
            "vision-language models",
            "vector databases",
            "Pinecone",
            "Autogen",
            "LangGraph",
            "R",
            "Mistral",
            "multi-modal learning",
            "CrewAI",
            "Anthropic",
            "OCR-based understanding systems",
            "cloud environments",
            "OpenAI",
            "document AI",
            "Weaviate",
            "transformer-based models",
            "Tensorflow",
            "Deep Learning",
            "Pytorch",
            "Docker",
            "Python",
            "Azure",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Senior Data Scientist (Gen AI)",
        "company_name": "Blend",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Description\n\nBlend is a premier AI services provider, committed to co-creating meaningful impact for its clients through the power of data science, AI, technology, and people. With a mission to fuel bold visions, Blend tackles significant challenges by seamlessly aligning human expertise with artificial intelligence. The company is dedicated to unlocking value and fostering innovation for its clients by harnessing world-class people and data-driven strategy. We believe that the power of people and AI can have a meaningful impact on your world, creating more fulfilling work and projects for our people and clients. For more information, visit www.blend360.com\n\nJob Description\n\nBlend is hiring a Senior Data Scientist (Generative AI) to spearhead the development of advanced AI-powered classification and matching systems on Databricks. You will contribute to flagship programs like the Diageo AI POC by building RAG pipelines, deploying agentic AI workflows, and scaling LLM-based solutions for high-precision entity matching and MDM modernization.\n\nKey Responsibilities\n\nDesign and implement end-to-end AI pipelines for product classification, fuzzy matching, and deduplication using LLMs, RAG, and Databricks-native workflows.\nDevelop scalable, reproducible AI solutions within Databricks notebooks and job clusters, leveraging Delta Lake, MLflow, and Unity Catalog.\nEngineer Retrieval-Augmented Generation (RAG) workflows using vector search and integrate with Python-based matching logic.\nBuild agent-based automation pipelines (rule-driven + GenAI agents) for anomaly detection, compliance validation, and harmonization logic.\nImplement explainability, audit trails, and governance-first AI workflows aligned with enterprise-grade MDM needs.\nCollaborate with data engineers, BI teams, and product owners to integrate GenAI outputs into downstream systems.\nContribute to modular system design and documentation for long-term scalability and maintainability.\n\nQualifications\n\nBachelor's/Master's in Computer Science, Artificial Intelligence, or related field.\n5+ years of overall Data Science experience with 2+ years in Generative AI / LLM-based applications.\nDeep experience with Databricks ecosystem: Delta Lake, MLflow, DBFS, Databricks Jobs & Workflows.\nStrong Python and PySpark skills with ability to build scalable data pipelines and AI workflows in Databricks.\nExperience with LLMs (e.g., OpenAI, LLaMA, Mistral) and frameworks like LangChain or LlamaIndex.\nWorking knowledge of vector databases (e.g., FAISS, Chroma) and prompt engineering for classification/retrieval.\nExposure to MDM platforms (e.g., Stibo STEP) and familiarity with data harmonization challenges.\nExperience with explainability frameworks (e.g., SHAP, LIME) and AI audit tooling.\n\nPreferred Skills\n\nKnowledge of agentic AI architectures and multi-agent orchestration.\nFamiliarity with Azure Data Hub and enterprise data ingestion frameworks.\nUnderstanding of data governance, lineage, and regulatory compliance in AI systems.\n\nAdditional Information\n\nThrive & Grow with Us:\n\nCompetitive Salary: Your skills and contributions are highly valued here, and we make sure your salary reflects that, rewarding you fairly for the knowledge and experience you bring to the table.\nDynamic Career Growth: Our vibrant environment offers you the opportunity to grow rapidly, providing the right tools, mentorship, and experiences to fast-track your career.\nIdea Tanks: Innovation lives here. Our Idea Tanks are your playground to pitch, experiment, and collaborate on ideas that can shape the future.\nGrowth Chats: Dive into our casual Growth Chats where you can learn from the bestwhether it's over lunch or during a laid-back session with peers, it's the perfect space to grow your skills.\nSnack Zone: Stay fuelled and inspired! In our Snack Zone, you'll find a variety of snacks to keep your energy high and ideas flowing.\nRecognition & Rewards: We believe great work deserves to be recognized. Expect regular Hive-Fives, shoutouts and the chance to see your ideas come to life as part of our reward program.\nFuel Your Growth Journey with Certifications: We're all about your growth groove! Level up your skills with our support as we cover the cost of your certifications.",
        "skills": [
            "LangChain",
            "SHAP",
            "LLMs",
            "LLaMA",
            "MLflow",
            "Chroma",
            "LIME",
            "FAISS",
            "Mistral",
            "Delta Lake",
            "OpenAI",
            "LlamaIndex",
            "Pyspark",
            "Databricks",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "eProSoft",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Senior Data Scientist Large Language Models (LLMs)\nLocation: Remote (United States or India)\nType: Full-Time\nAbout the Role:\nWe are seeking a highly experienced and hands-on Senior Data Scientist with deep expertise in Large Language Models (LLMs) to join our team. In this critical role, you will lead the development and optimization of advanced language models, focusing on fine-tuning, retrieval techniques, and real-world performance benchmarking. You'll play a central role in driving innovation and delivering high-accuracy, production-ready AI systems.\nKey Responsibilities:\nFine-tune LLMs (e.g., GPT, LLaMA, Mistral) for specific business tasks and domains\nDesign and implement Retrieval-Augmented Generation (RAG) pipelines\nWork with vector databases (e.g., Quadrant, FAISS, Pinecone, Weaviate) for efficient document retrieval\nBenchmark and evaluate model responses for relevance, accuracy, and consistency\nDrive improvements in response accuracy through data analysis and model iteration\nCollaborate with engineering, product, and research teams to deliver scalable AI solutions\nStay updated with the latest research and best practices in NLP and generative AI\nRequirements:\n7+ years of hands-on experience in data science or machine learning, with 3+ years focused on LLMs or NLP\nStrong expertise in LLM fine-tuning, RAG systems, and prompt engineering\nDeep understanding of vector similarity search and vector databases\nProven track record of improving LLM response quality through rigorous experimentation and evaluation\nProficiency in Python, PyTorch, Hugging Face Transformers, and other relevant tools\nFamiliarity with evaluation metrics for generative AI (e.g., BLEU, ROUGE, BERTScore, human evals)\nAbility to lead projects independently and mentor junior team members\nPreferred Qualifications:\nExperience deploying LLMs into production environments\nContributions to open-source LLM or NLP projects\nBackground in AI safety, interpretability, or explainability is a plus.\nWhy Join Us\nWork on cutting-edge generative AI technologies\nCollaborate with world-class teams across the US and India\nFlexible remote work environment\nOpportunity to shape the future of AI in production",
        "skills": [
            "Hugging Face Transformers",
            "Vector databases",
            "Evaluation metrics for generative AI",
            "Pytorch",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist Senior Consultant I",
        "company_name": "Allstate",
        "experience": "4-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "At Allstate, great things happen when our people work together to protect families and their belongings from life's uncertainties. And for more than 90 years our innovative drive has kept us a step ahead of our customers evolving needs. From advocating for seat belts, air bags and graduated driving laws, to being an industry leader in pricing sophistication, telematics, and, more recently, device and identity protection.\n\nJob Description\n\nJob Description Summary\n\nThis role is responsible for leading the use of data to make decisions. This includes: the development and execution of new machine learning predictive modeling algorithms, the coding/development of tools that use machine learning/predictive modeling to make business decisions, searching for and integrating new data (both internal and external) that improves our modeling and machine learning results (and ultimately our business decisions), and discovery of solutions to business problems that can be solved through the use of machine learning/predictive modeling. This role will also begin to manage projects of small to medium complexity.\n\nPurpose\n\nWe are looking for a Data Scientist who will lead the application of machine learning and advanced analytics to solve complex business problems. The ideal candidate will bring a strong mix of technical expertise, business acumen, and a passion for solving high-impact problems. This role involves developing machine learning models, integrating new internal and external data sources, and delivering solutions that enhance decision-making across the enterprise. Projects may span domains such as telematics, aerial imagery, and natural language processing (NLP). The role also requires project ownership from design to deployment and collaboration with cross-functional teams to translate insights into action.\n\nKey Responsibilities\n\nDesign, build, and validate statistical and machine learning models that address key business problems.\nPerform in-depth data exploration and analysis to uncover actionable insights and improve model performance.\nCommunicate analytical findings clearly and effectively to technical and non-technical stakeholders; collaborate with business and technology teams to ensure solutions are adopted and scaled.\nStay up to date with emerging modeling techniques, tools, and technologies, and integrate innovative approaches into existing workflows where appropriate.\nLead and manage end-to-end data science initiatives, ensuring high-quality delivery and measurable business impact.\nContribute to project planning by breaking down complex data science projects into manageable components and aligning deliverables with business timelines.\nProvide mentorship and technical guidance to junior team members and lead small technical teams as needed.\n\nMust Have Skills\n\n4 to 8 years of experience in applied data science, with a track record of delivering business value through machine learning solutions.\nProficiency in Python with experience in libraries such as scikit-learn, pandas, NumPy, and TensorFlow or PyTorch.\nStrong foundation in statistical analysis, regression modeling, classification techniques, clustering, time series forecasting, hypothesis testing, and multivariate analysis.\nHands-on experience building and deploying machine learning models in cloud environments (e.g., AWS, Azure, or GCP), with strong understanding of MLOps practices such as model versioning, CI/CD pipelines, monitoring, and automated retraining workflows.\nExperience working with large-scale structured and unstructured datasets using distributed data systems such as Hadoop, Hive, or Spark.\nAbility to translate complex, ambiguous business problems into structured data science problems and articulate solution approaches clearly.\nExcellent communication, storytelling, and stakeholder management skills.\nStrong analytical and problem-solving mindset with a bias for action.\nInnovation-oriented and proactive in identifying new opportunities for data-driven decision-making.\nExperience working in Agile or Scrum-based project environments.\n\nPreferred Skills\n\nExperience with Large Language Models (LLMs) and transformer architectures (e.g., GPT, BERT), including prompt engineering or fine-tuning for enterprise use cases.\nExperience with production-grade ML platforms (e.g., MLflow, Vertex AI, SageMaker) and orchestration tools (e.g., Airflow, Kubeflow) to operationalize models at scale\n\nPrimary Skills\n\nBusiness Case Analyses, Data Analytics, Predictive Analytics, Predictive Modeling, Waterfall Project Management\n\nShift Time\n\nShift B (India)\n\nRecruiter Info\n\nAnnapurna Jha\n\n[HIDDEN TEXT]\n\nAbout Allstate\n\nJoining our team isn't just a job it's an opportunity. One that takes your skills and pushes them to the next level. One that encourages you to challenge the status quo. And one where you can impact the future for the greater good.\n\nYou'll do all this in a flexible environment that embraces connection and belonging. And with the recognition of several inclusivity and diversity awards, we've proven that Allstate empowers everyone to lead, drive change and give back where they work and live.\n\nGood Hands. Greater Together.\n\nThe Allstate Corporation is one of the largest publicly held insurance providers in the United States. Ranked No. 84 in the 2023 Fortune 500 list of the largest United States corporations by total revenue, The Allstate Corporation owns and operates 18 companies in the United States, Canada, Northern Ireland, and India. Allstate India Private Limited, also known as Allstate India, is a subsidiary of The Allstate Corporation. The India talent center was set up in 2012 and operates under the corporation's Good Hands promise. As it innovates operations and technology, Allstate India has evolved beyond its technology functions to be the critical strategic business services arm of the corporation. With offices in Bengaluru and Pune, the company offers expertise to the parent organization's business areas including technology and innovation, accounting and imaging services, policy administration, transformation solution design and support services, transformation of property liability service design, global operations and integration, and training and transition.\n\nLearn more about Allstate India here.",
        "skills": [
            "Airflow",
            "scikit-learn",
            "MLflow",
            "GPT",
            "Vertex AI",
            "Kubeflow",
            "BERT",
            "Large Language Models",
            "time series forecasting",
            "regression modeling",
            "classification techniques",
            "SageMaker",
            "Hypothesis Testing",
            "Multivariate Analysis",
            "Tensorflow",
            "Numpy",
            "Pytorch",
            "Python",
            "AWS",
            "Hadoop",
            "Hive",
            "Pandas",
            "Gcp",
            "MLops",
            "Spark",
            "Clustering",
            "Azure",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "S&P Global",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Ahmedabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Role\n\nGrade Level (for internal use):\n\n09\n\nThe Team\n\nAs a member of the Data Transformation team you will work on building ML powered products and capabilities to power natural language understanding, data extraction, information retrieval and data sourcing solutions for S&P Global Market Intelligence and our clients. You will spearhead development of production-ready AI products and pipelines while leading-by-example in a highly engaging work environment. You will work in a (truly) global team and encouraged for thoughtful risk-taking and self-initiative.\n\nThe Impact\n\nThe Data Transformation team has already delivered breakthrough products and significant business value over the last 3 years.\nIn this role you will be developing our next generation of new products while enhancing existing ones aiming at solving high-impact business problems.\n\nWhat's In It For You\n\nBe a part of a global company and build solutions at enterprise scale\nCollaborate with a highly skilled and technically strong team\nContribute to solving high complexity, high impact problems\n\nKey Responsibilities\n\nDesign, Develop and Deploy ML powered products and pipelines\nPlay a central role in all stages of the data science project life cycle, including:\nIdentification of suitable data science project opportunities\nPartnering with business leaders, domain experts, and end-users to gain business understanding, data understanding, and collect requirements\nEvaluation/interpretation of results and presentation to business leaders\nPerforming exploratory data analysis, proof-of-concept modelling, model benchmarking and setup model validation experiments\nTraining large models both for experimentation and production\nDevelop production ready pipelines for enterprise scale projects\nPerform code reviews & optimization for your projects and team\nSpearhead deployment and model scaling strategies\nStakeholder management and representing the team in front of our leadership\nLeading and mentoring by example including project scrums\nWhat We're Looking For\n\n2+ years of professional experience in Data Science domain\nExpertise in Python (Numpy, Pandas, Spacy, Sklearn, Pytorch/TF2, HuggingFace etc.)\nExperience with SOTA models related to NLP and expertise in text matching techniques, including sentence transformers, word embeddings, and similarity measures\nExpertise in probabilistic machine learning model for classification, regression & clustering\nStrong experience in feature engineering, data preprocessing, and building machine learning models for large datasets.\nExposure to Information Retrieval, Web scraping and Data Extraction at scale\nOOP Design patterns, Test-Driven Development and Enterprise System design\nSQL (any variant, bonus if this is a big data variant)\nLinux OS (e.g. bash toolset and other utilities)\nVersion control system experience with Git, GitHub, or Azure DevOps.\nProblem-solving and debugging skills\nSoftware craftsmanship, adherence to Agile principles and taking pride in writing good code\nTechniques to communicate change to non-technical people\n\nNice to have\n\nPrior work to show on Github, Kaggle, StackOverflow etc.\nCloud expertise (AWS and GCP preferably)\nExpertise in deploying machine learning models in cloud environments\nFamiliarity in working with LLMs\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nGlobal Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_%20English_formattedESQA508c.pdf\n\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority Ratings - (Strategic Workforce Planning)\n\nJob ID: 315682\n\nPosted On: 2025-05-20\n\nLocation: Gurgaon, Haryana, India",
        "skills": [
            "TF2",
            "HuggingFace",
            "data preprocessing",
            "word embeddings",
            "Classification",
            "similarity measures",
            "text matching techniques",
            "Regression",
            "probabilistic machine learning",
            "feature engineering",
            "Spacy",
            "sentence transformers",
            "Github",
            "Sklearn",
            "Nlp",
            "Linux Os",
            "Python",
            "Azure DevOps",
            "Sql",
            "Numpy",
            "Git",
            "Pandas",
            "Pytorch",
            "Clustering"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Haber",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Role:\nAt Haber, we are redefining how industries operate by blending automation with intelligent decision making. As a Data Scientist, you will play a pivotal role in designing machine learning systems that power our suite of innovative productsincluding Elixa, Kaiznn, Mount Fuji, and upcoming platforms like our Fiber Morphology solution. You'll work extensively with time-series data, sensor feeds, and computer vision pipelines to uncover actionable insights in real-time. This is a high-impact, high-ownership role where your work will directly influence operational efficiency and sustainability across complex industrial environments. We value independent thinkers who are not just technical experts but also strategic problem-solvers.\nKey Responsibilities:\nLead the full machine learning system development lifecycle: data collection, cleaning, feature engineering, modeling, deployment, and evaluation\nWork with time-series sensor data and computer vision systems to build predictive and anomaly detection models\nDesign and develop scalable recommendation platforms for real-time industrial decision-making Apply machine learning, data mining, and statistical techniques (e.g., regression, clustering, collaborative filtering, PCA) to a wide range of problems\nDesign novel approaches to large-scale data analysis, including semi-structured and unstructured data\nCollaborate with engineering, QA, product, and operations teams to deliver end-to-end solutions Contribute to and stay up-to-date with research in areas such as machine learning, signal processing, and domain-specific optimization\nMake independent technical decisions and guide strategic direction based on data insights Mentor junior team members and foster a collaborative learning culture\nQualification:\nHaving 3-6 years of experience as a Data Scientist, preferably in a client facing role taking complete ownership of the Data Science lifecycle\nStrong programming skills in Python or R\nProven experience working with time-series data and real-time data pipelines\nSolid foundation in supervised and unsupervised ML methods\nExposure to sensor data, image processing, or computer vision techniques is a strong plus Experience with cloud environments, preferably AWS, for model deployment and monitoring Sound understanding of statistics, optimization, and data visualization\nAbility to work independently and drive projects from idea to execution\nStrong communication skills and the ability to collaborate across multidisciplinary teams\nA passion for solving tough, real-world problems using data\nWhat We Offer:\nReal-World Impact: Your models will be deployed in operational environments, not just experimental labs\nOwnership & Autonomy: Freedom to explore, decide, and build in a high-trust environment Diverse Tech Stack: Work on ML pipelines that combine time-series, sensor, and vision data\nFast-Track Learning: Be part of a startup where learning never stops and innovation is constant Collaborative Culture: A team that supports initiative, experimentation, and personal growth Product-Minded Thinking: Your work will be tightly integrated with product and business decisions",
        "skills": [
            "supervised ML methods",
            "cloud environments",
            "time-series data",
            "real-time data pipelines",
            "R",
            "sensor data",
            "Statistical Techniques",
            "Statistics",
            "Optimization",
            "unsupervised ML methods",
            "Image Processing",
            "Data Visualization",
            "Machine Learning",
            "AWS",
            "data mining",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Qualys",
        "experience": "2-5 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!\n\nJob Description\n\nWe are seeking a Data Scientist to develop next-generation Security Analytics products. You will work closely with engineers and product managers to prototype, design, develop, and optimize data-driven security solutions.\n\nAs a Data Scientist, you will focus on consolidating and analyzing diverse data sources to extract meaningful insights that drive product innovation and process optimization. The ideal candidate has a strong background in quantitative analysis, machine learning, and data-driven decision-making, with experience handling large datasets.\n\nResponsibilities:\n\nConduct exploratory data analysis (EDA) and apply statistical methods to extract actionable insights from complex datasets.\nCollaborate with Product Management and cross-functional stakeholders to define problem statements, develop solution strategies, and design scalable ML systems.\nDesign, develop, and deploy Machine Learning models, including traditional ML, Deep Learning, and Large Language Models (LLMs), to drive business impact.\nCreate insightful data visualizations, technical reports, and presentations to communicate findings to technical and non-technical audiences.\nDeploy ML models in production and implement monitoring frameworks to ensure model performance, stability, and continuous improvement.\n\nRequirements:\n\n\n2-5 years of experience in Machine Learning projects, including model development, deployment, and optimization.\nBS, MS, or Ph.D. in Computer Science, Statistics, or a related field.\nStrong communication, problem-solving, and analytical skills; a collaborative team player.\nDeep understanding of ML algorithms, their mathematical foundations, and real-world trade-offs.\nExpertise in at least two or more ML domains, including Linear Models, Tree-based Models, Traditional NLP, LLMs, Numerical optimization and Reinforcement Learning.\nHands-on experience with ML frameworks such as Scikit-Learn, TensorFlow, PyTorch, LangChain etc.\nStrong programming skills in Python and/or Java, R, Go, Scala.\nExperience in SQL, Pandas, and PySpark for efficient data manipulation.\nFamiliarity with microservice architectures, CI/CD, MLOps best practices.\n\nNice to Have:\n\n\nGenerative AI: Experience with fine-tuning and optimizing LLMs.\nFamiliarity with distributed computing frameworks such as Hadoop, Spark, and OpenSearch.\nPublished research in AI/ML in peer-reviewed journals or top conferences (e.g., NeurIPS, ICML, CVPR).\nPrior experience applying AI/ML to cybersecurity use cases.\nBasic proficiency in Unix/Linux environments for scripting and automation.",
        "skills": [
            "LangChain",
            "microservice architectures",
            "Go",
            "Scikit-Learn",
            "R",
            "Java",
            "Machine Learning",
            "Pyspark",
            "Scala",
            "Sql",
            "Deep Learning",
            "Tensorflow",
            "Pandas",
            "Pytorch",
            "MLops",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "NTT DATA, Inc.",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Make an impact with NTT DATA\n\nJoin a company that is pushing the boundaries of what is possible. We are renowned for our technical excellence and leading innovations, and for making a difference to our clients and society. Our workplace embraces diversity and inclusion it's a place where you can grow, belong and thrive.\n\nYour day at NTT DATA\n\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat You'll Be Doing\n\nKey Responsibilities:\n\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nKnowledge and Attributes:\n\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic Qualifications and Certifications:\n\nBachelor's degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired Experience:\n\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.\n\nWorkplace type:\n\nHybrid Working\n\nAbout NTT DATA\n\nNTT DATA is a $30+ billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long-term success. We invest over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure, and connectivity. We are also one of the leading providers of digital and AI infrastructure in the world. NTT DATA is part of NTT Group and headquartered in Tokyo.\n\nEqual Opportunity Employer\n\nNTT DATA is proud to be an Equal Opportunity Employer with a global culture that embraces diversity. We are committed to providing an environment free of unfair discrimination and harassment. We do not discriminate based on age, race, colour, gender, sexual orientation, religion, nationality, disability, pregnancy, marital status, veteran status, or any other protected category. Join our growing global team and accelerate your career with us. Apply today.",
        "skills": [
            "visualization techniques",
            "R",
            "statistical models",
            "data mining",
            "Python",
            "Sql",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Data Scientist (AXCCIO)",
        "company_name": "CryptoChakra",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Description\nCryptoChakra is a crypto price prediction and education portal currently in its development phase. We are rapidly growing and looking to expand significantly. Our mission is to provide accurate crypto price predictions and comprehensive educational resources for the crypto community.\nRole Description\nThis is a remote internship role for a Data Scientist (AXCCIO) at CryptoChakra. The intern will be responsible for day-to-day tasks such as analyzing data sets, developing statistical models, performing data visualization, and conducting data analysis. The role also involves gaining insights from data to support crypto price predictions and contributing to educational content.\nQualifications\nProficiency in Data Science, Data Analysis, and Data Analytics\nStrong skills in Statistics and developing statistical models\nExperience in Data Visualization tools and techniques\nExcellent analytical and problem-solving skills\nCapability to work independently and in a remote environment\nUnderstanding of crypto markets is a plus\nPursuing or completed a degree in Data Science, Statistics, Mathematics, Computer Science, or a related field",
        "skills": [
            "Statistical Models",
            "Data Analysis",
            "Statistics",
            "Data Science",
            "Data Visualization",
            "Data Analytics"
        ]
    },
    {
        "job_title": "Senior Data Scientist (Machine Learning Engineer) - Bangalore",
        "company_name": "technology",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "At Nielsen, we are passionate about our work to power a better media future for all people by providing powerful insights that drive client decisions and deliver extraordinary results. Our talented, global workforce is dedicated to capturing audience engagement with content - wherever and whenever it's consumed. Together, we are proudly rooted in our deep legacy as we stand at the forefront of the media revolution. When you join Nielsen, you will join a dynamic team committed to excellence, perseverance, and the ambition to make an impact together. We champion you, because when you succeed, we do too. We enable your best to power our future.\n\nJob Summary\n\nNielsen is a leading online, radio and television research company specializing in industry leading measurement solutions that provide clients with a comprehensive understanding of the online world. Our Data Science team is experimenting, testing, and driving major insights that impact both a global network of clients and our own Nielsen direction. As a Data Scientist you will be responsible for high-quality design, execution and delivery of global analytics solutions, performance dashboards, and analytical studies. Excited Come join us!\n\nThe Advanced Audience Team focuses on methodologies and pipelines that measure how effectively these campaigns reach targeted audiences, as defined by the client, or via demographics, or even pre-defined audiences from our partners.\n\nThe team builds and maintains robust data pipelines that support the advanced analytics needs of Nielsen's audience measurement products that provide value for clients.\n\nThe role entails ensuring data quality and accessibility, optimizing data flow and storage, and collaborating with other data scientists to implement scalable solutions for complex data challenges.\n\nResponsibilities\n\nDesign, develop, optimize, and maintain robust data architecture, methodologies, and pipelines, aligning with ETL principles and Nielsen's business objectives.\nRefactor research software prototypes to production grade standards, improving performance and reliability.\nTackle complex data challenges to deliver actionable insights, aiding in the achievement of organizational goals and meaningful client impact.\nDevelop data science methodologies and solutions that solve key client problems, and that are reproducible and scalable.\nEnsure the integration of disparate data sources into a cohesive and efficient data ecosystem, facilitating seamless data accessibility and interoperability for various analytical platforms and stakeholders throughout Nielsen.\nOffer mentorship, advice, and coaching to other professionals in data and analytics on best practices and standards.\nLead in the evaluation and deployment of cutting-edge methodologies and processes in data science, boosting the team's overall efficiency and effectiveness.\nCollaborate with business analysts and solutions architects to devise technical architectures for key enterprise projects and initiatives that improve client experience.\nEngage in continuous learning in areas like engineering, machine learning and data science.\n\nQualifications\n\nMaster's degree in Data Science, Computer Science, Statistics or Mathematics, Operation Research, or other hard sciences) with outstanding analytical expertise and strong technical leadership skills.\nExcellent software engineering fundamentals and debugging skills.\nSignificant experience with collaborative code development, including version control, unit/integration testing, code review and sharing.\nSignificant experience working in a cloud-based environment, ideally AWS.\nStrong machine learning fundamentals.\nProficient in Python.\nHave a thirst for new challenges. The candidate should be comfortable diving into the unknown and making sense of what's noise and what isn't.\nHave an eye for design at multiple levels: Infrastructure architecture (cloud infrastructure), ETL data pipelines, and analytic pipelines.\nHave Experience building analytic methodologies: either from scratch, using existing software libraries and APls, and/or combinations of the two.\nFeel comfortable and excited to bring innovative ideas and challenge existing methods and processes. This requires the ability to pick apart the implementation details from the overall design.\nHelp incubate a forward-thinking culture in a growing data-science organization. This includes prioritizing staying up-to-date with new software, methodologies, and industry research and facilitating the cross pollination of ideas across individuals and teams.\nBe self-motivated with a keen problem-solving aptitude and a continuous learning mindset.\nDemonstrates a strong work ethic, capable of working abstractly.\nStrong communication (verbal and written) and presentation skills in English.\n\nPlease be aware that job-seekers may be at risk of targeting by scammers seeking personal data or money. Nielsen recruiters will only contact you through official job boards, LinkedIn, or email with a nielsen.com domain. Be cautious of any outreach claiming to be from Nielsen via other messaging platforms or personal email addresses. Always verify that email communications come from an @ nielsen.com address. If you're unsure about the authenticity of a job offer or communication, please contact Nielsen directly through our official website or verified social media channels.",
        "skills": [
            "Data architecture methodologies",
            "ETL principles",
            "Analytic methodologies",
            "Collaborative code development",
            "Data pipelines",
            "Analytical platforms",
            "Unit Integration Testing",
            "Machine Learning",
            "Version Control",
            "Data Science",
            "Code Review",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - Generative AI",
        "company_name": "Respironics Inc",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Pharmaceutical",
        "job_description": "Job Title\nData Scientist - Generative AI\nJob Description\nWe are seeking a highly skilled Data Scientist with Generative AI expertise to join our team. In this role, you will develop, fine-tune, and optimize generative AI models to drive innovation across various applications, including text, image, audio, and video generation. You will collaborate with cross-functional teams, including machine learning engineers, software developers, and product managers, to create cutting-edge AI solutions.\nKey Responsibilities\nProficient in Python\nUnderstanding of the Pytorch and TensorFlow frameworks\nUnderstanding the NLP techniques and text processing tools such as NLTK, Gensim etc.\nProficient with LLM models from HuggingFace for different text processing tasks\nProficient with data handling capabilities and querying from SQL/No-SQL databases\nUnderstanding of the vision problems that can be solved using generative AI models\nExperience with prompt engineering techniques and leverage best practices with LLM technology\nDevelop and Optimize Generative AI Models - Design, train, and fine-tune models such as GPT, Stable Diffusion, StyleGAN, and LLaMA for real-world applications.\nData Preparation & Engineering - Gather, clean, and preprocess large-scale datasets for training and evaluation of generative models.\nExperimentation & Model Evaluation - Conduct A/B testing and assess model performance using quantitative metrics (e.g., FID, BLEU, perplexity, ROUGE etc.).\nResearch & Innovation - Stay updated with the latest advancements in generative AI, deep learning, and large language models (LLMs).\nDeployment & Scaling - Work with ML engineers to deploy models into production environments using cloud platforms (AWS, Azure) and frameworks like TensorFlow, PyTorch, and Hugging Face.\nCollaboration & Communication - Work closely with cross-functional teams to integrate AI solutions into business processes and applications.\nMinimum Qualifications:\nBachelor's or master's degree in computer science, AI, Data Science, Machine Learning, or a related field.\n3+ years of experience in machine learning, deep learning, or AI research, with at least 1 year of hands-on experience in generative AI.\nStrong proficiency in Python and ML frameworks like TensorFlow, PyTorch, or JAX.\nExperience working with LLMs, diffusion models, GANs, VAEs, or transformers.\nKnowledge of natural language processing (NLP), computer vision, or multimodal AI applications.\nFamiliarity with prompt engineering, fine-tuning, and RLHF (Reinforcement Learning from Human Feedback).\nExperience in cloud-based AI solutions and working with APIs (e.g., OpenAI, Hugging Face, Stability AI).\nStrong problem-solving skills and the ability to work in a fast-paced, research-driven environment.\nExperience with vector databases (e.g., FAISS, Pinecone) and retrieval-augmented generation (RAG).\nHands-on experience with MLOps tools (e.g., MLflow, Kubeflow, Docker, Kubernetes).\nUnderstanding of ethical AI and bias mitigation in generative models.\nStrong publication record or contributions to open-source AI projects.\n#LI-EU\n#LI-Hybrid\n#LI-PHILIN\nHow we work together\nWe believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.\nthis role is an office role.\n\nAbout Philips\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.\n. Learn more about .\n. Discover .\n. Learn more about .\n\nIf you're interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our culture of impact with care .",
        "skills": []
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "NTT DATA, Inc.",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Make an impact with NTT DATA\n\nJoin a company that is pushing the boundaries of what is possible. We are renowned for our technical excellence and leading innovations, and for making a difference to our clients and society. Our workplace embraces diversity and inclusion it's a place where you can grow, belong and thrive.\n\nYour day at NTT DATA\n\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat You'll Be Doing\n\nKey Responsibilities:\n\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nKnowledge and Attributes:\n\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic Qualifications and Certifications:\n\nBachelor's degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired Experience:\n\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.\n\nWorkplace type:\n\nHybrid Working\n\nAbout NTT DATA\n\nNTT DATA is a $30+ billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long-term success. We invest over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure, and connectivity. We are also one of the leading providers of digital and AI infrastructure in the world. NTT DATA is part of NTT Group and headquartered in Tokyo.\n\nEqual Opportunity Employer\n\nNTT DATA is proud to be an Equal Opportunity Employer with a global culture that embraces diversity. We are committed to providing an environment free of unfair discrimination and harassment. We do not discriminate based on age, race, colour, gender, sexual orientation, religion, nationality, disability, pregnancy, marital status, veteran status, or any other protected category. Join our growing global team and accelerate your career with us. Apply today.",
        "skills": [
            "visualization techniques",
            "Analytics",
            "R",
            "statistical models",
            "Data Science",
            "data mining",
            "Sql",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Recro",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job title: Data Scientist for AI products (Global)\nLocation: Bengaluru, India\nAre you ready to take the lead\nLeading global industrial gases and engineering company that provides high-quality solutions\nand services to a variety of end markets, including chemicals & energy, food & beverage, electronics, healthcare, manufacturing, metals, and mining. Industrial gases are used in countless applications, from life-saving oxygen for hospitals to high-purity & specialty gases for electronics manufacturing, hydrogen for clean fuels and more also delivers state-of-the-art gas processing solutions to support customer expansion, efficiency improvements, and emissions reductions.\nWhat you will enjoy doing\nAs a Data Scientist AI you will support AI team with extending existing and building\nnew AI products for a vast amount of uses cases across business and value chain\nYou will work directly with a variety of different data sources, types and structures to derive\nactionable insights\nDevelop, customize and manage AI software products based on Machine and Deep\nLearning backends will be your tasks\nYour role includes strong support on replication of existing products and pipelines to other\nsystems and geographies\nIn addition to that you will support in architectural design and defining data requirements\nfor new developments\nIt will be your responsibility to interact with business functions in identifying\nopportunities with potential business impact and to support development and deployment\nof models into production\nYou'll be working in the Artificial Intelligence team,s AI global corporate division engaged with\nreal business challenges and opportunities in multiple countries. Focus of this role is to support the\nAI team with extending existing and building new AI products for a vast amount of uses cases across business and value chain. You'll collaborate across different business and corporate functions in international team composed of Project Managers, Data Scientists, Data and Software Engineers in the AI team and others in the Global AI team.\nWhat Makes You Great\nYou have a Bachelor or Masters Degree in Data Science, Computational\nStatistics/Mathematics, Computer Science, Operations Research or related field\nFurther to that you have a strong understanding of and practical\nexperience with Multivariate Statistics, Machine Learning and Probability concepts\nYou gained experience in articulating business questions and using quantitative techniques\nto arrive at a solution using available data\nYou demonstrate hands-on experience with preprocessing, feature engineering, feature\nselection and data cleansing on real world datasets\nPreferably you have work experience in an engineering or technology role\nYou bring a strong background of Python and handling large data sets using SQL in a business\nenvironment (pandas, numpy, matplotlib, seaborn, sklearn, keras, tensorflow, pytorch,\nstatsmodels etc.) to the role\nYou have a sound knowledge of data architectures and concepts and practical experience in\nthe visualization of large datasets, e.g. with Tableau or PowerBI\nResult driven mindset and excellent communication skills with high social competence gives\nyou the ability to structure a project from idea to experimentation to prototype to\nimplementation\nVery good English skills\nAs a plus you have hands-on experience with DevOps and MS Azure, experience in Azure ML,\nKedro or Airflow, experience in MLflow or similar\nPreferred Qualifications:\nExperience working with data scientists, ML engineers, and developers.\nFamiliarity with IT administration and data security practices.\nKnowledge of stakeholder management in large, international organizations.",
        "skills": [
            "Airflow",
            "MLflow",
            "probability",
            "multivariate statistics",
            "Kedro",
            "statsmodels",
            "Machine Learning",
            "Sklearn",
            "Tableau",
            "Sql",
            "Ms Azure",
            "Devops",
            "Azure ML",
            "Seaborn",
            "Pandas",
            "Numpy",
            "Powerbi",
            "Matplotlib",
            "Keras",
            "Tensorflow",
            "Python",
            "Pytorch"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "MathCo",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "As a Senior Data Scientist, you will apply your expertise in statistics, machine learning, and data analysis to extract valuable insights from structured and unstructured data. You will work closely with cross-functional teams to develop predictive models, improve business processes, and contribute to data-driven decision-making across the organization.\n\nTech:\n\nRequired Skills (Must have)\n\nAdvanced programming knowledge in Python and SQL.\nAdvanced knowledge in Probability and Statistics, including hypothesis testing.\nAdvanced knowledge in Practical Machine Learning and awareness of the key-pitfalls in the practice of machine learning and approaches to addressing them.\nAdvanced knowledge of data visualization technologies like Tableau, and PowerBI, and comfortable using relevant libraries in Python like seaborn and matplotlib.\nExperienced in modern development tools and writing code collaboratively.\nIntermediate knowledge of Cloud technologies and experience in developing data science solutions in one or more cloud platforms.\nExperienced in modern development tools, writing code collaboratively and developing solutions on cloud platforms.\n\nNon Tech :\n\nAbility to recognize and pursue pragmatic alternatives vis--vis a perfect solution, balancing priorities of time with potential business impact.\nPlan projects, break them down across individual data scientists in the team, track their performance and manage risks.\nAbility to storyboard an entire presentation to a non-technical audience.\nAbility to work independently to develop data science solutions, while also being able to work as part of a team to communicate findings and collaborate on solutions.\nStrong written skills. This is required for submitting technical papers, whitepapers, and developing project documentations.\nTechnical leadership and mentorship to the community of data scientists in the organization.\n\nTech:\n\nRequired Skills (Good to have)\n\nAdvanced knowledge in one or more areas besides Machine Learning Operations Research, Natural Language Processing, Deep learning and its applications, Time Series forecasting at scale. Reinforcement Learning, Graph Machine Learning, Explainable Machine learning.\nAdvanced understanding of Cloud technologies and experience of deploying applications on cloud.\n\nNon Tech:\n\nAbility to solution critical business problems into its component parts and match each such part with an appropriate technical approach.",
        "skills": [
            "Probability and Statistics",
            "Matplotlib",
            "Machine Learning",
            "Cloud Technologies",
            "Seaborn",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Open CV)",
        "company_name": "Evnek",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist (OpenCV)\n\nExperience: 01 Year\n\nLocation: Bangalore\n\nNotice Period: Immediate Joiner\n\nJob Summary:\n\nWe are seeking a highly motivated and enthusiastic Data Scientist with a passion for computer vision and hands-on experience in OpenCV. This is an excellent opportunity for recent graduates or early-career professionals to contribute to real-world projects involving image processing, computer vision, and machine learning.\n\nKey Responsibilities:\n\nWork on image and video processing projects using OpenCV and Python.\nAssist in developing computer vision models for object detection, recognition, and tracking.\nPreprocess and analyze image datasets to extract meaningful insights.\nCollaborate with senior data scientists and engineers to integrate models into production environments.\nConduct experiments and contribute to research-oriented tasks in the field of computer vision.\nDocument processes, models, and results for internal review and external presentation.\n\nRequired Skills:\n\nStrong knowledge of OpenCV and image processing techniques.\nProficiency in Python and related libraries (e.g., NumPy, Pandas, Matplotlib).\nUnderstanding of basic machine learning and deep learning concepts.\nFamiliarity with tools like Jupyter Notebooks, TensorFlow, or PyTorch is a plus.\nGood problem-solving skills and attention to detail.\nExcellent communication and teamwork skills.\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Data Science, Electrical Engineering, or a related field.\nInternship or academic project experience in computer vision or image processing.\nExposure to cloud platforms (AWS, GCP, Azure) is a plus.\n\nWhat We Offer:\n\nA collaborative and learning-driven environment.\nMentorship from experienced professionals.\nOpportunity to work on innovative projects with real-world impact.\nCompetitive salary and benefits.",
        "skills": [
            "Jupyter Notebooks",
            "Deep Learning",
            "Matplotlib",
            "Tensorflow",
            "Numpy",
            "Machine Learning",
            "Pytorch",
            "Pandas",
            "Opencv",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Jet2 Travel Technologies Pvt Ltd.",
        "experience": "Fresher",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Senior Data Scientist will be a key part of the Data Science Team, executing the data science strategy and bringing together multiple large data sources for effective use across the business.\nYou will create algorithms and models to understand how changes in metrics in one area of the business impact other areas, always working with multiple teams across the business to deliver the data science roadmap.\nYou will collaborate multiple internal teams viz data engineering, product owners, business intelligence teams to gauge the data availability, quality, and architecture.",
        "skills": [
            "Models",
            "business intelligence",
            "Algorithms",
            "Data Science",
            "data engineering"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Talent Worx",
        "experience": "Fresher",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Summary:\n\nWe are seeking an experienced Data Scientist with a strong background in analytics, big data management, and advanced data science methodologies. The ideal candidate will have expertise in working with large datasets and proficiency in a wide range of tools and technologies including SQL, Python, R, SPSS, Artificial Intelligence/Machine Learning, Advanced Excel, Tableau, Power BI, Linux, and Cybersecurity. The candidate will be responsible for leveraging data to provide actionable insights and drive business strategies.\n\nRequirements\n\nKey Responsibilities:\n\nManage and analyze large-scale datasets, ensuring effective cleaning, processing, and interpretation of complex data.\n\nUtilize SQL, Python, and R to build and refine data models, run statistical analyses, and conduct advanced data analysis.\n\nApply Artificial Intelligence and Machine Learning algorithms to solve business problems and create predictive models.\n\nDesign and develop interactive dashboards and visualizations using Tableau and Power BI to communicate insights effectively to stakeholders.\n\nConduct Statistical Analysis Using SPSS And Advanced Excel Functions.\n\nWork with cross-functional teams to understand data needs and deliver data-driven solutions.\n\nProvide technical guidance and mentorship to junior team members.\n\nEnsure compliance with Cyber Security standards in data management and processing.\n\nStay updated with the latest advancements in data science, AI, ML, and related technologies.\n\nRequired Qualifications:\n\nEducational Background:\n\nB.Tech (CS/IT), B.E. (CS/IT), M.Tech, MCA, MBA (IT), M.Sc. (IT), B.Sc. Data Science, M.Sc. Data Science, BA Economics, or MA Economics from a recognized University or Institution.\n\nTechnical Skills:\n\nProficiency in SQL for data querying and management.\n\nExpertise in Python and R for data analysis and machine learning.\n\nStrong knowledge of SPSS for statistical analysis.\n\nProficiency in creating data visualizations and dashboards using Tableau and Power BI.\n\nAdvanced Skills In Excel For Data Manipulation And Analysis.\n\nDeep understanding of Artificial Intelligence (AI) and Machine Learning (ML) techniques.\n\nExperience With Linux For Handling Data Operations.\n\nKnowledge of Cyber Security practices and data protection principles.\n\nBenefits\n\n\nWe're hiring for one of the world's leading professional services organizations part of the prestigious Big 4 known for setting global standards in consulting, advisory, audit, risk, tax, and technology services.\n\nThis firm works with some of the biggest brands and governments worldwide, helping them solve complex challenges, drive innovation, and build sustainable futures.\n\nIf you're passionate about working in a fast-paced environment, love solving problems, and are excited about learning from the best minds in the industry, this is your opportunity.\n\nJoin a culture that values collaboration, continuous growth, diversity, and making a real-world impact across industries.",
        "skills": [
            "R",
            "Machine Learning",
            "Linux",
            "Artificial Intelligence",
            "Power Bi",
            "Spss",
            "Advanced Excel",
            "Cybersecurity",
            "Tableau",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kraft Heinz",
        "experience": "Fresher",
        "salary": null,
        "location": "Ahmedabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nJob Title Data Scientist Operations\n\nLocation Kraft Heinz Global Capability Center (GCC), Ahmedabad\n\nAbout Kraft Heinz\n\nAt Kraft Heinz, we are revolutionizing the food and beverage industry by leveraging data and innovation to deliver exceptional value to our customers. Our Global Capability Center (GCC) in Ahmedabad serves as a critical hub for operational excellence, driving efficiency and innovation across the organization.\n\nRole Overview\n\nThe Data Scientist Operations will play a key role in transforming operational processes through advanced analytics and data-driven decision-making. This role focuses on optimizing supply chain, manufacturing, and overall operations by developing predictive models, streamlining workflows, and uncovering insights to enhance efficiency and reduce costs.\n\nKey Responsibilities\n\nAdvanced Analytics and Data Modeling\nDevelop predictive models for demand forecasting, inventory optimization, and supply chain resilience.\nLeverage machine learning techniques to optimize production schedules, logistics, and procurement.\nBuild algorithms to predict and mitigate risks in operational processes.\nOperational Efficiency\nAnalyze manufacturing and supply chain data to identify bottlenecks and recommend process improvements.\nImplement solutions for waste reduction, cost optimization, and improved throughput.\nConduct root cause analysis for operational inefficiencies and develop actionable insights.\nCollaboration with Stakeholders\nPartner with operations, supply chain, and procurement teams to understand analytical needs and deliver insights.\nCollaborate with IT and data engineering teams to ensure data availability and accuracy.\nPresent findings and recommendations to non-technical stakeholders in an accessible manner.\nData Management and Tools\nWork with large datasets to clean, preprocess, and analyze data\n\nLocation(s)\n\nAhmedabad - Venus Stratum GCC\n\nKraft Heinz is an Equal Opportunity Employer Underrepresented Ethnic Minority Groups/Women/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity and other protected classes.",
        "skills": [
            "Inventory Optimization",
            "Root Cause Analysis",
            "Predictive Models",
            "Demand Forecasting",
            "Machine Learning",
            "Data Management",
            "Data Modeling",
            "Advanced Analytics"
        ]
    },
    {
        "job_title": "Data Scientist - Mumbai",
        "company_name": "Modulr",
        "experience": "Fresher",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Modulr\n\nAt Modulr our mission is to move money efficiently to power business productivity. We are building a new digital payments service to make money flow more efficiently through businesses and the economy. This is a new type of payments account that is built for businesses that need faster, easier and more reliable ways to move money.\n\nOur flexible platform and innovate approach allows our customers to utilise our payments account instead of a traditional bank account. Our accounts come with a sort code and account number, access to payment systems (eg Faster Payments and Visa) and everything our customers need from a payments account. They can instantly open as many accounts as they need, automate how they make and receive payments as well as the reconciliation processes these create. All in real time, 24/7 through their own systems utilising our API that integrates into any platform.\n\nOur founding team has a wealth of experience in the payments industry and growing successful businesses. Modulr is backed by payments giants PayPal Ventures and FIS Ventures, as well as startup and scale up specialists Blenheim Chalcot, General Atlantic, Frog and Highland Europe which enables us to leverage their resources and expertise to drive our growth initiatives. These partnerships provide us with the necessary provisions to propel our business to new heights.\n\nModulr values\n\nBuilding the extraordinary and going that extra mile\nOwning the opportunity, be passionate and proud of the time you invest\nMove at pace, reach your goals faster with us supporting you each step of the way\nAchieve it together, working as a team and being Modulite\n\nIn this role you will work in a cross-functional team who are asked to solve a problem, rather than be handed a task to do. This is an excellent opportunity to work in a high-growth environment with a fast-paced and collaborative culture where you will have great opportunities to work on challenging problems. You will be employed by Blenheim Chalcot supporting Modulr with their exciting growth plans.\n\nThe Tech Team\n\nAt Modulr we are focused on building autonomous squads that are focused on working in the best possible way. We operate in a tribes and squad model with each area focused on a particular growth area. Each squad is small and cross-functional. We have big ambitions and have many interesting challenges ahead. Our strategy is to provide our customers with seamless connectivity into payment schemes both domestically and overseas no matter where they're based.\n\nAbout the role:\n\nDesign, develop, analyse, document and support testing of products, systems or subsystems\nCoordinate with other teams at sub-system and system level to identify software needs and solutions\nImprove engineering standards, tooling, and processes\nDevelop and execute test procedures for software\nProactively identify issues, bottlenecks, gaps, or other areas of concerns or opportunities and work to either directly affect change, or advocate for that change\nWork in an agile environment - Scrum or Kanban\nDocument best practices, guides, systems design, reference architectures and implementations\nParticipate in design and code reviews\nContribute to the evolution of our architecture to make it more flexible and easier to use\n\nAbout You\n\n\nClear and responsive communication\nStrong analytical thinking, problem-solving, appropriate judgment and decision-making skill\nCommitment to collaboration and teamwork\nProven ability to effectively manage timelines and deliverables\nMeeting deadlines and strong attention to detail\nYou are a self-started, motivated team player who thrives in a fast-paced environment\n\nTechnical Knowledge Required\n\n\nExpert in Java (or similar object-oriented languages willing to learn Java)\nOpen-Source technologies, such as Spring and Hibernate.\nDemonstrated ability to work with Microservices\nOpen-source RDBMS technologies such as PostgreSQL.\nStrong understanding of OOP and design patterns.\nExperience of REST API design\n\nModInclusion\n\n\nWe believe that by seeing Modulr, and the world, from all sorts of angles, we can make life better for all. We want you to know that the things that make you, you like your identity, age, ability, and background are things that we will always celebrate and support with open arms. As such, we are keen to maximise the diversity of our workforce and actively encourage applications from anyone and everyone.\n\nBy submitting your CV you understand that we have a legitimate interest to use your personal data for the purposes of assessing your eligibility for this role. This means that we may use your personal data to contact you to discuss your CV or arrange an interview, or transfer your CV to the hiring manager(s) of the role you have applied for. You can ask us at any time to remove your CV from our database by emailing [HIDDEN TEXT] but please note that this means we will no longer consider you for the role you have applied for.",
        "skills": [
            "REST API design",
            "Java",
            "Oop",
            "Hibernate",
            "Design Patterns",
            "PostgreSQL",
            "Microservices",
            "Spring"
        ]
    },
    {
        "job_title": "Data Scientist (Open CV)",
        "company_name": "Evnek",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist (OpenCV)\n\nExperience: 01 Year\n\nLocation: Bangalore\n\nNotice Period: Immediate Joiner\n\nJob Summary:\n\nWe are seeking a highly motivated and enthusiastic Data Scientist with a passion for computer vision and hands-on experience in OpenCV. This is an excellent opportunity for recent graduates or early-career professionals to contribute to real-world projects involving image processing, computer vision, and machine learning.\n\nKey Responsibilities:\n\nWork on image and video processing projects using OpenCV and Python.\nAssist in developing computer vision models for object detection, recognition, and tracking.\nPreprocess and analyze image datasets to extract meaningful insights.\nCollaborate with senior data scientists and engineers to integrate models into production environments.\nConduct experiments and contribute to research-oriented tasks in the field of computer vision.\nDocument processes, models, and results for internal review and external presentation.\n\nRequired Skills:\n\nStrong knowledge of OpenCV and image processing techniques.\nProficiency in Python and related libraries (e.g., NumPy, Pandas, Matplotlib).\nUnderstanding of basic machine learning and deep learning concepts.\nFamiliarity with tools like Jupyter Notebooks, TensorFlow, or PyTorch is a plus.\nGood problem-solving skills and attention to detail.\nExcellent communication and teamwork skills.\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Data Science, Electrical Engineering, or a related field.\nInternship or academic project experience in computer vision or image processing.\nExposure to cloud platforms (AWS, GCP, Azure) is a plus.\n\nWhat We Offer:\n\nA collaborative and learning-driven environment.\nMentorship from experienced professionals.\nOpportunity to work on innovative projects with real-world impact.\nCompetitive salary and benefits.",
        "skills": [
            "Jupyter Notebooks",
            "Deep Learning",
            "Matplotlib",
            "Tensorflow",
            "Numpy",
            "Machine Learning",
            "Pytorch",
            "Pandas",
            "Opencv",
            "Python"
        ]
    },
    {
        "job_title": "Data Analyst / Operations Research Data Scientist",
        "company_name": "Recro",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Data Analyst / Operations Research Data Scientist\nMain skill requirements:\nProven knowledge of coding in Python\nAdvanced knowledge of regression & linear optimization (Python based, relevant libraries:\npandas, numpy, scikit-learn, or-tools)\n2+ years of working experience in data analytics with proven project/solution track record\nSkills in data analysis & visualization (Python based, relevant libraries: pandas, numpy, plotly)\nExperience as Operations Research analyst or similar: worked with optimisation models\n(e.g. Integer and Linear Programming) in the past\nProjects related with energy markets\nWorked in similar/ relevant industries: gas/ oil/ energy\nMain responsibilities:\nData Analysis and Preprocessing: Adapt process plant model structures based on specific\nplant data. Analyze, validate and clean past operation data, validate operation limits, and\nupdate affected regression calculations\nModel Validation and Testing: Run the models against past operations data, ensuring\ncompliance with upper and lower limits, production ramping speeds, and other\nparameters. Test optimization models against historical data and the plant model and\noptimise it.\nTechnical Integration: Coordinate the technical integration process with the business,\nproject management and other involved parties. Ensure working integration to adjacent\nsystems (e.g. Historian database, plant control system, etc.\nReport insights to stakeholders // Communication and Coordination: Engage in effective\ncommunication with business and ROC (Remote Operations Center) regarding process\nparameters, constraints, and test results. Support business value estimation by running\nscenarios against simulations and past data.\nTest Management and Continuous Optimization: Manage tests, ensuring execution and\nthorough analysis. Establish and maintain optimization models to run continuously. Monitor\nrun performance and take proactive actions if runs deviate from expectations.\nSoft skills:\nSelf driver attitude: Execute tasks self-responsible. Can-do and get-stuff-done attitude is\ncrucial\nGood communications skills, able to effectively discuss project requirements with different\nhierarchy levels and disciplines (engineering, business, etc.)\nFluent English (spoken and written) is a must, Portuguese is a plus\nMain working location will be Portugal (Lisbon)\nWillingness for business travel (on an occasional basis)",
        "skills": [
            "scikit-learn",
            "plotly",
            "or-tools",
            "Pandas",
            "Numpy",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Maximus",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "We are seeking a detail-oriented and analytical Data Science Analyst to join our team. In this role, you will leverage data-driven insights to support decision-making, optimize business processes, and enhance operational efficiency.You will be responsible for collecting, analyzing, and interpreting large datasets, building predictive models, and presenting findings to stakeholders.\nKey Responsibilities:\n- Collect, clean, and preprocess data from multiple sources to ensure accuracy and reliability.\n- Conductexploratory data analysis (EDA)to identify trends, patterns, and insights.\n- Develop andimplement predictive models and machine learning algorithmsto solve business problems.\n- Utilizestatistical methods and data visualization toolsto communicate findings effectively.\n- Collaborate with cross-functional teams, including business, engineering, and product teams, to drive data-driven decision-making.\n- Build and maintaindashboards and reports to monitor key performance indicators (KPIs).\n- Perform A/B testing and other experimental analysis to optimize strategies.\n- Stay up to date with industry trends, emerging technologies, and best practices in data science.\nRequired Qualifications & Skills:\n- Bachelors or masters degree in data science, Statistics, Computer Science, Mathematics, or a related field.\n- Proficiency in programming languages such as Python, R, or SQL.\n- Experience withAWS SageMaker, Jupyter Notebooks\n- Experience withdata visualization toolslike AWS QuickSight, Tableau, Power BI, or Matplotlib.\n-Strong understanding of statistical analysis, hypothesis testing, and machine learning techniques.\n- Familiarity with big data technologies and cloud platforms (e.g., AWS, Google Cloud, Azure) is a plus.\n- Ability to translate complex data into actionable business insights.\n- Strong problem-solving skills and attention to detail.\n- Excellent communication and presentation skills.\nPreferred Qualifications:\n- 7+ years of experience\n- Knowledge of deep learning frameworks (e.g., TensorFlow, PyTorch) is beneficial.\n- Exposure to data engineering concepts, including ETL processes and database management.",
        "skills": [
            "Tensorflow",
            "Predictive Modeling",
            "Eda",
            "Data Modeling",
            "Data Visualization",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Kolkata",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Chubb Business Services India Private Limited",
        "experience": "4-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Insurance",
        "job_description": "Address model drift and performance of business requirements as part of recurring operations, drive quantitative model maintenance prioritization, and inform stakeholders.\nUnderstand broad spectrum of Machine Learning and statistical models, impactful features from structured/unstructured data.\nUnderstand and have experience in models that help automation, getting insights, make smart decisions, are able to meet the desired KPIs post-production and predictive modelling in Python, Azure (optional), Insurance(optional)\nStay abreast of the developments in the rapidly changing field of AI and machine learning\nGain deeper understanding of Chubbs business and develop expertise on multiple lines of business\nCollaborate with business partners and peers within the organization to understand the machine learning models and develop robust model monitoring solutions that ensure data quality and analytic accuracy.\nExecute all aspects of analytics model monitoring including exploratory data analysis, data processing, and model monitoring.\nAbility to identify trends and outliers in model results and analyze drift and impact to business value.\nResearch, recommend, and implement statistical and other mathematical methodologies appropriate for model monitoring.\nCreate/Maintain excellent working relationships with business partners across the Chubb organization\nProvide supporting documentation for the models being monitored including documentation of methodologies used and data issues encountered.\nQualifications\nRequired:\nSome experience utilizing model monitoring methodologies.\nUnderstanding of data mining predictive modeling, and ML concepts/ Probabilistic Models, Ensemble Techniques, Hyperparameter Optimization, machine learning models/sound knowledge of Regression/ Logistic Regression, Random forest, XGBoost, SVM, Clustering, statistical modelling (GLMs in particular), etc.\nProficient in Python with strong capabilities in data analysis/modelling\n4-8 years programming experience in using Python libraries / machine learning libraries.\nStrong Programming experience in Python.\nStrong Programming experience in SQL.\nThe ability to multi-task, learn new things quickly, and demonstrate excellent problem-solving skills.\nPreferred:\nWorking knowledge/familiarity with Git version control.\nPractical experience with MS Azure and Databricks.\nExperience in architecting and consuming APIs at scale.\nDesirable to have experience in visualization tools like Qlik, Power BI, Tableau etc.\nExperience with Text Analytics and Natural Language Processing. Experience with BERT models/understanding of vector embeddings and operations on unstructured data.\nNice to have:\nAdvanced knowledge of model tuning, evaluation, and operationalization.\nExperience with Deep Learning libraries (Tensorflow / Keras, PyTorch, MXNet, etc.). Experience with Azure ML/ MLFlow.\nExperience with at least one other programming language (Python, R, Julia, Scala, Go, Java, C++).\nEducation:\nBachelor s degree in Statistics, Economics, Data Science, or related field. Advanced degree a plus.\nWhy Chubb Business Services India: A global organization of very advanced analytics capabilities\nHere are some reasons to join us:\nYou will be joining a group of diverse and driven professionals trained to succeed in today s advanced analytics landscape.\nOpportunity to cultivate a robust career; we accommodate your growth and development even as we expect you to make meaningful contributions to our company.",
        "skills": [
            "Data Scientist",
            "Azure",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Uplers",
        "experience": "4-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Recruiting",
        "job_description": "Requirements:\n4+ years of hands-on experience in data science and machine learning, with a solid track record of delivering successful projects.\nStrong analytical and problem-solving skills, with the ability to manage large, complex datasets.\nProficient in Python, with hands-on experience in libraries such as Pandas, Scikit-learn, TensorFlow, and PyTorch.\nExperience in Convolutional Neural Networks, Transformers, Transfer Learning from LLMs to SLMs (Smaller Language Migration).\nExperience with NLP techniques and deep learning models, including CNNs, RNNs, Transformers, and LLMs, especially in tasks like classification and sentiment analysis.\nProven experience in deploying machine learning models in production environments.\nExperience with cloud platforms (AWS, Azure, Google Cloud) and containerization technologies like Docker and Kubernetes is a plus.\nExcellent communication skills and the ability to work effectively in collaborative team environments.\nBachelors or masters degree in computer science, or a related field.",
        "skills": [
            "nlp techniques",
            "Problem Solving Skills",
            "python",
            "Docker",
            "Kubernetes",
            "Pytorch"
        ]
    },
    {
        "job_title": "MDM Senior Data Scientist",
        "company_name": "Amgen Inc",
        "experience": "5-9 Years",
        "salary": "INR 5 - 7 LPA ",
        "location": "Hyderabad",
        "industry": "Biotechnology",
        "job_description": "Roles & Responsibilities\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain , Autogen , and Hugging Face.\nDesign and develop intelligent pipelines using PySpark , TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models and manage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience\nMaster's degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelor's degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field",
        "skills": [
            "Tensorflow",
            "Pyspark",
            "Databricks",
            "Aws",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Staff Data Scientist - NLP & Applied AI",
        "company_name": "Uplers",
        "experience": "8-13 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Recruiting",
        "job_description": "Must have skills required :\nLLM, Nltk, Pytorch, Scikit-learn., Transformer based models, NLP, Python\nGood to have skills :\nAirflow, MLOps, Spark\nNeuron7 (One of Uplers Clients) is Looking for:\nStaff Data Scientist who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\nRole Overview Description\nStaff Data Scientist\nWhat Youll Do:\nLead the development and deployment of LLM and NLP-based solutions to process and analyze unstructured data at scale.\nDesign, train, and optimize machine learning models using libraries such as PyTorch, NLTK, and Scikit-learn.\nArchitect and deploy AI/ML products on cloud platforms like Azure, GCP, or AWS.\nCollaborate with data engineering teams to ensure seamless integration of AI models into production systems.\nPerform advanced SQL analytics to extract actionable insights from structured datasets.\nStay up-to-date with the latest advancements in NLP and machine learning techniques.\nMentor junior data scientists and foster a culture of technical excellence within the team.\nCommunicate complex technical concepts to non-technical stakeholders and customers.\nPartner with customers to understand their needs and translate them into technical solutions.\nWhat Were Looking For:\nMinimum 8 years of experience in data science, with a focus on NLP , LLM and unstructured data processing.\nProven track record of launching NLP-driven products to live users.\nExpertise in Python and standard libraries such as PyTorch, NLTK, and Scikit-learn.\nExperience with Transformer-based models (e.g., BERT, RAG, GPT, Gen AI ).\nStrong experience with one or more cloud platforms (Azure, GCP, AWS) for hosting and deploying AI/ML products.\nFamiliarity with data engineering pipelines and best practices.\nProficiency in SQL for analytics and data manipulation.\nExcellent problem-solving skills and ability to work with large-scale datasets.\nStrong interpersonal and communication skills, with the ability to mentor team members and interact with customers effectively.\nPreferred Skills:\nKnowledge of MLOps practices for model deployment and monitoring.\nFamiliarity with tools like Airflow, Spark, or similar data processing frameworks.\nBackground in working with customer-facing applications and APIs.",
        "skills": [
            "airflow",
            "MLops",
            "Spark",
            "Api",
            "cloud platform",
            "Sql"
        ]
    },
    {
        "job_title": "Sr. Data Scientist",
        "company_name": "Uplers",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Ahmedabad, Pune",
        "industry": "Information Technology",
        "job_description": "Must have skills required :\nPyspark, Python, Databricks, MLFlow, Pytorch, TensorFlow\nGood to have skills :\nAWS Sagemaker, Azure ML, Healthcare Industry, MosaicML\nInferenz (One of Uplers Clients) is Looking for:\nSr. Data Scientist who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\nRole Overview Description\nWe are looking for an experienced and results-driven Sr. Data Scientist with 5+ years of professional experience to join our data team. You will be responsible for analyzing large datasets, building predictive models, and providing actionable insights that drive strategic decision-making. This is an exciting opportunity to make a real impact by applying machine learning, statistical analysis, and business intelligence in a fast-paced environment.\nResponsibilities:\nExtract insights & build models to understand /predict Key metrices\nWork closely with stakeholders to identify business challenges and translate them into data science problems.\nCollect, clean, and preprocess structured and unstructured data from various sources.\nFocuses on Research Analysis, model development.\nBuilding models (regression, classification, clustering. Etc.) validate and deploy machine learning models to solve real-world business problems.\nPerform statistical analysis and generate data-driven insights and recommendations.\nCollaborate with engineering and product teams to implement data solutions.\nAutomate monitoring model performance and retraining.\nRequirements:\nBachelor's or master's degree in computer science, Data Science, Statistics, Mathematics, or a related field.\n5+ years of experience in a data science or machine learning role.\nStrong Python (PySpark, pandas, scikit-learn, TensorFlow/PyTorch).\nHands-on with MLflow for model tracking & deployment.\nExpertise in Databricks for scalable ML workflows.\nExpertise in handling data issues including data gaps, duplicates, data drifts etc.\nStrong SQL skills and experience working with large databases.\nSolid understanding of statistical modelling, A/B testing, and predictive analytics.\nStrong grasp of supervised/unsupervised learning, deep learning, and optimization techniques.\nExperience with A/B testing and model performance monitoring.\nExperience with cloud platforms (AWS, GCP, or Azure) and working knowledge of MLOps is a plus.\nExcellent problem-solving skills, communication skills, and attention to detail.\nExperience deploying models to production using CI/CD pipelines.\nPreferred Skills (Good to Have)\nMosaicML/Azure ML/AWS Sagemaker experience is plus. Healthcare Industry-specific experience.\nHealthcare Industry-specific experience.",
        "skills": [
            "mlflow",
            "Communication Skills",
            "Testing",
            "databricks",
            "Sql",
            "aws"
        ]
    },
    {
        "job_title": "Senior Data Scientist Engineer",
        "company_name": "NICE",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "How will you make an impact\nRegularly review production metrics and specific problem cases to find opportunities for improvement.\nHelp diagnose and resolve issues with production prompts in English.\nRefine prompts to generative AI systems to achieve customer goals.\nCollect and present quantitative analysis on solution success.\nWork with application developers to implement new production monitoring tools and metrics.\nWork with architects and Product Managers to implement prompts to support new features.\nMeet regularly with teams working in United States Mountain and Pacific time zones (UTC-7:00 and UTC-8:00).\nReview new prompts and prompt changes with Machine Learning Engineers.\nConsult with Machine Learning Engineers for more challenging problems.\nStay informed about new advances in prompt engineering.\nHave you got what it takes\nFluent in written and spoken English.\nBS in technology-related field such as computer science, business intelligence/analytics, or finance.\n4- 7 years work experience in a technology-related industry or position.\nFamiliarity with best practices in prompt engineering, to include differences in prompts between major LLM vendors.\nAbility to develop and maintain good working relationships with cross-functional teams.\nAbility to clearly communicate and present to internal and external stakeholders.\nExperience with Python and at least one web app framework for prototyping, e.g., Streamlit or Flask.",
        "skills": [
            "Llm",
            "Streamlit",
            "Machine Learning",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist, Actimize",
        "company_name": "NICE",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "Have you got what it take:\nAt least 5 years of experience in data analysis, preferably in the risk or fraud domain within the fintech industry.\nHigh proficiency in SQL Must. Experienced in writing complex queries and analyzing large datasets.\nHigh proficiency in Python for data analysis Must. Skilled in using Python libraries such as Pandas for data analysis and research, automation, and developing fraud detection logic.\nHigh proficiency in English and Strong written and verbal communication skills to effectively interact with clients, partners, and internal teams.\nAdvantage: Understanding of the US financial systems Familiarity with banking, payments, or e-commerce systems and processes.\nAdvantage: Experience with financial fraud prevention strategies.\nAdvantage: Ability to work independently, take initiative, and drive projects from ideation to completion.\nGreat team player with desire to mentor junior analysts and Data scientists.",
        "skills": [
            "Data Analysis And Interpretation",
            "Pandas",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist Engineer",
        "company_name": "Grid Dynamics",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "We are seeking an experienced Agentic Automation Engineer to join our team. The ideal candidate will have expertise in Conversational AI, specifically working with LLM-based conversational AI models. This role requires strong configuration skills and the ability to problem-solve while being aware of the constraints of such models.Essential functions\nKey Responsibilities\nDevelop and implement LLM-based conversational AI solutions\nConfigure and optimize AI models for specific use cases\nTroubleshoot and resolve issues related to AI model performance\nStay up to date with the latest advancements in LLM technologies and apply them to our projects\nQualifications\nExperience with the following LLM-related technologies:( At least 5 LLM technologies )\nChunking\nDocument Parsing and OCR\nDocument Parsing with VLMs (Vision Language Models)\nFunction Calling with LLMs\nBuilding Agentic Chains\nMemory Handling (Conversational Memory) and Tracking / Tracing Software Agents\nMap Reduce applied to RAG\nRetrieval Augmented Generation\nTraditional Search (BM25, NER based parsers, Keyword based search index)\nSemantic Search (Embeddings, Embedding models)\nQuantized Embeddings\nReference Management\nRe-ranking references\nRe-ranking retrieved chunks\nMultimodal RAG\nCost reduction techniques for LLMs\nSpeculative Decoding\nSpeech to Text using Whisper or similar models\nText to SQL/Text to Pandas\nMonte Carlo Tree Search, Tree of Thoughts, Chain of Thoughts, Reasoning Enhancement Techniques for LLMs\nMachine Learning Skills\nExperience with the following ML techniques:( At least 3 ML Skills )\nFine Tuning using LoRA\nMerging multiple LoRA adapters using MergeKit\nQuantising LLMs\nQuantising LoRA adapters\nCreating Table Schemas suitable for LLMs\nCode Folding for Coding LLMs\nImplementation of Code Interpreters (E.g., E2B Dev)\nRequired Knowledge\nUnderstanding of differences between code-focused LLMs, Completion LLMs, and Chat variants of LLMs\nFramework Experience (At least 3 frameworks)\nE2B Dev - Code Interpreters\nDSPY - Automation of Prompt Engineering\nLang chain\nOpenAI APIs, Claude APIs\nGuidance (For Prompt Following reliability)\nOpenAI whisper for STT",
        "skills": [
            "Cost Reduction",
            "Medical Insurance",
            "Coding",
            "Machine Learning",
            "Automation"
        ]
    },
    {
        "job_title": "Senior Data Scientist Engineer",
        "company_name": "Grid Dynamics",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "We are seeking an experienced Agentic Automation Engineer to join our team. The ideal candidate will have expertise in Conversational AI, specifically working with LLM-based conversational AI models. This role requires strong configuration skills and the ability to problem-solve while being aware of the constraints of such models.Essential functions\nKey Responsibilities\nDevelop and implement LLM-based conversational AI solutions\nConfigure and optimize AI models for specific use cases\nTroubleshoot and resolve issues related to AI model performance\nStay up to date with the latest advancements in LLM technologies and apply them to our projects\nQualifications\nExperience with the following LLM-related technologies:( At least 5 LLM technologies )\nChunking\nDocument Parsing and OCR\nDocument Parsing with VLMs (Vision Language Models)\nFunction Calling with LLMs\nBuilding Agentic Chains\nMemory Handling (Conversational Memory) and Tracking / Tracing Software Agents\nMap Reduce applied to RAG\nRetrieval Augmented Generation\nTraditional Search (BM25, NER based parsers, Keyword based search index)\nSemantic Search (Embeddings, Embedding models)\nQuantized Embeddings\nReference Management\nRe-ranking references\nRe-ranking retrieved chunks\nMultimodal RAG\nCost reduction techniques for LLMs\nSpeculative Decoding\nSpeech to Text using Whisper or similar models\nText to SQL/Text to Pandas\nMonte Carlo Tree Search, Tree of Thoughts, Chain of Thoughts, Reasoning Enhancement Techniques for LLMs\nMachine Learning Skills\nExperience with the following ML techniques:( At least 3 ML Skills )\nFine Tuning using LoRA\nMerging multiple LoRA adapters using MergeKit\nQuantising LLMs\nQuantising LoRA adapters\nCreating Table Schemas suitable for LLMs\nCode Folding for Coding LLMs\nImplementation of Code Interpreters (E.g., E2B Dev)\nRequired Knowledge\nUnderstanding of differences between code-focused LLMs, Completion LLMs, and Chat variants of LLMs\nFramework Experience (At least 3 frameworks)\nE2B Dev - Code Interpreters\nDSPY - Automation of Prompt Engineering\nLang chain\nOpenAI APIs, Claude APIs\nGuidance (For Prompt Following reliability)\nOpenAI whisper for STT",
        "skills": [
            "Cost Reduction",
            "Medical Insurance",
            "Coding",
            "Machine Learning",
            "Automation"
        ]
    },
    {
        "job_title": "Data Scientist (Predictive AI - Operational Research)",
        "company_name": "Grid Dynamics",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "We are looking for a highly motivated and experiencedData Scientistwith strong expertise inPredictive AIandOperational Research (OR)to drive data-driven decision-making and optimization for complex business processes. This role involves designing, developing, and deploying machine learning and mathematical models to solve real-world operational problems across domains such as supply chain, logistics, manufacturing, finance, and customer service.\nProject Description\nThe engineer is supposed to participate in various Predictive AI projects such as Demand Sensing and Forecasting, Price and Promotion Optimization and others.\nDetails on Tech stack\nPython\nExperience with linear optimization algorithms: Linear Programing and Mixed Integer Programing\nExperience with non-linear optimization algorithms: Gradient based and Metaheuristic algorithms (such as Genetic Algorithm, Simulation Annealing etc.)\nExperience with Simulation techniques (such as Montecarlo, MCMC, discreet event, agent-based modeling etc)\nAbility to recognize optimization type of problem (such as Max Flow, Schedule Problem, Knapsack problem etc)\nExperience with cloud (any cloud work - GCP,AWS)\nExperience with commercial and non commercial solvers (such as commercial Gurobi, Hexaly etc. & non commercial HiGHS, CP-SAT OrTools etc)\nNice to Have Requirements\nExperience with supply chain domain\nExperience with pricing and promotion domain\nExperience with deploying models\nExperience with time series predictive models\nExperience with feature engineering for time series data\nExperience with classical ML techniques\nExperience with CI/CD\nGoogle cloud (GCP) and Vertex AI\nPerks & Benefits:\nCompetitive salary & performance-based bonuses\nRemote work flexibility / Hybrid options\nContinuous learning budget & GenAI certifications\nOpportunity to work on cutting-edge AI projects\nDynamic and collaborative team environment",
        "skills": [
            "Operations Research",
            "Demand Forecasting",
            "Linear Programming",
            "Predictive Modeling",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Predictive AI - Operational Research)",
        "company_name": "Grid Dynamics",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "We are looking for a highly motivated and experiencedData Scientistwith strong expertise inPredictive AIandOperational Research (OR)to drive data-driven decision-making and optimization for complex business processes. This role involves designing, developing, and deploying machine learning and mathematical models to solve real-world operational problems across domains such as supply chain, logistics, manufacturing, finance, and customer service.\nProject Description\nThe engineer is supposed to participate in various Predictive AI projects such as Demand Sensing and Forecasting, Price and Promotion Optimization and others.\nDetails on Tech stack\nPython\nExperience with linear optimization algorithms: Linear Programing and Mixed Integer Programing\nExperience with non-linear optimization algorithms: Gradient based and Metaheuristic algorithms (such as Genetic Algorithm, Simulation Annealing etc.)\nExperience with Simulation techniques (such as Montecarlo, MCMC, discreet event, agent-based modeling etc)\nAbility to recognize optimization type of problem (such as Max Flow, Schedule Problem, Knapsack problem etc)\nExperience with cloud (any cloud work - GCP,AWS)\nExperience with commercial and non commercial solvers (such as commercial Gurobi, Hexaly etc. & non commercial HiGHS, CP-SAT OrTools etc)\nNice to Have Requirements\nExperience with supply chain domain\nExperience with pricing and promotion domain\nExperience with deploying models\nExperience with time series predictive models\nExperience with feature engineering for time series data\nExperience with classical ML techniques\nExperience with CI/CD\nGoogle cloud (GCP) and Vertex AI\nPerks & Benefits:\nCompetitive salary & performance-based bonuses\nRemote work flexibility / Hybrid options\nContinuous learning budget & GenAI certifications\nOpportunity to work on cutting-edge AI projects\nDynamic and collaborative team environment",
        "skills": [
            "Operations Research",
            "Demand Forecasting",
            "Linear Programming",
            "Predictive Modeling",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Predictive AI - Operational Research)",
        "company_name": "Grid Dynamics",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "We are looking for a highly motivated and experiencedData Scientistwith strong expertise inPredictive AIandOperational Research (OR)to drive data-driven decision-making and optimization for complex business processes. This role involves designing, developing, and deploying machine learning and mathematical models to solve real-world operational problems across domains such as supply chain, logistics, manufacturing, finance, and customer service.\nProject Description\nThe engineer is supposed to participate in various Predictive AI projects such as Demand Sensing and Forecasting, Price and Promotion Optimization and others.\nDetails on Tech stack\nPython\nExperience with linear optimization algorithms: Linear Programing and Mixed Integer Programing\nExperience with non-linear optimization algorithms: Gradient based and Metaheuristic algorithms (such as Genetic Algorithm, Simulation Annealing etc.)\nExperience with Simulation techniques (such as Montecarlo, MCMC, discreet event, agent-based modeling etc)\nAbility to recognize optimization type of problem (such as Max Flow, Schedule Problem, Knapsack problem etc)\nExperience with cloud (any cloud work - GCP,AWS)\nExperience with commercial and non commercial solvers (such as commercial Gurobi, Hexaly etc. & non commercial HiGHS, CP-SAT OrTools etc)\nNice to Have Requirements\nExperience with supply chain domain\nExperience with pricing and promotion domain\nExperience with deploying models\nExperience with time series predictive models\nExperience with feature engineering for time series data\nExperience with classical ML techniques\nExperience with CI/CD\nGoogle cloud (GCP) and Vertex AI\nPerks & Benefits:\nCompetitive salary & performance-based bonuses\nRemote work flexibility / Hybrid options\nContinuous learning budget & GenAI certifications\nOpportunity to work on cutting-edge AI projects\nDynamic and collaborative team environment",
        "skills": [
            "Operations Research",
            "Demand Forecasting",
            "Linear Programming",
            "Predictive Modeling",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist Engineer",
        "company_name": "NICE",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "Have you got what it takes\nFluent in written and spoken English.\nBS in technology-related field such as computer science, business intelligence/analytics, or finance.\n4- 7 years work experience in a technology-related industry or position.\nFamiliarity with best practices in prompt engineering, to include differences in prompts between major LLM vendors.\nAbility to develop and maintain good working relationships with cross-functional teams.\nAbility to clearly communicate and present to internal and external stakeholders.\nExperience with Python and at least one web app framework for prototyping, e.g., Streamlit or Flask.",
        "skills": [
            "Streamlit",
            "Analytics",
            "Llm",
            "Python"
        ]
    },
    {
        "job_title": "Specialist Data Scientist",
        "company_name": "NICE",
        "experience": "8-11 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "Have you got what it takes\nMaster's degree in the field of Computer Science, Technology, Engineering, Math, or equivalent practical experience\nMinimum of 8 years of data science work experience, including implementing machine learning and NLP models using real-life data.\nExperience with Retrieval-Augmented Generation (RAG) pipelines or LLMOps.\nAdvanced knowledge of statistics and machine learning algorithms.\nProficiency in Python programming and familiarity with R.\nExperience with deep learning models and libraries such as PyTorch, TensorFlow, and JAX.\nFamiliarity with relational databases and query languages (e.g., MSSQL) and basic SQL knowledge.\nHands-on experience with transformer models (BERT, FlanT5, Llama, etc.) and GenAI frameworks (HuggingFace, LangChain, Ollama, etc.).\nExperience deploying NLP models in production environments, ensuring scalability and performance using AWS/GCP/Azure\nStrong verbal and written communication skills, including effective presentation abilities.\nAbility to work independently and as part of a team, demonstrating analytical thinking and problem-solving skills.\nYou will have an advantage if you also have:\nExpertise with Big Data technologies (e.g., PySpark).\nBackground in knowledge graphs, graph databases, or GraphRAG architectures.\nUnderstanding of multimodal models (text, audio, vision).\nExperience in Customer Experience domains.\nExperience with package development and technical writing.\nFamiliarity with tools like Jira, Confluence, and source control packages and methodology.\nKnowledge and interest in foreign languages and linguistics.\nExperience working on international, globe-spanning teams and with AWS.\nPast participation in a formal research setting.\nExperience as part of a software organization.",
        "skills": [
            "Pyspark",
            "Pytorch",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "Key Qualifications\nExperience:\n12 years of software development experience (preferred)\n12 years of experience as a Data Scientist, with a focus on Time Series forecasting\n4+ years working with Azure Cloud platforms, including:\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF) Pipelines\nTechnical Skills:\nML Ops / DevOps practices and tooling\nCI/CD pipeline configuration and deployment\nMLFlow for experiment tracking and model management\nStrong programming skills in Python\nExperience with machine learning models such as:\nRandom Forest\nXGBoost\nLightGBM\nOther ensemble modeling algorithms\nEducation:\nBachelor's or Master's degree in Software Engineering, Computer Science, Statistics, or a related field\nPhD not required\nSoft Skills:\nFast learner with the ability to adapt to changing priorities\nStrong problem-solving and collaboration skills\nRole Responsibilities\nFunction as a developer with a strong focus on:\nML model implementation, experimentation, and deployment\nApplying software engineering best practices in machine learning projects\nConduct data analysis and work with forecasting algorithms, especially for time series data\nCollaborate with cross-functional teams to deliver production-level ML solutions\nTools & Technologies\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF)\nMLFlow\nCI/CD tools (e.g., GitHub Actions, Azure DevOps, Jenkins)\nPython and relevant ML libraries (e.g., scikit-learn, pandas, numpy, etc.)\nWhat We Offer\nOpportunity to work on cutting-edge ML projects\nA highly motivated and collaborative team\nCompetitive salary\nFlexible schedule\nBenefits package including:\nMedical insurance\nSports membership or wellness stipend\nCorporate social events\nProfessional development opportunities\nModern, well-equipped office environment",
        "skills": [
            "Forecasting",
            "Data Analysis",
            "Statistics",
            "Medical Insurance",
            "Devops",
            "Cloud",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - Product Development",
        "company_name": "Uplers",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "Must have skills required :\nExperience with AI/ML tools and frameworks\nGood to have skills :\nNLP, AI/ML, R Python\nGRADATIM (One of Uplers Clients) is Looking for:\nData Scientist Product Development who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.\nRole Overview Description\nSeeking a technical Data Scientist to build, implement, and integrate advanced ML/AI solutions (model development, data analysis, AI features) into insurance products, collaborating across teams.\nKey Responsibilities :\nML/Model Development: Design, build, optimize ML models (risk, fraud, claims, underwriting) using various algorithms; feature engineering, tuning, validation.\nData Engineering: Process data, build/optimize ETL pipelines using cloud platforms (AWS/Azure/GCP).\nAlgorithm Implementation: Develop/optimize AI/ML algorithms (incl. Deep Learning, RL) for production.\nIntegration/Deployment: Deploy models (API/microservices), use MLOps, collaborate with DevOps/Eng.\nResearch & Innovation: Stay updated on AI/ML trends, experiment with tools.\nCollaboration & Documentation: Work with PMs/Engineers, document processes, communicate findings.\nRequired Education\nMasters or Bachelors in Data Science, Computer Science, Statistics, or related field.\nRequired Technical Skills\nProgramming: Python or R (strong proficiency)\nML Libraries: TensorFlow, PyTorch, Scikit-learn\nData: SQL, NoSQL, Data Pipelines (Spark, Hadoop, Airflow)\nCloud ML: AWS SageMaker, Azure ML, or GCP Vertex AI\nMLOps: Familiarity (e.g., MLflow, Kubeflow, TensorBoard)\nRequired Knowledge\nModel evaluation metrics, statistical analysis, optimization techniques.\nPreferred Skills\nExperience in NLP, Computer Vision, Deep Learning (for insurance).\nFamiliarity with Graph Analytics (for fraud/network analysis).\nKnowledge of insurance processes or financial risk modeling.",
        "skills": [
            "Ai",
            "r",
            "Machine Learning",
            "Nlp",
            "python",
            "data scientist"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "Key Qualifications\nExperience:\n12 years of software development experience (preferred)\n12 years of experience as a Data Scientist, with a focus on Time Series forecasting\n4+ years working with Azure Cloud platforms, including:\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF) Pipelines\nTechnical Skills:\nML Ops / DevOps practices and tooling\nCI/CD pipeline configuration and deployment\nMLFlow for experiment tracking and model management\nStrong programming skills in Python\nExperience with machine learning models such as:\nRandom Forest\nXGBoost\nLightGBM\nOther ensemble modeling algorithms\nEducation:\nBachelor's or Master's degree in Software Engineering, Computer Science, Statistics, or a related field\nPhD not required\nSoft Skills:\nFast learner with the ability to adapt to changing priorities\nStrong problem-solving and collaboration skills\nRole Responsibilities\nFunction as a developer with a strong focus on:\nML model implementation, experimentation, and deployment\nApplying software engineering best practices in machine learning projects\nConduct data analysis and work with forecasting algorithms, especially for time series data\nCollaborate with cross-functional teams to deliver production-level ML solutions\nTools & Technologies\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF)\nMLFlow\nCI/CD tools (e.g., GitHub Actions, Azure DevOps, Jenkins)\nPython and relevant ML libraries (e.g., scikit-learn, pandas, numpy, etc.)\nWhat We Offer\nOpportunity to work on cutting-edge ML projects\nA highly motivated and collaborative team\nCompetitive salary\nFlexible schedule\nBenefits package including:\nMedical insurance\nSports membership or wellness stipend\nCorporate social events\nProfessional development opportunities\nModern, well-equipped office environment",
        "skills": [
            "Forecasting",
            "Data Analysis",
            "Statistics",
            "Medical Insurance",
            "Devops",
            "Cloud",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist / ML Engineer",
        "company_name": "RARR Technologies",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Overview:\nWe are seeking a talented and innovative Data Scientist to join our growing team. In this role, you will be responsible for applying machine learning and generative AI techniques to solve complex problems, drive business insights, and enhance product offerings. You will collaborate with engineers to build and deploy ML models in microservices architectures, ensuring the solutions are scalable, maintainable, and integrated with APIs.\nKey Responsibilities:\nAnalyze large, structured, and unstructured datasets to extract meaningful insights and identify business opportunities.\nDevelop, test, and implement machine learning and generative AI models to drive intelligent decision-making and automation.\nWork closely with engineering teams to integrate machine learning models into production systems using microservices architectures.\nDesign and develop APIs to enable seamless communication between data models and applications.\nBuild and maintain scalable data pipelines to facilitate data collection, transformation, and storage for model training and inference.\nUtilize Python and relevant libraries (e.g., Pandas, NumPy, TensorFlow, PyTorch) to preprocess data, train models, and perform statistical analysis.\nCollaborate with product and business teams to understand requirements and deploy machine learning solutions that meet business needs.\nPerform continuous monitoring, testing, and optimization of deployed models to ensure high performance and reliability.\nStay updated on the latest trends and advancements in machine learning, generative AI, and related fields to apply cutting-edge techniques in your work.\nDocument processes, methodologies, and model outputs for transparency and future improvements.\nRequired Skills & Qualifications:\nBachelors or Masters degree in Computer Science, Data Science, Statistics, or related field.\nProven experience in Python programming, including the use of libraries such as Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, or Keras for machine learning.\nHands-on experience with microservices architecture and containerization technologies like Docker or Kubernetes.\nExperience with building and deploying APIs to enable machine learning models to interact with other systems and applications.\nStrong understanding of Machine Learning algorithms, including supervised and unsupervised learning, deep learning, and generative AI techniques such as GANs (Generative Adversarial Networks) and language models.\nAbility to work with cloud platforms (AWS, GCP, or Azure) for model deployment and scalability.\nKnowledge of data engineering concepts, including data wrangling, ETL processes, and working with distributed systems.\nFamiliarity with modern version control systems (e.g., Git) and agile development practices.\nStrong analytical and problem-solving skills with the ability to communicate complex technical concepts to non-technical stakeholders.\nExperience with data visualization tools (e.g., Tableau, Power BI, or Matplotlib) is a plus.\nPreferred Qualifications:\nExperience with advanced Generative AI models, such as transformers (e.g., GPT, BERT).\nKnowledge of DevOps practices and CI/CD pipelines for machine learning deployment.\nFamiliarity with the integration of ML models into business applications and customer-facing products.\nStrong communication skills and the ability to collaborate with cross-functional teams including engineers, product managers, and business stakeholders.\nSkills Mentioned:\nAPI, Python, Generative AI, Machine Learning, Microservices",
        "skills": [
            "Generative AI",
            "Data Science",
            "Machine Learning",
            "Apis",
            "Python",
            "Microservices"
        ]
    },
    {
        "job_title": "Data Scientist, VP",
        "company_name": "NatWest Group",
        "experience": "6-12 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Banking",
        "job_description": "Join us as a Data Scientist\nIn this role, you'll drive and embed the design and implementation of data science tools and methods, which harness our data to drive market-leading purpose customer solutions with a focus on our Wealth business\nDay-to-day, you'll act as a subject matter expert and articulate advanced data science opportunities, bringing them to life through data visualisation\nIf you're ready for a new challenge, and are interested in identifying opportunities to support external customers by using your data science expertise, this could be the role for you\nWe're offering this role at vice president level\nWhat you'll do\nWe're looking for someone to understand the requirements and needs of our Wealth business stakeholders. You'll develop good relationships with them, form hypotheses, and identify suitable data science solutions to meet their needs to achieve our business strategy.\nYou'll be maintaining and developing external curiosity around new and emerging trends within data science, keeping up to date with emerging trends and tooling and sharing updates within and outside of the team.\nYou'll also be responsible for:\nProactively bringing together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques, and algorithms\nImplementing ethically sound models end-to-end and applying software engineering and a product development lens to complex business problems\nWorking with and leading both direct reports and wider teams in an agile way within multi-disciplinary data to achieve agreed project and Scrum outcomes\nUsing your data translation skills to work closely with business stakeholders to define business questions, problems or opportunities that can be supported through advanced analytics\nSelecting, building, training, and testing complex machine models, considering model valuation, model risk, governance, and ethics throughout to implement and scale models\nThe skills you'll need\nTo be successful in this role, you'll demonstrate evidence of leading data science development from idea to implementation and relevant work experience gained from delivering data science solutions as part of a multi-disciplinary team. We'll also expect you to hold an undergraduate or a master's degree in a quantitative discipline, or evidence of equivalent practical experience.\nYou'll also need a minimum of ten years of experience with statistical software, database languages, big data technologies, cloud environments and machine learning on large data sets. And we'll look to you to bring the ability to demonstrate leadership, self-direction and a willingness to both teach others and learn new techniques. Knowledge of the Wealth business and related data would be an advantage.\nAdditionally, you'll need:\nExperience of developing and deploying machine learning models and pipelines into a production environment\nExperience of translating business objectives into data science solutions with the ability to translate these to non-technical stakeholders\nExperience of working in cloud technology such as AWS, and programming languages such as Python and PySpark\nExtensive applied statistical data analysis experience, such as, linear models, multivariate analysis, stochastic models, and sampling methods\nEvidence of applied machine learning models including regression, classification, clustering, propensity models, and Gen AI",
        "skills": [
            "Gen AI",
            "data science tools",
            "Data Scientist",
            "Pyspark",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist, VP",
        "company_name": "NatWest Group",
        "experience": "6-12 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Banking",
        "job_description": "Join us as a Data Scientist\nIn this role, you'll drive and embed the design and implementation of data science tools and methods, which harness our data to drive market-leading purpose customer solutions with a focus on our Wealth business\nDay-to-day, you'll act as a subject matter expert and articulate advanced data science opportunities, bringing them to life through data visualisation\nIf you're ready for a new challenge, and are interested in identifying opportunities to support external customers by using your data science expertise, this could be the role for you\nWe're offering this role at vice president level\nWhat you'll do\nWe're looking for someone to understand the requirements and needs of our Wealth business stakeholders. You'll develop good relationships with them, form hypotheses, and identify suitable data science solutions to meet their needs to achieve our business strategy.\nYou'll be maintaining and developing external curiosity around new and emerging trends within data science, keeping up to date with emerging trends and tooling and sharing updates within and outside of the team.\nYou'll also be responsible for:\nProactively bringing together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques, and algorithms\nImplementing ethically sound models end-to-end and applying software engineering and a product development lens to complex business problems\nWorking with and leading both direct reports and wider teams in an agile way within multi-disciplinary data to achieve agreed project and Scrum outcomes\nUsing your data translation skills to work closely with business stakeholders to define business questions, problems or opportunities that can be supported through advanced analytics\nSelecting, building, training, and testing complex machine models, considering model valuation, model risk, governance, and ethics throughout to implement and scale models\nThe skills you'll need\nTo be successful in this role, you'll demonstrate evidence of leading data science development from idea to implementation and relevant work experience gained from delivering data science solutions as part of a multi-disciplinary team. We'll also expect you to hold an undergraduate or a master's degree in a quantitative discipline, or evidence of equivalent practical experience.\nYou'll also need a minimum of ten years of experience with statistical software, database languages, big data technologies, cloud environments and machine learning on large data sets. And we'll look to you to bring the ability to demonstrate leadership, self-direction and a willingness to both teach others and learn new techniques. Knowledge of the Wealth business and related data would be an advantage.\nAdditionally, you'll need:\nExperience of developing and deploying machine learning models and pipelines into a production environment\nExperience of translating business objectives into data science solutions with the ability to translate these to non-technical stakeholders\nExperience of working in cloud technology such as AWS, and programming languages such as Python and PySpark\nExtensive applied statistical data analysis experience, such as, linear models, multivariate analysis, stochastic models, and sampling methods\nEvidence of applied machine learning models including regression, classification, clustering, propensity models, and Gen AI",
        "skills": [
            "Gen AI",
            "data science tools",
            "Data Scientist",
            "Pyspark",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai City, Mumbai",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Innova solutinos",
        "experience": "6-10 Years",
        "salary": null,
        "location": "Chennai, Hyderabad, Noida",
        "industry": "Information Technology",
        "job_description": "Primary Skills\nStrong technical skills in Statistics, Machine Learning, Deep Learning, Natural Language Processing, or Artificial Intelligence, Chatbot frameworks Strong programming skills\nExcellent verbal and written communication skills required for interacting with management, clients and peers Ability to clearly summarize methodology and key points of program/report in technical documentation/specifications is also required\nSecondary Skill\nExperience with Generative AI, Cloud services (GCP, Azure)\nKnowledge and Competence\nStrong technical skills in Python and its related frameworks, Predictive Analytics, Statistics, Regression and Classification algorithms\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:MCA in Any Specialization, M.Tech in Any Specialization, MS/M.Sc(Science) in Any Specialization",
        "skills": [
            "Machine Learning",
            "Artificial Intelligence",
            "Natural Language Processing",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Delhi, Delhi NCR",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "MDM Data Scientist",
        "company_name": "Amgen Inc",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Biotechnology",
        "job_description": "Roles & Responsibilities\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain , Autogen , and Hugging Face.\nDesign and develop intelligent pipelines using PySpark , TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models and manage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience\nMaster's degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelor's degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field",
        "skills": [
            "Tensorflow",
            "Pyspark",
            "Databricks",
            "Sql",
            "Python",
            "Aws"
        ]
    },
    {
        "job_title": "Senior Data Scientist, Actimize",
        "company_name": "NICE",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Software",
        "job_description": "Have you got what it take:\nAt least 5 years of experience in data analysis, preferably in the risk or fraud domain within the fintech industry.\nHigh proficiency in SQL Must. Experienced in writing complex queries and analyzing large datasets.\nHigh proficiency in Python for data analysis Must. Skilled in using Python libraries such as Pandas for data analysis and research, automation, and developing fraud detection logic.\nHigh proficiency in English and Strong written and verbal communication skills to effectively interact with clients, partners, and internal teams.\nAdvantage: Understanding of the US financial systems Familiarity with banking, payments, or e-commerce systems and processes.\nAdvantage: Experience with financial fraud prevention strategies.\nAdvantage: Ability to work independently, take initiative, and drive projects from ideation to completion.\nGreat team player with desire to mentor junior analysts and Data scientists.",
        "skills": [
            "Data Analysis And Interpretation",
            "Pandas",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Associate Data Scientist",
        "company_name": "Amgen Inc",
        "experience": "2-6 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Biotechnology",
        "job_description": "Job Description: Data Scientist Healthcare & Product Analytics\nResponsibilities:\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing, and data analytics.\nGenerate reports, annotated code, and other project artifacts to document, archive, and communicate your work and outcomes.\nWhat We Expect of You\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster's degree and 1 to 3 years of Data Science experience with one or more analytic software tools or languages, and data visualization tools OR\nBachelor's degree and 3 to 5 years of Data Science experience with one or more analytic software tools or languages, and data visualization tools OR\nDiploma and 7 to 9 years of Data Science experience with one or more analytic software tools or languages, and data visualization tools\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction, and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering, and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Databricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",
        "skills": [
            "Databricks platform",
            "Data Collection & Cleaning",
            "applied analytics",
            "Data Management & Warehousing",
            "Data Science",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (US Value & Access Insights)",
        "company_name": "Amgen Inc",
        "experience": "3-7 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Biotechnology",
        "job_description": "In this vital role, you will be responsible for developing and deploying advanced machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets.\nThis role involves creating analytics solutions to address customer needs and opportunities.\nRoles & Responsibilities:\nAbility to work on upgrades and manage the execution of Proprietary AI engine built to optimize Copay and other GTN initiatives.\nEnsure models are trained with the latest data and meet the SLA expectations.\nAct as a subject matter expert in solving development and commercial questions.\nWork with a global cross-functional team on the AI tool's roadmap.\nWork in technical teams in the development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nUtilize technical skills such as hypothesis testing, machine learning, and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nModel/analytics experiment and development pipeline leveraging MLOps.\nCollaborate with technical teams to translate business needs into technical specifications, particularly focusing on AI-driven automation and insights.\nDevelop and integrate custom applications, intelligent dashboards, and automated workflows that incorporate AI capabilities to enhance decision-making and efficiency.\nWhat we expect of you\nBasic Qualifications:\nMaster's degree and 1 to 3 years of computer science, statistics, or STEM majors with a minimum of 1 year of Information Systems experience OR\nBachelor's degree and 3 to 5 years of computer science, statistics, or STEM majors with a minimum of 2 years of Information Systems experience OR\nDiploma and 7 to 9 years of computer science, statistics, or STEM majors with a minimum of 2 years of Information Systems experience.\nExperience with one or more analytic software tools or languages like R and Python.\nFoundational understanding of the US pharmaceutical ecosystem and Patient support services offerings (Copay) and other standard data sets including claims, prescription.\nStrong foundation in machine learning algorithms and techniques.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering, and classification.\nPreferred Qualifications:\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD).\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn).\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills.\nExperience with data engineering and pipeline development.\nKnowledge of NLP techniques for text analysis and sentiment analysis.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with AWS, Azure, or Google Cloud.\nExperience with Databricks platform for data analytics and MLOps.\nProfessional Certifications:\nAny AWS Developer certification (preferred).\nAny Python and ML certification (preferred).\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems.\nSkilled in breaking down problems, documenting problem statements, and estimating efforts.\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.",
        "skills": [
            "Tensorflow",
            "Pyspark",
            "Sql",
            "Python",
            "Aws",
            "Data Science"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "Essential Functions\nDesign, develop, and deploy forecasting algorithms using industry best practices\nWork on time series modeling and other predictive modeling techniques\nImplement and experiment with ML models as part of a developer-focused data science team\nDevelop scalable data pipelines and solutions using Python and PySpark\nCollaborate across engineering and analytics teams to deliver production-ready ML solutions\nKey Qualifications\nExperience:\n4+ years in a developer role focused on ML model implementation, experimentation, and related software engineering\n34 years of hands-on experience in Python and PySpark development\n34 years as a Data Scientist with strong focus on forecasting algorithms\n12 years of general software development experience\nExperience with Azure services, including:\nAzure Data Factory (ADF)\nAzure Pipelines\nAzure DevOps\nSkills & Tools:\nForecasting models and time series techniques (ARIMA, Prophet, exponential smoothing, etc.)\nML model development and evaluation\nPython (must-have)\nPySpark (must-have)\nML Ops tools and CI/CD workflows in Azure environment\nEducation:\nBachelor's or Master's degree in Computer Science, Software Engineering, Statistics, or a related discipline\nWhat We Offer\nOpportunity to work on cutting-edge, high-impact ML projects\nCollaborative and motivated team environment\nCompetitive salary and performance incentives\nFlexible working schedule\nComprehensive benefits package:\nMedical insurance\nSports or wellness benefits\nParticipation in corporate social events\nProfessional development support and career growth opportunities\nA modern, well-equipped office space",
        "skills": [
            "Algorithms",
            "Hive",
            "Software Development",
            "Natural Language Processing",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Bahwan CyberTek Private Limited",
        "experience": "6-10 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Software Engineering",
        "job_description": "Role & responsibilities:\nAnalyze structured and unstructured data to derive meaningful insights.\nDevelop, implement, and evaluate predictive models and algorithms.\nIdentify data-related challenges and opportunities.\nTo get involved Cleaning, preprocessin, and transforming raw data for analysis.\nStay updated with the latest trends in data science and machine learning.\nShould have hands on experience in Data Model development and Data Model Deployment.\nShould have experience in processing 1 TB of data volume\nPreferred candidate profile:\nProficiency in Python.\n6-8 Years of experience\nUnderstand the RFP requirements, Product Requrirements to help the BCT Solution / Retina Presales team on the Solutioning Architecture , Data flow diagram in sync with the Scope of Work\nStrong knowledge of SQL and databases.\nProvide Data models for the various Solutions in line with the expectations of the Retina Consulting team.\nHave discussions to understand the process requirements, data sources, data quality and data availability\nPrepare a documentation on the modelling required with clear objectives, inputs and output and how the model can be utilized within Retina\nJustify the selection of the suitable modelling algorithms as relevant for the problem outlined by the Consulting team.\nDevelop models with AI / ML in line with the documentation within the agreed period Demonstrate and fine tune the models as per expectations\nHands-on experience with machine learning frameworks (e.g., TensorFlow, PyTorch, Scikit-learn).\nFamiliarity with big data tools such as Hadoop or Spark is a plus.\nExcellent problem-solving and communication skills.\nNote:\nWork Hours: General\nWork Mode: Inperson / Work from office\nLocation: Chennai",
        "skills": [
            "Data Scientist",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Talodyn Networks Private Limited",
        "experience": "6-10 Years",
        "salary": "INR 18 - 18 LPA ",
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Job Description\nMedium-Senior Level Data Engineer / Data Analyst who can work independently on complex business requirements converting them into data transformation scripts that produce well structured, clean and explainable outcomes.\nThis role involves more than just technical skill, it requires the ability to communicate effectively with colleagues, challenge unclear or incomplete requirements, and proactively improve how data is collected, analyzed, and utilized.\nExperience and Education Required\nAt least 6 years of experience as Data Analyst / Data Engineer/Data Scientist\nJob Profile:\nData Collection & Integration: Identify, gather, and consolidate data from diverse sources, including internal databases and spreadsheets ensuring data integrity and relevance.\nData Cleaning & Transformation: Apply thorough data quality checks, cleaning processes, and transformations using Python (Pandas) and SQL to prepare datasets.\nAutomation & Scalability: Develop and maintain scripts that automate repetitive data preparation tasks.\nExploratory Analysis & Insight Generation: Utilize statistical methods, visualization tools, and critical thinking to uncover trends, patterns, and anomalies.\nRequirement Clarification & Communication: Interact directly with colleagues to clarify objectives, challenge assumptions.\nDocumentation & Best Practices: Maintain clear, concise documentation of data workflows, coding standards, and analytical methodologies to support knowledge transfer and scalability.\nCollaboration & Stakeholder Engagement: Work closely with colleagues who provide data, raising questions about data validity, sharing insights, and co-creating solutions that address evolving needs.\nAutonomy & Proactivity: Operate with minimal supervision, demonstrating initiative in problem-solving, prioritizing tasks, and continuously improving the quality and impact of your work\nTechnical Skills:\nMinimum of 3 years of experience as a Data Analyst, Data Engineer, or related role, ideally with a bachelor's degree or higher in a relevant field.\nStrong proficiency in Python (Pandas, Scikit-learn, Matplotlib) and SQL, with experience working across various data formats and sources.\nFamiliarity with Jupyter notebooks for rapid prototyping and comfort moving from exploratory analysis to production-level code.\nProven ability to automate data workflows, implement code-based best practices, and maintain documentation to ensure reproducibility and scalability.\nBehavioral Skills:\nExcellent communication skills for engaging with colleagues, clarifying requirements, and conveying analytical results in a meaningful, non-technical manner.\nDemonstrated critical thinking skills, including the willingness to question assumptions, evaluate data quality, and recommend alternative approaches when necessary.\nA self-directed, resourceful problem-solver who collaborates well with others while confidently managing tasks and priorities independently.",
        "skills": [
            "Data Analyst",
            "Data Scientist",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "NatWest Group",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Banking",
        "job_description": "Job Description\nJoin us as a Data Scientist\nYou'll design and implement data science tools and methods which harness our data in order to drive market leading purposeful customer solutions\nWe'll look to you to actively participate in the data community to identify and deliver opportunities to support the bank's strategic direction through better use of data\nThis is an opportunity to promote data literacy education with business stakeholders supporting them to foster a data driven culture and to make a real impact with your work\nWhat you'll do\nAs a Data Scientist, you'll bring together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques and algorithms to develop and implement ethically sound models end-to-end. We'll look to you to understand the needs of business stakeholders, form hypotheses and identify suitable data and analytics solutions to meet those needs in order to support the achievement of our business strategy.\nYou'll also be:\nUsing data translation skills to work closely with business stakeholders to define detailed business questions, problems or opportunities which can be supported through analytics\nApplying a software engineering and product development lens to business problems, creating, scaling and deploying software driven products and services\nWorking in an Agile way within multi-disciplinary data and analytics teams to achieve agreed project and scrum outcomes\nSelecting, building, training and testing machine learning models considering model valuation, model risk, governance and ethics, making sure that models are ready to implement and scale\nIteratively building and prototyping data analysis pipelines to provide insights that will ultimately lead to production deployment\nThe skills you'll need\nYou'll need a strong academic background in a STEM discipline such as Mathematics, Physics, Engineering or Computer Science. You'll have experience with statistical modelling and machine learning techniques.\nWe'll also look for financial services knowledge, and an ability to identify wider business impact, risk or opportunities and make connections across key outputs and processes\nYou'll also demonstrate:\nThe ability to use data to solve business problems from hypotheses through to resolution\nExperience using programming language and software engineering fundamentals\nExperience of Cloud applications and options\nExperience in synthesising, translating and visualising data and insights for key stakeholders\nExperience of exploratory data analysis\nGood communication skills with the ability to proactively engage with a wide range of stakeholders",
        "skills": [
            "Cloud & Tooling Exposure",
            "STEM",
            "Data Handling & Analytics",
            "Matplotlib",
            "Data Science",
            "Machine Learning",
            "Power Bi",
            "Tableau"
        ]
    },
    {
        "job_title": "Data Scientist, VP",
        "company_name": "NatWest Group",
        "experience": "6-12 Years",
        "salary": null,
        "location": "Gurugram",
        "industry": "Banking",
        "job_description": "Join us as a Data Scientist\nIn this role, you'll drive and embed the design and implementation of data science tools and methods, which harness our data to drive market-leading purpose customer solutions with a focus on our Wealth business\nDay-to-day, you'll act as a subject matter expert and articulate advanced data science opportunities, bringing them to life through data visualisation\nIf you're ready for a new challenge, and are interested in identifying opportunities to support external customers by using your data science expertise, this could be the role for you\nWe're offering this role at vice president level\nWhat you'll do\nWe're looking for someone to understand the requirements and needs of our Wealth business stakeholders. You'll develop good relationships with them, form hypotheses, and identify suitable data science solutions to meet their needs to achieve our business strategy.\nYou'll be maintaining and developing external curiosity around new and emerging trends within data science, keeping up to date with emerging trends and tooling and sharing updates within and outside of the team.\nYou'll also be responsible for:\nProactively bringing together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques, and algorithms\nImplementing ethically sound models end-to-end and applying software engineering and a product development lens to complex business problems\nWorking with and leading both direct reports and wider teams in an agile way within multi-disciplinary data to achieve agreed project and Scrum outcomes\nUsing your data translation skills to work closely with business stakeholders to define business questions, problems or opportunities that can be supported through advanced analytics\nSelecting, building, training, and testing complex machine models, considering model valuation, model risk, governance, and ethics throughout to implement and scale models\nThe skills you'll need\nTo be successful in this role, you'll demonstrate evidence of leading data science development from idea to implementation and relevant work experience gained from delivering data science solutions as part of a multi-disciplinary team. We'll also expect you to hold an undergraduate or a master's degree in a quantitative discipline, or evidence of equivalent practical experience.\nYou'll also need a minimum of ten years of experience with statistical software, database languages, big data technologies, cloud environments and machine learning on large data sets. And we'll look to you to bring the ability to demonstrate leadership, self-direction and a willingness to both teach others and learn new techniques. Knowledge of the Wealth business and related data would be an advantage.\nAdditionally, you'll need:\nExperience of developing and deploying machine learning models and pipelines into a production environment\nExperience of translating business objectives into data science solutions with the ability to translate these to non-technical stakeholders\nExperience of working in cloud technology such as AWS, and programming languages such as Python and PySpark\nExtensive applied statistical data analysis experience, such as, linear models, multivariate analysis, stochastic models, and sampling methods\nEvidence of applied machine learning models including regression, classification, clustering, propensity models, and Gen AI",
        "skills": [
            "Gen AI",
            "data science tools",
            "Data Scientist",
            "Pyspark",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Lowe s",
        "experience": "2-7 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Retail Technology",
        "job_description": "We are seeking an inspiring, technically savvy, data scientist who is passionate about building a best-in-class experimentation platform/program to support our rapidly growing suite of eCommerce products.\nAs a Senior Data Scientist, you will be a key player in driving data-driven decision-making across the organization, collaborating closely with engineering, product, marketing, and other cross-functional teams to deliver insights and products that shape the future of our business. You will also mentor junior data scientists and help to foster a culture of experimentation throughout the organization.\nRoles & Responsibilities:\nCore Responsibilities:\nData Pipeline Development and Optimization:\nDesign, develop, and maintain robust data pipelines to ensure efficient and accurate data flow from various sources to the data warehouse.\nEnsure the integrity and consistency of experimental data, enabling accurate analysis and reliable insights to drive decision-making and optimize business strategies.\nExperimentation Design & Analysis:\nSupport the design and execution of A/B tests, multivariate experiments, and randomized controlled trials (RCTs) to assess the impact of product changes, marketing campaigns, and customer experiences.\nDevelop and implement robust methodologies to measure the effectiveness of business initiatives (e.g., website features, promotions, UI changes, etc.) using experimentation frameworks.\nOwn the end-to-end experimentation pipeline, including hypothesis generation, experimental design, implementation, monitoring, and post-experiment analysis.\nIdentify and mitigate biases in experiment design and results, ensuring statistical rigor and reliability.\nAdvanced Statistical Analysis & Modelling:\nConduct advanced statistical analysis (e.g., causal inference, Bayesian analysis, regression modelling) to derive actionable insights from experimentation results.\nDevelop and refine models to predict customer behavior and optimize conversion rates, retention, and other key business metrics.\nAnalyze large-scale datasets and design efficient algorithms to support decision-making in areas like pricing, product recommendations, and personalization.\nContinuous Improvement & Innovation:\nStay current with the latest advancements in data science, statistics, and experimentation methodologies.\nPropose innovative approaches to enhance the experimentation framework, such as new experimental designs, alternative modelling techniques, or improved metrics.\nLead or participate in research to explore new ways of measuring and optimizing the customer journey in a retail/e-commerce setting.\nYears of Experience:\n5+ years of professional experience in data science, with at least 2 years focused on experimentation, A/B testing, and causal inference in a retail or e-commerce environment.\nProven track record of designing and analyzing large-scale A/B tests and experiments with demonstrable business impact.\nStrong experience with statistical analysis and modelling techniques, including hypothesis testing, regression analysis, and Bayesian statistics\nEducation Qualification & Certifications (optional)\nRequired Minimum Qualifications:\nPh.D. or master s degree in data science, Statistics, Mathematics, Computer Science, Economics, or a related field.\nSkill Set Required\nAdvanced knowledge of statistical methodologies for experiment design, analysis, and causal inference.\nExpertise in analytics/data software/tools such as Python, R, SQL, and experience with machine learning frameworks (e.g., TensorFlow, scikit-learn).\nStrong communication skills, with the ability to explain complex technical concepts to non-technical stakeholders and executive leadership.\nSolid understanding of e-commerce and retail metrics (e.g., conversion rate, customer lifetime value, churn, etc.) and how they relate to experimentation.\nSecondary Skills (desired)\nExperience with large-scale e-commerce platforms and digital product development.\nFamiliarity with the advanced causal and inferential analytics\nExperience with advanced techniques in machine learning or AI that complement experimentation (e.g., recommender systems, predictive modelling).\nFamiliarity with cloud-based platforms (e.g., AWS, Google Cloud, Azure).\nExperience working in an agile environment and collaborating with cross-functional teams in a fast-paced business setting.",
        "skills": [
            "Product management",
            "Supply Chain",
            "Analytics",
            "Machine Learning",
            "Agile",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai City, Mumbai",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Flexera Software",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Consulting",
        "job_description": "Key Deliverables:\nAutomate data curation workflows using ML techniques.\nDevelop and deploy models for data cleaning, validation, and standardization.\nLead ML solution design, experimentation, and performance evaluation.\nCollaborate with engineering teams to scale AI/ML pipelines in cloud environments.\nRole Responsibilities:\nDrive AI/ML initiatives to reduce manual data processing.\nConduct exploratory data analysis for feature engineering and insights.\nImplement deep learning models using modern frameworks.\nMentor team members and promote data science best practices.",
        "skills": [
            "scikit-learn",
            "Data Processing",
            "Tensorflow",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Principal Data Scientist - Search",
        "company_name": "Lowe s",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Retail Technology",
        "job_description": "We are seeking a talented Principal Data Scientist to lead the development of Product Search and Recommendation systems. The ideal candidate will have a strong background in natural language processing (NLP), machine learning, deep learning, and semantic understanding, and will be passionate about transforming the way users discover relevant content. As a Principal Data Scientist, you will be responsible for driving the vision and strategy for Ecommerce search and personalized recommendation systems, leveraging state-of-the-art techniques such as transformer models, embeddings, and knowledge graphs. You will collaborate closely with product teams, engineers, and business stakeholders to enhance user experience and business outcomes.\nRoles & Responsibilities\nLeadership & Strategy\nLead the development of cutting-edge Search and recommendation algorithms that improve relevance, personalization, and user engagement.\nDefine and execute the strategy for semantic understanding and recommendations, aligning with overall business goals.\nWork cross-functionally with product managers, engineers, and data scientists to drive the roadmap for search and recommendation improvements.\nSearch & NLP Expertise\nLead the design and implementation of Search/Recommendation models that go beyond traditional keyword matching to understand user intent and context.\nApply advanced NLP techniques such as transformers (BERT, GPT, T5), word embeddings, and contextualized word representations to enhance search relevance.\nUse techniques like sentence embeddings, document embeddings, and similarity measures to build scalable search systems that understand semantic meaning.\nModel Development & Experimentation\nConduct research and experiments to design, develop, and validate new Search models and Recommendation techniques.\nContinuously test, measure, and optimize models through A/B testing, real-time metrics, and user feedback loops.\nDevelop approaches to handle large-scale data, ensuring the models can be deployed efficiently in production environments.\nCollaboration & Mentorship\nLead and mentor a team of data scientists, guiding them in the development of semantic models and complex recommendation algorithms.\nFoster a culture of knowledge sharing and collaboration across teams to ensure that the best practices are followe'd in model development and deployment.\nCollaborate with engineers & product managers to ensure the effective integration of models into scalable production systems.\nThought Leadership\nStay up to date with the latest research in Search, NLP, and recommendation systems.\nEvangelize the use of cutting-edge techniques within the company to drive innovation in search and recommendations.\nYears of Experience\n8 years of experience executing and deploying data science, machine learning, deep learning, and generative AI solutions, preferably in a large-scale enterprise setting (fewer years may be accepted with a masters or doctorate degree)\n8 years of programming experience (fewer years may be accepted with a masters or doctorate degree)\n5 years of SQL experience and knowledge of various statistical modeling or machine learning techniques\nBachelors degree in mathematics, statistics, physics, economics, engineering, computer science, data or information science, or related quantitative analytic field (or equivalent work experience in lieu of degree)\nCandidates with Doctorate or masters degree are preferred\nEducation Qualification & Certifications\nbachelors degree (Required): Mathematics, Statistics, Physics, Economics, Engineering, Computer Science, Data or Information Science, or related quantitative analytic field (or equivalent work experience in a related field)\nDoctorate Degree (Preferred): Mathematics, Statistics, Physics, Economics, Engineering, Computer Science, Data or Information Science, or related quantitative analytic field\nSkill Set Required\nMachine Learning & AI\nSupervised/unsupervised learning (eg, regression, clustering)\nDeep learning (CNNs, RNNs, Transformers)\nNatural Language Processing (NLP) for search relevance\nExperience with generating vector embeddings\nStatistical & Mathematical Expertise\nProbability, statistics, and A/B testing\nLeadership & Collaboration\nCross-functional team collaboration (engineering, product, design)\nMentoring junior data scientists\nCommunicating technical concepts to non-technical stakeholders\nTools & Frameworks\nProgramming (Python, R, Scala)\nML frameworks (TensorFlow, PyTorch, scikit-learn)\nVersion control (Git)\nPerformance Evaluation\nModel evaluation using metrics (precision, recall, NDCG)\nOnline learning and incremental model updates\nUnderstanding of Data Engineering & Infrastructure\nBig data technologies (Apache Spark, Hadoop)\nData pipeline management (ETL processes)\nDatabase management (SQL, NoSQL, Elasticsearch)\nCloud platforms (AWS/GCP/Azure)",
        "skills": [
            "Version Control",
            "Gcp",
            "Machine Learning",
            "Natural Language Processing",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Associate Data Scientist",
        "company_name": "Amgen Inc",
        "experience": "2-6 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Biotechnology",
        "job_description": "Job Description: Data Scientist Healthcare & Product Analytics\nResponsibilities:\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing, and data analytics.\nGenerate reports, annotated code, and other project artifacts to document, archive, and communicate your work and outcomes.\nWhat We Expect of You\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster's degree and 1 to 3 years of Data Science experience with one or more analytic software tools or languages, and data visualization tools OR\nBachelor's degree and 3 to 5 years of Data Science experience with one or more analytic software tools or languages, and data visualization tools OR\nDiploma and 7 to 9 years of Data Science experience with one or more analytic software tools or languages, and data visualization tools\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction, and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering, and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Databricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",
        "skills": [
            "Databricks platform",
            "Data Collection & Cleaning",
            "applied analytics",
            "Data Management & Warehousing",
            "Data Science",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Information Technology",
        "job_description": "Key Qualifications\nExperience:\n12 years of software development experience (preferred)\n12 years of experience as a Data Scientist, with a focus on Time Series forecasting\n4+ years working with Azure Cloud platforms, including:\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF) Pipelines\nTechnical Skills:\nML Ops / DevOps practices and tooling\nCI/CD pipeline configuration and deployment\nMLFlow for experiment tracking and model management\nStrong programming skills in Python\nExperience with machine learning models such as:\nRandom Forest\nXGBoost\nLightGBM\nOther ensemble modeling algorithms\nEducation:\nBachelor's or Master's degree in Software Engineering, Computer Science, Statistics, or a related field\nPhD not required\nSoft Skills:\nFast learner with the ability to adapt to changing priorities\nStrong problem-solving and collaboration skills\nRole Responsibilities\nFunction as a developer with a strong focus on:\nML model implementation, experimentation, and deployment\nApplying software engineering best practices in machine learning projects\nConduct data analysis and work with forecasting algorithms, especially for time series data\nCollaborate with cross-functional teams to deliver production-level ML solutions\nTools & Technologies\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF)\nMLFlow\nCI/CD tools (e.g., GitHub Actions, Azure DevOps, Jenkins)\nPython and relevant ML libraries (e.g., scikit-learn, pandas, numpy, etc.)\nWhat We Offer\nOpportunity to work on cutting-edge ML projects\nA highly motivated and collaborative team\nCompetitive salary\nFlexible schedule\nBenefits package including:\nMedical insurance\nSports membership or wellness stipend\nCorporate social events\nProfessional development opportunities\nModern, well-equipped office environment",
        "skills": [
            "Forecasting",
            "Data Analysis",
            "Statistics",
            "Medical Insurance",
            "Devops",
            "Cloud",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist / ML Engineer",
        "company_name": "RARR Technologies",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Overview:\nWe are seeking a talented and innovative Data Scientist to join our growing team. In this role, you will be responsible for applying machine learning and generative AI techniques to solve complex problems, drive business insights, and enhance product offerings. You will collaborate with engineers to build and deploy ML models in microservices architectures, ensuring the solutions are scalable, maintainable, and integrated with APIs.\nKey Responsibilities:\nAnalyze large, structured, and unstructured datasets to extract meaningful insights and identify business opportunities.\nDevelop, test, and implement machine learning and generative AI models to drive intelligent decision-making and automation.\nWork closely with engineering teams to integrate machine learning models into production systems using microservices architectures.\nDesign and develop APIs to enable seamless communication between data models and applications.\nBuild and maintain scalable data pipelines to facilitate data collection, transformation, and storage for model training and inference.\nUtilize Python and relevant libraries (e.g., Pandas, NumPy, TensorFlow, PyTorch) to preprocess data, train models, and perform statistical analysis.\nCollaborate with product and business teams to understand requirements and deploy machine learning solutions that meet business needs.\nPerform continuous monitoring, testing, and optimization of deployed models to ensure high performance and reliability.\nStay updated on the latest trends and advancements in machine learning, generative AI, and related fields to apply cutting-edge techniques in your work.\nDocument processes, methodologies, and model outputs for transparency and future improvements.\nRequired Skills & Qualifications:\nBachelors or Masters degree in Computer Science, Data Science, Statistics, or related field.\nProven experience in Python programming, including the use of libraries such as Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, or Keras for machine learning.\nHands-on experience with microservices architecture and containerization technologies like Docker or Kubernetes.\nExperience with building and deploying APIs to enable machine learning models to interact with other systems and applications.\nStrong understanding of Machine Learning algorithms, including supervised and unsupervised learning, deep learning, and generative AI techniques such as GANs (Generative Adversarial Networks) and language models.\nAbility to work with cloud platforms (AWS, GCP, or Azure) for model deployment and scalability.\nKnowledge of data engineering concepts, including data wrangling, ETL processes, and working with distributed systems.\nFamiliarity with modern version control systems (e.g., Git) and agile development practices.\nStrong analytical and problem-solving skills with the ability to communicate complex technical concepts to non-technical stakeholders.\nExperience with data visualization tools (e.g., Tableau, Power BI, or Matplotlib) is a plus.\nPreferred Qualifications:\nExperience with advanced Generative AI models, such as transformers (e.g., GPT, BERT).\nKnowledge of DevOps practices and CI/CD pipelines for machine learning deployment.\nFamiliarity with the integration of ML models into business applications and customer-facing products.\nStrong communication skills and the ability to collaborate with cross-functional teams including engineers, product managers, and business stakeholders.\nSkills Mentioned:\nAPI, Python, Generative AI, Machine Learning, Microservices",
        "skills": [
            "Generative AI",
            "Data Science",
            "Machine Learning",
            "Apis",
            "Python",
            "Microservices"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology",
        "job_description": "Key Qualifications\nExperience:\n12 years of software development experience (preferred)\n12 years of experience as a Data Scientist, with a focus on Time Series forecasting\n4+ years working with Azure Cloud platforms, including:\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF) Pipelines\nTechnical Skills:\nML Ops / DevOps practices and tooling\nCI/CD pipeline configuration and deployment\nMLFlow for experiment tracking and model management\nStrong programming skills in Python\nExperience with machine learning models such as:\nRandom Forest\nXGBoost\nLightGBM\nOther ensemble modeling algorithms\nEducation:\nBachelor's or Master's degree in Software Engineering, Computer Science, Statistics, or a related field\nPhD not required\nSoft Skills:\nFast learner with the ability to adapt to changing priorities\nStrong problem-solving and collaboration skills\nRole Responsibilities\nFunction as a developer with a strong focus on:\nML model implementation, experimentation, and deployment\nApplying software engineering best practices in machine learning projects\nConduct data analysis and work with forecasting algorithms, especially for time series data\nCollaborate with cross-functional teams to deliver production-level ML solutions\nTools & Technologies\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF)\nMLFlow\nCI/CD tools (e.g., GitHub Actions, Azure DevOps, Jenkins)\nPython and relevant ML libraries (e.g., scikit-learn, pandas, numpy, etc.)\nWhat We Offer\nOpportunity to work on cutting-edge ML projects\nA highly motivated and collaborative team\nCompetitive salary\nFlexible schedule\nBenefits package including:\nMedical insurance\nSports membership or wellness stipend\nCorporate social events\nProfessional development opportunities\nModern, well-equipped office environment",
        "skills": [
            "Forecasting",
            "Data Analysis",
            "Statistics",
            "Medical Insurance",
            "Devops",
            "Cloud",
            "Python"
        ]
    },
    {
        "job_title": "RBL Bank | Hiring For Data Scientist - Airoli",
        "company_name": "RBL FinServe",
        "experience": "4-8 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai City, Mumbai",
        "industry": "Banking",
        "job_description": "Position Purpose\nSolution and deliver analytics offerings for the bank to drive revenue or enable cost optimization\nBuild models, move them to production and maintain/ enhance on an ongoing basis in production\nBecome an analytics consultant and evangelist within the bank finding analytical solutions to business problems\nPosition Responsibilities\nWork closely with the data warehouse team/other business teams to obtain relevant data for implementation. Be comfortable working with structured / unstructured data sources and be conversant to perform secondary research to explore third party data sources to enrich existing data\nSupport the overall digital acquisition strategy by focusing on segmenting/ predicting response rates for leads which are a pre-requisite for improving response rates\nCreate/supervise building of models around channel migration, cross sell, upsell and support the overall customer engagement strategy\nSupport the implementation of various technology (recommendation engine, campaign management solution, CRM) / data enablers (Creation of data sets, mart etc.) for the analytics practice within the bank\nImplementation of specific use cases on big data platforms\nQualifications and Experience Requirement\nQualifications\nEssential: Graduate (B.E / B.Sc Stats / M.Sc Stats or equivalent)\nExperience\nEssential:\n3 plus years in the analytics space\nManaged diverse stakeholders from various teams, in complex environments\nGrasp of basic Supervised/Unsupervised ML algorithms and a demonstrated ability to learn quickly\nThorough understanding of banking domain would be a plus point\nExperience in working with SQL & R/other similar statistical programming languages. Knowledge of other statistical programming languages like Python will be an added advantage\nSkills\nSkill Attribute\nTeam player, detail oriented, self-motivated individual\nCandidate should have a strong understanding of analytical modeling techniques and statistical concepts that are relevant to the application and evaluation of models",
        "skills": [
            "Analytics",
            "r",
            "Statistical Modeling",
            "Machine Learning",
            "python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Pitney Bowes (PBI)",
        "experience": "3-4 Years",
        "salary": null,
        "location": "Noida",
        "industry": "Information Technology",
        "job_description": "Identify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams\nRequirements and skills\nProven experience as a Data Scientist or Data Analyst\nExperience in data mining\nUnderstanding of machine-learning and operations research\nKnowledge of R, SQL and Python; familiarity with Scala, Java or C++ is an asset\nExperience using business intelligence tools (e.g. Tableau) and data frameworks\nAnalytical mind and business acumen.\nProblem-solving aptitude\nExcellent communication and presentation skills\nBSc/BA in Computer Science, Engineering or relevant field; graduate degree in Data Science or other quantitative field is preferred\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate",
        "skills": [
            "Analytical",
            "Computer Science",
            "Operations Research",
            "Business Intelligence",
            "Machine Learning",
            "Data Management",
            "C++",
            "Data Mining",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "NatWest Group",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Banking",
        "job_description": "Job Description\nJoin us as a Data Scientist\nYou'll design and implement data science tools and methods which harness our data in order to drive market leading purposeful customer solutions\nWe'll look to you to actively participate in the data community to identify and deliver opportunities to support the bank's strategic direction through better use of data\nThis is an opportunity to promote data literacy education with business stakeholders supporting them to foster a data driven culture and to make a real impact with your work\nWhat you'll do\nAs a Data Scientist, you'll bring together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques and algorithms to develop and implement ethically sound models end-to-end. We'll look to you to understand the needs of business stakeholders, form hypotheses and identify suitable data and analytics solutions to meet those needs in order to support the achievement of our business strategy.\nYou'll also be:\nUsing data translation skills to work closely with business stakeholders to define detailed business questions, problems or opportunities which can be supported through analytics\nApplying a software engineering and product development lens to business problems, creating, scaling and deploying software driven products and services\nWorking in an Agile way within multi-disciplinary data and analytics teams to achieve agreed project and scrum outcomes\nSelecting, building, training and testing machine learning models considering model valuation, model risk, governance and ethics, making sure that models are ready to implement and scale\nIteratively building and prototyping data analysis pipelines to provide insights that will ultimately lead to production deployment\nThe skills you'll need\nYou'll need a strong academic background in a STEM discipline such as Mathematics, Physics, Engineering or Computer Science. You'll have experience with statistical modelling and machine learning techniques.\nWe'll also look for financial services knowledge, and an ability to identify wider business impact, risk or opportunities and make connections across key outputs and processes\nYou'll also demonstrate:\nThe ability to use data to solve business problems from hypotheses through to resolution\nExperience using programming language and software engineering fundamentals\nExperience of Cloud applications and options\nExperience in synthesising, translating and visualising data and insights for key stakeholders\nExperience of exploratory data analysis\nGood communication skills with the ability to proactively engage with a wide range of stakeholders",
        "skills": [
            "Cloud & Tooling Exposure",
            "STEM",
            "Data Handling & Analytics",
            "Matplotlib",
            "Data Science",
            "Machine Learning",
            "Power Bi",
            "Tableau"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "Essential Functions\nDesign, develop, and deploy forecasting algorithms using industry best practices\nWork on time series modeling and other predictive modeling techniques\nImplement and experiment with ML models as part of a developer-focused data science team\nDevelop scalable data pipelines and solutions using Python and PySpark\nCollaborate across engineering and analytics teams to deliver production-ready ML solutions\nKey Qualifications\nExperience:\n4+ years in a developer role focused on ML model implementation, experimentation, and related software engineering\n34 years of hands-on experience in Python and PySpark development\n34 years as a Data Scientist with strong focus on forecasting algorithms\n12 years of general software development experience\nExperience with Azure services, including:\nAzure Data Factory (ADF)\nAzure Pipelines\nAzure DevOps\nSkills & Tools:\nForecasting models and time series techniques (ARIMA, Prophet, exponential smoothing, etc.)\nML model development and evaluation\nPython (must-have)\nPySpark (must-have)\nML Ops tools and CI/CD workflows in Azure environment\nEducation:\nBachelor's or Master's degree in Computer Science, Software Engineering, Statistics, or a related discipline\nWhat We Offer\nOpportunity to work on cutting-edge, high-impact ML projects\nCollaborative and motivated team environment\nCompetitive salary and performance incentives\nFlexible working schedule\nComprehensive benefits package:\nMedical insurance\nSports or wellness benefits\nParticipation in corporate social events\nProfessional development support and career growth opportunities\nA modern, well-equipped office space",
        "skills": [
            "Algorithms",
            "Hive",
            "Software Development",
            "Natural Language Processing",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Grid Dynamics",
        "experience": "1-4 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "Key Qualifications\nExperience:\n12 years of software development experience (preferred)\n12 years of experience as a Data Scientist, with a focus on Time Series forecasting\n4+ years working with Azure Cloud platforms, including:\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF) Pipelines\nTechnical Skills:\nML Ops / DevOps practices and tooling\nCI/CD pipeline configuration and deployment\nMLFlow for experiment tracking and model management\nStrong programming skills in Python\nExperience with machine learning models such as:\nRandom Forest\nXGBoost\nLightGBM\nOther ensemble modeling algorithms\nEducation:\nBachelor's or Master's degree in Software Engineering, Computer Science, Statistics, or a related field\nPhD not required\nSoft Skills:\nFast learner with the ability to adapt to changing priorities\nStrong problem-solving and collaboration skills\nRole Responsibilities\nFunction as a developer with a strong focus on:\nML model implementation, experimentation, and deployment\nApplying software engineering best practices in machine learning projects\nConduct data analysis and work with forecasting algorithms, especially for time series data\nCollaborate with cross-functional teams to deliver production-level ML solutions\nTools & Technologies\nAzure Databricks\nAzure ML Studio\nAzure Data Factory (ADF)\nMLFlow\nCI/CD tools (e.g., GitHub Actions, Azure DevOps, Jenkins)\nPython and relevant ML libraries (e.g., scikit-learn, pandas, numpy, etc.)\nWhat We Offer\nOpportunity to work on cutting-edge ML projects\nA highly motivated and collaborative team\nCompetitive salary\nFlexible schedule\nBenefits package including:\nMedical insurance\nSports membership or wellness stipend\nCorporate social events\nProfessional development opportunities\nModern, well-equipped office environment",
        "skills": [
            "Forecasting",
            "Data Analysis",
            "Statistics",
            "Medical Insurance",
            "Devops",
            "Cloud",
            "Python"
        ]
    },
    {
        "job_title": "Sr Advanced Data Scientist",
        "company_name": "Honeywell",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, Noida",
        "industry": "Software",
        "job_description": "This role is part of Honeywell V&V, where you will work as an AI Engineer on new product concepts and ideas, collaborating with global teams to develop innovative products and design solutions. Your responsibilities will include Advanced Concepts Development, Rapid Prototyping, and Integration.\nYou will demonstrate technical leadership in deploying AI solutions and have end-to-end ownership of projects from concept definition through to hardware deployment. You will also be involved in customer interaction, solution mapping, and influencing decisions for concept-to-product migration.\nJob Responsibilities\nTechnical Leadership:\nDeploy AI solutions with full ownership, from concept definition to deployment on hardware.\nCollaborate actively with stakeholders for new technologies and ideas incubation.\nResearch & Development:\nSelf-motivated in Intellectual Property (IP) creation for the product in your area of expertise.\nResearch, prototype, and develop new solutions, while mapping the right solutions to customer needs.\nPrototyping and Deployment:\nWork on the prototyping and deployment of new AI solutions.\nLead and work with the team to solve complex technical problems related to defect detection, inspections, and camera calibration, data fusion, and target tracking.\nProject Coordination:\nCoordinate with system engineers and project managers on product customization and project execution.\nInfluence decisions and guide the project through concept to product migration.\nCustomer Collaboration:\nWork closely with internal and external customers to understand their objectives.\nPresent new ideas and prototypes, and offer solutions that align with customer needs.\nSkills and Qualifications\nEducation:\nB.E / B.Tech / M.E / M.Tech in Electrical / Electronics / Computers.\nExperience:\n6+ years of experience in design and development of AI-based solutions.\nTechnical Expertise:\nProficiency with software development processes, especially working on Linux/Unix-based systems.\nExpertise in Machine Learning/AI Life Cycle with a strong command of Python and C++ for hardware deployment.\nHands-on experience with deep learning algorithm development is preferred.\nKnowledge of object-oriented programming is advantageous.\nKey Skills:\nAbility to multitask and work with minimal supervision.\nStrong communication, collaboration, and networking skills.\nKnowledge of AI/ML concepts, particularly for target tracking and perception using camera, lidar, and radar sensors.\nEffective problem-solving skills for defect detection, inspections, and sensor calibration.\nAbility to formulate and prototype new solutions, contributing to the development of innovative products.\nJob Distribution\nPrototyping/Deployment: 40%\nProject Coordination: 30%\nResearch and Development: 30%",
        "skills": [
            "C++",
            "Unix",
            "Deep Learning",
            "Linux",
            "Machine Learning",
            "Algorithm Development",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HCL Technologies Limited",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Hyderabad, Bengaluru",
        "industry": "Software",
        "job_description": "We are looking for a data scientist who will help us discover the information hidden in vast amounts of data, and help us make smarter decisions to deliver AI/ML based Enterprise Software Products.\nDevelop solutions related to machine learning, natural language processing and deep learning & Generative AI to address business needs.\nYour primary focus will be in applying Language/Vision techniques, developing llm based applications and building high quality prediction systems.\nAnalyze Data: Collaborate with cross-functional teams to understand data requirements and identify relevant data sources. Analyze and preprocess data to extract valuable insights and ensure data quality.\nEvaluation and Optimization: Evaluate model performance using appropriate metrics and iterate on solutions to enhance performance and accuracy. Continuously optimize algorithms and models to adapt to evolving business requirements.\nDocumentation and Reporting: Document methodologies, findings, and outcomes in clear and concise reports. Communicate results effectively to technical and non-technical stakeholders.\nWork experience background required:\nExperience building software from the ground up in a corporate or startup environment.\nEssential skillsets required:\n3-6 years experience in software development\nEducational Background: Strong computer science and Math/Statistics\nExperience with Open Source LLM and Langchain Framework and and designing efficient prompt for LLMs.\nProven ability with NLP and text-based extraction techniques.\nExperience in Generative AI technologies, such as diffusion and/or language models.\nExcellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.\nFamiliarity with cloud computing platforms such as GCP or AWS. Experience to deploy and monitor model in cloud environment.\nExperience with common data science toolkits, such as NumPy, Pandas etc\nProficiency in using query languages such as SQL\nGood applied statistics skills, such as distributions, statistical testing, regression, etc.\nExperience working with large data sets along with data modeling, language development, and database technologies\nKnowledge in Machine Learning and Deep Learning frameworks (e.g., TensorFlow, Keras, Scikit-Learn, CNTK, or PyTorch), NLP, Recommender systems, personalization, Segmentation, microservices architecture and API development.\nAbility to adapt to a fast-paced, dynamic work environment and learn new technologies quickly.\nExcellent verbal and written communication skills",
        "skills": [
            "Data Scientist",
            "Gcp",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Abbott",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Mumbai",
        "industry": "Medical Device, Medical",
        "job_description": "With the above requirements in mind, EPD is looking to fill a role of a Data Scientist to build and refine effective Data Science Solutions for Abbott EPD world-wide.\nCore Job Responsibilities:\nThe Data Scientist rapidly navigates from identifying priorities and helping to generate ideas to implementing solutions. They\nParticipate/drive data collection, cleaning, analysis and interpretation (EDA).\nCollaborate with the business partner and product owners to ideate on solutions to challenging problems.\nGenerate insightful visualizations to communicate findings.\nCarry out model selection, validation and possible ways for deployment (in collaboration with the engineering team).\nWrite high quality code with possibility of deployment in mind.\nShare the learnings and findings with other data scientists contributing to the collaborative environment.\nCollaborate with Sr. Data Scientists and take full responsibility for analysis and modeling tasks.\nBuild effective and efficient AA solutions to business needs, leveraging available market resources as much as possible.\nKeep himself/herself committed to continuous learning about the latest trends and technologies.\nWork closely with the Product Owners and the Engineering team to ensure delivery of the Data Science part of the projects within time, cost and quality.\nCollaborate with external vendors, evaluating their capabilities and ensuring their alignment with data science standards and project requirements.\nContinuously engage in hands-on data analysis, modeling, and prototyping DS frameworks to deliver high-quality outputs.\nSupervisory/Management Responsibilities:\nDirect Reports:None.\nIndirect Reports:None.\nPosition Accountability/Scope:\nThe Data Scientist is responsible for delivering targeted business impact per initiative in collaboration with key stakeholders and identifying next steps/future impactful opportunities. This individual contributor role involves working with cross-functional teams to build innovative solutions for internal business functions across different geographies.\nMinimum Education:\nMaster or PhD in relevant field (e.g., applied mathematics, computer science, engineering, applied statistics)\nMinimum Experience:\nAt least 3-5 years of relevant working experience, ideally in pharma environment\nSolid experience working on full-life cycle data science; experience in applying data science methods to business problems (experience in the financial/commercial or manufacturing / supply chain areas a plus).\nStrong experience in e.g., data mining, statistical modelling, predictive modelling, and development of machine learning algorithms\nProven problem-solving ability in international settings preferably with developing markets\nProven experience in working in cloud environment preferably AWS / Sagemaker\nStrong experience working on full-life cycle data science; experience in applying data science methods to business problems\nPractical experience in deploying machine learning solutions\nStrong understanding of good software engineering principles and best practices\nAbility to work and lead cross-functional teams to bring business and data science closer together - consultancy experience a plus\nIntrinsic motivation to guide people and make Advanced Analytics more accessible to a broader range of stakeholders\nDeep domain expertise in a specific field, such as Artificial Intelligence, Machine Learning, Natural Language Processing, or Computer Vision\nStrong programming skills in languages such as Python or R, with proficiency in data manipulation, wrangling, and modeling techniques\nStrong experience building and debugging complex SQL queries\nExcellent knowledge of statistical techniques, machine learning algorithms, and their practical implementation in real-world scenarios\nExceptional communication and presentation skills, with the ability to convey complex concepts and insights to both technical and non-technical stakeholders\nProven track record of delivering data-driven solutions that have had a measurable impact on business outcomes\nExposure to big data technologies (e.g., Hadoop, Spark) is highly desirable\nDemonstrated ability to drive the adoption of data science best practices, standards, and methodologies within an organization\nFluency in English a must, additional languages a plus\nRole:Full Stack Data Scientist\nIndustry Type:Medical Devices & Equipment\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate",
        "skills": [
            "Supply Chain",
            "Data Analysis",
            "Python.",
            "Pharma",
            "Data Collection",
            "Computer Science",
            "Computer Vision",
            "Debugging",
            "Machine Learning",
            "Data mining"
        ]
    },
    {
        "job_title": "Advanced Data Scientist",
        "company_name": "Honeywell",
        "experience": "6-9 Years",
        "salary": null,
        "location": "Bengaluru, Noida",
        "industry": "Software",
        "job_description": "Responsibilities:\nConduct research aligned to roadmaps to de-risk, mature, and insert advanced AI concepts into Honeywell products (latest advancements in Generative AI, Causal AI, Knowledge Graphs, Machine Learning, and Deep Learning techniques).\nResearch, design, and implement scalable AI-based solutions to solve high-value, challenging customer problems and drive new growth opportunities.\nLead prototyping (what-if) efforts supporting pilot programs for R&D purposes.\nCollaborate with cross-functional teams to develop data-driven solutions.\nResponsible for problem analysis, stakeholder interaction, solution design, front-end and back-end integration, maintenance, and support of data science and analytics solutions.\nDevelop guidelines and standards for analytics and machine learning models, their deployment, and associated processes.\nYou Must Have:\nOverall 6 to 9 years of experience.\nBachelor's or Master's degree in Engineering, Applied Mathematics, or a related field.\nStrong analytics skills with the ability to assess data, extract insights, and make recommendations.\nExperience manipulating data sets and building statistical models.\nStrong problem-solving skills, with an emphasis on product development.\nExperience supporting fast-paced startup engineering teams.\nIn-depth understanding of AI and ML trends, especially generative models and LLMs (Cloud and On-Prem).\nProficiency in programming languages and libraries.\nTechnical Skills:\nCloud Platforms: AWS, Azure\nLanguages: Advanced Python, PySpark\nFrameworks: Lambda, Django, Express\nDatabases: Postgres, MongoDB, Elasticsearch\nTesting: Automated unit testing with PyTest\nHands-on experience in design and development of performant, extensible applications\nExperience Requirement:\nMinimum 6 years of overall experience\nMinimum 4 years of relevant experience in data science-related product development",
        "skills": [
            "Analytics",
            "Deep Learning",
            "Front End",
            "Data Science",
            "Unit Testing",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist Lead",
        "company_name": "Wipro Limited",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Information Technology, Information Services",
        "job_description": "Role Purpose\nThe purpose of the role is to define, architect, and lead the delivery of machine learning (ML) and artificial intelligence (AI) solutions.\nKey Responsibilities\nDemand Generation\nSupport solution development by creating and refining AI/ML solutions, contributing to proof-of-concepts, and aligning offerings for solution-led sales. Collaborate with sales, pre-sales, and consulting teams to define AI/ML propositions and drive demand generation. Partner with colleges and institutes for recruitment, joint research initiatives, and delivery of data science courses.\nRevenue Generation\nBuild and operationalize machine learning and deep learning solutions for decision augmentation and automation. Develop and deploy ML/DL models, working closely with ML engineers, data engineers, and IT teams to evaluate deployment options. Integrate model performance management tools into the business infrastructure to ensure continuous improvement.\nTeam Management\nSupport recruitment to onboard suitable resources and provide onboarding, training, and skill enhancement opportunities to team members. Manage team attrition, conduct performance reviews, and offer constructive feedback. Act as a role model for organizational values, ensure adherence to performance frameworks, and lead employee engagement initiatives to enhance satisfaction and effectiveness.\nStakeholder Interaction\nInternally, collaborate with practice leads for requirements and inputs, and support sales, pre-sales, and consulting teams in defining AI/ML propositions. Work with talent transformation and competency groups to plan and deliver technical training. Externally, engage with clients to deliver projects and establish trust while collaborating with academic institutions on research, training, and data science courses.\nCompetencies Required\nFunctional Competencies\nPossess foundational knowledge of the domain, industry trends, and market intelligence, with the ability to apply systems thinking for problem-solving in complex environments. Demonstrate competence in leveraging technology and applying research methods to achieve results.\nBehavioral Competencies\nExhibit innovation, client-centricity, execution excellence, passion for results, team management, and stakeholder management.\nPerformance Deliverables\nThe role requires driving demand generation through successful order booking, delivering revenue through timely project completion, customer success stories, and use cases. It also involves building team capability through training and managing team attrition effectively.",
        "skills": [
            "Deep Learning",
            "Machine Learning",
            "python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Capgemini Technology Services India Limited",
        "experience": "6-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Capgemini Invent\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\nYour Role\nSeeking an experienced and result-oriented individual to join the People Analytics team in the role of Data Scientist-Senior Consultant.\nStrong analytical, statistical, and programming skills are essential.\nAbility to understand and break down complex business problems, define a solution, and implement using advanced analytical methods.\nCollaborate with Data Scientists and contribute individually to deliver analytical solutions.\nExcellent written and verbal communication skills for coordinating across teams and explaining solutions.\nGood to have knowledge of people/HR data & systems but not necessary.\nYour Profile\nApplied statistics skills such as distributions, statistical testing, regression, and proficient application of generative AI techniques for advanced data analysis.\nDeep knowledge and experience in creating and using advanced machine learning algorithms (Artificial Neural Networks, Naive Bayes, SVM, Decision Forests, etc.).\nGood working knowledge of machine learning packages like Pandas, NumPy, Scikit-learn, Keras, TensorFlow.\nWhat you will love about working here\nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\nExperience in model training and deployment on any of the Cloud environments like AWS, GCP, or Azure.\nStrong data manipulation skills using SQL, PLSQL, and hands-on working knowledge of Pyspark, Python, R, and Big Data (ability to pick up any new language as required).\nGood to have experience in Text analytics like NLP.\nGeneral data engineering skills, including expertise in data manipulation, transformation, and integration.",
        "skills": [
            "Data Scientist",
            "Plsql",
            "Svm",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Birlasoft Limited",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software Engineering",
        "job_description": "Required Skills\n5+ years of experience with Treasure Data, including data ingestion, workflows, and CDP capabilities.\nStrong understanding of pharma commercial operations, including HCP targeting, patient journey mapping, and sales analytics.\nProficiency in SQL and experience with data transformation workflows.\nHands-on experience with integrating Treasure Data into cloud ecosystems (AWS, Azure, GCP).\nFamiliarity with marketing automation platforms (e.g., Salesforce Marketing Cloud, Adobe Campaign).\n\nKnowledge of compliance standards in the pharma industry (e.g., HIPAA, GDPR",
        "skills": [
            "Data Scientist",
            "Azure"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Capgemini Technology Services India Limited",
        "experience": "6-9 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Your Role\nSeeking an experienced and result-oriented individual to join the People Analytics team in the role of Data Scientist-Senior Consultant.\nStrong analytical, statistical, and programming skills are essential.\nAbility to understand and break down complex business problems, define a solution, and implement using advanced analytical methods.\nCollaborate with Data Scientists and contribute individually to deliver analytical solutions.\nExcellent written and verbal communication skills for coordinating across teams and explaining solutions.\nGood to have knowledge of people/HR data & systems but not necessary.\nYour Profile\nApplied statistics skills such as distributions, statistical testing, regression, and proficient application of generative AI techniques for advanced data analysis.\nDeep knowledge and experience in creating and using advanced machine learning algorithms (Artificial Neural Networks, Naive Bayes, SVM, Decision Forests, etc.).\nGood working knowledge of machine learning packages like Pandas, NumPy, Scikit-learn, Keras, TensorFlow.\nExperience in model training and deployment on any of the Cloud environments like AWS, GCP, or Azure.\nStrong data manipulation skills using SQL, PLSQL, and hands-on working knowledge of Pyspark, Python, R, and Big Data (ability to pick up any new language as required).\nGood to have experience in Text analytics like NLP.\nGeneral data engineering skills, including expertise in data manipulation, transformation, and integration.\nWhat you will love about working here\nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.",
        "skills": [
            "Pyspark",
            "Data Scientist",
            "Plsql",
            "Sql"
        ]
    },
    {
        "job_title": "Sr Advanced Data Scientist",
        "company_name": "Honeywell",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, Noida",
        "industry": "Software",
        "job_description": "This role is part of Honeywell V&V, where you will work as an AI Engineer on new product concepts and ideas, collaborating with global teams to develop innovative products and design solutions. Your responsibilities will include Advanced Concepts Development, Rapid Prototyping, and Integration.\nYou will demonstrate technical leadership in deploying AI solutions and have end-to-end ownership of projects from concept definition through to hardware deployment. You will also be involved in customer interaction, solution mapping, and influencing decisions for concept-to-product migration.\nJob Responsibilities\nTechnical Leadership:\nDeploy AI solutions with full ownership, from concept definition to deployment on hardware.\nCollaborate actively with stakeholders for new technologies and ideas incubation.\nResearch & Development:\nSelf-motivated in Intellectual Property (IP) creation for the product in your area of expertise.\nResearch, prototype, and develop new solutions, while mapping the right solutions to customer needs.\nPrototyping and Deployment:\nWork on the prototyping and deployment of new AI solutions.\nLead and work with the team to solve complex technical problems related to defect detection, inspections, and camera calibration, data fusion, and target tracking.\nProject Coordination:\nCoordinate with system engineers and project managers on product customization and project execution.\nInfluence decisions and guide the project through concept to product migration.\nCustomer Collaboration:\nWork closely with internal and external customers to understand their objectives.\nPresent new ideas and prototypes, and offer solutions that align with customer needs.\nSkills and Qualifications\nEducation:\nB.E / B.Tech / M.E / M.Tech in Electrical / Electronics / Computers.\nExperience:\n6+ years of experience in design and development of AI-based solutions.\nTechnical Expertise:\nProficiency with software development processes, especially working on Linux/Unix-based systems.\nExpertise in Machine Learning/AI Life Cycle with a strong command of Python and C++ for hardware deployment.\nHands-on experience with deep learning algorithm development is preferred.\nKnowledge of object-oriented programming is advantageous.\nKey Skills:\nAbility to multitask and work with minimal supervision.\nStrong communication, collaboration, and networking skills.\nKnowledge of AI/ML concepts, particularly for target tracking and perception using camera, lidar, and radar sensors.\nEffective problem-solving skills for defect detection, inspections, and sensor calibration.\nAbility to formulate and prototype new solutions, contributing to the development of innovative products.\nJob Distribution\nPrototyping/Deployment: 40%\nProject Coordination: 30%\nResearch and Development: 30%",
        "skills": [
            "C++",
            "Unix",
            "Deep Learning",
            "Linux",
            "Machine Learning",
            "Algorithm Development",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - Digital Solutions",
        "company_name": "Siemens",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Job description\nWe are looking for a Data Scientist who will extract valuable insights and patterns from data to help our customers make informed decisions and solve problems. You will work with large amounts of data, using data experiments to generate models with strong predictive power, and then automate these in a MLOps framework.\nKey Responsibilities:\nUnderstand customer challenges and suggest them actionable insights with clear business benefit of the proposed solution\nWork with large, complex data sets and applying advanced analytical methods as needed\nDevelop deployable machine learning solutions that address the real root cause\nInteract with multiple stakeholders (from different levels) and effectively present findings by exploiting visual displays of complex quantity information in a simplified way\nBe the partner of choice regarding technology advisory for advanced analytics\nActively and proactively perform market/trend scouting to stay up to date on the state-of-the-art technologies.\nMandatory Requirements:\n5 to 8 years of hands-on experience in software development.\nDegree in Data Science, computer science, mathematics or a related field\nStrong programming skills, including Scikit-learn and TensorFlow/Keres.\nProven experience with solving numerical optimization problems\nExperience with cloud platforms such as AzureML\nExperience working in agile environment\nTeam player who is able to communicate and collaborate we'll with others\nFluent in English (spoken and written)\nMechanical, chemical or other engineering background\nExperience working with industrial control systems and Industrial Automation\nGood to Have:\nExposure to other cloud platforms like Azure and open-source cloud components.\nHands-on experience with Docker, Kubernetes.\nPreferred Skills & Attributes:\nStrong understanding of modern software architectures and DevOps principles.\nAbility to analyze complex problems and develop effective solutions.\nExcellent communication and teamwork skills, with experience in cross-functional collaboration.\nSelf-motivated and capable of working independently on complex projects.",
        "skills": [
            ".Net Core",
            "Aws Cloud",
            "Azure ML",
            "Software Development"
        ]
    },
    {
        "job_title": "Data Scientist - Digital Solutions",
        "company_name": "Siemens",
        "experience": "5-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Key Responsibilities:\nUnderstand customer challenges and suggest them actionable insights with clear business benefit of the proposed solution\nWork with large, complex data sets and applying advanced analytical methods as needed\nDevelop deployable machine learning solutions that address the real root cause\nInteract with multiple stakeholders (from different levels) and effectively present findings by exploiting visual displays of complex quantity information in a simplified way\nBe the partner of choice regarding technology advisory for advanced analytics\nActively and proactively perform market/trend scouting to stay up to date on the state-of-the-art technologies.\nMandatory Requirements:\n5 to 8 years of hands-on experience in software development.\nDegree in Data Science, computer science, mathematics or a related field\nStrong programming skills, including Scikit-learn and TensorFlow/Keres.\nProven experience with solving numerical optimization problems\nExperience with cloud platforms such as AzureML\nExperience working in agile environment\nTeam player who is able to communicate and collaborate well with others\nFluent in English (spoken and written)\nMechanical, chemical or other engineering background\nExperience working with industrial control systems and Industrial Automation\nGood to Have:\nExposure to other cloud platforms like Azure and open-source cloud components.\nHands-on experience with Docker, Kubernetes.\nPreferred Skills & Attributes:\nStrong understanding of modern software architectures and DevOps principles.\nAbility to analyze complex problems and develop effective solutions.\nExcellent communication and teamwork skills, with experience in cross-functional collaboration.\nSelf-motivated and capable of working independently on complex projects.",
        "skills": [
            "ML and DL",
            "Kubernetes.",
            "Data Science",
            "Python",
            "Docker",
            "Azure"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Siemens",
        "experience": "7-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Manufacturing",
        "job_description": "Job description\nJob Responsibilities:\nBuild and maintain univariate time series feature analysis\nTest and integrate new univariate time series models into IBP process\nParticipate in the development and deployment of complex multivariate forecasting models\nInterface with IBP process representatives on model trouble shooting and new feature analysis\nInterface with analytical and subject matter experts throughout SHS organization in the pursuit of improved forecasting outcomes\nProvide expert consultation for focused reviews with management team when needed.\nMaintain documentation of and control framework for deployed models and tools\nSkill Set:\n7 to 10 years of experience\n3-5 years of experience with classic time series analysis tools (e.g. ARIMA, State Space Models, VAR, Error Correction Models etc.)\n3-5 years of experience with machine learning models including Decision Tree, Regression, Gradient Boosting, and Deep Learning\n3-5 years of experience of Python / R coding\n3-5 years of experience with SQL coding\nExperience in working on Azure tech stack\nExperience building and deploying production time series forecast models\nExperience managing code repositories.\nExperience in Decision Science area / Operations Research / Applied Mathematics, Statistics, Data Science or Computer Science.\nKnowledge on LLMs\nNice to have:\n6+ years of experience implementing and managing time series analysis and forecast models\n6+ years of experience implementing and managing machine learning models\nExperience working on projects in Supply Chain Management or Marketing analysis.\nExperience working with high volume time series forecasting\nKnowledge of In Vitro Diagnostics market space\nExperience building & deploying Structural Equation Models\nExperience utilizing MLFlow\nExperience working in SnowFlake\nFamiliarity with Azure Databricks\nFamiliarity with M Competition outcomes\nExperience in GenAI projects\nMasters in Economics, Operations Research, Applied Mathematics, Statistics, or Data Science",
        "skills": [
            "Azure tech stack",
            "R coding",
            "SQL coding",
            "Data Science",
            "Python"
        ]
    },
    {
        "job_title": "Prin Analytics Data Scientist",
        "company_name": "Honeywell",
        "experience": "6-13 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Consumer Electronics",
        "job_description": "Key Deliverables:\nLead development of Oracle Fusion HCM dashboards using OTBI, BIP, and FDI.\nDesign secure data models, star schemas, and oversee metadata and BI tool administration.\nDrive predictive analytics and AI/ML integration to optimize workforce planning and HR processes.\nGuide cross-functional analytics initiatives and ensure data integrity and compliance.\nRole Responsibilities:\nManage HR analytics programs and support senior HR leadership with data-driven insights.\nTranslate HR data into actionable intelligence using SQL, Python/R, and visualization tools.\nEnsure compliance with data security standards across Oracle HCM analytics.\nLead and mentor a team of analytics developers in delivering strategic outcomes.",
        "skills": [
            "Fusion Data Intelligence",
            "Python",
            "Otbi",
            "Oracle Hcm",
            "Bip",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HCL Technologies Limited",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Hyderabad, Bengaluru",
        "industry": "Software",
        "job_description": "We are looking for a data scientist who will help us discover the information hidden in vast amounts of data, and help us make smarter decisions to deliver AI/ML based Enterprise Software Products.\nDevelop solutions related to machine learning, natural language processing and deep learning & Generative AI to address business needs.\nYour primary focus will be in applying Language/Vision techniques, developing llm based applications and building high quality prediction systems.\nAnalyze Data: Collaborate with cross-functional teams to understand data requirements and identify relevant data sources. Analyze and preprocess data to extract valuable insights and ensure data quality.\nEvaluation and Optimization: Evaluate model performance using appropriate metrics and iterate on solutions to enhance performance and accuracy. Continuously optimize algorithms and models to adapt to evolving business requirements.\nDocumentation and Reporting: Document methodologies, findings, and outcomes in clear and concise reports. Communicate results effectively to technical and non-technical stakeholders.\nWork experience background required:\nExperience building software from the ground up in a corporate or startup environment.\nEssential skillsets required:\n3-6 years experience in software development\nEducational Background: Strong computer science and Math/Statistics\nExperience with Open Source LLM and Langchain Framework and and designing efficient prompt for LLMs.\nProven ability with NLP and text-based extraction techniques.\nExperience in Generative AI technologies, such as diffusion and/or language models.\nExcellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.\nFamiliarity with cloud computing platforms such as GCP or AWS. Experience to deploy and monitor model in cloud environment.\nExperience with common data science toolkits, such as NumPy, Pandas etc\nProficiency in using query languages such as SQL\nGood applied statistics skills, such as distributions, statistical testing, regression, etc.\nExperience working with large data sets along with data modeling, language development, and database technologies\nKnowledge in Machine Learning and Deep Learning frameworks (e.g., TensorFlow, Keras, Scikit-Learn, CNTK, or PyTorch), NLP, Recommender systems, personalization, Segmentation, microservices architecture and API development.\nAbility to adapt to a fast-paced, dynamic work environment and learn new technologies quickly.\nExcellent verbal and written communication skills",
        "skills": [
            "Data Scientist",
            "Gcp",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist II",
        "company_name": "Honeywell",
        "experience": "1-7 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software",
        "job_description": "Innovate to solve the worlds most important challenges\nSoftware development for Honeywell Airfield systems and solution domain\nKey Areas of Responsibility\nWork within the scrum team to build software solution\nStudy requirements specifications to gain deeper understanding of performance expectations and coding requirements.\nDesign, build and test functional codebase to meet business needs\nContribute to DevOps pipeline including packing and distribution automation\nUnderstand end user scenarios and contribute to requirements and design reviews\nUnderstand/contribute to Acceptance Criteria and test case development for stories and verify the product/system against the same\nDevelop unit level automation test scripts for stories and effectively utilize them during regression to reduce cycle time and improve quality\nYOU MUST HAVE\nBachelor in Electronics, Computer Science or Information technology\nTechnical Skills\nExperienced Deep learning models DNNs, CNNs, RNNs\nKnowledge in sensor fusion, signal processing, Kalman filters and machine learning methods\nHands on experience in Lidars, camera and radar sensors\nExperience in object tracking using Lidar and Camera\nSoft Skills\nGood analytical thinking, troubleshooting problem solving skills, coupled with the drive to learn\nGood communication skills, both verbal and written\nSelf-motivated and the ability to work independently without supervision\nPrepared to travel domestic or overseas as per project or business needs\nAbility to work together with different functional delivery teams",
        "skills": [
            "Signal Processing",
            "Coding",
            "Test Scripts",
            "Machine Learning",
            "Scrum",
            "Automation"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HCL Technologies Limited",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Hyderabad, Bengaluru",
        "industry": "Information Technology",
        "job_description": "Job Responsibility:\nWe are looking for a data scientist who will help us discover the information hidden in vast amounts of data, and help us make smarter decisions to deliver AI/ML based Enterprise Software Products.\nDevelop solutions related to machine learning, natural language processing and deep learning & Generative AI to address business needs.\nYour primary focus will be in applying Language/Vision techniques, developing llm based applications and building high quality prediction systems.\nAnalyze Data: Collaborate with cross-functional teams to understand data requirements and identify relevant data sources. Analyze and preprocess data to extract valuable insights and ensure data quality.\nEvaluation and Optimization: Evaluate model performance using appropriate metrics and iterate on solutions to enhance performance and accuracy. Continuously optimize algorithms and models to adapt to evolving business requirements.\nDocumentation and Reporting: Document methodologies, findings, and outcomes in clear and concise reports. Communicate results effectively to technical and non-technical stakeholders.\nWork experience background required:\nExperience building software from the ground up in a corporate or startup environment.\nEssential skillsets required:\n3-6 years experience in software development\nEducational Background: Strong computer science and Math/Statistics\nExperience with Open Source LLM and Langchain Framework and and designing efficient prompt for LLMs.\nProven ability with NLP and text-based extraction techniques.\nExperience in Generative AI technologies, such as diffusion and/or language models.\nExcellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.\nFamiliarity with cloud computing platforms such as GCP or AWS. Experience to deploy and monitor model in cloud environment.\nExperience with common data science toolkits, such as NumPy, Pandas etc\nProficiency in using query languages such as SQL\nGood applied statistics skills, such as distributions, statistical testing, regression, etc.\nExperience working with large data sets along with data modeling, language development, and database technologies\nKnowledge in Machine Learning and Deep Learning frameworks (e.g., TensorFlow, Keras, Scikit-Learn, CNTK, or PyTorch), NLP, Recommender systems, personalization, Segmentation, microservices architecture and API development.\nAbility to adapt to a fast-paced, dynamic work environment and learn new technologies quickly.\nExcellent verbal and written communication skills",
        "skills": [
            "Machine Learning",
            "Nlp",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Python",
            "Sql",
            "Statistical Analysis",
            "Deep Learning",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Chennai, Bengaluru, Pune",
        "industry": "Information Technology",
        "job_description": "We are seeking an experienced Data Scientist to join our team in India. The ideal candidate will have 5-10 years of experience in data analysis, machine learning, and statistical modeling. You will be responsible for leveraging data to drive strategic business decisions and enhance our product offerings.\nResponsibilities\nDevelop and implement data models and algorithms to drive business solutions\nAnalyze large datasets to identify trends and patterns\nCollaborate with cross-functional teams to understand business requirements\nCreate visualizations and reports to communicate findings to stakeholders\nConduct experiments and A/B testing to validate solutions\nOptimize existing data processes and improve data quality\nSkills and Qualifications\nProficiency in programming languages such as Python or R\nStrong understanding of statistical analysis and machine learning techniques\nExperience with data visualization tools like Tableau or Power BI\nFamiliarity with SQL and database management\nKnowledge of big data technologies such as Hadoop or Spark\nExcellent problem-solving skills and attention to detail\nStrong communication skills to present complex data insights",
        "skills": [
            "Data Analysis",
            "Statistics",
            "Tensorflow",
            "Machine Learning",
            "Hadoop",
            "Data Visualization",
            "Big Data",
            "Python",
            "Sql",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Chennai, Bengaluru, Pune",
        "industry": "Information Technology",
        "job_description": "We are seeking an experienced Data Scientist to join our dynamic team in India. The ideal candidate will have a strong background in data analysis, machine learning, and statistical modeling, and will be responsible for turning data into actionable insights that drive strategic business decisions.\nResponsibilities\nAnalyze large datasets to extract actionable insights and drive business decisions.\nDevelop and implement machine learning models to solve complex business problems.\nCollaborate with cross-functional teams to understand their data needs and provide analytical support.\nCommunicate findings and recommendations to stakeholders through data visualization and presentations.\nStay updated with the latest trends in data science and apply them to improve processes.\nSkills and Qualifications\nMaster's or Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field.\nProficiency in programming languages such as Python, R, or SQL.\nExperience with data visualization tools like Tableau, Power BI, or similar.\nSolid understanding of machine learning algorithms and statistical analysis.\nKnowledge of big data technologies (e.g., Hadoop, Spark) is a plus.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and interpersonal skills.",
        "skills": [
            "Data Analysis",
            "Statistical Modeling",
            "SQL Databases",
            "Machine Learning",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Predictive Analytics",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Bengaluru, Chennai, Pune",
        "industry": "Information Technology",
        "job_description": "We are seeking a highly skilled Data Scientist to join our team in India. The ideal candidate will have a strong background in data analysis and machine learning, with the ability to derive insights from complex datasets.\nResponsibilities\nAnalyze large datasets to derive actionable insights and recommendations.\nDevelop predictive models and machine learning algorithms to solve business problems.\nCollaborate with cross-functional teams to understand their data needs and deliver solutions.\nVisualize data insights through dashboards and reports to communicate findings effectively.\nEnsure data quality and integrity through rigorous testing and validation processes.\nSkills and Qualifications\nMaster's or Bachelor's degree in Data Science, Statistics, Computer Science, or a related field.\n5-10 years of experience in data analysis, statistical modeling, and machine learning.\nProficiency in programming languages such as Python, R, or SQL.\nExperience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical methods and machine learning algorithms.\nFamiliarity with big data technologies such as Hadoop, Spark, or similar frameworks.\nExcellent problem-solving skills and the ability to work with large datasets.",
        "skills": [
            "SQL Databases",
            "Cloud Computing",
            "Predictive Modeling",
            "Machine Learning",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Statistical Analysis",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "6-14 Years",
        "salary": "INR 8.5 - 20 LPA ",
        "location": "Delhi, Kolkata, Chennai",
        "industry": "Technical Support, IT Management, Outsourcing, Cloud Data Services",
        "job_description": "Description\nWe are seeking a skilled Data Scientist to join our dynamic team in India. The ideal candidate will have 6-14 years of experience in data science, with a strong foundation in machine learning, statistical analysis, and data visualization.\nResponsibilities\nDevelop and implement machine learning models for predictive analysis.\nAnalyze large datasets to derive actionable insights and recommendations.\nCollaborate with cross-functional teams to understand business needs and provide data-driven solutions.\nDesign and conduct experiments to validate hypotheses.\nCreate visualizations and dashboards to present findings to stakeholders.\nStay updated with the latest trends and technologies in data science.\nSkills and Qualifications\nProficient in programming languages such as Python or R.\nExperience with data manipulation and analysis using libraries like Pandas, NumPy, and SciPy.\nStrong understanding of machine learning algorithms and frameworks (e.g., Scikit-learn, TensorFlow, Keras).\nExperience with SQL and database management systems.\nKnowledge of data visualization tools such as Tableau, Power BI, or Matplotlib.\nStrong statistical analysis skills and ability to interpret complex datasets.\nExcellent communication skills to articulate findings clearly to non-technical stakeholders.",
        "skills": [
            "Data Analysis",
            "Statistical Modeling",
            "SQL Databases",
            "Cloud Computing",
            "Tensorflow",
            "R Programming",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python Programming"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 5 - 10 LPA ",
        "location": "Ahmedabad, Bengaluru, Chennai",
        "industry": "Software Engineering",
        "job_description": "Description\nWe are seeking a Data Scientist with 5-10 years of experience to join our innovative team in India. The ideal candidate will have a strong background in data analysis, machine learning, and statistical modeling to drive business solutions.\nResponsibilities\nDevelop predictive models and machine learning algorithms to solve business problems.\nAnalyze large datasets to extract actionable insights and drive data-driven decision making.\nCollaborate with cross-functional teams to understand data needs and deliver solutions.\nCommunicate findings and recommendations effectively to stakeholders through visualizations and reports.\nContinuously improve existing models and processes through experimentation and research.\nSkills and Qualifications\nProficiency in programming languages such as Python and R.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, Scikit-learn).\nExperience with data manipulation and analysis tools (e.g., Pandas, NumPy).\nFamiliarity with SQL and database management systems.\nSolid understanding of statistical analysis and data mining techniques.\nAbility to work with big data technologies (e.g., Hadoop, Spark) is a plus.\nStrong problem-solving skills and a passion for data-driven decision making.\nExcellent communication skills to convey complex data insights to non-technical stakeholders.",
        "skills": [
            "Cloud Computing",
            "R Programming",
            "Machine Learning",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Python",
            "Sql",
            "Statistical Analysis",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-10 Years",
        "salary": "INR 7 - 20 LPA ",
        "location": "Bengaluru, Chennai, Pune",
        "industry": "Information Technology",
        "job_description": "We are looking for a Data Scientist with 5-10 years of experience to join our dynamic team in India. The ideal candidate will have a passion for data and the ability to turn complex data into actionable insights.\nResponsibilities\nAnalyze large data sets to identify trends and patterns.\nDevelop predictive models and machine learning algorithms.\nCollaborate with cross-functional teams to define and prioritize data projects.\nCommunicate findings and insights to stakeholders effectively.\nMaintain and improve existing data pipelines and processes.\nSkills and Qualifications\nProficiency in programming languages such as Python and R.\nExperience with data visualization tools like Tableau or Power BI.\nStrong knowledge of SQL and database management systems.\nFamiliarity with machine learning libraries such as Scikit-learn or TensorFlow.\nUnderstanding of statistical analysis and data mining techniques.\nExperience with big data technologies such as Hadoop or Spark is a plus.\nStrong problem-solving skills and attention to detail.",
        "skills": [
            "Model Deployment",
            "Tensorflow",
            "Machine Learning",
            "Hadoop",
            "Data Visualization",
            "Data Mining",
            "Big Data",
            "Python",
            "Statistical Analysis",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist jobs in Luxembourg - UK - Netherland - Finland",
        "company_name": "Vijay Sabarwal (Proprietorship of Advance Immigration Solutions)",
        "experience": "2-12 Years",
        "salary": "INR 45 - 55 LPA ",
        "location": "New Zealand, Canada, Australia",
        "industry": "Information Technology",
        "job_description": "URGENT HIRING !!!\nFor more information call or whatsapp +919220850077\nlocation's : Canada , Australia , New Zealand ( Not In India )\nBenefits : Medical Insurances , Travel allowances , Flight Tickets , Meals , etc\nData mining or extracting usable data from valuable data sources\nUsing machine learning tools to select features, create and optimize classifiers\nCarrying out the preprocessing of structured and unstructured data\nEnhancingdata collectionprocedures to include all relevant information for developing analytic systems\nProcessing, cleansing, and validating the integrity of data to be used for analysis\nAnalyzing large amounts of information to find patterns and solutions\nDeveloping prediction systems and machine learning algorithms\nPresenting results in a clear manner\nPropose solutions and strategies to tackle business challenges\nCollaborate with Business and IT teams",
        "skills": [
            "Machine Learning",
            "Power Bi",
            "Tableau",
            "AI ML",
            "Data Science",
            "Nlp",
            "Software Development",
            "MLops",
            "Docker",
            "Data Scientist",
            "Big Data Technologies",
            "Data Science - ML",
            "Azure",
            "Kubernetes",
            "Aws"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Dun & Bradstreet Technology And Corporate Services India LLP",
        "experience": "7-11 Years",
        "salary": null,
        "location": "Chennai",
        "industry": "Login to check your skill match score",
        "job_description": "As part of Dun & Bradstreet's Data & Analytics team, you will participate in all aspects of modeling engagement, including design, development, validation, calibration, documentation, approval, implementation, monitoring, and reporting. You will research complex business issues and recommend solutions, including model features, end products and any data required to support growing Dun & Bradstreet initiatives.\nKey Responsibilities:\nUtilize the latest data science techniques across both supervised and unsupervised machine learning methodologies, Natural Language Processing, and graph analysis in automating and scaling internal business processes.\nEstablish and maintain strong relationships with key business stakeholders.\nEngage clients and D&B colleagues to identify business needs and develop, implement, and manage solutions.\nStrong communication skills and the ability to simplify and explain complex concepts for stakeholders, clients, and senior leaders.\nParticipate in all aspects of a modeling engagement, including design, data requirements, development, validation, calibration, documentation, approval, implementation, monitoring and reporting.\nDevelop Global Analytic Solutions inclusive but not limited to statistical models based on D&Bs established best practices, methodologies, and tools.\nResearch complex business issues and recommend solutions, including model inputs and end products, focusing on addressing specific customer needs and use cases.\nServe as a Subject Matter Expert on predictive models within the team and with business users; consult with the business, as appropriate, on predictive modeling solutions.\nEnjoy and share academic literature and industry best practices. Identify business relevance of new methods and work with cross functional teams to create prototypes, assist in creating business cases and go to market strategies.\nValidate the performance of existing quantitative risk models and recommend changes when necessary.\nDrive timely retrieval of risk analytics data from existing systems to create algorithms that meet business needs.\nKey Skills:\nMasters Degree or Ph.D. in a quantitative/applied field preferred (Statistics, Econometrics, Computer Science, Operations Research, Mathematics, Engineering).\n7+ years operating successfully in data science roles, especially roles requiring cross-company collaboration and disciplined delivery of initiatives.\nHands-on experience applying modern machine learning techniques.\nAbility to program in other statistical analysis languages, proficiency in programming languages (Python, R, SQL).\nExperience in feature engineering, automation, network analysis or Natural Language Processing.\nExperience working with outside clients on statistical engagements.\nAbility to manage multiple assignments, many of which have challenging timelines.\nAbility to work independently, as well as collaborate effectively in a team environment.\nExcellent communication and presentation skills.\nProficiency in Microsoft Office Suite.\nShow an ownership mindset in everything you do. Be a problem solver, be curious and be inspired to take action. Be proactive, seek ways to collaborate and connect with people and teams in support of driving success.\nContinuous growth mindset, keep learning through social experiences and relationships with stakeholders, experts, colleagues and mentors as well as widen and broaden your competencies through structural courses and programs.\nWhere applicable, fluency in English and languages relevant to the working market.",
        "skills": [
            "Credit Risk",
            "Fraud Analytics",
            "Pyspark",
            "Statistical Modelling",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Dun & Bradstreet Technology And Corporate Services India LLP",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Remote",
        "industry": "Login to check your skill match score",
        "job_description": "As an Econometrician focused on Dun & Bradstreet Public Sector Advanced Analytics team, you should love tackling hard problems utilizing econometric and ML approaches using large datasets. You should be willing to learn new methods and technology, while serving as an expert in your respective domain.\nKey Skills:\nStrong Analytical and Problem-Solving Skills, Statistical Modelling, Data Analysis, Macro Economic Modeling, Machine Learning etc.,\nPrimary Responsibilities:\nHelp development of solutions that provide global or country-specific econometrics and time series insights that address and solve public sector customer problems.\nlowing precise methodological guidelines.\nDevelop approaches that leverage econometric, time series and ML methods blended with economic theory covering topics including but not limited to causal inference, climate risk and spatial economics for use as products.\nPreparing drafts of publication quality economic reports/commentaries/papers based on public sector data science solutions, also contribute towards scalability and automation of such reports.\nCommunicate analytical results in terms that are meaningful to business managers and senior leadership internally and externally.\nParticipate in all aspects of ongoing modeling engagements, including design, development, validation, calibration, documentation, approval, implementation, monitoring, visualization, and reporting.\nDevelop a working knowledge of how current systems and data sources are used in existing predictive modeling projects; drive timely retrieval of analytics data from existing system to create algorithms that meet business needs.\nRequirements:\n4 to 7 years of experience with a degree in Economics, Econometrics, Statistics, or Mathematics, with a quantitative specialization\nProgramming (Python/Pyspark ) skills are required.\nComprehensive knowledge of econometric, time series and ML modeling. Hands-on experience with topics such as causal inference, climate risk modeling and spatial economics is a plus.\nAbility to work on an interdisciplinary and cross-functional team.\nStrong collaboration and communication abilities (including writing) and project management skills.\nAbility to effectively communicate complex ideas to both a technical and non-technical audience.",
        "skills": [
            "macro economic Modeling",
            "Data Analysis And Interpretation",
            "Machine Learning",
            "Statistical Modelling"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Robotics Technologies",
        "experience": "5-15 Years",
        "salary": "INR 8 - 14.5 LPA ",
        "location": "Thane, Noida, Pune",
        "industry": "Technical Support, IT Management, Cloud Data Services, Information Services",
        "job_description": "Description\nWe are looking for a skilled Data Scientist to join our team in India. The ideal candidate will have 5-15 years of experience in data analysis, machine learning, and statistical modeling. You will be responsible for transforming data into actionable insights that drive business decisions.\nResponsibilities\nAnalyze and interpret complex data sets to identify trends and patterns.\nDevelop predictive models and machine learning algorithms to solve business problems.\nCollaborate with cross-functional teams to understand data requirements and deliver actionable insights.\nPresent findings and recommendations to stakeholders in a clear and concise manner.\nContinuously monitor and improve data collection and analysis processes.\nSkills and Qualifications\nProficient in programming languages such as Python or R.\nExperience with data visualization tools like Tableau, Power BI, or Matplotlib.\nStrong understanding of statistical analysis and machine learning techniques.\nFamiliarity with SQL and database management systems.\nKnowledge of big data technologies such as Hadoop or Spark is a plus.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills to effectively convey complex data insights.",
        "skills": [
            "SQL Expertise",
            "ETL Processes",
            "Data Analysis",
            "Statistical Modeling",
            "Cloud Computing",
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python Programming",
            "Deep Learning"
        ]
    },
    {
        "job_title": "IT_Industries4.0_Data Scientist_CoE",
        "company_name": "Welspun World",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Welspun\n\nWelspun World is one of India's fastest growing global conglomerates with businesses in Home Textiles, Flooring Solutions, Advanced Textiles, DI Pipes, Pig Iron, TMT bars, Stainless Steel, Alloy, Line Pipes, Infrastructure & Warehousing.\n\nAt Welspun, we strongly believe in our purpose to delight customers through innovation and technology, achieve inclusive & sustainable growth to remain eminent in all our businesses. From Homes to Highways, Hi-tech to Heavy metals, We lead tomorrow together to create a smarter & more sustainable world.\n\nJob Purpose/ Summary\n\nWelspun Transformation Private Limited is seeking a highly skilled and results-driven Senior Data Scientist with expertise in Python, PySpark, and Industry 4.0 technologies. The ideal candidate will have a strong background in designing and implementing advanced analytics solutions to optimize manufacturing processes, improve operational efficiency, and drive predictive maintenance initiatives. The role involves working closely with cross-functional teams to develop data-driven solutions that enhance decision-making across smart factories and industrial ecosystems.\n\nJob Title\n\nIT_Industries4.0_Data Scientist_CoE\n\nJob Description\nAs a Senior Manager in the IT Industries 4.0 Data Scientist Center of Excellence, you will be responsible for leading a team of data scientists to develop and implement advanced analytics models and solutions to drive business decisions and outcomes. You will be expected to leverage your expertise in Machine Learning, Predictive Maintenance, ETL Pipelines, Python, MQTT, OPC UA, and SCADA to deliver innovative solutions.\n\nPrincipal Accountabilities\nLead and manage a team of data scientists to develop and implement advanced analytics models and solutions.\n\nLeverage expertise in Machine Learning, Predictive Maintenance, ETL Pipelines, Python, MQTT, OPC UA, and SCADA to deliver innovative solutions.\n\nCollaborate with cross-functional teams to understand business needs and identify opportunities for leveraging company data to drive business solutions.\n\nDevelop custom data models and algorithms to apply to data sets.\n\nUse predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting, and other business outcomes.\n\nDevelop company A/B testing framework and test model quality.\n\nCoordinate with different functional teams to implement models and monitor outcomes.\n\nDevelop processes and tools to monitor and analyze model performance and data accuracy.\n\nMaintain a strong understanding of industry trends and best practices, and apply this knowledge to your work.\n\nDemonstrate strong business and commercial acumen, and maintain a global mindset in all work.\n\nEmphasize critical thinking, product and service management, IT application, and information security in all work.\n\nThe ideal candidate will have a strong ability to translate complex data into actionable strategies and techniques to drive business performance. They will also have a proven track record of leading and developing high-performing teams.\n\nKey Interactions\n\nTop Management,Mid Management,Junior Management,Cross-Functional Collaboration ,Client Relations ,Financial Auditing ,Vendor Management\n\nExperience\n\n6\n\nCompetency Name\n\nCompetency Name Proficiency Level Machine LearningExpert Predictive maintenanceExpert ETL pipelinesExpert PythonExpert Business & Commercial acumenExpert Global Mind-setExpert MQTT, OPC UA, and SCADAExpert MQTT, OPC UA, and SCADAExpert",
        "skills": [
            "Predictive Maintenance",
            "ETL Pipelines",
            "SCADA",
            "OPC UA",
            "Mqtt",
            "Machine Learning",
            "Python"
        ]
    },
    {
        "job_title": "Applied Data Scientist - BLR",
        "company_name": "Aarki",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us: Aarki is an AI-driven company specializing in mobile advertising solutions designed to fuel revenue growth. We leverage AI to discover audiences in a privacy-first environment through trillions of contextual bidding signals and proprietary behavioral models. Our comprehensive audience engagement platform includes creative strategy and execution. With over 14 years in the industry, we handle 5 million mobile ad requests per second from over 10B devices, driving performance for both publishers and brands. We are headquartered in San Francisco, CA, with a global presence across the United States, EMEA, and APAC.\nRole Overview\nWe are seeking a motivated and detail-oriented Applied Scientist to join our team. As an Applied Scientist, you will be involved in designing and implementing machine learning models and data pipelines to enhance our programmatic demand-side platform (DSP). You will work closely with senior research scientists and other team members to drive impactful data science projects and contribute to innovative solutions.\nThis is an on-site role, 5 days a week, based in our Bengaluru, India office.\nJoin us in pushing the boundaries of AI and mobile advertising in a collaborative environment that fosters creativity and growth. We offer a competitive salary, comprehensive benefits, and significant opportunities for career advancement.\nRole & Responsibilities\nDevelop and deploy machine learning models at scale to address key challenges in programmatic advertising, such as user response prediction, bid landscape forecasting, and fraud detection.\nConduct exploratory data analysis and apply statistical techniques to extract insights and support decision-making.\nBuild and maintain data pipelines to ensure efficient processing and integration of large-scale data for model training and evaluation.\nWork closely with senior data scientists and cross-functional teams including product, engineering, and business units to integrate models into production systems and applications.\nAssist in the development and implementation of best practices for model deployment, monitoring, and performance assessment.\nStay updated on recent developments in machine learning and data science, and apply relevant techniques to solve complex problems and enhance our platform.\nContribute to the exploration and adoption of new methodologies and technologies to advance our data science capabilities.\nSkills & Experience\nMinimum of five (6) years in data science with practical experience in machine learning, statistical analysis, and data modeling.\nBachelor's degree in Mathematics, Physics, Computer Science, or a related technical field. PLUS if master's degree is a plus in Mathematics, Physics, Computer Science, or a related technical field.\nPreferred experience with additional programming languages (e.g., Rust, C++, Java, Scala) and large-scale data processing systems.\nFamiliarity with RTB, auction theory, and high-throughput low-latency environments is a plus.\nProficiency in machine learning techniques such as regression, classification, and clustering. Experience with Python and libraries like Scikit-Learn, TensorFlow/PyTorch.\nProficiency in Python and SQL. Familiarity with Spark. Experience with libraries such as TensorFlow/PyTorch, Scikit-Learn\nStrong understanding of probability, statistics, and data analysis.\nAbility to work effectively in a team environment, with good communication skills to explain complex concepts to diverse stakeholders.",
        "skills": [
            "data pipelines",
            "Scikit-Learn",
            "Java",
            "Rust",
            "Machine Learning",
            "Scala",
            "Data Modeling",
            "Sql",
            "Tensorflow",
            "Pytorch",
            "Spark",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Truecaller",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Hello, Truecaller is calling you from Bangalore, India! Ready to pick up\nOur goal is to make communication smarter, safer, and more efficient, all while building trust everywhere. We're all about bringing you smart services with a big social impact, keeping you safe from fraud, harassment, scam calls or messages, so you can focus on the conversations that matter.\nTop 20 most downloaded apps globally, and world's #1 caller ID and spam-blocking service for Android and iOS, with extensive AI capabilities, with more than 400 million active users per month.\nFounded in 2009, listed on Nasdaq OMX Stockholm and is categorized as a Large Cap. Our focus on innovation, operational excellence, sustainable growth, and collaboration has resulted in consistently high profitability and strong EBITDA margins.\nA team of 400 people from 35 different nationalities spread across our headquarters in Stockholm and offices in Bangalore, Mumbai, Gurgaon and Tel Aviv with high ambitions.\nWe in the Insights Team areresponsible for SMS Categorization, Fraud detection and other Smart SMS features within the Truecaller app. The OTP & bank notifications, bill & travel reminder alerts are some examples of the Smart SMS features. The team has developed a patented offline text parser that powers all these features and the team is also exploring cutting edge technologies like LLM to enhance the Smart SMS features. The team's mission is to become the World's most loved and trusted SMS app which is aligned with Truecaller's vision to make communication safe and efficient. Smart SMS is used by over 90M users every day.\nAs a Senior Data Scientist, you will be responsible for collecting, organizing, analyzing, and interpreting Truecaller data with a focus on NLP. In this role, you will be pivotal in advancing our work with large language models and on-device models across diverse regions. Your expertise will enhance our natural language processing, machine learning, and predictive analytics capabilities.\nWhat you bring in:\n5+ years of experience in designing, developing, and deploying ML models at scale, with a focus on NLP-driven solutions.\nStrong background in Natural Language Processing (NLP), including text classification, entity recognition, language modeling, and transformer-based architectures.\nExperience in building and deploying models at scale, handling millions of messages efficiently while maintaining performance and accuracy. Also working with on-device models.\nAbility to not only build ML models but also take ownership of deploying them into production, ensuring scalability, reliability, and monitoring.\nKnowledge of anomaly detection, adversarial ML techniques, and risk modeling to identify and prevent spam and fraudulent messaging activities.\nStrong ability to take ML models from research and experimentation to production, working closely with ML engineers and data engineers.\nExpertise in machine learning libraries such as TensorFlow, PyTorch, pandas and Scikit-learn, along with NLP-specific tools like Hugging Face Transformers, spaCy with experience in TFlife, ONNX.\nHands-on experience fine-tuning LLMs including transformer-based architectures (BERT, GPT, LLaMA, T5, etc.) for domain-specific applications, including knowledge distillation, quantization, and model compression for efficiency.\nStrong ability to design, refine, and optimize prompts for LLM-based applications, ensuring high-quality responses and reduced model hallucinations.\nAbility to leverage data driven decision by experimentation, and statistical analysis to improve models and business outcomes.\nStrong understanding of designing, testing, and optimizing prompts for LLM-based applications to improve model accuracy and efficiency.\nProgramming knowledge in at least one language, such as Python or R. Preferably python.\nExpert knowledge of machine learning algorithms.\nFamiliarity with database modelling and data warehousing principles with a working knowledge of SQL\nExperience in building and optimizing large-scale data processing systems using Spark/PySpark\nStrong ability to work cross-functionally with engineers, product managers, and business stakeholders to align ML solutions with company objectives.\nThe impact you will create:\nTake a loosely defined business problem and break it into tractable data problems. For each data problem, clearly articulate the value of solving it, its impact, and its complexity.\nCollaborate with Product and Engineering to scope, design, and implement systems that solve complex business problems ensuring they are delivered on time and within scope.\nDesign, develop, and optimize state-of-the-art NLP models for large-scale message classification, fraud detection, and spam filtering, impacting millions of users globally.\nTake full ownership of ML model development, deployment, and monitoring, ensuring models are production-ready, scalable, and cost-efficient.\nLead data science projects from ideation to deployment, ensuring alignment with business objectives and timelines.\nManage and analyze large datasets collected from multiple countries, ensuring data integrity and consistency.\nStay updated on industry best practices and emerging technologies to drive innovation within the Data Team.\nYou work collaboratively across systems and teams to solve user and business problems. You are expected to help define success and design and build the systems to achieve it.\nTo work with the Product to decide on priorities and set direction, design solutions, and help the team implement them.\nIt would be great if you also have:\nUnderstanding of Conversational AI\nDeploying NLP models in production\nWorking knowledge of GCP components\nLife at Truecaller - Behind the code: https://www.instagram.com/lifeattruecaller/\nSounds like your dream job\nWe will fill the position as soon as we find the right candidate, so please send your application as soon as possible. As part of the recruitment process, we will conduct a background check.\nThis position is based in Bangalore, India.\nWe only accept applications in English.\nWhat we offer:\nA smart, talented and agile team: An international team where 35 nationalities are working together in several locations and time zones with a learning, sharing and fun environment.\nA great compensation package: Competitive salary, 30 days of paid vacation, flexible working hours, private health insurance, parental leave, telephone bill reimbursement, Udemy membership to keep learning and improving and Wellness allowance.\nGreat tech tools: Pick the computer and phone that you fancy the most within our budget ranges.\nOffice life: We strongly believe in the in-person collaboration and follow an office-first approach while offering some flexibility. Enjoy your days with great colleagues with loads of good stuff to learn from, daily lunch and breakfast and a wide range of healthy snacks and beverages. In addition, every now and then check out the playroom for a fun break or join our exciting parties and or team activities such as Lab days, sports meetups etc. There something for everyone!\nCome as you are: Truecaller is diverse, equal and inclusive. We need a wide variety of backgrounds, perspectives, beliefs and experiences in order to keep building our great products. No matter where you are based, which language you speak, your accent, race, religion, color, nationality, gender, sexual orientation, age, marital status, etc. All those things make you who you are, and that's why we would love to meet you.",
        "skills": [
            "Hugging Face Transformers",
            "Scikit-learn",
            "ONNX",
            "R",
            "spaCy",
            "Pyspark",
            "Sql",
            "Tensorflow",
            "Pandas",
            "Pytorch",
            "Spark",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Caterpillar Inc.",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Career Area\n\nTechnology, Digital and Data\n\nJob Description\n\nYour Work Shapes the World at Caterpillar Inc.\n\nWhen you join Caterpillar, you're joining a global team who cares not just about the work we do but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.\n\nCaterpillar Inc. is more than big, heavy equipment and much more than yellow iron. Our products have evolved from simple mechanical workhorses to sophisticated, electronically controlled work-site solutions. This transformation along with our smart factories and our integrated dealer network has wealth of data ready to be harvested to open new markets, fine tune our processes or deliver differentiated customer solutions. Analytics Professionals are the data driven, business professionals providing solutions to all areas across the Caterpillar enterprise.\n\nMarketing & Branding Analytics uses quantitative methods such as business simulations, data mining, and advanced statistical techniques to solve marketing challenges for internal and external Caterpillar customers. The Data Scientist II contributes to this mission by leveraging his or her quantitative analysis, data management, modeling and/or data visualization skills as an individual contributor to project teams tasked with solving business problems.\n\nJob Duties\n\nTypical problems include enhancing customer satisfaction by delivering key insights on customer experience, identifying sales, rental, and service opportunities for Caterpillar dealers, determining the principal drivers to maximize marketing return on investment; recommending optimum investment in each marketing channel.\nThe principal responsibility of the Data Scientist is to be an independent contributor to multi-person analytic teams. This position has a depth of knowledge in quantitative analytic methods, data management, and or associated digital technologies suitable to handle all but the most complex issues. Data Scientist is expected to be familiar with the company's processes, products, and organization, as well as its customers, competitors, and stakeholders. Work is typically directed by a direct supervisor, project or team lead through a review of results. Decisions on routine, medium risk issues that may affect the project team, suppliers or internal customers may be made by this position. Challenges include meeting expectations in delivering results, learning to refine solutions to better fit complex situations, making timely decisions, and communicating effectively with all project stakeholders. The Data Scientist also mentors and develops the capabilities and organizational knowledge of junior data scientists and associates.\nThe Data Scientist demonstrates thorough knowledge of statistical approaches, data management techniques, and/or related digital technologies, and the ability to handle complex issues. The incumbent demonstrates very good communication and presentation skills, being able to explain conclusions to customers who have limited knowledge and experience with quantitative analytical methods. As an individual contributor on teams, they should also exhibit strong initiative and teamwork skills, and a comprehensive knowledge of Caterpillar Inc., its products and services; its internal systems, processes, and procedures; and the external environment in which it competes.\nBackground/Experience:\nBachelor's degree, preferably in AI, Data Science, Computer Science, Statistics or a similar field with quantitative coursework, and 4-5 years of marketing analytics experience utilizing quantitative analysis, a Master's degree and 2-4 years of experience, or a PhD in one of the associated fields.\nStrong understanding of machine learning algorithms, data structures, and statistical methods.\nExperience with deep learning techniques and neural networks\nAbility to work with large datasets in cloud environment and perform data preprocessing and feature engineering.\nProficiency in Python and experience with machine learning frameworks such as TensorFlow or PyTorch.\nExperience in designing, developing, deploying Machine Learning models using AWS Sage Maker.\n\nPosting Dates\n\nCaterpillar is an Equal Opportunity Employer (EEO).\n\nNot ready to apply Join our Talent Community.",
        "skills": [
            "data preprocessing",
            "AWS Sage Maker",
            "deep learning techniques",
            "feature engineering",
            "statistical methods",
            "Quantitative Analysis",
            "Data Management",
            "Neural Networks",
            "Tensorflow",
            "Pytorch",
            "Data Visualization",
            "data structures",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Truecaller",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Hello, Truecaller is calling you from Bangalore, India! Ready to pick up\nOur goal is to make communication smarter, safer, and more efficient, all while building trust everywhere. We're all about bringing you smart services with a big social impact, keeping you safe from fraud, harassment, scam calls or messages, so you can focus on the conversations that matter.\nTop 20 most downloaded apps globally, and world's #1 caller ID and spam-blocking service for Android and iOS, with extensive AI capabilities, with more than 400 million active users per month.\nFounded in 2009, listed on Nasdaq OMX Stockholm and is categorized as a Large Cap. Our focus on innovation, operational excellence, sustainable growth, and collaboration has resulted in consistently high profitability and strong EBITDA margins.\nA team of 400 people from 35 different nationalities spread across our headquarters in Stockholm and offices in Bangalore, Mumbai, Gurgaon and Tel Aviv with high ambitions.\nWe in the Insights Team areresponsible for SMS Categorization, Fraud detection and other Smart SMS features within the Truecaller app. The OTP & bank notifications, bill & travel reminder alerts are some examples of the Smart SMS features. The team has developed a patented offline text parser that powers all these features and the team is also exploring cutting edge technologies like LLM to enhance the Smart SMS features. The team's mission is to become the World's most loved and trusted SMS app which is aligned with Truecaller's vision to make communication safe and efficient. Smart SMS is used by over 90M users every day.\nAs a Senior Data Scientist, you will be responsible for collecting, organizing, analyzing, and interpreting Truecaller data with a focus on NLP. In this role, you will be pivotal in advancing our work with large language models and on-device models across diverse regions. Your expertise will enhance our natural language processing, machine learning, and predictive analytics capabilities.\nWhat you bring in:\n5+ years of experience in designing, developing, and deploying ML models at scale, with a focus on NLP-driven solutions.\nStrong background in Natural Language Processing (NLP), including text classification, entity recognition, language modeling, and transformer-based architectures.\nExperience in building and deploying models at scale, handling millions of messages efficiently while maintaining performance and accuracy. Also working with on-device models.\nAbility to not only build ML models but also take ownership of deploying them into production, ensuring scalability, reliability, and monitoring.\nKnowledge of anomaly detection, adversarial ML techniques, and risk modeling to identify and prevent spam and fraudulent messaging activities.\nStrong ability to take ML models from research and experimentation to production, working closely with ML engineers and data engineers.\nExpertise in machine learning libraries such as TensorFlow, PyTorch, pandas and Scikit-learn, along with NLP-specific tools like Hugging Face Transformers, spaCy with experience in TFlife, ONNX.\nHands-on experience fine-tuning LLMs including transformer-based architectures (BERT, GPT, LLaMA, T5, etc.) for domain-specific applications, including knowledge distillation, quantization, and model compression for efficiency.\nStrong ability to design, refine, and optimize prompts for LLM-based applications, ensuring high-quality responses and reduced model hallucinations.\nAbility to leverage data driven decision by experimentation, and statistical analysis to improve models and business outcomes.\nStrong understanding of designing, testing, and optimizing prompts for LLM-based applications to improve model accuracy and efficiency.\nProgramming knowledge in at least one language, such as Python or R. Preferably python.\nExpert knowledge of machine learning algorithms.\nFamiliarity with database modelling and data warehousing principles with a working knowledge of SQL\nExperience in building and optimizing large-scale data processing systems using Spark/PySpark\nStrong ability to work cross-functionally with engineers, product managers, and business stakeholders to align ML solutions with company objectives.\nThe impact you will create:\nTake a loosely defined business problem and break it into tractable data problems. For each data problem, clearly articulate the value of solving it, its impact, and its complexity.\nCollaborate with Product and Engineering to scope, design, and implement systems that solve complex business problems ensuring they are delivered on time and within scope.\nDesign, develop, and optimize state-of-the-art NLP models for large-scale message classification, fraud detection, and spam filtering, impacting millions of users globally.\nTake full ownership of ML model development, deployment, and monitoring, ensuring models are production-ready, scalable, and cost-efficient.\nLead data science projects from ideation to deployment, ensuring alignment with business objectives and timelines.\nManage and analyze large datasets collected from multiple countries, ensuring data integrity and consistency.\nStay updated on industry best practices and emerging technologies to drive innovation within the Data Team.\nYou work collaboratively across systems and teams to solve user and business problems. You are expected to help define success and design and build the systems to achieve it.\nTo work with the Product to decide on priorities and set direction, design solutions, and help the team implement them.\nIt would be great if you also have:\nUnderstanding of Conversational AI\nDeploying NLP models in production\nWorking knowledge of GCP components\nLife at Truecaller - Behind the code: https://www.instagram.com/lifeattruecaller/\nSounds like your dream job\nWe will fill the position as soon as we find the right candidate, so please send your application as soon as possible. As part of the recruitment process, we will conduct a background check.\nThis position is based in Bangalore, India.\nWe only accept applications in English.\nWhat we offer:\nA smart, talented and agile team: An international team where 35 nationalities are working together in several locations and time zones with a learning, sharing and fun environment.\nA great compensation package: Competitive salary, 30 days of paid vacation, flexible working hours, private health insurance, parental leave, telephone bill reimbursement, Udemy membership to keep learning and improving and Wellness allowance.\nGreat tech tools: Pick the computer and phone that you fancy the most within our budget ranges.\nOffice life: We strongly believe in the in-person collaboration and follow an office-first approach while offering some flexibility. Enjoy your days with great colleagues with loads of good stuff to learn from, daily lunch and breakfast and a wide range of healthy snacks and beverages. In addition, every now and then check out the playroom for a fun break or join our exciting parties and or team activities such as Lab days, sports meetups etc. There something for everyone!\nCome as you are: Truecaller is diverse, equal and inclusive. We need a wide variety of backgrounds, perspectives, beliefs and experiences in order to keep building our great products. No matter where you are based, which language you speak, your accent, race, religion, color, nationality, gender, sexual orientation, age, marital status, etc. All those things make you who you are, and that's why we would love to meet you.",
        "skills": [
            "Hugging Face Transformers",
            "Scikit-learn",
            "ONNX",
            "R",
            "spaCy",
            "Pyspark",
            "Sql",
            "Tensorflow",
            "Pandas",
            "Pytorch",
            "Spark",
            "Python"
        ]
    },
    {
        "job_title": "Graduate Program - Innovation Impact -R&D Data Scientist Chemistry (d/f/m)",
        "company_name": "Henkel",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Innovation Impact Program Our 18-month Graduate Program starts on October 1st, 2025, at our Henkel Adhesive Technologies Innovation Center in Dsseldorf. You will be part of an exciting development journey, where you will receive a permanent contract from day one, relocation support, and hands-on experience across various innovative technologies. Upon completion, you will have the opportunity to transition into global roles within Henkel's innovation network.\n\nWhat Youll Do\n\nPartner with product developers and application engineers as part of our R&D team in Dsseldorf, Germany to understand technologies, data collection processes, and challenges driving the digital transformation within R&D\nDesign and conduct data analysis projects to identify trends and patterns in formulation data sets, and build and deploy machine learning models to predict formulation performance, stability, and quality\nCollaborate closely with R&D colleagues to translate data insights into new formulations, product development decisions, and application processes - optimizing R&D workflows\nDevelop and maintain data visualization dashboards to effectively communicate insights to technical and business stakeholders\nLeverage your digital skillset to drive awareness and application of data-based decision-making and digital tools across R&D\nSupport the digital transformation of R&D by implementing data-driven workflows for process development and knowledge management\n\nWhat makes you a good fit\n\nYou have recently completed, or are about to complete, a Master's or Ph.D. that combines a strong foundation in Chemistry, Engineering, or Materials Science with Data Science, Physics, Computer Science, or a related analytical fieldfor example, a Bachelor's in a natural science and a digitally focused Master's, or interdisciplinary studies that enable you to understand both lab-based R&D and data-driven approaches\nYou bring first hands-on experience with data analysis and modeling in scientific or engineering contexts, and have a strong grasp of statistical methods and machine learning algorithms (e.g., regression, classification)\nYou are proficient in data tools such as Python or R, DoE software like Minitab or JMP, and data visualization platforms (e.g., Excel, Power BI, Tableau); experience with IoT, finite element analysis (FEA), and computational fluid dynamics (CFD) is a plus\nYou are interested in working in a laboratory environment, motivated to learn about formulations, product development, and R&D workflows, and keen to apply your skills in a hands-on setting\nYou enjoy collaborating with non-digital colleagues, and can confidently explain digital tools and data concepts in a simple, practical way to support their daily work\nYou are interested in growing your career in Henkel's global Innovation network, with relocation as one possible path after the program.\nProficiency in English is essential, as you will work in an international environment\n\nReady to take the next step in your career Visit our program landing page for full details and apply today.\n\nSome perks of joining Henkel\n\nAt Henkel, we come from a broad range of backgrounds, perspectives, and life experiences. We believe the uniqueness of all our employees is the power in us. Become part of the team and bring your uniqueness to us! We welcome all applications across different genders, origins, cultures, religions, sexual orientations, generations and disabilities. If there are any specific accommodations or adjustments you require for the interview process or your on-site appointment, please inform us, and we'll do our best to accommodate your requests.",
        "skills": [
            "R",
            "Minitab",
            "Data Analysis",
            "Statistical methods",
            "Jmp",
            "Power Bi",
            "Machine Learning",
            "Tableau",
            "Excel",
            "Python",
            "Iot"
        ]
    },
    {
        "job_title": "Manager-Data Scientist (Credit Risk Models, 5+ years)",
        "company_name": "Time Hack Consulting",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nOne of the fastest growing fintech companies that has built a full-stack financial platform for Bharat 2.0. They have multiple financial products on their platform, viz, Lending, Insurance and Investments. They have an AUM of more than INR 1000 cr and about 500 employees.\n\nRole Responsibilities\n\nDevelop and implement advanced credit risk models using statistical methodologies.\nAnalyze large datasets to identify trends, patterns, and anomalies related to credit risk.\nCollaborate with cross-functional teams to gather requirements and ensure alignment with business objectives.\nMonitor and evaluate the performance of existing credit risk models and make necessary adjustments.\nCommunicate findings and recommendations to stakeholders in a clear and concise manner.\nLead a team of data scientists, providing mentorship and guidance to foster professional growth.\nDesign and execute experiments to validate model assumptions and improve accuracy.\nEnsure compliance with industry regulations and internal policies related to credit risk assessment.\nUtilize data visualization techniques to present complex data insights to non-technical stakeholders.\nStay updated with emerging trends and tools in data science and credit risk modeling.\nConduct risk assessments and stress testing to evaluate the impact of different scenarios.\nCollaborate with IT and data engineering teams to ensure data availability and integrity.\nPrepare reports and documentation summarizing model development processes and findings.\nContribute to the development of best practices for credit risk modeling and analytics.\nIdentify opportunities for process improvements within the team and across projects.\n\nQualifications\n\nBachelors in statistics, mathematics, computer science, or a related field.\nMinimum of 5 years of experience in credit risk modeling or data science.\nMininum 2 years of experience in managing a team is mandatory\nProficiency in statistical modeling techniques and machine learning algorithms.\nStrong programming skills in Python and experience with relevant libraries (e.g., Pandas, Scikit-learn).\nExperience with SQL for data extraction and manipulation.\nKnowledge of credit risk regulations and industry standards.\nFamiliarity with data visualization tools (e.g., Tableau, Power BI).\nAbility to communicate complex concepts to diverse audiences.\nProven track record of leading and mentoring junior team members.\nStrong analytical and problem-solving skills.\nExperience with data preprocessing and feature engineering.\nAbility to work collaboratively in a team-oriented environment.\nExcellent time management skills and attention to detail.\nStrong understanding of business drivers and implications of credit risk.\nWillingness to continuously learn and adapt to new methodologies and technologies.\n\nSkills: risk management,team leadership,tableau,predictive analytics,credit risk modeling,power bi,modeling,python,sql proficiency,problem-solving,data preprocessing,statistical modeling,scikit-learn,machine learning,data science,credit risk,feature engineering,machine learning algorithms,data visualization,pandas,sql,statistical modeling techniques",
        "skills": [
            "Statistical modeling techniques",
            "Data preprocessing",
            "Scikit-learn",
            "Credit risk modeling",
            "Feature engineering",
            "Data visualization tools",
            "Risk management",
            "Machine Learning Algorithms",
            "Power Bi",
            "Tableau",
            "Sql",
            "Predictive Analytics",
            "Pandas",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist - Finance Analytics",
        "company_name": "Signant Health",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role Overview\n\nWe're seeking a talented Lead Data Scientist to join our Finance Analytics team. In this role, you'll analyze financial data to drive insights, identify patterns, and recommend automation opportunities within our finance and accounting functions. You'll apply your technical expertise in data engineering and advanced analytics to transform raw financial data into meaningful insights, working collaboratively with finance teams to understand business needs\n\nKey Accountabilities/Decision Making & Influence\n\nApply advanced analytics techniques to extract insights from financial data sets\nBuild and optimize data pipelines using Python, Spark, and SQL to prepare data for analysis\nDevelop and implement machine learning models to identify patterns, anomalies, and opportunities for automation\nCreate interactive dashboards and visualizations using BI tools that communicate key insights effectively\nCollaborate with finance teams to understand their data needs and translate them into analytical solutions\nIdentify and track relevant metrics that provide meaningful business intelligence\nSupport data-driven decision making by presenting clear, actionable findings\nConduct exploratory data analysis to uncover trends and relationships in financial data\nMentor junior data scientists on analytical techniques and best practices\nImplement statistical analysis methods to validate findings and ensure data quality\nDocument methodologies, processes, and results to ensure reproducibility and knowledge sharing\n\nKnowledge, Skills & Attributes\n\n5-7 years of experience in data science or analytics, with exposure to financial or business data\nStrong technical background in data engineering and pipeline development\nAdvanced proficiency in Python and experience with Spark for large-scale data processing\nExperience working with data from Snowflake Data Lake or similar cloud-based data platforms\nDemonstrated skill in building dashboards and visualizations using BI tools (Power BI, Tableau, Looker, etc.)\nProficiency in SQL for data extraction and manipulation\nExperience applying machine learning algorithms to solve business problems\nAbility to communicate technical concepts to non-technical stakeholders\nUnderstanding of basic financial concepts and metrics (no deep finance expertise required)\nStrong problem-solving skills and attention to detail\nBachelor's degree in computer science, Data Science, Statistics, or related technical field\n\nWe would be thrilled if you bring in the below:\n\nExperience working in cross-functional teams in fast-paced environments\nFamiliarity with agile methodologies and collaborative development practices\nExperience with version control systems (Git) and collaborative coding\nKnowledge of cloud computing platforms (AWS, Azure, GCP)\nUnderstanding of data governance and data quality best practices\nDemonstrates curiosity and a willingness to learn about finance and accounting concepts\nShows creativity in presenting data insights through effective visualizations\nMaintains a continuous learning mindset, staying updated with emerging technologies and techniques in data science\n\nWe know that everyone has different wants and needs, which is why along with a highly competitive base salary we support our people and their loved ones with a variety of perks and benefits. As part of our team some of the benefits you can expect to receive are:\n\nMedical Insurance, Group Accidental Coverage/Insurance, Group Term Life Insurance\nCompany Paid Subscription to Calm The #1 app for mental fitness.\nEmployee Referral Program Bring the Best to Signant Health and earn a reward.\nWellness Program Participate in challenges and earn points for rewards.\nProof! Signant's Employee Recognition Program where you can accumulate points to redeem exciting merchandise, gift cards, tickets, and more.\nBurn Along Digital fitness and wellness platform\n\nDoes this sound like something you'd like to explore Then we'd love to hear from you!\n\nTo apply, please submit your CV and a cover letter letting us know why you think you'd be perfect for this role. We will begin reviewing submissions during the application period and will fill the vacancy as soon as a suitable candidate is identified.\n\nPlease note that Signant does not accept unsolicited resumes from Third Party vendors.\n\nAt Signant Health, accepting difference isn't enoughwe celebrate it, we support it, and we nurture it for the benefit of our team members, our clients and our community. Signant Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.",
        "skills": [
            "Snowflake Data Lake",
            "Looker",
            "Machine Learning",
            "Bi Tools",
            "Power Bi",
            "Spark",
            "Tableau",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "HR Data Scientist",
        "company_name": "Solventum",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Thank you for your interest in working for our Company. Recruiting the right talent is crucial to our goals. On April 1, 2024, 3M Healthcare underwent a corporate spin-off leading to the creation of a new company named Solventum. We are still in the process of updating our Careers Page and applicant documents, which currently have 3M branding. Please bear with us. In the interim, our Privacy Policy here: https://www.solventum.com/en-us/home/legal/website-privacy-statement/applicant-privacy/ continues to apply to any personal information you submit, and the 3M-branded positions listed on our Careers Page are for Solventum positions. As it was with 3M, at Solventum all qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.\n\nJob Description\n\n3M Health Care is now Solventum\n\nAt Solventum, we enable better, smarter, safer healthcare to improve lives. As a new company with a long legacy of creating breakthrough solutions for our customers toughest challenges, we pioneer game-changing innovations at the intersection of health, material and data science that change patients lives for the better while enabling healthcare professionals to perform at their best. Because people, and their wellbeing, are at the heart of every scientific advancement we pursue.\n\nWe partner closely with the brightest minds in healthcare to ensure that every solution we create melds the latest technology with compassion and empathy. Because at Solventum, we never stop solving for you.\n\nThe Impact You'll Make in this Role\n\nWe're seeking an HR Data Scientist to help build and strengthen our People Analytics capabilities within the People Experience team. Our mission is to create a best-in-class employee experience, with data and analytics at its core. As a data scientist, you'll have the opportunity to collaborate with team members on driving impactful analyses and/or data products that answer key talent and business questions.\n\nKey Responsibilities Include\n\nContribute to scalable data management to ensure data accuracy and accessibility, bringing together multiple HR data systems and business-related data sources\nPerform statistical analysis, selecting the best method for the specific question and context (e.g., ranging from basic regression to advanced clustering or predictive models), maintaining a balance between innovative approaches and interpretability\nCreate compelling data visualizations and data products to ensure insights are clearly and effectively communicated to key stakeholders\nCollaborate with stakeholders to scope and prioritize requests and drive insights aligned with organizational objectives.\nHelp contribute to raising the overall level of analytical fluency across HR and the business.\n\nYour Skills And Expertise\n\nTo set you up for success in this role from day one, Solventum requires (at a minimum) the following qualifications:\n\nBachelor's degree in I/O psychology, data science, applied statistics, human resources, business analytics, or a related field. Master's degree preferred.\n4+ years of professional experience in HR, business analytics, or a similar analytical position.\nExperience with advanced statistical programming tools (R preferred, Python, etc.), query languages, and data visualization tools.\nFamiliarity with range of statistical and data science methods and techniques, from regression and categorical data analysis, to organizational network analysis and predictive modeling.\n\nAdditional qualifications that could help you succeed even further in this role include:\n\nExperience with various types of people data, e.g., coming from HR information systems (HRIS), timekeeping, employee survey tools, workforce planning systems\nExperience interpreting complex analyses and findings for audiences with varying levels of analytical fluency, especially with the goal of driving action or supporting decision-making\nStrong problem-solving skills, including the ability to think creativity about the data and application of novel or innovative methodologies.\nExperience working with cloud computing platforms (e.g., AWS, Google Cloud, MS Azure).\n\nSolventum is committed to maintaining the highest standards of integrity and professionalism in our recruitment process. Applicants must remain alert to fraudulent job postings and recruitment schemes that falsely claim to represent Solventum and seek to exploit job seekers.\n\nPlease note that all email communications from Solventum regarding job opportunities with the company will be from an email with a domain of @solventum.com. Be wary of unsolicited emails or messages regarding Solventum job opportunities from emails with other email domains.\n\nPlease note: your application may not be considered if you do not provide your education and work history, either by: 1) uploading a resume, or 2) entering the information into the application fields directly.\n\nSolventum Global Terms of Use and Privacy Statement\n\nCarefully read these Terms of Use before using this website. Your access to and use of this website and application for a job at Solventum are conditioned on your acceptance and compliance with these terms.\n\nPlease access the linked document by clicking here, select the country where you are applying for employment, and review. Before submitting your application you will be asked to confirm your agreement with the\n\nterms.",
        "skills": [
            "HR business analytics",
            "applied statistics",
            "data visualization tools",
            "query languages",
            "Data Science",
            "Predictive Modeling"
        ]
    },
    {
        "job_title": "(Senior) Data Scientist",
        "company_name": "Elisa",
        "experience": "3-5 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Job\n\nDo you want to make a real impact by accelerating the transition to zero-carbon energy systems and work in a startup-like environment\n\nElisa's Smart Energy team is expanding rapidly, and we are seeking talented individuals to join our dynamic team of over 40 employees. Our multidisciplinary team leverages data and AI to address challenges posed by wind and solar energy becoming primary sources of power, through intelligently managed energy storage. As a (Senior) Data Scientist, you will bring your expertise in analyzing energy markets, implementing energy trading and market scheduling strategies, and contributing to the development of a 24/7 online system.\n\nElisa's Smart Energy Team's innovative AI-based solution optimizes energy storage assets, delivering financial, operational, and sustainability benefits to telecom, commercial, industrial and residential clients. We are not only helping businesses and individuals maximize their energy storage potential but also driving the green energy transition by enabling renewable energy production and stabilizing electricity grids. Learn more about our work to optimize batteries in the telecom towers to build a more energy-efficient future.\n\nAre you excited about transforming the energy ecosystem to a zero-carbon era Do you enjoy developing new solution concepts from the idea stage to pilot installations and full-scale commercial launches If this sounds like you, we want to hear from you! We are functioning like a startup within the larger Elisa corporation, proving that it is truly possible to combine the perks of a lean startup business with the resources and financial stability of a corporation. As we are growing fast and changing all the time, you will have a great chance to evolve the role yourself.\n\nWhat the role is all about\n\nDevelop predictive models to forecast energy prices and energy market behavior.\nAnalyze and interpret datasets from various energy markets, especially balancing reserve markets.\nDevelop algorithms to optimize trading and scheduling strategies using advanced analytics and machine learning techniques.\nVisualize and communicate complex data insights to stakeholders through dashboards, reports, and presentations.\nStay up-to-date with the latest developments in energy markets.\n\nWhat do we expect from you\n\nProven experience (3+ years) as a Data Scientist, in projects related to Time Series Predictions.\nProduction level coding experience in Python.\nStrong communication skills, with the ability to convey technical concepts to non-technical stakeholders.\nUnderstanding of energy markets, including the balancing reserve markets would be highly valued.\n\nMost of our team is based in Helsinki (some in Tampere and Turku), but like the rest of the elisians, we can work flexibly (in Southern Finland).\n\nWhy join us\n\nYou will be given a large portion of responsibility, but equal amounts of freedom and trust. We're a dedicated cross-disciplinary venture team, working on a new disruptive solution to the energy markets a true startup but inside Elisa.\n\nElisa has been ranked as one of the Finland's best employers for many years, for many reasons: huge amounts of flexibility, a friendly and laid-back working environment, great leadership that we are truly proud of, a continuous curiosity and an innovative aim to develop, as well as a fundamental invitation to come as you are. We understand and honor the fact that each Elisian comes with families, hobbies, problems, backgrounds and lives of their own. Read more about Elisa and the elisians here.\n\nWe can't wait to get to know you a bit better! We will start the interviews immediately with promising candidates, so submit your application through the link below as soon as possible! If you have any questions about the role, please turn to Ezgi Can Ozan at ezgi.ozan (a) elisa.com, or at +358 41 3120201.",
        "skills": [
            "Time Series Predictions",
            "Advanced Analytics",
            "Python",
            "Machine Learning"
        ]
    },
    {
        "job_title": "Senior Data Scientist_Experimentation",
        "company_name": "Lowe's India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Lowe's\n\nLowe's Companies, Inc. (NYSE: LOW) is a FORTUNE 50 home improvement company serving approximately 16 million customer transactions a week in the United States. With total fiscal year 2023 sales of more than $86 billion, Lowe's operates over 1,700 home improvement stores and employs approximately 300,000 associates. Based in Bengaluru, Lowe's India develops innovative technology products and solutions and delivers business capabilities to provide the best omnichannel experience for Lowe's customers. Lowe's India employs over 4,200 associates across technology, analytics, merchandising, supply chain, marketing, finance and accounting, product management and shared services. Lowe's India actively supports the communities it serves through programs focused on skill-building, sustainability and safe homes. For more information, visit, www.lowes.co.in.\n\nAbout The Team\n\nThe Experimentation Team at Lowe's drives end-to-end A/B testing, ensuring data-backed decisions before full rollouts. This progressive and rapidly growing team is focused on building scalable solutions that enhance decision-making and drive top and bottom-line business value along with creating great customer experiences.\n\nJob Summary\n\nWe are seeking an inspiring, technically savvy, data scientist who is passionate about building a best-in-class experimentation platform/program to support our rapidly growing suite of eCommerce products.\n\nAs a Senior Data Scientist, you will be a key player in driving data-driven decision-making across the organization, collaborating closely with engineering, product, marketing, and other cross-functional teams to deliver insights and products that shape the future of our business. You will also mentor junior data scientists and help to foster a culture of experimentation throughout the organization.\n\nRoles & Responsibilities\n\nCore Responsibilities:\n\nExperimentation Design & Analysis\n\nSupport the design and execution of A/B tests, multivariate experiments, and randomized controlled trials (RCTs) to assess the impact of product changes, marketing campaigns, and customer experiences.\nDevelop and implement robust methodologies to measure the effectiveness of business initiatives (e.g., website features, promotions, UI changes, etc.) using experimentation frameworks.\nOwn the end-to-end experimentation pipeline, including hypothesis generation, experimental design, implementation, monitoring, and post-experiment analysis.\nIdentify and mitigate biases in experiment design and results, ensuring statistical rigor and reliability.\n\nAdvanced Statistical Analysis & Modelling\n\nConduct advanced statistical analysis (e.g., causal inference, Bayesian analysis, regression modelling) to derive actionable insights from experimentation results.\nDevelop and refine models to predict customer behavior and optimize conversion rates, retention, and other key business metrics.\nAnalyze large-scale datasets and design efficient algorithms to support decision-making in areas like pricing, product recommendations, and personalization.\n\nContinuous Improvement & Innovation\n\nStay current with the latest advancements in data science, statistics, and experimentation methodologies.\nPropose innovative approaches to enhance the experimentation framework, such as new experimental designs, alternative modelling techniques, or improved metrics.\nLead or participate in research to explore new ways of measuring and optimizing the customer journey in a retail/e-commerce setting.\n\nYears Of Experience\n\n5+ years of professional experience in data science, with at least 2 years focused on experimentation, A/B testing, and causal inference in a retail or e-commerce environment.\nProven track record of designing and analyzing large-scale A/B tests and experiments with demonstrable business impact.\nStrong experience with statistical analysis and modelling techniques, including hypothesis testing, regression analysis, and Bayesian statistics\n\nEducation Qualification & Certifications (optional)\n\nRequired Minimum Qualifications\n\nPh.D. or master's degree in Data Science, Statistics, Mathematics, Computer Science, Economics, or a related field.\n\nSkill Set Required\n\nAdvanced knowledge of statistical methodologies for experiment design, analysis, and causal inference.\nExpertise in analytics/data software/tools such as Python, R, SQL, and experience with machine learning frameworks (e.g., TensorFlow, scikit-learn).\nStrong communication skills, with the ability to explain complex technical concepts to non-technical stakeholders and executive leadership.\nSolid understanding of e-commerce and retail metrics (e.g., conversion rate, customer lifetime value, churn, etc.) and how they relate to experimentation.\n\nSecondary Skills (desired)\n\nExperience with large-scale e-commerce platforms and digital product development.\nFamiliarity with the advanced causal and inferential analytics\nExperience with advanced techniques in machine learning or AI that complement experimentation (e.g., recommender systems, predictive modelling).\nFamiliarity with cloud-based platforms (e.g., AWS, Google Cloud, Azure).\nExperience working in an agile environment and collaborating with cross-functional teams in a fast-paced business setting.\n\nLowe's is an equal opportunity employer and administers all personnel practices without regard to race, color, religious creed, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law.\n\nStarting rate of pay may vary based on factors including, but not limited to, position offered, location, education, training, and/or experience. For information regarding our benefit programs and eligibility, please visit https://talent.lowes.com/us/en/benefits.",
        "skills": [
            "R",
            "statistical methodologies",
            "scikit-learn",
            "Bayesian Statistics",
            "causal inference",
            "Regression Analysis",
            "Tensorflow",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "GenAI Senior Data Scientist",
        "company_name": "Arivonix",
        "experience": "5-7 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Role:\nWe are seeking a Data Scientist / Generative AI Developer with a strong foundation in mathematics and advanced data science methodologies, coupled with expertise in Large Language Models (LLMs), Generative AI, and orchestration frameworks like LangChain or CrewAI. This is a key role in shaping cutting-edge machine learning solutions and integrating state-of-the-art AI capabilities, while collaborating across diverse teams.\nKey Responsibilities:\nDesign, develop, and implement machine learning algorithms and models, including creating specifications, design documents, and prototypes.\nBuild, fine-tune, and deploy solutions leveraging Large Language Models (LLMs) for various use cases.\nDevelop workflows and applications using LangChain, CrewAI, or similar GenAI orchestration frameworks to enable seamless AI integrations.\nOptimize model performance with advanced tuning techniques, feature engineering, and experimentation with LLMs.\nLead brainstorming sessions for system architecture and feature design to ensure scalability and efficiency.\nIntegrate and manage Vector Databases (VectorDBs) such as Pinecone, Weaviate, or similar, to handle embedding-based search and retrieval tasks.\nGather and document design requirements by interfacing with key stakeholders and external customers.\nCollaborate with Engineering, Data Science, Product, UX, Business Development, and Infrastructure teams.\nUphold and promote best coding practices, including thorough documentation and peer reviews.\nRequired Qualifications:\nMaster's degree in Data Science, Mathematics, or a related field.\n5+ years of experience in Data Science, Machine Learning, or a similar domain.\nExpertise in linear algebra, statistics, and probability, with hands-on experience in statistical testing, regression, and deep learning techniques.\nProficiency in machine learning frameworks such as TensorFlow, PyTorch, or MxNet.\nExperience with Large Language Models (LLMs) such as GPT-4, BERT, or T5.\nProven experience with orchestration frameworks like LangChain, CrewAI, or similar.\nStrong understanding of Vector Databases (VectorDBs) and embedding-based search.\nAdvanced development experience in Python and libraries like NumPy, Sci-Kit Learn, and Matplotlib.\nProven track record of model performance tuning and deploying optimized machine learning models.\nPreferred Skills:\nProficiency in creating scalable machine learning pipelines.\nFamiliarity with industry standards for code optimization and debugging.\nEducation Requirements:\nMaster's Degree in:\nData Science\nComputer Science\nArtificial Intelligence\nMathematics\nMachine Learning\nStatistics\nRelated fields\nSpecial Considerations:\nCertifications or postgraduate diplomas in AI/ML (e.g., from institutions like IIIT-Hyderabad, ISB, or online platforms like Stanford AI, Coursera, edX) could also supplement academic credentials if accompanied by relevant experience.\nKey Skills:\n-Experience with LLMs, LangChain/CrewAI, Vector Databases (e.g., Pinecone, Weaviate).\n-Data Science, Machine Learning, TensorFlow, PyTorch, Deep Learning\n-Expertise in Python\nIf you are a highly motivated professional with expertise in data science, a passion for mathematical problem-solving, and a drive to create impactful solutions, we'd love to have you on our team!",
        "skills": [
            "LangChain",
            "CrewAI",
            "MxNet",
            "Sci-Kit Learn",
            "Tensorflow",
            "Matplotlib",
            "Numpy",
            "Pytorch",
            "Python"
        ]
    },
    {
        "job_title": "Principal Data Scientist - Search",
        "company_name": "Lowe's India",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Lowe's\n\nLowe's Companies, Inc. (NYSE: LOW) is a FORTUNE 50 home improvement company serving approximately 16 million customer transactions a week in the United States. With total fiscal year 2024 sales of more than $83 billion, Lowe's operates over 1,700 home improvement stores and employs approximately 300,000 associates. Based in Mooresville, N.C., Lowe's supports the communities it serves through programs focused on creating safe, affordable housing, improving community spaces, helping to develop the next generation of skilled trade experts and providing disaster relief to communities in need. For more information, visit Lowes.com.\n\nAbout The Team\n\nProduct Discovery team is responsible for managing the Search & Recommendation functions to help customers find the relevant products. Team is focused on building scalable nextgen models using AI /ML to enhance product discovery by personalizing and optimizing the customer experience. Their key goals are:\n\nPersonalized Recommendations: Build recommendation systems that suggest products based on user behavior and preferences.\nSmarter Search: Use AI techniques like NLP to improve search accuracy and relevance.\nDynamic Personalization: Tailor product listings and content to individual users, boosting engagement and conversion.\nUser Insights: Leverage data to refine discovery algorithms and enhance the customer journey.\nScalable Infrastructure: Ensure algorithms remain efficient as data and user interactions grow.\nContinuous Improvement: Continuously refine models through feedback loops and performance metrics.\n\nUltimately, the team aims to drive higher conversions, better user experience, and operational efficiency by delivering smarter, scalable product discovery through a data driven approach.\n\nJob Summary\n\nWe are seeking a talented Principal Data Scientist to lead the development of Product Search and Recommendation systems. The ideal candidate will have a strong background in natural language processing (NLP), machine learning, deep learning, and semantic understanding, and will be passionate about transforming the way users discover relevant content. As a Principal Data Scientist, you will be responsible for driving the vision and strategy for Ecommerce search and personalized recommendation systems, leveraging state-of-the-art techniques such as transformer models, embeddings, and knowledge graphs. You will collaborate closely with product teams, engineers, and business stakeholders to enhance user experience and business outcomes.\n\nRoles & Responsibilities\n\nLeadership & Strategy\n\nLead the development of cutting-edge Search and recommendation algorithms that improve relevance, personalization, and user engagement.\n\nDefine and execute the strategy for semantic understanding and recommendations, aligning with overall business goals.\n\nWork cross-functionally with product managers, engineers, and data scientists to drive the roadmap for search and recommendation improvements.\n\nSearch & NLP Expertise Lead the design and implementation of Search/Recommendation models that go beyond traditional keyword matching to understand user intent and context. Apply advanced NLP techniques such as transformers (BERT, GPT, T5), word embeddings, and contextualized word representations to enhance search relevance. Use techniques like sentence embeddings, document embeddings, and similarity measures to build scalable search systems that understand semantic meaning.\nModel Development & Experimentation Conduct research and experiments to design, develop, and validate new Search models and Recommendation techniques. Continuously test, measure, and optimize models through A/B testing, real-time metrics, and user feedback loops. Develop approaches to handle large-scale data, ensuring the models can be deployed efficiently in production environments.\nCollaboration & Mentorship Lead and mentor a team of data scientists, guiding them in the development of semantic models and complex recommendation algorithms. Foster a culture of knowledge sharing and collaboration across teams to ensure that the best practices are followed in model development and deployment. Collaborate with engineers & product managers to ensure the effective integration of models into scalable production systems.\nThought Leadership Stay up to date with the latest research in Search, NLP, and recommendation systems. Evangelize the use of cutting-edge techniques within the company to drive innovation in search and recommendations.\n\nYears Of Experience\n\n8 years of experience executing and deploying data science, machine learning, deep learning, and generative AI solutions, preferably in a large-scale enterprise setting (fewer years may be accepted with a master's or doctorate degree)\n8 years of programming experience (fewer years may be accepted with a master's or doctorate degree)\n5 years of SQL experience and knowledge of various statistical modeling or machine learning techniques\nBachelor's degree in mathematics, statistics, physics, economics, engineering, computer science, data or information science, or related quantitative analytic field (or equivalent work experience in lieu of degree)\n\nCandidates with Doctorate or Master's degree are preferred\n\nEducation Qualification & Certifications\n\nBachelor's degree (Required): Mathematics, Statistics, Physics, Economics, Engineering, Computer Science, Data or Information Science, or related quantitative analytic field (or equivalent work experience in a related field)\nDoctorate Degree (Preferred): Mathematics, Statistics, Physics, Economics, Engineering, Computer Science, Data or Information Science, or related quantitative analytic field\n\nSkill Set Required\n\nMachine Learning & AI\n\nSupervised/unsupervised learning (e.g., regression, clustering)\nDeep learning (CNNs, RNNs, Transformers)\nNatural Language Processing (NLP) for search relevance\nExperience with generating vector embeddings\n\nStatistical & Mathematical Expertise\n\nProbability, statistics, and A/B testing\n\nLeadership & Collaboration\n\nCross-functional team collaboration (engineering, product, design)\nMentoring junior data scientists\nCommunicating technical concepts to non-technical stakeholders\n\nTools & Frameworks\n\nProgramming (Python, R, Scala)\nML frameworks (TensorFlow, PyTorch, scikit-learn)\nVersion control (Git)\n\nPerformance Evaluation\n\nModel evaluation using metrics (precision, recall, NDCG)\nOnline learning and incremental model updates\n\nUnderstanding of Data Engineering & Infrastructure\n\nBig data technologies (Apache Spark, Hadoop)\nData pipeline management (ETL processes)\nDatabase management (SQL, NoSQL, Elasticsearch)\nCloud platforms (AWS/GCP/Azure)\n\nLowe's is an equal opportunity employer and administers all personnel practices without regard to race, color, religious creed, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law.\n\nStarting rate of pay may vary based on factors including, but not limited to, position offered, location, education, training, and/or experience. For information regarding our benefit programs and eligibility, please visit https://talent.lowes.com/us/en/benefits.",
        "skills": [
            "T5",
            "scikit-learn",
            "GPT",
            "Word Embeddings",
            "R",
            "Transformers",
            "Document Embeddings",
            "BERT",
            "Statistical Modeling",
            "Sentence Embeddings",
            "Machine Learning",
            "Hadoop",
            "Scala",
            "Apache Spark",
            "Sql",
            "Deep Learning",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Intelligence Node",
        "experience": "7-10 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview:\nIntelligence Node is a real-time retail intelligence platform that empowers businesses to drive product-level profitability and grow margins using data-driven real-time competitive insights. Intelligence Node's cutting-edge technology leverages AI to aggregate and analyze billions of data points across 1,900 retail categories in 34 global markets. Intelligence Node's business model provides a comprehensive suite of solutions encompassing product matching, digital shelf analytics, assortment & availability, pricing & promotions, and brand compliance, to aid retailers and brands in all strategic and operational decision-making. Its patented product matching engine and proprietary AI offer 99% data accuracy and 10-second data refresh rates. In December 2024, Intelligence Node was acquired by Interpublic Group (IPG), a global leader in marketing solutions. Joining forces with IPG through this strategic acquisition allows Intelligence Node to deliver a future-ready combined solution to companies navigating the complexities of today's commerce landscape. Together, we can provide the comprehensive data, advanced analytics, and strategic expertise that enterprise brands and retailers need to win market share and drive sustainable growth.\nJob Description:\nAs a Senior Data Scientist at Intelligence Node, you will lead a team of data scientists and collaborate closely with key business stakeholders.\nDevelop advanced machine learning models and neural networks to solve complex problems in the retail and e-commerce sectors.\nExperiment with LLMs (Large Language Models)\nLead the design and development of scalable data pipelines.\nMentor junior data scientists\nDrive the deployment of predictive models into production.\nRequired Skill Set\nStrong proficiency in Python or C++\nDeep understanding of BERT, transformers, and other advanced machine learning architectures\nExpertise in computer vision techniques and applications e.g., specialized embedding techniques such as visual transformers and OCR.\nLLMs understanding and curiosity to explore (out of box deployment, RAG, fine tune etc.)\nExtensive retail and e-commerce experience is preferred.\nPhD in a related field is preferred.\nDesired profile of the candidate\nExceptional communication skills\nLeadership ability in fast-paced environments\nResults-oriented and analytical\nExperience leading cross-functional teams.\nAbility to derive actionable insights from complex data.\nExperience Required: 7-10yrs",
        "skills": [
            "visual transformers",
            "computer vision techniques",
            "LLMs",
            "advanced machine learning architectures",
            "BERT transformers",
            "Ocr",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Soul AI",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are looking for a Data Scientist to join and build intelligent, data-driven solutions for our client that enable impactful decisions. This role requires contributions across the data science lifecycle from data wrangling and exploratory analysis to building and deploying machine learning models.\nWhether you're just getting started or have years of experience, we're looking for individuals who are curious, analytical, and driven to make a difference with data.\nResponsibilities:\nDesign, develop, and deploy machine learning models and analytical solutions.\nConduct exploratory data analysis and feature engineering.\nOwn or contribute to the end-to-end data science pipeline: data cleaning, modeling, validation, and deployment.\nCollaborate with cross-functional teams (engineering, product, business) to define problems and deliver measurable impact.\nTranslate business challenges into data science problems and communicate findings clearly.\nImplement A/B tests, statistical tests, and experimentation strategies.\nSupport model monitoring, versioning, and continuous improvement in production environments.\nEvaluate new tools, frameworks, and best practices to improve model accuracy and scalability.\nRequired Skills:\nStrong programming skills in Python including libraries such as pandas, NumPy, scikit-learn, matplotlib, seaborn.\nProficient in SQL, comfortable querying large, complex datasets.\nSound understanding of statistics, machine learning algorithms, and data modeling.\nExperience building end-to-end ML pipelines.\nExposure to or hands-on experience with model deployment tools like FastAPI, Flask, MLflow.\nExperience with data visualization and insight communication.\nFamiliarity with version control tools (e.g., Git) and collaborative workflows.\nAbility to write clean, modular code and document processes clearly.\nNice to Have:\nExperience with deep learning frameworks like TensorFlow or PyTorch.\nFamiliarity with data engineering tools like Apache Spark, Kafka, Airflow, dbt.\nExposure to MLOps practices and managing models in production environments.\nWorking knowledge of cloud platforms like AWS, GCP, or Azure (e.g., SageMaker, BigQuery, Vertex AI).\nExperience designing and interpreting A/B tests or causal inference models.\nPrior experience in high-growth startups or cross-functional leadership roles.\nEducational Qualifications:\nBachelor's or Master's degree in Computer Science, Data Science, Mathematics, Engineering, or a related field.\nPh.D. holders or candidates with demonstrated applied research contributions are a plus.",
        "skills": [
            "scikit-learn",
            "MLflow",
            "end-to-end ML pipelines",
            "Statistics",
            "Data Modeling",
            "Sql",
            "Numpy",
            "Git",
            "Seaborn",
            "Pandas",
            "Matplotlib",
            "Flask",
            "Data Visualization",
            "FastAPI",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Quartic.ai",
        "experience": "5-7 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Company - Quartic.ai is a SaaS industrial AI start-up accelerating Industry 4.0 adoption by helping manufacturers unlock more value from existing infrastructure. Our flagship Quartic Platform empowers subject matter experts to build and deploy AI and IIoT applicationswithout needing programming or data science expertise. Used by Fortune 100 and 500 companies, the platform drives productivity and efficiency in sectors like pharmaceuticals, food & beverage, and CPG, supporting use cases such as process optimization, predictive maintenance, and energy management.\nAbout the Role - Harness small industrial datasets to boost yield, quality, and efficiency. You will clean data, build lightweight ML models, and run Bayesian/black-box optimization loopsrelying on deep mathematical and statistical understanding (PCA, PLS, Gaussian processes, etc.) to pick the right tool for each problem. No heavy software engineering; the focus is analytical rigor and measurable process impact.\nResponsibilities -\nData Quality + Exploration, audit, clean, and visualize sensor/log data quantify uncertainty Python (pandas, NumPy, Matplotlib), outlier & drift detection methods\nStatistical / Small-Data ML Modeling - regression, classification, clustering, PCA, PLS, Gaussian-process regression; rigorous CV and bootstrapping scikit-learn, XGBoost/LightGBM, PyMC/GPy, statsmodels\nBayesian & Black-Box Optimization tune process set-points or recipe parameters under real-world constraints; deliver closed-loop recommendations Optuna, Ax, BoTorch; expected-improvement, Thompson sampling\nInsight Delivery & Method Selection map business questions to the correct statistical/ML technique and communicate results to engineers & executives Clear slide decks, concise memos, stakeholder workshops\nQualifications -\nExperience - 5+ years applying advanced statistical methods and machine learning to industrial processes or similar complex systems, with demonstrable impact on process optimization\nEducation Master's in Statistics, Applied Mathematics, Industrial Engineering, or similar\nMathematical Depth linear algebra, probability, Bayesian inference, experimental error analysis\nStatistical Learning Mastery PCA, PLS, RFE, LASSO/Ridge, Gaussian processes; ability to justify technique choice for each industrial scenario\nOptimization Experience hands-on Bayesian or other black-box optimization applied to a real process (manufacturing, energy, materials, etc.)\nProgramming Fluency Python notebooks/scripts with pandas, NumPy, scikit-learn, and one gradient-boosting library\nCommunication translate quantitative findings into actionable operating guidance for mixed audiences\nPreferred Skills -\nProbabilistic-programming exposure (PyMC, Stan)\nML tracking/reproducibility tools (MLflow, DVC)\nPrior work in high-stakes industrial environments (chemicals, semiconductors, batteries, metals, etc.)",
        "skills": [
            "scikit-learn",
            "PyMC",
            "LightGBM",
            "BoTorch",
            "GPy",
            "Optuna",
            "statsmodels",
            "Matplotlib",
            "Numpy",
            "Pandas",
            "XGBoost",
            "Ax",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist, II",
        "company_name": "Zebra Technologies",
        "experience": "3-8 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Remote Work: Hybrid\n\nOverview:\n\nAt Zebra, we are a community of innovators who come together to create new ways of working to make everyday life better. United by curiosity and care, we develop dynamic solutions that anticipate our customer's and partner's needs and solve their challenges.\n\nBeing a part of Zebra Nation means being seen, heard, valued, and respected. Drawing from our diverse perspectives, we collaborate to deliver on our purpose. Here you are a part of a team pushing boundaries to redefine the work of tomorrow for organizations, their employees, and those they serve.\n\nYou have opportunities to learn and lead at a forward-thinking company, defining your path to a fulfilling career while channeling your skills toward causes that you care about locally and globally. We've only begun reimaging the future for our people, our customers, and the world.\n\nLet's create tomorrow together.\n\nLooking for a Data Science professional with expertise in PySpark/Databricks and experience working on different stages of a Data Science project life cycle. Incumbent is expected to build and optimize Data Pipelines, tune/enhance models, and explain output to business stakeholders. Team primarily works on Demand Forecasting, Promotion Modelling, and Inventory Optimization problems for CPG/Retail customers, prior experience in CPG/Retail is strongly preferred.\n\nResponsibilities:\n\nEssential Duties and Responsibilities:\n\nDesign, optimize, and maintain scalable ETL pipelines using PySpark and Databricks on cloud platforms (Azure/GCP).\nDevelop automated data validation process to proactively perform data quality checks.\nOptimal allocation of cloud resources to control cloud cost.\nCreate and schedule jobs on Databricks platform. Work with GitHub repositories and ensure that best practices are being implemented.\nWork on Supply Chain Optimization models, such as Demand Forecasting, Price Elasticity of Demand, and Inventory Optimization.\nBuild and tune forecast model, identify improvement opportunities, and perform experiments to prove value.\nIncumbent is expected to have frequent conversation with Business Stakeholders. Explain data deficiencies, forecast variances, and role of different forecast drivers.\nFollow best practices in Architecture, Coding, and BAU operations.\nCollaborate with cross-functional teams, such as Business Stakeholders, Engagement Managers, Data Ops/Job Monitoring, Product Engineering/UI, Data Science, and Data Engineering.\n\nQualifications:\n\nPreferred Education: bachelor's in computer science/IT or in similar field with strong programming exposure and master's degree in Statistics/Operations Research/Mathematics/Data Science.\n3 8 years of experience in Data Science/Data Engineering. Exposure to Demand Forecasting and Inventory optimization in CPG/Retail will be a big plus.\nProven experience building and optimizing Data Pipelines/ETL processes in PySpark, DataBricks, Python (Pandas/NumPy) and SQL. Experience working with Git as a collaboration tool.\nGood understanding of cloud platform preferably Azure.\nExposure to conventional time series forecasting (ESM, ARIMA) and Machine Learning models (GBM, ANN, Random Forests).\nThe role is expected to work independently with very low supervision.\nGood communication skills, ability to present output to business stakeholder and convey data deficiencies.\n\nTo protect candidates from falling victim to online fraudulent activity involving fake job postings and employment offers, please be aware our recruiters will always connect with you via @zebra.com email accounts. Applications are only accepted through our applicant tracking system and only accept personal identifying information through that system. Our Talent Acquisition team will not ask for you to provide personal identifying information via e-mail or outside of the system. If you are a victim of identity theft contact your local police department.",
        "skills": [
            "ANN",
            "Random Forests",
            "Demand Forecasting",
            "Inventory Optimization",
            "GBM",
            "Time Series Forecasting",
            "ETL processes",
            "Machine Learning models",
            "Esm",
            "Pyspark",
            "Sql",
            "Numpy",
            "Git",
            "Gcp",
            "Pandas",
            "Arima",
            "Databricks",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Intelligence Node",
        "experience": "7-10 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview:\nIntelligence Node is a real-time retail intelligence platform that empowers businesses to drive product-level profitability and grow margins using data-driven real-time competitive insights. Intelligence Node's cutting-edge technology leverages AI to aggregate and analyze billions of data points across 1,900 retail categories in 34 global markets. Intelligence Node's business model provides a comprehensive suite of solutions encompassing product matching, digital shelf analytics, assortment & availability, pricing & promotions, and brand compliance, to aid retailers and brands in all strategic and operational decision-making. Its patented product matching engine and proprietary AI offer 99% data accuracy and 10-second data refresh rates. In December 2024, Intelligence Node was acquired by Interpublic Group (IPG), a global leader in marketing solutions. Joining forces with IPG through this strategic acquisition allows Intelligence Node to deliver a future-ready combined solution to companies navigating the complexities of today's commerce landscape. Together, we can provide the comprehensive data, advanced analytics, and strategic expertise that enterprise brands and retailers need to win market share and drive sustainable growth.\nJob Description:\nAs a Senior Data Scientist at Intelligence Node, you will lead a team of data scientists and collaborate closely with key business stakeholders.\nDevelop advanced machine learning models and neural networks to solve complex problems in the retail and e-commerce sectors.\nExperiment with LLMs (Large Language Models)\nLead the design and development of scalable data pipelines.\nMentor junior data scientists\nDrive the deployment of predictive models into production.\nRequired Skill Set\nStrong proficiency in Python or C++\nDeep understanding of BERT, transformers, and other advanced machine learning architectures\nExpertise in computer vision techniques and applications e.g., specialized embedding techniques such as visual transformers and OCR.\nLLMs understanding and curiosity to explore (out of box deployment, RAG, fine tune etc.)\nExtensive retail and e-commerce experience is preferred.\nPhD in a related field is preferred.\nDesired profile of the candidate\nExceptional communication skills\nLeadership ability in fast-paced environments\nResults-oriented and analytical\nExperience leading cross-functional teams.\nAbility to derive actionable insights from complex data.\nExperience Required: 7-10yrs",
        "skills": [
            "visual transformers",
            "computer vision techniques",
            "LLMs",
            "advanced machine learning architectures",
            "BERT transformers",
            "Ocr",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Soul AI",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are looking for a Data Scientist to join and build intelligent, data-driven solutions for our client that enable impactful decisions. This role requires contributions across the data science lifecycle from data wrangling and exploratory analysis to building and deploying machine learning models.\nWhether you're just getting started or have years of experience, we're looking for individuals who are curious, analytical, and driven to make a difference with data.\nResponsibilities:\nDesign, develop, and deploy machine learning models and analytical solutions.\nConduct exploratory data analysis and feature engineering.\nOwn or contribute to the end-to-end data science pipeline: data cleaning, modeling, validation, and deployment.\nCollaborate with cross-functional teams (engineering, product, business) to define problems and deliver measurable impact.\nTranslate business challenges into data science problems and communicate findings clearly.\nImplement A/B tests, statistical tests, and experimentation strategies.\nSupport model monitoring, versioning, and continuous improvement in production environments.\nEvaluate new tools, frameworks, and best practices to improve model accuracy and scalability.\nRequired Skills:\nStrong programming skills in Python including libraries such as pandas, NumPy, scikit-learn, matplotlib, seaborn.\nProficient in SQL, comfortable querying large, complex datasets.\nSound understanding of statistics, machine learning algorithms, and data modeling.\nExperience building end-to-end ML pipelines.\nExposure to or hands-on experience with model deployment tools like FastAPI, Flask, MLflow.\nExperience with data visualization and insight communication.\nFamiliarity with version control tools (e.g., Git) and collaborative workflows.\nAbility to write clean, modular code and document processes clearly.\nNice to Have:\nExperience with deep learning frameworks like TensorFlow or PyTorch.\nFamiliarity with data engineering tools like Apache Spark, Kafka, Airflow, dbt.\nExposure to MLOps practices and managing models in production environments.\nWorking knowledge of cloud platforms like AWS, GCP, or Azure (e.g., SageMaker, BigQuery, Vertex AI).\nExperience designing and interpreting A/B tests or causal inference models.\nPrior experience in high-growth startups or cross-functional leadership roles.\nEducational Qualifications:\nBachelor's or Master's degree in Computer Science, Data Science, Mathematics, Engineering, or a related field.\nPh.D. holders or candidates with demonstrated applied research contributions are a plus.",
        "skills": [
            "scikit-learn",
            "MLflow",
            "end-to-end ML pipelines",
            "Statistics",
            "Data Modeling",
            "Sql",
            "Numpy",
            "Git",
            "Seaborn",
            "Pandas",
            "Matplotlib",
            "Flask",
            "Data Visualization",
            "FastAPI",
            "Python",
            "Machine Learning Algorithms"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Ciklum",
        "experience": "Fresher",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Ciklum is looking for a Senior Data Scientist to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\nAbout the role:\nAs a Senior Data Scientist, become a part of a cross-functional development team engineering experiences of tomorrow.\nResponsibilities:\nDevelopment of prototype solutions, mathematical models, algorithms, machine learning techniques, and robust analytics to support analytic insights and visualization of complex data sets\nWork on exploratory data analysis so you can navigate a dataset and come out with broad conclusions based on initial appraisals\nProvide optimization recommendations that drive KPIs established by product, marketing, operations, PR teams, and others\nInteracts with engineering teams and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability.\nWork directly with business analysts and data engineers to understand and support their use cases\nWork with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions\nDrive innovation by exploring new experimentation methods and statistical techniques that could sharpen or speed up our product decision-making processes\nCross-train other team members on technologies being developed, while also continuously learning new technologies from other team members\nContribute to the Unit activities and community building, participate in conferences, and provide excellence in exercise and best practices\nSupport marketing & sales activities, customer meetings and digital services through direct support for sales opportunities & providing thought leadership & content creation for the service\nRequirements:\nWe know that sometimes, you can't tick every box. We would still love to hear from you if you think you're a good fit!\nGeneral technical requirements\nBSc, MSc, or PhD in Mathematics, Statistics, Computer Science, Engineering, Operations Research, Econometrics, or related fields\nStrong knowledge of Probability Theory, Statistics, and a deep understanding of the Mathematics behind Machine Learning\nProficiency with CRISP-ML(Q) or TDSP methodologies for addressing commercial problems through data science solutions\nHands-on experience with various machine learning techniques, including but not limited to:\nRegression\nClassification\nClustering\nDimensionality reduction\nProficiency in Python for developing machine learning models and conducting statistical analyses\nStrong understanding of data visualization tools and techniques (e.g., Python libraries such as Matplotlib, Seaborn, Plotly, etc.) and the ability to present data effectively\nSpecific technical requirements:\nProficiency in SQL for data processing, data manipulation, sampling, and reporting\nExperience working with imbalanced datasets and applying appropriate techniques\nExperience with time series data, including preprocessing, feature engineering, and forecasting\nExperience with outlier detection and anomaly detection\nExperience working with various data types: text, image, and video data\nFamiliarity with AI/ML cloud implementations (AWS, Azure, GCP) and cloud-based AI/ML services (e.g., Amazon SageMaker, Azure ML)\nDomain experience:\nExperience with analyzing medical signals and images\nExpertise in building predictive models for patient outcomes, disease progression, readmissions, and population health risks\nExperience in extracting insights from clinical notes, medical literature, and patient-reported data using NLP and text mining techniques\nFamiliarity with survival or time-to-event analysis\nExpertise in designing and analyzing data from clinical trials or research studies\nExperience in identifying causal relationships between treatments and outcomes, such as propensity score matching or instrumental variable techniques\nUnderstanding of healthcare regulations and standards like HIPAA, GDPR (for healthcare data), and FDA regulations for medical devices and AI in healthcare\nExpertise in handling sensitive healthcare data in a secure, compliant way, understanding the complexities of patient consent, de-identification, and data sharing\nFamiliarity with decentralized data models such as federated learning to build models without transferring patient data across institutions\nKnowledge of interoperability standards such as HL7, SNOMED, FHIR, or DICOM\nAbility to work with clinicians, researchers, health administrators, and policy makers to understand problems and translate data into actionable healthcare insights\nGood to have skills:\nExperience with MLOps, including integration of machine learning pipelines into production environments, Docker, and containerization/orchestration (e.g., Kubernetes)\nExperience in deep learning development using TensorFlow or PyTorch libraries\nExperience with Large Language Models (LLMs) and Generative AI applications\nAdvanced SQL proficiency, with experience in MS SQL Server or PostgreSQL\nFamiliarity with platforms like Databricks and Snowflake for data engineering and analytics\nExperience working with Big Data technologies (e.g., Hadoop, Apache Spark)\nFamiliarity with NoSQL databases (e.g., columnar or graph databases like Cassandra, Neo4j)\nBusiness-related requirements:\nProven experience in developing data science solutions that drive measurable business impact, with a strong track record of end-to-end project execution\nAbility to effectively translate business problems into data science problems and create solutions from scratch using machine learning and statistical methods\nExcellent project management and time management skills, with the ability to manage complex, detailed work and effectively communicate progress and results to stakeholders at all levels\nDesirable:\nResearch experience with peer-reviewed publications\nRecognized achievements in data science competitions, such as Kaggle\nCertifications in cloud-based machine learning services (AWS, Azure, GCP)\nWhat`s in it for you\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune\nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\nAbout us:\nIndia is a strategic growth market for Ciklum.\nBe a part of a big story created right now. Let's grow our delivery center in India together! Boost your skills and knowledge: create and innovate with like-minded professionals all of that within a global company with a local spirit and start-up soul.\nSupported by Recognize Partners and expanding globally, we will engineer the experiences of tomorrow!\nBe bold, not bored!\nInterested already We would love to get to know you! Submit your application. We can't wait to see you at Ciklum.",
        "skills": [
            "Dimensionality Reduction",
            "Classification",
            "Data Processing",
            "Outlier Detection",
            "CRISP-ML Q",
            "TDSP",
            "Regression",
            "anomaly detection",
            "NoSQL Databases",
            "Machine Learning",
            "Tensorflow",
            "Data Manipulation",
            "Nlp",
            "Pytorch",
            "Docker",
            "Big Data Technologies",
            "Python",
            "AWS",
            "Clustering",
            "Sql",
            "Gcp",
            "Data Visualization",
            "Azure",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Data Scientist(LLM/Gen AI Engineer)",
        "company_name": "TIGI HR",
        "experience": "5-9 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position: LLM Engineer GenAI / ML (Python, LangChain)\nExperience: 59 Years\nLocation: Hyderabad / Gurgaon (WFO, Hybrid 3 Days Onsite)\nNotice Period: Immediate Joiners Only\nWe're hiring an LLM Engineer to join our GenAI delivery team, focused on developing advanced AI solutions for enterprise use. You'll work with cutting-edge tools and collaborate across teams to build and deploy scalable, real-world applications.\nKey Responsibilities:\nDevelop GenAI applications using prompt engineering, RAG, and agent frameworks\nWrite efficient code in Python, LangChain/LangGraph, and SQL\nIntegrate solutions with Azure, AWS, or GCP environments\nCollaborate with product and SME teams to align deliverables\nGuide technical implementation and contribute to the project roadmap\nMentor junior engineers and ensure best practices\nStay current with trends in LLM and GenAI\nRequirements:\n5+ years in ML engineering; 1+ years in LLM/GenAI projects\nProficiency in Python, SQL, and LangChain\nExperience with cloud platforms and collaborative development",
        "skills": [
            "LangGraph",
            "LangChain",
            "RAG",
            "agent frameworks",
            "prompt engineering",
            "Gcp",
            "Azure",
            "Python",
            "Sql",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PVAR SERVICES",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Delhi, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist - Marketing\nLocation: Delhi (Hybrid)\nExperience: 24 Years\nCTC: Up to 25 LPA\nKey Responsibilities:\n-Leverage data science techniques like segmentation, clustering, and predictive modeling to analyze customer behavior and guide personalized marketing strategies.\n-Extract, clean, and integrate data from platforms like CDP, Adobe Analytics, Google Analytics, CRM, and Salesforce to ensure analytical accuracy.\n-Design and interpret A/B tests to evaluate and optimize marketing tactics and website performance.\n-Contribute to the development of marketing attribution models to assess channel-wise impact on conversions and sales.\n-Partner with marketing, product, and IT teams to translate analytical insights into actionable marketing strategies.\nRequired Skills & Qualifications:\n-Bachelor's/Master's in Data Science, Statistics, Computer Science, or related field.\n-24 years of experience as a Data Scientist or Analyst in a marketing setting (B2B/financial services preferred).\n-Proficient in Python or R and SQL.\n-Strong foundation in statistics, ML algorithms, and data visualization.\n-Experience with libraries like Pandas, NumPy, and scikit-learn.\n-Excellent communication skills for non-technical stakeholder engagement.\n-Detail-oriented with strong analytical and problem-solving skills.\n-Comfortable working in cross-functional teams.",
        "skills": [
            "R",
            "ML algorithms",
            "scikit-learn",
            "Statistics",
            "Numpy",
            "Pandas",
            "Data Visualization",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Fraud Data Scientist",
        "company_name": "Straive",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Roles and Responsibilities:\nDesign, develop, and deploy machine learning models to detect and prevent fraudulent activities, such as Merchant Fraud, transactional fraud, account takeover, and identity theft\nWork with large datasets to identify patterns, trends, and anomalies that may indicate fraudulent activity\nUtilize data analytics tools and methodologies to conduct in-depth assessments and generate Fraud rules and reports on fraud trends (including first-party and third-party fraud).\nCollaborate with cross-functional teams, including risk management, operations, and compliance, to enhance fraud prevention measures.\nMonitor industry trends, regulatory changes, and best practices to continually enhance fraud prevention strategies.\nSkills Required:\nBachelor's degree in engineering, technology, computer science or related field.\n3+ years of proven data analytics experience in fraud prevention, risk management, or a related field\nFamiliarity with fraud detection software, risk assessment methodologies, and regulatory compliance.\nStrong experience in SQL or Python\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely.\nDetail-oriented with a proactive mindset toward problem-solving and risk mitigation.\nAbility to work collaboratively in a cross-functional team environment.",
        "skills": [
            "Regulatory Compliance",
            "fraud detection software",
            "risk assessment methodologies",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Commonwealth Bank",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Banking/Accounting/Financial Services",
        "job_description": "Organization:\nAt CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title: Data Scientist\nLocation: Bangalore\nBusiness & Team:Analytics BB\nImpact & contribution:\nAs a Data Scientist, you will have the opportunity to apply your quantitative and computational skills within an applied and production-oriented R&D function within the group, focusing on cutting-edge deep learning and generative AI techniques. Your role will have a significant impact on our innovation capabilities and business processes by leveraging these advanced technologies\nRoles & Responsibilities:\nModel Development: Design and implement deep learning models, focusing on Generative AI applications like text generation, image synthesis, personalized recommendations, or autonomous decision-making.\nFine-tune and adapt pre-trained models (e.g., GPT, DALL-E, Stable Diffusion) for specific tasks.\nDevelop foundational components of multi-agent systems where agents use AI to collaborate or solve problems.\nMulti-Agent Integration: Build and test individual AI agents and integrate them into a multi-agent framework using libraries such as Ray, OpenAI API, or custom architectures.\nDesign communication protocols between agents and their environment.\nEnd-to-End Deployment: Contribute to the deployment of at least one Generative AI model or a multi-agent application in production, ensuring scalability and performance.\nCollaboration and Research: Work closely with cross-functional teams to integrate Generative AI models into multi-agent solutions.\nStay updated with advancements in Generative AI and multi-agent systems and experiment with cutting-edge technologies.\nDocumentation: Maintain detailed documentation of experiments, models, and processes for reproducibility and team collaboration.\nEssential Skills:\n5+ years of experience\nProficiency with PyTorch, TensorFlow, or similar frameworks.\nExperience with LLM fine-tuning, prompt engineering, and model optimization.\nFamiliarity with multi-agent frameworks like Ray, LangChain, or custom architectures.\nWorking knowledge of distributed systems and cloud platforms (AWS, GCP, Azure).\nEducation Qualifications:Bachelor's degree in Engineering ( Computer Science/Information Technology).\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 30/05/2025",
        "skills": [
            "Ray",
            "model optimization",
            "cloud platforms",
            "LLM fine-tuning",
            "LangChain",
            "prompt engineering",
            "multi-agent frameworks",
            "Tensorflow",
            "AWS",
            "Distributed Systems",
            "Pytorch",
            "Azure",
            "Gcp"
        ]
    },
    {
        "job_title": "Senior Data Scientist-Gen AI",
        "company_name": "YASH Technologies",
        "experience": "Fresher",
        "salary": null,
        "location": "Indore, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Overview:\nWe are looking for a AI/ML Developer to join our team of researchers, data scientists, and developers. You will work on cutting-edge AI solutions across industries such as commerce, agriculture, insurance, financial markets, and procurement. Your role involves developing and optimizing machine learning and generative AI models to solve real-world challenges.\nKey Responsibilities:\nDevelop and optimize ML, NLP, Deep Learning, and Generative AI models.\nResearch and implement state-of-the-art algorithms for supervised and unsupervised learning.\nWork with large-scale datasets in distributed environments.\nUnderstand business processes to select and apply the best ML approaches.\nEnsure scalability and performance of ML solutions.\nCollaborate with cross-functional teams, including product owners, designers, and developers.\nSolve complex data integration and deployment challenges.\nCommunicate results effectively using data visualization.\nWork in global teams across different time zones.\nRequired Skills & Experience:\nStrong experience in Machine Learning, Deep Learning, NLP, and Generative AI.\nHands-on expertise in frameworks like TensorFlow, PyTorch, or Hugging Face Transformers.\nExperience with LLMs (Large Language Models), model fine-tuning, and prompt engineering.\nProficiency in Python, R, or Scala for ML development.\nKnowledge of cloud-based ML platforms (AWS, Azure, GCP).\nExperience with big data processing (Spark, Hadoop, or Dask).\nAbility to scale ML models from prototypes to production.\nStrong analytical and problem-solving skills.\nIf you're passionate about pushing the boundaries of ML and GenAI, we'd love to hear from you!",
        "skills": [
            "Generative AI",
            "LLMs",
            "Hugging Face Transformers",
            "R",
            "Dask",
            "Machine Learning",
            "Hadoop",
            "Scala",
            "Deep Learning",
            "Tensorflow",
            "Nlp",
            "Gcp",
            "Pytorch",
            "Spark",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist - Gen AI",
        "company_name": "Junglee Games",
        "experience": "1-6 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Junglee Games\nWith over 140 million users, Junglee Games is a leader in the online skill gaming space. Founded in San Francisco in 2012 and part of the Flutter Entertainment Group, we are revolutionizing how people play games. Our notable games include Howzat, Junglee Rummy, and Junglee Poker.\nOur team comprises over 1000 talented individuals who have worked on internationally acclaimed AAA titles like Transformers and Star Wars: The Old Republic and contributed to Hollywood hits such as Avatar. Junglee's mission is to build entertainment for millions of people around the world and connect them through games.\nJunglee Games is not just a gaming company but a blend of innovation, data science, cutting-edge tech, and, most importantly, a values-driven culture that is creating the next set of conscious leaders.\nJob overview\nAs our Data Scientist I/II/III or Artificial Learning Engineer, you will help us shape the future of real money gaming. You will develop and deploy GenAI models to personalize gameplay, generate game content, power conversational agents, and detect anomalies driving deeper engagement and operational efficiency.\nJob Location\nGurgaon\nKey Responsibilities\nDesign and implement Generative AI solutions (e.g., LLMs, diffusion models, transformers) for real-world gaming applications, including Fraud Detection, Recommender Systems, Responsible Gaming, Conversational AI (chatbots, virtual assistants) etc\nCollaborate cross-functionally with product, design, and engineering teams to define use cases and integrate GenAI models into live products.\nFine-tune and optimize large pre-trained models (e.g., GPT, LLaMA, Stable Diffusion) for domain-specific tasks.\nBuild robust pipelines for model training, inference, and continuous learning.\nImplement safeguards and ethical constraints to ensure GenAI systems are fair, explainable, and safe.\nEvaluate model performance using both quantitative metrics and real-user impact.\nStay current with the latest in GenAI research and tooling, bringing innovative approaches to production.\nQualifications & skills required\n16 years of experience in machine learning, with at least 12 years focused on Generative AI or LLM applications.\nDeep knowledge of transformer architectures, attention mechanisms, and diffusion models.\nProficiency in Python and GenAI/ML frameworks (e.g., Hugging Face Transformers, LangChain, OpenAI, TensorFlow, PyTorch).\nExperience with fine-tuning LLMs and integrating APIs or open-source models into scalable systems.\nSolid understanding of NLP, prompt engineering, and model evaluation techniques.\nExperience deploying ML models in production environments (REST APIs, microservices, etc.).\nFamiliarity with MLOps tools for model versioning, monitoring, and CI/CD.\nExperience with cloud platforms (AWS/GCP) and distributed training/inference.\nBachelor's or Master's degree in Computer Science, AI, or a related field\nBe a part of Junglee Games to:\nValue Customers & Data - Prioritize customers, use data-driven decisions, master KPIs, and leverage ideation and A/B testing to drive impactful outcomes.\nInspire Extreme Ownership - We embrace ownership, collaborate effectively, and take pride in every detail to ensure every game becomes a smashing success.\nLead with Love - We reject micromanagement and fear, fostering open dialogue, mutual growth, and a fearless yet responsible work ethic.\nEmbrace change - Change drives progress and our strength lies in adapting swiftly and recognizing when to evolve to stay ahead.\nPlay the Big Game - We think big, challenge norms, and innovate boldly, driving impactful results through fresh ideas and inventive problem-solving.\nAvail a comprehensive benefits package that includes paid gift coupons, fitness plans, gadget allowances, fuel costs, family healthcare, and much more.\nKnow more about us\nExplore the world of Junglee Games through our website, www.jungleegames.com.\nGet a glimpse of what Life at Junglee Games looks like on LinkedIn.\nHere is a quick snippet of the Junglee Games Offsite'24\nLiked what you saw so far Be A Junglee",
        "skills": [
            "LangChain",
            "Generative AI",
            "Transformers",
            "LLMs",
            "Hugging Face Transformers",
            "OpenAI",
            "diffusion models",
            "Tensorflow",
            "Nlp",
            "Gcp",
            "Pytorch",
            "MLops",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Developer - Data Scientist",
        "company_name": "Medline India",
        "experience": "1-3 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist (Entry to Mid-Level)\n\nLocation: Pune, India\n\nAbout the Role:\n\nWe are looking for a motivated Data Scientist to join our dynamic AI team in Pune. This role will focus on proving concepts through rapid prototypes and building Minimum Viable Products (MVPs) in context of machine learning (ML) and Generative AI (GenAI) solutions, including GenAI-based chatbots. The ideal candidate will have a strong foundation in data science, with hands-on experience in ML models, familiarity with GenAI based solutions, and a keen interest in emerging AI technologies.\n\nKey Responsibilities:\n\nAI & ML Model Development:\n\nExecute PoCs and MVPs for AI and ML projects, focusing on practical implementation.\nDevelop and refine ML models, including supervised and unsupervised learning algorithms.\nBuild and deploy GenAI solutions, such as Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) systems, and chatbots.\nCollaborate with AI Engineers to integrate models into enterprise applications.\n\nData Handling & Preprocessing:\n\nCollect, clean, and preprocess data to ensure quality inputs for model training.\nConduct exploratory data analysis (EDA) to uncover insights and inform model development.\nWork with Data Engineers to ensure smooth data pipelines and infrastructure.\n\nModel Evaluation & Optimization:\n\nEvaluate model performance using appropriate metrics and fine-tune algorithms for optimal results.\nMonitor GenAI outputs for issues such as bias, hallucinations, and accuracy.\nImplement feedback loops for continuous model improvement.\n\nCollaboration & Documentation:\n\nWork closely with the AI Lead and cross-functional teams to align projects with business objectives.\nDocument model development processes, code, and findings for transparency and reproducibility.\nContribute to the development of best practices and playbooks for AI and ML solutions.\n\nContinuous Learning & Development:\n\nStay updated with the latest trends and advancements in AI, ML, and GenAI technologies.\nParticipate in team knowledge-sharing sessions and training opportunities.\n\nQualifications & Skills:\n\nEducation:\n\nBachelor's or Master's degree in Computer Science, Data Science, AI, or a related field.\n\nExperience:\n\n1-3 years of experience in data science, machine learning, or AI-related roles.\nHands-on experience with ML model development and deployment.\nExposure to GenAI technologies, including LLMs and chatbot development, is a plus.\n\nTechnical Skills:\n\nProficient in Python and SQL for data manipulation and model development.\nFamiliarity with ML frameworks and libraries such as scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Matplotlib, Seaborn) and EDA techniques.\nBasic understanding of MLOps tools like MLflow, Airflow, or similar is an advantage.\nExposure to cloud platforms (Azure, AWS, GCP) for AI/ML model deployment is a plus.\n\nSoft Skills:\n\nStrong analytical and problem-solving skills.\nEagerness to learn and adapt in a fast-paced, evolving environment.\nDetail-oriented with a focus on delivering high-quality work.",
        "skills": [
            "Airflow",
            "scikit-learn",
            "MLflow",
            "Matplotlib",
            "Sql",
            "Tensorflow",
            "Pytorch",
            "Gcp",
            "Seaborn",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Junior Data Scientist",
        "company_name": "i2o Retail",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Description\ni2o Retail's platform empowers e-commerce decisions and supports growth on Amazon by providing insights to optimize sales. Brands can control unauthorized resellers, monitor price changes, and enhance key metrics to increase sales. i2o Retail offers solutions to help brands succeed in the competitive e-commerce landscape.\nRole Description\nThis is a full-time remote role for a Junior Data Scientist at i2o Retail. The Junior Data Scientist will be responsible for performing data analysis, applying statistical methods, and utilizing data analytics to support business decisions along with latest technologies using Gen AI and Agentic Frameworks. The role involves working with data to extract valuable insights and contribute to the development of data-driven strategies while working with business stakeholders.\nQualifications\nStrong Data Science and Data Analysis skills\nStatistics knowledge\nData Visualization expertise\nExperience in Data Analytics\nHigh proficiency in programming languages - Python\nKnowledge or projects around LLMs, Agentic frameworks\nAny projects on ML with human in the loop, would add value\nStrong problem-solving abilities\nAbility to work independently and remotely\nBachelor's degree in Data Science, Statistics, Computer Science, or related field\nStrong academic background",
        "skills": [
            "LLMs",
            "Data Analysis",
            "Agentic frameworks",
            "Statistics",
            "Data Science",
            "Data Visualization",
            "Data Analytics",
            "Python"
        ]
    },
    {
        "job_title": "Junior Data Scientist",
        "company_name": "LSEG (London Stock Exchange Group)",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "We are looking for a skilled Data Scientist to join our dynamic Analytics Centre of Excellence team. This role combines Data Science, Python development, and business analysis to deliver actionable insights and solutions that drive strategic decisions. Knowledge and experience will be put into practice to Quantitative Engineering, financial instrument pricing, and analytics to shape the direction of our technology and tools, impacting both internal and external customers.\n\nThis position offers a unique opportunity to work on Data Science projects, Python applications, and requirement analysis, based on your strengths and interests. Whether developing ML models, writing Python code, or collaborating with business partners, playing a key role in the successful delivery of projects.\n\nKey responsibilities:\n\nData Science: Own development of sophisticated data science and machine learning models, using large datasets to drive business decisions and data-driven strategies across the organization.\nPython Programming: Design, develop, and maintain Python-based software solutions that meet business needs, collaborating with team members to ensure seamless integration and optimization.\nRequirement Analysis: Work closely with cross-functional teams to validate requirements, analyze both qualitative and quantitative data, and contribute to the identification of short- and long-term solutions.\nProcess Improvement: Contribute to process improvements, problem resolution workflows, and preventive measures, while actively participating in project planning and technical solution design.\nProject Transparency: Provide regular updates on project progress and challenges, ensuring transparency with leadership and adhering to disciplined development processes.\nIndependent Work: Work autonomously with minimal guidance, demonstrating ownership of projects and accountability for your expertise and handling ambitious projects.\nFlexible Shift & On-Call Coverage: Available for a 12-9pm shift and on-call support as needed.\n\nSkills & experience required:\n\nStrong Python Skills: Proficient in Python for software development, maintenance, and integration.\nData Science Expertise: Extensive experience in machine learning, AI, and advanced analytics to design and implement data-driven solutions.\nRequirement Analysis: Strong ability to validate business requirements, ensuring alignment with technical solutions and providing realistic sizing.\nCommunication: Excellent written and verbal communication, capable of explaining complex ideas to both technical and non-technical audiences.\nFinancial Analysis: Experience with financial markets and quantitative finance.\nCollaboration: Proven ability to work in a multi-disciplinary team, with experience in mentoring junior developers.\nQuantitative Finance Knowledge: Good understanding of financial mathematics, including pricing financial instruments and modeling financial markets.\n\nDesirable skills:\n\nProject Management: Experience managing project updates, particularly with Agile methodology.\nRefinitiv Tools: Familiarity with Refinitiv Eikon/Elektron/Workspace products.\nESG Data Modeling: Experience in financial modeling, particularly with ESG (Environmental, Social, and Governance) data.\n\nAbout Lseg\n\nLSEG (London Stock Exchange Group) is a global leader in financial markets infrastructure and data. We enable businesses and economies to fund innovation, manage risk, and create jobs. With operations in 70 countries, LSEG employs 25,000 people globally, fostering a culture of growth, opportunity, and innovation.\n\nOur commitment to diversity and inclusion drives our success. We provide a supportive environment where all employees have the opportunity to grow, develop, and fulfill their potential.\n\nCommitment to inclusion\n\nWe embrace diversity and actively seek individuals with unique backgrounds and perspectives. We break down barriers and encourage teamwork, enabling innovation and the rapid development of solutions. LSEG is committed to providing reasonable accommodations for individuals with disabilities during the application process.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",
        "skills": [
            "Collaboration",
            "Financial Analysis",
            "Data Science Expertise",
            "Quantitative Finance Knowledge",
            "Requirement Analysis",
            "Python Programming"
        ]
    },
    {
        "job_title": "Lead Data Scientist",
        "company_name": "Wolters Kluwer",
        "experience": "Fresher",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Role:\n\nAs a Lead Data Scientist, you will independently execute specialized data tasks, focusing on model development, data interpretation, and driving forward the data science agenda. You will leverage advanced algorithms and statistical techniques to unearth insights and guide business decisions. This position is suited for self-driven professionals who excel at transforming raw data into strategic assets and are ready to contribute significantly to data science projects.\n\nResponsibilities:\n\nLead the development and deployment of advanced machine learning models.\nPerform in-depth data analysis to identify actionable insights.\nDevelop and maintain complex data processing pipelines.\nCollaborate with stakeholders to align data science initiatives with business goals.\nDrive feature engineering and selection processes.\nDesign and implement scalable data solutions for analytics.\nConduct exploratory data analysis to explore new opportunities.\nEnsure the robustness and reliability of data science projects.\nProvide guidance on best practices for data science workflows.\nStay ahead of trends and continuously improve technical skills and knowledge.\n\nSkills:\n\nAdvanced Statistical Methods: Proficient in applying complex statistical techniques.\nMachine Learning Expertise: In-depth knowledge of machine learning algorithms and their practical applications.\nPython/R/SAS: Advanced skills in Python, with knowledge in R or SAS for data analysis.\nBig Data Technologies: Familiarity with big data tools like Spark and Hadoop.\nData Engineering: Proficiency in building and managing data pipelines.\nPredictive Modeling: Expertise in developing and fine-tuning predictive models.\nCommunication: Excellent ability to translate data insights into business strategies.\nProject Management: Strong project management skills to oversee data initiatives.\n\nApplicants may be required to appear onsite at a Wolters Kluwer office as part of the recruitment process.",
        "skills": [
            "R",
            "Advanced Statistical Methods",
            "data engineering",
            "Predictive Modeling",
            "Machine Learning",
            "SAS",
            "Big Data Technologies",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Ciklum",
        "experience": "Fresher",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Ciklum is looking for a Senior Data Scientist to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\nAbout the role:\nAs a Senior Data Scientist, become a part of a cross-functional development team engineering experiences of tomorrow.\nResponsibilities:\nDevelopment of prototype solutions, mathematical models, algorithms, machine learning techniques, and robust analytics to support analytic insights and visualization of complex data sets\nWork on exploratory data analysis so you can navigate a dataset and come out with broad conclusions based on initial appraisals\nProvide optimization recommendations that drive KPIs established by product, marketing, operations, PR teams, and others\nInteracts with engineering teams and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability.\nWork directly with business analysts and data engineers to understand and support their use cases\nWork with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions\nDrive innovation by exploring new experimentation methods and statistical techniques that could sharpen or speed up our product decision-making processes\nCross-train other team members on technologies being developed, while also continuously learning new technologies from other team members\nContribute to the Unit activities and community building, participate in conferences, and provide excellence in exercise and best practices\nSupport marketing & sales activities, customer meetings and digital services through direct support for sales opportunities & providing thought leadership & content creation for the service\nRequirements:\nWe know that sometimes, you can't tick every box. We would still love to hear from you if you think you're a good fit!\nGeneral technical requirements\nBSc, MSc, or PhD in Mathematics, Statistics, Computer Science, Engineering, Operations Research, Econometrics, or related fields\nStrong knowledge of Probability Theory, Statistics, and a deep understanding of the Mathematics behind Machine Learning\nProficiency with CRISP-ML(Q) or TDSP methodologies for addressing commercial problems through data science solutions\nHands-on experience with various machine learning techniques, including but not limited to:\nRegression\nClassification\nClustering\nDimensionality reduction\nProficiency in Python for developing machine learning models and conducting statistical analyses\nStrong understanding of data visualization tools and techniques (e.g., Python libraries such as Matplotlib, Seaborn, Plotly, etc.) and the ability to present data effectively\nSpecific technical requirements:\nProficiency in SQL for data processing, data manipulation, sampling, and reporting\nExperience working with imbalanced datasets and applying appropriate techniques\nExperience with time series data, including preprocessing, feature engineering, and forecasting\nExperience with outlier detection and anomaly detection\nExperience working with various data types: text, image, and video data\nFamiliarity with AI/ML cloud implementations (AWS, Azure, GCP) and cloud-based AI/ML services (e.g., Amazon SageMaker, Azure ML)\nDomain experience:\nExperience with analyzing medical signals and images\nExpertise in building predictive models for patient outcomes, disease progression, readmissions, and population health risks\nExperience in extracting insights from clinical notes, medical literature, and patient-reported data using NLP and text mining techniques\nFamiliarity with survival or time-to-event analysis\nExpertise in designing and analyzing data from clinical trials or research studies\nExperience in identifying causal relationships between treatments and outcomes, such as propensity score matching or instrumental variable techniques\nUnderstanding of healthcare regulations and standards like HIPAA, GDPR (for healthcare data), and FDA regulations for medical devices and AI in healthcare\nExpertise in handling sensitive healthcare data in a secure, compliant way, understanding the complexities of patient consent, de-identification, and data sharing\nFamiliarity with decentralized data models such as federated learning to build models without transferring patient data across institutions\nKnowledge of interoperability standards such as HL7, SNOMED, FHIR, or DICOM\nAbility to work with clinicians, researchers, health administrators, and policy makers to understand problems and translate data into actionable healthcare insights\nGood to have skills:\nExperience with MLOps, including integration of machine learning pipelines into production environments, Docker, and containerization/orchestration (e.g., Kubernetes)\nExperience in deep learning development using TensorFlow or PyTorch libraries\nExperience with Large Language Models (LLMs) and Generative AI applications\nAdvanced SQL proficiency, with experience in MS SQL Server or PostgreSQL\nFamiliarity with platforms like Databricks and Snowflake for data engineering and analytics\nExperience working with Big Data technologies (e.g., Hadoop, Apache Spark)\nFamiliarity with NoSQL databases (e.g., columnar or graph databases like Cassandra, Neo4j)\nBusiness-related requirements:\nProven experience in developing data science solutions that drive measurable business impact, with a strong track record of end-to-end project execution\nAbility to effectively translate business problems into data science problems and create solutions from scratch using machine learning and statistical methods\nExcellent project management and time management skills, with the ability to manage complex, detailed work and effectively communicate progress and results to stakeholders at all levels\nDesirable:\nResearch experience with peer-reviewed publications\nRecognized achievements in data science competitions, such as Kaggle\nCertifications in cloud-based machine learning services (AWS, Azure, GCP)\nWhat`s in it for you\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune\nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\nAbout us:\nIndia is a strategic growth market for Ciklum.\nBe a part of a big story created right now. Let's grow our delivery center in India together! Boost your skills and knowledge: create and innovate with like-minded professionals all of that within a global company with a local spirit and start-up soul.\nSupported by Recognize Partners and expanding globally, we will engineer the experiences of tomorrow!\nBe bold, not bored!\nInterested already We would love to get to know you! Submit your application. We can't wait to see you at Ciklum.",
        "skills": [
            "CRISP-ML Q",
            "TDSP",
            "Regression",
            "Dimensionality Reduction",
            "Classification",
            "Machine Learning",
            "Clustering",
            "Sql",
            "Deep Learning",
            "Nlp",
            "MLops",
            "Gcp",
            "Data Visualization",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Blend",
        "experience": "Fresher",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Title: MMO Data Scientist Financial Services\nLocation: Hybrid in Hyderabad, India\nCompany: Blend360\n65 LPA\nAbout the Role: We are seeking a skilled Data Scientist with expertise in media mix modeling, Streamlit, and Python to join our team. The successful candidate will play a pivotal role in developing and implementing our Media Mix Optimization (MMO) solution, leveraging advanced analytics to optimize media spend and improve marketing effectiveness for our clients.\nResponsibilities:\nDevelop and maintain media mix models to analyze the effectiveness of various marketing channels.\nCollaborate with cross-functional teams to integrate data from multiple sources.\nUse Streamlit to build and deploy interactive data applications and dashboards.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nPerform data preprocessing, feature engineering, and model validation.\nImplement scenario planning and optimization algorithms to provide actionable media spend recommendations.\nCommunicate findings and insights to stakeholders through visualizations and reports.\nStay updated with the latest trends and best practices in data science and media mix modeling.\nRequirements:\nBachelor's or Master's degree in Data Science, Statistics, Computer Science, or a related field.\nProven experience in media mix modeling and marketing analytics.\nProficiency in Python and relevant libraries (e.g., pandas, numpy, scikit-learn).\nExperience with Streamlit for building interactive web applications.\nStrong analytical and problem-solving skills.\nExcellent communication and presentation skills.\nAbility to work collaboratively in a team environment.\nPreferred:\nExperience with machine learning and statistical modeling.\nFamiliarity with other data visualization tools and frameworks.",
        "skills": [
            "scikit-learn",
            "Streamlit",
            "Pandas",
            "Numpy",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "e-Hireo",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "JOB DESCRIPTION\nExperience : 2 - 4 Yrs / 4 - 6 Yrs\nLocation : Bengaluru\nDesignation : Data Scientist\nRoles and Responsibilities :\nData-Driven Analysis and Model Building :\nLead andoversee the creation, deployment, and continuous improvement of datamodels and analytical systems that optimize business operations.\nUtilize advanced machine learning andstatistical models toaddress complex business problems and improve decision-making processes.\nEnsure that all models and analyses are reproducible, scalable, and sustainable for long-term use.\nAutomation of Analytical Processes :\nDesign andbuild automation tools to streamline datapreparation, model training, testing, and deployment, improving overall efficiency.\nCollaborate withsoftware engineers andother technical teams to implement automation solutions that can be leveraged across the organization.\nWork towards reducing manual interventions and improving datapipeline processes byintegrating automation at various stages of the analysis workflow.\nCross-Team Collaboration for Actionable Insights:\nCollaborate withdifferent teams (e.g., business units, product teams, operations) tounderstand their challenges, identify opportunities, and deliver data-driven solutions that offer actionable insights.\nProvide guidance and mentorship tojunior team members, enabling them toextract valuable insights from data and build predictive models.\nCommunicate complex technical results andmodel findings ina clear andconcise manner to non-technical stakeholders, ensuring alignment between technical and business goals.\nData Sourcing and Integration :\nWork with both internal andexternal data sources to gather thenecessary data foranalysis, ensuring its quality, reliability, and relevance to business needs.\nEvaluate andintegrate new data sources that canenhance existing models or contribute tonew initiatives.\nCreate efficient processes for managing and cleaning large volumes of data from various sources to make it usable for analysis and model development.\nDeveloping Reliable Solutions :\nDevelop robust, scalable data solutions that support ongoing business analytics efforts, ensuring high availability and accuracy.\nCreate predictive models, statistical analyses, and simulations thathelp businesses forecast trends, identify patterns, and make informed decisions.\nTake ownership of the end-to-end lifecycle of datasolutions, from initial conception and experimentation to implementation and monitoring of performance.\nRequirements :\n5+ years of experience in analytics filed having Bachelors or Masters in a quantitative field\nAbility toprogram in python/SQL and proven problem solving and debugging skills, familiar with database technologies and tools\nStatistical knowledge of standard modelling concepts and techniques : Linear Regression, Classification models(Logistic, Random forest, clustering, Boosting methods), Neural Networks and Ensembles methods\nTechnical Skill :\nPython\nSQL\nAWS",
        "skills": [
            "Boosting methods",
            "Classification models",
            "Linear Regression",
            "Neural Networks",
            "Clustering",
            "Random Forest",
            "Sql",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist (co-founder) (equity only - no compensation)",
        "company_name": "PD Consulting",
        "experience": "5-7 Years",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Founding Data Scientist Equity-Only (2025 hrs/week)\n\nAbout NPPD CARE\n\nAt NPPD CARE, we are revolutionizing preventive healthcare by building a 360 mobile-first wellness ecosystem that blends emotional, behavioral, and physical health using AI, wearables, and daily interactions. Join us as a founding team member and help create the future of holistic wellness for millions!\n\nYour Mission\n\nAs our Founding Data Scientist, you will be the brain behind our AI and intelligence stack, driving innovation in emotional and behavioral health analytics. You'll build cutting-edge models to personalize wellness journeys, empower B2B clients with actionable insights, and shape the core technology that powers our AI wellness agents.\n\nKey Responsibilities\n\nDevelop & refine AI/ML models that fuse multi-modal data streams:\nActive Data: User inputs via daily check-ins (mood, energy, rituals)\nPassive Data: Phone sensors (sleep, motion, screen time)\nWearable Data: Heart rate, steps, stress levels, sleep quality\nBuild and optimize personalized habit formation engines and contextual nudging systems that adapt to users emotional and physical states\nCollaborate with product teams to enhance NLP models powering our voice/video AI wellness assistants for natural and empathetic conversations\nDesign and implement real-time analytics pipelines for B2B dashboards (hospitals, gyms, corporates) to track patient engagement, burnout risk, and wellness progress\nCreate feedback loops that continuously improve AI recommendations based on user behavior and outcomes\nArchitect scalable, maintainable ML infrastructure using cloud platforms (AWS/GCP) and open-source tools\nWork closely with product officers and developers to integrate AI seamlessly into the mobile app and ecosystem\nWhat We're Looking For\n\nProven experience (5+ years preferred) in data science, machine learning, or AI, especially with time-series and behavioral data\nStrong proficiency in Python, TensorFlow, PyTorch, or similar ML frameworks\nHands-on experience with wearable sensor data or health-related datasets is a big plus\nFamiliarity with NLP techniques and conversational AI models\nAbility to design and deploy end-to-end ML pipelines on cloud infrastructure (AWS, GCP, or Azure)\nPassion for health, wellness, and preventive care with a mission-driven mindset\nExcellent problem-solving skills and ability to work independently in a fast-paced startup environment\nStrong communication skills to collaborate across teams and explain complex AI concepts clearly\n\nWhat You'll Get\n\nFounding team equity stake - be a true co-creator of NPPD CARE's future success\nOwnership of the AI/ML technology stack and direct impact on product direction\nFlexible 2025 hours/week commitment, perfect for balancing other projects or studies\nOpportunity to work on a mission-first, India-focused wellness startup with global ambitions\nCollaborative culture with clear ownership, transparency, and a passionate team\nPotential for leadership role and compensation post-funding\n\nHiring Process\n\nInitial conversation to understand your experience and passion\nTechnical discussion and problem-solving session\nCultural fit and vision alignment interview\nOffer and onboarding as a founding team member\n\nSkills: data science,data,tensorflow,wearable data,machine learning,nlp,python,pytorch,cloud infrastructure,real-time analytics,ai,aiml,models",
        "skills": [
            "Real-time Analytics",
            "Wearable Data",
            "Tensorflow",
            "Machine Learning",
            "Nlp",
            "Pytorch",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist [Immediate Joiner- 15 days]",
        "company_name": "COMnet Solutions (Global)",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position: Data Scientist\nExperience Required: Minimum 8-10 years\nLocation: Mumbai / Gurgaon / Bangalore\nEmployment Type: Full-Time\nRole Overview\nWe are looking for a skilled and innovative Data Scientist with a minimum of 8 years of professional experience. The ideal candidate will have a strong analytical mindset, advanced technical expertise, and a proven ability to extract meaningful insights from large and complex datasets. You will play a critical role in driving data-driven decisions, building predictive models, and enabling scalable data solutions to support our business objectives.\nKey Responsibilities\nAnalyze complex datasets to uncover actionable insights, trends, and opportunities for optimization.\nDesign and implement analytical frameworks to solve business challenges.\nDevelop, train, and deploy predictive and prescriptive models using machine learning and statistical techniques.\nContinuously improve model performance and scalability.\nCollaborate with data engineering teams to ensure the availability and quality of data pipelines.\nImplement data preprocessing, transformation, and feature engineering processes.\nWork closely with cross-functional teams, including product managers, engineers, and business stakeholders, to align data initiatives with organizational goals.\nTranslate business problems into data science tasks and communicate results effectively to non-technical stakeholders.\nLeverage advanced data science tools and frameworks for statistical analysis, machine learning, and data visualization.\nStay updated on emerging technologies and incorporate best practices in data science workflows.\nDefine and track key metrics to measure the success of models and business impact.\nConduct A/B testing and experiments to validate hypotheses and model effectiveness.\nQualifications\nEducation:\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, Data Science, or a related field. A PhD is a plus.\nExperience:\nAt least 8-10 years of hands-on experience in data science, machine learning, or a related role.\nProven experience in deploying models in production environments.\nTechnical Skills:\nProficiency in programming languages such as Python, R, or Java.\nStrong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch, Scikit-learn).\nExperience with data visualization tools (e.g., Tableau, Power BI, Matplotlib, Seaborn).\nFamiliarity with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, Azure, GCP).\nExpertise in working with SQL and NoSQL databases.\nPreferred Qualifications:\nExperience in natural language processing (NLP), computer vision, or deep learning.\nKnowledge of CI/CD pipelines and MLOps best practices.\nCertification in data science, machine learning, or cloud computing (e.g., AWS Certified Data Analytics, Microsoft Azure Data Scientist Associate).\nSoft Skills:\nStrong problem-solving and critical-thinking abilities.\nExcellent communication and storytelling skills to convey insights effectively.\nAbility to work collaboratively in a fast-paced, cross-functional environment.",
        "skills": [
            "Scikit-learn",
            "R",
            "Sql",
            "Java",
            "Tensorflow",
            "Hadoop",
            "Tableau",
            "Power Bi",
            "Seaborn",
            "AWS",
            "Pytorch",
            "Nosql",
            "Python",
            "Azure",
            "Gcp",
            "Matplotlib",
            "Spark"
        ]
    },
    {
        "job_title": "Principal data Scientist",
        "company_name": "Elevation Capital",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Note: This role is with one of our portfolio companies. (Series A )\nAs a AI/Principal Data Scientist specializing in LLMs, ML, and SQL, you will be responsible for deeply understanding business problem statements, determining the required data, and building analytical solutions. You will use Large Language Models to generate features from textual data and ensure the correct machine learning models are applied for optimal results. In addition to leveraging SQL and basic coding for data manipulation, you will also run models independently to provide actionable insights that address key business challenges.\nKey Responsibilities:\nCollaborate with stakeholders to understand business problems and translate them into data-driven problem statements.\nLeverage Large Language Models (LLMs) to generate features from unstructured data (e.g., text) to enhance machine learning models.\nWrite and optimize SQL queries to manipulate, clean, and analyze structured data from various databases.\nUse basic coding skills (e.g., Python) to handle data preprocessing, transformation, and running ML models independently.\nDevelop, train, and evaluate machine learning models, ensuring the correct model is selected for each problem.\nPerform model validation, fine-tuning, and feature engineering to optimize performance\nCommunicate complex analytical findings in clear, concise ways to both technical and non-technical stakeholders.\nStay updated with the latest developments in LLMs, ML, and AI technologies to incorporate new techniques into solutions.\nQualifications:\nPh.D/ Master's degree in Computer Science, Artificial Intelligence, or a related field preferred.\n5+ years of proven experience in data analysis, SQL query writing, and feature engineering for both structured and unstructured data.\nStrong proficiency in using machine learning algorithms, including model selection, training, and evaluation.\nStrong coding skills in Python for data preprocessing, cleaning, and running machine learning models.\nExcellent communication skills for presenting insights and findings to both technical and non-technical audiences.\nDemonstrated ability to thrive in a fast-paced startup environment and with a proven track record of leading AI initiatives from concept to execution.\nAbility to develop and implement a strategic vision for AI within the company, aligning with business objectives.\nExcellent communication and interpersonal skills, capable of articulating complex AI concepts to non-technical stakeholders.",
        "skills": [
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "IT_Industries4.0_Data Scientist_CoE",
        "company_name": "Welspun World",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Welspun\n\nWelspun World is one of India's fastest growing global conglomerates with businesses in Home Textiles, Flooring Solutions, Advanced Textiles, DI Pipes, Pig Iron, TMT bars, Stainless Steel, Alloy, Line Pipes, Infrastructure & Warehousing.\n\nAt Welspun, we strongly believe in our purpose to delight customers through innovation and technology, achieve inclusive & sustainable growth to remain eminent in all our businesses. From Homes to Highways, Hi-tech to Heavy metals, We lead tomorrow together to create a smarter & more sustainable world.\n\nJob Purpose/ Summary\n\nWelspun Transformation Private Limited is seeking a highly skilled and results-driven Senior Data Scientist with expertise in Python, PySpark, and Industry 4.0 technologies. The ideal candidate will have a strong background in designing and implementing advanced analytics solutions to optimize manufacturing processes, improve operational efficiency, and drive predictive maintenance initiatives. The role involves working closely with cross-functional teams to develop data-driven solutions that enhance decision-making across smart factories and industrial ecosystems.\n\nJob Title\n\nIT_Industries4.0_Data Scientist_CoE\n\nJob Description\nAs a Senior Manager in the IT Industries 4.0 Data Scientist Center of Excellence, you will be responsible for leading a team of data scientists to develop and implement advanced analytics models and solutions to drive business decisions and outcomes. You will be expected to leverage your expertise in Machine Learning, Predictive Maintenance, ETL Pipelines, Python, MQTT, OPC UA, and SCADA to deliver innovative solutions.\n\nPrincipal Accountabilities\nLead and manage a team of data scientists to develop and implement advanced analytics models and solutions.\n\nLeverage expertise in Machine Learning, Predictive Maintenance, ETL Pipelines, Python, MQTT, OPC UA, and SCADA to deliver innovative solutions.\n\nCollaborate with cross-functional teams to understand business needs and identify opportunities for leveraging company data to drive business solutions.\n\nDevelop custom data models and algorithms to apply to data sets.\n\nUse predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting, and other business outcomes.\n\nDevelop company A/B testing framework and test model quality.\n\nCoordinate with different functional teams to implement models and monitor outcomes.\n\nDevelop processes and tools to monitor and analyze model performance and data accuracy.\n\nMaintain a strong understanding of industry trends and best practices, and apply this knowledge to your work.\n\nDemonstrate strong business and commercial acumen, and maintain a global mindset in all work.\n\nEmphasize critical thinking, product and service management, IT application, and information security in all work.\n\nThe ideal candidate will have a strong ability to translate complex data into actionable strategies and techniques to drive business performance. They will also have a proven track record of leading and developing high-performing teams.\n\nKey Interactions\n\nTop Management,Mid Management,Junior Management,Cross-Functional Collaboration ,Client Relations ,Financial Auditing ,Vendor Management\n\nExperience\n\n6\n\nCompetency Name\n\nCompetency Name Proficiency Level Machine LearningExpert Predictive maintenanceExpert ETL pipelinesExpert PythonExpert Business & Commercial acumenExpert Global Mind-setExpert MQTT, OPC UA, and SCADAExpert MQTT, OPC UA, and SCADAExpert",
        "skills": [
            "Predictive Maintenance",
            "ETL Pipelines",
            "SCADA",
            "OPC UA",
            "Mqtt",
            "Machine Learning",
            "Python"
        ]
    },
    {
        "job_title": "Applied Data Scientist - BLR",
        "company_name": "Aarki",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us: Aarki is an AI-driven company specializing in mobile advertising solutions designed to fuel revenue growth. We leverage AI to discover audiences in a privacy-first environment through trillions of contextual bidding signals and proprietary behavioral models. Our comprehensive audience engagement platform includes creative strategy and execution. With over 14 years in the industry, we handle 5 million mobile ad requests per second from over 10B devices, driving performance for both publishers and brands. We are headquartered in San Francisco, CA, with a global presence across the United States, EMEA, and APAC.\nRole Overview\nWe are seeking a motivated and detail-oriented Applied Scientist to join our team. As an Applied Scientist, you will be involved in designing and implementing machine learning models and data pipelines to enhance our programmatic demand-side platform (DSP). You will work closely with senior research scientists and other team members to drive impactful data science projects and contribute to innovative solutions.\nThis is an on-site role, 5 days a week, based in our Bengaluru, India office.\nJoin us in pushing the boundaries of AI and mobile advertising in a collaborative environment that fosters creativity and growth. We offer a competitive salary, comprehensive benefits, and significant opportunities for career advancement.\nRole & Responsibilities\nDevelop and deploy machine learning models at scale to address key challenges in programmatic advertising, such as user response prediction, bid landscape forecasting, and fraud detection.\nConduct exploratory data analysis and apply statistical techniques to extract insights and support decision-making.\nBuild and maintain data pipelines to ensure efficient processing and integration of large-scale data for model training and evaluation.\nWork closely with senior data scientists and cross-functional teams including product, engineering, and business units to integrate models into production systems and applications.\nAssist in the development and implementation of best practices for model deployment, monitoring, and performance assessment.\nStay updated on recent developments in machine learning and data science, and apply relevant techniques to solve complex problems and enhance our platform.\nContribute to the exploration and adoption of new methodologies and technologies to advance our data science capabilities.\nSkills & Experience\nMinimum of five (6) years in data science with practical experience in machine learning, statistical analysis, and data modeling.\nBachelor's degree in Mathematics, Physics, Computer Science, or a related technical field. PLUS if master's degree is a plus in Mathematics, Physics, Computer Science, or a related technical field.\nPreferred experience with additional programming languages (e.g., Rust, C++, Java, Scala) and large-scale data processing systems.\nFamiliarity with RTB, auction theory, and high-throughput low-latency environments is a plus.\nProficiency in machine learning techniques such as regression, classification, and clustering. Experience with Python and libraries like Scikit-Learn, TensorFlow/PyTorch.\nProficiency in Python and SQL. Familiarity with Spark. Experience with libraries such as TensorFlow/PyTorch, Scikit-Learn\nStrong understanding of probability, statistics, and data analysis.\nAbility to work effectively in a team environment, with good communication skills to explain complex concepts to diverse stakeholders.",
        "skills": [
            "data pipelines",
            "Scikit-Learn",
            "Java",
            "Rust",
            "Machine Learning",
            "Scala",
            "Data Modeling",
            "Sql",
            "Tensorflow",
            "Pytorch",
            "Spark",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "Graduate Program - Innovation Impact -R&D Data Scientist Chemistry (d/f/m)",
        "company_name": "Henkel",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Innovation Impact Program Our 18-month Graduate Program starts on October 1st, 2025, at our Henkel Adhesive Technologies Innovation Center in Dsseldorf. You will be part of an exciting development journey, where you will receive a permanent contract from day one, relocation support, and hands-on experience across various innovative technologies. Upon completion, you will have the opportunity to transition into global roles within Henkel's innovation network.\n\nWhat Youll Do\n\nPartner with product developers and application engineers as part of our R&D team in Dsseldorf, Germany to understand technologies, data collection processes, and challenges driving the digital transformation within R&D\nDesign and conduct data analysis projects to identify trends and patterns in formulation data sets, and build and deploy machine learning models to predict formulation performance, stability, and quality\nCollaborate closely with R&D colleagues to translate data insights into new formulations, product development decisions, and application processes - optimizing R&D workflows\nDevelop and maintain data visualization dashboards to effectively communicate insights to technical and business stakeholders\nLeverage your digital skillset to drive awareness and application of data-based decision-making and digital tools across R&D\nSupport the digital transformation of R&D by implementing data-driven workflows for process development and knowledge management\n\nWhat makes you a good fit\n\nYou have recently completed, or are about to complete, a Master's or Ph.D. that combines a strong foundation in Chemistry, Engineering, or Materials Science with Data Science, Physics, Computer Science, or a related analytical fieldfor example, a Bachelor's in a natural science and a digitally focused Master's, or interdisciplinary studies that enable you to understand both lab-based R&D and data-driven approaches\nYou bring first hands-on experience with data analysis and modeling in scientific or engineering contexts, and have a strong grasp of statistical methods and machine learning algorithms (e.g., regression, classification)\nYou are proficient in data tools such as Python or R, DoE software like Minitab or JMP, and data visualization platforms (e.g., Excel, Power BI, Tableau); experience with IoT, finite element analysis (FEA), and computational fluid dynamics (CFD) is a plus\nYou are interested in working in a laboratory environment, motivated to learn about formulations, product development, and R&D workflows, and keen to apply your skills in a hands-on setting\nYou enjoy collaborating with non-digital colleagues, and can confidently explain digital tools and data concepts in a simple, practical way to support their daily work\nYou are interested in growing your career in Henkel's global Innovation network, with relocation as one possible path after the program.\nProficiency in English is essential, as you will work in an international environment\n\nReady to take the next step in your career Visit our program landing page for full details and apply today.\n\nSome perks of joining Henkel\n\nAt Henkel, we come from a broad range of backgrounds, perspectives, and life experiences. We believe the uniqueness of all our employees is the power in us. Become part of the team and bring your uniqueness to us! We welcome all applications across different genders, origins, cultures, religions, sexual orientations, generations and disabilities. If there are any specific accommodations or adjustments you require for the interview process or your on-site appointment, please inform us, and we'll do our best to accommodate your requests.",
        "skills": [
            "R",
            "Minitab",
            "Data Analysis",
            "Statistical methods",
            "Jmp",
            "Power Bi",
            "Machine Learning",
            "Tableau",
            "Excel",
            "Python",
            "Iot"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Truecaller",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Hello, Truecaller is calling you from Bangalore, India! Ready to pick up\nOur goal is to make communication smarter, safer, and more efficient, all while building trust everywhere. We're all about bringing you smart services with a big social impact, keeping you safe from fraud, harassment, scam calls or messages, so you can focus on the conversations that matter.\nTop 20 most downloaded apps globally, and world's #1 caller ID and spam-blocking service for Android and iOS, with extensive AI capabilities, with more than 400 million active users per month.\nFounded in 2009, listed on Nasdaq OMX Stockholm and is categorized as a Large Cap. Our focus on innovation, operational excellence, sustainable growth, and collaboration has resulted in consistently high profitability and strong EBITDA margins.\nA team of 400 people from 35 different nationalities spread across our headquarters in Stockholm and offices in Bangalore, Mumbai, Gurgaon and Tel Aviv with high ambitions.\nWe in the Insights Team areresponsible for SMS Categorization, Fraud detection and other Smart SMS features within the Truecaller app. The OTP & bank notifications, bill & travel reminder alerts are some examples of the Smart SMS features. The team has developed a patented offline text parser that powers all these features and the team is also exploring cutting edge technologies like LLM to enhance the Smart SMS features. The team's mission is to become the World's most loved and trusted SMS app which is aligned with Truecaller's vision to make communication safe and efficient. Smart SMS is used by over 90M users every day.\nAs a Senior Data Scientist, you will be responsible for collecting, organizing, analyzing, and interpreting Truecaller data with a focus on NLP. In this role, you will be pivotal in advancing our work with large language models and on-device models across diverse regions. Your expertise will enhance our natural language processing, machine learning, and predictive analytics capabilities.\nWhat you bring in:\n5+ years of experience in designing, developing, and deploying ML models at scale, with a focus on NLP-driven solutions.\nStrong background in Natural Language Processing (NLP), including text classification, entity recognition, language modeling, and transformer-based architectures.\nExperience in building and deploying models at scale, handling millions of messages efficiently while maintaining performance and accuracy. Also working with on-device models.\nAbility to not only build ML models but also take ownership of deploying them into production, ensuring scalability, reliability, and monitoring.\nKnowledge of anomaly detection, adversarial ML techniques, and risk modeling to identify and prevent spam and fraudulent messaging activities.\nStrong ability to take ML models from research and experimentation to production, working closely with ML engineers and data engineers.\nExpertise in machine learning libraries such as TensorFlow, PyTorch, pandas and Scikit-learn, along with NLP-specific tools like Hugging Face Transformers, spaCy with experience in TFlife, ONNX.\nHands-on experience fine-tuning LLMs including transformer-based architectures (BERT, GPT, LLaMA, T5, etc.) for domain-specific applications, including knowledge distillation, quantization, and model compression for efficiency.\nStrong ability to design, refine, and optimize prompts for LLM-based applications, ensuring high-quality responses and reduced model hallucinations.\nAbility to leverage data driven decision by experimentation, and statistical analysis to improve models and business outcomes.\nStrong understanding of designing, testing, and optimizing prompts for LLM-based applications to improve model accuracy and efficiency.\nProgramming knowledge in at least one language, such as Python or R. Preferably python.\nExpert knowledge of machine learning algorithms.\nFamiliarity with database modelling and data warehousing principles with a working knowledge of SQL\nExperience in building and optimizing large-scale data processing systems using Spark/PySpark\nStrong ability to work cross-functionally with engineers, product managers, and business stakeholders to align ML solutions with company objectives.\nThe impact you will create:\nTake a loosely defined business problem and break it into tractable data problems. For each data problem, clearly articulate the value of solving it, its impact, and its complexity.\nCollaborate with Product and Engineering to scope, design, and implement systems that solve complex business problems ensuring they are delivered on time and within scope.\nDesign, develop, and optimize state-of-the-art NLP models for large-scale message classification, fraud detection, and spam filtering, impacting millions of users globally.\nTake full ownership of ML model development, deployment, and monitoring, ensuring models are production-ready, scalable, and cost-efficient.\nLead data science projects from ideation to deployment, ensuring alignment with business objectives and timelines.\nManage and analyze large datasets collected from multiple countries, ensuring data integrity and consistency.\nStay updated on industry best practices and emerging technologies to drive innovation within the Data Team.\nYou work collaboratively across systems and teams to solve user and business problems. You are expected to help define success and design and build the systems to achieve it.\nTo work with the Product to decide on priorities and set direction, design solutions, and help the team implement them.\nIt would be great if you also have:\nUnderstanding of Conversational AI\nDeploying NLP models in production\nWorking knowledge of GCP components\nLife at Truecaller - Behind the code: https://www.instagram.com/lifeattruecaller/\nSounds like your dream job\nWe will fill the position as soon as we find the right candidate, so please send your application as soon as possible. As part of the recruitment process, we will conduct a background check.\nThis position is based in Bangalore, India.\nWe only accept applications in English.\nWhat we offer:\nA smart, talented and agile team: An international team where 35 nationalities are working together in several locations and time zones with a learning, sharing and fun environment.\nA great compensation package: Competitive salary, 30 days of paid vacation, flexible working hours, private health insurance, parental leave, telephone bill reimbursement, Udemy membership to keep learning and improving and Wellness allowance.\nGreat tech tools: Pick the computer and phone that you fancy the most within our budget ranges.\nOffice life: We strongly believe in the in-person collaboration and follow an office-first approach while offering some flexibility. Enjoy your days with great colleagues with loads of good stuff to learn from, daily lunch and breakfast and a wide range of healthy snacks and beverages. In addition, every now and then check out the playroom for a fun break or join our exciting parties and or team activities such as Lab days, sports meetups etc. There something for everyone!\nCome as you are: Truecaller is diverse, equal and inclusive. We need a wide variety of backgrounds, perspectives, beliefs and experiences in order to keep building our great products. No matter where you are based, which language you speak, your accent, race, religion, color, nationality, gender, sexual orientation, age, marital status, etc. All those things make you who you are, and that's why we would love to meet you.",
        "skills": [
            "Hugging Face Transformers",
            "Scikit-learn",
            "ONNX",
            "R",
            "spaCy",
            "Pyspark",
            "Sql",
            "Tensorflow",
            "Pandas",
            "Pytorch",
            "Spark",
            "Python"
        ]
    },
    {
        "job_title": "Staff Data Scientist I.",
        "company_name": "InMobi Advertising",
        "experience": "7-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nInMobi is theleading provider of content, monetization, andmarketingtechnologiesthat fuel growthfor industries around the world. Ourend-to-end advertising software platform, connected content,andcommerce experiences activate audiences, driverealconnections, and diversify revenue for businesses everywhere.\n\nInMobi Advertising is an end-to-end advertising platform that helps advertisers drive real connections with consumers. We drive customer growth by helping businesses understand, engage, and acquire consumers effectively through data-driven media solutions. Learn more at advertising.inmobi.com.\n\nGlance is a consumer technology company that operates disruptive digital platforms, including Glance, Roposo, and Nostra. Glance's smart lockscreen and TV experience inspires consumers to make the most of every moment by surfing relevant content without the need for searching and downloading apps. Glance is currently available on over 450 million smartphones and televisions worldwide. Learn more at glance.com.\n\nBorn in India, InMobimaintains a large presence in Bangalore and San Mateo, CA, and has operations in New York,Singapore, Delhi, Mumbai, Beijing,Shanghai, Jakarta, Manila, Kuala Lumpur, Sydney, Melbourne,Seoul, Tokyo, London, and Dubai. To learn more, visit inmobi.com.\n\nOur Story\n\nBuilding a new company in the recession of 2007 was no ordinary task. Yet with passion and foresight, we charted our course, helping to transform the way consumers engage with their phones. Over the last 17 years, we have built a global Advertising Platform that powers our customers growth by helping them to engage with their audiences and drive real connections. InMobi has also built a second unicorn, Glance, which is advancing digital consumption and creating a new wave of disruption. Present on 400M devices across India, SEA, Japan and the US Glance is one of the largest content platforms globally with 200M daily active users.\n\nPosition Summary\n\nWe look for talented Data Scientists who can roll up their sleeves and have direct impact on our company metrics. The performance of our models and experiments are seen astonishingly quickly the learning loop is not measured in weeks or days, but hours and minutes. We live in what might be the fastest model-learning playgrounds in the world. We have built an infrastructure that enables model deployment at both scale and speed. As data scientists, we sit alongside engineering colleagues who enable our models to deploy. Combine this with our growing variable set of hundreds of potential features (and growing!), and this is a highly fertile environment for building, experimenting, refining and achieving real impact from your models. If models fire, the bottom-line impact to our teams is immediate you see the value of your work incredibly fast.\n\nWho Are You\n\nOur scientists are expected to possess deep expertise and experience in Ad Tech, AI/ML, and Data Science, particularly at scale. Familiarity with big data processing and cloud computing will be critical to succeed in this environment.\nIn addition to possessing a mathematical aptitude (Statistics, Probability Theory, Algorithms, Foundations of Machine Learning), they need to be competent with data science languages and tools, such as Python or Apache Spark; which will enable them to design scalable solutions for our advertising products, implement proof-of-concept, and evaluate them offline and online. They will also need to work with other engineers to take these solutions to live production and drive real business value.\nMost importantly, we look for a passion to investigate and learn about the world from data, to ask interesting and provocative questions, and be driven to put real models into production that drive real business value.\nWe are open to diverse academic backgrounds, providing an intent to think and problem-solve like a data scientist. Our team includes engineers, mathematicians, computer scientists, physicists, economists and social scientists a rock-star data scientist can come from any academic field.\n\nRequired\n\n\nMaster's in a quantitative field such as Computer Science, Electrical Engineering, Statistics, Mathematics, Operations Research or Economics, Analytics, Data Science. Or Bachelor's from a reputed College with additional experience.\n7-10 years of work experience in a quantitative field, with model building and validation experience.\nAd tech related industry experience is a plus. There, you would have applied algorithms and techniques from Machine Learning, Deep Learning and Statistics or other domains in solving real world problems and understand the practical issues of using these algorithms especially on large datasets.\nComfortable with software programming and statistical platforms such as R, Python etc. including visualization tools.\nComfortable with the big data ecosystem and Apache Spark. Familiarity with Microsoft Azure, AWS, or Google Cloud/Vertex AI will be a bonus.\nComfortable collaborating with cross-functional teams.\nFamiliarity with challenges of the identity-less world, particularly for iOS and Android\nExcellent technical and business communication skills and should know how to present technical ideas in a simple manner to business counterparts.\nPossess a high degree of curiosity and ability to rapidly learn new areas.\n\nWhat will you do\n\n\nYou will be responsible for leading one or more data science efforts for inMobi DSP, a gold mine that is being explored for riches. This involves project ideation and conceptualization, solution design, measurement and solution iteration, coaching, deployment and post deployment management.\nThis will also include designing, development, testing of product experiments.\nYou are expected to be a hands-on part of the role where you will also actively analyse data, design and develop models, and solve problems with the rest of the team.\nAdditionally, stakeholder management is needed. It will involve being the interface with internal stakeholders such as our Product, Engineering, Data, Infrastructure, and Business teams.\nOur team strives for thought leadership in the sector. We encourage and support all team members to write blogs, commentary and case studies published on the InMobi blog. We also support team members across our MLAI team to speak at industry conferences and represent InMobi's work.\nYou will learn how to design and build models for specific business problems. Even before that, you will be responsible for identifying the problem areas where AI can be applied to best business impact. You will learn to start a model design by anchoring in the business context and end user needs. You will learn how to connect model impact with real and measurable business impact.\nYou will work in a multi-functional team environment. You will collaborate and benefit from the skills of a diverse group of individuals from teams such as engineering, product, business, campaign management and creative development.\nYou will have the opportunity to experiment with multiple algorithms. Enduring learning comes from building, launching and reviewing performance of a particular algorithm; from asking why something worked or why it did not work; from asking how to tailor techniques to fit the problem at hand. We have an environment that makes this possible at speed.\nImportantly, you will learn to become creative in designing models to be successful. Model design is not one-size-fits-all. Our models need to fit our particular problems and be modified to perform. Tougher problems require layers of models, and feedback mechanisms in a dynamic environment such as ours.\nWe are a company that innovates and demonstrates our thought leadership to the world, whether in products, research papers or conferences there are many opportunities for you to shine.\n\nWhat does InMobi do\n\n\nInMobi is one of the largest Independent Adtech player globally with a record of accomplishment for innovation, customer centricity and privacy first approach, we have been recognized on both 2018 and 2019 CNBC disruptor 50 list and as one of the Fast Company's 2018's most innovative company.\n\nInMobi is an equal opportunity employer\n\nInMobi is a place where everyone can grow. So however you identify and whatever background you bring with you, we invite you to apply if this sounds like a role that would make you excited to get to work every day. InMobi provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\n\nThe InMobi Culture\n\nAt InMobi, culture isn't a buzzword; it's an ethos woven by every InMobian, reflecting our diverse backgrounds and experiences.\n\nWe thrive on challenges and seize every opportunity for growth. Our core values of thinking big, being passionate, showing accountability, and taking ownership with freedom guide us in every decision we make.\n\nWe believe in nurturing and investing in your development through continuous learning and career progression with our InMobi Live Your Potential program.\n\nInMobi is proud to be an Equal Employment Opportunity and we make reasonable accommodations for qualified individuals with disabilities.\n\nVisit https://www.inmobi.com/company/careers to better understand our benefits, values and more!\n\nThe InMobi Culture\n\nAt InMobi, culture isn't a buzzword; it's an ethos woven by every InMobian, reflecting our diverse backgrounds and experiences.\n\nWe thrive on challenges and seize every opportunity for growth. Our core values of thinking big, being passionate, showing accountability, and taking ownership with freedom guide us in every decision we make.\n\nWe believe in nurturing and investing in your development through continuous learning and career progression with our InMobi Live Your Potential program.\n\nInMobi is proud to be an Equal Employment Opportunity and we make reasonable accommodations for qualified individuals with disabilities.\n\nVisit https://www.inmobi.com/company/careers to better understand our benefits, values, and more!",
        "skills": [
            "Google Cloud Vertex AI",
            "Ad Tech",
            "R",
            "Ai",
            "Ml",
            "Data Science",
            "Apache Spark",
            "Big Data",
            "Microsoft Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Staff Data Scientist",
        "company_name": "Anumana",
        "experience": "5-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position: Staff Data Scientist\nExperience Range: 5 to 9 yrs\nJob Location: Bangalore\nWork Mode: Hybrid (3 days in the office, 2 days remote)\nAbout Anumana: https://anumana.ai/\nAnumana is a new AI-driven health technology company from nference, developing and delivering ECG algorithms enabling early diagnosis and intervention.\nJob Summary:\nWe are seeking a highly skilled and motivated Staff Data Scientist with 5 to 9 years of experience to join our dynamic team. The ideal candidate will have a strong background in training machine learning models using PyTorch and TensorFlow, with a focus on making models production-ready and evaluating them with quality metrics. Expertise in training Transformers and convolution-based large models is essential, and experience in signal processing and image processing will be a significant plus.\nKey Responsibilities:\nModel Training: Develop and train machine learning models using PyTorch and TensorFlow. Focus on large-scale models, including Transformers and convolution-based architectures.\nModel Evaluation: Evaluate models using a range of quality metrics to ensure high performance and reliability.\nProduction Readiness: Work on making models production-ready, ensuring they are robust, scalable, and optimized for deployment.\nData Handling: Handle and process large datasets efficiently, adapting architectures and loss functions as needed to improve model performance.\nSignal and Image Processing: Utilize signal processing and image processing techniques to enhance model capabilities (preferred but not mandatory).\nInnovative Solutions: Help innovate, identify problems, recommend solutions, and perform triage in a collaborative team environment and collaborate with various teams on new product features and improvements of existing products\nText/LLM Models: Experience in working with text data and large language models (LLMs) is a plus.\nFundamentals: Possess a deep understanding of machine learning and deep learning basics.\nSoftware Development: Demonstrate strong software and algorithm development skills. Participate in developing and reviewing code, design documents, use case reviews, and test plan reviews.\nSkills Required: Python, Deep Learning, Problem Solving, Data Structures, Algorithms, Pytorch, Machine Learning, Transformers, CNNs\nGood to have: Computer Vision, Signal and Image Processing\nBenefits:\nBe a part of Google of biomedicine as recognized by the Washington Post\nWork with some of the brilliant minds of the world solving exciting real-world problems through Artificial Intelligence, Machine Learning, analytics and insights through triangulating unstructured and structured information from the biomedical literature as well as from large-scale molecular and real-world datasets.\nOur benefits package includes the best of what leading organizations provide, such as stock options, paid time off, healthcare insurance, gym/broadband reimbursement.",
        "skills": [
            "Transformers",
            "CNNs",
            "Tensorflow",
            "Machine Learning",
            "Algorithms",
            "Pytorch",
            "Data Structures",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Senior Data Scientist (4-6 years)",
        "company_name": "Fam",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Fam (previously FamPay)\n\nFam is India's first payments app for everyone above 11. FamApp helps make online and offline payments through UPI and FamCard. We are on a mission to raise a new, financially aware generation, and drive 250 million+ youngest users in India to kickstart their financial journey super early in their life.\n\nFounded in 2019 by IIT Roorkee alumni, Fam is backed by some of the most respected investors around the world like Elevation Capital, Y-Combinator, Peak XV (Sequoia Capital) India, Venture Highway, Global Founder's Capital and the likes of Kunal Shah, Amrish Rao as angel investors.\n\nAbout this Role:\n\nWe are looking for a Senior Data Scientist to join our growing data team at Fam. In this role, you will leverage data to drive key business decisions and build predictive models that optimize user experience, platform performance, and operational efficiency. You will work on large-scale data sets, employ advanced statistical and machine learning techniques and collaborate closely with cross-functional teams to unlock insights and innovations that impact millions of users and transactions. If you're passionate about data and making a tangible impact in the fintech space, this role is for you.\n\nOn the Job\n\nBuild and deploy machine learning models to solve complex business problems related to user behaviour, transactions, fraud detection, and more\nAnalyze large and complex datasets to uncover actionable insights and trends that inform business decisions\nCollaborate with engineering, product, and business teams to understand requirements and deliver data-driven solutions\nDevelop and implement statistical models, data pipelines, and algorithms to enhance product performance and user experience\nEnsure the quality, accuracy, and scalability of models and analyses by applying best practices in data science and software engineering\nCommunicate findings effectively to both technical and non-technical stakeholders through clear visualizations and reports\nMentor junior data scientists and promote a data-driven culture within the team\n\nMust-haves (Min. Qualifications)\n\n\n4-6 years of experience in data science, with a strong background in machine learning, statistics, and data analysis\nExpertise in programming languages such as Python or R, and experience with libraries like Pandas, Scikit-learn, TensorFlow, PyTorch etc\nSolid understanding of data wrangling, data visualization, and exploratory data analysis\nExperience with statistical modeling, time series forecasting, and anomaly detection\nStrong knowledge of databases (SQL, NoSQL) and distributed data processing tools (Spark, Hadoop)\nProven track record of applying machine learning techniques to real-world business problems in a fast-paced environment\nExcellent communication skills and the ability to translate complex data insights into clear, actionable recommendations for business teams\n\nGood to have\n\n\nExperience in fintech, payments, or financial services industry\nExposure to deep learning, NLP, or reinforcement learning techniques\nExperience with cloud platforms (AWS, GCP, Azure) and data tools like BigQuery, Redshift, clickhouse etc\nFamiliarity with data engineering concepts (ETL pipelines, data lakes, etc.)\nContributions to open-source projects or active involvement in the data science community\n\nWhy join us\n\n\nWork on high-impact projects that shape the future of fintech and affect millions of users\nOpportunity to apply cutting-edge machine learning and data science techniques at scale\nBe part of a dynamic, fast-growing company with a culture of continuous learning and innovation\nWork with a talented, data-driven team that's passionate about solving complex problems\nIf you're excited about leveraging data to drive innovation and have a passion for building data science solutions at scale, we'd love to hear from you!\n\nWhy should you join us\n\nEvery once in a while, a product comes to life that makes people think, why was this not done earlier The users love it, the investors dream about the great returns and the team feels the joy and pride every day. We have strong indicators for users & investors, and we are building a team that will have stories to tell all their life.\n\nWe believe that a great product is built by a high-quality team that finds purpose and joy in their work, and we also go beyond it to put heavy emphasis on having fun at work as well.\n\nPerks:\n\nIndustry's best ESOPs scheme\nMedical Insurances suiting your needs\nAccess to Mental Health professionals\nTop gadgets to achieve skill level - Bruce Wayne\nFlexible work schedule so you never miss brunch, lunch, or dinner plans\nFriendly leaves policy that'll make your friends jealous\nRelocation Support\nMeals in office\n\nHere's all the tea on FamApp\n\n\nFamApp focuses on financial inclusion of the next generation by providing UPI & card payments to everyone above 11 years old. Our flagship Spending Account, FamX, seamlessly integrates UPI and card payments, enabling users to manage, save, and learn about their finances effortlessly.\n\nRevolutionizing Payments and FinTech\n\nFamApp has enabled 6 million+ users to make UPI and card payments across India, removing the inconvenience of carrying cash everywhere. Users get to customise their FamX card with doodles, which lets them add a personal touch to their payments.\n\nTrusted by leading investors\n\nWe're proud to be supported by renowned investors like Elevation Capital, Y-Combinator, Peak XV (formerly Sequoia Capital India), Venture Highway, Global Founder's Capital, and esteemed angels Kunal Shah and Amrish Rao.\n\nJoin Our Dynamic Team\n\nAt Fam, our people-first approach is reflected in our generous leave policies, flexible work schedules, comprehensive health benefits, and free mental health sessions. We don't mean to brag, but we promise you'll be surrounded by some of the most fun, talented and passionate people in the startup space.\n\nWant to see what makes life at Fam so awesome Check out our shenanigans at @lifeatfam",
        "skills": [
            "R",
            "Data Lakes",
            "Scikit-learn",
            "Statistics",
            "reinforcement learning",
            "ETL Pipelines",
            "anomaly detection",
            "Time Series Forecasting",
            "Clickhouse",
            "Statistical Modeling",
            "Data Analysis",
            "Machine Learning",
            "Exploratory Data Analysis",
            "Nosql",
            "Tensorflow",
            "Nlp",
            "Pytorch",
            "data wrangling",
            "Python",
            "AWS",
            "BigQuery",
            "Hadoop",
            "Redshift",
            "Sql",
            "Deep Learning",
            "Pandas",
            "Gcp",
            "Data Visualization",
            "Spark",
            "Azure"
        ]
    },
    {
        "job_title": "Sr Data Scientist I Global",
        "company_name": "McCormick & Company",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "At McCormick, we bring our passion for flavor to work each day. We encourage growth, respect everyone's contributions and do what's right for our business, our people, our communities and our planet. Join us on our quest to make every meal and moment better.\n\nFounded in Baltimore, MD in 1889 in a room and a cellar by 25-year-old Willoughby McCormick with three employees, McCormick is a global leader in flavour. With over 14,000 employees around the world and more than $6 Billion in annual sales, the Company manufactures, markets, and distributes spices, seasoning mixes, condiments and other flavourful products to the entire food industry, retail outlets, food manufactures, food service businesses and consumers.\n\nWhile our global headquarters are in the Baltimore, Maryland, USA area, McCormick operates and serves customers from nearly 60 locations in 25 countries and 170 markets in Asia-Pacific, China, Europe, Middle East and Africa, and the Americas.\n\nAt McCormick, we have over a 100-year legacy based on our Power of People principle. This principle fosters an unusually dedicated workforce requiring a culture of respect, recognition, inclusion and collaboration based on the highest ethical values.\n\nPosition Overview\n\nTo provide the business with data insights that will help increase revenue and/or lower costs. ^ Provide technical leadership and focus on generating insights for business customers. ^ Individual should help shape strategies for our consumer units globally.\n\nKey Responsibilities\n\nLeadership, Project Lead, management of key stakeholders for key data science projects and presentation of results.\n\nBringing new insights to the business, leads data mining and analytics, interpreting and reporting of large integrated data sets built with structured and unstructured data; develops tools to leverage new proprietary data sources (SAP Analytics Cloud SAC, S/4Hana LeoConnect Fiori, Analysis for Office A40). Statistical model building and deployment in the areas of forecasting, marketing mix and consumer insights.\n\nContinuously seeks out industry best practice and skills development to create new capabilities for data analytics at McCormick to drive marketing strategy.\n\nRequired Qualifications & Experience\n\nBachelor's Degree in Statistics (or Statistics Major) or Data Analytics. Minimum of 8 years of experience in analytics.\n\nExtensive experience analyzing complex datasets, generating insights and building robust statistical models.\n\nComfortable working with structured and unstructured data. Expertise in R, MS-Office required. Familiarity with brands (McCormick and competition) and their metrices.\n\nInterpersonal Skills\n\nAble to influence and navigate across multiple functions and regions (Procurement, Marketing, Sales, R&D, Finance etc.) to leverage enterprise data seamlessly for insights that can be leveraged across the business. Sound timely decisions grounded in data but also intuition, welldeveloped emotional intelligence. This role will interact with subordinates and superiors in the global analytics organization as well as with the internal customers/sponsors of the work. He/she will most likely join a senior colleague in meetings with the business and on occasion, present findings from the analyses.\n\nMcCormick & Company is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, colour, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.",
        "skills": [
            "R",
            "LeoConnect",
            "MS-Office",
            "Data Analytics",
            "Fiori"
        ]
    },
    {
        "job_title": "Manager-Data Scientist (Credit Risk Models, 5+ years)",
        "company_name": "Time Hack Consulting",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nOne of the fastest growing fintech companies that has built a full-stack financial platform for Bharat 2.0. They have multiple financial products on their platform, viz, Lending, Insurance and Investments. They have an AUM of more than INR 1000 cr and about 500 employees.\n\nRole Responsibilities\n\nDevelop and implement advanced credit risk models using statistical methodologies.\nAnalyze large datasets to identify trends, patterns, and anomalies related to credit risk.\nCollaborate with cross-functional teams to gather requirements and ensure alignment with business objectives.\nMonitor and evaluate the performance of existing credit risk models and make necessary adjustments.\nCommunicate findings and recommendations to stakeholders in a clear and concise manner.\nLead a team of data scientists, providing mentorship and guidance to foster professional growth.\nDesign and execute experiments to validate model assumptions and improve accuracy.\nEnsure compliance with industry regulations and internal policies related to credit risk assessment.\nUtilize data visualization techniques to present complex data insights to non-technical stakeholders.\nStay updated with emerging trends and tools in data science and credit risk modeling.\nConduct risk assessments and stress testing to evaluate the impact of different scenarios.\nCollaborate with IT and data engineering teams to ensure data availability and integrity.\nPrepare reports and documentation summarizing model development processes and findings.\nContribute to the development of best practices for credit risk modeling and analytics.\nIdentify opportunities for process improvements within the team and across projects.\n\nQualifications\n\nBachelors in statistics, mathematics, computer science, or a related field.\nMinimum of 5 years of experience in credit risk modeling or data science.\nMininum 2 years of experience in managing a team is mandatory\nProficiency in statistical modeling techniques and machine learning algorithms.\nStrong programming skills in Python and experience with relevant libraries (e.g., Pandas, Scikit-learn).\nExperience with SQL for data extraction and manipulation.\nKnowledge of credit risk regulations and industry standards.\nFamiliarity with data visualization tools (e.g., Tableau, Power BI).\nAbility to communicate complex concepts to diverse audiences.\nProven track record of leading and mentoring junior team members.\nStrong analytical and problem-solving skills.\nExperience with data preprocessing and feature engineering.\nAbility to work collaboratively in a team-oriented environment.\nExcellent time management skills and attention to detail.\nStrong understanding of business drivers and implications of credit risk.\nWillingness to continuously learn and adapt to new methodologies and technologies.\n\nSkills: risk management,team leadership,tableau,predictive analytics,credit risk modeling,power bi,modeling,python,sql proficiency,problem-solving,data preprocessing,statistical modeling,scikit-learn,machine learning,data science,credit risk,feature engineering,machine learning algorithms,data visualization,pandas,sql,statistical modeling techniques",
        "skills": [
            "Statistical modeling techniques",
            "Data preprocessing",
            "Scikit-learn",
            "Credit risk modeling",
            "Feature engineering",
            "Data visualization tools",
            "Risk management",
            "Machine Learning Algorithms",
            "Power Bi",
            "Tableau",
            "Sql",
            "Predictive Analytics",
            "Pandas",
            "Python"
        ]
    },
    {
        "job_title": "Lead Data Scientist - Finance Analytics",
        "company_name": "Signant Health",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role Overview\n\nWe're seeking a talented Lead Data Scientist to join our Finance Analytics team. In this role, you'll analyze financial data to drive insights, identify patterns, and recommend automation opportunities within our finance and accounting functions. You'll apply your technical expertise in data engineering and advanced analytics to transform raw financial data into meaningful insights, working collaboratively with finance teams to understand business needs\n\nKey Accountabilities/Decision Making & Influence\n\nApply advanced analytics techniques to extract insights from financial data sets\nBuild and optimize data pipelines using Python, Spark, and SQL to prepare data for analysis\nDevelop and implement machine learning models to identify patterns, anomalies, and opportunities for automation\nCreate interactive dashboards and visualizations using BI tools that communicate key insights effectively\nCollaborate with finance teams to understand their data needs and translate them into analytical solutions\nIdentify and track relevant metrics that provide meaningful business intelligence\nSupport data-driven decision making by presenting clear, actionable findings\nConduct exploratory data analysis to uncover trends and relationships in financial data\nMentor junior data scientists on analytical techniques and best practices\nImplement statistical analysis methods to validate findings and ensure data quality\nDocument methodologies, processes, and results to ensure reproducibility and knowledge sharing\n\nKnowledge, Skills & Attributes\n\n5-7 years of experience in data science or analytics, with exposure to financial or business data\nStrong technical background in data engineering and pipeline development\nAdvanced proficiency in Python and experience with Spark for large-scale data processing\nExperience working with data from Snowflake Data Lake or similar cloud-based data platforms\nDemonstrated skill in building dashboards and visualizations using BI tools (Power BI, Tableau, Looker, etc.)\nProficiency in SQL for data extraction and manipulation\nExperience applying machine learning algorithms to solve business problems\nAbility to communicate technical concepts to non-technical stakeholders\nUnderstanding of basic financial concepts and metrics (no deep finance expertise required)\nStrong problem-solving skills and attention to detail\nBachelor's degree in computer science, Data Science, Statistics, or related technical field\n\nWe would be thrilled if you bring in the below:\n\nExperience working in cross-functional teams in fast-paced environments\nFamiliarity with agile methodologies and collaborative development practices\nExperience with version control systems (Git) and collaborative coding\nKnowledge of cloud computing platforms (AWS, Azure, GCP)\nUnderstanding of data governance and data quality best practices\nDemonstrates curiosity and a willingness to learn about finance and accounting concepts\nShows creativity in presenting data insights through effective visualizations\nMaintains a continuous learning mindset, staying updated with emerging technologies and techniques in data science\n\nWe know that everyone has different wants and needs, which is why along with a highly competitive base salary we support our people and their loved ones with a variety of perks and benefits. As part of our team some of the benefits you can expect to receive are:\n\nMedical Insurance, Group Accidental Coverage/Insurance, Group Term Life Insurance\nCompany Paid Subscription to Calm The #1 app for mental fitness.\nEmployee Referral Program Bring the Best to Signant Health and earn a reward.\nWellness Program Participate in challenges and earn points for rewards.\nProof! Signant's Employee Recognition Program where you can accumulate points to redeem exciting merchandise, gift cards, tickets, and more.\nBurn Along Digital fitness and wellness platform\n\nDoes this sound like something you'd like to explore Then we'd love to hear from you!\n\nTo apply, please submit your CV and a cover letter letting us know why you think you'd be perfect for this role. We will begin reviewing submissions during the application period and will fill the vacancy as soon as a suitable candidate is identified.\n\nPlease note that Signant does not accept unsolicited resumes from Third Party vendors.\n\nAt Signant Health, accepting difference isn't enoughwe celebrate it, we support it, and we nurture it for the benefit of our team members, our clients and our community. Signant Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.",
        "skills": [
            "Snowflake Data Lake",
            "Looker",
            "Machine Learning",
            "Bi Tools",
            "Power Bi",
            "Spark",
            "Tableau",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist - AI",
        "company_name": "Standard Chartered India",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Summary\n\nWe are looking for a Data Scientist with strong data and analytical skills, you will be tasked with managing all aspects any new work/project assessments thoroughly from inception to delivery. You will need to build close working relationships with COO business and CDO in order to gather specific modelling requirements and define and scope the work. In addition, as Data Scientist you will also work closely with internal staff, clients and 3rd parties.\n\nKey Responsibilities\n\nDesign Machine Learning, Natural Language and Decision Optimisation applications architecture.\nDesign ML, NLP and DO models, algorithms for predictive & prescriptive analytics.\nTranslate business requirements to reporting dashboard and analytics.\nPerform advanced analytics and statistical modelling on structured and unstructured data.\nWork with software engineers to integrate and deploy AI applications.\nAssist in the creation of data definitions for new database file/table development and/or changes to existing ones as needed for analysis.\nSupport the business with clear and insightful analysis applying advanced modelling techniques leveraging the data at hand.\nRespond to and resolve data mining performance issues.\nMonitor data mining system performance and implement efficiency improvements.\nDevelop and implement change management plans to ensure successful adoption of AI solutions across the organization.\nProvide training and support to business users to help them understand and leverage AI tools and technologies.\nFoster a culture of innovation and continuous improvement by promoting the benefits of AI and encouraging experimentation.\nWork with senior management to develop and refine the bank's AI strategy, ensuring alignment with overall business objectives.\nIdentify and prioritize AI use cases that have the potential to deliver significant business value.\nDevelop business cases for AI projects, including cost-benefit analysis, risk assessment, and ROI estimation.\n\nSkills And Experience\n\n8+ Years of Software development experience using .Net framework or Java\nCompleted real world ML, NLP and DO projects using R or Python\nExtensive experience in SQL and NoSQL database design, queries and stored procedures\nHands-on experience with Windows Server, Azure, AWS and Git\nHighly proficient in data visualisation tools...\nDocumentation - Requirements/Use Cases/Business Rules/User Stories, Etc.\nReport Types - Gap Analysis/Problem Analysis/Initial Assessments, Etc.\nProcess/Data Modelling - As Is/To Be/ Visio/Enterprise Architect/System Architect, Etc.\nStrong query language skills (SQL, Hive, ETL, Hadoop, Spark, R, Python)\nGood experience with Business Intelligence tools and Decision Support Systems\nStrong data analysis skills using Hive, Spark, R, Python, Dremio, MicroStrategy and Tableau.\nProven experience in working with key stakeholders within the business.\nProven problem-solving skills\nWorkshop Facilitation\n\nQualifications\n\nMaster or PHD in Mathematics, Statistics, Knowledge Engineering or Data science.\n\nCompleted real world ML, NLP and DO projects using R or Python\n\nAbout Standard Chartered\n\nWe're an international bank, nimble enough to act, big enough for impact. For more than 170 years, we've worked to make a positive difference for our clients, communities, and each other. We question the status quo, love a challenge and enjoy finding new opportunities to grow and do better than before. If you're looking for a career with purpose and you want to work for a bank making a difference, we want to hear from you. You can count on us to celebrate your unique talents and we can't wait to see the talents you can bring us.\n\nOur purpose, to drive commerce and prosperity through our unique diversity, together with our brand promise, to be here for good are achieved by how we each live our valued behaviours. When you work with us, you'll see how we value difference and advocate inclusion.\n\nTogether We\n\nDo the right thing and are assertive, challenge one another, and live with integrity, while putting the client at the heart of what we do\nNever settle, continuously striving to improve and innovate, keeping things simple and learning from doing well, and not so well\nAre better together, we can be ourselves, be inclusive, see more good in others, and work collectively to build for the long term\n\nWhat We Offer\n\nIn line with our Fair Pay Charter, we offer a competitive salary and benefits to support your mental, physical, financial and social wellbeing.\n\nCore bank funding for retirement savings, medical and life insurance, with flexible and voluntary benefits available in some locations.\nTime-off including annual leave, parental/maternity (20 weeks), sabbatical (12 months maximum) and volunteering leave (3 days), along with minimum global standards for annual and public holiday, which is combined to 30 days minimum.\nFlexible working options based around home and office locations, with flexible working patterns.\nProactive wellbeing support through Unmind, a market-leading digital wellbeing platform, development courses for resilience and other human skills, global Employee Assistance Programme, sick leave, mental health first-aiders and all sorts of self-help toolkits\nA continuous learning culture to support your growth, with opportunities to reskill and upskill and access to physical, virtual and digital learning.\nBeing part of an inclusive and values driven organisation, one that embraces and celebrates our unique diversity, across our teams, business functions and geographies - everyone feels respected and can realise their full potential.",
        "skills": [
            "Dremio",
            "R",
            ".NET Framework",
            "Microstrategy",
            "data visualisation tools",
            "Java",
            "Hadoop",
            "Windows Server",
            "Tableau",
            "Sql",
            "Nosql",
            "Git",
            "Hive",
            "Spark",
            "Azure",
            "Python",
            "AWS",
            "Etl"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ford Motor Company",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nEmployees in this job function are responsible for designing, developing, testing and maintaining software applications and products to meet customer needs both on-prem and cloud native. They are involved in the entire software development lifecycle including designing software architecture, writing code, testing for quality and deploying the software to meet customer requirements. Full-stack software engineering roles, who can develop all components of software including user interface and server side also fall within this job function.\n\nWhat you'll be able to do:\n\nThe Software Engineer will work on a Balanced Product Team and collaborate with the Product Manager, Product Designer, and other Software Engineers to deliver analytic solutions. The Software Engineer will be responsible for the development and ongoing support/maintenance of the analytic solutions.\n\nProduct And Requirements Management: Participate in and/or lead the development of requirements, features, user stories, use cases, and test cases. Participate in stand-up operations meetings.\nAuthor: Process and Design Documents\nDesign/Develop/Test/Deploy: Work with the Business Customer, Product Owner, Architects, Product Designer, Software Engineers, and Security Controls Champion on solution design, development, and deployment.\nOperations: Generate Metrics, Perform User Access Authorization, Perform Password Maintenance, and Build Deployment Pipelines.\nIncident, Problem, And Change/Service Requests: Participate and/or lead incident, problem, change and service request-related activities. Includes root cause analysis (RCA). Includes proactive problem management/defect prevention activities.\n\nResponsibilities\n\n\nThe minimum requirements we seek:\n\n5+ years experience in Software Engineering.\nBachelor's degree in computer science, computer engineering or a combination of education and equivalent experience.\n1+ year experience with developing for and deploying to cloud platforms (e.g. GCP, Azure)\nImplement and optimize cloud services and tools (e.g. Terraform, BigQuery, GCP)\nExperience in development using combination of the following technologies:\nLanguages: Java / JS / TS / Python\nFrontend frameworks: Angular / React\nBackend frameworks: Spring / Node\nProven experience understanding, practicing, and advocating for software engineering disciplines from Clean Code, Software Artmanship, and Lean including:\nPaired / Mobbing programming\nTest-first/Test Driven Development (TDD)\nEvolutionary design\nMinimum Viable Product\nWillingness to collaborate daily with team members.\nA strong curiosity around how to best use technology to amaze and delight our customers\nUsing CI/CD tools and pipelines e.g. Tekton, GIT Action, Cloud Build, etc.\n\nQualifications\n\n\nOur preferred qualifications:\n\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders\nDelivered products that include web front-end development; JavaScript, CSS, frameworks like Angular, etc.\nComfortable with Continuous Integration/Continuous Delivery tools and pipelines e.g. Tekton, Cloud Build, etc.\nExperience with machine learning, mathematical modeling, and data analysis is a plus\nExperience with CA Agile Central (Rally, JIRA), backlogs, iterations, user stories, or similar Agile Tools\nExperience in the development of microservices\nUnderstanding of fundamental data modeling\nStrong analytical and problem-solving skills",
        "skills": [
            "Tekton",
            "GIT Action",
            "Cloud Build",
            "Java",
            "BigQuery",
            "Node",
            "Spring",
            "Angular",
            "React",
            "Javascript",
            "Gcp",
            "Terraform",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "Zepto",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "The ideal candidate's favourite words are data, scale and agility !\nWe're looking for a highly skilled and passionate Data Scientist to tackle complex challenges, develop innovative solutions, and provide strategic insights that shape our products and user experiences\nResponsibilities\nAnalyse raw data: assessing quality, cleansing, structuring for downstream processing\nDesign accurate and scalable prediction algorithms\nConduct rigorous exploratory data analysis (EDA) to uncover patterns, trends, and opportunities for optimisation\nGenerate actionable insights for business improvements\nQualifications\nMaster's or Ph.D. degree in a quantitative field such as Computer Science, Statistics, Mathematics, Economics, or a related discipline\nAt-least 5 years of professional experience with deep understanding of data mining / pipelining predictive modeling, model serving, machine-learning, clustering and classification techniques, and algorithms\nProven ability to develop AI solutions to improve business and customer metrics\nFamiliarity with Big Data frameworks and recommendation systems\nIf you thrive in a fast-paced environment and are eager to make a tangible difference, we encourage you to apply.",
        "skills": [
            "machine-learning",
            "classification techniques",
            "Big Data frameworks",
            "pipelining",
            "recommendation systems",
            "model serving",
            "Predictive Modeling",
            "data mining",
            "Clustering"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Intelligence Node",
        "experience": "7-10 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview:\nIntelligence Node is a real-time retail intelligence platform that empowers businesses to drive product-level profitability and grow margins using data-driven real-time competitive insights. Intelligence Node's cutting-edge technology leverages AI to aggregate and analyze billions of data points across 1,900 retail categories in 34 global markets. Intelligence Node's business model provides a comprehensive suite of solutions encompassing product matching, digital shelf analytics, assortment & availability, pricing & promotions, and brand compliance, to aid retailers and brands in all strategic and operational decision-making. Its patented product matching engine and proprietary AI offer 99% data accuracy and 10-second data refresh rates. In December 2024, Intelligence Node was acquired by Interpublic Group (IPG), a global leader in marketing solutions. Joining forces with IPG through this strategic acquisition allows Intelligence Node to deliver a future-ready combined solution to companies navigating the complexities of today's commerce landscape. Together, we can provide the comprehensive data, advanced analytics, and strategic expertise that enterprise brands and retailers need to win market share and drive sustainable growth.\nJob Description:\nAs a Senior Data Scientist at Intelligence Node, you will lead a team of data scientists and collaborate closely with key business stakeholders.\nDevelop advanced machine learning models and neural networks to solve complex problems in the retail and e-commerce sectors.\nExperiment with LLMs (Large Language Models)\nLead the design and development of scalable data pipelines.\nMentor junior data scientists\nDrive the deployment of predictive models into production.\nRequired Skill Set\nStrong proficiency in Python or C++\nDeep understanding of BERT, transformers, and other advanced machine learning architectures\nExpertise in computer vision techniques and applications e.g., specialized embedding techniques such as visual transformers and OCR.\nLLMs understanding and curiosity to explore (out of box deployment, RAG, fine tune etc.)\nExtensive retail and e-commerce experience is preferred.\nPhD in a related field is preferred.\nDesired profile of the candidate\nExceptional communication skills\nLeadership ability in fast-paced environments\nResults-oriented and analytical\nExperience leading cross-functional teams.\nAbility to derive actionable insights from complex data.\nExperience Required: 7-10yrs",
        "skills": [
            "visual transformers",
            "computer vision techniques",
            "LLMs",
            "advanced machine learning architectures",
            "BERT transformers",
            "Ocr",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Bluevine",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Bluevine\n\nBluevine is transforming small business banking with innovative solutions like checking, lending, and creditall tailored to help entrepreneurs thrive. With best-in-class technology, advanced security, and a deep understanding of the small business community, we're empowering entrepreneurs to grow with confidence.\n\nBacked by leading investors like Lightspeed Venture Partners, Menlo Ventures, 83North, and Citi Ventures, we've been supporting SMBs since 2013, serving over 500,000 customers nationwide and growing a dynamic global team of 500 people. Our mission To fuel small businesses with the financial tools they need to succeed.\n\nAt Bluevine, you'll be part of a collaborative, fast-paced team that's reshaping the future of banking. Ready to make an impact\n\nAbout the Role:\n\nWe are building a new, cutting-edge data science team in India and are looking for an experienced Senior Data Scientist to join us. This role is an exciting opportunity to apply your expertise in advanced statistical techniques, machine learning, and predictive modeling to solve complex business problems. You will be instrumental in shaping the data science function, developing scalable models, and collaborating with teams across the globe.\n\nWhat You'll Do:\n\nAdvanced Modeling & Analytics: Utilize advanced statistical, machine learning, and predictive modeling techniques to design, build, and improve decision-making systems that drive business outcomes.\nScalable Model Implementation: Apply modern software engineering practices to implement machine learning models in a scalable, robust, and maintainable manner. Ensure models are production-ready and optimized for performance.\nMLOps & System Integration: Lead technical MLOps projects including the redesign of existing infrastructures, maintaining and improving current models and systems, and integrating the latest technologies into our data science workflows.\nData Collection & Management: Manage and lead initiatives to design and implement robust data collection procedures necessary for building comprehensive analytical systems.\nCollaboration & Leadership: Work closely with Data Scientists, the R&D department, and cross-functional teams across India, the US, and Israel to drive data science initiatives. Provide mentorship to junior data scientists and contribute to the growth of the team.\n\nWhat We're Looking For:\n\n5+ Years of Experience: A minimum of 5 years of professional experience as a Data Scientist, with a proven track record of developing and deploying machine learning models in a production environment.\nProficiency in Python: Extensive experience in developing and deploying scalable production-level code in Python, with a deep understanding of machine learning libraries such as Scikit-learn, PyTorch, TensorFlow, or similar.\nDatabase Expertise: Strong experience working with relational databases, writing optimized SQL queries, and managing large datasets.\nEnd-to-End ML Lifecycle: Comprehensive experience in the full lifecycle of ML model development, including ideation, training/testing, deployment, and monitoring in a production setting.\nData Analysis & Insight Generation: Ability to analyze complex datasets and extract actionable insights that inform business decisions.\nEducational Background: A Master's or Ph.D. in Statistics, Computer Science, Engineering, or a related quantitative discipline.\nCommunication Skills: Proven ability to communicate complex technical concepts and findings to non-technical stakeholders, with a focus on clear and actionable insights.\n\nBonus Points:\n\nCross-Functional Experience: Experience working cross-functionally with Data/MLOps Engineering, Analytics, Business, and Product teams.\nDomain Expertise: Previous experience in specialized areas such as fraud detection, credit risk prediction, or graph models and analytics.\nMLOps & Cloud Experience: Hands-on experience with MLOps, particularly within the AWS ecosystem, and familiarity with tools like Apache Airflow.\nExperimental Design: Expertise in designing and executing experiments to measure model impact and effectiveness.\n\nBenefits & Perks\n\nExcellent group health coverage and life insurance\nStock options\nHybrid work model\nMeal allowance\nTransportation assistance (terms and conditions apply)\nGenerous paid time off plan, Holidays\nCompany-sponsored mental health benefits\nFinancial advisory services for both short- and long-term goals\nLearning and development opportunities to support career growth\nCommunity-based volunteering opportunities",
        "skills": [
            "Scikit-learn",
            "Apache Airflow",
            "Tensorflow",
            "Pytorch",
            "MLops",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist I - Newton",
        "company_name": "Affle",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Designation: Data Scientist I\n\nOffice Location: Gurgaon\n\nPosition Description: The Data Scientist is crucial in leveraging data to derive meaningful insights and solutions for complex business problems. This individual will lead and guide the data science team in developing advanced analytical models, algorithms, and statistical analyses. They will collaborate with cross-functional teams to identify\n\nopportunities for leveraging data-driven solutions, making strategic decisions, and enhancing overall business performance. The Data Scientist will be responsible for designing and implementing machine learning models, conducting data exploration, and communicating findings to non-technical stakeholders.\n\nPrimary Responsibilities:\n\nCollaborate with business stakeholders to understand and translate their goals into data science initiatives.\nLead the development and implementation of machine learning models and algorithms to place ads in the context of cutting-edge privacy frameworks efficiently.\nDevelop strategies to optimize budget allocation in scenarios with high cardinality and uncertainty.\nConduct exploratory data analysis to discover complex data sets patterns, trends, and insights.\nCommunicate complex analytical findings in a clear and actionable manner to non-technical stakeholders.\nStay abreast of the latest advancements in data science, machine learning, and relevant technologies.\nDevelop and train ML/DL models for forecasting/classification\nBuild AI agents & RAG-based systems\nLeverage data-driven insights and predictive modeling to build PoCs.\nLeverage LangChain, LangGraph, or similar for orchestration\nUse SQL for data analysis, feature engineering, and reporting\n\nRequired Skills:\n\nQualification in a quantitative field such as Computer Science, Statistics, Physics, or Mathematics.\nExcellent problem-solving skills.\nStrong coding skills.\n2+ years of relevant work experience in data science and machine learning.\nSolid background in statistical analysis, hypothesis testing, and experimental design.\nProficiency in Python and SQL\nPreferred GCP Platform\nExposure to RAG, LLMs, and agentic workflows - Finetuning & Evaluation\nFamiliar with LangChain, LangGraph, or similar toolkits\nPlus points if you have experience with Apple Ads (formerly ASA) or have worked in the AdTech space.\nEffective communication skills with the ability to convey technical concepts to non-technical audiences\n\nWork Environment Details:\n\nAbout Affle:\n\nAffle is a global technology company with a proprietary consumer intelligence platform that transforms ads into recommendations helping marketers to effectively identify, engage, acquire and drive transactions with their potential and existing users. While Affle Consumer platform is used by online & offline companies for measurable mobile advertising, its mTraction Enterprise platform helps offline companies to go online through platform-based app development, enablement ofO2O commerce and through its customer data platform. Affle India successfully completed its IPO in India on 08.08.19 and now trades on the stock exchanges (BSE: 542752 & NSE: AFFLE). Affle Holdings is the Singapore based promoter for Affle India and its investors include Microsoft, D2C (An NTT DoCoMo subsidiary), Itochu, Bennett Coleman & Company (BCCL) amongst others.\n\nFor more details please visit: www.affle.com\n\nAbout Newton:\n\nScale the Impact of Apple Search Ads Campaigns with Newton: Performance-focused Apple Search Ads management platform, powered by data intelligence and in-depth expertise to deliver business growth. Newton is crafted for targeting and optimizing your Apple Search Ads user acquisition campaign for bottom-of-funnel campaign goals - app installs, user activation, product purchases, and more. Utilizing AI-powered keyword suggestions, market insights, automated bid optimization, comprehensive funnel analytics, and dedicated client support, brands can improve visibility and expand the market share of their iOS app with Apple Search Ads. This results in acquiring new customers and driving business growth with an impressive ROI from your mobile advertising efforts.\n\nFor more details please visit: https://newtonco.ai/",
        "skills": [
            "LangGraph",
            "LangChain",
            "Feature Engineering",
            "RAG-based systems",
            "Data Exploration",
            "Machine Learning",
            "Statistical Analysis",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Fraud Data Scientist",
        "company_name": "Straive",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Roles and Responsibilities:\nDesign, develop, and deploy machine learning models to detect and prevent fraudulent activities, such as Merchant Fraud, transactional fraud, account takeover, and identity theft\nWork with large datasets to identify patterns, trends, and anomalies that may indicate fraudulent activity\nUtilize data analytics tools and methodologies to conduct in-depth assessments and generate Fraud rules and reports on fraud trends (including first-party and third-party fraud).\nCollaborate with cross-functional teams, including risk management, operations, and compliance, to enhance fraud prevention measures.\nMonitor industry trends, regulatory changes, and best practices to continually enhance fraud prevention strategies.\nSkills Required:\nBachelor's degree in engineering, technology, computer science or related field.\n3+ years of proven data analytics experience in fraud prevention, risk management, or a related field\nFamiliarity with fraud detection software, risk assessment methodologies, and regulatory compliance.\nStrong experience in SQL or Python\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely.\nDetail-oriented with a proactive mindset toward problem-solving and risk mitigation.\nAbility to work collaboratively in a cross-functional team environment.",
        "skills": [
            "Regulatory Compliance",
            "fraud detection software",
            "risk assessment methodologies",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Manager-Data Scientist (Credit Risk Models, 5+ years)",
        "company_name": "Time Hack Consulting",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Company Overview\n\nOne of the fastest growing fintech companies that has built a full-stack financial platform for Bharat 2.0. They have multiple financial products on their platform, viz, Lending, Insurance and Investments. They have an AUM of more than INR 1000 cr and about 500 employees.\n\nRole Responsibilities\n\nDevelop and implement advanced credit risk models using statistical methodologies.\nAnalyze large datasets to identify trends, patterns, and anomalies related to credit risk.\nCollaborate with cross-functional teams to gather requirements and ensure alignment with business objectives.\nMonitor and evaluate the performance of existing credit risk models and make necessary adjustments.\nCommunicate findings and recommendations to stakeholders in a clear and concise manner.\nLead a team of data scientists, providing mentorship and guidance to foster professional growth.\nDesign and execute experiments to validate model assumptions and improve accuracy.\nEnsure compliance with industry regulations and internal policies related to credit risk assessment.\nUtilize data visualization techniques to present complex data insights to non-technical stakeholders.\nStay updated with emerging trends and tools in data science and credit risk modeling.\nConduct risk assessments and stress testing to evaluate the impact of different scenarios.\nCollaborate with IT and data engineering teams to ensure data availability and integrity.\nPrepare reports and documentation summarizing model development processes and findings.\nContribute to the development of best practices for credit risk modeling and analytics.\nIdentify opportunities for process improvements within the team and across projects.\n\nQualifications\n\nBachelors in statistics, mathematics, computer science, or a related field.\nMinimum of 5 years of experience in credit risk modeling or data science.\nMininum 2 years of experience in managing a team is mandatory\nProficiency in statistical modeling techniques and machine learning algorithms.\nStrong programming skills in Python and experience with relevant libraries (e.g., Pandas, Scikit-learn).\nExperience with SQL for data extraction and manipulation.\nKnowledge of credit risk regulations and industry standards.\nFamiliarity with data visualization tools (e.g., Tableau, Power BI).\nAbility to communicate complex concepts to diverse audiences.\nProven track record of leading and mentoring junior team members.\nStrong analytical and problem-solving skills.\nExperience with data preprocessing and feature engineering.\nAbility to work collaboratively in a team-oriented environment.\nExcellent time management skills and attention to detail.\nStrong understanding of business drivers and implications of credit risk.\nWillingness to continuously learn and adapt to new methodologies and technologies.\n\nSkills: risk management,team leadership,tableau,predictive analytics,credit risk modeling,power bi,modeling,python,sql proficiency,problem-solving,data preprocessing,statistical modeling,scikit-learn,machine learning,data science,credit risk,feature engineering,machine learning algorithms,data visualization,pandas,sql,statistical modeling techniques",
        "skills": [
            "Statistical modeling techniques",
            "Data preprocessing",
            "Scikit-learn",
            "Credit risk modeling",
            "Feature engineering",
            "Data visualization tools",
            "Risk management",
            "Machine Learning Algorithms",
            "Power Bi",
            "Tableau",
            "Sql",
            "Predictive Analytics",
            "Pandas",
            "Python"
        ]
    },
    {
        "job_title": "Sr Data Scientist I Global",
        "company_name": "McCormick & Company",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "At McCormick, we bring our passion for flavor to work each day. We encourage growth, respect everyone's contributions and do what's right for our business, our people, our communities and our planet. Join us on our quest to make every meal and moment better.\n\nFounded in Baltimore, MD in 1889 in a room and a cellar by 25-year-old Willoughby McCormick with three employees, McCormick is a global leader in flavour. With over 14,000 employees around the world and more than $6 Billion in annual sales, the Company manufactures, markets, and distributes spices, seasoning mixes, condiments and other flavourful products to the entire food industry, retail outlets, food manufactures, food service businesses and consumers.\n\nWhile our global headquarters are in the Baltimore, Maryland, USA area, McCormick operates and serves customers from nearly 60 locations in 25 countries and 170 markets in Asia-Pacific, China, Europe, Middle East and Africa, and the Americas.\n\nAt McCormick, we have over a 100-year legacy based on our Power of People principle. This principle fosters an unusually dedicated workforce requiring a culture of respect, recognition, inclusion and collaboration based on the highest ethical values.\n\nPosition Overview\n\nTo provide the business with data insights that will help increase revenue and/or lower costs. ^ Provide technical leadership and focus on generating insights for business customers. ^ Individual should help shape strategies for our consumer units globally.\n\nKey Responsibilities\n\nLeadership, Project Lead, management of key stakeholders for key data science projects and presentation of results.\n\nBringing new insights to the business, leads data mining and analytics, interpreting and reporting of large integrated data sets built with structured and unstructured data; develops tools to leverage new proprietary data sources (SAP Analytics Cloud SAC, S/4Hana LeoConnect Fiori, Analysis for Office A40). Statistical model building and deployment in the areas of forecasting, marketing mix and consumer insights.\n\nContinuously seeks out industry best practice and skills development to create new capabilities for data analytics at McCormick to drive marketing strategy.\n\nRequired Qualifications & Experience\n\nBachelor's Degree in Statistics (or Statistics Major) or Data Analytics. Minimum of 8 years of experience in analytics.\n\nExtensive experience analyzing complex datasets, generating insights and building robust statistical models.\n\nComfortable working with structured and unstructured data. Expertise in R, MS-Office required. Familiarity with brands (McCormick and competition) and their metrices.\n\nInterpersonal Skills\n\nAble to influence and navigate across multiple functions and regions (Procurement, Marketing, Sales, R&D, Finance etc.) to leverage enterprise data seamlessly for insights that can be leveraged across the business. Sound timely decisions grounded in data but also intuition, welldeveloped emotional intelligence. This role will interact with subordinates and superiors in the global analytics organization as well as with the internal customers/sponsors of the work. He/she will most likely join a senior colleague in meetings with the business and on occasion, present findings from the analyses.\n\nMcCormick & Company is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, colour, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.",
        "skills": [
            "R",
            "LeoConnect",
            "MS-Office",
            "Data Analytics",
            "Fiori"
        ]
    },
    {
        "job_title": "Senior Data Scientist (4-6 years)",
        "company_name": "Fam",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Fam (previously FamPay)\n\nFam is India's first payments app for everyone above 11. FamApp helps make online and offline payments through UPI and FamCard. We are on a mission to raise a new, financially aware generation, and drive 250 million+ youngest users in India to kickstart their financial journey super early in their life.\n\nFounded in 2019 by IIT Roorkee alumni, Fam is backed by some of the most respected investors around the world like Elevation Capital, Y-Combinator, Peak XV (Sequoia Capital) India, Venture Highway, Global Founder's Capital and the likes of Kunal Shah, Amrish Rao as angel investors.\n\nAbout this Role:\n\nWe are looking for a Senior Data Scientist to join our growing data team at Fam. In this role, you will leverage data to drive key business decisions and build predictive models that optimize user experience, platform performance, and operational efficiency. You will work on large-scale data sets, employ advanced statistical and machine learning techniques and collaborate closely with cross-functional teams to unlock insights and innovations that impact millions of users and transactions. If you're passionate about data and making a tangible impact in the fintech space, this role is for you.\n\nOn the Job\n\nBuild and deploy machine learning models to solve complex business problems related to user behaviour, transactions, fraud detection, and more\nAnalyze large and complex datasets to uncover actionable insights and trends that inform business decisions\nCollaborate with engineering, product, and business teams to understand requirements and deliver data-driven solutions\nDevelop and implement statistical models, data pipelines, and algorithms to enhance product performance and user experience\nEnsure the quality, accuracy, and scalability of models and analyses by applying best practices in data science and software engineering\nCommunicate findings effectively to both technical and non-technical stakeholders through clear visualizations and reports\nMentor junior data scientists and promote a data-driven culture within the team\n\nMust-haves (Min. Qualifications)\n\n\n4-6 years of experience in data science, with a strong background in machine learning, statistics, and data analysis\nExpertise in programming languages such as Python or R, and experience with libraries like Pandas, Scikit-learn, TensorFlow, PyTorch etc\nSolid understanding of data wrangling, data visualization, and exploratory data analysis\nExperience with statistical modeling, time series forecasting, and anomaly detection\nStrong knowledge of databases (SQL, NoSQL) and distributed data processing tools (Spark, Hadoop)\nProven track record of applying machine learning techniques to real-world business problems in a fast-paced environment\nExcellent communication skills and the ability to translate complex data insights into clear, actionable recommendations for business teams\n\nGood to have\n\n\nExperience in fintech, payments, or financial services industry\nExposure to deep learning, NLP, or reinforcement learning techniques\nExperience with cloud platforms (AWS, GCP, Azure) and data tools like BigQuery, Redshift, clickhouse etc\nFamiliarity with data engineering concepts (ETL pipelines, data lakes, etc.)\nContributions to open-source projects or active involvement in the data science community\n\nWhy join us\n\n\nWork on high-impact projects that shape the future of fintech and affect millions of users\nOpportunity to apply cutting-edge machine learning and data science techniques at scale\nBe part of a dynamic, fast-growing company with a culture of continuous learning and innovation\nWork with a talented, data-driven team that's passionate about solving complex problems\nIf you're excited about leveraging data to drive innovation and have a passion for building data science solutions at scale, we'd love to hear from you!\n\nWhy should you join us\n\nEvery once in a while, a product comes to life that makes people think, why was this not done earlier The users love it, the investors dream about the great returns and the team feels the joy and pride every day. We have strong indicators for users & investors, and we are building a team that will have stories to tell all their life.\n\nWe believe that a great product is built by a high-quality team that finds purpose and joy in their work, and we also go beyond it to put heavy emphasis on having fun at work as well.\n\nPerks:\n\nIndustry's best ESOPs scheme\nMedical Insurances suiting your needs\nAccess to Mental Health professionals\nTop gadgets to achieve skill level - Bruce Wayne\nFlexible work schedule so you never miss brunch, lunch, or dinner plans\nFriendly leaves policy that'll make your friends jealous\nRelocation Support\nMeals in office\n\nHere's all the tea on FamApp\n\n\nFamApp focuses on financial inclusion of the next generation by providing UPI & card payments to everyone above 11 years old. Our flagship Spending Account, FamX, seamlessly integrates UPI and card payments, enabling users to manage, save, and learn about their finances effortlessly.\n\nRevolutionizing Payments and FinTech\n\nFamApp has enabled 6 million+ users to make UPI and card payments across India, removing the inconvenience of carrying cash everywhere. Users get to customise their FamX card with doodles, which lets them add a personal touch to their payments.\n\nTrusted by leading investors\n\nWe're proud to be supported by renowned investors like Elevation Capital, Y-Combinator, Peak XV (formerly Sequoia Capital India), Venture Highway, Global Founder's Capital, and esteemed angels Kunal Shah and Amrish Rao.\n\nJoin Our Dynamic Team\n\nAt Fam, our people-first approach is reflected in our generous leave policies, flexible work schedules, comprehensive health benefits, and free mental health sessions. We don't mean to brag, but we promise you'll be surrounded by some of the most fun, talented and passionate people in the startup space.\n\nWant to see what makes life at Fam so awesome Check out our shenanigans at @lifeatfam",
        "skills": [
            "R",
            "Data Lakes",
            "Scikit-learn",
            "Statistics",
            "reinforcement learning",
            "ETL Pipelines",
            "anomaly detection",
            "Time Series Forecasting",
            "Clickhouse",
            "Statistical Modeling",
            "Data Analysis",
            "Machine Learning",
            "Exploratory Data Analysis",
            "Nosql",
            "Tensorflow",
            "Nlp",
            "Pytorch",
            "data wrangling",
            "Python",
            "AWS",
            "BigQuery",
            "Hadoop",
            "Redshift",
            "Sql",
            "Deep Learning",
            "Pandas",
            "Gcp",
            "Data Visualization",
            "Spark",
            "Azure"
        ]
    },
    {
        "job_title": "Lead Data Scientist - Finance Analytics",
        "company_name": "Signant Health",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role Overview\n\nWe're seeking a talented Lead Data Scientist to join our Finance Analytics team. In this role, you'll analyze financial data to drive insights, identify patterns, and recommend automation opportunities within our finance and accounting functions. You'll apply your technical expertise in data engineering and advanced analytics to transform raw financial data into meaningful insights, working collaboratively with finance teams to understand business needs\n\nKey Accountabilities/Decision Making & Influence\n\nApply advanced analytics techniques to extract insights from financial data sets\nBuild and optimize data pipelines using Python, Spark, and SQL to prepare data for analysis\nDevelop and implement machine learning models to identify patterns, anomalies, and opportunities for automation\nCreate interactive dashboards and visualizations using BI tools that communicate key insights effectively\nCollaborate with finance teams to understand their data needs and translate them into analytical solutions\nIdentify and track relevant metrics that provide meaningful business intelligence\nSupport data-driven decision making by presenting clear, actionable findings\nConduct exploratory data analysis to uncover trends and relationships in financial data\nMentor junior data scientists on analytical techniques and best practices\nImplement statistical analysis methods to validate findings and ensure data quality\nDocument methodologies, processes, and results to ensure reproducibility and knowledge sharing\n\nKnowledge, Skills & Attributes\n\n5-7 years of experience in data science or analytics, with exposure to financial or business data\nStrong technical background in data engineering and pipeline development\nAdvanced proficiency in Python and experience with Spark for large-scale data processing\nExperience working with data from Snowflake Data Lake or similar cloud-based data platforms\nDemonstrated skill in building dashboards and visualizations using BI tools (Power BI, Tableau, Looker, etc.)\nProficiency in SQL for data extraction and manipulation\nExperience applying machine learning algorithms to solve business problems\nAbility to communicate technical concepts to non-technical stakeholders\nUnderstanding of basic financial concepts and metrics (no deep finance expertise required)\nStrong problem-solving skills and attention to detail\nBachelor's degree in computer science, Data Science, Statistics, or related technical field\n\nWe would be thrilled if you bring in the below:\n\nExperience working in cross-functional teams in fast-paced environments\nFamiliarity with agile methodologies and collaborative development practices\nExperience with version control systems (Git) and collaborative coding\nKnowledge of cloud computing platforms (AWS, Azure, GCP)\nUnderstanding of data governance and data quality best practices\nDemonstrates curiosity and a willingness to learn about finance and accounting concepts\nShows creativity in presenting data insights through effective visualizations\nMaintains a continuous learning mindset, staying updated with emerging technologies and techniques in data science\n\nWe know that everyone has different wants and needs, which is why along with a highly competitive base salary we support our people and their loved ones with a variety of perks and benefits. As part of our team some of the benefits you can expect to receive are:\n\nMedical Insurance, Group Accidental Coverage/Insurance, Group Term Life Insurance\nCompany Paid Subscription to Calm The #1 app for mental fitness.\nEmployee Referral Program Bring the Best to Signant Health and earn a reward.\nWellness Program Participate in challenges and earn points for rewards.\nProof! Signant's Employee Recognition Program where you can accumulate points to redeem exciting merchandise, gift cards, tickets, and more.\nBurn Along Digital fitness and wellness platform\n\nDoes this sound like something you'd like to explore Then we'd love to hear from you!\n\nTo apply, please submit your CV and a cover letter letting us know why you think you'd be perfect for this role. We will begin reviewing submissions during the application period and will fill the vacancy as soon as a suitable candidate is identified.\n\nPlease note that Signant does not accept unsolicited resumes from Third Party vendors.\n\nAt Signant Health, accepting difference isn't enoughwe celebrate it, we support it, and we nurture it for the benefit of our team members, our clients and our community. Signant Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.",
        "skills": [
            "Snowflake Data Lake",
            "Looker",
            "Machine Learning",
            "Bi Tools",
            "Power Bi",
            "Spark",
            "Tableau",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Staff Data Scientist I.",
        "company_name": "InMobi Advertising",
        "experience": "7-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nInMobi is theleading provider of content, monetization, andmarketingtechnologiesthat fuel growthfor industries around the world. Ourend-to-end advertising software platform, connected content,andcommerce experiences activate audiences, driverealconnections, and diversify revenue for businesses everywhere.\n\nInMobi Advertising is an end-to-end advertising platform that helps advertisers drive real connections with consumers. We drive customer growth by helping businesses understand, engage, and acquire consumers effectively through data-driven media solutions. Learn more at advertising.inmobi.com.\n\nGlance is a consumer technology company that operates disruptive digital platforms, including Glance, Roposo, and Nostra. Glance's smart lockscreen and TV experience inspires consumers to make the most of every moment by surfing relevant content without the need for searching and downloading apps. Glance is currently available on over 450 million smartphones and televisions worldwide. Learn more at glance.com.\n\nBorn in India, InMobimaintains a large presence in Bangalore and San Mateo, CA, and has operations in New York,Singapore, Delhi, Mumbai, Beijing,Shanghai, Jakarta, Manila, Kuala Lumpur, Sydney, Melbourne,Seoul, Tokyo, London, and Dubai. To learn more, visit inmobi.com.\n\nOur Story\n\nBuilding a new company in the recession of 2007 was no ordinary task. Yet with passion and foresight, we charted our course, helping to transform the way consumers engage with their phones. Over the last 17 years, we have built a global Advertising Platform that powers our customers growth by helping them to engage with their audiences and drive real connections. InMobi has also built a second unicorn, Glance, which is advancing digital consumption and creating a new wave of disruption. Present on 400M devices across India, SEA, Japan and the US Glance is one of the largest content platforms globally with 200M daily active users.\n\nPosition Summary\n\nWe look for talented Data Scientists who can roll up their sleeves and have direct impact on our company metrics. The performance of our models and experiments are seen astonishingly quickly the learning loop is not measured in weeks or days, but hours and minutes. We live in what might be the fastest model-learning playgrounds in the world. We have built an infrastructure that enables model deployment at both scale and speed. As data scientists, we sit alongside engineering colleagues who enable our models to deploy. Combine this with our growing variable set of hundreds of potential features (and growing!), and this is a highly fertile environment for building, experimenting, refining and achieving real impact from your models. If models fire, the bottom-line impact to our teams is immediate you see the value of your work incredibly fast.\n\nWho Are You\n\nOur scientists are expected to possess deep expertise and experience in Ad Tech, AI/ML, and Data Science, particularly at scale. Familiarity with big data processing and cloud computing will be critical to succeed in this environment.\nIn addition to possessing a mathematical aptitude (Statistics, Probability Theory, Algorithms, Foundations of Machine Learning), they need to be competent with data science languages and tools, such as Python or Apache Spark; which will enable them to design scalable solutions for our advertising products, implement proof-of-concept, and evaluate them offline and online. They will also need to work with other engineers to take these solutions to live production and drive real business value.\nMost importantly, we look for a passion to investigate and learn about the world from data, to ask interesting and provocative questions, and be driven to put real models into production that drive real business value.\nWe are open to diverse academic backgrounds, providing an intent to think and problem-solve like a data scientist. Our team includes engineers, mathematicians, computer scientists, physicists, economists and social scientists a rock-star data scientist can come from any academic field.\n\nRequired\n\n\nMaster's in a quantitative field such as Computer Science, Electrical Engineering, Statistics, Mathematics, Operations Research or Economics, Analytics, Data Science. Or Bachelor's from a reputed College with additional experience.\n7-10 years of work experience in a quantitative field, with model building and validation experience.\nAd tech related industry experience is a plus. There, you would have applied algorithms and techniques from Machine Learning, Deep Learning and Statistics or other domains in solving real world problems and understand the practical issues of using these algorithms especially on large datasets.\nComfortable with software programming and statistical platforms such as R, Python etc. including visualization tools.\nComfortable with the big data ecosystem and Apache Spark. Familiarity with Microsoft Azure, AWS, or Google Cloud/Vertex AI will be a bonus.\nComfortable collaborating with cross-functional teams.\nFamiliarity with challenges of the identity-less world, particularly for iOS and Android\nExcellent technical and business communication skills and should know how to present technical ideas in a simple manner to business counterparts.\nPossess a high degree of curiosity and ability to rapidly learn new areas.\n\nWhat will you do\n\n\nYou will be responsible for leading one or more data science efforts for inMobi DSP, a gold mine that is being explored for riches. This involves project ideation and conceptualization, solution design, measurement and solution iteration, coaching, deployment and post deployment management.\nThis will also include designing, development, testing of product experiments.\nYou are expected to be a hands-on part of the role where you will also actively analyse data, design and develop models, and solve problems with the rest of the team.\nAdditionally, stakeholder management is needed. It will involve being the interface with internal stakeholders such as our Product, Engineering, Data, Infrastructure, and Business teams.\nOur team strives for thought leadership in the sector. We encourage and support all team members to write blogs, commentary and case studies published on the InMobi blog. We also support team members across our MLAI team to speak at industry conferences and represent InMobi's work.\nYou will learn how to design and build models for specific business problems. Even before that, you will be responsible for identifying the problem areas where AI can be applied to best business impact. You will learn to start a model design by anchoring in the business context and end user needs. You will learn how to connect model impact with real and measurable business impact.\nYou will work in a multi-functional team environment. You will collaborate and benefit from the skills of a diverse group of individuals from teams such as engineering, product, business, campaign management and creative development.\nYou will have the opportunity to experiment with multiple algorithms. Enduring learning comes from building, launching and reviewing performance of a particular algorithm; from asking why something worked or why it did not work; from asking how to tailor techniques to fit the problem at hand. We have an environment that makes this possible at speed.\nImportantly, you will learn to become creative in designing models to be successful. Model design is not one-size-fits-all. Our models need to fit our particular problems and be modified to perform. Tougher problems require layers of models, and feedback mechanisms in a dynamic environment such as ours.\nWe are a company that innovates and demonstrates our thought leadership to the world, whether in products, research papers or conferences there are many opportunities for you to shine.\n\nWhat does InMobi do\n\n\nInMobi is one of the largest Independent Adtech player globally with a record of accomplishment for innovation, customer centricity and privacy first approach, we have been recognized on both 2018 and 2019 CNBC disruptor 50 list and as one of the Fast Company's 2018's most innovative company.\n\nInMobi is an equal opportunity employer\n\nInMobi is a place where everyone can grow. So however you identify and whatever background you bring with you, we invite you to apply if this sounds like a role that would make you excited to get to work every day. InMobi provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\n\nThe InMobi Culture\n\nAt InMobi, culture isn't a buzzword; it's an ethos woven by every InMobian, reflecting our diverse backgrounds and experiences.\n\nWe thrive on challenges and seize every opportunity for growth. Our core values of thinking big, being passionate, showing accountability, and taking ownership with freedom guide us in every decision we make.\n\nWe believe in nurturing and investing in your development through continuous learning and career progression with our InMobi Live Your Potential program.\n\nInMobi is proud to be an Equal Employment Opportunity and we make reasonable accommodations for qualified individuals with disabilities.\n\nVisit https://www.inmobi.com/company/careers to better understand our benefits, values and more!\n\nThe InMobi Culture\n\nAt InMobi, culture isn't a buzzword; it's an ethos woven by every InMobian, reflecting our diverse backgrounds and experiences.\n\nWe thrive on challenges and seize every opportunity for growth. Our core values of thinking big, being passionate, showing accountability, and taking ownership with freedom guide us in every decision we make.\n\nWe believe in nurturing and investing in your development through continuous learning and career progression with our InMobi Live Your Potential program.\n\nInMobi is proud to be an Equal Employment Opportunity and we make reasonable accommodations for qualified individuals with disabilities.\n\nVisit https://www.inmobi.com/company/careers to better understand our benefits, values, and more!",
        "skills": [
            "Google Cloud Vertex AI",
            "Ad Tech",
            "R",
            "Ai",
            "Ml",
            "Data Science",
            "Apache Spark",
            "Big Data",
            "Microsoft Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Sr Data Scientist",
        "company_name": "Tanla Platforms Limited",
        "experience": "4-10 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "You'll be Responsible for:\nDesign, develop and implement cutting-edge AI/ML solutions, including Large Language Models (LLMs) and Generative AI applications.\nLead projects end-to-end while mentoring team members in AI-ML, including traditional ML and emerging AI technologies\nDrive innovation in AI agent development and orchestration for automated decision-making systems\nEstablish best practices for responsible AI development and deployment\nWhat you'd have:\nAI-ML Engineer or Data Scientist with 4 - 10 years of relevant experience\nStrong expertise in modern AI frameworks and tools:\nML/DL frameworks (TensorFlow, PyTorch, Keras, Sklearn)\nLLM frameworks (LangChain, LlamaIndex, CrewAI, AutoGen)\nVector databases (Pinecone, Weaviate, ChromaDB) Good To Have\nHands-on experience with:\nGenerative AI and foundation models\nPrompt engineering and LLM fine-tuning\nAI agent development and orchestration\nRAG (Retrieval-Augmented Generation) systems\nProven experience in data preparation, including:\nAdvanced data preprocessing techniques\nFeature engineering\nData quality assessment and improvement\nTools & Programming skills:\nProficiency in data exploration tools (Pandas, Numpy, Polars)\nStrong programming skills in Python; R is a plus\nDeep understanding of:\nStatistics and probability\nML/DL algorithms\nAI governance and responsible AI practices\nExperience with:\nLarge-scale data processing\nModel deployment and MLOps\nCost optimization for AI systems\nTrack record of 3-5 successful AI/ML projects in production\nExcellent communication skills and team leadership ability\nContinuous learning mindset for emerging AI technologies\nSkills:\nStatistics and Mathematics\nClassical Machine Learning\nDeep Learning and Neural Networks\nGenerative AI and LLMs\nAgentic AI Development\nMLOps and Production Engineering\nData Engineering\nModel Optimization and Tuning\nAI Governance and Ethics\nTeam Leadership\nWhy join us\nImpactful Work: Drive innovation in AI technologies while safeguarding Tanla's assets, data, and reputation\nGrowth Opportunities: Be part of a rapidly growing company in the telecom and CPaaS space, with clear paths for professional advancement\nInnovative Environment: Work with a world-class team on cutting-edge AI technologies in a challenging and collaborative environment\nResearch Opportunities: Contribute to the advancement of GenAI and Agentic AI technologies\nTanla is an equal opportunity employer. We champion diversity and are committed to creating an inclusive environment for all employees.\nwww.tanla.com",
        "skills": [
            "LangChain",
            "CrewAI",
            "Polars",
            "Pinecone",
            "AutoGen",
            "R",
            "ChromaDB",
            "Weaviate",
            "LlamaIndex",
            "Tensorflow",
            "Sklearn",
            "Numpy",
            "Pandas",
            "Pytorch",
            "Keras",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist (Credit Risk Modelling)",
        "company_name": "Time Hack Consulting",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who are we\n\nWe are revolutionising the lending industry in India, making easy and fast credit available to all salaried employees at the click of a few buttons.\n\nWe're a passionate team backed by seasoned entrepreneurs and ex-bankers, driven by a shared mission to build India's most loved digital lending brand.\n\nWhat will you get\n\nYour compensation will be among the best in the industry\nExciting stock options and a variable pay structure\nFast-track growth opportunitieshow much and how fast it grows depends on your performance and the company's growth\nAs we say, the sky is the only limit!\n\nAre you the right person for the job\n\nMinimum 4+ years of experience in data analytics, preferably in the financial industry\nPreferred coding languages: SQL, R, Python, PySpark\nSolid foundation in classical statistical techniques: Regression, Logistic Regression, Clustering, Dimensionality Reduction, etc.\nProven experience in credit risk modelingincluding acquisition, behavior, and collections scorecards\nHands-on with Machine Learning algorithms such as KNN, Naive Bayes, Decision Trees, CART, Boosting & Bagging models, SVM, Neural Networks, Ensemble models, etc.\nExperience working with large-scale datasets and performing deep root cause analysis\nStrong verbal and written communication skills\n\nWhat will you do\n\nBuild the most loved brand for borrowing money from our customers!\n\nLead analytics initiatives across the companyrisk, marketing, and business strategy\nProvide analytical solutions using statistical modeling and ML\nMonitor, maintain, and enhance credit risk scorecards (Application, Behaviour, Collections, Fraud)\nConduct advanced analytics to understand and reduce portfolio losses\nBuild and deploy predictive models across lending decision areas\nCollaborate with Product, Sales, and Risk teams to understand needs and build solutions\nDesign early warning systems based on segmentation and real-time analytics\nSupport and improve offer generation and distribution using data insights\nLiaise with IT & Credit teams, and external vendors to deliver on analytics use-cases\nOwn delivery of pricing models, risk strategies, and propensity analytics\n\nSkills: bagging,boosting,pyspark,decision trees,python,r,machine learning,cart,svm,credit risk modeling,regression,clustering,neural networks,ensemble models,sql,credit risk,knn,logistic regression,dimensionality reduction,naive bayes",
        "skills": [
            "Credit Risk Modeling",
            "Regression",
            "Ensemble Models",
            "Dimensionality Reduction",
            "R",
            "Boosting",
            "bagging",
            "cart",
            "Logistic Regression",
            "Machine Learning",
            "Pyspark",
            "Neural Networks",
            "Svm",
            "Knn",
            "Clustering",
            "Sql",
            "Naive Bayes",
            "Decision Trees",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - II",
        "company_name": "G2",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About G2 - Our People\n\nG2 was founded to create a place where people will love to work. We strive to create meaning in work and provide more than just a job: a true calling. At the heart of our community and culture are our people. Our global G2 team comes from a wide range of backgrounds and experiences, and that's what makes our G2 community strong and vibrant. We want everyone to bring their authentic selves to work, and we do this through our company and team events, our G2 Gives charitable initiatives, and our Employee Resource Groups (ERGs).\n\nOur employee-led, leadership-supported ERGs celebrate the diversity of our team, foster inclusivity and belonging, and create a space to connect to each other. Through connections and understanding, we build a stronger and more dynamic global team and help every person reach their personal peak.\n\nWe support our employees well-being by providing extensive benefits, including flexible work, aligned time off, and various leave options such as maternity, paternity, and sabbatical leaves. Click here to learn more about our benefits.\n\nAbout G2 - The Company\n\nWhen you join G2, you join the global team behind the largest and most trusted software marketplace. Every month, 5.5 million people come to G2 to inform smarter software decisions based on honest peer reviews. Authenticity is our focus, and every day we help thousands of companies, and hundreds of employees, propel their potential. Ready for meaningful work that starts and ends with compassion and heart You've come to the right place.\n\nG2 is going through exciting growth! We've recently secured our Series D funding of $157 million, which will further allow us to grow and develop our product and people. Read about it here!\n\nAbout The Role\n\nG2 is looking for a Data Scientist - II, your role encompasses leading model development and contributing to machine learning product development. You'll own end-to-end data science workflows, experiment with advanced algorithms, mentor junior team members, and drive innovation within the data science domain. You will work on Improving G2's intent scoring, content moderation, and other AI-driven features through the use of machine learning.This is a hybrid position, with the team meeting in person 2-3 days a week at our Bengaluru office.\n\nIn This Role, You Will\n\nModeling and Statistical Analysis:\n\nIndependently lead the development of machine learning models, owning feature engineering, extraction, model selection, and optimization\nDesign experiments by formulating statistical hypotheses, defining data requirements, pre-processing and cleaning the data, and performing the hypothesis testing.\nOperationalise models at scale applying AI and engineering best practices by working with the ML engineers.\nExperiment with various algorithms and techniques to advance model performance.\nDefine feedback and evaluation methods for the business problems.\nDemonstrate excellent coding and debugging skills.\n\nBusiness, Data Understanding, And Impact\n\nMake impactful contributions by leveraging AI and Machine Learning expertise to address pressing business challenges.\nCollaborate with cross functional teams to understand the business requirements and data architecture.\nTranslate business requirements into technical solutions by working with the business and senior data scientists.\nIdentify and document the data requirements and manage data collection and preparation for projects.\nDesign and document training and testing strategy.\nDocument methodologies, findings, and outcomes of model experiments and present it to the team and key stakeholders.\n\nMentorship And Collaboration\n\nMentor junior team members, providing technical support, guidance on model development, and best practices implementation.\nCoach junior team members, helping them understand complex datasets, models, and business requirements by giving clear and actionable feedback.\nEncourage the development of best practices and innovative approaches in data analysis and modeling.\n\nQualifications\n\n4+ years of experience as a data scientist involved in data extraction, analysis, and modeling.\n4+ years of experience in Python and SQL or related tools for machine learning.\nUnderstanding of statistics and linear algebra.\nProficiency in machine learning algorithms and all stages of machine learning.\nFamiliarity with neural networks and deep learning.\nBasic knowledge about AWS services and cloud databases.\nProficiency in handling structured and unstructured data.\n\nYou would be successful in this role if you describe yourself as:\n\nSuccessful end-to-end delivery of data science products.\nExposure to MLOps tools like MLFlow, KubeFlow, DVC,AWS Sagemaker, Seldon etc\nExperience deploying models in a AWS cloud environment - with specific experience with AWS tools such as Sagemaker and Step Functions.\nExpertise with Natural Language Processing and Understanding.\nExperience with libraries and frameworks for training ML and DL models (PySpark, Tensorflow).\nExperience and expertise in ML Operations best practices.\n\nOur Commitment to Inclusivity and Diversity\n\nAt G2, we are committed to creating an inclusive and diverse environment where people of every background can thrive and feel welcome. We consider applicants without regard to race, color, creed, religion, national origin, genetic information, gender identity or expression, sexual orientation, pregnancy, age, or marital, veteran, or physical or mental disability status.\n\nLearn more about our commitments here Commitments",
        "skills": [
            "KubeFlow",
            "Seldon",
            "DVC",
            "AWS Sagemaker",
            "MLFlow",
            "Machine Learning",
            "Natural Language Processing",
            "Pyspark",
            "Neural Networks",
            "Sql",
            "Deep Learning",
            "Tensorflow",
            "MLops",
            "Python",
            "Statistical Analysis",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "LSEG (London Stock Exchange Group)",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "The Emerging Tech Standard Delivery Team will orchestrate adoption of new technologies and building Enterprise Solutions for D&A Operations. The individual will work closely with Specialists, Operations & Technology delivery teams to devise Enterprise solutions that deliver business and customer value. The role will manage stakeholders and co-ordinate with teams highly proficient in data science, machine learning, database development and front-end application engineering. The individual needs to self-learn in areas including emerging technology, industry methods of working, financial content, and business processes used across D&A teams. Beyond this, they should have a good sense of numbers and data analysis and be articulate and able to connect with technical and non-technical people.\n\nThe Lead Data Scientist will work closely with Technology to influence the D&A Operations technology roadmap and will partner with D&A Ops leaders to build a culture that cultivates Innovation, speed, and quality.\n\nKey areas of focus for the role include:\n\nProduct Owner- Be the product owner for Enterprise Extraction solution and work with D&A Operations Leads and their teams to triage use cases based on Business benefit.\nTechnology & Practices: Supports building and deploying of Enterprise solutions using emerging technology, data science, machine learning and developer toolkits in an Agile manner that delivers value for D&A Operations.\nBusiness Expertise: Knowledge with the latest trends regarding Enterprise Solutions.\nStrategic Thinking: Strategic thinker required in this role with a need to innovate and build Enterprise level solutions to accelerate automations across D&A.\nCulture Building: Works in partnership with Operations/Specialist Leaders to build knowledge, awareness, and organizational capabilities that enable innovation at scale.\nMentoring & Guidance: Mentor Data Science specialists ensuring deep expertise and rapid pace of delivery\n\nKey Responsibilities\n\nTechnology & Business Practices\n\nSupport the development of Enterprise Solutions, MVP to test the applicability of emerging tech and ways of working.\nBe the champion & product owner of enterprise extraction solution and work closely with D&A Ops for triaging and solutioning of use cases.\nLead the design and development of Enterprise Solutions, capabilities powered by AI/ML models.\nSecure access to new data sources and technology for LSEG Operations in anticipation future needs to meet GSOs and DSOs\n\nCulture Building\n\nCreate an Operations culture of innovation that support Ops strategic focus areas removing perceived barriers to execution.\nDrive MVE team development to improve efficiency and expand Ops capabilities.\nWork with communities of practice for AI/ML and other emerging technologies and methods of working.\nCollaborate with D&A, Tech Ops & other stakeholders to influence technology roadmaps.\n\nMentor\n\nMentor & guide the Data scientist team of AI/ML, adoption of industry leading practices and realization of business outcomes.\n\nKey Behaviours:\n\nContinuously adopt a pragmatic, flexible, and responsive approach.\nAbility to priorities critical tasks whilst simultaneously managing competing demands.\nManage stakeholder engagements internally and externally to discover problems and use cases, as well as to translate business impact of on-going and completed projects.\nCreates compelling proposals with technologists and business to drive innovation from conception to production with appropriate success metrics.\nUse deep domain expertise in financial market content, data science and LSEG data and products.\nUnderstand and staying aware of leading-edge techniques, tools and promote industry leading practices.\nDevelop and implement operational strategy for the Tech Standard team.\nProvide input and challenge for resolution of key issues that directly impact business.\nContribute to development of wider Customer Operations strategies.\nAddress complex business issues beyond immediate needs or where established policies can be adapted.\n\nEssential\n\nCandidate Profile / Key Skills\n\nGood interpersonal and communication (verbal and written) skills and demonstrable ability to work effectively with all organisational levels.\nDemonstrable experience working with AI/ML professionals & Tech Ops teams.\nExperience managing full stack data scientists for 5-7 years.\nGood understanding of Python programming concepts and leading practices and Solid SQL skills.\nAbility to translate business, customers and end-users requirements to this into technical specifications, solutions and products, and metrics.\nExtensive knowledge of MLOps approaches.\nExtensive experience in Cloud computing.\nEntrepreneurial thinker with the ability to take a problem statement, gather data and propose new / creative solutions.\nEducated to degree level or equivalent.\n\nPreferred\n\nExperience in large investment banking or financial services organisations.\nFamiliar with Refinitiv or LSEG processes and products.\nExperience working with executives and / or C-Suite individuals.\nExperience working with a global team.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",
        "skills": [
            "Ai",
            "Emerging technology",
            "Ml",
            "Cloud Computing",
            "MLops",
            "Data Science",
            "Agile",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - Stats",
        "company_name": "C5i",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Summary:\nAs a part of the data science team of C5i, we are looking for a Data Scientist who is diligent, detail-driven, and ambitious to provide the SME expertise, design solutions and best practices to lead the projects for our partners across the globe for all AI implementations. This position will require a strong background and working experience in developing scalable, sustainable and on prem agnostic data marts/lakes to enable effective deployment of AI algorithms developed to solve custom problems in the partner ecosystems for sandboxes as well as for the production deployments for continuous modeling, testing, improvement and deployment. This role expects you to be a self-starter and ability to handle ambiguity with respect to the expectations, objective, data and would require discrete and constant attention to details.\nJob Responsibilities:\nDevelop clear, concise, actionable solutions and recommendations for Client's business needs.\nWork with complex data files and develop initial understanding on data files.\nUndertake exploratory data analysis to derive initial findings and create hypothesis.\nWork on problem specific data models.\nUndertake hands on work on data analytics, model development and testing and preparing the data files for visualization platforms.\nUndertake business analysis on the data and provide insights.\nCoordinate with decision makers to translate business questions into a verifiable hypothesis and data models.\nWork hands-on across various analytics problems and provide thought leadership on problems.\nInteract with other stakeholders (Data engineers, BI specialists, offsite team, client teams) on daily/weekly basis to gather requirements/ provide updates.\nRequirements & Qualifications:\n3-6 years of experience, having worked on developing / implementing and operationalizing data / analytics use cases.\nGraduation or Post graduation in Statistics, Maths, Management etc.\nGood understanding of statistics along with analysis techniques like Regression, Cluster, Factor, Conjoint etc.\nMust have worked with on complex data science models.\nHands on experience in PowerPoint / Excel is a must with experience in Tableau / BI an addon.\nHands on experience on SQL, R or Python coding (any one).\nStrong logical, analytical, and problem-solving skills.\nExcellent verbal and written communication skills.",
        "skills": [
            "Tableau BI",
            "R",
            "Powerpoint",
            "Regression",
            "Python",
            "Sql",
            "Excel"
        ]
    },
    {
        "job_title": "Senior Data Scientist_Experimentation",
        "company_name": "Lowe's India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Lowe's\n\nLowe's Companies, Inc. (NYSE: LOW) is a FORTUNE 50 home improvement company serving approximately 16 million customer transactions a week in the United States. With total fiscal year 2023 sales of more than $86 billion, Lowe's operates over 1,700 home improvement stores and employs approximately 300,000 associates. Based in Bengaluru, Lowe's India develops innovative technology products and solutions and delivers business capabilities to provide the best omnichannel experience for Lowe's customers. Lowe's India employs over 4,200 associates across technology, analytics, merchandising, supply chain, marketing, finance and accounting, product management and shared services. Lowe's India actively supports the communities it serves through programs focused on skill-building, sustainability and safe homes. For more information, visit, www.lowes.co.in.\n\nAbout The Team\n\nThe Experimentation Team at Lowe's drives end-to-end A/B testing, ensuring data-backed decisions before full rollouts. This progressive and rapidly growing team is focused on building scalable solutions that enhance decision-making and drive top and bottom-line business value along with creating great customer experiences.\n\nJob Summary\n\nWe are seeking an inspiring, technically savvy, data scientist who is passionate about building a best-in-class experimentation platform/program to support our rapidly growing suite of eCommerce products.\n\nAs a Senior Data Scientist, you will be a key player in driving data-driven decision-making across the organization, collaborating closely with engineering, product, marketing, and other cross-functional teams to deliver insights and products that shape the future of our business. You will also mentor junior data scientists and help to foster a culture of experimentation throughout the organization.\n\nRoles & Responsibilities\n\nCore Responsibilities:\n\nExperimentation Design & Analysis\n\nSupport the design and execution of A/B tests, multivariate experiments, and randomized controlled trials (RCTs) to assess the impact of product changes, marketing campaigns, and customer experiences.\nDevelop and implement robust methodologies to measure the effectiveness of business initiatives (e.g., website features, promotions, UI changes, etc.) using experimentation frameworks.\nOwn the end-to-end experimentation pipeline, including hypothesis generation, experimental design, implementation, monitoring, and post-experiment analysis.\nIdentify and mitigate biases in experiment design and results, ensuring statistical rigor and reliability.\n\nAdvanced Statistical Analysis & Modelling\n\nConduct advanced statistical analysis (e.g., causal inference, Bayesian analysis, regression modelling) to derive actionable insights from experimentation results.\nDevelop and refine models to predict customer behavior and optimize conversion rates, retention, and other key business metrics.\nAnalyze large-scale datasets and design efficient algorithms to support decision-making in areas like pricing, product recommendations, and personalization.\n\nContinuous Improvement & Innovation\n\nStay current with the latest advancements in data science, statistics, and experimentation methodologies.\nPropose innovative approaches to enhance the experimentation framework, such as new experimental designs, alternative modelling techniques, or improved metrics.\nLead or participate in research to explore new ways of measuring and optimizing the customer journey in a retail/e-commerce setting.\n\nYears Of Experience\n\n5+ years of professional experience in data science, with at least 2 years focused on experimentation, A/B testing, and causal inference in a retail or e-commerce environment.\nProven track record of designing and analyzing large-scale A/B tests and experiments with demonstrable business impact.\nStrong experience with statistical analysis and modelling techniques, including hypothesis testing, regression analysis, and Bayesian statistics\n\nEducation Qualification & Certifications (optional)\n\nRequired Minimum Qualifications\n\nPh.D. or master's degree in Data Science, Statistics, Mathematics, Computer Science, Economics, or a related field.\n\nSkill Set Required\n\nAdvanced knowledge of statistical methodologies for experiment design, analysis, and causal inference.\nExpertise in analytics/data software/tools such as Python, R, SQL, and experience with machine learning frameworks (e.g., TensorFlow, scikit-learn).\nStrong communication skills, with the ability to explain complex technical concepts to non-technical stakeholders and executive leadership.\nSolid understanding of e-commerce and retail metrics (e.g., conversion rate, customer lifetime value, churn, etc.) and how they relate to experimentation.\n\nSecondary Skills (desired)\n\nExperience with large-scale e-commerce platforms and digital product development.\nFamiliarity with the advanced causal and inferential analytics\nExperience with advanced techniques in machine learning or AI that complement experimentation (e.g., recommender systems, predictive modelling).\nFamiliarity with cloud-based platforms (e.g., AWS, Google Cloud, Azure).\nExperience working in an agile environment and collaborating with cross-functional teams in a fast-paced business setting.\n\nLowe's is an equal opportunity employer and administers all personnel practices without regard to race, color, religious creed, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law.\n\nStarting rate of pay may vary based on factors including, but not limited to, position offered, location, education, training, and/or experience. For information regarding our benefit programs and eligibility, please visit https://talent.lowes.com/us/en/benefits.",
        "skills": [
            "R",
            "statistical methodologies",
            "scikit-learn",
            "Bayesian Statistics",
            "causal inference",
            "Regression Analysis",
            "Tensorflow",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ford Motor Company",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nEmployees in this job function are responsible for designing, developing, testing and maintaining software applications and products to meet customer needs both on-prem and cloud native. They are involved in the entire software development lifecycle including designing software architecture, writing code, testing for quality and deploying the software to meet customer requirements. Full-stack software engineering roles, who can develop all components of software including user interface and server side also fall within this job function.\n\nWhat you'll be able to do:\n\nThe Software Engineer will work on a Balanced Product Team and collaborate with the Product Manager, Product Designer, and other Software Engineers to deliver analytic solutions. The Software Engineer will be responsible for the development and ongoing support/maintenance of the analytic solutions.\n\nProduct And Requirements Management: Participate in and/or lead the development of requirements, features, user stories, use cases, and test cases. Participate in stand-up operations meetings.\nAuthor: Process and Design Documents\nDesign/Develop/Test/Deploy: Work with the Business Customer, Product Owner, Architects, Product Designer, Software Engineers, and Security Controls Champion on solution design, development, and deployment.\nOperations: Generate Metrics, Perform User Access Authorization, Perform Password Maintenance, and Build Deployment Pipelines.\nIncident, Problem, And Change/Service Requests: Participate and/or lead incident, problem, change and service request-related activities. Includes root cause analysis (RCA). Includes proactive problem management/defect prevention activities.\n\nResponsibilities\n\n\nThe minimum requirements we seek:\n\n5+ years experience in Software Engineering.\nBachelor's degree in computer science, computer engineering or a combination of education and equivalent experience.\n1+ year experience with developing for and deploying to cloud platforms (e.g. GCP, Azure)\nImplement and optimize cloud services and tools (e.g. Terraform, BigQuery, GCP)\nExperience in development using combination of the following technologies:\nLanguages: Java / JS / TS / Python\nFrontend frameworks: Angular / React\nBackend frameworks: Spring / Node\nProven experience understanding, practicing, and advocating for software engineering disciplines from Clean Code, Software Artmanship, and Lean including:\nPaired / Mobbing programming\nTest-first/Test Driven Development (TDD)\nEvolutionary design\nMinimum Viable Product\nWillingness to collaborate daily with team members.\nA strong curiosity around how to best use technology to amaze and delight our customers\nUsing CI/CD tools and pipelines e.g. Tekton, GIT Action, Cloud Build, etc.\n\nQualifications\n\n\nOur preferred qualifications:\n\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders\nDelivered products that include web front-end development; JavaScript, CSS, frameworks like Angular, etc.\nComfortable with Continuous Integration/Continuous Delivery tools and pipelines e.g. Tekton, Cloud Build, etc.\nExperience with machine learning, mathematical modeling, and data analysis is a plus\nExperience with CA Agile Central (Rally, JIRA), backlogs, iterations, user stories, or similar Agile Tools\nExperience in the development of microservices\nUnderstanding of fundamental data modeling\nStrong analytical and problem-solving skills",
        "skills": [
            "Tekton",
            "GIT Action",
            "Cloud Build",
            "Java",
            "BigQuery",
            "Node",
            "Spring",
            "Angular",
            "React",
            "Javascript",
            "Gcp",
            "Terraform",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - AI",
        "company_name": "Standard Chartered India",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Summary\n\nWe are looking for a Data Scientist with strong data and analytical skills, you will be tasked with managing all aspects any new work/project assessments thoroughly from inception to delivery. You will need to build close working relationships with COO business and CDO in order to gather specific modelling requirements and define and scope the work. In addition, as Data Scientist you will also work closely with internal staff, clients and 3rd parties.\n\nKey Responsibilities\n\nDesign Machine Learning, Natural Language and Decision Optimisation applications architecture.\nDesign ML, NLP and DO models, algorithms for predictive & prescriptive analytics.\nTranslate business requirements to reporting dashboard and analytics.\nPerform advanced analytics and statistical modelling on structured and unstructured data.\nWork with software engineers to integrate and deploy AI applications.\nAssist in the creation of data definitions for new database file/table development and/or changes to existing ones as needed for analysis.\nSupport the business with clear and insightful analysis applying advanced modelling techniques leveraging the data at hand.\nRespond to and resolve data mining performance issues.\nMonitor data mining system performance and implement efficiency improvements.\nDevelop and implement change management plans to ensure successful adoption of AI solutions across the organization.\nProvide training and support to business users to help them understand and leverage AI tools and technologies.\nFoster a culture of innovation and continuous improvement by promoting the benefits of AI and encouraging experimentation.\nWork with senior management to develop and refine the bank's AI strategy, ensuring alignment with overall business objectives.\nIdentify and prioritize AI use cases that have the potential to deliver significant business value.\nDevelop business cases for AI projects, including cost-benefit analysis, risk assessment, and ROI estimation.\n\nSkills And Experience\n\n8+ Years of Software development experience using .Net framework or Java\nCompleted real world ML, NLP and DO projects using R or Python\nExtensive experience in SQL and NoSQL database design, queries and stored procedures\nHands-on experience with Windows Server, Azure, AWS and Git\nHighly proficient in data visualisation tools...\nDocumentation - Requirements/Use Cases/Business Rules/User Stories, Etc.\nReport Types - Gap Analysis/Problem Analysis/Initial Assessments, Etc.\nProcess/Data Modelling - As Is/To Be/ Visio/Enterprise Architect/System Architect, Etc.\nStrong query language skills (SQL, Hive, ETL, Hadoop, Spark, R, Python)\nGood experience with Business Intelligence tools and Decision Support Systems\nStrong data analysis skills using Hive, Spark, R, Python, Dremio, MicroStrategy and Tableau.\nProven experience in working with key stakeholders within the business.\nProven problem-solving skills\nWorkshop Facilitation\n\nQualifications\n\nMaster or PHD in Mathematics, Statistics, Knowledge Engineering or Data science.\n\nCompleted real world ML, NLP and DO projects using R or Python\n\nAbout Standard Chartered\n\nWe're an international bank, nimble enough to act, big enough for impact. For more than 170 years, we've worked to make a positive difference for our clients, communities, and each other. We question the status quo, love a challenge and enjoy finding new opportunities to grow and do better than before. If you're looking for a career with purpose and you want to work for a bank making a difference, we want to hear from you. You can count on us to celebrate your unique talents and we can't wait to see the talents you can bring us.\n\nOur purpose, to drive commerce and prosperity through our unique diversity, together with our brand promise, to be here for good are achieved by how we each live our valued behaviours. When you work with us, you'll see how we value difference and advocate inclusion.\n\nTogether We\n\nDo the right thing and are assertive, challenge one another, and live with integrity, while putting the client at the heart of what we do\nNever settle, continuously striving to improve and innovate, keeping things simple and learning from doing well, and not so well\nAre better together, we can be ourselves, be inclusive, see more good in others, and work collectively to build for the long term\n\nWhat We Offer\n\nIn line with our Fair Pay Charter, we offer a competitive salary and benefits to support your mental, physical, financial and social wellbeing.\n\nCore bank funding for retirement savings, medical and life insurance, with flexible and voluntary benefits available in some locations.\nTime-off including annual leave, parental/maternity (20 weeks), sabbatical (12 months maximum) and volunteering leave (3 days), along with minimum global standards for annual and public holiday, which is combined to 30 days minimum.\nFlexible working options based around home and office locations, with flexible working patterns.\nProactive wellbeing support through Unmind, a market-leading digital wellbeing platform, development courses for resilience and other human skills, global Employee Assistance Programme, sick leave, mental health first-aiders and all sorts of self-help toolkits\nA continuous learning culture to support your growth, with opportunities to reskill and upskill and access to physical, virtual and digital learning.\nBeing part of an inclusive and values driven organisation, one that embraces and celebrates our unique diversity, across our teams, business functions and geographies - everyone feels respected and can realise their full potential.",
        "skills": [
            "Dremio",
            "R",
            ".NET Framework",
            "Microstrategy",
            "data visualisation tools",
            "Java",
            "Hadoop",
            "Windows Server",
            "Tableau",
            "Sql",
            "Nosql",
            "Git",
            "Hive",
            "Spark",
            "Azure",
            "Python",
            "AWS",
            "Etl"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Bluevine",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Bluevine\n\nBluevine is transforming small business banking with innovative solutions like checking, lending, and creditall tailored to help entrepreneurs thrive. With best-in-class technology, advanced security, and a deep understanding of the small business community, we're empowering entrepreneurs to grow with confidence.\n\nBacked by leading investors like Lightspeed Venture Partners, Menlo Ventures, 83North, and Citi Ventures, we've been supporting SMBs since 2013, serving over 500,000 customers nationwide and growing a dynamic global team of 500 people. Our mission To fuel small businesses with the financial tools they need to succeed.\n\nAt Bluevine, you'll be part of a collaborative, fast-paced team that's reshaping the future of banking. Ready to make an impact\n\nAbout the Role:\n\nWe are building a new, cutting-edge data science team in India and are looking for an experienced Senior Data Scientist to join us. This role is an exciting opportunity to apply your expertise in advanced statistical techniques, machine learning, and predictive modeling to solve complex business problems. You will be instrumental in shaping the data science function, developing scalable models, and collaborating with teams across the globe.\n\nWhat You'll Do:\n\nAdvanced Modeling & Analytics: Utilize advanced statistical, machine learning, and predictive modeling techniques to design, build, and improve decision-making systems that drive business outcomes.\nScalable Model Implementation: Apply modern software engineering practices to implement machine learning models in a scalable, robust, and maintainable manner. Ensure models are production-ready and optimized for performance.\nMLOps & System Integration: Lead technical MLOps projects including the redesign of existing infrastructures, maintaining and improving current models and systems, and integrating the latest technologies into our data science workflows.\nData Collection & Management: Manage and lead initiatives to design and implement robust data collection procedures necessary for building comprehensive analytical systems.\nCollaboration & Leadership: Work closely with Data Scientists, the R&D department, and cross-functional teams across India, the US, and Israel to drive data science initiatives. Provide mentorship to junior data scientists and contribute to the growth of the team.\n\nWhat We're Looking For:\n\n5+ Years of Experience: A minimum of 5 years of professional experience as a Data Scientist, with a proven track record of developing and deploying machine learning models in a production environment.\nProficiency in Python: Extensive experience in developing and deploying scalable production-level code in Python, with a deep understanding of machine learning libraries such as Scikit-learn, PyTorch, TensorFlow, or similar.\nDatabase Expertise: Strong experience working with relational databases, writing optimized SQL queries, and managing large datasets.\nEnd-to-End ML Lifecycle: Comprehensive experience in the full lifecycle of ML model development, including ideation, training/testing, deployment, and monitoring in a production setting.\nData Analysis & Insight Generation: Ability to analyze complex datasets and extract actionable insights that inform business decisions.\nEducational Background: A Master's or Ph.D. in Statistics, Computer Science, Engineering, or a related quantitative discipline.\nCommunication Skills: Proven ability to communicate complex technical concepts and findings to non-technical stakeholders, with a focus on clear and actionable insights.\n\nBonus Points:\n\nCross-Functional Experience: Experience working cross-functionally with Data/MLOps Engineering, Analytics, Business, and Product teams.\nDomain Expertise: Previous experience in specialized areas such as fraud detection, credit risk prediction, or graph models and analytics.\nMLOps & Cloud Experience: Hands-on experience with MLOps, particularly within the AWS ecosystem, and familiarity with tools like Apache Airflow.\nExperimental Design: Expertise in designing and executing experiments to measure model impact and effectiveness.\n\nBenefits & Perks\n\nExcellent group health coverage and life insurance\nStock options\nHybrid work model\nMeal allowance\nTransportation assistance (terms and conditions apply)\nGenerous paid time off plan, Holidays\nCompany-sponsored mental health benefits\nFinancial advisory services for both short- and long-term goals\nLearning and development opportunities to support career growth\nCommunity-based volunteering opportunities",
        "skills": [
            "Scikit-learn",
            "Data Analysis",
            "Tensorflow",
            "Predictive Modeling",
            "Machine Learning",
            "MLops",
            "Pytorch",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Kredivo Group",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "As a Data Scientist you'll be part of a team that is working on cutting edge machine learning models. You will work side by side with Analysts and Machine Learning Engineers and take full ownership of your work from the initial idea-generation phase to the implementation of the final product. Our ideal candidate is result-focused, innovative, has a solid quantitative background and a good business understanding.\n\nAbout The Role\n\nWorking in a multi-disciplined team where you'll take full ownership of turning discoveries and ideas into machine learning models.\nPrototyping diverse machine learning models, from credit scoring to recommendation engine.\nResearching & Development on how to improve our models, by using our vast amount and unique set of data.\nActively contribute to taking Data Science at Kredivo to the next level.\n\nAbout You\n\nMinimum of 5 years of hands-on experience in risk modeling within the financial services industry or other similar industries and at least 2 years experience in managing a team.\nStrong domain expertise and solid understanding of financial products, risk metrics, and regulatory requirements.\nUnderstand risk modeling best practices and capable of delivering end-to-end, robust and impactful data products.\nProven track record of delivering high-quality data products in a fast-paced, dynamic environment.\nExcellent problem-solving skills with a demonstrated ability to think analytically and strategically.\nStrong communication skills with the ability to convey complex concepts to both technical and non-technical audiences.\n\nBonus Points (optional) For\n\nMasters, PhD or equivalent experience in a quantitative field (i.e, Mathematics, Statistics, Econometrics, Artificial Intelligence, Physics).\nExperience working with large-scale datasets and distributed computing frameworks (e.g., Spark).",
        "skills": [
            "Financial Products",
            "large-scale datasets",
            "risk modeling",
            "risk metrics",
            "distributed computing frameworks",
            "Regulatory Requirements",
            "Machine Learning",
            "data products",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Prescience Decision Solutions",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Conduct product assortment analysis, identify trends and patterns in category performance, and support new product launches.\nUtilize data and visualization techniques to provide actionable insights for store teams, territory, and district managers.\nDesign and test campaign effectiveness, analyze cross-sell and up-sell opportunities, and perform Market Basket Analysis.\nAnalyze loyalty and customer data to design direct marketing campaigns and measure their effectiveness.\nTranslate complex data insights into understandable and actionable information for non-technical and business teams.\n\nRequirements\n\n3+yeras of Proven experience as a Data Scientist or similar role.\n\nStrong knowledge of data analysis, statistics, and machine learning.\n\nProficiency in data science tools and programming languages such as Python,SQL, etc.\n\nFamiliarity with:\n\nSupervised and unsupervised learning techniques\nDeep learning and reinforcement learning,clustering,machine learning, statistics and data mining to solve clustering, classification, regression problems\nEvaluation metrics, feature engineering, model selection and validation, ensemble methods, and explainable AI\nGood to have experience in visualization tools such as Power BI/Tableau\nExcellent storytelling skills to explain complex data to non-technical/business teams\nExperience in analytics, preferably within Retail, CPG, or E-commerce domains.\nExcellent communication and collaboration skills with the ability to work effectively in a team environment.\n\nBenefits\n\nCompetitive salary and performance-based bonuses.\nComprehensive insurance plans.\nCollaborative and supportive work environment.\nChance to learn and grow with a talented team.\nA positive and fun work environment.",
        "skills": [
            "Evaluation metrics",
            "Validation",
            "Data Analysis",
            "reinforcement learning",
            "model selection",
            "Statistics",
            "Ensemble methods",
            "Feature engineering",
            "supervised learning",
            "Explainable AI",
            "Power Bi",
            "Tableau",
            "Unsupervised Learning",
            "Data mining",
            "Clustering",
            "Sql",
            "Deep Learning",
            "Machine Learning",
            "Python"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "Zepto",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "The ideal candidate's favourite words are data, scale and agility !\nWe're looking for a highly skilled and passionate Data Scientist to tackle complex challenges, develop innovative solutions, and provide strategic insights that shape our products and user experiences\nResponsibilities\nAnalyse raw data: assessing quality, cleansing, structuring for downstream processing\nDesign accurate and scalable prediction algorithms\nConduct rigorous exploratory data analysis (EDA) to uncover patterns, trends, and opportunities for optimisation\nGenerate actionable insights for business improvements\nQualifications\nMaster's or Ph.D. degree in a quantitative field such as Computer Science, Statistics, Mathematics, Economics, or a related discipline\nAt-least 5 years of professional experience with deep understanding of data mining / pipelining predictive modeling, model serving, machine-learning, clustering and classification techniques, and algorithms\nProven ability to develop AI solutions to improve business and customer metrics\nFamiliarity with Big Data frameworks and recommendation systems\nIf you thrive in a fast-paced environment and are eager to make a tangible difference, we encourage you to apply.",
        "skills": [
            "machine-learning",
            "classification techniques",
            "Big Data frameworks",
            "pipelining",
            "recommendation systems",
            "model serving",
            "Predictive Modeling",
            "data mining",
            "Clustering"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "DATAECONOMY",
        "experience": "3-10 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description:\n\nWe are looking for a highly skilled SeniorData Scientist with 3-9 years of experience specializing in Python, Large Language Models (LLMs), NLP, Machine Learning, and Generative AI. The ideal candidate will have a deep understanding of building intelligent systems using modern AI frameworks and deploying them into scalable, production-grade environments. You will work closely with cross-functional teams to build innovative AI solutions that deliver real business value.\nResponsibilities:\nDesign, develop, and deploy ML/NLP solutions using Python and state-of-the-art AI frameworks.\nApply LLMs and Generative AI techniques to solve real-world problems.\nBuild, train, fine-tune, and evaluate models for NLP and GenAI tasks.\nCollaborate with data engineers, MLOps, and product teams to operationalize models.\nContribute to the development of scalable AI services and applications.\nAnalyze large datasets to extract insights and support model development.\nMaintain clean, modular, and version-controlled code using Git.\n\nRequirements\n\nMust-Have Skills:\n3-10 years of hands-on experience with Python for data science and ML applications.\nStrong expertise in Machine Learning algorithms and model development.\nProficient in Natural Language Processing (NLP) and text analytics.\nExperience with Large Language Models (LLMs) and Generative AI frameworks (e.g., LangChain, Hugging Face Transformers).\nFamiliarity with model deployment and real-world application integration.\nExperience with version control systems like Git.\nGood to Have:\nExperience with PySpark for distributed data processing.\nExposure to MLOps practices and model lifecycle management.\nFamiliarity with cloud platforms such as AWS, GCP, or Azure.\nKnowledge of vector databases (e.g., FAISS, Pinecone) and embeddings.\nEducational Qualification:\nBachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related field.\n\nBenefits\n\nWork with cutting-edge technologies in a collaborative and forward-thinking environment.\nOpportunities for continuous learning, skill development, and career growth.\nExposure to high-impact projects in AI and data science.",
        "skills": [
            "Generative AI",
            "FAISS",
            "Pinecone",
            "Git",
            "Machine Learning",
            "Gcp",
            "MLops",
            "Pyspark",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist (Generative AI)",
        "company_name": "Philips Ambulatory Monitoring & Diagnostics",
        "experience": "10-12 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title\n\nData Scientist (Generative AI)\n\nJob Description\n\nJob title:\n\nData Scientist (Generative AI)\n\nYour Role\n\nWe are seeking a highly skilled Data Scientist with Generative AI expertise to join our team. In this role, you will develop, fine-tune, and optimize generative AI models to drive innovation across various applications, including text, image, audio, and video generation. You will collaborate with cross-functional teams, including machine learning engineers, software developers, and product managers, to create cutting-edge AI solutions.\n\nYou're The Right Fit If\n\nBachelor's or master's degree in computer science, AI, Data Science, Machine Learning, or a related field.\n10+ years of experience in machine learning, deep learning, or AI research, with at least 1 year of hands-on experience in generative AI.\nStrong proficiency in Python and ML frameworks like TensorFlow, PyTorch, or JAX.\nExperience working with LLMs, diffusion models, GANs, VAEs, or transformers.\nKnowledge of natural language processing (NLP), computer vision, or multimodal AI applications.\nFamiliarity with prompt engineering, fine-tuning, and RLHF (Reinforcement Learning from Human Feedback).\nExperience in cloud-based AI solutions and working with APIs (e.g., OpenAI, Hugging Face, Stability AI).\nStrong problem-solving skills and the ability to work in a fast-paced, research-driven environment.\nExperience with vector databases (e.g., FAISS, Pinecone) and retrieval-augmented generation (RAG).\nHands-on experience with MLOps tools (e.g., MLflow, Kubeflow, Docker, Kubernetes).\nUnderstanding of ethical AI and bias mitigation in generative models.\nStrong publication record or contributions to open-source AI projects.\n\nHow We Work Together\n\nWe believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.\n\nOnsite roles require full-time presence in the company's facilities.\n\nField roles are most effectively done outside of the company's main facilities, generally at the customers or suppliers locations.\n\nIndicate if this role is an office/field/onsite role.\n\nAbout Philips\n\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.\n\nLearn more about our business.\nDiscover our rich and exciting history.\nLearn more about our purpose.\n\nIf you're interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion here.",
        "skills": [
            "Prompt engineering",
            "LLMs",
            "Fine-tuning",
            "GANs",
            "Bias mitigation",
            "Ethical AI",
            "VAEs",
            "Cloud-based AI solutions",
            "Multimodal AI applications",
            "Diffusion models",
            "Transformers",
            "Computer Vision",
            "Tensorflow",
            "Jax",
            "Pytorch",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "HiLabs",
        "experience": "3-5 Years",
        "salary": "INR 33 - 37 LPA ",
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "The HiLabs Story\n\nHiLabs is a leading provider of AI-powered solutions to clean dirty data, unlocking its hidden potential for healthcare transformation. HiLabs is committed to transforming the healthcare industry through innovation, collaboration, and a relentless focus on improving patient outcomes.\n\nHiLabs Team\n\nMultidisciplinary industry leaders\nHealthcare domain experts\nAI/ML and data science experts\nProfessionals hailing from the worlds best universities, business schools, and engineering institutes including Harvard, Yale, Carnegie Mellon, Duke, Georgia Tech, Indian Institute of Management (IIM), and Indian Institute of Technology (IIT).\n\nJob Title: Senior Data Scientist\n\nJob Location: Bangalore, Karnataka\n\nJob summary: HiLabs is looking for highly motivated and skilled Lead/Sr. Data Scientist focused on the application of emerging technologies. The candidates must be well versed with Python, Scala, Spark, SQL and AWS platform. The individuals who will join the new Evolutionary Platform team should be continually striving to advance AI/ML excellence and technology innovation. The mission is to power the next generation of the digital product and services through innovation, collaboration, and transparency. You will be a technology leader and doer who enjoys working in a dynamic, fast- paced environment.\n\nResponsibilities\n\nLeverage AI/ML techniques and solutions to identify and mathematically interpret complex healthcare problems.\nFull-stack development of data pipelines involving Big Data.\nDesign and development of robust application/data pipelines using Python, Scala, Spark, and SQL\nLead a team of Data Scientists, developers as well as clinicians to strategize, design and evaluate AI based solutions to healthcare problems.\nIncrease efficiency and improve the quality of solutions offered.\nManaging the complete ETL pipeline development process from conception to deployment\nCollaborating with and guiding the team on writing, building, and deployment of data software\nFollowing best design and development practices to ensure high quality code.\nDesign, build and maintain efficient, secure, reusable, and reliable code\nPerform code reviews, testing, and debugging\n\nDesired Profile\n\nBachelor's or Master's degrees in computer science, Mathematics, or any other quantitative discipline from Premium/Tier 1 institutions\n3 to 5 years of experience in developing robust ETL data pipelines and implementing advanced AI/ML algorithms (GenAI is a plus).\nStrong experience working with technologies like Python, Scala, Spark, Apache Solr, MySQL, Airflow, AWS etc.\nExperience working with Relational databases like MySQL, SQLServer, Oracle etc.\nGood understanding of large system architecture and design\nUnderstands the core concepts of Machine Learning and the math behind it.\nExperience working in AWS/Azure cloud environment\nExperience using Version Control tools such as Bitbucket/GIT code repository\nExperience using tools like Maven/Jenkins, JIRA\nExperience working in an Agile software delivery environment, with exposure to continuous integration and continuous delivery tools\nGreat collaboration and interpersonal skills\nAbility to work with team members and lead by example in code, feature development, and knowledge sharing\n\nHiLabs is an equal opportunity employer (EOE). No job applicant or employee shall receive less favorable treatment or be disadvantaged because of their gender, marital or family status, color, race, ethnic origin, religion, disability, or age; nor be subject to less favorable treatment or be disadvantaged on any other basis prohibited by applicable law.\n\nHiLabs is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse and inclusive workforce to support individual growth and superior business results.\n\nThank you for reviewing this opportunity with HiLabs! If this position appears to be a good fit for your skillset, we welcome your application.\n\nHiLabs Total Rewards\n\nCompetitive Salary, Accelerated Incentive Policies, H1B sponsorship, Comprehensive benefits package that includes ESOPs, financial contribution for your ongoing professional and personal development, medical coverage for you and your loved ones, 401k, PTOs & a collaborative working environment, Smart mentorship, and highly qualified multidisciplinary, incredibly talented professionals from highly renowned and accredited medical schools, business schools, and engineering institutes.\n\nCCPA disclosure notice - https://www.hilabs.com/privacy",
        "skills": [
            "Airflow",
            "Jenkins",
            "Apache Solr",
            "MySQL",
            "Maven",
            "Scala",
            "Spark",
            "JIRA",
            "Sql",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Digitalapi.ai",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About DigitalAPICraft\n\nDigitalAPICraft(DAC)is a fast-growing product company and a preferred Google ISV partner. We are one of the Top 10 Companies in Deloitte Technology Fast 50 Companies India, 2018 and recently recognised in the Gartner Market guide as one of the leading API portal providers. DAC offers an AI-enabled API Marketplace platform that empowers enterprises to maximize the strategic value of their digital ecosystems. Born from the passion and expertise of a team of tech enthusiasts, we have emerged as a global force in the API and cloud engineering space. With our global presence, we serve clients across diverse industries, including banking, insurance, telecom, energy, healthcare, retail, and media.\n\nData Scientist - AI/ML\n\nResponsibilities\n\nJob Description :\n\nDesign, develop, and deploy generative AI models using GCP (Google Cloud Platform), Python, SQL, Vertex AI, and other relevant technologies.\nCollaborate with cross-functional teams to understand business requirements and develop tailored generative AI solutions that address specific use cases.\nUtilize machine learning techniques to train generative models on large datasets and generate realistic and creative outputs.\nImplement natural language processing (NLP) algorithms and techniques to enhance the capabilities of generative AI models.\nCreate interactive data visualizations to showcase the outputs of generative models and communicate insights effectively to stakeholders.\nConduct performance tuning and optimization of generative AI models to ensure scalability, efficiency, and quality of outputs.\nStay updated on the latest advancements in generative AI, machine learning, and related technologies, and contribute to research and development efforts.\nMentor junior team members and collaborate with peers to foster a culture of learning and innovation.\n\nRequirements\n\nMust Have:\n\nProficiency in GCP (Google Cloud Platform), Python, SQL, and Vertex AI.\nStrong understanding of machine learning concepts and algorithms and experience with classical ML models\nExperience with data visualization techniques and tools to communicate insights effectively.\nFamiliarity with natural language processing (NLP) techniques and libraries.\nDemonstrated experience in developing and deploying machine learning models in a professional setting.\n\nGood To Have\n\nExperience with LLM (Large Language Models) prompt tuning to optimize generative AI models.\nExperience in terraform to deploy infrastructure using CI/CD pipelines\nKnowledge of vector databases and their applications in managing and analyzing large datasets.\nFamiliarity with RAG (Retrieval-Augmented Generation) models and their use cases.\nUnderstanding of langchains (Language Model Chains) and their implications for generative AI.\n\nEducation And Experience\n\nBachelor's degree or higher in Computer Science, Data Science, or a related field.\nMinimum of 3 - 5 years of experience in machine learning, or a related role.",
        "skills": [
            "natural language processing NLP techniques",
            "machine learning concepts and algorithms",
            "Vertex AI",
            "GCP Google Cloud Platform",
            "data visualization techniques",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Kavi India",
        "experience": "Fresher",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Summary:\n\nWe are looking for a driven and curious Junior Data Scientist to join our growing AI & Data\nScience team. The ideal candidate will have a foundational understanding of data analysis and\nmachine learning, with a strong interest in emerging AI paradigms such as Retrieval-Augmented\nGeneration (RAG) and agentic AI systems. This role provides an exciting opportunity to work on\nreal-world applications of LLMs and support the development of intelligent, task-oriented agents.\n\nKey Responsibilities:\nClean, transform, and analyze data (structured and unstructured) from diverse sources.\nPerform exploratory data analysis to identify actionable insights.\nSupport the development and evaluation of traditional and deep learning models.\nAssist in integrating LLMs into workflows using RAG pipelines.\nCollaborate with senior team members to build and refine agentic AI systems for task\nautomation.\nContribute to the design and testing of knowledge retrieval systems, including vector stores\nand embedding models.\nDevelop dashboards, visualizations, and documentation to effectively communicate insights\nand findings.\n\nRequired Skills & Qualifications:\n\nBachelor's degree in data science, Computer Science, Statistics, Engineering, or a related\ndiscipline.\nStrong programming proficiency in Python, including experience with libraries like pandas,\nnumpy, scikit-learn, and either LangChain or LlamaIndex.\nFundamental understanding of Natural Language Processing (NLP) and large language\nmodels (LLMs).\nHands-on experience with SQL and database interactions.\nA strong passion for advancements in Artificial Intelligence and a commitment to ongoing\nlearning.\nExposure to Retrieval-Augmented Generation (RAG) architectures, including vector\ndatabases like FAISS, Chroma, or Pinecone.\nFamiliarity with LLM frameworks, such as the OpenAI API and Hugging Face Transformers.\nKnowledge of agentic AI systems, including task planning and autonomous agents\nExperience with cloud platforms (AWS, GCP, Azure) and version control systems (Git).",
        "skills": [
            "LangChain",
            "scikit-learn",
            "Hugging Face Transformers",
            "Pinecone",
            "OpenAI API",
            "agentic AI systems",
            "Chroma",
            "FAISS",
            "LlamaIndex",
            "Sql",
            "Git",
            "Pandas",
            "Numpy",
            "Gcp",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Soul AI",
        "experience": "Fresher",
        "salary": null,
        "location": "India",
        "industry": "Login to check your skill match score",
        "job_description": "Step into the world of AI innovation with the Experts Community of Soul AI (By Deccan AI). We are looking for India's top 1%Data Scientists for a unique job opportunity to work with the industry leaders.\nWho can be a part of the community\nWe are looking for top-tier Data Scientists with expertise in predictive modeling, statistical analysis, and A/B testing. If you have experience in this field then this is your chance to collaborate with industry leaders.\nWhat's in it for you\nPay above market standards\nThe role is going to be contract based with project timelines from 2 - 12 months, or freelancing.\nBe a part of an Elite Community of professionals who can solve complex AI challenges.\nWork location could be:\nRemote (Highly likely)\nOnsite on client location\nDeccan AI's Office: Hyderabad or Bangalore\nResponsibilities:\nLead design, development, and deployment of scalable data science solutions optimizing large-scale data pipelines in collaboration with engineering teams.\nArchitect advanced machine learning models (deep learning, RL, ensemble) and apply statistical analysis for business insights.\nApply statistical analysis, predictive modeling, and optimization techniques to derive actionable business insights.\nOwn the full lifecycle of data science projectsfrom data acquisition, preprocessing, and exploratory data analysis (EDA) to model development, deployment, and monitoring.\nImplement MLOps workflows (model training, deployment, versioning, monitoring) and conduct A/B testing to validate models.\nRequired Skills:\nExpert in Python, data science libraries (Pandas, NumPy, Scikit-learn), and R with extensive experience with machine learning (XGBoost, PyTorch, TensorFlow) and statistical modeling.\nProficient in building scalable data pipelines (Apache Spark, Dask) and cloud platforms (AWS, GCP, Azure).\nExpertise in MLOps (Docker, Kubernetes, MLflow, CI/CD) along with strong data visualization skills (Tableau, Plotly Dash) and business acumen.\nNice to Have:\nExperience with NLP, computer vision, recommendation systems, or real-time data processing (Kafka, Flink).\nKnowledge of data privacy regulations (GDPR, CCPA) and ethical AI practices.\nContributions to open-source projects or published research.\nWhat are the next steps\n1. Register on our Soul AI website.\n2. Our team will review your profile.\n3. Clear all the screening rounds: Clear the assessments once you are shortlisted. As soon as you qualify all the screening rounds (assessments, interviews) you will be added to our Expert Community!\n4. Profile matching and Project Allocation: Be patient while we align your skills and preferences with the available project.\nSkip the Noise. Focus on Opportunities Built for You!",
        "skills": [
            "Plotly",
            "Scikit-learn",
            "MLflow",
            "CI CD",
            "R",
            "Dash",
            "Dask",
            "Statistical Modeling",
            "Apache Spark",
            "Tableau",
            "Tensorflow",
            "Numpy",
            "Pandas",
            "Gcp",
            "Pytorch",
            "MLops",
            "Docker",
            "XGBoost",
            "Azure",
            "Python",
            "Kubernetes",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist II",
        "company_name": "Amazon Web Services (AWS)",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Description\n\nAWS Infrastructure Services owns the design, planning, delivery, and operation of all AWS global infrastructure. In other words, we're the people who keep the cloud running. We support all AWS data centers and all of the servers, storage, networking, power, and cooling equipment that ensure our customers have continual access to the innovation they rely on. We work on the most challenging problems, with thousands of variables impacting the supply chain and we're looking for talented people who want to help.\n\nYou'll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. You'll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you'll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion.\n\nDo you love problem solving Are you looking for real world Supply Chain challenges Do you have a desire to make a major contribution to the future, in the rapid growth environment of Cloud Computing\n\nAmazon Web Services is looking for a highly motivated, Data Scientist to help build scalable, predictive and prescriptive business analytics solutions that supports AWS Supply Chain and Procurement organization. You will be part of the Supply Chain Analytics team working with Global Stakeholders, Data Engineers, Business Intelligence Engineers and Business Analysts to achieve our goals.\n\nWe are seeking an innovative and technically strong data scientist with a background in optimization, machine learning, and statistical modeling/analysis. This role requires a team member to have strong quantitative modeling skills and the ability to apply optimization/statistical/machine learning methods to complex decision-making problems, with data coming from various data sources. The candidate should have strong communication skills, be able to work closely with stakeholders and translate data-driven findings into actionable insights. The successful candidate will be a self-starter, comfortable with ambiguity, with strong attention to detail and ability to work in a fast-paced and ever-changing environment.\n\nKey job responsibilities\n\nDemonstrate thorough technical knowledge on feature engineering of massive datasets, effective exploratory data analysis, and model building using industry standard time Series Forecasting techniques like ARIMA, ARIMAX, Holt Winter and formulate ensemble model.\nProficiency in both Supervised(Linear/Logistic Regression) and UnSupervised algorithms(k means clustering, Principle Component Analysis, Market Basket analysis).\nExperience in solving optimization problems like inventory and network optimization . Should have hands on experience in Linear Programming.\nWork closely with internal stakeholders like the business teams, engineering teams and partner teams and align them with respect to your focus area\nDetail-oriented and must have an aptitude for solving unstructured problems. You should work in a self-directed environment, own tasks and drive them to completion.\nExcellent business and communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions\nWork with distributed machine learning and statistical algorithms to harness enormous volumes of data at scale to serve our customers\n\nAbout The Team\n\nDiverse Experiences\n\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn't followed a traditional path, or includes alternative experiences, don't let it stop you from applying.\n\nWhy AWS\n\nAmazon Web Services (AWS) is the world's most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that's why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nWork/Life Balance\n\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there's nothing we can't achieve.\n\nInclusive Team Culture\n\nAWS values curiosity and connection. Our employee-led and company-sponsored affinity groups promote inclusion and empower our people to take pride in what makes us unique. Our inclusion events foster stronger, more collaborative teams. Our continual innovation is fueled by the bold ideas, fresh perspectives, and passionate voices our teams bring to everything we do.\n\nMentorship and Career Growth\n\nWe're continuously raising our performance bar as we strive to become Earth's Best Employer. That's why you'll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nBasic Qualifications\n\nMasters with 5+ years of experience or Bachelors with 8+ years of experience in quantitative field (Computer Science, Mathematics, Machine Learning, AI, Statistics, Operational research or equivalent)\nExperience in Python, R or another scripting language; command line / notebook usage. Knowledge and expertise with Data modelling skills, SQL, MySQL, and Databases (RDBMS, NOSQL)\nExtensive knowledge and practical experience in several of the following areas: machine learning, statistics, Optimization using Linear Programming.\nEvidence of using of relevant statistical measures such as Hypothesis testing, confidence intervals, significance of error measurements, development and evaluation data sets, etc. in data analysis projects\nExcellent written and verbal communication skills for both technical and non-technical audiences\n\nPreferred Qualifications\n\nExperience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company\nFunctional knowledge of AWS platforms such as S3, Glue, Athena, Sagemaker, Lambda, EC2, Batch, Step Function.\nExperience in creating powerful data driven visualizations to describe your ML modeling results to stakeholders\n\nOur inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you're applying in isn't listed, please contact your Recruiting Partner.\n\nCompany - ADSIPL - Karnataka\n\nJob ID: A2959646",
        "skills": [
            "R",
            "Statistical Modeling",
            "Distributed Machine Learning",
            "Optimization",
            "Linear Programming",
            "Nosql",
            "Machine Learning",
            "RDBMS",
            "MySQL",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PVAR SERVICES",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Delhi, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist - Marketing\nLocation: Delhi (Hybrid)\nExperience: 24 Years\nCTC: Up to 25 LPA\nKey Responsibilities:\n-Leverage data science techniques like segmentation, clustering, and predictive modeling to analyze customer behavior and guide personalized marketing strategies.\n-Extract, clean, and integrate data from platforms like CDP, Adobe Analytics, Google Analytics, CRM, and Salesforce to ensure analytical accuracy.\n-Design and interpret A/B tests to evaluate and optimize marketing tactics and website performance.\n-Contribute to the development of marketing attribution models to assess channel-wise impact on conversions and sales.\n-Partner with marketing, product, and IT teams to translate analytical insights into actionable marketing strategies.\nRequired Skills & Qualifications:\n-Bachelor's/Master's in Data Science, Statistics, Computer Science, or related field.\n-24 years of experience as a Data Scientist or Analyst in a marketing setting (B2B/financial services preferred).\n-Proficient in Python or R and SQL.\n-Strong foundation in statistics, ML algorithms, and data visualization.\n-Experience with libraries like Pandas, NumPy, and scikit-learn.\n-Excellent communication skills for non-technical stakeholder engagement.\n-Detail-oriented with strong analytical and problem-solving skills.\n-Comfortable working in cross-functional teams.",
        "skills": [
            "R",
            "ML algorithms",
            "scikit-learn",
            "Statistics",
            "Numpy",
            "Pandas",
            "Data Visualization",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Clinical Data Scientist",
        "company_name": "AstraZeneca",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Introduction to role:\nAs a Clinical Data Scientist, you will be at the forefront of configuring and maintaining elements of Centralised Monitoring (CM) to ensure its readiness for new and existing projects. You will create and update CM system documentation, set up standard CM functionalities, and develop bespoke CM functionalities of lower complexity. Collaborating closely with Data Scientists within CM, you will support their work and contribute to groundbreaking advancements in clinical studies.\nAccountabilities:\nWhat you'll do:\nPerform basic system administration activities including access management, change management, issue management, and related documentation.\nConfigure systems to enable CM support of new studies, including tasks like configuration of data sources and setup of new working spaces and repositories.\nUnder supervision, develop and maintain lower complexity CM functionalities, involving gathering requirements, programming, testing, and creating required documentation.\nDeliver elements of or end-to-end mapping of standard CM functionalities, including programming, testing, validation, and creating required documentation.\nConfigure systems to enable execution of CM analyses for assigned studies, preparing systems for new study set-up and maintaining system data sources.\nEssential Skills/Experience:\nEducational degree (BSc/MSc/PhD) in Life Science, Computer Science, or Information Science and experience in the application of information and knowledge management in a clinical or scientific setting, or equivalent combination of education and experience.\nR Programming\nBasic knowledge of at least one programming language used for data analysis or demonstrated ability/capabilities to acquire such skills in a reasonable timeframe.\nGood communication skills in both written and spoken English.\nTeam player.\nDesirable Skills/Experience:\nPrevious experience with at least one BI platform is an advantage.\nPrevious experience in life science and clinical studies is an advantage.\nWhen we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That's why we work, on average, a minimum of three days per week from the office. But that doesn't mean we're not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.\nLearn from our diverse team of the brightest minds as we seamlessly and inclusively work together. It's where the best of both academia and industry come together to share knowledge, grow together and build meaningful careers.\nReady to make a difference Apply now!",
        "skills": [
            "R Programming"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "LSEG (London Stock Exchange Group)",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nWe are looking for a skilled Senior Data Scientist to join our dynamic Analytics Centre of Excellence team. This role combines Data Science, Python development, and business analysis to deliver actionable insights and solutions that drive strategic decisions. Knowledge and experience will be put into practice to Quantitative Engineering, financial instrument pricing, and analytics to shape the direction of our technology and tools, impacting both internal and external customers.\n\nThis position offers a unique opportunity to work on Data Science projects, Python applications, and requirement analysis, based on your strengths and interests. Whether developing ML models, writing Python code, or collaborating with business partners, playing a key role in the successful delivery of projects.\n\nKey responsibilities:\n\nData Science: Lead the development of sophisticated data science and machine learning models, using large datasets to drive business decisions and data-driven strategies across the organization.\nPython Programming: Design, develop, and maintain Python-based software solutions that meet business needs, collaborating with team members to ensure seamless integration and optimization.\nRequirement Analysis: Work closely with cross-functional teams to define and validate requirements, analyze both qualitative and quantitative data, and contribute to the identification of short- and long-term solutions.\nProcess Improvement: Contribute to process improvements, problem resolution workflows, and preventive measures, while actively participating in project planning and technical solution design.\nProject Transparency: Provide regular updates on project progress and challenges, ensuring transparency with leadership and adhering to disciplined development processes.\nIndependent Work: Work autonomously with minimal guidance, demonstrating ownership of projects and accountability for your expertise and handling ambitious projects.\nFlexible Shift & On-Call Coverage: Available for a 12-9pm shift and on-call support as needed.\n\nSkills & experience required:\n\nStrong Python Skills: Proficient in Python for software development, maintenance, and integration.\nData Science Expertise: Extensive experience in machine learning, AI, and advanced analytics to design and implement data-driven solutions.\nRequirement Analysis: Strong ability to determine and validate business requirements, ensuring alignment with technical solutions.\nCommunication: Excellent written and verbal communication, capable of explaining complex ideas to both technical and non-technical audiences.\nFinancial Analysis: Experience with financial markets and quantitative finance.\nCollaboration & Leadership: Proven ability to work in a multi-disciplinary team, with experience leading teams and mentoring junior developers.\nQuantitative Finance Knowledge: Solid understanding of financial mathematics, including pricing financial instruments and modeling financial markets.\n\nDesirable skills:\n\nProject Management: Experience managing projects, particularly with Agile methodology.\nRefinitiv Tools: Familiarity with Refinitiv Eikon/Elektron/Workspace products.\nESG Data Modeling: Experience in financial modeling, particularly with ESG (Environmental, Social, and Governance) data.\n\nAbout Lseg\n\nLSEG (London Stock Exchange Group) is a global leader in financial markets infrastructure and data. We enable businesses and economies to fund innovation, manage risk, and create jobs. With operations in 70 countries, LSEG employs 25,000 people globally, fostering a culture of growth, opportunity, and innovation.\n\nOur commitment to diversity and inclusion drives our success. We provide a supportive environment where all employees have the opportunity to grow, develop, and fulfill their potential.\n\nCommitment to inclusion\n\nWe embrace diversity and actively seek individuals with unique backgrounds and perspectives. We break down barriers and encourage teamwork, enabling innovation and the rapid development of solutions. LSEG is committed to providing reasonable accommodations for individuals with disabilities during the application process.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",
        "skills": [
            "ESG Data Modeling",
            "Financial Analysis",
            "Refinitiv Tools",
            "Ai",
            "quantitative finance",
            "Machine Learning",
            "Advanced Analytics",
            "Requirement Analysis",
            "Python"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Blend",
        "experience": "Fresher",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Title: MMO Data Scientist Financial Services\nLocation: Hybrid in Hyderabad, India\nCompany: Blend360\n65 LPA\nAbout the Role: We are seeking a skilled Data Scientist with expertise in media mix modeling, Streamlit, and Python to join our team. The successful candidate will play a pivotal role in developing and implementing our Media Mix Optimization (MMO) solution, leveraging advanced analytics to optimize media spend and improve marketing effectiveness for our clients.\nResponsibilities:\nDevelop and maintain media mix models to analyze the effectiveness of various marketing channels.\nCollaborate with cross-functional teams to integrate data from multiple sources.\nUse Streamlit to build and deploy interactive data applications and dashboards.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nPerform data preprocessing, feature engineering, and model validation.\nImplement scenario planning and optimization algorithms to provide actionable media spend recommendations.\nCommunicate findings and insights to stakeholders through visualizations and reports.\nStay updated with the latest trends and best practices in data science and media mix modeling.\nRequirements:\nBachelor's or Master's degree in Data Science, Statistics, Computer Science, or a related field.\nProven experience in media mix modeling and marketing analytics.\nProficiency in Python and relevant libraries (e.g., pandas, numpy, scikit-learn).\nExperience with Streamlit for building interactive web applications.\nStrong analytical and problem-solving skills.\nExcellent communication and presentation skills.\nAbility to work collaboratively in a team environment.\nPreferred:\nExperience with machine learning and statistical modeling.\nFamiliarity with other data visualization tools and frameworks.",
        "skills": [
            "scikit-learn",
            "Streamlit",
            "Pandas",
            "Numpy",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Lead)",
        "company_name": "Quantium",
        "experience": "Fresher",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Type: Permanent - Full Time\n\nLocation: Hyderabad\n\nJob Category: Data Management\n\nJob Description\n\nRole summary\n\nThe Lead Data Scientist, Foundational Analytics is pivotal in ensuring that core data assets support the growth of our business. The appointed Lead will be required to apply their experience in managing a range of analytical resources and functions to develop a centre of excellence for analytic capabilities, fostering a culture of innovation, collaboration and expertise. They will work collaboratively with key stakeholders across the business to understand and set appropriate analytical strategies that support business growth, and apply these strategies to their business planning, and in particular support business growth initiatives through allocation of appropriate resources, subject matter expertise and long range planning. They will act as experts in developing the way we manage our foundational data assets and drive innovative analytical solutions that create value. They will also work with Data Engineers and implementation teams on solution design and with business stakeholders on change management.\n\nKey Responsibilities\n\nDevelop relationships with key internal and external stakeholders across Quantium to understand and support the strategic goals and key initiatives of the company\nDevelop and execute strategy and business plans for the ongoing development of our foundational data assets\nManage Lead and Senior Analysts in other verticals to achieve business plan goals on an ongoing basis across a variety of teams\nSet and manage aligned KPIs across one analytics team focussed on a single data partner\nDevelop a culture of innovation and analytic excellence by focussing on development of existing team members and hiring the best talent available\nDrive accountability within the teams to achieve deadlines and the best possible analytic outcomes\nSupport growth of the business through implementation of capabilities that streamline the ingestion and preparation of new data assets\nSupport development of technology solutions that focus on automation of solutions, operational excellence and innovation by working with implementation and technology teams on solution design\n\nKey activities\n\nSet and manage business plans for three or more foundational data assets including data curation, customer segmentation and other foundational analytics\nEngage with business and external stakeholders to gather feedback and requirements to drive the foundational analytics roadmap for key business initiatives\nSet up regular forums to engage stakeholders across the business that use these foundational data assets to receive and give feedback on current and future initiatives and ensure they are supported appropriately\nManage the high-level prioritisation process across teams including the use of WIPs, charter cards and business plans which are reviewed by the team on a regular basis, and manage stakeholders expectations regarding timeframes\nDevelop and implement resource plans to support business plans and roadmaps\nResponsible for team recruitment, for both external and internal candidates\nEnsure tailored performance plans and KPIs are set, monitored and managed on a regular basis for all team members\nProvide coaching and support to direct reports to develop their management and analytical capabilities\nMonitor and approve technical designs that implement analytical outcomes and models\nDrive a culture of innovation by engaging with analysts across the team through a variety of forums including one to ones, skip level meetings, lunch time sessions and analytical showcases\nFoster a cohesive team spirit through team meetings, awards and strong communication\nTaking a structured and analytical approach in troubleshooting data issues and problems\nPerformance manager for junior team members and enabling the team to succeed by resolving issues and building on strengths\n\nExperience And Education Required\n\nStrong background and understanding of data structures, data manipulation and transformation\nPrevious experience in the fields of data science or high-level data analysis; data modelling experience highly regarded\nPrevious experience working directly with Big Data Engineers highly regarded\nPrevious experience with project management and people management\nProven ability and experience in optimising performance of an existing process with a can-do attitude\nDesire to work with a team of feedback-driven analysts and developers in a cross-functional and agile environment\nLead level data science/data analytics and management of one or more team of analysts\nVariety of analytical roles preferred, including consulting\n\nSkills Required\n\nSound knowledge of technical analytics discipline, including data preparation, feature engineering and foundational analytics concepts, preferably including model development and model training\nSufficient skills in several disciplines or techniques to autonomously apply without the need for material guidance or revision by others; and\nSufficient skills in at least one discipline or technique to be an authoritative source of guidance and support for the team, and to be able to solve problems of a high technical complexity in this domain\nStrong problem-solving skills, including ability to apply a systematic problem-solving approach\nSolid interpersonal skills, as the role will need to work in a cross-functional team with both other analysts and specialists from non-analytics disciplines\nVery strong client and stakeholder management skills\nAbility to autonomously carry out work following analytics best practice as defined at Quantium for the relevant types of tools or techniques, suggesting improvements where appropriate\nAbility to motivate peers to follow analytics best practice as defined at Quantium, by positively presenting the benefits and importance of these ways of working\nCommercial acumen to understand business needs and the commercial impacts of different analytics solutions or approaches\nAttention to detail\nDrive for continuous improvement",
        "skills": [
            "project management",
            "model training",
            "People Management",
            "model development",
            "foundational analytics concepts",
            "feature engineering",
            "Problem-solving",
            "Data Manipulation",
            "data preparation"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "LSEG (London Stock Exchange Group)",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "We are looking for a skilled Data Scientist to join our dynamic Analytics Centre of Excellence team. This role combines Data Science, Python development, and business analysis to deliver actionable insights and solutions that drive strategic decisions. Knowledge and experience will be put into practice to Quantitative Engineering, financial instrument pricing, and analytics to shape the direction of our technology and tools, impacting both internal and external customers.\n\nThis position offers a unique opportunity to work on Data Science projects, Python applications, and requirement analysis, based on your strengths and interests. Whether developing ML models, writing Python code, or collaborating with business partners, playing a key role in the successful delivery of projects.\n\nKey responsibilities:\n\nData Science: Own development of sophisticated data science and machine learning models, using large datasets to drive business decisions and data-driven strategies across the organization.\nPython Programming: Design, develop, and maintain Python-based software solutions that meet business needs, collaborating with team members to ensure seamless integration and optimization.\nRequirement Analysis: Work closely with cross-functional teams to validate requirements, analyze both qualitative and quantitative data, and contribute to the identification of short- and long-term solutions.\nProcess Improvement: Contribute to process improvements, problem resolution workflows, and preventive measures, while actively participating in project planning and technical solution design.\nProject Transparency: Provide regular updates on project progress and challenges, ensuring transparency with leadership and adhering to disciplined development processes.\nIndependent Work: Work autonomously with minimal guidance, demonstrating ownership of projects and accountability for your expertise and handling ambitious projects.\nFlexible Shift & On-Call Coverage: Available for a 12-9pm shift and on-call support as needed.\n\nSkills & experience required:\n\nStrong Python Skills: Proficient in Python for software development, maintenance, and integration.\nData Science Expertise: Extensive experience in machine learning, AI, and advanced analytics to design and implement data-driven solutions.\nRequirement Analysis: Strong ability to validate business requirements, ensuring alignment with technical solutions and providing realistic sizing.\nCommunication: Excellent written and verbal communication, capable of explaining complex ideas to both technical and non-technical audiences.\nFinancial Analysis: Experience with financial markets and quantitative finance.\nCollaboration: Proven ability to work in a multi-disciplinary team, with experience in mentoring junior developers.\nQuantitative Finance Knowledge: Good understanding of financial mathematics, including pricing financial instruments and modeling financial markets.\n\nDesirable skills:\n\nProject Management: Experience managing project updates, particularly with Agile methodology.\nRefinitiv Tools: Familiarity with Refinitiv Eikon/Elektron/Workspace products.\nESG Data Modeling: Experience in financial modeling, particularly with ESG (Environmental, Social, and Governance) data.\n\nAbout Lseg\n\nLSEG (London Stock Exchange Group) is a global leader in financial markets infrastructure and data. We enable businesses and economies to fund innovation, manage risk, and create jobs. With operations in 70 countries, LSEG employs 25,000 people globally, fostering a culture of growth, opportunity, and innovation.\n\nOur commitment to diversity and inclusion drives our success. We provide a supportive environment where all employees have the opportunity to grow, develop, and fulfill their potential.\n\nCommitment to inclusion\n\nWe embrace diversity and actively seek individuals with unique backgrounds and perspectives. We break down barriers and encourage teamwork, enabling innovation and the rapid development of solutions. LSEG is committed to providing reasonable accommodations for individuals with disabilities during the application process.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.",
        "skills": [
            "Financial Analysis",
            "Collaboration",
            "Data Science Expertise",
            "Quantitative Finance Knowledge",
            "ESG Data Modeling",
            "Project Management",
            "Refinitiv Tools",
            "Requirement Analysis",
            "Python Programming"
        ]
    },
    {
        "job_title": "Associate - Data Scientist",
        "company_name": "United Airlines",
        "experience": "Fresher",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Achieving our goals starts with supporting yours. Grow your career, access top-tier health and wellness benefits, build lasting connections with your team and our customers, and travel the world using our extensive route network.\n\nCome join us to create what's next. Let's define tomorrow, together.\n\nDescription\n\nUnited's Digital Technology team designs, develops, and maintains massively scaling technology solutions brought to life with innovative architectures, data analytics, and digital solutions.\n\nFind your future at United! We're reinventing what our industry looks like, and what an airline can be from the planes we fly to the people who fly them. When you join us, you're joining a global team of 100,000+ connected by a shared passion with a wide spectrum of experience and skills to lead the way forward.\n\n\n\nAchieving our ambitions starts with supporting yours. Evolve your career and find your next opportunity. Get the care you need with industry-leading health plans and best-in-class programs to support your emotional, physical, and financial wellness. Expand your horizons with travel across the world's biggest route network. Connect outside your team through employee-led Business Resource Groups.\n\n\n\nCreate what's next with us. Let's define tomorrow together.\n\nJob Overview And Responsibilities\n\nThe Analytics and Innovation (Ai) team at United Airlines works to innovate, transform, and solve complex business problems resulting in significant and measurable enterprise impact through the application of deep cross-functional business expertise and advanced analytics. We work with business and technology partners across the enterprise (e.g. Airport Operations, Contact Centers, Network Planning, Revenue Management, Digital Products, Human Resources, etc.) to influence strategic decisions, enterprise strategy, business process, and to help connect the dots across the enterprise. We aim to be an organization with whom and within everyone loves to work!\n\nData Scientists within Ai partner with business and technology leaders across the company to deploy AI & ML powered solutions to support and automate business processes. The team works closely with other teams in digital technology with complementing skills and capabilities. The key objectives are to drive incremental revenue, boost customer engagement and to automate manual processes by leveraging state-of-the-art ML techniques. Data Scientists contribute to developing smart and innovative solutions across many of United's departments.\n\nWork with other data scientists and a variety of stakeholders (IT, DE, business etc.) to deliver high-quality models\nGathering and analyzing data of United's various operations to make fact-based, data-driven recommendations\nExecute solutions to business problems using data analysis, machine learning and optimization tools\nEnable data transfer between platforms and monitor model performance for scalable project deployments\nResearch latest machine learning approaches and share the knowledge across the team\nEffectively structure communication of insights from work streams and deliver clear and professional presentations to the team/team leaders/managers\nPartner and engage with data engineering and operations teams to inform and support business stakeholders with day-to-day project updates.\n\nThis position is offered on local terms and conditions. Expatriate assignments and sponsorship for employment visas, even on a time-limited visa status, will not be awarded. This position is for\n\nUnited Airlines Business Services Pvt. Ltd - a wholly owned subsidiary of United Airlines Inc.\n\nQualifications\n\nWhat's needed to succeed (Minimum Qualifications):\n\nBTech/Bachelor's degree in Engineering, Technology, Computer Science, Statistics, Operations Research, Applied Mathematics, Economics\nFull time or internship experience in analytics/data science\n0-3 years relevant work experience\nExcellent analytical and problem-solving skills\nProficiency in Programming Languages (Python/R), SQL, Excel and other Microsoft Office products\nIntermediate knowledge in machine learning algorithms like Bagging (Decision Trees, RF), Boosting (XGBoost, GBM) , Clustering (KMeans)\nProven comfort and an intellectual curiosity for working with very large sets of data\nUnderstanding of probability & statistics\nAbility to bridge business context with technical details and explain and discuss mathematical and machine learning technicalities to a business audience\nStrong interpersonal skills with a focus on teamwork\n\nWhat will help you propel from the pack (Preferred Qualifications):\n\nMTech/Master's degree in Data Science, Business Analytics, Engineering, Technology, Computer Science, , Statistics, Operations Research, Applied Mathematics, Economics\nKnowledge of deep learning/neural networks and experience with NLP/CV\nHands on experience in using AWS/Azure etc\n\nGGN00001951",
        "skills": [
            "KMeans",
            "GBM",
            "R",
            "Boosting",
            "bagging",
            "Machine Learning",
            "Clustering",
            "Sql",
            "XGBoost",
            "Decision Trees",
            "Excel",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "Peoplefy",
        "experience": "12-14 Years",
        "salary": null,
        "location": "Pune, India",
        "industry": "Login to check your skill match score",
        "job_description": "Hi All,\nWe are urgently hiring for a Principal Data Scientist with reputed Client for Kharadi Pune, location.\nExperience - 12 to 13 + Years of Total experience\nRelevant Experience - 7 to 8 Years of experience as DS - Optimization, prediction and Forecasting\nMission:\nUnderstanding of the customer needs (level = difficult) from RDI, Manufacturing, Marketing etc.\nIdentification of the statistical/numerical methods to answer this kind of problems\nDetermination of the relevant data among those available and identification of the missing data to acquire if possible\nConsolidation/Aggregation of the finally selected data.\nAnalysis and building of the model\nDetailed presentation of the results and/or the model (performances and limits) so that the final customer can use it or upgrade the results with the future available data\nDirect the external research that helps to resolve ambitious technical problems. Develop partnerships that take advantage of new opportunities.\nAssure the technical mentoring of less experienced people\nTransmit knowledge by development and deployment of training modules and various other communication methods\nParticipate in the development of the Mtier roadmap and the Data Scientist Network\nInterested candidate kindly share your resume on[HIDDEN TEXT]",
        "skills": [
            "Forecasting",
            "prediction",
            "Optimization"
        ]
    },
    {
        "job_title": "Data Scientist (Open CV)",
        "company_name": "Evnek",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Data Scientist (OpenCV)\n\nExperience: 01 Year\n\nLocation: Bangalore\n\nNotice Period: Immediate Joiner\n\nJob Summary:\n\nWe are seeking a highly motivated and enthusiastic Data Scientist with a passion for computer vision and hands-on experience in OpenCV. This is an excellent opportunity for recent graduates or early-career professionals to contribute to real-world projects involving image processing, computer vision, and machine learning.\n\nKey Responsibilities:\n\nWork on image and video processing projects using OpenCV and Python.\nAssist in developing computer vision models for object detection, recognition, and tracking.\nPreprocess and analyze image datasets to extract meaningful insights.\nCollaborate with senior data scientists and engineers to integrate models into production environments.\nConduct experiments and contribute to research-oriented tasks in the field of computer vision.\nDocument processes, models, and results for internal review and external presentation.\n\nRequired Skills:\n\nStrong knowledge of OpenCV and image processing techniques.\nProficiency in Python and related libraries (e.g., NumPy, Pandas, Matplotlib).\nUnderstanding of basic machine learning and deep learning concepts.\nFamiliarity with tools like Jupyter Notebooks, TensorFlow, or PyTorch is a plus.\nGood problem-solving skills and attention to detail.\nExcellent communication and teamwork skills.\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Data Science, Electrical Engineering, or a related field.\nInternship or academic project experience in computer vision or image processing.\nExposure to cloud platforms (AWS, GCP, Azure) is a plus.\n\nWhat We Offer:\n\nA collaborative and learning-driven environment.\nMentorship from experienced professionals.\nOpportunity to work on innovative projects with real-world impact.\nCompetitive salary and benefits.",
        "skills": [
            "Jupyter Notebooks",
            "Tensorflow",
            "Matplotlib",
            "Deep Learning",
            "Numpy",
            "Machine Learning",
            "Pytorch",
            "Pandas",
            "Opencv",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist (Artificial Intelligence, Machine Learning)",
        "company_name": "Franklin Templeton India",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Hyderabad, India",
        "industry": "Login to check your skill match score",
        "job_description": "At Franklin Templeton, we're driving our industry forward by developing new and innovative ways to help our clients achieve their investment goals. Our dynamic and diversified firm spans asset management, wealth management, and fintech, offering many ways to help investors make progress toward their goals. Our talented teams working around the globe bring expertise that's both broad and unique. From our welcoming, inclusive, and flexible culture to our global and diverse business, we offer opportunities not only to help you reach your potential but also to contribute to our clients achievements.\n\nCome join us in delivering better outcomes for our clients around the world!\n\nAt Franklin Templeton, everything we do is focused on one thing delivering better client outcomes. We do that by partnering closely with our clients, assessing their strategic needs, and identifying the solution or solutions that can meet the challenge. Over 9,500 employees working in 30 countries around the world are dedicated to servicing investment solutions for our clients in more than 155 countries. For more than 75 years our success has been a direct result of the talent, skills and persistence of our people, and we are looking for qualified candidates to join our team.\n\nWhat is the Data Scientist in FTT Digital Technology group responsible for\n\nAs a Data Scientist, you will play a critical role in leveraging data analytics to tackle business challenges and drive insights. Your responsibilities will include working on machine learning models and applying advanced analytics techniques to solve problems and support decision-making. You will be involved in various aspects of the data pipeline, from collection to analysis, and will contribute to the development of data-driven solutions. You'll work under the guidance of senior team members, gaining hands-on experience and growing your skills in both traditional and innovative data science methodologies\n\nWhat are the ongoing responsibilities of a Data Scientist\n\nData Collection And Preprocessing\n\nImplement data collection strategies to gather relevant data from different sources.\nClean, preprocess, and validate data to ensure accuracy and reliability for subsequent analysis.\nAssist in maintaining and enhancing data pipelines in collaboration with data engineering teams.\n\nStatistical Analysis\n\nPerform exploratory data analysis to identify trends, patterns, and insights within datasets.\nApply basic statistical techniques to test hypotheses, validate assumptions, and draw conclusions from data.\n\nMachine Learning And AI Model Development\n\nDevelop and optimize machine learning models to address business problems and improve processes.\nExplore and apply Generative AI techniques under the supervision of senior team members to contribute to innovative solutions.\nAssist in model evaluation, validation, and deployment, monitoring performance and making adjustments as needed.\n\nUnderstanding Human Behavior For AI Applications\n\nAnalyze data related to human behavior to help develop models that predict and influence outcomes.\nWork with domain experts to incorporate insights into AI models, enhancing their relevance and accuracy.\n\nData Engineering Collaboration\n\nCollaborate with data engineering teams to ensure smooth integration of models into existing data systems.\nContribute to designing and implementing scalable data storage solutions.\n\nCross-functional Collaboration\n\nWork with product management, marketing, and business stakeholders to understand requirements and provide data-driven insights.\nCommunicate analytical concepts and insights effectively to non-technical stakeholders through reports and visualizations.\n\nContinuous Learning And Innovation\n\nStay updated on the latest developments in data science, machine learning, and AI technologies.\nExperiment with new methodologies and tools to enhance project outcomes and expand your skill set.\n\nWhat ideal qualifications, skills & experience would help someone to be Successful\n\nMaster's or Bachelor's degree in Statistics, Mathematics, Econometrics, Computer Science, Engineering, or related disciplines\n2-4 years of experience in data science, predictive modeling, and machine learning.\nProficiency in Python, R, or SQL, with hands-on experience in data analysis and model development.\nBasic knowledge of Generative AI models and their applications\nAbility to translate business problems into analytical tasks.\nSkill in explaining statistical and machine learning techniques to business partners.\nExperience in creating data-driven stories and insights.\nProficiency in handling large datasets, including data cleansing, manipulation, and mining.\nCapability to tackle ambiguous challenges with a problem-solving mindset.\nCuriosity and willingness to learn independently.\nStrong written and verbal communication skills.\nEffective organizational and planning abilities\nAbility to work well under pressure and adapt in a dynamic environment.\nTeam-oriented approach with a capacity to work independently when needed.\nStrong interpersonal skills and the ability to build relationships with colleagues and stakeholders\n\nJob Level - Individual Contributor\n\nWork Shift Timing - 2:00 PM - 11:00 PM IST\n\nExperience our welcoming culture and reach your professional and personal potential!\n\nOur culture is shaped by our diverse global workforce and strongly held core values. Regardless of your interests, lifestyle, or background, there's a place for you at Franklin Templeton. We provide employees with the tools, resources, and learning opportunities to help them excel in their career and personal life.\n\nHear more from our employees\n\nBy joining us, you will become part of a culture that focuses on employee well-being and provides multidimensional support for a positive and healthy lifestyle. We understand that benefits are at the core of employee well-being and may vary depending on individual needs. Whether you need support for maintaining your physical and mental health, saving for life's adventures, taking care of your family members, or making a positive impact in your community, we aim to have them covered.\n\nHighlights Of Our Benefits Include\n\nProfessional development growth opportunities through in-house classes and over 150 Web-based training courses\nAn educational assistance program to financially help employees seeking continuing education\nMedical, Life and Personal Accident Insurance benefit for employees. Medical insurance also cover employee's dependents (spouses, children and dependent parents)\nLife insurance for protection of employees families\nPersonal accident insurance for protection of employees and their families\nPersonal loan assistance\nEmployee Stock Investment Plan (ESIP)\n12 weeks Paternity leave\nOnsite fitness center, recreation center, and cafeteria\nTransport facility\nChild day care facility for women employees\nCricket grounds and gymnasium\nLibrary\nHealth Center with doctor availability\nHDFC ATM on the campus\n\nLearn more about the wide range of benefits we offer at Franklin Templeton\n\nFranklin Templeton is an Equal Opportunity Employer. We are committed to providing equal employment opportunities to all applicants and existing employees, and we evaluate qualified applicants without regard to ancestry, age, color, disability, genetic information, gender, gender identity, or gender expression, marital status, medical condition, military or veteran status, national origin, race, religion, sex, sexual orientation, and any other basis protected by federal, state, or local law, ordinance, or regulation.\n\nFranklin Templeton is committed to fostering a diverse and inclusive environment. If you believe that you need an accommodation or adjustment to search for or apply for one of our positions, please send an email to [HIDDEN TEXT]. In your email, please include the accommodation or adjustment you are requesting, the job title, and the job number you are applying for. It may take up to three business days to receive a response to your request. Please note that only accommodation requests will receive a response.",
        "skills": [
            "Data Analysis",
            "R",
            "Generative AI",
            "Machine Learning",
            "Sql",
            "Python",
            "Data Cleansing"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PepsiCo",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Food Processing & Packaged Food",
        "job_description": "Overview\n\nWe are PepsiCo\nPepsiCo is one of the world's leading food and beverage companies with more than $79 Billion in Net Revenue and a global portfolio of diverse and beloved brands. We have a complementary food and beverage portfolio that includes 22 brands that each generate more than $1 Billion in annual retail sales. PepsiCo's products are sold in more than 200 countries and territories around the world. PepsiCo's strength is its people. We are over 250,000 game changers, mountain movers and history makers, located around the world, and united by a shared set of values and goals.\nWe believe that acting ethically and responsibly is not only the right thing to do, but also the right thing to do for our business. At PepsiCo, we aim to deliver top-tier financial performance over the long term by integrating sustainability into our business strategy, leaving a positive imprint on society and the environment. We call thisWinning with Purpose. For more information on PepsiCo and the opportunities it holds, visit.\nData Science Team works in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Azure Machine Learning Services and Pipelines.\nPepsiCo Data Analytics & AI Overview:\nWith data deeply embedded in our DNA, PepsiCo Data, Analytics and AI (DA&AI) transforms data into consumer delight. We build and organize business-ready data that allows PepsiCo's leaders to solve their problems with the highest degree of confidence. Our platform of data products and services ensures data is activated at scale. This enables new revenue streams, deeper partner relationships, new consumer experiences, and innovation across the enterprise.\nThe Data Science Pillar in DA&AI will be the organization where Data Scientist and ML Engineers report to in the broader D+A Organization. Also DS will lead, facilitate and collaborate on the larger DS community in PepsiCo. DS will provide the talent for the development and support of DS component and its life cycle within DA&AIProducts. And will support pre-engagement activities as requested and validated by the prioritization framework of DA&AI.\nData Scientist: Hyderabad and Gurugram\nYou will be part of a collaborative interdisciplinary team around data, where you will be responsible of our continuous delivery of statistical/ML models. You will work closely with process owners, product owners and final business users. This will provide you the correct visibility and understanding of criticality of your developments.\n\nResponsibilities\n\nDelivery of key Advanced Analytics/Data Science projects within time and budget, particularly around DevOps/MLOps and Machine Learning models in scope\nActive contributor to code & development in projects and services\nPartner with data engineers to ensure data access for discovery and proper data is prepared for model consumption.\nPartner with ML engineers working on industrialization.\nCommunicate with business stakeholders in the process of service design, training and knowledge transfer.\nSupport large-scale experimentation and build data-driven models.\nRefine requirements into modelling problems.\nInfluence product teams through data-based recommendations.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate reusable packages or libraries.\nEnsure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards\nLeverage big data technologies to help process data and build scaled data pipelines (batch to real time)\nImplement end-to-end ML lifecycle with Azure Machine Learning and Azure Pipelines\nAutomate ML models deployments\n\nQualifications\n\nBE/B.Tech in Computer Science, Maths, technical fields.\nOverall 5+ years of experience working as a Data Scientist.\n4+ years experience building solutions in the commercial or in the supply chain space.\n4+ years working in a team to deliver production level analytic solutions. Fluent in git (version control). Understanding of Jenkins, Docker are a plus.\nFluent in SQL syntaxis.\n4+ years experience in Statistical/ML techniques to solve supervised (regression, classification) and unsupervised problems.\n4+ years experience in developing business problem related statistical/ML modeling with industry tools with primary focus on Python or Pyspark development.\nSkills, Abilities, Knowledge:\nData Science - Hands on experience and strong knowledge of building machine learning models - supervised and unsupervised models. Knowledge of Time series/Demand Forecast models is a plus\nProgramming Skills - Hands-on experience in statistical programming languages like Python, Pyspark and database query languages like SQL\nStatistics - Good applied statistical skills, including knowledge of statistical tests, distributions, regression, maximum likelihood estimators\nCloud (Azure) - Experience in Databricks and ADF is desirable\nFamiliarity with Spark, Hive, Pig is an added advantage\nBusiness storytelling and communicating data insights in business consumable format. Fluent in one Visualization tool.\nStrong communications and organizational skills with the ability to deal with ambiguity while juggling multiple priorities\nExperience with Agile methodology for team work and analytics product creation.",
        "skills": [
            "Big Data",
            "Databricks",
            "Sql",
            "Artificial Intelligence",
            "Python",
            "Devops",
            "Azure Machine Learning",
            "Machine Learning",
            "Pyspark",
            "MLops"
        ]
    },
    {
        "job_title": "Lead - Data Scientist (Tier1/2)",
        "company_name": "IndusInd Bank",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About the Digital Business Unit at IndusInd:\nThe mandate of the Digital Business Unit at IndusInd Bank is as follows:\nBuilding customer centric products with human centered design principles for retail Individual and micro, small and medium enterprise (MSME) customer segments\nBuild innovative products and propositions backed with problem solving mindset to discover and solve latent needs of customers\nBuild Embedded Finance (Banking as a Service) applications\nEnsure designs are highly available, highly modular, highly scalable and highly secure\nDrive digital business\nSome of the applications managed by the Digital Business Unit include IndusMobile (bank's mobile app for retail individual clients), Indusnet (net banking application of the bank), IndusMerchantSolutions app, Whatsapp Banking, Chatbots, easycredit for Individuals, easycredit for Business Owners, savings, current account and fixed deposit online platforms. There are many more innovative digital products and solutions in pipeline\nThe unit's objectives are three fold (a) Drive better customer experience and engagement (b) transform existing lines of businesses and (c) build new digital only or banking as a service led digital business models\nAbout the role:\nYou would be part of asset analytics and data science team and work on cutting edge problems for the bank. The individual will work closely with the stakeholders across risk, business, partnerships, digital and strategy in creating and refining strategies to augment profitability and growth for the bank. The incumbent will majorly be responsible with coming up data driven and actionable insights and presenting them to relevant stakeholders The candidate will work in close collaboration with digital product, growth, and marketing teams\nOverall, Job Description\nExperience querying databases and using statistical computer languages: R, Python, SLQ, etc.\nUse predictive modelling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.\nExperienced in working with large and multiple datasets, data warehouses and ability to pull data using relevant programs and coding.\nWell versed with necessary data reprocessing and feature engineering skills.\nStrong background in Statistical Analysis. Constantly look and research on ML algorithms and data sources for better prediction\nWork and coordinate with multiple stakeholders to identify opportunities for leveraging company data to drive business solutions, implement models and monitor outcomes.\nAssess the effectiveness and accuracy of new data sources and data gathering techniques and develop processes and tools to monitor and analyze model performance and data accuracy.\nExperience in establishing/scaling up data science functions\nProven ability to discover solutions hidden in large datasets and to drive business results with their data-based insights\nLeverage analytics to increase customer lifetime value for clients acquired digitally by pitching right product to the right client at the right time\nHelp define pricing models for digital value propositions for various segments of users / clients to ensure profitability of the portfolio and to ensure achievement of business outcomes\nWork with product, growth, and marketing teams across product/campaign lifecycle\nEmpower product and marketing teams by creating automated dashboards and reports using PowerBI\nSkills/Capabilities\nCandidate should be from Tier1/Tier2 institute\n5-7 years of relevant experience in Data Science in Banking/ NBFC/ Fintecth\nModel development experience in R, Python, SAS\nStrong and in-depth understanding of statistics\nStrong strategic thought leadership and problem-solving skills with ability to tackle unstructured and complex business problems\nAbility to build & use relationships and influence broadly across the organization\nResults driven with strong project management skills, ability to work on multiple priorities\nHandling Big Data, Segmentation, Analytics, Machine Learning, Artificial Intelligence, Statistics and Hypothesis Testing",
        "skills": [
            "R",
            "Feature Engineering",
            "Data Reprocessing",
            "Statistical Analysis",
            "Powerbi",
            "Artificial Intelligence",
            "Python",
            "SAS",
            "Predictive Modelling",
            "Big Data",
            "Machine Learning"
        ]
    },
    {
        "job_title": "(IND) Distinguished, Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWalmart is a multi-national people led tech powered omnichannel retailer with over 2.2M associates. At People Tech, we help, support and empower the HR department with technology and solutions to improving productivity, harnessing the opportunities for growth, focusing on well-being and cultivating cultures of belonging to all Walmart's associates.\n\nEnterprise Business Services (EBS) is seeking a highly skilled and experienced Distinguished Data Scientist directly reporting to the VP and org head at IDC. As a Distinguished Data Scientist specializing in GenAI and Agentic AI, you will lead the development and deployment of AI/ML models for associate facing systems & technologies; work with a team of Data scientists and engineers, and collaborate with product leaders to drive innovation. Your role will also involve overseeing project lifecycles, ensuring compliance with standards, and enhancing Walmart's reputation through research and partnerships.\n\nWhat you'll do...\n\nAbout Team:\n\nThe Enterprise Business Services supports the successful deployment and adoption of new People technology, Finance and Risk Technologies across the enterprise. As a Fortune #1 company, our work impacts millions of associates globally. We strive to continuously improve people technology and products to help managers and associates so they can focus on what matters most - supporting our customers and members. Walmart Global Techs Enterprise Business Services, which is invested in building a compact, robust organization that includes technology solutions for Transaction ; Risk Technology, Finance, People, and the Associate Digital Experience.\n\nWhat you will do:\n\nAs a Distinguished Data Scientist with specialization in GenAI and Agentic AI, your role will be pivotal in:\n\nLeading the formulation of strategies and roadmaps for the design, development, and deployment of AI/ML, NLP, and GenAI models, ensuring their seamless transition into production environments and guaranteeing their reliability and scalability.\nConstructing and fine-tuning cutting-edge large language models (LLMs) by leveraging Walmarts vast datasets.\nWorking with a team of Data scientists and engineers in solving intricate AI/ML challenges through research and development, pushing the limits of innovation and making groundbreaking contributions to the field.\nOverseeing the entire lifecycle of projects, from inception and data collection to model prototyping and deployment, while effectively managing stakeholder relationships and facilitating cross-functional communication.\nCollaborating extensively with product and engineering leaders to accelerate innovations in discovery experiences, utilizing insights, frameworks, machine learning prototypes, and emphasizing the strategic importance of AI initiatives.\nUpholding and enforcing rigorous standards, governance, and best practices in AI/ML model development, ensuring strict compliance with model and data governance standards.\nEnhancing external reputation by publishing your teams work in prestigious AI/ML conferences and nurturing partnerships with academic institutions.\nAdhering strictly to Walmarts policies, procedures, mission, values, standards of ethics, and integrity.\nDesigning end-to-end system architecture for GenAI/AI/ML and data-intensive applications, setting new benchmarks in the industry.\nLeading, managing, and mentoring a team of data scientists, helping them grow professionally and enhance their business understanding.\nDeveloping and deploying robust, production-grade real-time/batch machine learning services that set industry benchmarks.\nCollaborating with product managers to design user journeys, feedback loops and analyze user telemetry, thus creating seamless user experiences.\nIdentifying or proposing innovative AI/ML use-cases to business teams to boost business processes, and developing quick MVPs/POCs to enable stakeholders to make data-driven decisions.\n\nWhat you'll bring:\n\nSome of the skills needed for this role are but not limited to - includes a mix of Engineering, Algorithms, Data Science, Machine Learning, and Artificial Intelligence:\n\nGenAI: understanding of usecases in developing and implementing AI models and algorithms.\nPython: Knowledge of Python, including libraries such as NumPy, Pandas, and Scikit-learn.\nApache Spark: Experience with big data processing frameworks like Apache Spark.\nScala: Knowledge of Scala for data processing and analysis.\nMachine Learning: Expertise in machine learning techniques and frameworks such as TensorFlow, Keras, and PyTorch.\nData Science: Strong foundation in data science principles, including statistical analysis, data visualization, and data manipulation.\nAlgorithms: Ability to design and implement efficient algorithms for data processing and analysis.\nEngineering: Experience in software engineering practices, including version control, testing, and deployment.\n\nAbout Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. Thats what we do at Walmart Global Tech. Were a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the worlds largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human led is our true disruption.\n\nWays of Working\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nWalmart Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve to live better when we really know them. That means understanding, respecting, and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 6 years experience in an analytics related field. Option 2: Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 3: 8 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\nPardhanani Wilshire Ii, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2111672",
        "skills": [
            "GenAI",
            "Engineering",
            "Data Science",
            "Machine Learning",
            "Algorithms",
            "Scala",
            "Apache Spark",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ericsson",
        "experience": "8-12 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Ericsson\n\nEricsson is a leading provider of telecommunications equipment and services to mobile and fixed network operators globally. Our innovative solutions empower individuals, businesses, and societies to explore their full potential in the Networked Society. We are seeking a highly skilled and experienced Data Scientist to join our dynamic team at Ericsson. As a Data Scientist, you will be responsible for leveraging advanced analytics and machine learning techniques to drive actionable insights and solutions for our telecom domain. This role requires a deep understanding of data science methodologies, strong programming skills, and proficiency in cloud-based environments.\n\nKey Responsibilities\n\nDevelop and deploy machine learning models for various applications including chat-bot, XGBoost, random forest, NLP, computer vision, and generative AI.\nUtilize Python for data manipulation, analysis, and modeling tasks.\nProficient in SQL for querying and analyzing large datasets.\nExperience with Docker and Kubernetes for containerization and orchestration of applications.\nBasic knowledge of PySpark for distributed computing and data processing.\nCollaborate with cross-functional teams to understand business requirements and translate them into analytical solutions.\nDeploy machine learning models into production environments and ensure scalability and reliability.\nPreferably have experience working with Google Cloud Platform (GCP) services for data storage, processing, and deployment.\n\nQualifications\n\nBachelor's degree in computer science, Statistics, Mathematics, or a related field. A Master's degree or PhD is preferred.\n8-12 years of experience in data science and machine learning roles, preferably within the telecommunications or related industry.\nProven experience in model development, evaluation, and deployment.\nStrong programming skills in Python and SQL.\nFamiliarity with Docker, Kubernetes, and PySpark.\nSolid understanding of machine learning techniques and algorithms.\nExperience working with cloud platforms, preferably GCP.\nExcellent problem-solving skills and ability to work independently as well as part of a team.\nStrong communication and presentation skills, with the ability to explain complex analytical concepts to non-technical stakeholders.\n\n]]>",
        "skills": [
            "Generative AI",
            "Machine Learning",
            "Nlp",
            "Docker",
            "Pyspark",
            "XGBoost",
            "Random Forest",
            "Sql",
            "Kubernetes",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "(IND) DISTINGUISHED, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "14-16 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThe Data Science team at Walmart Global marketplace is focused on applying the latest research in machine learning, statistics and optimization to solve business problems in areas powering Marketplace growth and operation efficiency for multiple countries within Walmart's global operations. We, build and deploy Machine Learning models and use the latest algorithms and technology to empower business decision-making. We work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions.\n\nWhat you'II Do:\n\nAs a Distinguished Data Scientist in the Marketplace data sciences team, you'll have the opportunity to -\n\nDrive innovative strategic solutions for Marketplace utilizing advanced SOTA AI and ML solutions at large scale and at accelerated pace.\nDrive data-derived insights by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nWork closely with Directors, Sr. Managers of Data Science, and leaders of Architecture, Engineering, Product & business teams to drive the Organizational strategy around Marketplace.\nDirect the gathering of data, assess data validity and synthesize data into large analytics datasets to support project goals\nMentor and guide a set of Data Scientists, Data Analysts across IDC and US to drive Marketplace growth and operation efficiency using advanced Machine Learning and Gen AI.\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train AI/ML models for replication for future projects\nDeploy and maintain the data science solutions\nCommunicate recommendations to business partners and influence future plans based on insights\nConsult with product and business stakeholders regarding algorithm-based recommendations and be a thought-leader to develop these into business actions.\nGuides data scientists, senior data scientists and staff data scientists across the domain DS team to ensure on-time delivery of ML products.\nProactive collaboration with cross-functional teams on driving the end-to-end content improvement.\nResearch oriented development to find innovative solutions on content improvement, with a strong grasp of multi-lingual and multi-modal foundational models.\nGuides the data science team members to ensure on-time delivery of ML products\nProactive identification of complex business problems that can be solved using advanced ML and Computer Vision, finding opportunities and gaps in the current business domain\nEvaluates proposed business cases for projects and initiatives\nTranslates business requirements into strategies, initiatives, and projects and aligns them to business strategy and objectives, and drives the execution of deliverables\nSets relevant deliverables based on the established success criteria and define key metrics to measure progress and effectiveness of the solution\nQuantifies business impact and ensures regular impact measurement of all ML products in the domain.\nIdentifies and reviews model evaluation metrics based on analytical requirements\nEnsures testing information is documented and maintained by the team\nGuide teams to perform model deployment following MLOps guidelines.\nPlay a key role to solve complex problems, pivotal to Walmart's business and drive actionable insights\nUtilize product mindset to build, scale and deploy holistic data science products after successful prototyping\nDemonstrate incremental solution approach with agile and flexible ability to overcome practical problems\nArticulate and present recommendations to business partners and influence plans based on insights\nPartner and engage with associates in other regions for delivering the best services to customers around the globe\nWork with the customer-centric mindset to deliver high-quality business-driven analytic solutions.\nDrive innovation in approach, method, practices, process, outcome, delivery, or any component of end-to-end problem solving\nProactively engages in the external community to build Walmart's brand and learn more about industry practices\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity\n\nWhat You'II Bring:\n\nBachelor's with > 20 years, Masters > 17 years OR Ph.D. in Comp Science/Statistics/Mathematics with > 14 years of relevant experience. Educational qualifications should be Computer Science/Statistics/Mathematics or a related area.\nExperience of acting as a tech lead for > 10 years.\nAbility to lead data science projects end to end.\nDeep experience in building data science solutions at the scale of billions of datapoints across many countries.\nStrong experience in machine learning, supervised and unsupervised: NLP, Classification, Data/Text Mining, Multi-modal supervised and unsupervised models, Neural Networks, Deep Learning Algorithms, Generative AI models.\nExperience in analyzing complex problems and translating them into analytical solutions.\nDeep experience in analyzing complex problems and translating them into analytical solutions.\nStrong experience in Computer Vision, image processing, object detection, Deep learning. Exposure to image features, edge detection, blur, sharpness, corner detection, face detection, knowledge of how image data is represented.\nKnowledge of Deep learning, CNNs, and components of CNNs, loss functions for classification, object detection, segmentation.\nExperience in machine learning: Classification models, regression models, NLP, Forecasting, Unsupervised models, Optimization, Recommendation models, deep-learning, Graph ML, Causal inference, Causal ML, Statistical Learning, experimentation\nAbility to utilize data science solutions outcomes to drive predictive and prescriptive analytics.\nExperience with big data analytics - identifying trends, patterns, and outliers in large volumes of data\nAbility to scale and deploy data science solutions.\nExperience in LLMs, VLMs, embedding generation from multimodal data, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nStrong Experience in Python, PySpark, OpenCV, Python, C++, Keras, tensorflow, pytorch, big data platforms like Hadoop\nGoogle Cloud platform, Vertex AI, Kubeflow, model deployment, Kafka streaming, API development & deployment, CI/CD, MLOPs\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 6 years experience in an analytics related field. Option 2: Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 3: 8 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2140023",
        "skills": [
            "Generative AI",
            "Statistical Models",
            "Kubeflow",
            "Vertex AI",
            "Machine Learning",
            "Hadoop",
            "Google Cloud Platform",
            "Pyspark",
            "Kafka",
            "Deep Learning",
            "Tensorflow",
            "Pytorch",
            "MLops",
            "Opencv",
            "Keras",
            "Python",
            "Computer Vision"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST - MLE",
        "company_name": "Walmart Global Tech India",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThis is the team which builds reusable technologies that aid in acquiring customers, onboarding and empowering merchants besides ensuring a seamless experience for both these stakeholders. We also optimize tariffs and assortment, adhering to the Walmart philosophy - Everyday Low Cost. In addition to ushering in affordability, we also create personalized experiences for customers the omnichannel way, across all channels - in-store, on the mobile app and websites. Marketplace is the gateway to domestic and international Third-Party sellers; we enable them to manage their end-to-end onboarding, catalog management, order fulfilment, return & refund management. Our team is responsible for design, development, and operations of large-scale distributed systems by leveraging cutting-edge technologies in web/mobile, cloud, big data & AI/ML. We interact with multiple teams across the company to provide scalable robust technical solutions.\n\nWhat you'll do:\n\nSenior Machine Learning Engineeris responsible forbuildingscalableend-to-enddata science solutionsfor ourdata products.\n\nWork closely with data engineers and data analysts to help build ML- and statistics-driven data quality and continuous data monitoring workflows\nSolve business problems byscalingadvanced Machine Learning algorithms and complex statistical models on large volumes of data\nOwn theMLOpslifecycle, from data monitoring to refactoringdata science code tobuilding robust model monitoring workflows formodel lifecycle management\nDemonstratestrong thought-leadership and consult with product and business stakeholders to build, scale and deploy holisticmachine learning solutionsafter successful prototyping.\nFollowindustry best practices, stay up to date with and extend the state of the art in machine learning research and practice and drive innovation\nPromoteand support company policies, procedures, mission, values, and standards of ethics and integrity.\n\nWhat you'll bring:\n\nKnowledge of the foundations of machine learning and statistics\nExperience withweb service standards and related patterns (REST,gRPC)\nExperienced in architecting solutions with Continuous Integration and Continuous Delivery in mind\nFamiliar with distributed in-memory computing technologies\nSolid experience working with state-of-the-art supervised and unsupervised machine learning algorithms on real-world problems\nStrong Python coding and package development skills\nExperience with Big Data and analytics in general leveraging technologies like Hadoop, Spark, and MapReduce;Ability to work in a big data ecosystem - expert in SQL/Hive and ability to work with Spark.\nAble to refactordata science code andhas collaborated with data scientists in developing ML solutions.\nExperience playing the role of full-stack data scientist and taking solutions to production.\nExperience developing proper metrics instrumentation in software components, to help facilitate real-time and remote troubleshooting/performance monitoring.\nEducational qualifications should be preferably in Computer Science, Statistics,Engineeringor a related area.\nGood effective communication (both written and verbal) skills and the ability to present complex ideas in a clear & concise way, to different audiences. Ateam player with good work ethics\n\n\n\nMandatory Skills: Machine Learning, Big Data Skills, Python, R\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2155678",
        "skills": [
            "R",
            "GRPC",
            "Machine Learning",
            "REST",
            "Hadoop",
            "Spark",
            "Big Data",
            "Mapreduce",
            "Python",
            "Sql"
        ]
    },
    {
        "job_title": "Data Scientist III",
        "company_name": "Walmart Global Tech India",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nWalmart U.S. Tech: This is the team which builds reusable technologies that aid in acquiring customers, onboarding and empowering merchants besides ensuring a seamless experience for both these stakeholders. They also optimize tariffs and assortment, adhering to the Walmart philosophy - Everyday Low Cost. In addition to ushering in affordability, they also create personalized experiences for customers the omnichannel way, across all channels - in-store, on the mobile app and websites.\n\nOur team is responsible for design, development, and operations of large-scale data systems operating at petabytes scale. We focus on real-time indexing pipelines, web crawling, streaming analytics, distributed machine learning infrastructure. We interact with multiple teams across the company to provide scalable robust technical solutions.\n\nWhat you will do:\n\nYou'll have the opportunity to\n\nAs a Data scientist, Search for Walmart Global Tech, you'll have the opportunity to Apply and/or develop ML solutions to develop efficient and scalable models at Walmart scale.\nLeverage data science tools and techniques, keeping abreast with the latest in the community to solve problems for Walmart.\nCollaborate with counterparts in business, engineering, and science to find impactful solutions to business problems.\nDefine and/or own the model goodness metrics and track the business impact over time.\nPresent recommendations from complex analysis to business partners in clear and actionable form, influencing the future.\nDevelop PoC, present lucidly to the business and evolve the solutions.\nTake forward the solutions into Pipelines/APIs as needed by the business.\nResearch, learn/disseminate & adapt new technologies to solve problems & improve upon existing solutions.\nAdopt Wal-Mart's quality standards and develop and recommend process standards and best practices across the retail industry.\nManage the continuous improvement of data science and machine learning by following industry best practices and staying up to date with and extending the state-of-the-art in Machine Learning research.\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nHandle multiple projects at the same time.\nPeer review and publish work in top tier ML/AI conferences such as NIPS, ICML, AAAI and COLT\nParticipate and speak at various external forums such as research conferences and technical summits.\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\n\nWhat you will bring:\n\nMaster's degree in Computer Science/Mathematics/Statistics or a related area with 3-6 years of experience. Experience should be relevant to the role.\nExperience in handling large datasets using big data tools such as Spark, Hive, and BigQuery.\nAbility to apply deep understanding of machine learning algorithms and deep learning libraries, like TensorFlow or PyTorch, in problem-solving.\nExpected proficiency in Python with the ability to write efficient, reusable code for automating machine learning pipelines and data processes. Familiarity with Java is optional but can be beneficial.\nDemonstrable expertise in Natural Language Processing (NLP), information retrieval, ranking, along with solid foundational knowledge of machine learning to optimize and innovate in project tasks.\nCapability to implement and optimize large language models (LLMs) to leverage user-provided data, enhancing the user experience and generating valuable insights.\nProven ability to learn and adapt to new technologies and frameworks quickly\nExcellent organization, communication, interpersonal skills.\nExperience with multiple stakeholder management, data based story-telling, mentoring peers and juniors, multiple project handling at the same time.\n\nAdditional Qualifications:\n\nLarge scale distributed systems experience, including scalability and fault tolerance.\nExposure to cloud infrastructure, such as Open Stack, Azure, GCP, or AWS\nA continuous drive to explore, improve, enhance, automate and optimize systems and tools.\nExposure to information retrieval, statistics, and machine learning.\n\nAbout Global Tech:\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That's what we do at Walmart Global Tech. We're a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world's largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail\n\nFlexible, hybrid work:\n\nWe use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.\n\nBenefits:\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nEqual Opportunity Employer:\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas, and opinions while being inclusive of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics or related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field. Option 3: 4 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2104572",
        "skills": [
            "Tensorflow",
            "BigQuery",
            "Machine Learning",
            "Hive",
            "Pytorch",
            "Spark",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist - Periscope",
        "company_name": "McKinsey & Company",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who You'll Work With\n\nYou will be based in one of our offices in Bengaluru or Gurugram as part of our Growth, Marketing & Sales practice.\n\nThe Growth, Marketing & Sales practice strives to help client in both consumer and business-to-business environments on a wide variety of marketing and sales topics.\n\nThe mission of this practice is to help clients achieve marketing-driven profit growth.\n\nOur clients benefit from our experience in core areas of marketing such as customer insights, marketing ROI, digital marketing, CLM pricing, and sales and channel management.\n\nYour Impact\n\nYou will also be responsible for developing deep Retail domain understanding in at least one of the following areas Customer Analytics, Pricing Optimization, Inventory Management, E-commerce, or Digital Transformation.\n\nCollaboration with business stakeholders, engineers and internal teams to build and implement extraordinary retail focused data products (reusable asset) and solutions and delivering them right to the client will be of utmost importance.\n\nYou will work on the frameworks and libraries that our teams of Data Scientists and Data Engineers use to progress from data to impact.\n\nYou will guide global companies through data science solutions to transform their businesses and enhance performance across industries including E-commerce, Grocery, F&B and CPG\n\nYour Qualifications and Skills\n\nMaster's degree in computer science, engineering or mathematics, or equivalent experience\n5+ years of relevant experience with strong foundations of statistics and machine learning techniques\nProven experience applying machine learning techniques to solve business problems\nProven experience in translating technical methods to non-technical stakeholder\nProven experience writing production-grade code (Python / Pyspark) for machine learning in a professional setting\nStrong understanding of analytics libraries (e.g., pandas, numpy, matplotlib, scikit-learn, statsmodels, kedro, mlflow)\nExperience in any cloud platforms (AWS, Azure, or GCP)\nFamiliarity with containerization technologies (Docker, Docker-compose)\nExperience with Git-based workflows and automation frameworks, specifically GitHub Actions and GitLab CI/CD\nFamiliarity or hands-on experience with data visualization tools (PowerBI, Tableau, etc.)",
        "skills": [
            "scikit-learn",
            "mlflow",
            "GitHub Actions",
            "Docker-compose",
            "GitLab CI CD",
            "kedro",
            "statsmodels",
            "Pyspark",
            "Tableau",
            "Pandas",
            "Numpy",
            "Gcp",
            "Powerbi",
            "Docker",
            "Matplotlib",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist - NLP",
        "company_name": "Gartner",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About This Role ()\n\nIn Gartner's Services Data Science team, we innovate the way our team helps clients receive value, so technology leaders will be able to make smarter decisions in a different way.\n\nWe are searching for a talented data scientist to join our team.You will have access to the best facilities, technology and expertise within the industry and will work on challenging business problems. This is an excellent opportunity to be part of a new venture, in a start-up environment where you can truly develop your skill set and knowledge and bring impact to the team.\n\nWhat You'll Do\n\nHere is a sample of some initiatives that our data scientists work on\n\nDesigning and implementing state of the art Large Language Model (LLM) based agents that seamlessly synthesize complex information and initiate important actions in a business workflow.\nUsing advanced Generative AI techniques deriving actionable insights from unstructured text data, such as call transcripts and emails.\nPredicting client interest basis their digital footprint and making relevant recommendations to drive higher client value delivery\nLeverage statistical and machine learning techniques to extract actionable insights from client retention data.\nDevelop customer churn prediction models that proactively identify at-risk clients,\nBuild tools to process structured and unstructured data\nEngineering features and signals to train ML model from diverse data collection\n\nWhat You'll Need\n\nBS/MS/PhD in Computer Science or other technology, Math, Physics, Statistics or Economics (focus on Natural Language Processing, Information Retrieval a plus)\n3+ years experience in data science methodologies as applied to live initiatives or software development\nMinimum 3+ years of experience in python coding and statistical analysis\nMinimum 2 years working experience in several of the following:\nPrompt Engineering and working with LLMs\nMachine Learning and statistical techniques\nData mining and recommendation systems\nNatural Language Processing and Information Retrieval\nExperience working with large volumes of data\nUser behavior modeling\n\nWhat you are :\n\nA team player.You get along well with your colleagues and are always ready to help get things done. You enjoy working on projects with multiple people and share knowledge.\nPassionate about learning.You thrive on complex technical challenges and are always eager to learn the latest technologies.\nOrganized and detailed-oriented.You think ahead of time about how best to implement new features, and your code is clean,well-organizedand properly documented.\nInnovative. You are always proactively looking for opportunities to problem solve using innovative methods that impact the business\n\nWhat We Offer\n\nA collaborative, positive culture. You'll work with people who are as enthusiastic, smart and driven as you are. You'll be managed by the best too.\nLimitless growth and learning opportunities. We offer the excitement of a fast-paced entrepreneurial workplace and the professional growth opportunities of an established global organization.\n\nWho are we\n\nAt Gartner, Inc. (NYSE:IT), we guide the leaders who shape the world.\n\nOur mission relies on expert analysis and bold ideas to deliver actionable, objective insight, helping enterprise leaders and their teams succeed with their mission-critical priorities.\n\nSince our founding in 1979, we've grown to more than 20,000 associates globally who support 15,000 client enterprises in 90 countries and territories. We do important, interesting and substantive work that matters. That's why we hire associates with the intellectual curiosity, energy and drive to want to make a difference. The bar is unapologetically high. So is the impact you can have here.\n\nWhat makes Gartner a great place to work\n\nOur sustained success creates limitless opportunities for you to grow professionally and flourish personally. We have a vast, virtually untapped market potential ahead of us, providing you with an exciting trajectory long into the future. How far you go is driven by your passion and performance.\n\nWe hire remarkable people who collaborate and win as a team. Together, our singular, unifying goal is to deliver results for our clients.\n\nOur teams are inclusive and composed of individuals from different geographies, cultures, religions, ethnicities, races, genders, sexual orientations, abilities and generations.\n\nWe invest in great leaders who bring out the best in you and the company, enabling us to multiply our impact and results. This is why, year after year, we are recognized worldwide as a great place to work.\n\nWhat do we offer\n\nGartner offers world-class benefits, highly competitive compensation and disproportionate rewards for top performers.\n\nIn our hybrid work environment, we provide the flexibility and support for you to thrive working virtually when it's productive to do so and getting together with colleagues in a vibrant community that is purposeful, engaging and inspiring.\n\nReady to grow your career with Gartner Join us.\n\nThe policy of Gartner is to provide equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, ancestry, disability, veteran status, or any other legally protected status and to seek to advance the principles of equal employment opportunity.\n\nGartner is committed to being an Equal Opportunity Employer and offers opportunities to all job seekers, including job seekers with disabilities. If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access the Company's career webpage as a result of your disability. You may request reasonable accommodations by calling Human Resources at +1 (203) 964-0096 or by sending an email to [HIDDEN TEXT].\n\nJob Requisition ID:96693\n\nBy submitting your information and application, you confirm that you have read and agree to the country or regional recruitment notice linked below applicable to your place of residence.\n\nGartner Applicant Privacy Link: https://jobs.gartner.com/applicant-privacy-policy\n\nFor efficient navigation through the application, please only use the back button within the application, not the back arrow within your browser.",
        "skills": [
            "Recommendation Systems",
            "Generative AI",
            "Prompt Engineering",
            "Machine Learning",
            "Natural Language Processing",
            "Data Mining",
            "Information Retrieval",
            "Statistical Analysis",
            "Python"
        ]
    },
    {
        "job_title": "IN-Senior Associate_Data Scientist_D&A_Advisory_Gurgaon",
        "company_name": "PwC India",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nDesign, develop, and maintain interactive dashboards and visualizations.\nWork closely with data analysts to understand data requirements and translate them into visualformats.\nSelect appropriate visualization tools (PowerBI/Tableau/Qlik) and techniques to display data effectively.\nEnsure accuracy, consistency, and integrity of the data displayed.\nConduct user testing and gather feedback to improve visualization designs.\nTrain clients and team members on data visualization best practices and tools.\n\nMandatory Skill Sets\n\n\nPower BI/Tableau/Qlik\n\nPreferred Skill Sets\n\nPower BI/Tableau/Qlik\n\nYears Of Experience Required\n\n4 7 yrs\n\nEducation Qualification\n\nB.tech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nStructured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date",
        "skills": [
            "Tableau",
            "Power Bi",
            "Qlik"
        ]
    },
    {
        "job_title": "IN-Senior Associate_Data Scientist_D&A_Advisory_Gurgaon",
        "company_name": "PwC India",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nProven experience as a Data Scientist or similar role in a consulting or corporate environment.\nProficient in programming languages such as Python or R.\nExperience with data manipulation and analysis using SQL and big data technologies.\nStrong knowledge of machine learning libraries (e.g., TensorFlow, PyTorch, Scikit-learn).\nResearch and develop cutting-edge AI models and components to solve client-specific challenges.\nCollaborate with clients and cross-functional teams to define project scopes and deliver innovative AI solutions.\nOptimize AI algorithmsforperformance and scalability.\n\nMandatory Skill Sets\n\n\nTensorFlow, PyTorch, Scikit-learn, SQL\n\nPreferred Skill Sets\n\nTensorFlow, PyTorch, Scikit-learn, SQL\n\nYears Of Experience Required\n\n4 7 yrs\n\nEducation Qualification\n\nB.tech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nStructured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date",
        "skills": [
            "R",
            "Scikit-learn",
            "Tensorflow",
            "Pytorch",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Aladdin Financial Engineering Data Scientist Associate",
        "company_name": "BlackRock",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About This Role\n\nBlackRock Overview:\n\nBlackRock is one of the world's preeminent asset management firms and a premier provider of global investment management, risk management and advisory services to institutional, intermediary and individual investors around the world. BlackRock offers a range of solutions from rigorous fundamental and quantitative active management approaches aimed at maximizing outperformance to highly efficient indexing strategies designed to gain broad exposure to the world's capital markets. Our clients can access our investment solutions through a variety of product structures, including individual and institutional separate accounts, mutual funds and other pooled investment vehicles, and the industry-leading iShares ETFs.\n\nAladdin Financial Engineering Group (AFE)\n\nAFE is a diverse and global team with a keen interest and expertise in all things related to technology and financial analytics. The group is responsible for the research and development of quantitative financial and behavioral models and tools across many different areas single-security pricing, prepayment models, risk, return attribution, liquidity, optimization and portfolio construction, scenario analysis and simulations, etc. and covering all asset classes. The group is also responsible for the technology platform that delivers those models to our internal partners and external clients, and their integration with Aladdin.\n\nAFE conducts leading research on the areas above, delivering state-of-the-art models. AFE publishes applied scientific research frequently, and our members present regularly at leading industry conferences. AFE engages constantly with the sales team in client visits and meetings.\n\nJob Description\n\nYou can help conduct research to build quantitative financial models and portfolio analytics that help managing most of the money of the world's largest asset manager. You can bring all yourself to the job. From the top of the firm down we embrace the values, identities and ideas brought by our employees.\n\nWe are looking for curious people with a strong background in data science, quantitative research and machine learning, have awesome problem-solving skills, insatiable appetite for learning and innovating, adding to BlackRock's vibrant research culture.\n\nIf any of this excites you, we are looking to expand our team. We currently have Data Scientist role with the AFE Investment AI (IAI) Team, India (Mumbai or Gurugram location). The securities market is undergoing a massive transformation as the industry is embracing machine learning and, more broadly, AI, to help evolve the investment process. Pioneering this journey at BlackRock, the team has better deliver applied AI investment analytics to help both BlackRock and Aladdin clients achieve scale through automation while safeguarding alpha generation. The IAI team combines AI / ML methodology and technology skills with deep subject matter expertise in fixed income, equity, and multi-asset markets, and the buyside investment process.\n\nWe are building next generation liquidity, security similarity and pricing models leveraging our expertise in quantitative research, data science and machine learning. The models we build use innovative machine learning approaches, have real practical value and are used by traders and portfolio managers alike. Our models use cutting edge econometric/statistical methods and tools. The models themselves have real practical value and are used by traders, portfolio managers and risk managers representing different investment styles (fundamental vs. quantitative) and across different investment horizons. Research is conducted predominantly in Python and Scala, and implemented into production by a separate, dedicated team of developers. These models have a huge footprint of usage across the entire Aladdin client base, and so we place special emphasis on scalability and ensuring adherence to BlackRock's rigorous standards of model governance and control.\n\nBackground And Responsibilities\n\nWe are looking to hire a Data Scientist with 4+ years experience to join AFE Investment AI India team focusing on Trading and Liquidity to work closely with other data scientists/researchers to support Risk Mangers, Portfolio Managers and Traders. We build cutting edge liquidity analytics using a wide range of ML algos and a broad array of technologies (Python, Scala, Spark/Hadoop, GCP, Azure). This role is a great opportunity to work closely with the Portfolio Managers, Risk Managers and Trading team, spanning areas such as:\n\nDesign, develop, and maintain data pipelines to extract, transform, and load data from various sources into our data warehouse/lake.\nWork with data scientists and analysts to understand data needs and design appropriate data models.\nImplement data quality checks and ensure the accuracy and consistency of data throughout the processing pipeline.\nPerform analysis of large data sets comprising of market data, trading data and derived analytics.\nDesign and develop model surveillance framework.\nAutomate data processing tasks using scripting languages (e.g., Python, Scala) and orchestration tools (e.g., Airflow, Luigi).\nUtilize cloud-based data platforms (e.g., GCP, Azure, etc.) to manage and process large datasets efficiently.\nImplement the ML models/analytics for Trading/Liquidity and integrate into Aladdin analytical system in accordance with BlackRock's model governance policy.\n\nQualifications\n\nB.Tech / B.E. / M.Sc. degree in a quantitative discipline (Mathematics, Physics, Computer Science, Finance or similar area). MS/M.Tech. / PhD is a plus.\nStrong background in Mathematics, Statistics, Probability, Linear Algebra\nKnowledgeable about data mining, data analytics, data modeling\nExperience with data engineering tools and technologies (e.g., Apache Spark, Hadoop).\nStrong understanding of relational and non-relational databases (e.g., SQL, NoSQL).\nProficiency in scripting languages for data manipulation and automation (e.g., Python, Scala).\nExperience working with cloud platforms for data storage and processing (e.g., Azure, GCP).\nAbility to work independently and efficiently in a fast-paced and team-oriented environment.\nPrevious experience or knowledge in fixed income market and market liquidity is not required but a big plus.\n\nFor professionals with no prior financial industry experience, this position is a unique opportunity to gain in-depth knowledge of the asset management process in a world-class organization.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",
        "skills": [
            "Airflow",
            "Luigi",
            "Nosql",
            "Hadoop",
            "Gcp",
            "Scala",
            "Spark",
            "Azure",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Senior Principal Data Scientist",
        "company_name": "GSK",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Site Name: Bengaluru Luxor North Tower, Poznan Pastelowa\n\nPosted Date: May 5 2025\n\nKey Responsibilities\n\nLead NLP Initiatives: Drive development and implementation of advanced NLP models to analyze unstructured text data related to clinical monitoring, quality, and risk management.\nBuild and Optimize Predictive Models: Apply machine learning techniques to identify patterns and trends, with a focus on preventing/corrective actions for improving quality assurance practices.\nCollaborate Across Teams: Partner with cross-functional teams, including other data science business and tech teams within GSK, quality assurance, and clinical operations, to understand and solve critical business challenges.\nEnsure Model Governance: Implement and monitor best practices for model governance, accuracy, and reliability in a highly regulated environment, ensuring adherence to industry standards and regulatory requirements.\nData Preprocessing and Cleaning: Oversee data acquisition, preprocessing, and quality control of complex datasets, working with structured and unstructured data in the R&D domain.\nLead Data Science Projects: Mentor and guide junior data scientists and analysts, ensuring robust project management, timely delivery, and effective stakeholder communication.\nContinuous Improvement: Identify opportunities to improve data workflows, tooling, and processes for enhanced productivity and reproducibility.\nResearch and Innovation: Stay abreast of industry trends and advancements in NLP, machine learning, and generative AI to drive innovation within the quality and risk management framework.\nPerformance Metrics and Reporting: Establish and track key performance indicators to assess model impact and value, aligning outcomes with organizational quality and risk management goals.\nDevelop Gen AI Solutions: Explore and integrate generative AI models to innovate on complex language tasks such as summarization, data synthesis, and anomaly detection.\n\nEducation Requirements\n\n\nA bachelor's degree in computer science, statistics, mathematics\n\nJob Related Experience\n\nProven experience in data science, predictive modelling, and statistical analysis.\nProficiency in programming languages commonly used in data science, such as Python, R, and SQL.\nExperience with data visualization tools like Power BI, Shiny Web Apps, and similar platforms.\nApplication of machine learning algorithms and statistical modelling techniques.\nNatural Language Processing (NLP) to derive actionable insights.\nStrong problem-solving skills and the ability to translate business problems into analytical use-cases.\nExcellent communication skills to effectively interact with business stakeholders and tech partners.\nA good understanding of drug research and development and quality\n\nOther Job-Related Skills\n\nAdvanced knowledge of analytics tools and capabilities: The candidate should be proficient in using advanced analytics tools and be capable of leveraging them to analyse complex data sets.\nStrong understanding of R&D Quality and Risk Management: The candidate should have a deep understanding of quality and risk management within an R&D context.\nData Analysis Skills: The candidate should be able to interpret complex data and translate it into information that can be understood by non-technical stakeholders.\nCommunication Skills: The candidate should have strong communication skills to effectively convey data insights to business stakeholders.\nProblem-Solving Skills: The candidate should have strong problem-solving skills to translate business problems into analytical use-cases.\nTechnical Skills: Proficiency in programming languages commonly used in data science, such as Python, R, and SQL, and experience with data visualization tools like Power BI, Shiny Web Apps, and similar platforms.\nKnowledge of Machine Learning: The candidate should have a deep understanding of machine learning algorithms and statistical modelling techniques.\nFamiliarity with Natural Language Processing (NLP): The candidate should have knowledge of NLP to handle use-cases in areas like text summarizations, sentiment analysis, topic modelling, and trend analysis.\nLeadership Skills: The candidate should have excellent leadership skills to effectively interact with business stakeholders and tech partners, and to drive the implementation of data science tools and solutions in a matrixed environment.\n\nWhy GSK\n\n\nUniting science, technology and talent to get ahead of disease together.\n\n\nGSK is a global biopharma company with a special purpose to unite science, technology and talent to get ahead of disease together so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns as an organisation where people can thrive. We prevent and treat disease with vaccines, specialty and general medicines. We focus on the science of the immune system and the use of new platform and data technologies, investing in four core therapeutic areas (infectious diseases, HIV, respiratory/ immunology and oncology).\n\nOur success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves feeling welcome, valued, and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.\n\nImportant notice to Employment businesses/ Agencies\n\nGSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.\n\nIt has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.\n\nGlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.\n\nIf you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in gsk.com, you should disregard the same and inform us by emailing [HIDDEN TEXT], so that we can confirm to you if the job is genuine.",
        "skills": [
            "R",
            "Machine Learning",
            "Statistical Analysis",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Staff, Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "8-14 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Staff, Data Scientist\nExperience: 8 - 14 years\nLocation: Bangalore\nAbout EBS team:\nEnterprise Business Services is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, Associate Digital Experience. Our team is responsible for design and development of solution that knows our consumer's needs better than ever by predicting what they want based on unconstrained demand, and efficiently unlock strategic growth, economic profit, and wallet share by orchestrating intelligent, connected planning and decisioning across all functions. We interact with multiple teams across the company to provide scalable robust technical solutions. This role will play crucial role in overseeing the planning, execution and delivery of complex projects within team\nAbout Team\nThe data science team at Enterprise Business Services Pillar at Walmart Global Tech focuses on using the latest research in machine learning, statistics, and optimization to solve business problems. We mine data, distill insights, extract information, build analytical models, deploy Machine Learning algorithms, and use the latest algorithms and technology to empower business decision-making. In addition, we work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions. Advanced analytical algorithms driven by our team will help Walmart to optimize business operations, business practices and change the way our customers shop.\nThe data science community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Labs will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat You Will Do\nAs a Staff Data Scientist for Walmart Global Tech, you'll have the opportunity to Drive data-derived insights across a wide range of retail & Finance divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assess data validity and synthesize data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train AI/ML models for replication for future projects\nGuides. data scientists, senior data scientists & staff data scientists across multiple sub-domains to ensure on-time delivery of ML products\nDrive efficiency across the domain in terms of DS and ML best practices, ML Ops practices, resource utilization, reusability and multi-tenancy.\nLead multiple complex ML products and guide senior tech leads in the domain in efficiently leading their products.\nDrive synergies across different products in terms of algorithmic innovation and sharing of best practices.\nWhat You Will Bring\nMaster's with > 12 years OR Ph.D. with > 8 years of relevant experience. Educational qualifications should be Computer Science/Statistics/Mathematics or a related area.\nMinimum 6 years of experience as a data science technical lead\nAbility to lead multiple data science projects end to end.\nDeep experience in building data science solution in areas like fraud prevention, forecasting, shrink and waste reduction, inventory management, recommendation, assortment and price optimization\nDeep experience in simultaneously leading multiple data science initiatives end to end from translating business needs to analytical asks, leading the process of building solutions and the eventual act of deployment and maintenance of them Strong experience in machine learning: Classification models, regression models, NLP, Forecasting, Unsupervised models, Optimization, Graph ML, Causal inference, Causal ML, Statistical Learning, experimentation & Gen-AI\nIn Gen-AI, it is desirable to have experience in embedding generation from training materials, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nAbility to scale and deploy data science solutions.\nStrong Experience with one or more of Python and R.\nExperience in GCP/Azure\nStrong Experience in Python, PySpark\nGoogle Cloud platform, Vertex AI, Kubeflow, model deployment\nStrong Experience with big data platforms Hadoop (Hive, Map Reduce, HQL, Scala)\nExperience with GPU/CUDA for computational efficiency\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.",
        "skills": [
            "Computational Algorithms",
            "GPU CUDA",
            "Statistical Models",
            "AI ML Models",
            "R",
            "Vertex AI",
            "Kubeflow",
            "Map Reduce",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Pyspark",
            "Scala",
            "Hive",
            "Gcp",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Manager - PD (Data Scientist)",
        "company_name": "KPMG India",
        "experience": "9-11 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Description\n\nRoles & responsibilities\n\nWe are seeking a skilled Data Scientist to join our growing team. The ideal candidate will have a passion for harnessing data to solve complex problems, drive decision-making, and create value. You will work closely with cross-functional teams to analyze data, develop predictive models, and contribute to data-driven strategies that support our business objectives.\n\nKey Responsibilities\n\nData Analysis & Modeling: Analyze large datasets to extract insights and build predictive models to solve business problems.\nMachine Learning Development: Design, implement, and evaluate machine learning models using frameworks such as TensorFlow, PyTorch, or scikit-learn.\nData Wrangling: Clean, transform, and prepare data from multiple sources for analysis and modeling purposes.\nAutoML Implementation: Utilize AutoML tools to streamline model selection and hyperparameter tuning, enhancing model performance and efficiency.\nCloud Computing: Leverage cloud platforms (e.g., AWS, Google Cloud, Azure) for data storage, processing, and deployment of machine learning models.\nCollaboration: Work closely with cross-functional teams including data engineers, product managers, and stakeholders to define project requirements and translate business needs into data solutions.\nAI Impact Assessment: Evaluate and implement AI technologies that can enhance data science processes and outcomes, keeping abreast of industry trends and emerging technologies.\nDocumentation & Reporting: Document methodologies, model performance, and insights for stakeholders, ensuring transparency and reproducibility of analytics processes.\n\nMandatory technical & functional skills\n\nProficiency in programming languages such as Python or R and SQL.\nExperience with machine learning and deep learning libraries and frameworks, including TensorFlow, Keras, and Scikit-learn.\nStrong knowledge of data wrangling tools and techniques (e.g., Pandas, NumPy).\nFamiliarity with cloud computing services (Preferably Azure) for data storage and processing.\nUnderstanding of AutoML tools and frameworks to automate model selection and hyperparameter tuning.\nExperience with data visualization tools (e.g., Tableau, Matplotlib, Seaborn) to present insights effectively.\nKnowledge of big data technologies (e.g., Hadoop, Spark) is a plus.\n\nPreferred Technical & Functional Skills\n\n\nStrong oral and written communication skills with the ability to communicate technical and non-technical concepts to peers and stakeholders\n\nAbility to work independently with minimal supervision, and escalate when needed\n\nKey behavioural attributes/requirements\n\nAbility to mentor junior developers\n\nAbility to own project deliverables, not just individual tasks\n\nUnderstand business objectives and functions to support data needs\n\n#KGS\n\nResponsibilities\n\nRoles & responsibilities\n\nWe are seeking a skilled Data Scientist to join our growing team. The ideal candidate will have a passion for harnessing data to solve complex problems, drive decision-making, and create value. You will work closely with cross-functional teams to analyze data, develop predictive models, and contribute to data-driven strategies that support our business objectives.\n\nQualifications\n\nThis role is for you if you have the below\n\nEducational Qualifications\n\nBachelor's degree in Computer Science or a related field (or equivalent work experience).\n\nWork Experience\n\n\n9+ Years of Experience",
        "skills": [
            "Scikit-learn",
            "R",
            "AutoML",
            "Numpy",
            "Sql",
            "Keras",
            "Tensorflow",
            "Hadoop",
            "Tableau",
            "Pandas",
            "Seaborn",
            "Python",
            "Azure",
            "Matplotlib",
            "Spark"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Ericsson",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Join our Team\n\nAbout this opportunity:\n\nWe are now looking for a Senior Data Scientist to be responsible for developing AI/ML methods, processes, and systems to extract knowledge or insights to drive the future of artificial intelligence. Provide data science tasks, perform advanced statistical analysis, and create insights into data to provide to the business actionable insights, identify trends, and measure performance which address business problems. Collaborate with business and process owners to understand business issues, and with engineers to implement and deploy scalable solutions, where applicable.\n\nWhat you will do:\n\nApply and/or develop statistical modeling techniques (such as deep neural networks, Bayesian models, Generative AI, Forecasting), optimization methods and other ML techniques.\nSynthesize problems into data questions.\nConvert data into practical insights.\nAnalyze and investigate data quality for identified data and communicate it Product Owner, Business Analyst, and other relevant stakeholders.\nCollect Data, explore it, and perform analysis to extract information suitable to the business need. Identify gaps in the data, aggregate data as per business need. Design & perform Data Analysis, Data Validation, Data Transformation, Feature Extraction.\nDecide approach for addressing business needs with Data & analytics. Understand end user needs and work accordingly with identifying new features in the data.\nDevelop Data Science and Engineering Infrastructure &Tools.\nDerive key metrics suitable for the use-case and present the analysis to key stakeholder.\n\nThe skills you bring:\n\n5-10 years of relevant Industry experience.\nA bachelor's or higher degree in Computer Science, Statistics, Mathematics, or related disciplines.\nAbility to analyze data and communicate outcome to key stakeholders exploring new data source.\nExcellent coding skills in python, R, SQL etc. Understanding of cloud services.\nEvidence of academic training in Statistics. Deep/broad knowledge of machine learning, statistics, optimization, or related field.\nA genuine curiosity about new and applied technology and software engineering coupled with a high degree of business understanding.\nExperience in large scale product development is a plus.\nExposure to Generative AI and Large Language Models.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 766042\n\n]]>",
        "skills": [
            "Generative AI",
            "Feature Extraction",
            "Deep Neural Networks",
            "R",
            "Forecasting Optimization Methods",
            "Data Analysis",
            "Bayesian Models",
            "Data Validation",
            "Cloud Services",
            "Sql",
            "Python",
            "Data Transformation"
        ]
    },
    {
        "job_title": "Principal, Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "12-20 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nRole: Principal, Data Scientist\n\nLocation: Chennai\n\nExperience: 12 to 20 years\n\nAbout team:\n\nWalmart is a multi-national people led tech powered omnichannel retailer with over 2.2M associates. At People Tech, we help, support and empower the HR department with technology and solutions to improving productivity, harnessing the opportunities for growth, focusing on well-being and cultivating cultures of belonging to all Walmart's associates.\n\nEnterprise Business Services (EBS) is seeking a highly skilled and experienced Principal Data Scientist directly reporting to the VP and org head at IDC. As a Principal Data Scientist specializing in GenAI and Agentic AI, you will lead the development and deployment of AI/ML models for associate facing systems & technologies; work with a team of Data scientists and engineers, and collaborate with product leaders to drive innovation. Your role will also involve overseeing project lifecycles, ensuring compliance with standards, and enhancing Walmart's reputation through research and partnerships.\n\nWalmart's Enterprise Business Services (EBS) is a powerhouse of several exceptional teams delivering world-class technology solutions and services making a profound impact at every level of Walmart. As a key part of Walmart Global Tech, our teams set the bar for operational excellence and leverage emerging technology to support millions of customers, associates, and stakeholders worldwide. Each time an associate turns on their laptop, a customer makes a purchase, a new supplier is onboarded, the company closes the books, physical and legal risk is avoided, and when we pay our associates consistently and accurately, that is EBS. Joining EBS means embarking on a journey of limitless growth, relentless innovation, and the chance to set new industry standards that shape the future of Walmart.\n\nWhat you will do:\n\nAs a Principal Data Scientist with specialization in GenAI and Agentic AI, your role will be pivotal in working with the Engineering and DS teams to formulation of strategies and roadmaps for the design, development, and deployment of AI/ML, NLP, and GenAI models, ensuring their seamless transition into production environments and guaranteeing their reliability and scalability.\nYou should be working on cutting-edge large language models (LLMs) by leveraging Walmart's vast datasets and working with a team of Data scientists and engineers in solving intricate AI/ML challenges through research and development, pushing the limits of innovation and making groundbreaking contributions to the field.\nOverseeing the entire lifecycle of projects, from inception and data collection to model prototyping and deployment, while effectively managing stakeholder relationships and facilitating cross-functional communication.\nCollaborating extensively with product and engineering leaders to accelerate innovations in discovery experiences, utilizing insights, frameworks, machine learning prototypes, and emphasizing the strategic importance of AI initiatives.\nAdhering strictly to Walmart's policies, procedures, mission, values, standards of ethics, and integrity.Designing end-to-end system architecture for GenAI/AI/ML and data-intensive applications, setting new benchmarks in the industry.\nDeveloping and deploying robust, production-grade real-time/batch machine learning services that set industry benchmarks.\nCollaborating with product managers to design user journeys, feedback loops and analyze user telemetry, thus creating seamless user experiences.\nIdentifying or proposing innovative AI/ML use-cases to business teams to boost business processes and developing quick MVPs/POCs to enable stakeholders to make data-driven decisions.\n\nWhat you'll bring :\n\nGenAI: understanding of usecases in developing and implementing AI models and algorithms.\nPython: Knowledge of Python, including libraries such as NumPy, Pandas, and Scikit-learn.\nApache Spark: Experience with big data processing frameworks like Apache Spark.\nScala: Knowledge of Scala for data processing and analysis.\nMachine Learning: Expertise in machine learning techniques and frameworks such as TensorFlow, Keras, and PyTorch.\nData Science: Strong foundation in data science principles, including statistical analysis, data visualization, and data manipulation.\nAlgorithms: Ability to design and implement efficient algorithms for data processing and analysis.\nEngineering: Experience in software engineering practices, including version control, testing, and deployment.\n\nAbout Walmart Global Tech\n\nFrom entry-level to executive positions, Walmart provides limitless opportunities for growth, and career development. Walmart started small, with a single discount store and the simple philosophy of selling more for less. Today, we are a growing technology-enabled company founded on the same values as our first store. We establish clear expectations, empower associates to manage their work, and hold ourselves and one another to a high standard. Walmart's scale enables us to have an. No other company has the reach of Walmart, with 2.3 million associates worldwide and over 230 million weekly customers. Walmart is reshaping retail by investing in an expanding workforce. While technology is at the heart of our digital transformation, people are the reason we succeed and the force behind our innovations. We train our team in the skillsets of the future and bring in experts like you to help us grow.\n\nFlexible, Hybrid Work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 5 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 3 years experience in an analytics related field. Option 3: 7 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\nRmz Millenia Business Park, No 143, Campus 1B (1St -6Th Floor), Dr. Mgr Road, (North Veeranam Salai) Perungudi , India R-2111697",
        "skills": [
            "GenAI",
            "Scikit-learn",
            "Machine Learning",
            "Scala",
            "Apache Spark",
            "AI ML",
            "Tensorflow",
            "Numpy",
            "Data Science",
            "Data Manipulation",
            "Nlp",
            "Algorithms",
            "Software Engineering",
            "Pytorch",
            "Pandas",
            "Data Visualization",
            "Keras",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "DATA SCIENTIST III",
        "company_name": "Walmart Global Tech India",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team\n\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart's product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\n\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\n\nWhat you'll do:\n\n\n\nAs a Data Scientist - ML Engineer, you'll have the opportunity to:\n\nDesign large scale AI/ML products/systems impacting millions of customers\n\nDevelop highly scalable, timely, highly-performant, instrumented and accurate data pipelines\n\nIdentify, develop, and deliverimprovements on data performance,dataquality, and cost, which needs to be monitored and analyzed\n\nEnable data governancepractices and processby being a passionate adopter and ambassador\n\nRun fine-tuning and optimization experiments on large language models and advanced machine learning algorithms\n\nBuild production quality AI/ML solutions for large scale data with low latency requirements\n\nCollaborate with multiple stakeholders to drive innovation at scale\n\nLeverage and contribute to generative AI advancements within the organization, fostering an environment of innovation and progress\n\nBuild a strong external presence through publishing your team's work in top-tier AI/ML conferences and developing partnerships with academic institutions\n\nAdhere to Walmart's policies, procedures, mission, values, standards of ethics and integrity\n\nAdopt to Walmart's quality standards, develop/recommend process standards and best practices across the retail industry\n\nWhat You'll Bring\n\nPhD with >1 years of relevant experience / 4-year bachelor's degree with > 4 years of experience / Master's degree with > 2 years of experience. Educational qualifications should be preferably in Computer Science or a strongly quantitative discipline.\n\nDevelopment and deployment of AI/ML models\n\nProven track record of delivering high-impact AI/ML solutions\n\nPast experience in programming skills across big data and ML engineering stack\n\nStrong communication skills with inclination to high ownership and commitment\n\nAble to refactordata science code andhas collaborated with data scientists in developing ML solutions.\n\nMandatory Skills: Machine Learning, Big Data Skills, Python, R\n\nAdditional Qualifications:Good to have experience inareas such ascomputer vision,forecasting, real-timeanalytics,conversationalAIassistants\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics or related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field. Option 3: 4 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2127909",
        "skills": [
            "Generative AI",
            "R",
            "Statistics",
            "Optimization",
            "Machine Learning",
            "Artificial Intelligence",
            "Big Data",
            "Python",
            "Computer Vision",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "IDFC FIRST Bank",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Requirements\n\nRole/ Job Title: Data Scientist\n\nFunction/ Department: Data & Analytics\n\nJob Purpose\n\nIn this specialized role, you will leverage your expertise in machine learning and statistics to derive valuable insights from data. Your role will include developing predictive models, interpreting data and working closely with out ML engineers to ensure the effective deployment and functioning of these models.\n\nKey / Primary Responsibilities\n\nLead cross-functional teams in the design, development, and deployment of Generative AI solutions, with a strong focus on Large Language Models (LLMs).\nArchitect, train, and fine-tune state-of-the-art LLMs (e.g., GPT, BERT, T5) for various business applications, ensuring alignment with project goals.\nDeploy and scale LLM-based solutions, integrating them seamlessly into production environments and optimizing for performance and efficiency.\nDevelop and maintain machine learning workflows and pipelines for training, evaluating, and deploying Generative AI models, using Python or R, and leveraging libraries like Hugging Face Transformers, TensorFlow, and PyTorch.\nCollaborate with product, data, and engineering teams to define and refine use cases for LLM applications such as conversational agents, content generation, and semantic search.\nDesign and implement fine-tuning strategies to adapt pre-trained models to domain-specific tasks, ensuring high relevance and accuracy.\nEvaluate and optimize LLM performance, including handling challenges such as prompt engineering, inference time, and model bias.\nManage and process large, unstructured datasets using SQL and NoSQL databases, ensuring smooth integration with AI models.\nBuild and deploy AI-driven APIs and services, providing scalable access to LLM-based solutions.\nUse data visualization tools (e.g., Matplotlib, Seaborn, Tableau) to communicate AI model performance, insights, and results to non-technical stakeholders.\n\nSecondary Responsibilities\n\nContribute to data analysis projects, with a strong emphasis on text analytics, natural language understanding, and Generative AI applications.\nBuild, validate, and deploy predictive models specifically tailored to text data, including models for text generation, classification, and entity recognition.\nHandle large, unstructured text datasets, performing essential preprocessing and data cleaning steps, such as tokenization, lemmatization, and noise removal, for machine learning and NLP tasks.\nWork with cutting-edge text data processing techniques, ensuring high-quality input for training and fine-tuning Large Language Models (LLMs).\nCollaborate with cross-functional teams to develop and deploy scalable AI-powered solutions that process and analyze textual data at scale.\n\nKey Success Metrics\n\nEnsure timely deliverables. Spot Training Infrastructure fixes. Lead technical aspects of the projects. Error free deliverables.\n\nEducation Qualification\n\nGraduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)\n\nPost-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA\n\nExperience: 5-10 years of relevant experience",
        "skills": [
            "T5",
            "Generative AI",
            "Hugging Face Transformers",
            "GPT",
            "R",
            "Statistics",
            "BERT",
            "Matplotlib",
            "Machine Learning",
            "Tableau",
            "Sql",
            "Nosql",
            "Tensorflow",
            "Pytorch",
            "Data Visualization",
            "Seaborn",
            "Python"
        ]
    },
    {
        "job_title": "Staff, Data Scientist, Machine learning Engineer",
        "company_name": "Walmart Global Tech India",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\n\n\nAbout Team\n\nAs the Conversational AI team we are building completely new capabilities to allow our customers to shop, by seamlessly interacting with their connected devices using spoken and written language. This team is part of the Emerging tech organization and will build new voice experiences both in-house and in collaboration with strategic partners. Voice as a medium for shopping is still in its infancy and as part of this team you will get to work on industry leading solutions and be at the forefront of this emerging platform.\n\n\n\nWhat you will do:\n\nAs a Staff Machine Learning Engineer for Walmart, you'll have the opportunity to\n\nPartner with key business stakeholders and be a thought leader in the Conversational AI space for driving the development and planning of POCs and production AI solutions.\nLead the design, development and deployment of complex machine learning solutions that drive business results and impact millions of customers worldwide.\nCollaborate with other engineers and data scientists to integrate machine learning models into products and services.\nUphold ML engineering best practices - enforcing high standard for quality, reliability, and security in deployed machine learning solutions.\nStay up to date with the latest AI/ML technologies and trend - acting as a thought leader for ML Engineering within the organization and in the broader technical community.\nEnhance current Deployment Solutions to make it cost as well as time efficient.\nProvide technical leadership, guidance and mentorship to a small group of highly skilled and motivated engineers.\nLead innovation and efficiency through the complete problem-solving cycle, from approach to methods to development and results.\nPartner and engage with associates in other regions for delivering the best services to customers around the globe.\nProactively participate in the external community to strengthen Walmart's brand and gain insights into industry practices.\nLead multiple initiatives within the platform, with focus on efficiency, innovation, and leverage.\n\n\n\n\n\nWhat you will bring:\n\nBachelors with 10+ years or Masters with 8+ years of relevant experience. Educational qualifications should be in Computer Science or related fields\n7+ year experience in deploying and scaling AI/ML systems for real-time inferencing and streaming applications.\n7+ year of experience with MLE technologies: Python, TensorFlow/Pytorch, Numpy, scikit-learn, Unix, Docker, CPU and GPU architectures, LangChain, Vector DB, etc.\nHave experience in designing the pipelines with different orchestration methods like Airflow, Kubernetes, Job Scheduler etc.\n3+ year experience in deep learning and generative AI technologies, with focus on optimised model deployments.\n6+ year experience with big data stack and relational/no-SQL databases: BigQuery, Spark, HDFC, MySQL, Cassandra, etc.\n4+ years of experience working with Kafka, ActiveMQ, Caches such as Redis or memcached, Elastic Search.\n4+ year of software engineering experience in developing services using Java/C++\nExperience using managed servicing public cloud platforms such as Azure, GCP or AWS\nUsage of Model Deployment Framework like Triton Server, Seldon Core, Vertex AI is a plus.\nExtensive experience in working with large datasets and building batch data processing pipelines\nAdvanced knowledge of performance, scalability, and system architecture with an eye toward avoiding and reducing technical debt\nGood problem-solving attitude and analytical skills\nStrong communication skills with inclination to high ownership and commitment\n\n\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2124910",
        "skills": [
            "Airflow",
            "Vector DB",
            "scikit-learn",
            "generative AI technologies",
            "CPU and GPU architectures",
            "Triton Server",
            "Vertex AI",
            "HDFC",
            "LangChain",
            "Job Scheduler",
            "Seldon Core",
            "Unix",
            "Cassandra",
            "Kafka",
            "Memcached",
            "Deep Learning",
            "Tensorflow",
            "Docker",
            "MySQL",
            "Python",
            "AWS",
            "Java",
            "BigQuery",
            "Activemq",
            "Redis",
            "Numpy",
            "Gcp",
            "Pytorch",
            "Spark",
            "Elastic Search",
            "Azure",
            "Kubernetes"
        ]
    },
    {
        "job_title": "Data Scientist II",
        "company_name": "Worley",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Navi Mumbai, Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Building on our past. Ready for the future\n\nWorley is a global professional services company of energy, chemicals and resources experts. We partner with customers to deliver projects and create value over the life of their assets. We're bridging two worlds, moving towards more sustainable energy sources, while helping to provide the energy, chemicals and resources needed now.\n\nWorley Digital\n\nAt Worley, our Digital team collaborates closely with the business to deliver efficient, technology-enabled sustainable solutions, that will be transformational for Worley. This team, aptly named Worley Digital, is currently seeking talented individuals who would be working on a wide range of latest technologies, including solutions based on Automation, Generative AI.\n\nWhat drives us at Worley Digital It's our shared passion for pushing the boundaries of technological innovation, embracing best practices, and propelling Worley to the forefront of industry advancements. If you're naturally curious, open-minded, and a self-motivated learner - one who's ready to invest time and effort to stay future-ready - then Worley could be your ideal workplace.\n\nPosition Title (Global): Data Scientist II\n\nMAJOR ACCOUNTABILITIES OF POSITION:\n\nUnderstanding business objectives and developing models that help to achieve them, along with metrics to track their progress Utilize the existing frameworks, standards, patterns to create architectural foundation and services necessary for AI applications that scale from multi-user to enterprise class\nManaging Data Science project life cycle from exploratory data analysis to productization (Alpha/Beta Release) .Manage small team to collaborate with Architecture, Data Warehouse, Data Governance teams for providing analytics as service.\nMentor team member for AI/ML development\nVerifying data quality, and/or ensuring it via data cleaning Supervising the data acquisition process if more data is needed Finding available datasets online that could be used for training Defining validation strategies Defining the preprocessing or feature engineering to be done on a given dataset.\nDefining data augmentation pipelines Training models and tuning their hyperparameters Analyzing the errors of the model and designing strategies to overcome them Deploying models to production.\nDevelopment of the ML algorithms that could be used to solve a given problem and ranking them by their success probability.\nExploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world\n\nKnowledge / Experience / Competencies Required\n\nIT Skills & Experience (Priority wise):\n\nProven experience as a Data Scientist AI/ML or similar role\nAbility to write robust code in Python.\nExperience in the Generative AI components like LLMs, LangChain, LlamaIndex, OpenAI, Mistral, Llama etc.\nExperience in supervised/semi-supervised and unsupervised machine learning algorithms.\nExperience using the cognitive APIs machine learning studios on cloud.\nUp to speed on NLP (Summarization, Translation models, Named Entity Recognition)\nHands-on knowledge of image processing with deep learning (CNN, RNN, LSTM, GAN)\nUnderstanding of complete AI/ML project life cycle.\nUnderstanding of data structures, data modelling and software architecture.\n\nPeople Skills:\n\nAbility to communicate clearly and concisely and a flexible mindset to handle a quickly changing culture\nAbility to work independently and/or as part of cross-domain big team\nProfessional and open communication to all internal and external interfaces.\nAccurately report to management in a timely and effective manner.\n\nOther Skills:\n\nOutstanding analytical and problem-solving skills\nTaking ownership of the tasks in hand and being accountable for deliverables.\n\nEducation Qualifications, Accreditation, Training:\n\nMinimum 47 years experience as a Data Scientist on AI and ML projects.\nMaster's in information technology / Big Data/Data Science/AI/Computer Science or related field\n\nMoving forward together\n\nWe're committed to building a diverse, inclusive and respectful workplace where everyone feels they belong, can bring themselves, and are heard. We provide equal employment opportunities to all qualified applicants and employees without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by law.\n\nWe want our people to be energized and empowered to drive sustainable impact. So, our focus is on a values-inspired culture that unlocks brilliance through belonging, connection and innovation.\n\nAnd we're not just talking about it; we're doing it. We're reskilling our people, leveraging transferable skills, and supporting the transition of our workforce to become experts in today's low carbon energy infrastructure and technology.\n\nWhatever your ambition, there's a path for you here. And there's no barrier to your potential career success. Join us to broaden your horizons, explore diverse opportunities, and be part of delivering sustainable change.\n\nEducation Qualifications, Accreditation, Training:\n\nMaster's in information technology / Big Data/Data Science/AI/Computer Science or related field\n\nMoving forward together\n\nWe're committed to building a diverse, inclusive and respectful workplace where everyone feels they belong, can bring themselves, and are heard. We provide equal employment opportunities to all qualified applicants and employees without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by law.\n\nWe want our people to be energized and empowered to drive sustainable impact. So, our focus is on a values-inspired culture that unlocks brilliance through belonging, connection and innovation.\n\nAnd we're not just talking about it; we're doing it. We're reskilling our people, leveraging transferable skills, and supporting the transition of our workforce to become experts in today's low carbon energy infrastructure and technology.\n\nWhatever your ambition, there's a path for you here. And there's no barrier to your potential career success. Join us to broaden your horizons, explore diverse opportunities, and be part of delivering sustainable change.\n\nCompany\n\nWorley\n\nPrimary Location\n\nIND-MM-Navi Mumbai\n\nOther Locations\n\nIND-KR-Bangalore, IND-MM-Mumbai, IND-MM-Pune, IND-TN-Chennai, IND-GJ-Vadodara, IND-AP-Hyderabad, IND-WB-Kolkata\n\nJob\n\nDigital Platforms & Data Science\n\nSchedule\n\nFull-time\n\nEmployment Type\n\nEmployee\n\nJob Level\n\nExperienced\n\nJob Posting\n\nApr 25, 2025\n\nUnposting Date\n\nMay 26, 2025\n\nReporting Manager Title\n\nSenior Manager",
        "skills": [
            "LlamaIndex",
            "LLMs",
            "OpenAI",
            "supervised machine learning algorithms",
            "LSTM",
            "Translation models",
            "Named Entity Recognition",
            "LangChain",
            "cognitive APIs",
            "machine learning studios on cloud",
            "GAN",
            "Generative AI",
            "Mistral",
            "unsupervised machine learning algorithms",
            "Image Processing",
            "Cnn",
            "Deep Learning",
            "Data Modelling",
            "Python",
            "Software Architecture",
            "Summarization",
            "Nlp",
            "Rnn",
            "data structures"
        ]
    },
    {
        "job_title": "(IND) SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThis is the team which builds reusable technologies that aid in acquiring customers, onboarding and empowering merchants besides ensuring a seamless experience for both these stakeholders. We also optimize tariffs and assortment, adhering to the Walmart philosophy - Everyday Low Cost. In addition to ushering in affordability, we also create personalized experiences for customers the omnichannel way, across all channels - in-store, on the mobile app and websites. Marketplace is the gateway to domestic and international Third-Party sellers; we enable them to manage their end-to-end onboarding, catalog management, order fulfilment, return & refund management. Our team is responsible for design, development, and operations of large-scale distributed systems by leveraging cutting-edge technologies in web/mobile, cloud, big data & AI/ML. We interact with multiple teams across the company to provide scalable robust technical solutions.\n\nWhat you'll do:\n\nAs a Data Scientist for Walmart , you'll have the opportunity to\n\nDrive data-derived insights across the wide range of retail divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assessing data validity and synthesizing data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train statistical models and machine learning algorithms for replication for future projects\nCommunicate recommendations to business partners and influencing future plans based on insights\n\nWhat you'll bring:\n\nVery good knowledge of the foundations of machine learning and statistics\nHand on Experience in building and maintaining Gen AI powered solutions in production\nExperience in Analyzing the Complex Problems and translate it into data science algorithms\nExperience in machine learning, supervised and unsupervised and deep learning.\nHands on experience in Computer Visions and NLP.\nExperience with big data analytics - identifying trends, patterns, and outliers in large volumes of data\nStrong Experience in Python with excellent knowledge of Data Structures\nStrong Experience with big data platforms Hadoop (Hive, Pig, Map Reduce, HQL, Scala, Spark)\nHands on experience with Git\nExperience with SQL and relational databases, data warehouse\nQualifications\nBachelors with > 7 years of experience / Master's degree with > 5 years of experience. Educational qualifications should be preferably in Computer Science/Mathematics/Statistics or a related area. Experience should be relevant to the role.\nGood to have:\nExperience in ecommerce domain.\nExperience in R and Julia\nDemonstrated success in data science platforms like Kaggle.\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2146190",
        "skills": [
            "Relational Databases",
            "Computational Algorithms",
            "Techniques",
            "Map Reduce",
            "Statistical Models",
            "Git",
            "Sql",
            "Pig",
            "Computer Vision",
            "Hql",
            "Hadoop",
            "Machine Learning",
            "Big Data Analytics",
            "Hive",
            "Python",
            "Data Structures",
            "Scala",
            "Nlp",
            "Data Warehouse",
            "Spark"
        ]
    },
    {
        "job_title": "Senior Principal Data Scientist",
        "company_name": "GSK",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Site Name: Bengaluru Luxor North Tower, Poznan Pastelowa\n\nPosted Date: May 5 2025\n\nKey Responsibilities\n\nLead NLP Initiatives: Drive development and implementation of advanced NLP models to analyze unstructured text data related to clinical monitoring, quality, and risk management.\nBuild and Optimize Predictive Models: Apply machine learning techniques to identify patterns and trends, with a focus on preventing/corrective actions for improving quality assurance practices.\nCollaborate Across Teams: Partner with cross-functional teams, including other data science business and tech teams within GSK, quality assurance, and clinical operations, to understand and solve critical business challenges.\nEnsure Model Governance: Implement and monitor best practices for model governance, accuracy, and reliability in a highly regulated environment, ensuring adherence to industry standards and regulatory requirements.\nData Preprocessing and Cleaning: Oversee data acquisition, preprocessing, and quality control of complex datasets, working with structured and unstructured data in the R&D domain.\nLead Data Science Projects: Mentor and guide junior data scientists and analysts, ensuring robust project management, timely delivery, and effective stakeholder communication.\nContinuous Improvement: Identify opportunities to improve data workflows, tooling, and processes for enhanced productivity and reproducibility.\nResearch and Innovation: Stay abreast of industry trends and advancements in NLP, machine learning, and generative AI to drive innovation within the quality and risk management framework.\nPerformance Metrics and Reporting: Establish and track key performance indicators to assess model impact and value, aligning outcomes with organizational quality and risk management goals.\nDevelop Gen AI Solutions: Explore and integrate generative AI models to innovate on complex language tasks such as summarization, data synthesis, and anomaly detection.\n\nEducation Requirements\n\n\nA bachelor's degree in computer science, statistics, mathematics\n\nJob Related Experience\n\nProven experience in data science, predictive modelling, and statistical analysis.\nProficiency in programming languages commonly used in data science, such as Python, R, and SQL.\nExperience with data visualization tools like Power BI, Shiny Web Apps, and similar platforms.\nApplication of machine learning algorithms and statistical modelling techniques.\nNatural Language Processing (NLP) to derive actionable insights.\nStrong problem-solving skills and the ability to translate business problems into analytical use-cases.\nExcellent communication skills to effectively interact with business stakeholders and tech partners.\nA good understanding of drug research and development and quality\n\nOther Job-Related Skills\n\nAdvanced knowledge of analytics tools and capabilities: The candidate should be proficient in using advanced analytics tools and be capable of leveraging them to analyse complex data sets.\nStrong understanding of R&D Quality and Risk Management: The candidate should have a deep understanding of quality and risk management within an R&D context.\nData Analysis Skills: The candidate should be able to interpret complex data and translate it into information that can be understood by non-technical stakeholders.\nCommunication Skills: The candidate should have strong communication skills to effectively convey data insights to business stakeholders.\nProblem-Solving Skills: The candidate should have strong problem-solving skills to translate business problems into analytical use-cases.\nTechnical Skills: Proficiency in programming languages commonly used in data science, such as Python, R, and SQL, and experience with data visualization tools like Power BI, Shiny Web Apps, and similar platforms.\nKnowledge of Machine Learning: The candidate should have a deep understanding of machine learning algorithms and statistical modelling techniques.\nFamiliarity with Natural Language Processing (NLP): The candidate should have knowledge of NLP to handle use-cases in areas like text summarizations, sentiment analysis, topic modelling, and trend analysis.\nLeadership Skills: The candidate should have excellent leadership skills to effectively interact with business stakeholders and tech partners, and to drive the implementation of data science tools and solutions in a matrixed environment.\n\nWhy GSK\n\n\nUniting science, technology and talent to get ahead of disease together.\n\n\nGSK is a global biopharma company with a special purpose to unite science, technology and talent to get ahead of disease together so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns as an organisation where people can thrive. We prevent and treat disease with vaccines, specialty and general medicines. We focus on the science of the immune system and the use of new platform and data technologies, investing in four core therapeutic areas (infectious diseases, HIV, respiratory/ immunology and oncology).\n\nOur success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves feeling welcome, valued, and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.\n\nImportant notice to Employment businesses/ Agencies\n\nGSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.\n\nIt has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.\n\nGlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.\n\nIf you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in gsk.com, you should disregard the same and inform us by emailing [HIDDEN TEXT], so that we can confirm to you if the job is genuine.",
        "skills": [
            "R",
            "Machine Learning",
            "Statistical Analysis",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Associate Level 1 / Senior Associate - Data Scientist (Tabular & Text) - GM Data & AI Lab",
        "company_name": "BNP Paribas",
        "experience": "1-3 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "About BNP Paribas Group\n\nBNP Paribas is a top-ranking bank in Europe with an international profile. It operates in 71 countries and has almost 199 000 employees. The Group ranks highly in its three core areas of activity: Domestic Markets and International Financial Services (whose retail banking networks and financial services are grouped together under Retail Banking & Services) and Corporate & Institutional Banking, centred on corporate and institutional clients. The Group helps all of its clients (retail, associations, businesses, SMEs, large corporates and institutional) to implement their projects by providing them with services in financing, investment, savings and protection. In its Corporate & Institutional Banking and International Financial Services activities, BNP Paribas enjoys leading positions in Europe, a strong presence in the Americas and has a solid and fast-growing network in the Asia/Pacific region.\n\nAbout BNP Paribas India Solutions\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, a leading bank in Europe with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 6000 employees, to provide support and develop best-in-class solutions.\n\nAbout Business Line/Function\n\nGM Data & AI Lab leverages the power of Machine Learning and Deep learning to drive innovation in various business lines. Our primary goal is to harness the potential of vast amounts of structured and unstructured data to improve our services and provide value. Today we are a team of around 40+ Data Scientists based in Paris, London, Frankfurt, Lisbon, New York, Singapore and Mumbai.\n\nJob Title\n\nData Scientist (Tabular & Text)\n\nDate\n\nDepartment:\n\nFront Office Support\n\nLocation:\n\nMumbai\n\nBusiness Line / Function\n\nGlobal Markets Data & AI Lab\n\nReports To\n\n(Direct)\n\nGrade\n\n(if applicable)\n\n(Functional)\n\nNumber Of Direct Reports\n\nNA\n\nDirectorship / Registration\n\nNA\n\nPosition Purpose\n\nYour work will span across multiple areas, including predictive modelling, automation and process optimization. We use AI to discover patterns, classify information, and predict likelihoods. Our team works on building, refining, testing, and deploying these models to support various business use cases, ultimately driving business value and innovation.\n\nAs a Data Scientist on our team, you can expect to work on challenging projects, collaborate with stakeholders to identify business problems, and have the opportunity to learn and grow with our team. A typical day may involve working on model development, meeting with stakeholders to discuss project requirements/updates, and brainstorming/debugging with colleagues on various technical aspects.\n\nAt the Lab, we're passionate about staying at the forefront of AI research, bridging the gap between research & industry to drive innovation and to make a real impact on our businesses.\n\nResponsibilities\n\nDevelop and maintain AI models from inception to deployment, including data collection, analysis, feature engineering, model development, evaluation, and monitoring.\nIdentify areas for model improvement through independent research and analysis, and develop recommendations for updates and enhancements.\nWorking with expert colleagues and business representatives to examine the results and keep models grounded in reality.\nDocumenting each step of the development and informing decision makers by presenting them options and results.\nEnsure the integrity and security of data.\nProvide support for production models delivered by the Mumbai team but potentially as well for other models to any of the Asian/EU/US time zones.\n\nTechnical & Behavioral Competencies\n\nQualifications: Bachelors / Master / PhD degree in Computer Science / Data Science / Mathematics / Statistics / relevant STEM field.\nKnowledge of key concepts in Statistics and Mathematics such as Statistical methods for Machine learning, Probability Theory and Linear Algebra.\nExperience with Machine Learning & Deep Learning concepts including data representations, neural network architectures, custom loss functions.\nProven track record of building AI model from scratch or finetuning on large models for Tabular or/and Textual data.\nProgramming skillsin Pythonand knowledge of common numerical and machine-learning packages (like NumPy,scikit-learn, pandas,PyTorch, transformers, langchain).\nAbility to write clear and concise code in python.\nIntellectually curious and willing to learn challenging concepts daily.\nKnowledge of current Machine Learning/Artificial Intelligence literature.\n\nSkills Referential\n\nBehavioural Skills:\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nCommunication skills - oral & written\n\nAttention to detail / rigor\n\nTransversal Skills\n\nAnalytical Ability\n\nEducation Level\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 1 year",
        "skills": [
            "langchain",
            "Transformers",
            "scikit-learn",
            "Numpy",
            "Machine Learning",
            "Pandas",
            "Pytorch",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HCLTech",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Working knowledge of Azure AI\nWorking knowledge of LLM , RAG- Data insights , Reasoning Agent , Chain of thought\nKnowledge of Graph DB , COSMOS DB , Neo4j\nPrompt engineering\nAPI building\nDomain knowledge in Finance , data management",
        "skills": [
            "Prompt engineering",
            "API building",
            "Graph DB",
            "Azure AI",
            "Neo4j",
            "COSMOS DB"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Havells India Ltd",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Sr. Data Scientist\nCompany-Havells India Ltd\nLocation: Noida HO\nAbout the Role:\nHavells is seeking a passionate and self-starting Data Scientist to join our team. This role is pivotal in driving growth, enhancing consumer experience, optimizing sales, and achieving cost savings through advanced analytics and AI/ML. You will handle Big Data, support routine data analytics/data mining requests, and lead predictive, AI/ML, and advanced analytics projects across various domains including Consumer, Sales Ops, Retail, and SCM.\nKey Responsibilities:\nSupport insights from consumer, loyalty, app, sales, and transactional data.\nCollaborate with the Data Engineering team on building Data Lake/Warehouse.\nData preparation, new attribute development, and creating Single View of customers and retailers.\nUtilize data mining/AI/ML for upsell/cross-sell, retention, loyalty, engagement campaigns, and target audience identification.\nBuild and maintain predictive analytics AI/ML models for use cases in Consumer Domain, Consumer Experience (CX), and e-Commerce (D2C) platform.\nSupport AI/ML models for Sales Transformation or SCM use cases.\nWork on GenAI use cases and projects.\nConduct deep data mining to support digital analytics, website behavior, app behavior analytics, call center/CS behavior, NPS, retailer, and electrician loyalty.\nKnowledge of data visualization, MIS/Dashboarding tools.\nSupport ad hoc analysis on consumer purchase behavior/market basket analysis.\nCandidate Profile:\nEducation: BE/BTech, MTech, MSc Stats, MBA, or Diploma in Analytics/Decision Science.\nExperience: 8-9+ years in advanced & predictive analytics in at least two domains (consumer, sales transformation, SCM, or Manufacturing Plant Analytics). Proficient in modern analytical tools and techniques (Python, R, SQL, SPSS, SAS).\nTools/Techniques: Python, PySpark, SQL, Hadoop, Hive, Regression, Classification, Decision Trees, Random Forests, SGBoost Clustering, SVM, Time Series, NLP, Neural Networks, Large Language Models, Computer Vision, Object Detection, CNN, Object Tracking. Familiarity with MLOps & MS Databricks cloud technologies is an advantage.\nPersonal Attributes: Strong analytical mindset, expertise in big data & cloud, self-starter, hands-on, excellent interpersonal skills, ability to work with cross-functional teams, translate technical data concepts into business language, and solve business problems via data. Ability to initiate work in ambiguous environments. Flair for consumer, digital & technology.\nApart from Easy Apply, Please fill the details on this link--> https://forms.office.com/r/aZSxQuTnGS\nJoin us and be a part of our journey to leverage analytics and AI/ML for transformative growth!\n#DataScientist #Analytics #AI #MachineLearning #BigData #DataScience #PredictiveAnalytics #ConsumerExperience #SalesOptimization #SCM #Havells #JobOpening #Hiring",
        "skills": [
            "object detection",
            "Random Forests",
            "R",
            "SGBoost",
            "Classification",
            "Large Language Models",
            "MS Databricks",
            "Regression",
            "object tracking",
            "Cnn",
            "Pyspark",
            "SAS",
            "Svm",
            "Nlp",
            "Spss",
            "Python",
            "Hadoop",
            "Neural Networks",
            "Clustering",
            "Sql",
            "Hive",
            "MLops",
            "Decision Trees",
            "Time Series",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Assistant Manager - Data Scientist",
        "company_name": "SBI Card",
        "experience": "Fresher",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nJOB DESCRIPTION\n\nSBI Card is a leading pure-play credit card issuer in India, offering a wide range of credit cards to cater to diverse customer needs. We are constantly innovating to meet the evolving financial needs of our customers, empowering them with digital currency for seamless payment experience and indulge in rewarding benefits. At SBI Card, the motto Make Life Simple inspires every initiative, ensuring that customer convenience is at the forefront of all that we do. We are committed to building an environment where people can thrive and create a better future for everyone.\n\nSBI Card is proud to be an equal opportunity & inclusive employer and welcome employees without any discrimination on the grounds of race, colour, gender, religion, creed, disability, sexual orientation, gender identity, marital status, caste etc. SBI Card is committed to fostering an inclusive and diverse workplace where all employees are treated equally with dignity and respect which makes it a promising place to work.\n\nJoin us to shape the future of digital payment in India and unlock your full potential.\n\nWhat's In It For YOU\n\nSBI Card truly lives by the work-life balance philosophy. We offer a robust wellness and wellbeing program to support mental and physical health of our employees\nAdmirable work deserves to be rewarded. We have a well-curated bouquet of rewards and recognition program for the employees\nDynamic, Inclusive and Diverse team culture\nGender Neutral Policy\nInclusive Health Benefits for all - Medical Insurance, Personal Accidental, Group Term Life Insurance and Annual Health Checkup, Dental and OPD benefits\nCommitment to the overall development of an employee through a comprehensive learning & development framework\n\nRole Purpose\n\nTo prepare business insights and data visualization dashboards, assist in the development of credit and fraud risk models, and perform credit and fraud decision engine deployment activities for the organization.\n\nRole Accountability\n\nExplore data and transform it into business insights by applying both data analytics and business knowledge\nProvide user-friendly visualization of data and business insights by creating dashboards leveraging any business intelligence (BI) software\nDevelop credit and fraud risk models by applying advanced AI/ML techniques.\nEnsure model deployment either in credit & fraud decision engines or turbine/data lake\nMonitor & validate all models/scorecards\nProcess Adherence as per MOU\n\nMeasures of Success\n\nEnsure the timely development of the models as per plan\nModel efficacy in terms of consistency & reliability\nModel validation & redevelopment as per agreed timelines\nEffective resolution of Business problems related to modeling or data visualization\n\nTechnical Skills / Experience / Certifications\n\nBasic knowledge of handling Big Data in Hadoop/Spark/AWS platforms\nBasic knowledge in AI/ML techniques like Random Forest, Gradient Boosting, Support Vector Machines, Neural Networks, etc.\nGood understanding of programming languages SQL, Python,\n\nCompetencies critical to the role\n\nStrong analytical and problem solving skills\n\nQualification\n\nGraduate / Master's degree in in any relevant field\n\nPreferred Industry\n\nFSI / Any",
        "skills": [
            "Ai",
            "Gradient Boosting",
            "Support Vector Machines",
            "Ml",
            "Hadoop",
            "Neural Networks",
            "Big Data",
            "Sql",
            "Random Forest",
            "Spark",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HDFC Bank",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Vertical : Credit analytics and innovation\nJob Purpose\nLooking for exceptional Data Scientist with a proven track record of experience in Predictive analytics.\nJob Responsibilities(JR) : 6 8 Areas\nActionable (4-6)\nScorecard Building and Modelling\nDeveloping analytical models to assess risk profile of a customer / identifying cross-sell opportunities/fraud control for new and existing customers\nResponsible for development and deployment of various scorecards / models for the credit risk / Retail asset products of the Bank within the specified timelines. Ensure the deployment also is line with Bank's strategies so as to gain maximum efficiencies.\nEstimate probability of default arising out of the scorecards and monitor deviation from expected default rates.\nMonitor portfolio and identify pockets of opportunity for business growth and risk alerts to control NPAs. Communicate these alerts to the respective owners and follow-up for closure\nFormulating data cuts and strategies to assess credit risk for new and existing customers of the Bank. Give feedback to stakeholders to improve the usage of the models\nDevelop novel approaches/algorithms in Risk modeling to constantly improve the operational efficacy of current models and its impact to stake holders\nTo adopt the latest trends wrt fintech companies by applying the best-in-class financial models, risk strategies\nTo work on various aspects related to automation of scorecards in Analytics domain.\nExplore data available within the Banks ecosystem but not used currently to meet the above stated business objectives\nInnovation\nThinking, evolving and making significant changes in the processes, techniques, methods so as to save time, energy and money.\nAny other assignments depending on the needs of the organization\nEducational Qualifications\nKey Skills\n1. MCA/MTech/MSc Statistics or Economics/BTech/MBA or equivalent in Computer science/Statistics or related areas with strong inclination towards machine learning algorithms is preferred\nExcellent problem solving capabilities with strong fundamentals in computer science\nStrong technical & analytical skills\nHands-on experience in data wrangling (ETL), feature engineering, building and deploying scalable machine learning algorithms\nStrong written and verbal communication skills\nExcellent knowledge of current trends in the field of machine learning/analytics field\nExperience Required\nExperience of 2+ years in analytics/predictive modeling/machine learning\nExperience of 2+ years in Python/R in building as well as deploying scalable analytics based models\nExposure to banking preferable\nMajor Stakeholders (intra team and cross functional stakeholders, who would need to be interacted with for discharging duties)\nCredit Policy\nUnderwriting\nProcess\nFraud Control\nProduct\nCredit Bureaus\nThird Party Analytics Service Providers\nIT\nLegal\nInformation Security",
        "skills": [
            "risk modeling",
            "R",
            "Scorecard Building",
            "Modeling",
            "Feature engineering",
            "Machine Learning Algorithms",
            "data wrangling",
            "Python",
            "Predictive Analytics",
            "Etl"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST (OR)",
        "company_name": "Walmart Global Tech India",
        "experience": "2-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThe Supply Chain & Store Ops Data Science team is a highly motivated group of applied scientists and machine learning engineers dedicated to solving sophisticated and high-impact problems that power Walmart's supply chain network. We leverage advanced algorithms in operations research and machine learning to tackle challenging problems like vehicle routing problem and trip dispensing optimization.\n\nWhat you'll do:\n\nDesign, development, and deployment of optimization algorithms and machine learning models, with the consideration of optimality and computational complexity.\nBuild production models on large-scale datasets to help Walmart supply chain business use cases, such as vehicle routing optimization and dispensing time optimization by utilizing statistical modeling, machine learning and optimization techniques.\nCollaborate closely with cross-functional teams to understand business objectives, requirements, and constraints.\nBuild data pipeline from multiple sources and extract data insight with data exploration and visualization techniques.\nExplore and latest trends of machine learning and optimization techniques to come up with solutions for complex data challenges.\n\nWhat you'll bring:\n\nPh.D. degree in Operations Research, Industrial Engineering, Statistics, Computer Science, Machine Learning, AI, or other quantitative fields with 2+ years of experience; or Master's degree in the same domains with 5+ years of experience post-grad school.\nSolid knowledge in advanced optimization algorithms including mixed integer programming models, network flows models, heuristics, meta-heuristics and etc.\nKnowledge in developing flexible models based on Vehicle Routing, Scheduling and Graph Theory to solve for challenging combinatorial optimization problems.\nPassionate about building scalable, reliable data driven products.\nAbility to translate business needs into analytical frameworks.\nHands-on development experience with the ability to write production code in object-oriented languages like Java, C++, or Python.\nStrong analytical skills and experience with SQL.\nProven track record of building and deploying algorithms/models in production in a real-world setting.\nDemonstrated ability to drive projects from conception to completion in a fast-paced, dynamic environment.\nExcellent communication and leadership skills with the ability to influence and inspire cross-functional teams.\nKnowledge of supply chain and transportation operation is a plus.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2127435",
        "skills": [
            "Scheduling",
            "mixed integer programming",
            "heuristics",
            "Statistical Modeling",
            "network flows models",
            "data exploration",
            "graph theory",
            "vehicle routing",
            "meta-heuristics",
            "Java",
            "Machine Learning",
            "data pipeline",
            "Optimization Algorithms",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "V.I.E. Associate Data Scientist",
        "company_name": "Societe Generale",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Reference 250004AK\n\nResponsibilities\n\nSocit Gnrale Global Solution Centre (SG GSC), a 100% owned subsidiary of European banking major Socit Gnrale (SG). Our role and purpose are to enable the strategic vision of Socit Gnrale Group. We are doing this by pioneering cutting edge innovation from Design Thinking to Smart Automation & Artificial Intelligence and applying it to banking. SG Global Solution Centre provides services in the areas of Application Development and Maintenance, Infrastructure Management, Business Process Management, and Knowledge Process Management to Socit Gnrale's business lines around the world. We are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.\n\nDAI is a specific department which is widely engaged in Data Science and AI related projects with SGGSC.\n\nRole Description\nConduct data quality checks and exploratory analysis\nWrite code to automate data extractions and timely communicate any roadblocks or challenges to the Management\nParticipate in standup meetings and take proactive initiatives for the team\nProcess, cleans and verify the integrity of data used for analysis\nJob Description\n\nThe task includes ability to scan the tool thoroughly following the SAS rules, raise alerts, optimize these scenarios, reduce false positives etc.\nAbility to communicate his or her analyzes, especially by using visualization tools in a clear way with decision orientation\nStrengthening the regulatory and accounting requirements relating to the supervision and monitoring of risk models and the resulting issues\nParticipate in the development and / or evolution of statistical models and machine learning applied to Financial Security\nWill be responsible for conducting an internal model review and calibration as well\nProduce and analyze model performance indicators\nThe Data Scientist will support our software developers, database architects, data analysts and data engineers on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects\nThey must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives\n\nMandatory Skills\n\nPython/R for data quality and exploratory analyses\nBasics of data / modular programming\nExperience with identifying trends, patterns, and outliers in data\n\nDue to the VIE eligibility criteria, a questionnaire will be provided. Please make sure to answer all the questions for an efficient analysis of your application.\n\nRequired\n\nProfile required\n\nPLEASE NOTE that since this program is primarily an international development program, candidates cannot apply to a VIE assignment in their own country of citizenship.\n\nTo facilitate the examination of your application by our English-speaking managers, we thank you for applying in English.\n\nStudies & experience:\n\nGraduate with a Master's degree from a Business/Engineering School or University with a specialization in Mathematics or Statistics\nA previous experience in data science projects would be appreciated\nExperience in observing and identifying trends, patterns and outliers in data\n\nLanguage skills:\n\nFluent in English\n\nTechnical, operational & soft skills:\n\nProven working knowledge of Python/R and basic knowledge of data/modular programming\nProficient/Good command of MS Office\nA quick learner with passion towards the subject\nGood communication and presentation skills\n\nThe VIE assignment in a nutshell\n\nThis VIE in Bangalore is to begin as soon as possible, but you need to plan 3 months between your application date and the beginning of your VIE assignment. It will last 12months.\n\nIn case a visa is necessary, please make sure your passport is valid up to 6 months after the end of the VIE contract to not delay your departure.\n\nThe VIE is a specific contract, under Business France's eligibility criteria, opened to candidates under 28 and from the member states of the European Economic Space. For further information (including your financial indemnities), please see Mon VIE-VIA Business France.\n\nWhy join us\n\nAs soon as you arrive, you will be integrated into our teams and will learn every day alongside our experts, who will support you in your tasks. Gradually, you will become more independent in your projects, making this experience a real career accelerator. You will also discover all the diversity of our businesses, in a sector that is constantly evolving and innovating.\n\nAt the end of your VIE, various opportunities could be offered to you, in France and abroad.\n\nBusiness insight\n\nAt Socit Gnrale, we are convinced that people are drivers of change, and that the world of tomorrow will be shaped by all their initiatives, from the smallest to the most ambitious. Whether you're joining us for a period of months, years or your entire career, together we can have a positive impact on the future. Creating, daring, innovating, and taking action are part of our DNA. If you too want to be directly involved, grow in a stimulating and caring environment, feel useful on a daily basis and develop or strengthen your expertise, you will feel right at home with us!\n\nStill hesitating\n\nYou should know that our employees can dedicate several days per year to solidarity actions during their working hours, including sponsoring people struggling with their orientation or professional integration, participating in the financial education of young apprentices, and sharing their skills with charities. There are many ways to get involved.\n\nWe are committed to support accelerating our Group's ESG strategy by implementing ESG principles in all our activities and policies. They are translated in our business activity (ESG assessment, reporting, project management or IT activities), our work environment and in our responsible practices for environment protection.\n\nDiversity and Inclusion\n\nWe are an equal opportunities employer and we are proud to make diversity a strength for our company. Societe Generale is committed to recognizing and promoting all talents, regardless of their beliefs, age, disability, parental status, ethnic origin, nationality, gender identity, sexual orientation, membership of a political, religious, trade union or minority organisation, or any other characteristic that could be subject to discrimination.",
        "skills": [
            "data quality checks",
            "R",
            "statistical models",
            "data modular programming",
            "exploratory analysis",
            "Data Extraction",
            "Machine Learning",
            "Python"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout The Team\n\nCentroid team at Walmart serves as the backbone of Walmart's end-to-end supply chain strategy. They are entrusted with the task of designing and implementing a long-term supply chain strategy that uses advanced data analytics and data science. Their primary objective is to ensure that Walmart provides top-tier customer service while supporting the increasing demand over time and simultaneously operating at low and efficient costs.\n\nThe team utilizes sophisticated data analysis methods to understand patterns, identify potential bottlenecks, and predict future trends. This enables them to optimize processes, make informed business decisions, and enhance overall operational efficiency.\n\nOne of Centroid's key responsibilities also includes the creation of a Digital Twin Simulation platform for Walmart's supply chain. This innovative tool allows the team to test and validate all future strategies and tactical decisions before they are launched operationally. It also enables a deep assessment of long-term strategic sensitivity.\n\nIn essence, the Centroid team's work is integral to ensuring Walmart's supply chain is robust, flexible, and capable of adapting to ever-changing market demands. Their work helps to keep Walmart at the forefront of retail supply chain management, delivering exceptional service to customers while maintaining efficient operational costs.\n\nWhat You'll Do -\n\nDevelop and manage advanced data analytics models to optimize supply chain strategies, balancing customer satisfaction with operational cost and asset efficiency.\nLeverage data analytics to identify opportunities for improvement and drive impactful results through collaboration with cross-functional teams.\nEstablish relationships across Walmart functional areas to identify best practices, solicit data/input, coordinate interdisciplinary initiatives, and rally support for data-driven recommendations.\nSecure alignment and support from relevant business partners and management for data-centric projects, leading discussions to drive necessary change.\nUtilize all available data resources effectively to ensure successful project outcomes.\nCommunicate data insights clearly and persuasively through emails, verbal discussions, and presentations, tailoring communication methods to the audience for maximum impact.\nCollaborate with multiple supply chain business teams to proactively identify, assess, and leverage cost-saving and service improvement opportunities through advanced data analytics.\nUtilize advanced analytics models to derive insights that will inform policy design across various supply chain areas, laying out multiple scenarios and performing sensitivity analysis.\nCollaborate with Data Scientists and Engineers to productionize and scale advanced analytics models as needed.\nDevelop and present compelling data-driven narratives/documents/visuals to influence key stakeholders in their decision-making.\nProvide coaching and training support to other team members in the supply chain area, leveraging your expertise in advanced data analytics.\n\nWhat You'll Bring -\n\nStrong analytical acumen with technical expertise in Advanced Data Analytics and modelling\nExpert in SQL, - BigQuery like cloud data platforms.\nExpert in programming in Python, (or R)\nExperience in using data visualization tools like Tableau and Looker and be able to drive powerful insights.\nExperience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, and/or Spark)\nExperience in operating from a cloud environment such as Google Could Platform or Microsoft Azure.\nAbility to work in a fast-paced, iterative development environment.\nStrong communication skills, both written and verbal, plus ability to work with cross functional teams of technical and non-technical members.\nStrong ability to understand the business and have good stakeholder management capabilities.\nExperience of working in cross-functional environment and leading or mentoring teams.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2153626",
        "skills": [
            "R",
            "Map Reduce",
            "SQL - BigQuery",
            "Looker",
            "Advanced Data Analytics",
            "Hive",
            "Hadoop",
            "Spark",
            "Tableau",
            "Microsoft Azure",
            "Python"
        ]
    },
    {
        "job_title": "Aladdin Financial Engineering Data Scientist Associate",
        "company_name": "BlackRock",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About This Role\n\nBlackRock Overview:\n\nBlackRock is one of the world's preeminent asset management firms and a premier provider of global investment management, risk management and advisory services to institutional, intermediary and individual investors around the world. BlackRock offers a range of solutions from rigorous fundamental and quantitative active management approaches aimed at maximizing outperformance to highly efficient indexing strategies designed to gain broad exposure to the world's capital markets. Our clients can access our investment solutions through a variety of product structures, including individual and institutional separate accounts, mutual funds and other pooled investment vehicles, and the industry-leading iShares ETFs.\n\nAladdin Financial Engineering Group (AFE)\n\nAFE is a diverse and global team with a keen interest and expertise in all things related to technology and financial analytics. The group is responsible for the research and development of quantitative financial and behavioral models and tools across many different areas single-security pricing, prepayment models, risk, return attribution, liquidity, optimization and portfolio construction, scenario analysis and simulations, etc. and covering all asset classes. The group is also responsible for the technology platform that delivers those models to our internal partners and external clients, and their integration with Aladdin.\n\nAFE conducts leading research on the areas above, delivering state-of-the-art models. AFE publishes applied scientific research frequently, and our members present regularly at leading industry conferences. AFE engages constantly with the sales team in client visits and meetings.\n\nJob Description\n\nYou can help conduct research to build quantitative financial models and portfolio analytics that help managing most of the money of the world's largest asset manager. You can bring all yourself to the job. From the top of the firm down we embrace the values, identities and ideas brought by our employees.\n\nWe are looking for curious people with a strong background in data science, quantitative research and machine learning, have awesome problem-solving skills, insatiable appetite for learning and innovating, adding to BlackRock's vibrant research culture.\n\nIf any of this excites you, we are looking to expand our team. We currently have Data Scientist role with the AFE Investment AI (IAI) Team, India (Mumbai or Gurugram location). The securities market is undergoing a massive transformation as the industry is embracing machine learning and, more broadly, AI, to help evolve the investment process. Pioneering this journey at BlackRock, the team has better deliver applied AI investment analytics to help both BlackRock and Aladdin clients achieve scale through automation while safeguarding alpha generation. The IAI team combines AI / ML methodology and technology skills with deep subject matter expertise in fixed income, equity, and multi-asset markets, and the buyside investment process.\n\nWe are building next generation liquidity, security similarity and pricing models leveraging our expertise in quantitative research, data science and machine learning. The models we build use innovative machine learning approaches, have real practical value and are used by traders and portfolio managers alike. Our models use cutting edge econometric/statistical methods and tools. The models themselves have real practical value and are used by traders, portfolio managers and risk managers representing different investment styles (fundamental vs. quantitative) and across different investment horizons. Research is conducted predominantly in Python and Scala, and implemented into production by a separate, dedicated team of developers. These models have a huge footprint of usage across the entire Aladdin client base, and so we place special emphasis on scalability and ensuring adherence to BlackRock's rigorous standards of model governance and control.\n\nBackground And Responsibilities\n\nWe are looking to hire a Data Scientist with 4+ years experience to join AFE Investment AI India team focusing on Trading and Liquidity to work closely with other data scientists/researchers to support Risk Mangers, Portfolio Managers and Traders. We build cutting edge liquidity analytics using a wide range of ML algos and a broad array of technologies (Python, Scala, Spark/Hadoop, GCP, Azure). This role is a great opportunity to work closely with the Portfolio Managers, Risk Managers and Trading team, spanning areas such as:\n\nDesign, develop, and maintain data pipelines to extract, transform, and load data from various sources into our data warehouse/lake.\nWork with data scientists and analysts to understand data needs and design appropriate data models.\nImplement data quality checks and ensure the accuracy and consistency of data throughout the processing pipeline.\nPerform analysis of large data sets comprising of market data, trading data and derived analytics.\nDesign and develop model surveillance framework.\nAutomate data processing tasks using scripting languages (e.g., Python, Scala) and orchestration tools (e.g., Airflow, Luigi).\nUtilize cloud-based data platforms (e.g., GCP, Azure, etc.) to manage and process large datasets efficiently.\nImplement the ML models/analytics for Trading/Liquidity and integrate into Aladdin analytical system in accordance with BlackRock's model governance policy.\n\nQualifications\n\nB.Tech / B.E. / M.Sc. degree in a quantitative discipline (Mathematics, Physics, Computer Science, Finance or similar area). MS/M.Tech. / PhD is a plus.\nStrong background in Mathematics, Statistics, Probability, Linear Algebra\nKnowledgeable about data mining, data analytics, data modeling\nExperience with data engineering tools and technologies (e.g., Apache Spark, Hadoop).\nStrong understanding of relational and non-relational databases (e.g., SQL, NoSQL).\nProficiency in scripting languages for data manipulation and automation (e.g., Python, Scala).\nExperience working with cloud platforms for data storage and processing (e.g., Azure, GCP).\nAbility to work independently and efficiently in a fast-paced and team-oriented environment.\nPrevious experience or knowledge in fixed income market and market liquidity is not required but a big plus.\n\nFor professionals with no prior financial industry experience, this position is a unique opportunity to gain in-depth knowledge of the asset management process in a world-class organization.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",
        "skills": [
            "Airflow",
            "Luigi",
            "Hadoop",
            "Gcp",
            "Scala",
            "Spark",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Distinguished, Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nAs a Distinguished Data Scientist specializing in GenAI and Agentic AI, you will lead the development and deployment of AI/ML models for associate facing systems & technologies; work with a team of Data scientists and engineers, and collaborate with product leaders to drive innovation. Your role will also involve overseeing project lifecycles, ensuring compliance with standards, and enhancing Walmart's reputation through research and partnerships.\n\nWhat you'll do...\n\nAbout Team:\n\nThe Enterprise Business Services supports the successful deployment and adoption of new People technology, Finance and Risk Technologies across the enterprise. As a Fortune #1 company, our work impacts millions of associates globally. We strive to continuously improve people technology and products to help managers and associates so they can focus on what matters most - supporting our customers and members. Walmart Global Tech's Enterprise Business Services, which is invested in building a compact, robust organization that includes technology solutions for Transaction & Risk Technology, Finance, People, and the Associate Digital Experience.\n\nWhat you will do:\n\nAs a Distinguished Data Scientist with specialization in GenAI and Agentic AI, your role will be pivotal in:\n\nLeading the formulation of strategies and roadmaps for the design, development, and deployment of AI/ML, NLP, and GenAI models, ensuring their seamless transition into production environments and guaranteeing their reliability and scalability.\nConstructing and fine-tuning cutting-edge large language models (LLMs) by leveraging Walmart's vast datasets.\nworking with a team of Data scientists and engineers in solving intricate AI/ML challenges through research and development, pushing the limits of innovation and making groundbreaking contributions to the field.\nOverseeing the entire lifecycle of projects, from inception and data collection to model prototyping and deployment, while effectively managing stakeholder relationships and facilitating cross-functional communication.\nCollaborating extensively with product and engineering leaders to accelerate innovations in discovery experiences, utilizing insights, frameworks, machine learning prototypes, and emphasizing the strategic importance of AI initiatives.\nUpholding and enforcing rigorous standards, governance, and best practices in AI/ML model development, ensuring strict compliance with model and data governance standards.\nEnhancing external reputation by publishing your team's work in prestigious AI/ML conferences and nurturing partnerships with academic institutions.\nAdhering strictly to Walmart's policies, procedures, mission, values, standards of ethics, and integrity.\nDesigning end-to-end system architecture for GenAI/AI/ML and data-intensive applications, setting new benchmarks in the industry.\nLeading, managing, and mentoring a team of data scientists, helping them grow professionally and enhance their business understanding.\nDeveloping and deploying robust, production-grade real-time/batch machine learning services that set industry benchmarks.\nCollaborating with product managers to design user journeys, feedback loops and analyze user telemetry, thus creating seamless user experiences.\nIdentifying or proposing innovative AI/ML use-cases to business teams to boost business processes, and developing quick MVPs/POCs to enable stakeholders to make data-driven decisions.\n\nWhat you'll bring:\n\nSome of the skills needed for this role are but not limited to - includes a mix of Engineering, Algorithms, Data Science, Machine Learning, and Artificial Intelligence:\n\nGenAI: understanding of usecases in developing and implementing AI models and algorithms. Python: Knowledge of Python, including libraries such as NumPy, Pandas, and Scikit-learn. Apache Spark: Experience with big data processing frameworks like Apache Spark. Scala: Knowledge of Scala for data processing and analysis. Machine Learning: Expertise in machine learning techniques and frameworks such as TensorFlow, Keras, and PyTorch. Data Science: Strong foundation in data science principles, including statistical analysis, data visualization, and data manipulation. Algorithms: Ability to design and implement efficient algorithms for data processing and analysis. Engineering: Experience in software engineering practices, including version control, testing, and deployment.\n\nAbout Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That's what we do at Walmart Global Tech. We're a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world's largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human led is our true disruption.\n\nWays of Working\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 6 years experience in an analytics related field. Option 2: Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 3: 8 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\nRmz Millenia Business Park, No 143, Campus 1B (1St -6Th Floor), Dr. Mgr Road, (North Veeranam Salai) Perungudi , India R-2111693",
        "skills": [
            "GenAI",
            "data science principles",
            "Scikit-learn",
            "Agentic AI",
            "Deployment",
            "efficient algorithms",
            "Data Manipulation",
            "Version Control",
            "Testing",
            "Scala",
            "Apache Spark",
            "AI ML",
            "Tensorflow",
            "Numpy",
            "Nlp",
            "Pytorch",
            "software engineering practices",
            "Pandas",
            "Data Visualization",
            "Keras",
            "Python",
            "Statistical Analysis"
        ]
    },
    {
        "job_title": "(IND) Distinguished, Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWalmart is a multi-national people led tech powered omnichannel retailer with over 2.2M associates. At People Tech, we help, support and empower the HR department with technology and solutions to improving productivity, harnessing the opportunities for growth, focusing on well-being and cultivating cultures of belonging to all Walmart's associates.\n\nEnterprise Business Services (EBS) is seeking a highly skilled and experienced Distinguished Data Scientist directly reporting to the VP and org head at IDC. As a Distinguished Data Scientist specializing in GenAI and Agentic AI, you will lead the development and deployment of AI/ML models for associate facing systems & technologies; work with a team of Data scientists and engineers, and collaborate with product leaders to drive innovation. Your role will also involve overseeing project lifecycles, ensuring compliance with standards, and enhancing Walmart's reputation through research and partnerships.\n\nWhat you'll do...\n\nAbout Team:\n\nThe Enterprise Business Services supports the successful deployment and adoption of new People technology, Finance and Risk Technologies across the enterprise. As a Fortune #1 company, our work impacts millions of associates globally. We strive to continuously improve people technology and products to help managers and associates so they can focus on what matters most - supporting our customers and members. Walmart Global Techs Enterprise Business Services, which is invested in building a compact, robust organization that includes technology solutions for Transaction ; Risk Technology, Finance, People, and the Associate Digital Experience.\n\nWhat you will do:\n\nAs a Distinguished Data Scientist with specialization in GenAI and Agentic AI, your role will be pivotal in:\n\nLeading the formulation of strategies and roadmaps for the design, development, and deployment of AI/ML, NLP, and GenAI models, ensuring their seamless transition into production environments and guaranteeing their reliability and scalability.\nConstructing and fine-tuning cutting-edge large language models (LLMs) by leveraging Walmarts vast datasets.\nWorking with a team of Data scientists and engineers in solving intricate AI/ML challenges through research and development, pushing the limits of innovation and making groundbreaking contributions to the field.\nOverseeing the entire lifecycle of projects, from inception and data collection to model prototyping and deployment, while effectively managing stakeholder relationships and facilitating cross-functional communication.\nCollaborating extensively with product and engineering leaders to accelerate innovations in discovery experiences, utilizing insights, frameworks, machine learning prototypes, and emphasizing the strategic importance of AI initiatives.\nUpholding and enforcing rigorous standards, governance, and best practices in AI/ML model development, ensuring strict compliance with model and data governance standards.\nEnhancing external reputation by publishing your teams work in prestigious AI/ML conferences and nurturing partnerships with academic institutions.\nAdhering strictly to Walmarts policies, procedures, mission, values, standards of ethics, and integrity.\nDesigning end-to-end system architecture for GenAI/AI/ML and data-intensive applications, setting new benchmarks in the industry.\nLeading, managing, and mentoring a team of data scientists, helping them grow professionally and enhance their business understanding.\nDeveloping and deploying robust, production-grade real-time/batch machine learning services that set industry benchmarks.\nCollaborating with product managers to design user journeys, feedback loops and analyze user telemetry, thus creating seamless user experiences.\nIdentifying or proposing innovative AI/ML use-cases to business teams to boost business processes, and developing quick MVPs/POCs to enable stakeholders to make data-driven decisions.\n\nWhat you'll bring:\n\nSome of the skills needed for this role are but not limited to - includes a mix of Engineering, Algorithms, Data Science, Machine Learning, and Artificial Intelligence:\n\nGenAI: understanding of usecases in developing and implementing AI models and algorithms.\nPython: Knowledge of Python, including libraries such as NumPy, Pandas, and Scikit-learn.\nApache Spark: Experience with big data processing frameworks like Apache Spark.\nScala: Knowledge of Scala for data processing and analysis.\nMachine Learning: Expertise in machine learning techniques and frameworks such as TensorFlow, Keras, and PyTorch.\nData Science: Strong foundation in data science principles, including statistical analysis, data visualization, and data manipulation.\nAlgorithms: Ability to design and implement efficient algorithms for data processing and analysis.\nEngineering: Experience in software engineering practices, including version control, testing, and deployment.\n\nAbout Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. Thats what we do at Walmart Global Tech. Were a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the worlds largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human led is our true disruption.\n\nWays of Working\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nWalmart Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve to live better when we really know them. That means understanding, respecting, and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 6 years experience in an analytics related field. Option 2: Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 3: 8 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\nPardhanani Wilshire Ii, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2111672",
        "skills": [
            "GenAI",
            "Engineering",
            "Data Science",
            "Machine Learning",
            "Algorithms",
            "Scala",
            "Apache Spark",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Team\nEnterprise Business Services is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, Associate Digital Experience. Our team is responsible for design and development of solution that knows our consumer's needs better than ever by predicting what they want based on unconstrained demand, and efficiently unlock strategic growth, economic profit, and wallet share by orchestrating intelligent, connected planning and decisioning across all functions. We interact with multiple teams across the company to provide scalable robust technical solutions.This role will play crucial role in overseeing the planning, execution and delivery of complex projects within team\nWe are a Data Science & ML Engineering team under the Enterprise Business Services organisation focussing on Finance domain. Our team is building products for Finance business from scratch, dealing with challenges from gathering user data, developing machine learning models to understand business processes better and making them more holistically data driven and intelligent.\nAutomated Forecasting, Anomaly Detection, NLP, Gen AI, Optimisation are only some of the areas we are actively working on. We leverage machine learning, natural language processing (NLP), LLMs and big data technologies to get the scale and precision required for solving these problems. We are also working on some cutting edge deep learning and Gen AI solutions to solve some of the business critical problems.\nWhat you'll do:\nAs a Data Scientist for Walmart , you'll have the opportunity to\nDrive data-derived insights across the wide range of retail divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assessing data validity and synthesizing data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train statistical models and machine learning algorithms for replication for future projects\nProductionize the models and make those available at scale\nBuild and maintain end to end Machine Learning pipelines\nCommunicate recommendations to business partners and influencing future plans based on insights\nWhat you'll bring:\nBachelors with > 4 years of experience / Master's degree with > 2 years of experience. Educational qualifications should be preferably in Computer Science/Mathematics/Statistics or a related area. Experience should be relevant to the role.\nHigh proficiency in data mining, modeling, validation and insight generation. Excellent working knowledge of statistics, mathematics, machine learning, data mining, deep learning\nHigh proficiency in coding including Python with excellent knowledge of Data Structures\nExperience in Analyzing the Complex Problems and translate it into data science algorithms\nExperience in machine learning, supervised and unsupervised and deep learning. Hands on experience in NLP.\nExperience with big data analytics - identifying trends, patterns, and outliers in large volumes of data\nStrong Experience with big data platforms Hadoop (Hive, Pig, Map Reduce, HQL, Scala, Spark)\nHands on experience with Git\nExperience with SQL and relational databases, data warehouse\nGood to have:\nExperience in Finance domain.\nExperience in Gen AI\nDemonstrated success in data science platforms like Kaggle.\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions while being inclusive of all people.",
        "skills": [
            "Relational Databases",
            "Computational Algorithms",
            "Statistical Models",
            "Map Reduce",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Data Structures",
            "Scala",
            "Data Warehouse",
            "Pig",
            "Sql",
            "Deep Learning",
            "Git",
            "Hive",
            "Spark",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - Periscope",
        "company_name": "McKinsey & Company",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Who You'll Work With\n\nYou will be based in one of our offices in Bengaluru or Gurugram as part of our Growth, Marketing & Sales practice.\n\nThe Growth, Marketing & Sales practice strives to help client in both consumer and business-to-business environments on a wide variety of marketing and sales topics.\n\nThe mission of this practice is to help clients achieve marketing-driven profit growth.\n\nOur clients benefit from our experience in core areas of marketing such as customer insights, marketing ROI, digital marketing, CLM pricing, and sales and channel management.\n\nYour Impact\n\nYou will also be responsible for developing deep Retail domain understanding in at least one of the following areas Customer Analytics, Pricing Optimization, Inventory Management, E-commerce, or Digital Transformation.\n\nCollaboration with business stakeholders, engineers and internal teams to build and implement extraordinary retail focused data products (reusable asset) and solutions and delivering them right to the client will be of utmost importance.\n\nYou will work on the frameworks and libraries that our teams of Data Scientists and Data Engineers use to progress from data to impact.\n\nYou will guide global companies through data science solutions to transform their businesses and enhance performance across industries including E-commerce, Grocery, F&B and CPG\n\nYour Qualifications and Skills\n\nMaster's degree in computer science, engineering or mathematics, or equivalent experience\n5+ years of relevant experience with strong foundations of statistics and machine learning techniques\nProven experience applying machine learning techniques to solve business problems\nProven experience in translating technical methods to non-technical stakeholder\nProven experience writing production-grade code (Python / Pyspark) for machine learning in a professional setting\nStrong understanding of analytics libraries (e.g., pandas, numpy, matplotlib, scikit-learn, statsmodels, kedro, mlflow)\nExperience in any cloud platforms (AWS, Azure, or GCP)\nFamiliarity with containerization technologies (Docker, Docker-compose)\nExperience with Git-based workflows and automation frameworks, specifically GitHub Actions and GitLab CI/CD\nFamiliarity or hands-on experience with data visualization tools (PowerBI, Tableau, etc.)",
        "skills": [
            "scikit-learn",
            "mlflow",
            "GitHub Actions",
            "Docker-compose",
            "GitLab CI CD",
            "kedro",
            "statsmodels",
            "Pyspark",
            "Tableau",
            "Pandas",
            "Numpy",
            "Gcp",
            "Powerbi",
            "Docker",
            "Matplotlib",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "PRINCIPAL, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "15-20 Years",
        "salary": null,
        "location": "Chennai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nRole: Principal, Data Scientist\n\nExperience: 15 - 20 years\n\nLocation: Chennai\n\nAbout EBS team:\n\nEnterprise Business Services is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, Associate Digital Experience. Our team is responsible for design and development of solution that knows our consumers needs better than ever by predicting what they want based on unconstrained demand, and efficiently unlock strategic growth, economic profit, and wallet share by orchestrating intelligent, connected planning and decisioning across all functions. We interact with multiple teams across the company to provide scalable robust technical solutions. This role will play crucial role in overseeing the planning, execution and delivery of complex projects within team\n\nWalmarts Enterprise Business Services (EBS) is a powerhouse of several exceptional teams delivering world-class technology solutions and services making a profound impact at every level of Walmart.\n\nAs a key part of Walmart Global Tech, our teams set the bar for operational excellence and leverage emerging technology to support millions of customers, associates, and stakeholders worldwide. Each time an associate turns on their laptop, a customer makes a purchase, a new supplier is onboarded, the company closes the books, physical and legal risk is avoided, and when we pay our associates consistently and accurately, that is EBS. Joining EBS means embarking on a journey of limitless growth, relentless innovation, and the chance to set new industry standards that shape the future of Walmart.\n\nAbout Team\n\nThe data science team at Enterprise Business Services Pillar at Walmart Global Tech focuses on using the latest research in machine learning, statistics, and optimization to solve business problems. We mine data, distill insights, extract information, build analytical models, deploy Machine Learning algorithms, and use the latest algorithms and technology to empower business decision-making. In addition, we work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions. Advanced analytical algorithms driven by our team will help Walmart to optimize business operations, business practices and change the way our customers shop.\n\nThe data science community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Labs will eventually benefit our operations ; our associates, helping Customers Save Money to Live Better.\n\nWhat You Will Do\n\nAs a Staff Data Scientist for Walmart Global Tech, you'll have the opportunity to Drive data-derived insights across a wide range of retail ; Finance divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assess data validity and synthesize data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train AI/ML models for replication for future projects\nDeploy and maintain the data science solutions\nCommunicate recommendations to business partners and influence future plans based on insights\nConsult with business stakeholders regarding algorithm-based recommendations and be a thought-leader to develop these into business actions.\nClosely partners with the Senior Manager ; Director of Data Science to drive data science adoption in the domain\nGuides. data scientists, senior data scientists ; staff data scientists across multiple sub-domains to ensure on-time delivery of ML products\nDrive efficiency across the domain in terms of DS and ML best practices, ML Ops practices, resource utilization, reusability and multi-tenancy.\nLead multiple complex ML products and guide senior tech leads in the domain in efficiently leading their products.\nDrive synergies across different products in terms of algorithmic innovation and sharing of best practices.\nProactive identification of complex business problems that can be solved using advanced ML, finding opportunities and gaps in the current business domain\nEvaluates proposed business cases for projects and initiatives\n\nWhat You Will Bring\n\nMasters with > 16 years OR Ph.D. with > 15 years of relevant experience. Educational qualifications should be Computer Science/Statistics/Mathematics or a related area.\nMinimum 10 years of experience as a data science technical lead\nAbility to lead multiple data science projects end to end.\nDeep experience in building data science solution in areas like fraud prevention, forecasting, shrink and waste reduction, inventory management, recommendation, assortment and price optimization\nDeep experience in simultaneously leading multiple data science initiatives end to end from translating business needs to analytical asks, leading the process of building solutions and the eventual act of deployment and maintenance of them Strong experience in machine learning: Classification models, regression models, NLP, Forecasting, Unsupervised models, Optimization, Graph ML, Causal inference, Causal ML, Statistical Learning, experimentation ; Gen-AI\nIn Gen-AI, it is desirable to have experience in embedding generation from training materials, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nAbility to scale and deploy data science solutions.\nStrong Experience with one or more of Python and R.\nExperience in GCP/Azure\nStrong Experience in Python, PySpark\nGoogle Cloud platform, Vertex AI, Kubeflow, model deployment\nStrong Experience with big data platforms Hadoop (Hive, Map Reduce, HQL, Scala)\nExperience with GPU/CUDA for computational efficiency\n\nAbout Walmart Global Tech\n\nFrom entry-level to executive positions, Walmart provides limitless opportunities for growth, and career development. Walmart started small, with a single discount store and the simple philosophy of selling more for less. Today, we are a growing technology-enabled company founded on the same values as our first store. We establish clear expectations, empower associates to manage their work, and hold ourselves and one another to a high standard. Walmarts scale enables us to have an. No other company has the reach of Walmart, with 2.3 million associates worldwide and over 230 million weekly customers. Walmart is reshaping retail by investing in an expanding workforce. While technology is at the heart of our digital transformation, people are the reason we succeed and the force behind our innovations. We train our team in the skillsets of the future and bring in experts like you to help us grow.\n\nFlexible, Hybrid Work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nEqual Opportunity Employer\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nBelonging at Walmart\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feels included, everyone wins. Approximately 90% of the U.S. population lives within 10 miles of a Walmart or Sam's Club our associates and customers reflect the makeup of all of America, as well as the 18 other countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nBelonging: We aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual.\n\nAssociates: We want to ensure our associates worldwide are seen for their unique contributions, supported in their daily work, and connected to co-workers.\n\nWalmart is the U.S. largest private employer.\nOur policies, practices, and programs promote fairness and the same treatment for all associates. Everyone in our workforce has the same access to opportunities for growth, development, and advancement.\nWe transparently report on our workforce twice a year and we have associate resource groups to further engagement, networking, connection and a sense of community.\n\nBusiness ; Customers: We provide an assortment of products and services that meet the unique needs of our customers and members while strengthening our connection to the communities we serve.\n\nWe operate sensory friendly hours in all stores from 8 a.m. to 10 a.m. daily and offer Carolines Carts - a specially designed shopping cart for children and adults with disabilities.\nOur focus every day is how we can best serve our customers with quality food and goods at everyday low prices, which are 10-25% lower than those of competitors.\n\nCommunities: Walmart thrives when we take a shared value approach, complementing business with philanthropy to strengthen the communities where we operate and prioritize issues that are meaningful to our business and all customers.\n\nWalmart is one of the most charitable companies in the Fortune 500. Last year we gave away over 8% profits through a combination of in-kind and cash gifts totaling more than $1.7 billion.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 5 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 3 years experience in an analytics related field. Option 3: 7 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\nRmz Millenia Business Park, No 143, Campus 1B (1St -6Th Floor), Dr. Mgr Road, (North Veeranam Salai) Perungudi , India R-2126871",
        "skills": [
            "Data Science Techniques",
            "Computational Algorithms",
            "GPU CUDA",
            "Statistical Models",
            "AI ML Models",
            "R",
            "Map Reduce",
            "Kubeflow",
            "Vertex AI",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Scala",
            "Pyspark",
            "Hive",
            "Gcp",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Aladdin Financial Engineering Data Scientist Associate",
        "company_name": "BlackRock",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About This Role\n\nBlackRock Overview:\n\nBlackRock is one of the world's preeminent asset management firms and a premier provider of global investment management, risk management and advisory services to institutional, intermediary and individual investors around the world. BlackRock offers a range of solutions from rigorous fundamental and quantitative active management approaches aimed at maximizing outperformance to highly efficient indexing strategies designed to gain broad exposure to the world's capital markets. Our clients can access our investment solutions through a variety of product structures, including individual and institutional separate accounts, mutual funds and other pooled investment vehicles, and the industry-leading iShares ETFs.\n\nAladdin Financial Engineering Group (AFE)\n\nAFE is a diverse and global team with a keen interest and expertise in all things related to technology and financial analytics. The group is responsible for the research and development of quantitative financial and behavioral models and tools across many different areas single-security pricing, prepayment models, risk, return attribution, liquidity, optimization and portfolio construction, scenario analysis and simulations, etc. and covering all asset classes. The group is also responsible for the technology platform that delivers those models to our internal partners and external clients, and their integration with Aladdin.\n\nAFE conducts leading research on the areas above, delivering state-of-the-art models. AFE publishes applied scientific research frequently, and our members present regularly at leading industry conferences. AFE engages constantly with the sales team in client visits and meetings.\n\nJob Description\n\nYou can help conduct research to build quantitative financial models and portfolio analytics that help managing most of the money of the world's largest asset manager. You can bring all yourself to the job. From the top of the firm down we embrace the values, identities and ideas brought by our employees.\n\nWe are looking for curious people with a strong background in data science, quantitative research and machine learning, have awesome problem-solving skills, insatiable appetite for learning and innovating, adding to BlackRock's vibrant research culture.\n\nIf any of this excites you, we are looking to expand our team. We currently have Data Scientist role with the AFE Investment AI (IAI) Team, India (Mumbai or Gurugram location). The securities market is undergoing a massive transformation as the industry is embracing machine learning and, more broadly, AI, to help evolve the investment process. Pioneering this journey at BlackRock, the team has better deliver applied AI investment analytics to help both BlackRock and Aladdin clients achieve scale through automation while safeguarding alpha generation. The IAI team combines AI / ML methodology and technology skills with deep subject matter expertise in fixed income, equity, and multi-asset markets, and the buyside investment process.\n\nWe are building next generation liquidity, security similarity and pricing models leveraging our expertise in quantitative research, data science and machine learning. The models we build use innovative machine learning approaches, have real practical value and are used by traders and portfolio managers alike. Our models use cutting edge econometric/statistical methods and tools. The models themselves have real practical value and are used by traders, portfolio managers and risk managers representing different investment styles (fundamental vs. quantitative) and across different investment horizons. Research is conducted predominantly in Python and Scala, and implemented into production by a separate, dedicated team of developers. These models have a huge footprint of usage across the entire Aladdin client base, and so we place special emphasis on scalability and ensuring adherence to BlackRock's rigorous standards of model governance and control.\n\nBackground And Responsibilities\n\nWe are looking to hire a Data Scientist with 4+ years experience to join AFE Investment AI India team focusing on Trading and Liquidity to work closely with other data scientists/researchers to support Risk Mangers, Portfolio Managers and Traders. We build cutting edge liquidity analytics using a wide range of ML algos and a broad array of technologies (Python, Scala, Spark/Hadoop, GCP, Azure). This role is a great opportunity to work closely with the Portfolio Managers, Risk Managers and Trading team, spanning areas such as:\n\nDesign, develop, and maintain data pipelines to extract, transform, and load data from various sources into our data warehouse/lake.\nWork with data scientists and analysts to understand data needs and design appropriate data models.\nImplement data quality checks and ensure the accuracy and consistency of data throughout the processing pipeline.\nPerform analysis of large data sets comprising of market data, trading data and derived analytics.\nDesign and develop model surveillance framework.\nAutomate data processing tasks using scripting languages (e.g., Python, Scala) and orchestration tools (e.g., Airflow, Luigi).\nUtilize cloud-based data platforms (e.g., GCP, Azure, etc.) to manage and process large datasets efficiently.\nImplement the ML models/analytics for Trading/Liquidity and integrate into Aladdin analytical system in accordance with BlackRock's model governance policy.\n\nQualifications\n\nB.Tech / B.E. / M.Sc. degree in a quantitative discipline (Mathematics, Physics, Computer Science, Finance or similar area). MS/M.Tech. / PhD is a plus.\nStrong background in Mathematics, Statistics, Probability, Linear Algebra\nKnowledgeable about data mining, data analytics, data modeling\nExperience with data engineering tools and technologies (e.g., Apache Spark, Hadoop).\nStrong understanding of relational and non-relational databases (e.g., SQL, NoSQL).\nProficiency in scripting languages for data manipulation and automation (e.g., Python, Scala).\nExperience working with cloud platforms for data storage and processing (e.g., Azure, GCP).\nAbility to work independently and efficiently in a fast-paced and team-oriented environment.\nPrevious experience or knowledge in fixed income market and market liquidity is not required but a big plus.\n\nFor professionals with no prior financial industry experience, this position is a unique opportunity to gain in-depth knowledge of the asset management process in a world-class organization.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",
        "skills": [
            "Airflow",
            "Luigi",
            "Nosql",
            "Hadoop",
            "Gcp",
            "Scala",
            "Spark",
            "Azure",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist - NLP",
        "company_name": "Gartner",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About This Role ()\n\nIn Gartner's Services Data Science team, we innovate the way our team helps clients receive value, so technology leaders will be able to make smarter decisions in a different way.\n\nWe are searching for a talented data scientist to join our team.You will have access to the best facilities, technology and expertise within the industry and will work on challenging business problems. This is an excellent opportunity to be part of a new venture, in a start-up environment where you can truly develop your skill set and knowledge and bring impact to the team.\n\nWhat You'll Do\n\nHere is a sample of some initiatives that our data scientists work on\n\nDesigning and implementing state of the art Large Language Model (LLM) based agents that seamlessly synthesize complex information and initiate important actions in a business workflow.\nUsing advanced Generative AI techniques deriving actionable insights from unstructured text data, such as call transcripts and emails.\nPredicting client interest basis their digital footprint and making relevant recommendations to drive higher client value delivery\nLeverage statistical and machine learning techniques to extract actionable insights from client retention data.\nDevelop customer churn prediction models that proactively identify at-risk clients,\nBuild tools to process structured and unstructured data\nEngineering features and signals to train ML model from diverse data collection\n\nWhat You'll Need\n\nBS/MS/PhD in Computer Science or other technology, Math, Physics, Statistics or Economics (focus on Natural Language Processing, Information Retrieval a plus)\n3+ years experience in data science methodologies as applied to live initiatives or software development\nMinimum 3+ years of experience in python coding and statistical analysis\nMinimum 2 years working experience in several of the following:\nPrompt Engineering and working with LLMs\nMachine Learning and statistical techniques\nData mining and recommendation systems\nNatural Language Processing and Information Retrieval\nExperience working with large volumes of data\nUser behavior modeling\n\nWhat you are :\n\nA team player.You get along well with your colleagues and are always ready to help get things done. You enjoy working on projects with multiple people and share knowledge.\nPassionate about learning.You thrive on complex technical challenges and are always eager to learn the latest technologies.\nOrganized and detailed-oriented.You think ahead of time about how best to implement new features, and your code is clean,well-organizedand properly documented.\nInnovative. You are always proactively looking for opportunities to problem solve using innovative methods that impact the business\n\nWhat We Offer\n\nA collaborative, positive culture. You'll work with people who are as enthusiastic, smart and driven as you are. You'll be managed by the best too.\nLimitless growth and learning opportunities. We offer the excitement of a fast-paced entrepreneurial workplace and the professional growth opportunities of an established global organization.\n\nWho are we\n\nAt Gartner, Inc. (NYSE:IT), we guide the leaders who shape the world.\n\nOur mission relies on expert analysis and bold ideas to deliver actionable, objective insight, helping enterprise leaders and their teams succeed with their mission-critical priorities.\n\nSince our founding in 1979, we've grown to more than 20,000 associates globally who support 15,000 client enterprises in 90 countries and territories. We do important, interesting and substantive work that matters. That's why we hire associates with the intellectual curiosity, energy and drive to want to make a difference. The bar is unapologetically high. So is the impact you can have here.\n\nWhat makes Gartner a great place to work\n\nOur sustained success creates limitless opportunities for you to grow professionally and flourish personally. We have a vast, virtually untapped market potential ahead of us, providing you with an exciting trajectory long into the future. How far you go is driven by your passion and performance.\n\nWe hire remarkable people who collaborate and win as a team. Together, our singular, unifying goal is to deliver results for our clients.\n\nOur teams are inclusive and composed of individuals from different geographies, cultures, religions, ethnicities, races, genders, sexual orientations, abilities and generations.\n\nWe invest in great leaders who bring out the best in you and the company, enabling us to multiply our impact and results. This is why, year after year, we are recognized worldwide as a great place to work.\n\nWhat do we offer\n\nGartner offers world-class benefits, highly competitive compensation and disproportionate rewards for top performers.\n\nIn our hybrid work environment, we provide the flexibility and support for you to thrive working virtually when it's productive to do so and getting together with colleagues in a vibrant community that is purposeful, engaging and inspiring.\n\nReady to grow your career with Gartner Join us.\n\nThe policy of Gartner is to provide equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, ancestry, disability, veteran status, or any other legally protected status and to seek to advance the principles of equal employment opportunity.\n\nGartner is committed to being an Equal Opportunity Employer and offers opportunities to all job seekers, including job seekers with disabilities. If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access the Company's career webpage as a result of your disability. You may request reasonable accommodations by calling Human Resources at +1 (203) 964-0096 or by sending an email to [HIDDEN TEXT].\n\nJob Requisition ID:96693\n\nBy submitting your information and application, you confirm that you have read and agree to the country or regional recruitment notice linked below applicable to your place of residence.\n\nGartner Applicant Privacy Link: https://jobs.gartner.com/applicant-privacy-policy\n\nFor efficient navigation through the application, please only use the back button within the application, not the back arrow within your browser.",
        "skills": [
            "Recommendation Systems",
            "Generative AI",
            "Prompt Engineering",
            "Machine Learning",
            "Natural Language Processing",
            "Data Mining",
            "Information Retrieval",
            "Statistical Analysis",
            "Python"
        ]
    },
    {
        "job_title": "IN-Senior Associate_Data Scientist_D&A_Advisory_Gurgaon",
        "company_name": "PwC India",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nDesign, develop, and maintain interactive dashboards and visualizations.\nWork closely with data analysts to understand data requirements and translate them into visualformats.\nSelect appropriate visualization tools (PowerBI/Tableau/Qlik) and techniques to display data effectively.\nEnsure accuracy, consistency, and integrity of the data displayed.\nConduct user testing and gather feedback to improve visualization designs.\nTrain clients and team members on data visualization best practices and tools.\n\nMandatory Skill Sets\n\n\nPower BI/Tableau/Qlik\n\nPreferred Skill Sets\n\nPower BI/Tableau/Qlik\n\nYears Of Experience Required\n\n4 7 yrs\n\nEducation Qualification\n\nB.tech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nStructured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date",
        "skills": [
            "Tableau",
            "Power Bi",
            "Qlik"
        ]
    },
    {
        "job_title": "IN-Senior Associate_Data Scientist_D&A_Advisory_Gurgaon",
        "company_name": "PwC India",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nProven experience as a Data Scientist or similar role in a consulting or corporate environment.\nProficient in programming languages such as Python or R.\nExperience with data manipulation and analysis using SQL and big data technologies.\nStrong knowledge of machine learning libraries (e.g., TensorFlow, PyTorch, Scikit-learn).\nResearch and develop cutting-edge AI models and components to solve client-specific challenges.\nCollaborate with clients and cross-functional teams to define project scopes and deliver innovative AI solutions.\nOptimize AI algorithmsforperformance and scalability.\n\nMandatory Skill Sets\n\n\nTensorFlow, PyTorch, Scikit-learn, SQL\n\nPreferred Skill Sets\n\nTensorFlow, PyTorch, Scikit-learn, SQL\n\nYears Of Experience Required\n\n4 7 yrs\n\nEducation Qualification\n\nB.tech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nStructured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date",
        "skills": [
            "R",
            "Scikit-learn",
            "Tensorflow",
            "Pytorch",
            "Sql",
            "Python"
        ]
    },
    {
        "job_title": "(IND) SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "5-7 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team:\n\nThis is the team which builds reusable technologies that aid in acquiring customers, onboarding and empowering merchants besides ensuring a seamless experience for both these stakeholders. We also optimize tariffs and assortment, adhering to the Walmart philosophy - Everyday Low Cost. In addition to ushering in affordability, we also create personalized experiences for customers the omnichannel way, across all channels - in-store, on the mobile app and websites. Marketplace is the gateway to domestic and international Third-Party sellers; we enable them to manage their end-to-end onboarding, catalog management, order fulfilment, return & refund management. Our team is responsible for design, development, and operations of large-scale distributed systems by leveraging cutting-edge technologies in web/mobile, cloud, big data & AI/ML. We interact with multiple teams across the company to provide scalable robust technical solutions.\n\nWhat you'll do:\n\nAs a Data Scientist for Walmart , you'll have the opportunity to\n\nDrive data-derived insights across the wide range of retail divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assessing data validity and synthesizing data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train statistical models and machine learning algorithms for replication for future projects\nCommunicate recommendations to business partners and influencing future plans based on insights\n\nWhat you'll bring:\n\nVery good knowledge of the foundations of machine learning and statistics\nHand on Experience in building and maintaining Gen AI powered solutions in production\nExperience in Analyzing the Complex Problems and translate it into data science algorithms\nExperience in machine learning, supervised and unsupervised and deep learning.\nHands on experience in Computer Visions and NLP.\nExperience with big data analytics - identifying trends, patterns, and outliers in large volumes of data\nStrong Experience in Python with excellent knowledge of Data Structures\nStrong Experience with big data platforms Hadoop (Hive, Pig, Map Reduce, HQL, Scala, Spark)\nHands on experience with Git\nExperience with SQL and relational databases, data warehouse\nQualifications\nBachelors with > 7 years of experience / Master's degree with > 5 years of experience. Educational qualifications should be preferably in Computer Science/Mathematics/Statistics or a related area. Experience should be relevant to the role.\nGood to have:\nExperience in ecommerce domain.\nExperience in R and Julia\nDemonstrated success in data science platforms like Kaggle.\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2146190",
        "skills": [
            "Relational Databases",
            "Computational Algorithms",
            "Techniques",
            "Map Reduce",
            "Statistical Models",
            "Git",
            "Sql",
            "Pig",
            "Computer Vision",
            "Hql",
            "Hadoop",
            "Machine Learning",
            "Big Data Analytics",
            "Hive",
            "Python",
            "Data Structures",
            "Scala",
            "Nlp",
            "Data Warehouse",
            "Spark"
        ]
    },
    {
        "job_title": "SENIOR, DATA SCIENTIST",
        "company_name": "Walmart Global Tech India",
        "experience": "3-5 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout The Team\n\nCentroid team at Walmart serves as the backbone of Walmart's end-to-end supply chain strategy. They are entrusted with the task of designing and implementing a long-term supply chain strategy that uses advanced data analytics and data science. Their primary objective is to ensure that Walmart provides top-tier customer service while supporting the increasing demand over time and simultaneously operating at low and efficient costs.\n\nThe team utilizes sophisticated data analysis methods to understand patterns, identify potential bottlenecks, and predict future trends. This enables them to optimize processes, make informed business decisions, and enhance overall operational efficiency.\n\nOne of Centroid's key responsibilities also includes the creation of a Digital Twin Simulation platform for Walmart's supply chain. This innovative tool allows the team to test and validate all future strategies and tactical decisions before they are launched operationally. It also enables a deep assessment of long-term strategic sensitivity.\n\nIn essence, the Centroid team's work is integral to ensuring Walmart's supply chain is robust, flexible, and capable of adapting to ever-changing market demands. Their work helps to keep Walmart at the forefront of retail supply chain management, delivering exceptional service to customers while maintaining efficient operational costs.\n\nWhat You'll Do -\n\nDevelop and manage advanced data analytics models to optimize supply chain strategies, balancing customer satisfaction with operational cost and asset efficiency.\nLeverage data analytics to identify opportunities for improvement and drive impactful results through collaboration with cross-functional teams.\nEstablish relationships across Walmart functional areas to identify best practices, solicit data/input, coordinate interdisciplinary initiatives, and rally support for data-driven recommendations.\nSecure alignment and support from relevant business partners and management for data-centric projects, leading discussions to drive necessary change.\nUtilize all available data resources effectively to ensure successful project outcomes.\nCommunicate data insights clearly and persuasively through emails, verbal discussions, and presentations, tailoring communication methods to the audience for maximum impact.\nCollaborate with multiple supply chain business teams to proactively identify, assess, and leverage cost-saving and service improvement opportunities through advanced data analytics.\nUtilize advanced analytics models to derive insights that will inform policy design across various supply chain areas, laying out multiple scenarios and performing sensitivity analysis.\nCollaborate with Data Scientists and Engineers to productionize and scale advanced analytics models as needed.\nDevelop and present compelling data-driven narratives/documents/visuals to influence key stakeholders in their decision-making.\nProvide coaching and training support to other team members in the supply chain area, leveraging your expertise in advanced data analytics.\n\nWhat You'll Bring -\n\nStrong analytical acumen with technical expertise in Advanced Data Analytics and modelling\nExpert in SQL, - BigQuery like cloud data platforms.\nExpert in programming in Python, (or R)\nExperience in using data visualization tools like Tableau and Looker and be able to drive powerful insights.\nExperience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, and/or Spark)\nExperience in operating from a cloud environment such as Google Could Platform or Microsoft Azure.\nAbility to work in a fast-paced, iterative development environment.\nStrong communication skills, both written and verbal, plus ability to work with cross functional teams of technical and non-technical members.\nStrong ability to understand the business and have good stakeholder management capabilities.\nExperience of working in cross-functional environment and leading or mentoring teams.\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1- Bachelor's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Master's degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2153626",
        "skills": [
            "R",
            "Map Reduce",
            "SQL - BigQuery",
            "Looker",
            "Advanced Data Analytics",
            "Hive",
            "Hadoop",
            "Spark",
            "Tableau",
            "Microsoft Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "HCLTech",
        "experience": "Fresher",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Working knowledge of Azure AI\nWorking knowledge of LLM , RAG- Data insights , Reasoning Agent , Chain of thought\nKnowledge of Graph DB , COSMOS DB , Neo4j\nPrompt engineering\nAPI building\nDomain knowledge in Finance , data management",
        "skills": [
            "Prompt engineering",
            "API building",
            "Graph DB",
            "Azure AI",
            "Neo4j",
            "COSMOS DB"
        ]
    },
    {
        "job_title": "Staff, Data Scientist",
        "company_name": "Walmart Global Tech India",
        "experience": "8-14 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Role: Staff, Data Scientist\nExperience: 8 - 14 years\nLocation: Bangalore\nAbout EBS team:\nEnterprise Business Services is invested in building a compact, robust organization that includes service operations and technology solutions for Finance, People, Associate Digital Experience. Our team is responsible for design and development of solution that knows our consumer's needs better than ever by predicting what they want based on unconstrained demand, and efficiently unlock strategic growth, economic profit, and wallet share by orchestrating intelligent, connected planning and decisioning across all functions. We interact with multiple teams across the company to provide scalable robust technical solutions. This role will play crucial role in overseeing the planning, execution and delivery of complex projects within team\nAbout Team\nThe data science team at Enterprise Business Services Pillar at Walmart Global Tech focuses on using the latest research in machine learning, statistics, and optimization to solve business problems. We mine data, distill insights, extract information, build analytical models, deploy Machine Learning algorithms, and use the latest algorithms and technology to empower business decision-making. In addition, we work with engineers to build reference architectures and machine learning pipelines in a big data ecosystem to productize our solutions. Advanced analytical algorithms driven by our team will help Walmart to optimize business operations, business practices and change the way our customers shop.\nThe data science community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Labs will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat You Will Do\nAs a Staff Data Scientist for Walmart Global Tech, you'll have the opportunity to Drive data-derived insights across a wide range of retail & Finance divisions by developing advanced statistical models, machine learning algorithms and computational algorithms based on business initiatives\nDirect the gathering of data, assess data validity and synthesize data into large analytics datasets to support project goals\nUtilize big data analytics and advanced data science techniques to identify trends, patterns, and discrepancies in data. Determine additional data needed to support insights\nBuild and train AI/ML models for replication for future projects\nGuides. data scientists, senior data scientists & staff data scientists across multiple sub-domains to ensure on-time delivery of ML products\nDrive efficiency across the domain in terms of DS and ML best practices, ML Ops practices, resource utilization, reusability and multi-tenancy.\nLead multiple complex ML products and guide senior tech leads in the domain in efficiently leading their products.\nDrive synergies across different products in terms of algorithmic innovation and sharing of best practices.\nWhat You Will Bring\nMaster's with > 12 years OR Ph.D. with > 8 years of relevant experience. Educational qualifications should be Computer Science/Statistics/Mathematics or a related area.\nMinimum 6 years of experience as a data science technical lead\nAbility to lead multiple data science projects end to end.\nDeep experience in building data science solution in areas like fraud prevention, forecasting, shrink and waste reduction, inventory management, recommendation, assortment and price optimization\nDeep experience in simultaneously leading multiple data science initiatives end to end from translating business needs to analytical asks, leading the process of building solutions and the eventual act of deployment and maintenance of them Strong experience in machine learning: Classification models, regression models, NLP, Forecasting, Unsupervised models, Optimization, Graph ML, Causal inference, Causal ML, Statistical Learning, experimentation & Gen-AI\nIn Gen-AI, it is desirable to have experience in embedding generation from training materials, storage and retrieval from Vector Databases, set-up and provisioning of managed LLM gateways, development of Retrieval augmented generation based LLM agents, model selection, iterative prompt engineering and finetuning based on accuracy and user-feedback, monitoring and governance.\nAbility to scale and deploy data science solutions.\nStrong Experience with one or more of Python and R.\nExperience in GCP/Azure\nStrong Experience in Python, PySpark\nGoogle Cloud platform, Vertex AI, Kubeflow, model deployment\nStrong Experience with big data platforms Hadoop (Hive, Map Reduce, HQL, Scala)\nExperience with GPU/CUDA for computational efficiency\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.",
        "skills": [
            "Computational Algorithms",
            "GPU CUDA",
            "Statistical Models",
            "AI ML Models",
            "R",
            "Vertex AI",
            "Kubeflow",
            "Map Reduce",
            "Machine Learning",
            "Big Data Analytics",
            "Hadoop",
            "Hql",
            "Pyspark",
            "Scala",
            "Hive",
            "Gcp",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Data scientist- Python- AI/ML GEN AI- Across india",
        "company_name": "Capgemini Engineering",
        "experience": "Fresher",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "Develop strategies/solutions to solve problems in logical yet creative ways, leveraging state-of-the-art machine learning, deep learning and GEN AI techniques.\nTechnically lead a team of data scientists to produce project deliverables on time and with high quality.\nIdentify and address client needs in different domains, by analyzing large and complex data sets, processing, cleansing, and verifying the integrity of data, and performing exploratory data analysis (EDA) using state-of-the-art methods.\nSelect features, build and optimize classifiers/regressors, etc. using machine learning and deep learning techniques.\nEnhance data collection procedures to include information that is relevant for building analytical systems, and ensure data quality and accuracy.\nPerform ad-hoc analysis and present results in a clear manner to both technical and non-technical stakeholders.\nCreate custom reports and presentations with strong data visualization and storytelling skills to effectively communicate analytical conclusions to senior officials in a company and other stakeholders.\nExpertise in data mining, EDA, feature selection, model building, and optimization using machine learning and deep learning techniques.\nStrong programming skills in Python.\nExcellent communication and interpersonal skills, with the ability to present complex analytical concepts to both technical and non-technical stakeholders.\nPrimary Skills :\n- Excellent understanding and hand-on experience of data-science and machine learning techniques & algorithms for supervised & unsupervised problems, NLP and computer vision and GEN AI. Good applied statistics skills, such as distributions, statistical inference & testing, etc.\n- Excellent understanding and hand-on experience on building Deep-learning models for text & image analytics (such as ANNs, CNNs, LSTM, Transfer Learning, Encoder and decoder, etc).\n- Proficient in coding in common data science language & tools such as R, Python.\n- Experience with common data science toolkits, such as NumPy, Pandas, Matplotlib, StatsModel, Scikitlearn, SciPy, NLTK, Spacy, OpenCV etc.\n- Experience with common data science frameworks such as Tensorflow, Keras, PyTorch, XGBoost,etc.\n- Exposure or knowledge in cloud (Azure/AWS).\n- Experience on deployment of model in production.",
        "skills": [
            "GEN AI",
            "StatsModel",
            "Feature Selection",
            "R",
            "Spacy",
            "Matplotlib",
            "Machine Learning",
            "Scipy",
            "Data Mining",
            "Model Building",
            "Deep Learning",
            "Tensorflow",
            "Numpy",
            "Nltk",
            "Pytorch",
            "Pandas",
            "Opencv",
            "XGBoost",
            "Keras",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "Havells India Ltd",
        "experience": "8-10 Years",
        "salary": null,
        "location": "Noida, India",
        "industry": "Login to check your skill match score",
        "job_description": "Job Title: Sr. Data Scientist\nCompany-Havells India Ltd\nLocation: Noida HO\nAbout the Role:\nHavells is seeking a passionate and self-starting Data Scientist to join our team. This role is pivotal in driving growth, enhancing consumer experience, optimizing sales, and achieving cost savings through advanced analytics and AI/ML. You will handle Big Data, support routine data analytics/data mining requests, and lead predictive, AI/ML, and advanced analytics projects across various domains including Consumer, Sales Ops, Retail, and SCM.\nKey Responsibilities:\nSupport insights from consumer, loyalty, app, sales, and transactional data.\nCollaborate with the Data Engineering team on building Data Lake/Warehouse.\nData preparation, new attribute development, and creating Single View of customers and retailers.\nUtilize data mining/AI/ML for upsell/cross-sell, retention, loyalty, engagement campaigns, and target audience identification.\nBuild and maintain predictive analytics AI/ML models for use cases in Consumer Domain, Consumer Experience (CX), and e-Commerce (D2C) platform.\nSupport AI/ML models for Sales Transformation or SCM use cases.\nWork on GenAI use cases and projects.\nConduct deep data mining to support digital analytics, website behavior, app behavior analytics, call center/CS behavior, NPS, retailer, and electrician loyalty.\nKnowledge of data visualization, MIS/Dashboarding tools.\nSupport ad hoc analysis on consumer purchase behavior/market basket analysis.\nCandidate Profile:\nEducation: BE/BTech, MTech, MSc Stats, MBA, or Diploma in Analytics/Decision Science.\nExperience: 8-9+ years in advanced & predictive analytics in at least two domains (consumer, sales transformation, SCM, or Manufacturing Plant Analytics). Proficient in modern analytical tools and techniques (Python, R, SQL, SPSS, SAS).\nTools/Techniques: Python, PySpark, SQL, Hadoop, Hive, Regression, Classification, Decision Trees, Random Forests, SGBoost Clustering, SVM, Time Series, NLP, Neural Networks, Large Language Models, Computer Vision, Object Detection, CNN, Object Tracking. Familiarity with MLOps & MS Databricks cloud technologies is an advantage.\nPersonal Attributes: Strong analytical mindset, expertise in big data & cloud, self-starter, hands-on, excellent interpersonal skills, ability to work with cross-functional teams, translate technical data concepts into business language, and solve business problems via data. Ability to initiate work in ambiguous environments. Flair for consumer, digital & technology.\nApart from Easy Apply, Please fill the details on this link--> https://forms.office.com/r/aZSxQuTnGS\nJoin us and be a part of our journey to leverage analytics and AI/ML for transformative growth!\n#DataScientist #Analytics #AI #MachineLearning #BigData #DataScience #PredictiveAnalytics #ConsumerExperience #SalesOptimization #SCM #Havells #JobOpening #Hiring",
        "skills": [
            "object detection",
            "Random Forests",
            "R",
            "SGBoost",
            "Classification",
            "Large Language Models",
            "MS Databricks",
            "Regression",
            "object tracking",
            "Cnn",
            "Pyspark",
            "SAS",
            "Svm",
            "Nlp",
            "Spss",
            "Python",
            "Hadoop",
            "Neural Networks",
            "Clustering",
            "Sql",
            "Hive",
            "MLops",
            "Decision Trees",
            "Time Series",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Assistant Manager - Data Scientist",
        "company_name": "SBI Card",
        "experience": "Fresher",
        "salary": null,
        "location": "Gurugram, Gurugram, India",
        "industry": "Login to check your skill match score",
        "job_description": "About Us\n\nJOB DESCRIPTION\n\nSBI Card is a leading pure-play credit card issuer in India, offering a wide range of credit cards to cater to diverse customer needs. We are constantly innovating to meet the evolving financial needs of our customers, empowering them with digital currency for seamless payment experience and indulge in rewarding benefits. At SBI Card, the motto Make Life Simple inspires every initiative, ensuring that customer convenience is at the forefront of all that we do. We are committed to building an environment where people can thrive and create a better future for everyone.\n\nSBI Card is proud to be an equal opportunity & inclusive employer and welcome employees without any discrimination on the grounds of race, colour, gender, religion, creed, disability, sexual orientation, gender identity, marital status, caste etc. SBI Card is committed to fostering an inclusive and diverse workplace where all employees are treated equally with dignity and respect which makes it a promising place to work.\n\nJoin us to shape the future of digital payment in India and unlock your full potential.\n\nWhat's In It For YOU\n\nSBI Card truly lives by the work-life balance philosophy. We offer a robust wellness and wellbeing program to support mental and physical health of our employees\nAdmirable work deserves to be rewarded. We have a well-curated bouquet of rewards and recognition program for the employees\nDynamic, Inclusive and Diverse team culture\nGender Neutral Policy\nInclusive Health Benefits for all - Medical Insurance, Personal Accidental, Group Term Life Insurance and Annual Health Checkup, Dental and OPD benefits\nCommitment to the overall development of an employee through a comprehensive learning & development framework\n\nRole Purpose\n\nTo prepare business insights and data visualization dashboards, assist in the development of credit and fraud risk models, and perform credit and fraud decision engine deployment activities for the organization.\n\nRole Accountability\n\nExplore data and transform it into business insights by applying both data analytics and business knowledge\nProvide user-friendly visualization of data and business insights by creating dashboards leveraging any business intelligence (BI) software\nDevelop credit and fraud risk models by applying advanced AI/ML techniques.\nEnsure model deployment either in credit & fraud decision engines or turbine/data lake\nMonitor & validate all models/scorecards\nProcess Adherence as per MOU\n\nMeasures of Success\n\nEnsure the timely development of the models as per plan\nModel efficacy in terms of consistency & reliability\nModel validation & redevelopment as per agreed timelines\nEffective resolution of Business problems related to modeling or data visualization\n\nTechnical Skills / Experience / Certifications\n\nBasic knowledge of handling Big Data in Hadoop/Spark/AWS platforms\nBasic knowledge in AI/ML techniques like Random Forest, Gradient Boosting, Support Vector Machines, Neural Networks, etc.\nGood understanding of programming languages SQL, Python,\n\nCompetencies critical to the role\n\nStrong analytical and problem solving skills\n\nQualification\n\nGraduate / Master's degree in in any relevant field\n\nPreferred Industry\n\nFSI / Any",
        "skills": [
            "Ai",
            "Gradient Boosting",
            "Support Vector Machines",
            "Ml",
            "Hadoop",
            "Neural Networks",
            "Big Data",
            "Sql",
            "Random Forest",
            "Spark",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Associate Level 1 / Senior Associate - Data Scientist (Tabular & Text) - GM Data & AI Lab",
        "company_name": "BNP Paribas",
        "experience": "1-3 Years",
        "salary": null,
        "location": "Mumbai, India",
        "industry": "Login to check your skill match score",
        "job_description": "About BNP Paribas Group\n\nBNP Paribas is a top-ranking bank in Europe with an international profile. It operates in 71 countries and has almost 199 000 employees. The Group ranks highly in its three core areas of activity: Domestic Markets and International Financial Services (whose retail banking networks and financial services are grouped together under Retail Banking & Services) and Corporate & Institutional Banking, centred on corporate and institutional clients. The Group helps all of its clients (retail, associations, businesses, SMEs, large corporates and institutional) to implement their projects by providing them with services in financing, investment, savings and protection. In its Corporate & Institutional Banking and International Financial Services activities, BNP Paribas enjoys leading positions in Europe, a strong presence in the Americas and has a solid and fast-growing network in the Asia/Pacific region.\n\nAbout BNP Paribas India Solutions\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, a leading bank in Europe with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 6000 employees, to provide support and develop best-in-class solutions.\n\nAbout Business Line/Function\n\nGM Data & AI Lab leverages the power of Machine Learning and Deep learning to drive innovation in various business lines. Our primary goal is to harness the potential of vast amounts of structured and unstructured data to improve our services and provide value. Today we are a team of around 40+ Data Scientists based in Paris, London, Frankfurt, Lisbon, New York, Singapore and Mumbai.\n\nJob Title\n\nData Scientist (Tabular & Text)\n\nDate\n\nDepartment:\n\nFront Office Support\n\nLocation:\n\nMumbai\n\nBusiness Line / Function\n\nGlobal Markets Data & AI Lab\n\nReports To\n\n(Direct)\n\nGrade\n\n(if applicable)\n\n(Functional)\n\nNumber Of Direct Reports\n\nNA\n\nDirectorship / Registration\n\nNA\n\nPosition Purpose\n\nYour work will span across multiple areas, including predictive modelling, automation and process optimization. We use AI to discover patterns, classify information, and predict likelihoods. Our team works on building, refining, testing, and deploying these models to support various business use cases, ultimately driving business value and innovation.\n\nAs a Data Scientist on our team, you can expect to work on challenging projects, collaborate with stakeholders to identify business problems, and have the opportunity to learn and grow with our team. A typical day may involve working on model development, meeting with stakeholders to discuss project requirements/updates, and brainstorming/debugging with colleagues on various technical aspects.\n\nAt the Lab, we're passionate about staying at the forefront of AI research, bridging the gap between research & industry to drive innovation and to make a real impact on our businesses.\n\nResponsibilities\n\nDevelop and maintain AI models from inception to deployment, including data collection, analysis, feature engineering, model development, evaluation, and monitoring.\nIdentify areas for model improvement through independent research and analysis, and develop recommendations for updates and enhancements.\nWorking with expert colleagues and business representatives to examine the results and keep models grounded in reality.\nDocumenting each step of the development and informing decision makers by presenting them options and results.\nEnsure the integrity and security of data.\nProvide support for production models delivered by the Mumbai team but potentially as well for other models to any of the Asian/EU/US time zones.\n\nTechnical & Behavioral Competencies\n\nQualifications: Bachelors / Master / PhD degree in Computer Science / Data Science / Mathematics / Statistics / relevant STEM field.\nKnowledge of key concepts in Statistics and Mathematics such as Statistical methods for Machine learning, Probability Theory and Linear Algebra.\nExperience with Machine Learning & Deep Learning concepts including data representations, neural network architectures, custom loss functions.\nProven track record of building AI model from scratch or finetuning on large models for Tabular or/and Textual data.\nProgramming skillsin Pythonand knowledge of common numerical and machine-learning packages (like NumPy,scikit-learn, pandas,PyTorch, transformers, langchain).\nAbility to write clear and concise code in python.\nIntellectually curious and willing to learn challenging concepts daily.\nKnowledge of current Machine Learning/Artificial Intelligence literature.\n\nSkills Referential\n\nBehavioural Skills:\n\nAbility to collaborate / Teamwork\n\nCritical thinking\n\nCommunication skills - oral & written\n\nAttention to detail / rigor\n\nTransversal Skills\n\nAnalytical Ability\n\nEducation Level\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 1 year",
        "skills": [
            "langchain",
            "Transformers",
            "scikit-learn",
            "Numpy",
            "Machine Learning",
            "Pandas",
            "Pytorch",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "DATA SCIENTIST III",
        "company_name": "Walmart Global Tech India",
        "experience": "2-4 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "Position Summary...\n\nWhat you'll do...\n\nAbout Team\n\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart's product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\n\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\n\nWhat you'll do:\n\n\n\nAs a Data Scientist - ML Engineer, you'll have the opportunity to:\n\nDesign large scale AI/ML products/systems impacting millions of customers\n\nDevelop highly scalable, timely, highly-performant, instrumented and accurate data pipelines\n\nIdentify, develop, and deliverimprovements on data performance,dataquality, and cost, which needs to be monitored and analyzed\n\nEnable data governancepractices and processby being a passionate adopter and ambassador\n\nRun fine-tuning and optimization experiments on large language models and advanced machine learning algorithms\n\nBuild production quality AI/ML solutions for large scale data with low latency requirements\n\nCollaborate with multiple stakeholders to drive innovation at scale\n\nLeverage and contribute to generative AI advancements within the organization, fostering an environment of innovation and progress\n\nBuild a strong external presence through publishing your team's work in top-tier AI/ML conferences and developing partnerships with academic institutions\n\nAdhere to Walmart's policies, procedures, mission, values, standards of ethics and integrity\n\nAdopt to Walmart's quality standards, develop/recommend process standards and best practices across the retail industry\n\nWhat You'll Bring\n\nPhD with >1 years of relevant experience / 4-year bachelor's degree with > 4 years of experience / Master's degree with > 2 years of experience. Educational qualifications should be preferably in Computer Science or a strongly quantitative discipline.\n\nDevelopment and deployment of AI/ML models\n\nProven track record of delivering high-impact AI/ML solutions\n\nPast experience in programming skills across big data and ML engineering stack\n\nStrong communication skills with inclination to high ownership and commitment\n\nAble to refactordata science code andhas collaborated with data scientists in developing ML solutions.\n\nMandatory Skills: Machine Learning, Big Data Skills, Python, R\n\nAdditional Qualifications:Good to have experience inareas such ascomputer vision,forecasting, real-timeanalytics,conversationalAIassistants\n\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That's what we do at Walmart Global Tech. We're a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.\n\nFlexible, hybrid work\n\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\n\nBenefits\n\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\n\nBelonging\n\nWe aim to create a culture where every associate feels valued for who they are, rooted in respect for the individual. Our goal is to foster a sense of belonging, to create opportunities for all our associates, customers and suppliers, and to be a Walmart for everyone.\n\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, we're able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer\n\nWalmart, Inc., is an Equal Opportunities Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being inclusive of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics or related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field. Option 3: 4 years experience in an analytics or related field.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2127893",
        "skills": [
            "Generative AI",
            "R",
            "Statistics",
            "Optimization",
            "Machine Learning",
            "Big Data",
            "Python",
            "Deep Learning",
            "Computer Vision"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Red Hat",
        "experience": "7-9 Years",
        "salary": null,
        "location": "Bengaluru, India",
        "industry": "Login to check your skill match score",
        "job_description": "About The Job\n\nThe Data Development, Insights & Strategy team is a highly focused effort to use the combination of our product fleet data, our business and sales data to provide strategic insight and act on the moments that matter in near real time to both drive meaningful customer interactions, product evolution, modernize/drive our business systems and identifying opportunities to maximize our understanding and execution of product & business models which in turn maximize our ability to successfully capture our market opportunity.\n\nWe are seeking a highly experienced Senior Data Scientist to lead our initiatives in natural language processing (NLP), machine learning, and data analytics. The ideal candidate will be passionate about creating and developing applications, particularly within the language sector, and have excellent analytical and machine learning knowledge. The focus of the role will be to transform natural language data and requests into creative and interactive features part of our finance & operations business. If you want to build Next generation AI/ML applications, if you have rock-solid development skills, if you thrive in a start-up like environment, if you think outside of the box and love working in a team with similar minded engineers, if you are creative and want to help solve real world customer problems and love presenting innovative solutions, if you believe in community and OpenSource development culture then we are the TEAM for you.\n\nIn this role, you will be the voice of Red Hat's technical authority, creating and delivering features/ capabilities that helps Red Hat's data community and in turn our customers achieve their business and technical goals with Red Hat products effectively. You will work collaboratively with members of DDIS, finance, operations, sales, marketing, and product to construct fit for purpose analytical solutions that will be leveraged by business teams to drive proactive customer engagement.\n\nThe successful candidate will have a track record of strong technical skills, drive to be innovative & creative, excellent communication and collaboration skills, a keen attention to detail, and a passion for quality and open source software. If you enjoy broadening your technical skills, and working in an environment that thrives on creativity, experimentation, and community innovation, then this is the job for you!\n\nWhat will you do\n\nLead the research and implementation of advanced algorithms and tools for NLP/GenAI tasks.\nDrive the development of next-generation AI/ML applications in a highly collaborative environment.\nOversee the design, implementation, and delivery of AI solutions.\nEnsure the successful training and evaluation of NLP models, refining them based on statistical analysis.\nCollaborate with cross-functional teams, including finance, operations, sales, and marketing, to understand and meet business needs.\n\nWhat will you bring\n\n7+ years of professional experience in NLP, with a strong command of Python and frameworks such as Spacy and Hugging Face.\nProven expertise in designing and delivering NLP applications across all stages of the data science lifecycle.\nDeep understanding of machine learning frameworks and experience in Generative AI application development.\nWorking knowledge of TensorFlow, TensorFlow Serving, Keras, PyTorch.\nComfortable working with a small team in a fast-paced, highly collaborative environment\nExperience with GenAI application development - LLMs, Embedding models, Vector Databases\nKeep up to date in the rapidly changing field of AI/ML by constantly researching and learning new techniques and information related to NLP.\nBachelor's degree or above in Computer Science, Math, Computational Linguistics, or other related fields. Demonstrated experience as an NLP, or experience in a similar role/industry\nExperience developing highly scalable backend microservices in AWS\n\nPersonal qualities and communication\n\nCommunicate and influence for impact by bringing data to life via clear narratives and/or storytelling\nCommunication skills and experience in interacting with cross functional business and engineering teams\nCapability in undertaking business needs analysis in direct consultation.\nBe a self-starter, displaying initiative in seeing needs, building functionality and leading insights for organizational change.\nCapability to develop a detailed understanding of our business requirements.\nExcellent communication, presentation, and writing skills\n\nAbout Red Hat\n\nRed Hat is the world's leading provider of enterprise open source software solutions, using a community-powered approach to deliver high-performing Linux, cloud, container, and Kubernetes technologies. Spread across 40+ countries, our associates work flexibly across work environments, from in-office, to office-flex, to fully remote, depending on the requirements of their role. Red Hatters are encouraged to bring their best ideas, no matter their title or tenure. We're a leader in open source because of our open and inclusive environment. We hire creative, passionate people ready to contribute their ideas, help solve complex problems, and make an impact.\n\nInclusion at Red Hat\n\nRed Hat's culture is built on the open source principles of transparency, collaboration, and inclusion, where the best ideas can come from anywhere and anyone. When this is realized, it empowers people from different backgrounds, perspectives, and experiences to come together to share ideas, challenge the status quo, and drive innovation. Our aspiration is that everyone experiences this culture with equal opportunity and access, and that all voices are not only heard but also celebrated. We hope you will join our celebration, and we welcome and encourage applicants from all the beautiful dimensions that compose our global village.\n\nEqual Opportunity Policy (EEO)\n\nRed Hat is proud to be an equal opportunity workplace and an affirmative action employer. We review applications for employment without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, ancestry, citizenship, age, veteran status, genetic information, physical or mental disability, medical condition, marital status, or any other basis prohibited by law.\n\nRed Hat does not seek or accept unsolicited resumes or CVs from recruitment agencies. We are not responsible for, and will not pay, any fees, commissions, or any other payment related to unsolicited resumes or CVs except as required in a written contract between Red Hat and the recruitment agency or party requesting payment of a fee.\n\nRed Hat supports individuals with disabilities and provides reasonable accommodations to job applicants. If you need assistance completing our online job application, email [HIDDEN TEXT]. General inquiries, such as those regarding the status of a job application, will not receive a reply.",
        "skills": [
            "Generative AI",
            "TensorFlow Serving",
            "Hugging Face",
            "Vector Databases",
            "Embedding models",
            "Spacy",
            "Machine Learning",
            "Tensorflow",
            "Pytorch",
            "Keras",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Staff Data Scientist",
        "company_name": "Enphase Energy",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Energy",
        "job_description": "Strong understanding of industrial processes, sensor data, and IoT platforms, essential for building effective predictive maintenance models.\nExperience translating theoretical concepts into engineered features, with a demonstrated ability to create features capturing important events or transitions within the data.\nExpertise in crafting custom features that highlight unique patterns specific to the dataset or problem, enhancing model predictive power. Ability to combine and synthesize information from multiple data sources to develop more informative features.\nAdvanced knowledge in Apache Spark (PySpark, SparkSQL, SparkR) and distributed computing, demonstrated through efficient processing and analysis of large-scale datasets. Proficiency in Python, R, and SQL, with a proven track record of writing optimized and efficient Spark code for data processing and model training.\nHands-on experience with cloud-based machine learning platforms such as AWS SageMaker and Databricks, showcasing scalable model development and deployment.\nDemonstrated capability to develop and implement custom statistical algorithms tailored to specific anomaly detection tasks.\nProficiency in statistical methods for identifying patterns and trends in large datasets, essential for predictive maintenance. Demonstrated expertise in engineering features to highlight deviations or faults for early detection. Proven leadership in managing predictive maintenance projects from conception to deployment, with a successful track record of cross-functional team collaboration.\nExperience extracting temporal features, such as trends, seasonality, and lagged values, to improve model accuracy. Skills in filtering, smoothing, and transforming data for noise reduction and effective feature extraction.\nExperience optimizing code for performance in high-throughput, low-latency environments. Experience deploying models into production, with expertise in monitoring their performance and integrating them with CI/CD pipelines using AWS, Docker, or Kubernetes.\nFamiliarity with end-to-end analytical architectures, including data lakes, data warehouses, and real-time processing systems.\nExperience creating insightful dashboards and reports using tools such as Power BI, Tableau, or custom visualization frameworks to effectively communicate model results to stakeholders.\n6-8 years of experience in data science with a significant focus on predictive maintenance and anomaly detection.\nQualifications\nBachelor s or Master s degree/ Diploma in Engineering, Statistics, Mathematics or Computer Science\n6+ years of experience as a Data Scientist\nStrong problem-solving skills\nProven ability to work independently and accurately",
        "skills": [
            "Python/R/SQL",
            "anomaly detection",
            "Feature Engineering",
            "Predictive Maintenance",
            "AWS/Databricks",
            "Apache Spark"
        ]
    },
    {
        "job_title": "Senior Principal Data Scientist, CADD",
        "company_name": "Sandoz",
        "experience": "5-9 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Pharmaceutical",
        "job_description": "Your Responsibilities Include:\nDrive the design of medicinal chemistry efforts by applying in-depth knowledge of structure-activity relationships (SAR), a profound understanding of target biology, and predictive methods for assessing on- and off-target activity, physical properties, pharmacokinetics / pharmacodynamics (PK/PD), and synthetic feasibility.\nThrive at the intersection of experimental and groundbreaking digital technologies, with a particular emphasis on expertise in machine learning and artificial intelligence (AI) as applied to small molecule drug discovery.\nStay abreast of scientific literature and engage with internal and external scientists to incorporate biological insights into lead characterization and screening initiatives.\nCollaborate with interdisciplinary project teams to facilitate effective decision-making throughout the target identification to candidate nomination process. This involves applying and developing predictive models based on high-content and time-resolved screening data, including imaging techniques.\nDrive hypothesis generation to enhance clinical success rates for programs involving small molecules, peptides, RNAs, protein degradation, molecular glues, transient covalent inhibitors, and kinetic stabilization of drug-target complexes.\nTake a leading role in cross-disciplinary mechanistic studies using physics-based modeling and simulation, biophysical characterization, and cellular validation. These studies will inform the strategic targeting strategies of discovery projects, aiming for optimal mechanisms of action (MoAs).\nMinimum Requirements:\nAdvanced degree in medicinal chemistry, computational chemistry, computational biology, computational chemical biology, or a related field. Candidates with a laboratory-based background in chemistry and biology, supplemented with strong computational experience, are also encouraged to apply.\n5+ years of experience working with project teams in a drug discovery environment.\nProven track record of innovation through analogue design, leading to significant impact on discovery projects.\nFamiliarity with drug design tools and high-performance computing environments and strong publication history in peer-reviewed journals.\nSkills and Abilities:\nProactively anticipates project needs with a clinical focus.\nDemonstrates rigor and diligence in idea substantiation, analogue design, and experimentation.\nStrong team orientation with multitasking and adaptability in support and leadership roles.\nEffective listener with excellent written and oral communication skills.\nProficient in data visualization to effectively communicate insights.",
        "skills": [
            "Python (Programming Language)",
            "Apache Hadoop",
            "Data Science",
            "Data Management",
            "Data Visualization",
            "Big Data",
            "Machine Learning Algorithms",
            "Data Governance"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "ADP Private Limited",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Software",
        "job_description": "Minimum experience of 5+ years with very good proficiency in Python and libraries like Pandas, Numpy, Sklearn etc.,\nGood knowledge of ML models development using classification, regression and clustering.\nShould have excellent knowledge and experience of developing models using Deep learning (TensorFlow, Keras, PyTorch) and Neural Networks.\nGood knowledge of NLP and related frameworks.\nBuilding solutions with GenAI and various LLM models for specific use cases.\nExperience in data preparation, clean up techniques and also be good at analyzing data.\nAbility to interpret model validation using relevant metrics.\nFine tune and retrain existing models (BERT, NLP/Spacey) and manage the re-training pipeline.\nProficiency in data visualization frameworks such as Matplotlib or Seaborn.\nKnowledge of AWS cloud environment like ECS fargate, lambda and serverless.\nAbility to build and deploy models in AWS cloud environment, look into security aspects.\nCollaborate with Product Owners, business stakeholders and DevOps.\nFamiliar with agile scrum process.",
        "skills": [
            "Nlp",
            "Data Scientist",
            "Python"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PHOTON",
        "experience": "8-12 Years",
        "salary": null,
        "location": "Delhi, Mumbai, Pune",
        "industry": "Information Technology",
        "job_description": "Role Overview:\nWe are seeking aSenior Machine Learning Software Engineerto join our team and drive the development ofcutting-edge AI/ML solutions. You will play a key role indesigning, building, and optimizingourmachine learning infrastructurewhile collaborating withProduct Managers, Data Scientists, and Engineersto bring innovative models into production. The ideal candidate should have a strong background inML operations, software engineering, and automationto ensure a scalable and robust AI-driven ecosystem.\nKey Responsibilities:\nOperationalizeleadingdata and analytics technologieswhile following best practices.\nPartner withProduct Managersto understand customer requirements,design prototypes, and deploy solutions to production.\nDesign, build, harden, and maintainthe core infrastructure supporting theML platform and lifecycle.\nDefine and contribute to thetechnology strategy, roadmap, and prioritiesfor AI/ML initiatives.\nAutomate every aspect of theML infrastructuretominimize human interventionand improve efficiency.\nEncourage andimplement best practicesindata engineering and software development.\nWritehigh-quality, production-grade codethat aligns with company standards and contribute tonew standardization efforts.\nImplementautomated testingto ensure reliability and correctness of ML models and infrastructure.\nReview and mentorother engineers to promote high-quality engineering practices.\nActively participate intechnical communities-of-practiceandcontinuing education programsto advance personal and organizational growth.\nFacilitateproblem diagnosis and resolutionacrosstechnical and functionalareas.\nRequired Skills & Competencies:\n8+ yearsof experience insoftware engineering, machine learning, or data engineering.\nExpertise indesigning, coding, and scriptingto developreal-world AI/ML solutions.\nStrong programming skills inPython, Java, or Scalawith hands-on experience inML frameworks.\nExperience withML model deployment, MLOps, and automation.\nProficiency incloud-based ML solutions.\nKnowledge ofdata pipeline orchestration tools.\nStrong background indata engineering concepts.\nExperience working withcontainerization.\nStrong analytical and problem-solving skills with acollaborative mindset.\nExcellentcommunication and leadership skillswith the ability tomentor and guidejunior engineers.\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate",
        "skills": [
            "ML operations",
            "Predictive",
            "data pipeline orchestration tools",
            "Algorithms",
            "Machine Learning",
            "python",
            "Data Science",
            "data mining",
            "Predictive Modeling",
            "Software Engineering"
        ]
    },
    {
        "job_title": "Senior Data Scientist-",
        "company_name": "Mastercard",
        "experience": "4-7 Years",
        "salary": null,
        "location": "Navi Mumbai",
        "industry": "Financial Services",
        "job_description": "Role\nPlan and direct data science / machine learning projects within the team.\nDesign and implement machine learning models for a number of financial applications including but not limited to: Transaction Classification, Temporal Analysis, Risk modeling from structured and unstructured data.\nMeasure, validate, implement, monitor and improve performance of both internal and external facing machine learning models.\nPropose creative solutions to existing challenges that are new to the company, the financial industry and to data science.\nPresent technical problems and findings to business leaders internally and to clients succinctly and clearly.\nLeverage best practices in machine learning and data engineering to develop scalable solutions.\nIdentify areas where resources fall short of needs and provide thoughtful and sustainable solutions to benefit the team\nBe a strong, confident, and excellent writer and speaker, able to communicate your analysis, vision and roadmap effectively to a wide variety of stakeholders\nAll About You:\n4-7 years in data science/ machine learning model development and deployments\nExposure to financial transactional structured and unstructured data, transaction classification, risk evaluation and credit risk modeling is a plus.\nA strong understanding of NLP, Statistical Modeling, Visualization and advanced Data Science techniques/methods.\nGain insights from text, including non-language tokens and use the thought process of annotations in text analysis.\nSolve problems that are new to the company, the financial industry and to data science\nSQL / Database Experience Is Preferred\nExperience with Kubernetes, Containers, Docker, REST APIs, Event Streams or other delivery mechanisms.\nFamiliarity with relevant technologies (e.g. TensorFlow, Python, Sklearn, Pandas, etc.).\nStrong desire to collaborate and ability to come up with creative solutions.\nAdditional Finance And FinTech Experience Preferred.\nBachelor's or Master's Degree in Computer Science, Information Technology, Engineering, Mathematics, Statistics. M.S preferred",
        "skills": [
            "Risk Evaluation",
            "Credit Risk Models",
            "Data Science",
            "Machine Learning",
            "Nlp",
            "Python",
            "Tensorflow"
        ]
    },
    {
        "job_title": "Data Scientist/Senior Data Scientist - NLP GenAi",
        "company_name": "Fractal Analytics",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Noida, Mumbai, Pune",
        "industry": "Consulting",
        "job_description": "Responsibilities:\nDesign and implement advanced solutions utilizing Large Language Models (LLMs).\nDemonstrate self-driven initiative by taking ownership and creating end-to-end solutions.\nConduct research and stay informed about the latest developments in generative AI and LLMs.\nDevelop and maintain code libraries, tools, and frameworks to support generative AI development.\nParticipate in code reviews and contribute to maintaining high code quality standards.\nEngage in the entire software development lifecycle, from design and testing to deployment and maintenance.\nCollaborate closely with cross-functional teams to align messaging, contribute to roadmaps, and integrate software into different repositories for core system compatibility.\nPossess strong analytical and problem-solving skills.\nDemonstrate excellent communication skills and the ability to work effectively in a team environment.\nPrimary Skills:\nNatural Language Processing (NLP): Hands-on experience in use case classification, topic modeling, Q&A and chatbots, search, Document AI, summarization, and content generation.\nAND/OR\nComputer Vision and Audio: Hands-on experience in image classification, object detection, segmentation, image generation, audio, and video analysis.\nGenerative AI:\nProficiency with SaaS LLMs, including Lang chain, llama index, vector databases, Prompt engineering (COT, TOT, ReAct, agents). Experience with Azure OpenAI, Google Vertex AI, AWS Bedrock for text/audio/image/video modalities.\nFamiliarity with Open-source LLMs, including tools like TensorFlow/Pytorch and huggingface. Techniques such as quantization, LLM finetuning using PEFT, RLHF, data annotation workflow, and GPU utilization.\nCloud: Hands-on experience with cloud platforms such as Azure, AWS, and GCP. Cloud certification is preferred.\nApplication Development: Proficiency in Python, Docker, FastAPI/Django/Flask, and Git.",
        "skills": [
            "Gcp",
            "Docker",
            "FastAPI",
            "Azure",
            "Python",
            "AWS"
        ]
    },
    {
        "job_title": "Senior Data Scientist, Product Data & Analytics",
        "company_name": "Mastercard",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Gurugram",
        "industry": "Financial Services",
        "job_description": "Role\nResponsible for developing data-driven innovative scalable analytical solutions and identifying opportunities to support business and client needs in a quantitative manner and facilitate informed recommendations / decisions.\nAccountable for delivering high quality project solutions and tools within agreed upon timelines and budget parameters and conducting post- implementation reviews.\nContributes to the development of custom analyses and solutions, derives insights from extracted data to solve critical business questions.\nActivities include developing and creating predictive models, behavioural segmentation frameworks, profitability analyses, ad hoc reporting, and data visualizations.\nAble to develop AI/ML capabilities, as needed on large volumes of data to support analytics and reporting needs across products, markets and services.\nAble to build end to end reusable, multi-purpose AI models to drive automated insights and recommendations.\nLeverage open and closed source technologies to solve business problems.\nWork closely with global & regional teams to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support analytics and reporting needs across products, markets, and services. Support initiatives in developing predictive models, behavioural segmentation frameworks, profitability analyses, ad hoc reporting, and data visualizations.\nTranslates client/ stakeholder needs into technical analyses and/or custom solutions in collaboration with internal and external partners, derive insights and present findings and outcomes to clients/stakeholders to solve critical business questions.\nCreate repeatable processes to support development of modelling and reporting\nDelegate and reviews work for junior level colleagues to ensure downstream applications and tools are not compromised or delayed.\nServes as a mentor for junior-level colleagues, and develops talent via ongoing technical training, peer review etc.\nAll About You\n6-8 years of experience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis.\nAdvanced SQL skills, ability to write optimized queries for large data sets.\nExperience on Platforms/Environments: Cloudera Hadoop, Big data technology stack, SQL Server, Microsoft BI Stack, Cloud, Snowflake, and other relevant technologies.\nData visualization tools (Tableau, Domo, and/or Power BI/similar tools) experience is a plus\nExperience with data validation, quality control and cleansing processes to new and existing data sources.\nExperience on Classical and Deep Machine Learning Algorithms like Logistic Regression, Decision trees, Clustering (K-means, Hierarchical and Self-organizing Maps), TSNE, PCA, Bayesian models, Time Series ARIMA/ARMA, Random Forest, GBM, KNN, SVM, Bayesian, Text Mining techniques, Multilayer Perceptron, Neural Networks - Feedforward, CNN, NLP, etc.\nExperience on Deep Learning algorithm techniques, open-source tools and technologies, statistical tools, and programming environments such as Python, R, and Big Data platforms such as Hadoop, Hive, Spark, GPU Clusters for deep learning.\nExperience in automating and creating data pipeline via tools such as Alteryx, SSIS. Nifi is a plus\nFinancial Institution or a Payments experience a plus\nAdditional Competencies\nExcellent English, quantitative, technical, and communication (oral/written) skills.\nOwnership of end-to-end Project Delivery/Risk Mitigation\nVirtual team management and manage stakeholders by influence\nAnalytical/Problem Solving\nAble to prioritize and perform multiple tasks simultaneously\nAble to work across varying time zone.\nStrong attention to detail and quality\nCreativity/Innovation\nSelf-motivated, operates with a sense of urgency.\nIn depth technical knowledge, drive, and ability to learn new technologies.\nMust be able to interact with management, internal stakeholders",
        "skills": [
            "Data Management",
            "Advanced Sql",
            "Tableau",
            "Microsoft Bi",
            "Data Science",
            "product data management"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "Fractal Analytics",
        "experience": "16-20 Years",
        "salary": null,
        "location": "Noida, Mumbai, Pune",
        "industry": "Consulting",
        "job_description": "Responsibilities:\nBuild Solutions that identify intent and entities, other features from user comments, chat transcript and other unstructured text data.\nDesign and implement advanced solutions utilizing NLP traditional Models and Large Language Models (LLMs).\nDemonstrate self-driven initiative by taking ownership and creating end-to-end solutions.\nConduct research and stay informed about the latest developments in generative AI and LLMs.\nTrain and fine-tune pre-trained NLP models and run evaluation experiments.\nPerform statistical analysis of results and refine models.\nBasic logical pseudo code writing.\nDevelop and maintain code libraries, tools, and frameworks to support generative AI development.\nParticipate in code reviews and contribute to maintaining high code quality standards.\nEngage in the entire software development lifecycle, from design and testing to deployment and maintenance.\nCollaborate closely with cross-functional teams to align messaging, contribute to roadmaps, and integrate software into different repositories for core system compatibility.\nPossess strong analytical and problem-solving skills.\nDemonstrate excellent communication skills and the ability to work effectively in a team environment.\nPrimary skills:\nNatural Language Processing (NLP): Hands-on experience in use case classification, topic modeling, QA and chatbots, search, Document AI, summarization, and content generation.\nAND/OR\nComputer Vision and Audio: Hands-on experience in image classification, object detection, segmentation, image generation, audio, and video analysis.\nGenerative AI:\nProficiency with SaaS LLMs, including Lang chain, llama index, vector databases, Prompt engineering (COT, TOT, ReAct, agents). Experience with Azure OpenAI, Google Vertex AI, AWS Bedrock for text/audio/image/video modalities.\nFamiliarity with Open-source LLMs, including tools like TensorFlow/Pytorch and huggingface. Techniques such as quantization, LLM finetuning using PEFT, RLHF, data annotation workflow, and GPU utilization.\nCloud: Hands-on experience with cloud platforms such as Azure, AWS, and GCP. Cloud certification is preferred.\nApplication Development: Proficiency in Python, Docker, FastAPI/Django/Flask, and Git\nDomain Skill: Consulting experience with FS,Banking domain for at least 3 years.\nRequired qualifications:\n16-20 years into Deep Learning framewokrs like Keras, Pytorch, TensorFlow\nExperience of design, building and deployment of ML/NLP Solutions\nProficient in Python SQL\nExperience in NLP tools like Genism, spacy, Stanford NLP, HuggingFace\nMust have excellent project/program management skills and have experience managing multiple work streams and projects at one time\nHave business acumen to manage revenues profitably and meet financial goals consistently. Able to quantify business value for clients and create win-win commercial propositions.\nPreferred qualifications:\nExperience automating data within Tableau/Qlik/ggplot/Shiny to tell a story through interactive visualizations\nClient relationship management: Build deep client relationship, network be a thought partner. Anticipate business problems deliver par excellence.\nSales Support account growth: Actively focus on opportunities to grow the client along with the senior engagement manager. Support the sales team as required for RFPs and regular sales pitches\nFirm building: Contribute to firm growth by participating and conducting training sessions.\nCoaching grooming: Coach groom the team on gaining knowledge skills on first principles of analytics techniques, problem-solving, project management, client relationship management\nEDUCATION:\nB.E/B.Tech/M.Tech in Computer Science or related technical degree OR Equivalent",
        "skills": [
            "Nlp",
            "Machine Learning",
            "Azure",
            "Python"
        ]
    },
    {
        "job_title": "Principal/Lead Data Scientist",
        "company_name": "Fractal Analytics",
        "experience": "11-15 Years",
        "salary": null,
        "location": "Chennai, Mumbai, Pune",
        "industry": "Consulting",
        "job_description": "Responsibilities:\nLeading AI solution end - to - end in given client projects, including AI solution design, execution and client and team management\nLeading project teams through the solution development, validation and, deployment, monitoring ensuring deliverable quality and timeliness\nSolving internal and client deadlocks ensuring alignments on key technical aspects of the AI solution\nPartnering with Sales teams during new business and renewal business development conversations to ensure new and incremental business\nManage reporting team of data scientists and Sr. Data scientists providing career guidance for growth, resolving issues and ensuring high utilization in client projects\nProviding regular updates to executive team on client projects being handled, calling out project status, risks, issues, resolutions from a technical pov\nCollaborate with other functional teams such as Engineering, MLOps, DevOps, etc. as required in the client projects ensuring clear and effective communications enabling integration of AI solutions into client production systems.\nCollaborate with internal and client stakeholders to understand business requirements and translate them into AI solutions.\nEnsure optimizations in code and model using best-in-class techniques in ML and Deep learning\nLead process of extensively clean, preprocess and transform to raw datasets to ensure its quality and suitability for model training.\nImplement best practices throughout the design and execution of AI solution\nQualifications:\nBachelors or Masters degree in Data Science, Computer Science or a related field.\nMinimum 11-15 years of experience in AI and Machine Learning\nStrong proficiency in Python, PySpark, cloud platforms such as AWS, Azure and GCP\nStrong proficiency in machine learning, deep learning, supervised/unsupervised learning algorithms and their execution\nGood to have experience and understanding of Generative AI concepts\nPrevious experience in leading data science projects is must\nPrevious experience in new/ renewal business development is must\nPrevious experience in client and internal team management is must\nHands-on experience with model training, hyperparameter tuning, and deployment.\nExcellent problem-solving skills and attention to detail.\nStrong communication and teamwork skills",
        "skills": [
            "Pyspark",
            "Azure",
            "Python",
            "Aws"
        ]
    },
    {
        "job_title": "Staff, Data Scientist",
        "company_name": "Warner Bros. Discovery",
        "experience": "2-6 Years",
        "salary": null,
        "location": "Hyderabad",
        "industry": "Media and Entertainment",
        "job_description": "As a Staff Data Scientist, you will play a critical role in advancing data-driven solutions to complex business challenges, influencing data strategy efforts for WB D Businesses . The responsibilities include\nAnalyze complex, high volumes of data from various sources using various tools and data analytics techniques.\nPartner with stakeholders to understand business questions and provide answers using the most appropriate mathematical techniques .\nModel Development and Implementation Design, develop, and implement statistical models , predictive models and machine learning algorithms that inform strategic decisions across various business units.\nExploratory Data Analysis Utilize exploratory data analysis techniques to identify and investigate new opportunities through innovative analytical and engineering methods.\nAdvanced Analytics Solutions Collaborate with Product and Business stakeholders to understand business challenges and develop sophisticated analytical solutions.\nData Automation Advance automation initiatives that reduce the time spent on data preparation, enabling more focus on strategic analysis.\nInnovative Frameworks Construction Develop and enhance frameworks that improve productivity and are intuitive for adoption across other data teams and be abreast with innovative machine learning techniques (e.g., deep learning, reinforcement learning, ensemble methods) and emerging AI technologies to stay ahead of industry trends.\nCollaborate with data engineering teams to architect and scale robust, efficient data pipelines capable of handling large, complex datasets, ensuring the smooth and automated flow of data from raw collection to insights generation.\nD eployment of machine learning models into production environments, collaborating with DevOps and engineering teams to ensure smooth integration and scalability .\nQuality Assurance Implement robust systems to detect, alert, and rectify data anomalies.\nQualifications & Experiences\nBachelors degree, MS, or greater in Computer/Data Science, Engineering, Mathematics, Statistics, or related quantitative discipline.\n8 + years relevant experience in Data Science .\nExpertise in a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, random forests, deep learning etc.) and experience with applications of these techniques .\nExpertise in advanced statistical techniques and concepts (regressions, statistical tests etc.) and experience with application of these tools.\nA demonstrated track record of utilizing data science to solve business problems in a professional environment .\nExpertise in SQL and either Python or R, including experience with application deployment packages like R Streamlit or Shiny.\nExperience with database technologies such as Databricks, Snowflake, and others.\nFamiliarity with BI tools (Power BI, Looker, Tableau) and experience managing workflows in an Agile environment.\nStrong analytical and problem-solving abilities.\nExcellent communication skills to effectively convey complex data-driven insights to stakeholders.\nHigh attention to detail and capability to work independently in managing multiple priorities under tight deadlines.\nProficiency in big data technologies (e.g., Spark, Kafka, Hive).\nExperience working in a cloud environment (AWS, Azure, GCP) to facilitate data solutions.\nAbility to collaborate effectively with business partners and develop and maintain productive professional relationships.\nExperience with adhering to established data management practices and standards.\nAbility to communicate to all levels of business, prioritize and manage assignments to meet deadlines and establish strong relationships.\nInterest in movies, games, and comics is a plus .",
        "skills": [
            "Data Science",
            "R Programming",
            "Spark",
            "Databricks",
            "Azure",
            "Python",
            "Sql",
            "Aws"
        ]
    },
    {
        "job_title": "Principal/Lead Data Scientist",
        "company_name": "Fractal Analytics",
        "experience": "11-15 Years",
        "salary": null,
        "location": "Gurugram, Bengaluru",
        "industry": "Consulting",
        "job_description": "Responsibilities:\nLeading AI solution end - to - end in given client projects, including AI solution design, execution and client and team management\nLeading project teams through the solution development, validation and, deployment, monitoring ensuring deliverable quality and timeliness\nSolving internal and client deadlocks ensuring alignments on key technical aspects of the AI solution\nPartnering with Sales teams during new business and renewal business development conversations to ensure new and incremental business\nManage reporting team of data scientists and Sr. Data scientists providing career guidance for growth, resolving issues and ensuring high utilization in client projects\nProviding regular updates to executive team on client projects being handled, calling out project status, risks, issues, resolutions from a technical pov\nCollaborate with other functional teams such as Engineering, MLOps, DevOps, etc. as required in the client projects ensuring clear and effective communications enabling integration of AI solutions into client production systems.\nCollaborate with internal and client stakeholders to understand business requirements and translate them into AI solutions.\nEnsure optimizations in code and model using best-in-class techniques in ML and Deep learning\nLead process of extensively clean, preprocess and transform to raw datasets to ensure its quality and suitability for model training.\nImplement best practices throughout the design and execution of AI solution\nQualifications:\nBachelors or Masters degree in Data Science, Computer Science or a related field.\nMinimum 11-15 years of experience in AI and Machine Learning\nStrong proficiency in Python, PySpark, cloud platforms such as AWS, Azure and GCP\nStrong proficiency in machine learning, deep learning, supervised/unsupervised learning algorithms and their execution\nGood to have experience and understanding of Generative AI concepts\nPrevious experience in leading data science projects is must\nPrevious experience in new/ renewal business development is must\nPrevious experience in client and internal team management is must\nHands-on experience with model training, hyperparameter tuning, and deployment.\nExcellent problem-solving skills and attention to detail.\nStrong communication and teamwork skills",
        "skills": [
            "Pyspark",
            "Azure",
            "Python",
            "Aws"
        ]
    },
    {
        "job_title": "Principal Data Scientist",
        "company_name": "GENERAL ELECTRIC (GE)",
        "experience": "9-16 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Machinery Manufacturing",
        "job_description": "Key Deliverables:\nLead development and implementation of computer vision algorithms for rail component detection and measurement\nOptimize algorithms for high-speed, real-time performance\nCollaborate across teams to integrate computer vision solutions into products\nConduct research to stay updated with advancements in computer vision and deep learning\nRole Responsibilities:\nDevelop and optimize computer vision algorithms using Python/C++ and deep learning techniques\nProvide technical guidance to a team of data scientists and machine learning engineers\nDesign and implement synthetic data generation and AI annotation pipelines\nWork with cloud platforms (GCP/Azure/AWS) and containerization technologies (Docker/Kubernetes)",
        "skills": [
            "Tensorflow",
            "C++",
            "Opencv",
            "Python",
            "MATLAB"
        ]
    },
    {
        "job_title": "Staff Data Scientist",
        "company_name": "Enphase Energy",
        "experience": "6-8 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Energy",
        "job_description": "Strong understanding of industrial processes, sensor data, and IoT platforms, essential for building effective predictive maintenance models.\nExperience translating theoretical concepts into engineered features, with a demonstrated ability to create features capturing important events or transitions within the data.\nExpertise in crafting custom features that highlight unique patterns specific to the dataset or problem, enhancing model predictive power. Ability to combine and synthesize information from multiple data sources to develop more informative features.\nAdvanced knowledge in Apache Spark (PySpark, SparkSQL, SparkR) and distributed computing, demonstrated through efficient processing and analysis of large-scale datasets. Proficiency in Python, R, and SQL, with a proven track record of writing optimized and efficient Spark code for data processing and model training.\nHands-on experience with cloud-based machine learning platforms such as AWS SageMaker and Databricks, showcasing scalable model development and deployment.\nDemonstrated capability to develop and implement custom statistical algorithms tailored to specific anomaly detection tasks.\nProficiency in statistical methods for identifying patterns and trends in large datasets, essential for predictive maintenance. Demonstrated expertise in engineering features to highlight deviations or faults for early detection. Proven leadership in managing predictive maintenance projects from conception to deployment, with a successful track record of cross-functional team collaboration.\nExperience extracting temporal features, such as trends, seasonality, and lagged values, to improve model accuracy. Skills in filtering, smoothing, and transforming data for noise reduction and effective feature extraction.\nExperience optimizing code for performance in high-throughput, low-latency environments. Experience deploying models into production, with expertise in monitoring their performance and integrating them with CI/CD pipelines using AWS, Docker, or Kubernetes.\nFamiliarity with end-to-end analytical architectures, including data lakes, data warehouses, and real-time processing systems.\nExperience creating insightful dashboards and reports using tools such as Power BI, Tableau, or custom visualization frameworks to effectively communicate model results to stakeholders.\n6-8 years of experience in data science with a significant focus on predictive maintenance and anomaly detection.\nQualifications\nBachelor s or Master s degree/ Diploma in Engineering, Statistics, Mathematics or Computer Science\n6+ years of experience as a Data Scientist\nStrong problem-solving skills\nProven ability to work independently and accurately",
        "skills": [
            "Python/R/SQL",
            "anomaly detection",
            "Feature Engineering",
            "Predictive Maintenance",
            "AWS/Databricks",
            "Apache Spark"
        ]
    },
    {
        "job_title": "Senior Data Scientist",
        "company_name": "Vinirma Consulting Private Limited",
        "experience": "7-15 Years",
        "salary": null,
        "location": "United Arab Emirates",
        "industry": "Login to check your skill match score",
        "job_description": "Nair Systems,is a DIFC based, boutique leading edge technology consulting and professional services firm focusing on Digital Banking, Artificial Intelligence, Big Data & Data Analytics, Cyber Security, Blockchain and Cryptocurrency and Robotic Process Automation.\nNair Systemsis currently looking forSenior Data Scientist for ourUAE operations with the following Skill set and terms & conditions.\nSpecialist Skills / Technical Knowledge Required for this role:\nAdvanced knowledge of statistical and data mining techniques (regression, decision trees, clustering, neural networks, etc.)\nKnowledge of additional programming languages is a plus (Python, C++, Java)\nIntellectual curiosity, along with excellent problem-solving and quantitative skills, including the ability to disaggregate issues, identify root causes and recommend solutions\nProven leaders with the ability in inspire others, build strong relationships, and create a true followership, result-driven achievers\nDistinctive communications skills and ability to communicate analytical and technical content in an easy to understand way\nPrevious experience required (if any)\nBachelor's degree in quantitative field like Computer Science, Engineering, Statistics, Mathematics or related field required. Advanced degree is a strong plus\n7-8 years of hands-on mathematical modelling experience in business environment\nProven experience in working with large datasets and relational databases (SQL)Strong knowledge of one or more data science or decision science domains (e.g. [un)]supervised learning, explainable artificial intelligence, econometrics, deep learning, Natural Language Processing, time series forecasting, deployment, causal inference, uplift modelling, and/or optimization)\nAbility to describe analytic processes from start to finish in your area of expertise\nKey Accountabilities of the roleResults Required\nWork with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.\nMine and analyse data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies.\nAssess the effectiveness and accuracy of new data sources and data gathering techniques.\nDevelop custom data models and algorithms to apply to data sets.\nUse predictive modelling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.\nDevelop company A/B testing framework and test model quality.\nCoordinate with different functional teams to implement models and monitor outcomes.\nDevelop processes and tools to monitor and analyse model performance and data accuracy.\nJoining time frame:2 weeks (maximum 1 month)\nShould yoube interested in this opportunity, please send your latest resume at [HIDDEN TEXT]",
        "skills": [
            "Java",
            "C++",
            "Data Scientist",
            "Python",
            "Sql Server2008"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "PHOTON",
        "experience": "8-12 Years",
        "salary": null,
        "location": "Hyderabad, Bengaluru, Chennai",
        "industry": "Information Technology",
        "job_description": "Role Overview:\nWe are seeking aSenior Machine Learning Software Engineerto join our team and drive the development ofcutting-edge AI/ML solutions. You will play a key role indesigning, building, and optimizingourmachine learning infrastructurewhile collaborating withProduct Managers, Data Scientists, and Engineersto bring innovative models into production. The ideal candidate should have a strong background inML operations, software engineering, and automationto ensure a scalable and robust AI-driven ecosystem.\nKey Responsibilities:\nOperationalizeleadingdata and analytics technologieswhile following best practices.\nPartner withProduct Managersto understand customer requirements,design prototypes, and deploy solutions to production.\nDesign, build, harden, and maintainthe core infrastructure supporting theML platform and lifecycle.\nDefine and contribute to thetechnology strategy, roadmap, and prioritiesfor AI/ML initiatives.\nAutomate every aspect of theML infrastructuretominimize human interventionand improve efficiency.\nEncourage andimplement best practicesindata engineering and software development.\nWritehigh-quality, production-grade codethat aligns with company standards and contribute tonew standardization efforts.\nImplementautomated testingto ensure reliability and correctness of ML models and infrastructure.\nReview and mentorother engineers to promote high-quality engineering practices.\nActively participate intechnical communities-of-practiceandcontinuing education programsto advance personal and organizational growth.\nFacilitateproblem diagnosis and resolutionacrosstechnical and functionalareas.\nRequired Skills & Competencies:\n8+ yearsof experience insoftware engineering, machine learning, or data engineering.\nExpertise indesigning, coding, and scriptingto developreal-world AI/ML solutions.\nStrong programming skills inPython, Java, or Scalawith hands-on experience inML frameworks.\nExperience withML model deployment, MLOps, and automation.\nProficiency incloud-based ML solutions.\nKnowledge ofdata pipeline orchestration tools.\nStrong background indata engineering concepts.\nExperience working withcontainerization.\nStrong analytical and problem-solving skills with acollaborative mindset.\nExcellentcommunication and leadership skillswith the ability tomentor and guidejunior engineers.\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate",
        "skills": [
            "Predictive",
            "ML operations",
            "data pipeline orchestration tools",
            "Algorithms",
            "Machine Learning",
            "python",
            "Data Science",
            "data mining",
            "Predictive Modeling",
            "Software Engineering"
        ]
    },
    {
        "job_title": "Data Scientist",
        "company_name": "BNY Mellon International Operations India Private Limited",
        "experience": "4-6 Years",
        "salary": null,
        "location": "Pune",
        "industry": "Financial Services",
        "job_description": "Description\nWe are seeking an experienced Data Scientist to join our team. The ideal candidate will have a strong background in data analysis, machine learning, and statistics, with 4-6 years of experience in the job market context of India.\nResponsibilities\nDesign and develop machine learning models for predictive and prescriptive analytics.\nAnalyze large, complex datasets using statistical and machine learning techniques.\nCollaborate with cross-functional teams to identify business problems and develop data-driven solutions.\nCommunicate findings and insights to stakeholders in a clear and concise manner.\nContinuously monitor and evaluate model performance, making adjustments as needed.\nStay up-to-date with the latest developments in machine learning, data science, and analytics.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, or related field.\n4-6 years of experience in data science, machine learning, or a related field.\nStrong proficiency in Python, R, or another programming language commonly used in data science.\nExperience with machine learning libraries such as scikit-learn, TensorFlow, or PyTorch.\nProficiency in SQL and working with relational databases.\nSolid understanding of statistics and probability theory.\nExcellent communication and collaboration skills.\nAbility to work independently and manage multiple projects simultaneously.\nExperience with big data technologies such as Hadoop, Spark, or Kafka is a plus.",
        "skills": [
            "Machine Learning",
            "Data Visualization",
            "Big Data",
            "Python",
            "Deep Learning"
        ]
    },
    {
        "job_title": "Senior Data Scientist - Conversational AI",
        "company_name": "Ringcentral",
        "experience": "3-6 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Information Technology",
        "job_description": "Say hello to possibilities.\nIt s not everyday that you consider starting a new career challenge.\nWe re RingCentral, a global leader in cloud-based communications and collaboration software. We are fundamentally changing the nature of human interaction giving people the freedom to connect powerfully and personally from anywhere, at any time, on any device.\nWe re a $2 billion company that s growing at 30+% annually and we re expanding our Solutions Engineering Team to make sure we stay ahead of the competition.\nJob Title Sr Data Scientist - Conversational AI\nWe are seeking an experienced Sr Data Scientist with a minimum of 4 years of experience in NLP, who is passionate about exploring and utilizing the latest developments in NLP space. The ideal candidate will have proficiency in Python, Pytorch, transformers, and ONNX, with experience in training and creating inference pipelines.\nResponsibilities\nUtilize latest NLP techniques to develop and improve natural language understanding models\nDevelop and deploy Multilingual NLP models in production environment\nCollaborate with cross-functional teams to identify and solve business problems using NLP techniques\nEvaluate and optimize model performance through experimentation and analysis\nDevelop low latency inference pipelines by doing model optimisations\nDevelop data labeling and augmentation techniques to improve model performance\nStay up-to-date with the latest advancements in the field of NLP and share your knowledge with the team.\nRequirements\nMinimum of 4+ years of experience in NLP\nProficiency in Python, PyTorch, transformers, and ONNX\nExperience in developing and deploying NLP models (including LLMs) in production environment\nPrior experience in working on Large Language Models and Prompt Engineering\nExperience in fine tuning of LLMs\nExperience in creating inference pipelines by doing model optimisations (quantization, pruning etc.)\nStrong understanding of NLP concepts such as summarization, auto-labeling, and augmentation techniques\nStrong analytical and problem-solving skills\nExcellent communication and teamwork skills\nBachelors or Masters degree in Computer Science or a related field.\nIf you are a creative problem solver with a passion for NLP, please apply with your resume and a cover letter explaining why you are the best candidate for this position. We look forward to hearing from you!\nWhat we offer\nMediclaim Benefits\nPaid Holidays\nCasual/Sick Leave\nPrivilege Leave\nCaRing Days\nBereavement Leave\nMaternity Leave\nPaternity Leave\nWellness Coaching\nEmployee Referral Bonus\nProfessional Development Allowances\nAbout RingCentral\nRingCentral, Inc. (NYSE RNG) is a leading provider of business cloud communications and contact center solutions based on its powerful Message Video Phone (MVP ) global platform. More flexible and cost effective than legacy on-premises PBX and video conferencing systems that it replaces, RingCentral empowers modern mobile and distributed workforces to communicate, collaborate, and connect via any mode, any device, and any location. RingCentral is headquartered in Belmont, California, and has offices around the world.\nRingCentral is an equal opportunity employer that truly values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\nRole:Data Scientist\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate",
        "skills": [
            "Cost",
            "Analytical",
            "Python",
            "Cloud"
        ]
    },
    {
        "job_title": "Gen AI Data Scientist",
        "company_name": "Kyndryl Solutions Private Limited",
        "experience": "5-10 Years",
        "salary": null,
        "location": "Bengaluru",
        "industry": "Software Engineering",
        "job_description": "The Role\nAs a Data Scientist at Kyndryl you are the bridge between business problems and innovative solutions, using a powerful blend of well-defined methodologies, statistics, mathematics, domain expertise, consulting, and software engineering. You'll wear many hats, and each day will present a new puzzle to solve, a new challenge to conquer.\nYou will dive deep into the heart of our business, understanding its objectives and requirements viewing them through the lens of business acumen, and converting this knowledge into a data problem. You'll collect and explore data, seeking underlying patterns and initial insights that will guide the creation of hypotheses.\nIn this role, you will embark on a transformative process of business understanding, data understanding, and data preparation. Utilizing statistical and mathematical modeling techniques, you'll have the opportunity to create models that defy convention models that hold the key to solving intricate business challenges. With an acute eye for accuracy and generalization, you'll evaluate these models to ensure they not only solve business problems but do so optimally.\nAdditionally, you're not just building and validating models you're deploying them as code to applications and processes, ensuring that the model(s) you've selected sustains its business value throughout its lifecycle.\nYour expertise doesn't stop at data; you'll become intimately familiar with our business processes and have the ability to navigate their complexities, identifying issues and crafting solutions that drive meaningful change in these domains. You will develop and apply standards and policies that protect our organization's most valuable asset ensuring that data is secure, private, accurate, available, and, most importantly, usable. Your mastery extends to data management, migration, strategy, change management, and policy and regulation.\nIf you're ready to embrace the power of data to transform our business and embark on an epic data adventure, then join us at Kyndryl. Together, let's redefine what's possible and unleash your potential.\nYour Future at Kyndryl\nEvery position at Kyndryl offers a way forward to grow your career. We have opportunities that you won't find anywhere else, including hands-on experience, learning opportunities, and the chance to certify in all four major platforms. Whether you want to broaden your knowledge base or narrow your scope and specialize in a specific sector, you can find your opportunity here.\nWho You Are\nWho You Are\nYou're good at what you do and possess the required experience to prove it. However, equally as important you have a growth mindset; keen to drive your own personal and professional development. You are customer-focused someone who prioritizes customer success in their work. And finally, you're open and borderless naturally inclusive in how you work with others.\nRequired Skills and Experience\n5+ years of AI and Data consulting experience, or other relevant experience in AI & Analytics domain, with a proven track record of building and maintaining client relationships\nCollaborate with customers and account partners to identify new AI opportunities, and build proposals and pitch materials to position Kyndryl as a trusted partner for AI & Data transformation\nOrganize and lead educational and ideation AI and Generative AI workshops for customers\nUnderstand customer's needs and assess their AI maturity\nEvaluate and recommend appropriate AI frameworks, tools and platforms\nLead AI feasibility studies and PoC projects to demonstrate the value and viability of AI solutions\nCollaborate with AI architects and engineers to design and develop AI and Generative AI solutions tailored to the clients requirements\nLead integration and deployment of AI solutions into customers existing infrastructure\nStay updated with the latest advancements in AI and Generative AI technologies, trends and best practices and develop thought leadership and PoV on AI\nWork with cross-functional team and partners to develop/ enhance and package AI offerings and assets for customer-facing discussions\nIn-depth understanding of AI technologies, including machine learning, deep learning, and generative AI techniques (such as generative adversial networks (GANs), variational autoencoders (VAEs), etc.)\nGood knowledge of AI frameworks, tools, and platform such as TensorFlow, PyTorch\nExperience working with customers in different industries and understanding their specific challenges and requirements\nPreferred Skills and Experience\nBachelor's degree in computer science, Information Security, or a related field\nSkilled in planning, organization, analytics, and problem-solving.\nExcellent communication and interpersonal skills to work collaboratively with clients and team members.\nComfortable working with statistics.\nAdvanced skills in PowerPoint and Excel.\nStrong Business acumen and understanding of industry trends and challenges in AI and Generative AI .\nStrong leadership skills and proactive and self-driven attitude.\nAdvanced communication and storytelling skills.\nHighly adaptable and flexible through shifting priorities.",
        "skills": [
            "Gen AI",
            "Gen AI Data Scientist"
        ]
    }
]