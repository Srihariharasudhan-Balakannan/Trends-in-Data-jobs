job_title,company_name,experience,salary,location,industry,job_description,skills
Asst./Dy Manager - Data engineer,Ajanta Pharma,5-7 Years,,Mumbai,"Manufacturing, Pharmaceutical","Ajanta Pharma is looking for Asst./Dy Manager - Data engineer to join our dynamic team and embark on a rewarding career journey
Liaising with coworkers and clients to elucidate the requirements for each task
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed
Reformulating existing frameworks to optimize their functioning
Testing such structures to ensure that they are fit for use
Preparing raw data for manipulation by data scientists
Detecting and correcting errors in your work
Ensuring that your work remains backed up and readily accessible to relevant coworkers
Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs","NoSQL Databases, Cloud Computing, Etl Tools, Data Modeling, Data Warehousing, Sql, Python"
Consultant - Sr.Data Engineer (DBT+Snowflake),Genpact,Fresher,,Bengaluru,IT/Computers - Hardware & Networking,"Ready to shape the future of work
At Genpact, we don&rsquot just adapt to change&mdashwe drive it. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.
If you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that&rsquos shaping the future, this is your moment.
Genpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.
Inviting applications for the role ofConsultant -Sr.Data Engineer (DBT+Snowflake)
!
In this role, the Sr.Data Engineer is responsible for providing technical direction and lead a group of one or more developer to address a goal.
Job Description:
Develop, implement, and optimize data pipelines using Snowflake, with a focus on Cortex AI capabilities.
Extract, transform, and load (ETL) data from various sources into Snowflake, ensuring data integrity and accuracy.
Implement Conversational AI solutions using Snowflake Cortex AI to facilitate data interaction through ChatBot agents.
Collaborate with data scientists and AI developers to integrate predictive analytics and AI models into data workflows.
Monitor and troubleshoot data pipelines to resolve data discrepancies and optimize performance.
Utilize Snowflake's advanced features, including Snowpark, Streams, and Tasks, to enable data processing and analysis.
Develop and maintain data documentation, best practices, and data governance protocols.
Ensure data security, privacy, and compliance with organizational and regulatory guidelines.
Responsibilities:
. Bachelor&rsquos degree in Computer Science, Data Engineering, or a related field.
. experience in data engineering, with experience working with Snowflake.
. Proven experience in Snowflake Cortex AI, focusing on data extraction, chatbot development, and Conversational AI.
. Strongproficiency in SQL, Python, and data modeling.
. Experience with data integration tools (e.g., Matillion, Talend, Informatica).
. Knowledge of cloud platforms such as AWS, Azure, or GCP.
. Excellent problem-solving skills, with a focus on data quality and performance optimization.
. Strong communication skills and the ability to work effectively in a cross-functional team.
Proficiency in using DBT's testing and documentation features to ensure the accuracy and reliability of data transformations.
Understanding of data lineage and metadata management concepts, and ability to track and document data transformations using DBT's lineage capabilities.
Understanding of software engineering best practices and ability to apply these principles to DBT development, including version control, code reviews, and automated testing.
Should have experience building data ingestion pipeline.
Should have experience with Snowflake utilities such as SnowSQL, SnowPipe, bulk copy, Snowpark, tables, Tasks, Streams, Time travel, Cloning, Optimizer, Metadata Manager, data sharing, stored procedures and UDFs, Snowsight.
Should have good experience in implementing CDC or SCD type 2
Proficiency in working with Airflow or other workflow management tools for scheduling and managing ETL jobs.
Good to have experience in repository tools like Github/Gitlab, Azure repo
Qualifications/Minimum qualifications
B.E./ Masters in Computer Science, Information technology, or Computer engineering or any equivalent degree with good IT experience and relevant of working experience as a Sr. Data Engineer with DBT+Snowflake skillsets
Skill Matrix:
DBT (Core or Cloud), Snowflake, AWS/Azure, SQL, ETL concepts, Airflow or any orchestration tools, Data Warehousing concepts
Why join Genpact
Be a transformation leader - Work at the cutting edge of AI, automation, and digital innovation
Make an impact - Drive change for global enterprises and solve business challenges that matter
Accelerate your career - Get hands-on experience, mentorship, and continuous learning opportunities
Work with the best - Join 140,000+ bold thinkers and problem-solvers who push boundaries every day
Thrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress
Come join the tech shapers and growth makers at Genpact and take your career in the only direction that matters: Up.
Let&rsquos build tomorrow together.
Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.
Furthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Senior Data Engineer,Adidas,7-12 Years,,Gurugram,Sporting Goods,"Data Lakehouse & Data Product Engineer
Purpose & Overall Relevance for the Organization:
Be part of a team building a state-of-the-art data lakehouse and create data products which are ingested from heterogeneous source systems.
Opportunity to work in leading-edge technology like Databricks.
Collaborate with multiple stakeholders to drive the data roadmap by translating stories into data pipelines.
Learn and build knowledge around data domains and business processes.
Set the mindset to lifeblood for adidas data-driven strategy by enabling the data backbone, powering AI and BI solutions that produce unseen insights for the entire company.
What You Bring:
Expert in SAP data models:
Expert in SAP ECC and SAP S4HANA data models and processes. SAP process knowledge in sales and distribution (SD) is preferred.
SAP BW Development Experience:
Strong background in SAP BW Development, with knowledge of SAP ECC and S4HANA versions and their integration with data warehouse solutions.
Exposure to Spark Architecture:
Familiarity with Spark, PySpark, and Python is key, especially the willingness to learn and create well-documented, maintainable, and production-ready solutions.
Strong ETL Build Knowledge:
Knowledge of Slowly Changing Dimension (SCD) and heterogeneous data harmonization methods.
Experienced with SQL:
Foundation in writing and interpreting SQL queries is essential, with the expectation to handle more complex queries.
Familiarity with Data Tools:
Familiar with tools such as Databricks and AWS, and have knowledge of different storage formats like Delta, Parquet, Avro, etc.
Excellent Analytical and Team Leadership Skills:
Convince with excellent analytical skills, ability to work in a team, as well as lead a team of engineers.
Requisite Education and Experience / Minimum Qualifications:
Four-year college or university degree with focus on Business Administration or IT or related areas, or equivalent combination of education and experience.
Proficient spoken and written command of English.
At least 7 years of experience in IT.
5 years of experience in relevant area.
2 years of experience in team management.
COURAGE: Speak up when you see an opportunity; step up when you see a need.
OWNERSHIP: Pick up the ball. Be proactive, take responsibility and follow-through.
INNOVATION: Elevate to win. Be curious, test and learn new and better ways of doing things.
TEAMPLAY: Win together. Work collaboratively and cultivate a shared mindset.
INTEGRITY: Play by the rules. Hold yourself and others accountable to our company's standards.
RESPECT: Value all players. Display empathy, be inclusive and show dignity to all.","SAP BW Development, SAP S4HANA, Pyspark, Spark, Sql, Sap Ecc"
Data Engineer,Vichara Technologies,6-12 Years,,"Delhi, Delhi NCR",Consulting,"Key deliverables:
Enhance and maintain the MDM platform
Monitor and optimize system performance
Troubleshoot and resolve technical issues
Support production incident resolution
Role responsibilities:
Develop and refactor Python and SQL code
Integrate REST APIs within MDM workflows
Utilize Azure Databricks and ADF for ETL tasks
Work on Markit EDM or Semarchy-based solutions","Rest Api, Adf, Azure Databricks, Python, Sql"
Data Engineer,Vichara Technologies,6-12 Years,,Hyderabad,Consulting,"Key deliverables:
Enhance and maintain the MDM platform
Monitor and optimize system performance
Troubleshoot and resolve technical issues
Support production incident resolution
Role responsibilities:
Develop and refactor Python and SQL code
Integrate REST APIs within MDM workflows
Utilize Azure Databricks and ADF for ETL tasks
Work on Markit EDM or Semarchy-based solutions","Rest Api, Adf, Azure Databricks, Python, Sql"
Data Engineer 3,Vichara Technologies,7-12 Years,,Bengaluru,Consulting,"Key deliverables:
Enhance and maintain the MDM platform to support business needs
Develop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow
Monitor and improve system performance and troubleshoot data pipeline issues
Resolve production issues and ensure platform reliability
Role responsibilities:
Collaborate with data engineering and analytics teams for scalable solutions
Apply DevOps practices to streamline deployment and automation
Integrate cloud-native tools and services (AWS, Azure) with the data platform
Utilize dbt and version control (Git) for data transformation and management","Airflow, snowflake, dbt, Python, Sql, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Hyderabad,Consulting,"Role Responsibilities:
Build scalable data pipelines and applications from scratch on AWS
Design and manage data ingestion processes and workflows
Collaborate with functional and data science teams to deliver data solutions
Optimize data storage and organization using HDFS, S3, and Delta Lake
Job Requirements:
Strong programming experience in Python or Java
Hands-on experience with AWS services and deployment
Proficiency in HDFS, S3, SQL, and data ingestion tools
Exposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Bengaluru,Consulting,"Role Responsibilities:
Build scalable data pipelines and applications from scratch on AWS
Design and manage data ingestion processes and workflows
Collaborate with functional and data science teams to deliver data solutions
Optimize data storage and organization using HDFS, S3, and Delta Lake
Job Requirements:
Strong programming experience in Python or Java
Hands-on experience with AWS services and deployment
Proficiency in HDFS, S3, SQL, and data ingestion tools
Exposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Senior Data Engineer,Eagleview,5-10 Years,,Bengaluru,Software,"Role & responsibilities
Design, develop, and optimize scalable data pipelines for ETL/ELT processes.
Develop and maintain Python-based data processing scripts and automation tools.
Write and optimize complex SQL queries (preferably in Snowflake) for data transformation and analytics.
Experience with Jenkins or other CI/CD tools.
Experience developing with Snowflake as the data platform.
Experience with ETL/ELT tools (preferably Fivetran, dbt).
Implement version control best practices using Git or other tools to manage code changes.
Collaborate with cross-functional teams (analysts, product managers, and engineers) to understand business needs and translate them into technical data solutions.
Ensure data integrity, security, and governance across multiple data sources.
Optimize query performance and database architecture for efficiency and scalability.
Lead troubleshooting and debugging efforts for data-related issues.
Document data workflows, architectures, and best practices to ensure maintainability and knowledge sharing.
Preferred candidate profile
5+ years of experience in Data Engineering, Software Engineering, or a related field.
Bachelors or masters degree in computer science, Computer Engineering, or a related discipline
High proficiency in SQL (preferably Snowflake) for data modeling, performance tuning, and optimization.
Strong expertise in Python for data processing and automation.
Experience with Git or other version control tools in a collaborative development environment.
Strong communication skills and ability to collaborate with cross-functional teams for requirements gathering and solution design.
Experience working with large-scale, distributed data systems and cloud data warehouse.","snowflake, dbt, fivetran, Git, python, Sql"
Azure Data Engineer,Zf Lifetec India,5-11 Years,,Pune,Manufacturing,"What you can look forward to as Azure Data Engineer
End-to-End Ownership: Lead and drive analytics initiatives, ensuring high-quality deliverables with minimal oversight
Structured Software Development: Apply engineering best practices, including CI/CD, version control, testing, and code reviews, to ensure robust, maintainable, and scalable solutions
Design, implement, and scale reliable and efficient data models and pipelines in Databricks, Azure Cloud, and other relevant platforms to support analytics and AI initiatives
Lead AI & ML initiatives, according to AI & automation roadmap of analytics team
Data Modeling & Optimization: Develop efficient data models, ensuring high performance for analytical workloads and business intelligence applications
Ensure seamless integration of data products with BI tools, APIs, and other business applications, providing structured, well-documented, and high-quality datasets
Your profile as Azure Data Engineer
Degree in computer science, math or engineering with emphasis on machine learning / analytics or equivalent in experience
5-11 years of experience in Azure Data engineering, Analytics or AI/ML integration
Strong experience in Python, R and Spark (Pyspark is desirable)
Deployment of ML models in production & Industry experience in agile development and DevOps Databases (ex. Oracle, AWS S3)
Big Data Platforms (ex. Databricks, GCP, Cloudera) & Soft skills include working well in teams dealing with uncertainty and ambiguity, flexibility
Deep understanding of structured software development practices (version control, CI/CD, testing) & Excellent problem-solving skills and ability to work independently on complex projects","R, Pyspark, Azure, Python, Devops"
Azure Data Engineer,Tiger Analytics,6-11 Years,,Hyderabad,Consulting,"Role Overview:
We are seeking street-smart and technically strong Senior Data Engineers / Leads who can take ownership of designing and developing cutting-edge data and AI platforms using Azure-native technologies and Databricks. You will play a critical role in building scalable data pipelines, modern data architectures, and intelligent analytics solutions.
Key Responsibilities:
Design and implement scalable, metadata-driven frameworks for data ingestion, quality, and transformation across both batch and streaming datasets.
Develop and optimize end-to-end data pipelines to process structured and unstructured data, enabling the creation of analytical data products.
Build robust exception handling, logging, and monitoring mechanisms for better observability and operational support.
Take ownership of complex modules and lead the development of critical data workflows and components.
Provide guidance to data engineers and peers on best practices.
Collaborate with cross-functional teamsincluding business consultants, data architects & scientists, and application developersto deliver impactful analytics solutions.
Required Qualifications:
5+ yearsof overall technical experience, with a minimum of2 yearsof hands-on experience with Microsoft Azure and Databricks.
Proven experience delivering at least one end-to-endData Lakehousesolution on Azure Databricks using the Medallion Architecture.
Strong working knowledge of theDatabricks ecosystem, including: PySpark, Notebooks, Structured Streaming, Unity Catalog, Delta Live Tables, Workflows, and SQL Warehouse.
Advancedprogramming, unit testing, and debugging skills in Python and SQL.
Hands-on experience withAzure-native servicessuch as: Azure Data Factory, ADLS Gen2, Azure SQL Database, and Event Hub.
Solid understanding ofdata modeling techniques, including both Dimensional and Third Normal Form (3NF) models.
Exposure to developingLLM/Generative AI-powered applications.
Must have excellent understanding ofCI/CD workflowsusing Azure DevOps.
Bonus: Knowledge of Azure infrastructure, including provisioning, networking, security, and governance.
Educational Background:
Bachelor's degree (B.E/B.Tech) in Computer Science, Information Technology, or a related field from a reputed institute (preferred).","Azure Data Engineer, Pyspark, Azure Data Factory, Azure Data Lake, Spark, Azure Databricks, Sql, Python"
Data Engineer 3,Vichara Technologies,7-12 Years,,Pune,Consulting,"Key deliverables:
Enhance and maintain the MDM platform to support business needs
Develop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow
Monitor and improve system performance and troubleshoot data pipeline issues
Resolve production issues and ensure platform reliability
Role responsibilities:
Collaborate with data engineering and analytics teams for scalable solutions
Apply DevOps practices to streamline deployment and automation
Integrate cloud-native tools and services (AWS, Azure) with the data platform
Utilize dbt and version control (Git) for data transformation and management","Airflow, snowflake, dbt, Python, Sql, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Gurugram,Consulting,"Role Responsibilities:
Build scalable data pipelines and applications from scratch on AWS
Design and manage data ingestion processes and workflows
Collaborate with functional and data science teams to deliver data solutions
Optimize data storage and organization using HDFS, S3, and Delta Lake
Job Requirements:
Strong programming experience in Python or Java
Hands-on experience with AWS services and deployment
Proficiency in HDFS, S3, SQL, and data ingestion tools
Exposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Data Engineer(Snowflake+AWS) job opening at GlobalData,GlobalData Publications Inc,4-6 Years,,Hyderabad,Information Services,"Responsibilities:
Develop and optimize Snowflake data warehousing solutions.
Implement and manage AWS DevOps pipelines for continuous integration and delivery.
Collaborate with cross-functional teams to gather and analyze requirements.
Perform data analysis, modeling, and integration to ensure data quality and integrity.
Monitor and troubleshoot Snowflake, and AWS DevOps performance issues.
Automate processes and workflows to improve efficiency and scalability.
Ensure security and compliance best practices are followed.
Required Skills:
Extensive experience in database development (SQL, NoSQL).
Proficiency in Snowflake development and data warehousing concepts, Snowpipe, and Time Travel.
Strong knowledge of AWS services (EC2, S3, Lambda, RDS, etc.).
Experience with AWS DevOps tools (CodePipeline, CodeBuild, CodeDeploy).
Proficiency in programming languages such as Python, Java or node.js
Strong understanding of data modelling, ETL processes, and data integration.
Hands-on experience with CI/CD pipelines and automation.
Excellent problem-solving and analytical skills.
Preferred Skills:
Knowledge of containerization and orchestration (Docker, Kubernetes).
Familiarity with data visualization tools (Tableau, Power BI).
Experience with data governance and security practices.
Strong communication and collaboration skills.
Ability to work in a fast-paced, dynamic environment.","Aws, S3, Lambda, RDS, Ec2, Power Bi, Tableau"
Data Engineer 3,Vichara Technologies,7-12 Years,,"Delhi, Delhi NCR",Consulting,"Key deliverables:
Enhance and maintain the MDM platform to support business needs
Develop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow
Monitor and improve system performance and troubleshoot data pipeline issues
Resolve production issues and ensure platform reliability
Role responsibilities:
Collaborate with data engineering and analytics teams for scalable solutions
Apply DevOps practices to streamline deployment and automation
Integrate cloud-native tools and services (AWS, Azure) with the data platform
Utilize dbt and version control (Git) for data transformation and management","Airflow, snowflake, dbt, Python, Sql, AWS"
Senior Data Engineer - Visa,Foray Software Private Limited,3-7 Years,,Bengaluru,Consulting,"Role Responsibilities:
Build and manage data pipelines using Spark and Hadoop ecosystem
Develop scalable solutions with Hive, HDFS, Kafka, and Sqoop
Write code using Scala with exposure to Python
Maintain jobs using schedulers like AutoSys and version control with Git
Job Requirements:
2+ years relevant experience in Big Data platforms
Hands-on with Oracle/MS-SQL databases
Exposure to data visualization platforms like Tableau or AtScale (preferred)
Strong problem-solving and communication skills","Hive, Hadoop, Scala, Spark, Kafka"
Senior Data Engineer - Data Management,Accordion India,2-5 Years,,Hyderabad,Consulting,"What You will do:
Understand the business requirements thoroughly to design and develop the BI architecture.
Determine business intelligence and data warehousing solutions that meet business needs.
Perform data warehouse design and modelling according to established standards.
Work closely with the business teams to arrive at methodologies to develop KPIs and Metrics.
Work with Project Manager in developing and executing project plans within assigned schedule and timeline.
Develop standard reports and functional dashboards based on business requirements.
Ensure to develop and deliver high quality reports in timely and accurate manner.
Conduct training programs and knowledge transfer sessions to junior developers when needed. Recommend improvements to provide optimum reporting solutions.
Ideally, you have:
Undergraduate degree (B.E/B.Tech.) from tier-1/tier-2 colleges are preferred.
2 - 5 years of experience in related field.
Proven expertise in SSIS, SSAS and SSRS (MSBI Suite).
In-depth knowledge of databases (SQL Server, MySQL, Oracle etc.) and data warehouse (Azure Synapse, AWS Redshift, Google BigQuery, Snowflake etc.).
In-depth knowledge of business intelligence tools (any one of Power BI, Tableau, Qlik, DOMO, Looker etc.).
Good understanding of Azure (Data Factory Pipelines, SQL Database Managed Instances, DevOps, Logic Apps, Analysis Services), AWS (Glue, Aurora Database, Dynamo Database, Redshift, QuickSight).
Proven abilities to take on initiative and be innovative.
Analytical mind with problem solving attitude.","Sql, Hadoop, Spark, Data Warehousing, Python, Etl, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Pune,Consulting,"Role Responsibilities:
Build scalable data pipelines and applications from scratch on AWS
Design and manage data ingestion processes and workflows
Collaborate with functional and data science teams to deliver data solutions
Optimize data storage and organization using HDFS, S3, and Delta Lake
Job Requirements:
Strong programming experience in Python or Java
Hands-on experience with AWS services and deployment
Proficiency in HDFS, S3, SQL, and data ingestion tools
Exposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Senior Data Engineer,Zeta Cards,4-6 Years,,"Mumbai, India",Banking/Accounting/Financial Services,"About Zeta
Zeta is a Next-Gen Banking Tech company that empowers banks and fintechs to launch banking products for the future. It was founded by and Ramki Gaddipati in 2015.
Our flagship processing platform - Zeta Tachyon - is the industry's first modern, cloud-native, and fully API-enabled stack that brings together issuance, processing, lending, core banking, fraud & risk, and many more capabilities as a single-vendor stack. 20M+ cards have been issued on our platform globally.
Zeta is actively working with the largest Banks and Fintechs in multiple global markets transforming customer experience for multi-million card portfolios.
Zeta has over 1700+ employees - with over 70% roles in R&D - across locations in the US, EMEA, and Asia. We raised $280 million at a $1.5 billion valuation from Softbank, Mastercard, and other investors in 2021.
Learn more @, , ,
About the Role
In this role, you'll design robust data models using SQL, dbt, and Redshift, while driving best practices across development, deployment, and monitoring. You'll also collaborate closely with product and engineering to ensure data quality and impactful delivery.
Responsibilities
Create optimised data models with SQL, DBT and Redshift
Write functional and column level test for Models
Build reports from the data models
Collaborate with product to clarify requirement and create design document
Get design reviewed from Architect/Principal/Lead Engineer
Contribute to code reviews
Set up and monitor Airflow DAGs
Set up and use CI/CD pipelines
Leverage Kubernetes operators for deployment automation
Ensure data quality
Drive best practices in Data models development, deployment, and monitoring
Mentor colleagues and contribute to team growth
Skills
Strong expertise in SQL for complex data querying and optimization
Hands-on experience with Apache Airflow for orchestration and scheduling
Solid understanding of data modeling and data warehousing concepts
Experience with dbt (Data Build Tool) for data transformation and modeling
Exposure to Amazon Redshift or other cloud data warehouses
Familiarity with CI/CD tools such as Jenkins
Experience using Bitbucket for version control
Monitoring and alerting using Grafana and Prometheus
Working knowledge of JIRA for agile project tracking
Familiarity with Kubernetes for deployment automation and orchestration
Experience and Qualification
4-6 years of relevant experience in data engineering
s/Master's degree in engineering (computer science, information systems)","CI CD tools, dbt, Apache Airflow, Bitbucket, Amazon Redshift, Prometheus, Grafana, JIRA, Kubernetes, Sql"
Data Engineer,Vivotex India Private Limited,5-10 Years,INR 0.5 - 17 LPA,"Hyderabad, Bengaluru, Chennai",Login to check your skill match score,"location: Hyderabad,Bangalore,pune,Chennai
Skill set: SQL, Python, PySpark, and experience with cloud platforms (e.g., Snowflake, AWS).
Client: NTT Data (CTH Position)
Experience 5+
Immediate Joiners only","dbt, Snow Flake, Data Engineer, Data Warehouse, Sql"
Data Engineer,Robotics Technologies,4-13 Years,INR 6.5 - 24 LPA,"Gurugram, Delhi, Kolkata","Management Information Systems, Outsourcing, Cloud Data Services, Information Services","Description
We are seeking a skilled Data Engineer to join our team in India. The ideal candidate will be responsible for building and maintaining the infrastructure that supports our data-driven initiatives. You will work closely with cross-functional teams to ensure data is accessible, reliable, and secure.
Responsibilities
Design, develop, and maintain scalable data pipelines and architectures.
Ensure data quality and reliability through comprehensive testing and validation.
Collaborate with data scientists and analysts to understand data requirements and deliver necessary data solutions.
Optimize and improve data storage and retrieval processes for efficiency.
Monitor and troubleshoot data-related issues and performance bottlenecks.
Implement data security and compliance measures as per industry standards.
Skills and Qualifications
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Proficiency in programming languages such as Python, Java, or Scala.
Strong understanding of SQL and experience with relational databases like MySQL, PostgreSQL, or Oracle.
Experience with big data technologies such as Hadoop, Spark, or Kafka.
Familiarity with cloud platforms like AWS, Google Cloud, or Azure.
Knowledge of data warehousing solutions like Snowflake, Redshift, or BigQuery.
Experience with ETL tools and frameworks.
Strong analytical and problem-solving skills.
Ability to work collaboratively in a team environment and communicate effectively.","Nosql, Hadoop, Data Modeling, Spark, Kafka, Data Warehousing, Sql, Python, Etl, AWS"
Principal Data Engineer,Dayworks Private Limited,8-12 Years,,Pune,Login to check your skill match score,"This role is for one of Weekday's clients
Min Experience: 8 years
Location: Pune
JobType: full-time
Title: Principal Data Engineer
Job Description:
Job Summary:
Lead the design, development, and optimization of enterprise-scale data pipelines and architectures to drive data-driven decision-making. Architect efficient data tables and schemas to enhance query performance, ensuring seamless integration with cross-functional teams
Key Responsibilities:
Data Pipeline
Design and implement high-performance data pipelines using ETL processes, batch/streaming frameworks (e.g., Apache Spark, Airflow), and cloud platforms (AWS).
Optimize data ingestion, transformation, and storage workflows to meet scalability, reliability, and latency requirements.
Data Architecture & Query Efficiency
Architect database schemas and dimensional models (e.g., star/snowflake) to maximize query performance and reduce latency.
Implement indexing strategies, partitioning, and materialized views to optimize data retrieval.
Collaboration & Governance
Partner with data scientists, product managers, and engineers to align data infrastructure with business needs.
Establish data governance frameworks, ensuring compliance with security, privacy, and quality standards.
Mentor junior engineers and foster best practices in data engineering
Innovation & Leadership
Stay ahead of industry trends (e.g., AI/ML integration, real-time analytics) and advocate for emerging technologies.
Lead technical strategy for data infrastructure, balancing innovation with operational stability.
Technical Expertise:
8+ years of experience in data engineering, with a focus on ETL, data modelling, and cloud-based architectures.
Proficiency in SQL, Python, and tools like Spark and Snowflake.
Strong knowledge of database design, dimensional modeling, and query optimization.
Leadership & Collaboration
Proven ability to lead teams, mentor engineers, and drive technical initiatives.
Experience translating business requirements into scalable technical solutions.
Preferred qualification
Expertise in machine learning model deployment or real-time analytics.
Familiarity with DevOps practices (CI/CD, Docker, Kubernetes).
Our Core Values
Data Fanatics: Our edge is always found in the data
Partner Obsessed: We are obsessed with partner success
Team of Doers: We have a bias for action
Game Changers: We encourage innovation","SparkML, Data Modelling, Spark, Apache Spark"
Platform Data Engineer,Elitez Recruitment Services Private Limited,5-13 Years,INR 120 - 216 LPA,"Bengaluru, Chennai","Enterprise Software, Software Engineering, Software, Consumer Software, Information Services","Docker/Kubernetes Application Development and Kube Admin Skills (Incl. Helm)
Phyton Development is important
Observability Skills (Monitoring, and Logging ELK, Prometheus/Grafana preferably)
Airflow 2.7 + (Both as a user level who develops DAGs, and has knowledge of Airflow internals including customizations)
CICD (Azure DevOps)
Any experience with Control-M is good
Kubernetes:
- In depth mastery of core Kubernetes concepts, including Control Plane Architecture, Networking (CNI, Service, Ingress, NetworkPolicy), Storage (CSI, PV, PVC), Security (RBAC, Secrets), Scheduling, and Resource Management
- Must have proven track record of successfully deploying and managing large-scale, mission-critical Kubernetes clusters in production environment
Observability:
- Mastery of core observability principles, including Metrics, Logs, Traces, Baggage, SLOs/SLIs, Alerting, Instrumentation, and Correlation.
- Must have proven experience engineering/developing and implementing comprehensive observability solutions using the OpenTelemetry framework, resulting in significant reductions in Mean Time To Detection (MTTD) or Mean Time To Resolution (MTTR).
Frameworks: Opentelemetry or equivalent
Airflow:
- Strong understanding of core Airflow architecture, including Control-M integration, DAG best practices, scheduling, operators, and executors.
- Must have proven experience owning and stabilizing production Airflow environments, ensuring reliable execution of critical data workflows that support business operations.
Coding & Scripting: (Technologies)
- Proficiency in Python for automation, tooling, and systems development, coupled with strong Bash scripting skills.
- Demonstrated experience building impactful internal tools or services that significantly enhance developer productivity and platform capabilities.
- Demonstrated expertise in identifying and resolving highly complex, cross-functional issues within distributed systems.","Java, Golang, Graphql, Docker, Node.js, Rest Apis, Sql, Kubernetes, Microservices, AWS"
Data Engineer,Robotics Technologies,7-12 Years,INR 29 - 50 LPA,"Navi Mumbai, Mumbai City, Pune","Management Information Systems, Email, VoIP, Identity Management","Description
We are seeking a skilled Data Engineer to join our team in India. The ideal candidate will have extensive experience in designing, building, and maintaining scalable data pipelines and databases, ensuring high data quality and availability for analytical needs.
Responsibilities
Design and implement data pipelines to support data transformation and analysis.
Ensure data quality and integrity through automated testing and monitoring.
Collaborate with data scientists and analysts to understand data requirements and deliver solutions.
Optimize data storage and retrieval processes for efficiency and scalability.
Develop documentation for data processes and systems to ensure knowledge transfer and compliance.
Skills and Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
7-12 years of experience in data engineering or a related role.
Proficiency in programming languages such as Python, Java, or Scala.
Strong experience with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).
Familiarity with data warehousing solutions (e.g., Amazon Redshift, Google BigQuery).
Experience with ETL tools and frameworks (e.g., Apache NiFi, Talend).
Knowledge of cloud platforms (AWS, Azure, Google Cloud) and their data services.
Understanding of data modeling concepts and best practices.","Cloud Technologies, Data Modeling, Apis, Spark, Kafka, Data Warehousing, Sql, Python, Etl, AWS"
leaad data engineer,Teizo Soft Private Limited,10-13 Years,INR 26.5 - 34.5 LPA,Hyderabad,Login to check your skill match score,"Lead Data Engineer Hyderabad (Work from Office)
Send profiles to [HIDDEN TEXT]
Required Skills & Experience: 10+ years of data engineering experience, with 2+ years in a technical leadership role.
Deep expertise in FiveTran, Snowflake, and SQL development for data processing.
Strong Python skills for data transformation, orchestration, and automation.
Extensive experience in ETL pipeline design and data integration strategies.
Proven ability in dimensional modeling, star schema design, and data warehousing best practices.
Hands-on experience with business KPIs and reporting analytics to influence data model architecture.
Strong leadership in mentoring Data Engineers and BI Engineers, providing architectural guidance.
Exceptional problem-solving abilities, communication skills, and stakeholder management.
Preferred Skills:
Experience with cloud platforms like AWS, Azure, or GCP for data solutions.
Knowledge of big data technologies such as Apache Spark or Kafka.
Strong grasp of data security and governance principles.
Location: Hyderabad Mode of Work: Work from Office Apply Now: [HIDDEN TEXT]
Hashtags: #DataEngineer #Snowflake #BI #Data #Python #FiveTran #SQL #ETL #CloudData #DataWarehousing #Leadership #Hyderabad #Hiring #Recruitment","Cloud Data Platforms (AWS, Hadoop Data Security & Governance, Data Engineering Leadership, SQL Query Optimization, Data Migration & Integration, Data Science & Analytics, Gcp, Big Data Technologies, Spark, Kafka, Azure"
Gcp Data Engineer,Telus International,3-8 Years,,Bengaluru,BPO,"Role & responsibilities.
As a Data Engineer, you'll focus on solving problems and creating value for business by building solutions that are reliable and scalable to work with the size and scope of the company. You will be tasked with creating a custom-built pipeline on GCP stack, and you will be part of teams that implement vendor sourced enterprise software, configuring that software, customizing it, and integrating with other internal systems.
Required Skills:
3-10+ years of industry experience in software development, data engineering, business intelligence, or related field with experience in manipulating, processing, and extracting value from datasets.
Design, build and deploy internal applications to support our technology life cycle, collaboration and spaces, service delivery management, data and business intelligence among others.
Building Modular code for multi usable pipeline or any kind of complex Ingestion Framework used to ease the job to load the data into Datalake or Data Warehouse from multiple sources.
Work closely with analysts and business process owners to translate business requirements into technical solutions.
Coding experience in scripting and languages (Python, SQL, PySpark).
Expertise in Google Cloud Platform (GCP) technologies in the data warehousing space (BigQuery,Dataproc, GCP Workflows, Dataflow, Cloud Scheduler, Secret Manager,Batch, Cloud Logging, Cloud SDK, Google Cloud Storage, IAM, Vertex AI).
Maintain highest levels of development practices including: technical design, solution development, systems configuration, test documentation/execution, issue identification and resolution, writing clean, modular and self-sustaining code, with repeatable quality and predictability.
Understanding CI/CD Processes usingPulumi,Github, Cloud Build, Cloud SDK, Docker
Experience withSAS/SQL Server/SSISis an added advantage.
Qualifications:
Bachelor's degree in Computer Science or related technical field, or equivalent practical experience.
GCP Certified Data Engineer (preferred)
Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences.","cd, dataflow cloud, python, Ci, Sas, Pyspark, Data Warehousing, Sql"
Data Engineer | Tealium IQ |Tag Management,Datamatics Global Services Limited,5-8 Years,,Bengaluru,Information Technology,"Job description
Extensive experience in the process flows associated with tracking design and build.
Extensive experience in Tealium IQ and the configuration of one or more digital analytics tools across websites and apps (ideally Amplitude but not essential).
Extensive experience in the wider marketing stack (Google, Facebook, etc) and implementation of associated pixels.
Strong understanding of privacy requirements where they relate to digital data, with experience in using OneTrust or similar Consent Management Platform.
Ideally have knowledge of Front End coding languages (Javascript, HTML, CSS).
Experienced on API integrations.
Strong stakeholder manager, able to priorities and communicate accordingly in a fast-paced, product-led organization.
Excellent Test Script and Documentation skills
Experience working in more than one of the following service delivery functions; development, support, software design or environment configuration
Ability to translate business goals into technology solutions
Experience of working in a matrix managed environment, with mixed teams including contractors and outsourced vendors","Tag Management, Tealium IQ, Test Script, css, Html Scripting, Api Integration, Javascript"
Senior Azure Data Engineer,IQVIA,4-8 Years,,"Gurugram, Bengaluru, Cochin / Kochi / Ernakulam",Hospital,"Project Role:Azure date engineer
Work Experience:4 to 8 Years
Work location:Bangalore / Gurugram / Kochi
Work Mode:Hybrid
Must Have Skills:Azure Data engineer, SQL, Spark/Pyspark
Job Overview:
Responsible for the on-time completion of projects or components of large, complex projects for clients in the life sciences field. Identifies and elevates potential new business opportunities and assists in the sales process.
Skills required:
Experience in developing Azure components like Azure data factory, Azure data Bircks, Logic Apps, Functions
Develop efficient & smart data pipelines in migrating various sources on to Azure datalake
Proficient in working with Delta Lake, Parquet file formats
Designs, implements, and maintain the CI/CD pipelines, deploy, merge codes
Expert in programming in SQL, Pyspark, Python
Creation of databases on Azure data lake with best data warehousing practises
Build smart metadata databases and solutions, parameterization, configurations
Develop Azure frameworks, develops automated systems for deployment & monitoring
Hands-on experience in continuous delivery and continuous integration of CI/CD pipelines, CI/CD infrastructure and process troubleshooting.
Extensive experience with version control systems like Git and their use in release management, branching, merging, and integration strategies
Essential Functions:
Participates or leads teams in the design, development and delivery of consulting projects or components of larger, complex projects.
Reviews and analyzes client requirements or problems and assists in the development of proposals of cost effective solutions that ensure profitability and high client satisfaction.
Provides direction and guidance to Analysts, Consultants, and where relevant, to Statistical Services assigned to engagement.
Develops detailed documentation and specifications.
Performs qualitative and/or quantitative analyses to assist in the identification of client issues and the development of client specific solutions.
Designs, structures and delivers client reports and presentations that are appropriate to the characteristics or needs of the audience.
May deliver some findings to clients.
Qualifications
Bachelor's Degree Req
Master's Degree Business Administration Pref
4-8 years of related experience in consulting and/or life sciences industry Req.","Spark/Pyspark, Logic Apps, Azure Data engineer, Azure data Bircks, Azure Data Factory, Sql"
Senior Azure Data Engineer,IQVIA,4-8 Years,,"Gurugram, Bengaluru, Cochin / Kochi / Ernakulam",Hospital,"Project Role:Azure date engineer
Work Experience:4 to 8 Years
Work location:Bangalore / Gurugram / Kochi
Work Mode:Hybrid
Must Have Skills:Azure Data engineer, SQL, Spark/Pyspark
Job Overview:
Responsible for the on-time completion of projects or components of large, complex projects for clients in the life sciences field. Identifies and elevates potential new business opportunities and assists in the sales process.
Skills required:
Experience in developing Azure components like Azure data factory, Azure data Bircks, Logic Apps, Functions
Develop efficient & smart data pipelines in migrating various sources on to Azure datalake
Proficient in working with Delta Lake, Parquet file formats
Designs, implements, and maintain the CI/CD pipelines, deploy, merge codes
Expert in programming in SQL, Pyspark, Python
Creation of databases on Azure data lake with best data warehousing practises
Build smart metadata databases and solutions, parameterization, configurations
Develop Azure frameworks, develops automated systems for deployment & monitoring
Hands-on experience in continuous delivery and continuous integration of CI/CD pipelines, CI/CD infrastructure and process troubleshooting.
Extensive experience with version control systems like Git and their use in release management, branching, merging, and integration strategies
Essential Functions:
Participates or leads teams in the design, development and delivery of consulting projects or components of larger, complex projects.
Reviews and analyzes client requirements or problems and assists in the development of proposals of cost effective solutions that ensure profitability and high client satisfaction.
Provides direction and guidance to Analysts, Consultants, and where relevant, to Statistical Services assigned to engagement.
Develops detailed documentation and specifications.
Performs qualitative and/or quantitative analyses to assist in the identification of client issues and the development of client specific solutions.
Designs, structures and delivers client reports and presentations that are appropriate to the characteristics or needs of the audience.
May deliver some findings to clients.
Qualifications
Bachelor's Degree Req
Master's Degree Business Administration Pref
4-8 years of related experience in consulting and/or life sciences industry Req.","Spark/Pyspark, Logic Apps, Azure Data engineer, Azure data Bircks, Azure Data Factory, Sql"
Data Engineer,Infosys Limited,Fresher,,"Bengaluru, India",IT/Computers - Software,"Key Responsibilities:
We are seeking a skilled and detail oriented Data Warehouse Engineer to design build and maintain scalable data warehouse solutions
You will be responsible for developing efficient data pipelines integrating diverse data sources ensuring data accuracy and enabling high quality analytics to drive business decisions
Responsibilities
Design develop and maintain data warehouse architectures and systems
Build robust ETL Extract Transform Load processes for structured and unstructured data sources
Optimize data models database performance and storage solutions
Collaborate with data analysts data scientists and business stakeholders to understand data requirements
Implement data quality checks and ensure data governance best practices
Develop and maintain documentation related to data warehouse design data flow and processes
Monitor system performance and proactively identify areas for improvement
Support ad hoc data requests and reporting needs
Stay up to date with emerging data technologies and industry best practices
Preferred Skills:
Technology->ETL & Data Quality->ETL - Others,Technology->Database->Data Modeling,Technology->Data Management - DB->DB2,Technology->Data on Cloud-DataStore->Snowflake",
Data Engineer,Commonwealth Bank,4-6 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization
At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title: Data Engineer
Location: Bangalore
Business & Team:
The Business Banking Technology Domain works in an Agile methodology with our business banking business to plan, prioritise and deliver on high value technology objectives with key results that meet our regulatory obligations and protect the community.
You will work within the VRM Crew that is working on initiatives such as Gen AI based cash flow coach to provide relevant data to our regulators
Impact & Contribution:
As a Data Engineer and database engineer you will be creating and managing the cloud databases and data pipelines that underpin our decoupled cloud architecture and API first approach. You have proven expertise in database design, data ingestion, transformation, data writing, scheduling and query management within a cloud environment.
You will have proven experience and expertise in working with AWS Cloud Infrastructure Engineers, Software/API Developers to design, develop, deploy and operate data services and solutions that underpin a cloud ecosystem. You will take ownership and accountability of functional and non-functional design and work within a team of Engineers to create innovative solutions that unlock value and modernise technology designs.
You will role model continuous improvement mindset in the team, and in your project interactions, by taking technical ownership of key assets, including roadmaps and technical direction of data services running on our AWS environments.
Roles & Responsibilities:
Can design and implement databases for data integration in the enterprise
Can performance tune applications from a database code and design perspective
Can automate data ingestion and transformation processes using scheduling tools.
Monitor and troubleshoot data pipelines to ensure reliability and performance.
Can design application logical database requirements and implement physical solutions
Can collaborate with business and technical teams in order to design and build critical databases and data pipelines
Essential Skills:
Minimum 4 to 6 years of experience
AWS Data products such as AWS Glue and AWS EMR
Oracle and AWS Aurora RDS such as PostgreSQL
AWS S3 ingestion, transformation and writing to databases
Proficiency in programming languages like Python, Scala or Java for developing data ingestion and transformation scripts.
Strong knowledge of SQL for writing, optimizing, and debugging queries.
Familiarity with database design, indexing, and normalization principles.
Understanding of data formats (JSON, CSV, XML) and techniques for converting between them. Ability to handle data validation, cleaning, and transformation.
Proficiency in automation tools and scripting (e.g., bash scripting, cron jobs) for scheduling and monitoring data processes.
Experience with version control systems (e.g., Git) for managing code and collaboration.
Education Qualifications:
Bachelor's degree in engineering in Computer Science/Information Technology.
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 29/06/2025","Experience with version control systems e.g. Git, Oracle and AWS Aurora RDS such as PostgreSQL, Strong knowledge of SQL, AWS S3 ingestion transformation and writing to databases, AWS Data products such as AWS Glue and AWS EMR"
Data Engineer - Manager,State Street Corporation,8-12 Years,,Bengaluru,Login to check your skill match score,"Technical Lead (overall 10+ years good development experience) with good hands-on experience in Python/PySpark with strong experience in Spark scala/PySpark and Spark SQL for big data processing and Databricks, AWS services,Kafka,Airflow.
Hands-on experience in SQL, PL/SQL, Unix Shell Scripting, Autosys.
Experience with relational databases such as Oracle, PostgreSQL as well as NoSQL databases
Data integration techniques and tools including APIs and data streaming
Strong analytical and problem solving skills, with the ability to identify and resolve issues in data pipelines and systems
Experience with Agile software development methodology and version control systems like GitHub
Expertise in Data Modeling and DB design with the skills in performance tuning
Support existing data platforms and products in production for any critical issues and provide resolution fixes
Have good communication skill, and collaborate well with different teams/stakeholders and vendors
Expected to spend 90% of the time on hands-on development, design and architecture and remaining 10% on guiding the team on technology and removing other impediments Capital Markets Projects experience preferred Provides advanced technical expertise in analyzing, designing, estimating, and developing software applications to project schedule. Oversees systems design and implementation of most complex design components. Creates project plans and deliverables and monitors task deadlines. Oversees, maintains and supports existing software applications. Provides subject matter expertise in reviewing, analyzing, and resolving complex issues. Designs and executes end to end system tests of new installations and/or software prior to release to minimize failures and impact to business and end users. Responsible for resolution, communication, and escalation of critical technical issues. Prepares user and systems documentation as needed. Identifies and recommends Industry best practices. Serves as a mentor to junior staff.
Ability to lead a team of agile developers.
Worked in a complex deadline driven projects with minimal supervision. Ability to architect/design/develop with minimum requirements by effectively.
2+ experience on Apache Kafka streaming processingand Python scripting.
End to end understanding of software architecture, design, development and implementation.
Min 8+ years of good experience on AWS clouds and Data bricks,Airflow.
Excellent work ethics with ability to work independently.
Experience in Banking Domain preferred.","Airflow, NoSQL databases, Spark SQL, data streaming, Github, Apis, Aws Services, Data Modeling, PostgreSQL, Performance Tuning, Pyspark, Pl Sql, Kafka, Autosys, Db Design, Sql, Unix Shell Scripting, Spark, Databricks, Oracle, Python"
Senior Data Engineer,Commonwealth Bank,8-10 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title:Senior Data Engineer-Big Data
Location:Bengaluru
Business & Team:RM & FS Data Engineering
Impact & contribution:
As a Senior Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you're ahead of trends and work at the forefront of Big Data and Data warehouse technologies.
Which is why we're the perfect fit for you. Here, you'll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers most complex data-centric problems.
To us, data is everything. It is what powers our cutting-edge features and it's the reason we can provide seamless experiences for millions of customers from app to branch.
We're responsible for CommBank's key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We work across the Cloudera Hadoop Big Data, Teradata Group Data Warehouse and Ab Initio platforms.
Roles & Responsibilities:
Passionate about building next generation data platforms and data pipeline solution across the bank.
Enthusiastic, be able to contribute and learn from wider engineering talent in the team.
Ready to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.
Capable to create both technology blueprints and engineering roadmaps, for a multi- year data transformational journey.
Can lead and drive a culture where quality, excellence and openness are championed.
Constantly thinking outside the box and breaking boundaries to solve complex data problems.
Have experience in providing data driven solutions that source data from various enterprise data platform into Cloudera Hadoop Big Data environment, using technologies like Spark, MapReduce, Hive, Sqoop, Kafka transform and process the source data to produce data assets and transform and egression to other data platforms like Teradata or RDBMS system.
Are experienced in building effective and efficient Big Data and Data Warehouse frameworks, capabilities, and features, using common programming language (Scala, Java, or Python), with proper data quality assurance and security controls.
Are experienced in providing data driven solutions in the Cloud to build various enterprise data platform into AWS platform using technologies like S3, EMR, Glue, Iceberg, Kinesis or MSK/Kafka transform and process the data to produce data assets for Redshift and DocumentDB.
Are confident in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.
Can collaborate, co-create and contribute to existing Data Engineering practices in the team.
Have experience and responsible for data security and data management.
Have a natural drive to educate, communicate and coordinate with different internal stakeholders.
Essential Skills:
Preferably with at least 8+ years of hands-on experience in a Data Engineering role.
Experience in designing, building, and delivering enterprise-wide data ingestion, data integration and data pipeline solutions using common programming language (Scala, Java, or Python) in a Big Data and Data Warehouse platform.
Experience in building data solution in Hadoop platform, using Spark, Hive,MapReduce, Sqoop, Kafka and various ETL frameworks for distributed data storage and processing. Preferably with at least 8+ years of hands-on experience.
Experience in building data solution using AWS Cloud technology (EMR, Glue, Iceberg, Kinesis, MSK/Kafka, Redshift, DocumentDB, S3, etc.). Preferably with 2+ years of hands-on experience and certified AWS Data Engineer.
Strong Unix/Linux Shell scripting and programming skills in Scala, Java, or Python.
Proficient in SQL scripting, writing complex SQLs for building data pipelines.
Experience in working in Agile teams, including working closely with internal business stakeholders.
Familiarity with data warehousing and/or data mart build experience in Teradata, Oracle or RDBMS system is a plus.
Certification on Cloudera CDP, Hadoop, Spark, Teradata, AWS, Ab Initio is a plus.
Experience in Ab Initio software products (GDE, Co Operating System, Express It, etc.) is a plus.
Educational Qualifications: B.Tech and above
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 05/06/2025","DocumentDB, MSK, Teradata, Iceberg, Glue, Java, S3, Hadoop, Scala, Kafka, Big Data, Emr, Redshift, Sql, Mapreduce, Hive, Kinesis, Sqoop, Spark, Cloudera, Ab Initio, Python, AWS"
Senor Data Engineer with Snowflake Experience.( 5+ Years of Exp),3Pillar Global,5-8 Years,,"Indi, India",Login to check your skill match score,"Distinguished Tech Innovator:3Pillar warmly extends an invitation for you to join an elite team of visionaries. Beyond software development, we are dedicated to engineering solutions that challenge conventional norms. Envision you: steering projects that redefine urban living, establish new media channels for enterprise companies, or drive innovation in healthcare. Your invaluable expertise will serve as the cornerstone in shaping the future direction of our role transcends the ordinary realms of coding it's about orchestrating technological marvels that disrupt industries. Seize this extraordinary opportunity to lead a team that is actively shaping the tech landscape for our clients, and sets global standards along the way.
Minimum Qualification
Experience: 5-8 years of experience in data engineering, DevOps, or a related technical field.
Programming & Scripting: Strong programming skills in Python and Linux Bash for automation and data workflows.
Framework Proficiency: Hands-on experience with Luigi for orchestrating complex data workflows.
Data Processing & Storage: Expertise in Hadoop ecosystem tools and managing SQL databases for data storage and query optimization.
AWS Cloud Services: In-depth knowledge of AWS EC2, S3, RDS, and EMR to deploy and manage data solutions.
Monitoring & Alerting Tools: Familiarity with monitoring solutions for real-time tracking and troubleshooting of data pipelines.
Communication & Leadership: Proven ability to lead projects, communicate with stakeholders, and guide junior team members.
Additional experience
Data Architecture: Experience designing or optimizing data lake solutions.
Security Practices: Understanding of data security practices, data governance, and compliance for secure data processing.
Automation & CI/CD: Familiarity with CI/CD tools to support automation of deployment and testing.
Big Data Technologies: Knowledge of big data processing tools like Spark, Hive, or related AWS services.
Advanced Analytics: Background in analytics or data science to contribute to more data-driven decision-making.
Cross-Functional Collaboration: Experience collaborating with non-technical teams on business goals and technical solutions.
Regards
Rajan
3Pillar Global","Luigi, CI CD tools, SQL databases, Linux Bash, Hadoop ecosystem tools, Monitoring solutions, Aws Ec2, Hive, Python, Spark"
Senior Data Engineer,Swiss Re,12-14 Years,,"Hyderabad, India",Consulting/Advisory Services,"As a Senior Data Engineer, you will be responsible for designing and implementing complex data pipelines and analytics solutions to support key decision-making business processes in our Property & Casualty business domain. You will gain exposure to a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re Property & Casualty.
You will be expected to take end-to-end ownership of deliverables, gaining a full understanding of the Property & Casualty data and the business logic required to deliver analytics solutions.
Key responsibilities include:
Work closely with Product Owners and Architects to understand requirements, formulate solutions, and evaluate the implementation effort.
Design, develop and maintain scalable data transformation pipelines.
Data model Design, Data architecture implementation
Working with Palantir platform for implementation
Evaluate new capabilities of the analytics platform, develop prototypes and assist in drawing Development of single source of truth about our application landscape.
Collaborate within a global development team to design and deliver solutions
Assist stakeholders with data-related functional and technical issues
Working with data governance platform for data management and stewardship.
About the Team:
This position is part of the Property & Casualty Data Integration and Analytics project within the Reinsurance Data office team under Data & Foundation. We are part of a global strategic initiative to make better use of our Property & Casualty data and to enhance our ability to make data driven decisions across the Property & Casualty reinsurance value chain.
About You:
You enjoy the challenge of solving complex big data analytics problems using state-of-the-art technologies as part of a growing global team of data engineering professionals. You are a self-starter with strong problem-solving skills and capable of owning and implementing solutions from start to finish. Key qualifications include:
Bachelor's degree level or equivalent in Computer Science, Data Science or similar discipline
At least 12 years of experience working with large scale software systems
At least 6 years of experience in Pyspark and Proficient in designing Large Scale Data Engineering solutions
Minimum of 2 years of experience with Palantir Foundry, including familiarity with tools such as code repositories and Workshop.
Proficient in SQL (Spark SQL preferred)
Experience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)
Experience with TypeScript/JavaScript/HTML/CSS a plus
Knowledge of data management fundamentals and data warehousing principals
Demonstrated strength in data modelling, ETL and storage/Data Lake development
Experience with Scrum/Agile development methodologies
Knowledge of Insurance Domain, Financial Industry or Finance function in other industries is a strong plus
Experienced in working with a diverse multi-location team of internal and external professionals
Strong analytical and problem-solving skills
Self-starter with a positive attitude and a willingness to learn
Ability to manage own workload self-directed
Ability and enthusiasm to work in a global and multicultural environment
Strong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments
Keywords:
Reference Code:133975","Data Lake development, Palantir Foundry, Pyspark, Spark SQL, Hadoop, CSS, Agile Development Methodologies, Sql, HTML, Typescript, Hive, Javascript, Spark, Etl"
Senior Data Engineer,Swiss Re,12-14 Years,,"Bengaluru, India",Consulting/Advisory Services,"As a Senior Data Engineer, you will be responsible for designing and implementing complex data pipelines and analytics solutions to support key decision-making business processes in our Property & Casualty business domain. You will gain exposure to a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re Property & Casualty.
You will be expected to take end-to-end ownership of deliverables, gaining a full understanding of the Property & Casualty data and the business logic required to deliver analytics solutions.
Key responsibilities include:
Work closely with Product Owners and Architects to understand requirements, formulate solutions, and evaluate the implementation effort.
Design, develop and maintain scalable data transformation pipelines.
Data model Design, Data architecture implementation
Working with Palantir platform for implementation
Evaluate new capabilities of the analytics platform, develop prototypes and assist in drawing Development of single source of truth about our application landscape.
Collaborate within a global development team to design and deliver solutions
Assist stakeholders with data-related functional and technical issues
Working with data governance platform for data management and stewardship.
About the Team:
This position is part of the Property & Casualty Data Integration and Analytics project within the Reinsurance Data office team under Data & Foundation. We are part of a global strategic initiative to make better use of our Property & Casualty data and to enhance our ability to make data driven decisions across the Property & Casualty reinsurance value chain.
About You:
You enjoy the challenge of solving complex big data analytics problems using state-of-the-art technologies as part of a growing global team of data engineering professionals. You are a self-starter with strong problem-solving skills and capable of owning and implementing solutions from start to finish. Key qualifications include:
Bachelor's degree level or equivalent in Computer Science, Data Science or similar discipline
At least 12 years of experience working with large scale software systems
At least 6 years of experience in Pyspark and Proficient in designing Large Scale Data Engineering solutions
Minimum of 2 years of experience with Palantir Foundry, including familiarity with tools such as code repositories and Workshop.
Proficient in SQL (Spark SQL preferred)
Experience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)
Experience with TypeScript/JavaScript/HTML/CSS a plus
Knowledge of data management fundamentals and data warehousing principals
Demonstrated strength in data modelling, ETL and storage/Data Lake development
Experience with Scrum/Agile development methodologies
Knowledge of Insurance Domain, Financial Industry or Finance function in other industries is a strong plus
Experienced in working with a diverse multi-location team of internal and external professionals
Strong analytical and problem-solving skills
Self-starter with a positive attitude and a willingness to learn
Ability to manage own workload self-directed
Ability and enthusiasm to work in a global and multicultural environment
Strong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments
Keywords:
Reference Code:133972","Data Lake development, Palantir Foundry, Typescript, Sql, Scrum, Agile, HTML, CSS, Hadoop, Data Modelling, Pyspark, Etl, Hive, Spark SQL, Javascript, Spark"
Data Engineer,NTT Data,4-6 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324632
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","snowflake, Google DataProc, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Solr, Databricks, Java, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324609
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Azure Data Engineer, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,NTT Data,Fresher,,"Bengaluru, India",IT/Computers - Software,"Req ID:321499
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties: . Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.
. Work closely with Data modeller to ensure data models support the solution design
. Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.
. Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.
. Develop documentation and artefacts to support projects
Minimum Skills Required: . ADF
. Fivetran (orchestration & integration)
. SQL
. Snowflake DWH
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.",
Data Engineer,NTT Data,Fresher,,"Bengaluru, India",IT/Computers - Software,"Req ID:321498
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties: . Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.
. Work closely with Data modeller to ensure data models support the solution design
. Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.
. Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.
. Develop documentation and artefacts to support projects
Minimum Skills Required: . ADF
. Fivetran (orchestration & integration)
. SQL
. Snowflake DWH
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.",
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324653
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Azure Data Engineer, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,Nineleaps Technology Solutions Private Limited,2-4 Years,,Bengaluru,"Retail Technology, Health Care, Banking, Finance, Manufacturing, Logistics","We are looking for a Data Engineer to join our team.
Primary Responsibilities:
Experience in Big data Distributed ecosystems:- Hadoop, Hive
Excellent knowledge of HQL/PrestoQL:- Optimisations, complex aggregations, performance tuning
Experience building data processing frameworks, and big data pipelines.
Solid understanding of DWH architecture, ELT/ETL processes, and data structures
Basic Understanding of Python:- ETL
Keep the SLA of maintained datasets(Tier 1, 2) to above 99%. (On-call)
Maintain the SLAs for Tier 3 tables as decided
Desired Skills:
StrongSQL Experience,
Hadoop,
Hive,
Spark/PySpark,
ETL","Spike, Hive, Hadoop, Py Spark, Sql, Etl"
Data Engineer,Tech Tammina,2-5 Years,,Remote,Information Technology,"Must be strong with Python for ML pipelines specifically with Pytorch and scikit-learn AWS is required, building pipelines within Should have a background in LLM (langchain, agents, extensive prompt engineering)
The strong additional requirements below are required.
Responsibilities:
Ingesting, structuring and analyzing a wide range of unstructured datasources
Designing, maintaining and orchestrating data pipelines in an AWS environment for production processing and training flows
Continuously evaluate, analyze, test and improve the quality, privacy and performance of our data systems
Contribute across the product, where - from front-end UX and product design, API/systems architecture and ML processing/training
Minimum Qualifications:
3+ years of experience ingesting, analyzing and structuring a wide variety of datasources
Significant experience building and maintaining data pipelines in a production environment
Strong database/SQL, python, pandas (or equivalent) experience
Prior experience working in fast paced environments and tackling problems across the stack with quick iterations while maintaining a high quality bar.
Strong Additional Qualifications:
Significant healthcare data experience
LLM experience (langchain, agents, extensive prompt engineering)
MLE Experience - pytorch, scikit-learn, etc.
Extensive production AWS, container and/or data orchestration experience
Fullstack development experience (JS/TS/Node in particular)
Demonstrated experience in similar roles in a startup or consultancy","Training, Product Design, Consultancy, Healthcare, Front End, Orchestration, Sql Database, Javascript, Python, AWS"
Data Modeler - Principal Data Engineer - R01548372,Brillio,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Principal Data Engineer
Primary Skills
Data Modeling (ER Studio/ER Win)
SpecializationJob requirements
About Brillio:
Brillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.
Brillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.
10+ years of overall IT experience with more than 2+ years of experience in Snowflake with hands on experience in modelling
In-depth understanding of Data Lake/ODS, ETL concept and modeling structure principles.
Ability to partner closely with the clients to deliver and drive Physical and logical model solutions
Experience in Data warehousing - Dimensions, Facts, and Data modeling.
Excellent communication and ability to lead teams.
Good to have exposure to AWS eco systems.
Excellent communication and ability to lead teams. Technical Skills
Data Modeling concepts (Advanced Level)
Modeling experience across large volume-based environments
Experience with ER Studio
Overall understanding of Database space (Databases, Data Warehouses, Reporting, Analytics)
Well versed with Data Modeling platforms like SQLDBM etc
Strong skills in Entity Relationship Modeling
Good knowledge about Database Designing Administration, Advanced Data Modeling for Star Schema design
Good understanding of Normalized Denormalized Star Snowflake design concepts SQL query development for investigation and analysis","Entity Relationship Modeling, SQL query development, snowflake, SQLDBM, Advanced Data Modeling, Star Schema Design, Data Modeling, Data Warehousing, Database Designing, Data Lake, Etl, Er Studio"
IN_Senior Associate_ AWS Data Engineer_Advisory Corporate_Advisory_ Bangalore,PwC India,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
SAP
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.
Those in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Python,
Py Spark,
Sql,
Redshift ,
S3 ,
Cloud Watch,
Lambda,
AWS Glue
EMR
Step Function
Databricks
Having knowledge on visulalization tool will add value
Experience
Should have worked in technical delivery of above services preferable in similar organizations and having good communication skills
Location
Bangalore
Academics
Should be engineer (Preferably Comp Science)
Certifications
Preference of AWS Data Engineer Certification
Mandatory Skills: AWS Python pyspark Databricks
Preferred Skills: AZURE
Years of Experience: 5-7
Education Qualification-Btech MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Technology
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
AWS Devops
Optional Skills
Acceptance Test Driven Development (ATDD), Acceptance Test Driven Development (ATDD), Accepting Feedback, Active Listening, Analytical Thinking, API Management, Application Development, Application Frameworks, Application Lifecycle Management, Application Software, Business Process Improvement, Business Process Management (BPM), Business Requirements Analysis, C++ Programming Language, Client Management, Code Review, Coding Standards, Communication, Computer Engineering, Computer Science, Continuous Integration/Continuous Delivery (CI/CD), Creativity, Debugging, Embracing Change, Emotional Regulation + 30 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Step Function, S3, AWS Glue, Emr, Cloud Watch, Redshift, Sql, Lambda, Py Spark, Databricks, Python, AWS"
Data Engineer - SAP HANA and Snow Flake,Ericsson,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Join our Team
About this opportunity:
Ericsson's Automation Chapter is seeking a highly motivated and self-driven Data Engineer and Senior Data Engineer with strong expertise in SAP HANA and SAP BODS. The ideal candidates will be focused on SAP-centric development and integration, ensuring that enterprise data flows are robust, scalable, and optimized for analytics consumption. You will collaborate with a high-performing team that builds and supports end-to-end data solutions aligned with our SAP ecosystem. You are adaptable and a flexible problem-solver with deep hands-on experience in HANA modeling and ETL workflows, capable of switching contexts across a range of projects with varying scale and complexity.
What you bring:
Design, develop, and optimize SAP HANA objects such as Calculation Views, SQL Procedures, and Custom Functions.
Develop robust and reusable ETL pipelines using SAP BODS for both SAP and third-party system integration.
Enable seamless data flow between SAP ECC and external platforms, ensuring accuracy and performance.
Collaborate with business analysts, architects, and integration specialists to translate requirements into technical deliverables.
Tune and troubleshoot HANA and BODS jobs for performance, scalability, and maintainability.
Ensure compliance with enterprise data governance, lineage, and documentation standards.
Support ongoing enhancements, production issues, and business-critical data deliveries.
Experience
8+ years of experience in SAP data engineering roles.
Strong hands-on experience in SAP HANA (native development, modeling, SQL scripting).
Proficient in SAP BODS, including job development, data flows, and integration techniques.
Experience working with SAP ECC data structures, IDOCs, and remote function calls.
Knowledge of data warehouse concepts, data modeling, and performance optimization techniques.
Strong debugging and analytical skills, with the ability to independently drive technical solutions.
Familiarity with version control tools and SDLC processes.
Excellent communication skills and ability to work collaboratively in cross-functional teams.
Education
Bachelor's degree in computer science, Information Systems, Electronics & Communication, or a related field.
Why join Ericsson
At Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.
What happens once you apply
Click Here to find all you need to know about what our typical hiring process looks like.
Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.
Primary country and city: India (IN) || Bangalore
Req ID: 765249","performance optimization, SDLC processes, ETL pipelines, Idocs, Sap Hana, Sap Bods, Data Modeling, Version Control Tools, Sap Ecc, Sql Scripting"
"Senior Data Engineer, Global Converse, ITC",Nike,6-8 Years,,India,Login to check your skill match score,"Who You'll Work With
You will sit on the Converse Enterprise Data & Analytics technology team, reporting to the Enterprise Data & Analytics Director based at Converse HQ. You will provide people leadership for data engineers and scrum leads who are also based at ITC. Furthermore, you will be part of an agile scrum team, working closely with data product managers, lead engineers, and other cross-functional team members to deliver data and analytics capabilities for Converse.
Who We Are Looking For
The Data Engineering Senior Supervisor possesses a unique blend of technical expertise, leadership acumen, and strategic vision. They oversee the development and maintenance of large-scale data systems, ensuring data quality, integrity, and security. From a leadership perspective, they have experience in managing and mentoring teams of data engineers, providing guidance on technical best practices, and fostering a culture of innovation and collaboration. They are skilled in agile project management methodologies and have a proven track record of delivering complex data engineering projects on time and within budget.
The Data Engineering Senior Supervisor needs to have a deep understanding of the organization's data strategy and is able to align their team's efforts with business objectives. Their expertise enables them to communicate effectively with stakeholders, support technical decision-making, and drive business growth through data-driven insights.
Experienced in building cloud-scalable, real-time, and high-performance data lake solutions using Databricks, Snowflake, and/or AWS, with proficiency in relational SQL, scripting languages (Shell, Python), and source control tools (GitHub).
Skilled in developing complex data solutions, with a strong understanding of end-to-end design, data structures, and algorithms. Proficient in workflow scheduling tools (Matillion, Brickflow) and familiar with Terraform, Serverless, and Jenkins.
Proven ability to lead and influence others, with experience in agile team leadership and influencing stakeholders. Effective communicator, both verbally and in writing, with the ability to identify and resolve people and process-related issues.
Strong problem-solving and analytical mindset, with the ability to take a broad perspective to identify innovative solutions. Willing to learn new skills and technologies, and able to quickly pick up new programming languages, technologies, and frameworks.
Ability to understand business needs and develop technical solutions to meet them. Strong understanding of solution and technical design.
Minimum 6 years relevant work experience; Bachelor's degree or equivalent combination of education, experience or training
What You'll Work On
As a Data Engineering Senior Supervisor on the Enterprise Data & Analytics team, you will drive work to completion through leading others along with hands-on development responsibilities. You will deliver quality components of a larger data pipeline to support data analytics products with guidance from experienced peers while working within an agile framework that focuses on delivering quality incremental value. As a Senior Supervisor, you will also be responsible for providing people leadership to data engineers, automation engineers, and scrum leads.
Be responsible for the team's delivery of scalable data solutions to solve a customer need
Guide the team in troubleshooting data issues and performing root cause analysis to proactively resolve product issues to improve data quality
Manage, mentor, and inspire a team; managing performance, goals and development potential
Build reusable components, frameworks and libraries at scale to support analytics products
Clean, prepare and optimize data for ingestion and consumption by implementing automated workflows
Collaborate on the implementation of new data management projects and re-structure of the current data architecture
Build continuous integration, test-driven development and production deployment frameworks
Collaboratively review design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards","Matillion, Brickflow, snowflake, Serverless, Github, Sql, Jenkins, Shell, Terraform, Databricks, Python, AWS"
IN_Senior Associate_ AWS Data Engineer_Advisory Corporate_Advisory_ Bangalore,PwC India,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
SAP
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.
Those in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Python,
Py Spark,
Sql,
Redshift ,
S3 ,
Cloud Watch,
Lambda,
AWS Glue
EMR
Step Function
Databricks
Having knowledge on visulalization tool will add value
Experience
Should have worked in technical delivery of above services preferable in similar organizations and having good communication skills
Location
Bangalore
Academics
Should be engineer (Preferably Comp Science)
Certifications
Preference of AWS Data Engineer Certification
Mandatory Skills: AWS Python pyspark Databricks
Preferred Skills: AZURE
Years of Experience: 5-7
Education Qualification-Btech MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Technology
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
AWS Devops
Optional Skills
Acceptance Test Driven Development (ATDD), Acceptance Test Driven Development (ATDD), Accepting Feedback, Active Listening, Analytical Thinking, API Management, Application Development, Application Frameworks, Application Lifecycle Management, Application Software, Business Process Improvement, Business Process Management (BPM), Business Requirements Analysis, C++ Programming Language, Client Management, Code Review, Coding Standards, Communication, Computer Engineering, Computer Science, Continuous Integration/Continuous Delivery (CI/CD), Creativity, Debugging, Embracing Change, Emotional Regulation + 30 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Step Function, S3, AWS Glue, Emr, Cloud Watch, Redshift, Sql, Lambda, Py Spark, Databricks, Python, AWS"
Data Engineer (Talend &Pyspark),NTT Data,4-6 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:321800
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer (Talend &Pyspark) to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties: Key Responsibilities:
. Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
. Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
. Demonstrate proficiency in coding skills, utilizing languages such as PySpark, Talend to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
. Collaborate seamlessly across diverse technical stacks, including AWS.
. Develop and deliver detailed presentations to effectively communicate complex technical concepts.
. Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
. Adhere to Agile practices throughout the solution development process.
. Design, build, and deploy databases and data stores to support organizational requirements.
Minimum Skills Required: Basic Qualifications:
. 4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
. Ability to travel at least 25%.
Preferred Skills:
. Demonstrate production experience in core data platforms such as AWS.
. Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in S3 or other NoSQL storage systems.
. Exhibit a strong understanding of Data integration technologies, encompassing Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services.
. Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
. Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","NoSQL storage systems, NiFi, AWS Data Migration Services, Streamsets, Kafka, AWS, S3, Talend, Pyspark, Spark"
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324609
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Azure Data Engineer, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Sr. Data Engineer,NTT Data,7-9 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:309994
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Sr. Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Primary Responsibility as Data Engineer
1) Build and implement PySpark-based data pipelines in Azure Synapse to transform and load data into ADLS in Delta format
2) Design and implement dimensional (star/snowflake) and 3NF data models. Optimized for access using Power BI
3) Perform unit testing of data pipelines and its transformations
4) Design and build metadata driven data pipelines using Pyspark in Azure Synapse
5) Analyze and optimize Spark SQL queries
6) Optimize integration of data lake with Power BI semantic model
7) Collaborate with cross-functional teams (data architects, engineers, and POs) to ensure data models align with business needs
8) Perform STM(source to target mapping) from source to multiple layers in the data lake
9) Maintain version control and CI/CD pipelines in Git and Azure DevOps
10) Integrate Azure Purview to enable access controls. Additionally implement row level security
7+ Years of experience in : SQL, PySpark
. Hands-on experience with Azure Synapse, ADLS, Delta format, and metadata-driven data pipelines
. Experienced in implementing dimensional (star/snowflake) and 3NF data models
.Experienced in PySpark and Spark SQL, including query optimization and performance tuning
. Experienced in writing complex SQL including windowing functions
. Experienced in performing Source-to-Target Mapping (STM) by analyzing source systems and defining transformation logic
. Strong problem-solving and analytical skills for debugging and optimizing data pipelines in Azure Synapse
. Familiarity with CI/CD practices in Git and Azure DevOps
. Working experience in Azure DevOps-based development environment
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Delta format, 3NF data models, windowing functions, ADLS, Azure Purview, Sql, Spark SQL, Azure Synapse, Git, Pyspark, Azure DevOps"
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324612
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Kudu, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,NTT Data,4-6 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324631
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Kudu, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
"Financial Data Engineer, Investments, Associate",BlackRock,3-6 Years,,"Mumbai, India",Login to check your skill match score,"About This Role
At BlackRock, we are looking for a Data Engineer who enjoys building and supporting high impact data pipelines to solve complex challenges while working closely with your colleagues throughout the business.
We recognize that strength comes from diversity, and will embrace your outstanding skills, curiosity, drive, and passion while giving you the opportunity to grow technically while learning from hands-on leaders in technology and finance.
With over USD $11 trillion of assets we have an outstanding responsibility: our technology empowers millions of investors to save for retirement, pay for college, buy a home and improve their financial wellbeing.
Being a financial technologist at BlackRock means you get the best of both worlds: working for one of the most successful financial companies and also working in a software development team responsible for next generation technology and solutions.
We are seeking a high-reaching individual to help implement financial data engineering projects, initially focusing on our Index Fixed Income Group for the BGM DnA (Data and Analytics) team in India. We are a community of highly qualified Data Engineers, Content & DevOps Specialists who have a passion for working on data solutions that help drive the agenda for our business partners
Our team is based in San Francisco, London & Hungary, and we will complete the global circle with a new engineering team in Mumbai.
About BlackRock Global Markets
BlackRock Global Markets (BGM) functions are at the core of BlackRock's markets and investments platform, including ETF and Index Investments (Engine), Global Trading, Securities Lending, Fixed Income, Liquidity and Financing. BGM is passionate about advancing the investment processes and platform architecture in these areas and on ensuring we engage with other market participants in a collaborative, strategic way.
You should be
Someone who is passionate about solving sophisticated business problems through data!
Capable of the design, implementation, and optimization of data pipelines, ETL processes, and data storage solutions
Able to work closely with multi-functional teams (e.g., Data Science, Product, Analytics, and Citizen Developer teams) to ensure the data infrastructure meets business needs.
Enthusiastic about establishing and maintaining standard methodologies for data engineering, focusing on data quality, security, and scalability.
Key Requirements
3-6 years Data Engineering experience preferably in the financial sector
Familiarity with any aspect of Fixed Income Index and Market Data including ICE, Bloomberg, JP Morgan, FTSE/Russell, and IBOXX. Liquidity, Venue, and Direct Broker Dealer Market Maker Axe Data. Pricing Data from sources like S&P Global Live Bond Pricing or Bloombergs IBVAL.
Understand Portfolio Management Fundamentals: Asset Management and FI Trading.
A passion for Financial and Capital Markets.
Proven experience working in an agile development team.
Strong problem solving skills.
Strong SQL and Python skills with a proven track record optimizing SQL queries.
Curiosity of financial markets.
Good To Have
Bachelor's degree in Computer Science, Engineering, Finance, Economics, or a related field. A Master's degree or equivalent experience is a plus.
Knowledge of Linux and scripting languages such as Bash
Experience with MySQL, PostgreSQL, Greenplum, Snowflake or similar databases.
Strong experience with ETL/ELT tools like DBT, Pentaho, Informatica or similar technologies.
Experience with DevOps and tools like Azure DevOps
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","dbt, snowflake, Pentaho, PostgreSQL, Bash, Greenplum, Informatica, Sql, ELT, Linux, MySQL, Python, Azure DevOps, Etl"
Data Engineer,NTT Data,3-5 Years,,"Chennai, India",IT/Computers - Software,"Req ID:303790
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).
Job Duties: As a data engineer you will have a strong background in data engineering, an ability to interpret complex datasets, and the skills to provide actionable insights.
- Works closely with the GenAI architect to design and implement automated data pipelines
- Analyzes and understands source data and prepares and manages data sets
- Supports CI/CD efforts
Extract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.
. Perform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.
. Develop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.
. Collaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.
. Continuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.
Minimum Skills Required: 3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].
3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Preferred Skills:
. Strong proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].
. Experience with cloud data platforms such as [Platforms - e.g., AWS Redshift, Google BigQuery, Azure Synapse].
. Familiarity with ETL (Extract, Transform, Load) processes and tools.
. Knowledge of machine learning techniques and tools.
. Experience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.
. Understanding of data governance and data privacy regulations.
. Ability to query and manipulate databases and data warehouses.
. Excellent analytical and problem-solving skills.
. Strong communication skills with the ability to explain complex data insights to non-technical stakeholders.
. Detail-oriented with a commitment to accuracy.
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","R, Google BigQuery, Excel, Sql, AWS, Etl, Python, Azure Synapse, Redshift"
"Data Engineer - ADF, Data Bricks",Zensar Technologies,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Description
Description of role and key responsibilities
The candidate will be required to deliver to all stages of the data engineering process data ingestion, transformation, data modelling and data warehousing, and build self-service data products. The role is a mix of Azure cloud delivery and on-prem (SQL) development. Ultimately all on-prem will be migrated to cloud and decommissioned but we are only part way along that journey.
There will be a dual reporting line between the main business technology area (Asset Lending) providing the day-to-day direction and management of work items, and the Head of Data for Corporate Banking Technology who will provide guidance on overall Data strategy and alignment with wider bank.
The role itself will work closely with our Architect, Engineering lead, Analytics team, DevOps, DBAs, and upstream Application teams in Asset Finance, Working Capital and ABL.
Specifically, The Person Will
Work closely with end-users and Data Analysts to understand the business and their data requirements
Carry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks
Building dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks
Build and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)
Build and maintain business focused data products and data marts
Build and maintain Azure Analysis Services databases and cubes
Share support and operational duties within the wider engineering and data teams
Work with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.
Help define test criteria to establish clear conditions for success and ensure alignment with business objectives.
Manage their user stories and acceptance criteria through to production into day-to-day support
Assist in the testing and validation of new requirements and processes to ensure they meet business needs
Stay up-to-date with industry trends and best practices in data engineering
Core skills and knowledge
Excellent data analysis and exploration using T-SQL
Strong SQL programming (stored procedures, functions)
Extensive experience with SQL Server and SSIS
Knowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)
Experience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2
Experience in building robust and performant ETL processes
Build and maintain Analysis Services databases and cubes (both multidimensional and tabular)
Experience in using source control & ADO
Understanding and experience of deployment pipelines
Excellent analytical and problem-solving skills, with the ability to think critically and strategically.
Strong communication and interpersonal skills, with the ability to engage and influence stakeholders at all levels.
To always act with integrity and embrace the philosophy of treating our customers fairly
Analytical, ability to arrive at solutions that fit current / future business processes
Effective writing and verbal communication
Organisational skills: Ability to effectively manage and co-ordinate themselves.
Ownership and self-motivation
Delivery focus
Assertive, resilient and persistent
Team oriented
Deal well with pressure and highly effective at multi-tasking and juggling priorities
Any other attributes that would be helpful, but not essential for the role.
Deeper programming ability (C#, .Net Core)
Build infrastructure-as-code deployment pipelines
Asset Finance knowledge
Vehicle Finance knowledge
ABL and Working Capital knowledge
Any financial services and banking experience","Analysis Services, Synapse Analytics, Data Vault 2.0, Kimball dimensional modelling, Azure Data Factory, T-sql, SQL Server, Databricks, ADO, SSIS"
Senior Data Engineer,HCLTech,6-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role : Senior Data Engineer.
Experience : 6 to 10 years.
Location : Bengaluru/Chennai/Hyderabad/Pune/Delhi NCR.
Notice Period : Immediate to 45 Days.
Business domain knowledge : SaaS, SFDC, NetSuite.
Areas we support : Product, Finance (ARR reporting), GTM, Marketing, Sales.
Tech Stack : Fivetran, Snowflake, DBT, Tableau, GitHub.
What we are looking for:
1. SQL and data modeling at intermediate level - write complex SQL queries, build data model and experience with data transformation.
2. Problem solver: Person who can weed through ambiguity of the ask.
3. Bias for Action: Asks questions, reaches out to stakeholders, comes up with solutions.
4. Communication: Effectively communicates with stakeholder and team members.
5.Documentation: Can create BRD.
6. Someone well versed in Finance (ARR reporting) and/or GTM (sales and marketing) would be an added advantage.
7. Experience in SAAS, NetSuite and Salesforce will be a plus.
8. Independent, self-starter, motivated and experience with working in an onsite/offshore environment.","dbt, snowflake, Fivetran, Github, Data Transformation, Tableau, Data Modeling, Sql"
"Data Engineer - ADF, Data Bricks",Zensar Technologies,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Description
Description of role and key responsibilities
The candidate will be required to deliver to all stages of the data engineering process data ingestion, transformation, data modelling and data warehousing, and build self-service data products. The role is a mix of Azure cloud delivery and on-prem (SQL) development. Ultimately all on-prem will be migrated to cloud and decommissioned but we are only part way along that journey.
There will be a dual reporting line between the main business technology area (Asset Lending) providing the day-to-day direction and management of work items, and the Head of Data for Corporate Banking Technology who will provide guidance on overall Data strategy and alignment with wider bank.
The role itself will work closely with our Architect, Engineering lead, Analytics team, DevOps, DBAs, and upstream Application teams in Asset Finance, Working Capital and ABL.
Specifically, The Person Will
Work closely with end-users and Data Analysts to understand the business and their data requirements
Carry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks
Building dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks
Build and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)
Build and maintain business focused data products and data marts
Build and maintain Azure Analysis Services databases and cubes
Share support and operational duties within the wider engineering and data teams
Work with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.
Help define test criteria to establish clear conditions for success and ensure alignment with business objectives.
Manage their user stories and acceptance criteria through to production into day-to-day support
Assist in the testing and validation of new requirements and processes to ensure they meet business needs
Stay up-to-date with industry trends and best practices in data engineering
Core skills and knowledge
Excellent data analysis and exploration using T-SQL
Strong SQL programming (stored procedures, functions)
Extensive experience with SQL Server and SSIS
Knowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)
Experience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2
Experience in building robust and performant ETL processes
Build and maintain Analysis Services databases and cubes (both multidimensional and tabular)
Experience in using source control & ADO
Understanding and experience of deployment pipelines
Excellent analytical and problem-solving skills, with the ability to think critically and strategically.
Strong communication and interpersonal skills, with the ability to engage and influence stakeholders at all levels.
To always act with integrity and embrace the philosophy of treating our customers fairly
Analytical, ability to arrive at solutions that fit current / future business processes
Effective writing and verbal communication
Organisational skills: Ability to effectively manage and co-ordinate themselves.
Ownership and self-motivation
Delivery focus
Assertive, resilient and persistent
Team oriented
Deal well with pressure and highly effective at multi-tasking and juggling priorities
Any other attributes that would be helpful, but not essential for the role.
Deeper programming ability (C#, .Net Core)
Build infrastructure-as-code deployment pipelines
Asset Finance knowledge
Vehicle Finance knowledge
ABL and Working Capital knowledge
Any financial services and banking experience","Analysis Services, Synapse Analytics, Data Vault 2.0, Kimball dimensional modelling, Azure Data Factory, T-sql, SQL Server, Databricks, ADO, SSIS"
"Financial Data Engineer, Investments, Associate",BlackRock,3-6 Years,,"Mumbai, India",Login to check your skill match score,"About This Role
At BlackRock, we are looking for a Data Engineer who enjoys building and supporting high impact data pipelines to solve complex challenges while working closely with your colleagues throughout the business.
We recognize that strength comes from diversity, and will embrace your outstanding skills, curiosity, drive, and passion while giving you the opportunity to grow technically while learning from hands-on leaders in technology and finance.
With over USD $11 trillion of assets we have an outstanding responsibility: our technology empowers millions of investors to save for retirement, pay for college, buy a home and improve their financial wellbeing.
Being a financial technologist at BlackRock means you get the best of both worlds: working for one of the most successful financial companies and also working in a software development team responsible for next generation technology and solutions.
We are seeking a high-reaching individual to help implement financial data engineering projects, initially focusing on our Index Fixed Income Group for the BGM DnA (Data and Analytics) team in India. We are a community of highly qualified Data Engineers, Content & DevOps Specialists who have a passion for working on data solutions that help drive the agenda for our business partners
Our team is based in San Francisco, London & Hungary, and we will complete the global circle with a new engineering team in Mumbai.
About BlackRock Global Markets
BlackRock Global Markets (BGM) functions are at the core of BlackRock's markets and investments platform, including ETF and Index Investments (Engine), Global Trading, Securities Lending, Fixed Income, Liquidity and Financing. BGM is passionate about advancing the investment processes and platform architecture in these areas and on ensuring we engage with other market participants in a collaborative, strategic way.
You should be
Someone who is passionate about solving sophisticated business problems through data!
Capable of the design, implementation, and optimization of data pipelines, ETL processes, and data storage solutions
Able to work closely with multi-functional teams (e.g., Data Science, Product, Analytics, and Citizen Developer teams) to ensure the data infrastructure meets business needs.
Enthusiastic about establishing and maintaining standard methodologies for data engineering, focusing on data quality, security, and scalability.
Key Requirements
3-6 years Data Engineering experience preferably in the financial sector
Familiarity with any aspect of Fixed Income Index and Market Data including ICE, Bloomberg, JP Morgan, FTSE/Russell, and IBOXX. Liquidity, Venue, and Direct Broker Dealer Market Maker Axe Data. Pricing Data from sources like S&P Global Live Bond Pricing or Bloombergs IBVAL.
Understand Portfolio Management Fundamentals: Asset Management and FI Trading.
A passion for Financial and Capital Markets.
Proven experience working in an agile development team.
Strong problem solving skills.
Strong SQL and Python skills with a proven track record optimizing SQL queries.
Curiosity of financial markets.
Good To Have
Bachelor's degree in Computer Science, Engineering, Finance, Economics, or a related field. A Master's degree or equivalent experience is a plus.
Knowledge of Linux and scripting languages such as Bash
Experience with MySQL, PostgreSQL, Greenplum, Snowflake or similar databases.
Strong experience with ETL/ELT tools like DBT, Pentaho, Informatica or similar technologies.
Experience with DevOps and tools like Azure DevOps
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","dbt, snowflake, Pentaho, PostgreSQL, Bash, Greenplum, Informatica, Sql, ELT, Linux, MySQL, Python, Azure DevOps, Etl"
Data Engineer,NTT Data,3-5 Years,,"Chennai, India",IT/Computers - Software,"Req ID:303790
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).
Job Duties: As a data engineer you will have a strong background in data engineering, an ability to interpret complex datasets, and the skills to provide actionable insights.
- Works closely with the GenAI architect to design and implement automated data pipelines
- Analyzes and understands source data and prepares and manages data sets
- Supports CI/CD efforts
Extract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.
. Perform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.
. Develop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.
. Collaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.
. Continuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.
Minimum Skills Required: 3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].
3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Preferred Skills:
. Strong proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].
. Experience with cloud data platforms such as [Platforms - e.g., AWS Redshift, Google BigQuery, Azure Synapse].
. Familiarity with ETL (Extract, Transform, Load) processes and tools.
. Knowledge of machine learning techniques and tools.
. Experience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.
. Understanding of data governance and data privacy regulations.
. Ability to query and manipulate databases and data warehouses.
. Excellent analytical and problem-solving skills.
. Strong communication skills with the ability to explain complex data insights to non-technical stakeholders.
. Detail-oriented with a commitment to accuracy.
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","R, Google BigQuery, Excel, Sql, AWS, Etl, Python, Azure Synapse, Redshift"
Senior Data Engineer,HCLTech,6-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role : Senior Data Engineer.
Experience : 6 to 10 years.
Location : Bengaluru/Chennai/Hyderabad/Pune/Delhi NCR.
Notice Period : Immediate to 45 Days.
Business domain knowledge : SaaS, SFDC, NetSuite.
Areas we support : Product, Finance (ARR reporting), GTM, Marketing, Sales.
Tech Stack : Fivetran, Snowflake, DBT, Tableau, GitHub.
What we are looking for:
1. SQL and data modeling at intermediate level - write complex SQL queries, build data model and experience with data transformation.
2. Problem solver: Person who can weed through ambiguity of the ask.
3. Bias for Action: Asks questions, reaches out to stakeholders, comes up with solutions.
4. Communication: Effectively communicates with stakeholder and team members.
5.Documentation: Can create BRD.
6. Someone well versed in Finance (ARR reporting) and/or GTM (sales and marketing) would be an added advantage.
7. Experience in SAAS, NetSuite and Salesforce will be a plus.
8. Independent, self-starter, motivated and experience with working in an onsite/offshore environment.","dbt, snowflake, Fivetran, Github, Data Transformation, Tableau, Data Modeling, Sql"
Data Engineer (Talend &Pyspark),NTT Data,4-6 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:321800
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer (Talend &Pyspark) to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties: Key Responsibilities:
. Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
. Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
. Demonstrate proficiency in coding skills, utilizing languages such as PySpark, Talend to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
. Collaborate seamlessly across diverse technical stacks, including AWS.
. Develop and deliver detailed presentations to effectively communicate complex technical concepts.
. Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
. Adhere to Agile practices throughout the solution development process.
. Design, build, and deploy databases and data stores to support organizational requirements.
Minimum Skills Required: Basic Qualifications:
. 4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
. Ability to travel at least 25%.
Preferred Skills:
. Demonstrate production experience in core data platforms such as AWS.
. Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in S3 or other NoSQL storage systems.
. Exhibit a strong understanding of Data integration technologies, encompassing Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services.
. Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
. Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","NoSQL storage systems, NiFi, AWS Data Migration Services, Streamsets, Kafka, AWS, S3, Talend, Pyspark, Spark"
"Associate, Senior Data Engineer",BlackRock,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role
At BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.
This role sits within Preqin, a part of BlackRock. Preqin plays a key role in how we are revolutionizing private markets data and technology for clients globally, complementing our existing Aladdin technology platform to deliver investment solutions for the whole portfolio.
Join the multi-disciplinary Orion team, where innovation meets real business impact. We're revolutionizing data management by harnessing the power of automation, machine learning, and AI to transform how we collect and process data. As part of the Orion team, you'll be at the forefront of innovation, developing and maintaining our cutting-edge workflow orchestration platform. This role offers the opportunity to work with advanced technologies to streamline large-scale data handling from diverse sources, ensuring that data is always accessible, accurate, and relevant to our clients needs.
About The Role
As an Associate Data Engineer, you will be part of a high-performing engineering pod. Your work ensures our data flows are robust, scalable, and aligned with business needs, forming the backbone of Orion's data-driven products and insights. This role requires an individual with excellent technical skills to evolve our back-end infrastructure. Our Data Engineers have an opportunity to significantly accelerate these changes and help shape our organization for the future. You will utilize your expertise to ingest, model, and maintain data that enhances our internal data processing capabilities. As such, we seek an individual who demonstrates efficient and productive ways of processing data as this project increases in scale. We seek an individual who will be instrumental in bridging the gap between technical and non-technical team members, ensuring clear and effective communication of technical concepts and data-driven strategies.
Your Responsibilities Will Include
Design and implement robust data models (dimensional, Data Vault, etc.) that support business intelligence and analytics requirements
Build scalable, reliable ETL/ELT pipelines using Python that process data from multiple sources
Create and maintain analytics-ready datasets with efficient query performance for reporting and business insights
Develop database schemas and optimize database performance for both transactional and analytical workloads
Establish and enforce data governance practices, including data quality standards and metadata management
Prioritise work based on data-driven insights and outcome-based goals in collaboration with stakeholders.
Work closely with engineering teams across the business, ensuring the best technical solutions are adopted, and elevate development standards through knowledge sharing and best practices.
Collaborate across engineering, product, and data scientist teams to translate business requirements into technical solutions and ensuring our data assets are organized and accessible.
Actively participate in technical discussions about new product directions, data modelling, and architectural decisions, ensuring our technology platform remains extensible.
Accelerate data collection as scale from millions of sources and across various databases.
Improve and maintain observability and alerting across our data systems, ensuring visibility into pipeline health and data quality.
Propose smart, pragmatic, and diverse approaches to address a variety of business problems.
Explore new technologies, approaches, and ideas that help to drive our business goals in unexpected ways.
What We Are Looking For
At least 4 years of experience in data engineering with strong database expertise
Experience building ETL/ELT pipelines using Python and implementing efficient data processing workflows
Strong SQL skills with ability to write complex queries for reporting and analytics needs
Experience with data modelling methodologies (dimensional/Kimball, Data Vault) and database schema design
Proficiency working with both transactional and analytical database systems (Postgres, Snowflake, etc.)
Experience of working within cloud provider services Azure or AWS (preferred) and utilisation of infrastructure as code
A data-driven mindset to make development decisions based on robust analyses
Ability to collaborate effectively with designers, engineering and data scientist teams to build our technical solutions
You have driven technical solution design, taking the balance of engineering quality, testing, scalability and security into consideration
A let's do it and challenge accepted attitude when faced with less known or challenging tasks, with a willingness to learn new technologies and ways of working
Excellent verbal and written communication and interpersonal skills, with the ability to influence at all organizational levels and bridge technical perspectives
Proficiency in English required; additional languages and prior work experience at a global firm are desirable
Experience processing structured and unstructured data
Ability to perform well in a fast-paced environment, developing iterative sustainable solutions with best practices (security, code quality, documentation) and long-term vision
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","kimball, snowflake, data vault, Postgres, Azure, Sql, Python, ELT, AWS, Etl"
Data Engineer,NTT Data,4-6 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324638
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).
Key Responsibilities:
Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.
Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.
Demonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.
Collaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.
Develop and deliver detailed presentations to effectively communicate complex technical concepts.
Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.
Adhere to Agile practices throughout the solution development process.
Design, build, and deploy databases and data stores to support organizational requirements.
Basic Qualifications:
4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.
2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.
Experience with Informatica, Python, Databricks, Azure Data Engineer
Ability to travel at least 25%.
Preferred Skills:
Demonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.
Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.
Exhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.
Showcase professional written and verbal communication skills to effectively convey complex technical concepts.
Undergraduate or Graduate degree preferred
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Kudu, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,Havells India Ltd,3-5 Years,,"Noida, India",Login to check your skill match score,"Job Title: Data Engineer
Location: Noida
Experience: 3+ years
Job Description: We are seeking a skilled and experienced Data Engineer to join our dynamic team. The ideal candidate will have a strong background in data engineering, with a focus on PySpark, Python, and SQL. Experience with Azure Databricks is a plus.
Key Responsibilities:
Design, develop, and maintain scalable data pipelines and systems.
Work closely with data scientists and analysts to ensure data quality and availability.
Implement data integration and transformation processes using PySpark and Python.
Optimize and maintain SQL databases and queries.
Collaborate with cross-functional teams to understand data requirements and deliver solutions.
Monitor and troubleshoot data pipeline issues to ensure data integrity and performance.
Required Skills and Qualifications:
Bachelor's degree in Computer Science, Information Technology, or a related field.
3+ years of experience in data engineering.
Proficiency in PySpark, Python, and SQL.
Experience with Azure Databricks is a plus.
Strong problem-solving skills and attention to detail.
Excellent communication and teamwork abilities.
Preferred Qualifications:
Experience with cloud platforms such as Azure, AWS, or Google Cloud.
Knowledge of data warehousing concepts and technologies.
Familiarity with ETL tools and processes.
How to Apply: Apart from Easy apply on Linkedin also Click on this link https://forms.office.com/r/N0nYycJ36P
#DataEngineer #Hiring #JobOpening #PySpark #Python #SQL #AzureDatabricks #TechJobs #DataEngineering #CareerOpportunity","Pyspark, Azure Databricks, Python, Sql"
Senior Data Engineer,Ericsson,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Join our Team
About this opportunity:
Ericsson's Automation Chapter is seeking a highly motivated and self-driven Data Engineer and Senior Data Engineer with strong expertise in SAP HANA and SAP BODS. The ideal candidates will be focused on SAP-centric development and integration, ensuring that enterprise data flows are robust, scalable, and optimized for analytics consumption. You will collaborate with a high-performing team that builds and supports end-to-end data solutions aligned with our SAP ecosystem. You are adaptable and a flexible problem-solver with deep hands-on experience in HANA modeling and ETL workflows, capable of switching contexts across a range of projects with varying scale and complexity.
What you bring:
Design, develop, and optimize SAP HANA objects such as Calculation Views, SQL Procedures, and Custom Functions.
Develop robust and reusable ETL pipelines using SAP BODS for both SAP and third-party system integration.
Enable seamless data flow between SAP ECC and external platforms, ensuring accuracy and performance.
Collaborate with business analysts, architects, and integration specialists to translate requirements into technical deliverables.
Tune and troubleshoot HANA and BODS jobs for performance, scalability, and maintainability.
Ensure compliance with enterprise data governance, lineage, and documentation standards.
Support ongoing enhancements, production issues, and business-critical data deliveries.
Experience
8+ years of experience in SAP data engineering roles.
Strong hands-on experience in SAP HANA (native development, modeling, SQL scripting).
Proficient in SAP BODS, including job development, data flows, and integration techniques.
Experience working with SAP ECC data structures, IDOCs, and remote function calls.
Knowledge of data warehouse concepts, data modeling, and performance optimization techniques.
Strong debugging and analytical skills, with the ability to independently drive technical solutions.
Familiarity with version control tools and SDLC processes.
Excellent communication skills and ability to work collaboratively in cross-functional teams.
Education
Bachelor's degree in computer science, Information Systems, Electronics & Communication, or a related field.
Why join Ericsson
At Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.
What happens once you apply
Click Here to find all you need to know about what our typical hiring process looks like.
Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.
Primary country and city: India (IN) || Bangalore
Req ID: 763290","performance optimization, SDLC processes, ETL pipelines, Idocs, Sap Hana, Sap Bods, Data Modeling, Version Control Tools, Sql Scripting, Sap Ecc"
Data Modeler - Principal Data Engineer - R01548372,Brillio,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Principal Data Engineer
Primary Skills
Data Modeling (ER Studio/ER Win)
SpecializationJob requirements
About Brillio:
Brillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.
Brillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.
10+ years of overall IT experience with more than 2+ years of experience in Snowflake with hands on experience in modelling
In-depth understanding of Data Lake/ODS, ETL concept and modeling structure principles.
Ability to partner closely with the clients to deliver and drive Physical and logical model solutions
Experience in Data warehousing - Dimensions, Facts, and Data modeling.
Excellent communication and ability to lead teams.
Good to have exposure to AWS eco systems.
Excellent communication and ability to lead teams. Technical Skills
Data Modeling concepts (Advanced Level)
Modeling experience across large volume-based environments
Experience with ER Studio
Overall understanding of Database space (Databases, Data Warehouses, Reporting, Analytics)
Well versed with Data Modeling platforms like SQLDBM etc
Strong skills in Entity Relationship Modeling
Good knowledge about Database Designing Administration, Advanced Data Modeling for Star Schema design
Good understanding of Normalized Denormalized Star Snowflake design concepts SQL query development for investigation and analysis","Entity Relationship Modeling, SQL query development, snowflake, SQLDBM, Advanced Data Modeling, Star Schema Design, Data Modeling, Data Warehousing, Database Designing, Data Lake, Etl, Er Studio"
IN_Senior Associate_ AWS Data Engineer_Advisory Corporate_Advisory_ Bangalore,PwC India,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
SAP
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.
Those in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Python,
Py Spark,
Sql,
Redshift ,
S3 ,
Cloud Watch,
Lambda,
AWS Glue
EMR
Step Function
Databricks
Having knowledge on visulalization tool will add value
Experience
Should have worked in technical delivery of above services preferable in similar organizations and having good communication skills
Location
Bangalore
Academics
Should be engineer (Preferably Comp Science)
Certifications
Preference of AWS Data Engineer Certification
Mandatory Skills: AWS Python pyspark Databricks
Preferred Skills: AZURE
Years of Experience: 5-7
Education Qualification-Btech MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Technology
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
AWS Devops
Optional Skills
Acceptance Test Driven Development (ATDD), Acceptance Test Driven Development (ATDD), Accepting Feedback, Active Listening, Analytical Thinking, API Management, Application Development, Application Frameworks, Application Lifecycle Management, Application Software, Business Process Improvement, Business Process Management (BPM), Business Requirements Analysis, C++ Programming Language, Client Management, Code Review, Coding Standards, Communication, Computer Engineering, Computer Science, Continuous Integration/Continuous Delivery (CI/CD), Creativity, Debugging, Embracing Change, Emotional Regulation + 30 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Step Function, S3, AWS Glue, Emr, Cloud Watch, Redshift, Sql, Lambda, Py Spark, Databricks, Python, AWS"
"Senior Data Engineer, Global Converse, ITC",Nike,6-8 Years,,India,Login to check your skill match score,"Who You'll Work With
You will sit on the Converse Enterprise Data & Analytics technology team, reporting to the Enterprise Data & Analytics Director based at Converse HQ. You will provide people leadership for data engineers and scrum leads who are also based at ITC. Furthermore, you will be part of an agile scrum team, working closely with data product managers, lead engineers, and other cross-functional team members to deliver data and analytics capabilities for Converse.
Who We Are Looking For
The Data Engineering Senior Supervisor possesses a unique blend of technical expertise, leadership acumen, and strategic vision. They oversee the development and maintenance of large-scale data systems, ensuring data quality, integrity, and security. From a leadership perspective, they have experience in managing and mentoring teams of data engineers, providing guidance on technical best practices, and fostering a culture of innovation and collaboration. They are skilled in agile project management methodologies and have a proven track record of delivering complex data engineering projects on time and within budget.
The Data Engineering Senior Supervisor needs to have a deep understanding of the organization's data strategy and is able to align their team's efforts with business objectives. Their expertise enables them to communicate effectively with stakeholders, support technical decision-making, and drive business growth through data-driven insights.
Experienced in building cloud-scalable, real-time, and high-performance data lake solutions using Databricks, Snowflake, and/or AWS, with proficiency in relational SQL, scripting languages (Shell, Python), and source control tools (GitHub).
Skilled in developing complex data solutions, with a strong understanding of end-to-end design, data structures, and algorithms. Proficient in workflow scheduling tools (Matillion, Brickflow) and familiar with Terraform, Serverless, and Jenkins.
Proven ability to lead and influence others, with experience in agile team leadership and influencing stakeholders. Effective communicator, both verbally and in writing, with the ability to identify and resolve people and process-related issues.
Strong problem-solving and analytical mindset, with the ability to take a broad perspective to identify innovative solutions. Willing to learn new skills and technologies, and able to quickly pick up new programming languages, technologies, and frameworks.
Ability to understand business needs and develop technical solutions to meet them. Strong understanding of solution and technical design.
Minimum 6 years relevant work experience; Bachelor's degree or equivalent combination of education, experience or training
What You'll Work On
As a Data Engineering Senior Supervisor on the Enterprise Data & Analytics team, you will drive work to completion through leading others along with hands-on development responsibilities. You will deliver quality components of a larger data pipeline to support data analytics products with guidance from experienced peers while working within an agile framework that focuses on delivering quality incremental value. As a Senior Supervisor, you will also be responsible for providing people leadership to data engineers, automation engineers, and scrum leads.
Be responsible for the team's delivery of scalable data solutions to solve a customer need
Guide the team in troubleshooting data issues and performing root cause analysis to proactively resolve product issues to improve data quality
Manage, mentor, and inspire a team; managing performance, goals and development potential
Build reusable components, frameworks and libraries at scale to support analytics products
Clean, prepare and optimize data for ingestion and consumption by implementing automated workflows
Collaborate on the implementation of new data management projects and re-structure of the current data architecture
Build continuous integration, test-driven development and production deployment frameworks
Collaboratively review design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards","Matillion, Brickflow, snowflake, Serverless, Github, Sql, Jenkins, Shell, Terraform, Databricks, Python, AWS"
Data Engineer - SAP HANA and Snow Flake,Ericsson,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Join our Team
About this opportunity:
Ericsson's Automation Chapter is seeking a highly motivated and self-driven Data Engineer and Senior Data Engineer with strong expertise in SAP HANA and SAP BODS. The ideal candidates will be focused on SAP-centric development and integration, ensuring that enterprise data flows are robust, scalable, and optimized for analytics consumption. You will collaborate with a high-performing team that builds and supports end-to-end data solutions aligned with our SAP ecosystem. You are adaptable and a flexible problem-solver with deep hands-on experience in HANA modeling and ETL workflows, capable of switching contexts across a range of projects with varying scale and complexity.
What you bring:
Design, develop, and optimize SAP HANA objects such as Calculation Views, SQL Procedures, and Custom Functions.
Develop robust and reusable ETL pipelines using SAP BODS for both SAP and third-party system integration.
Enable seamless data flow between SAP ECC and external platforms, ensuring accuracy and performance.
Collaborate with business analysts, architects, and integration specialists to translate requirements into technical deliverables.
Tune and troubleshoot HANA and BODS jobs for performance, scalability, and maintainability.
Ensure compliance with enterprise data governance, lineage, and documentation standards.
Support ongoing enhancements, production issues, and business-critical data deliveries.
Experience
8+ years of experience in SAP data engineering roles.
Strong hands-on experience in SAP HANA (native development, modeling, SQL scripting).
Proficient in SAP BODS, including job development, data flows, and integration techniques.
Experience working with SAP ECC data structures, IDOCs, and remote function calls.
Knowledge of data warehouse concepts, data modeling, and performance optimization techniques.
Strong debugging and analytical skills, with the ability to independently drive technical solutions.
Familiarity with version control tools and SDLC processes.
Excellent communication skills and ability to work collaboratively in cross-functional teams.
Education
Bachelor's degree in computer science, Information Systems, Electronics & Communication, or a related field.
Why join Ericsson
At Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.
What happens once you apply
Click Here to find all you need to know about what our typical hiring process looks like.
Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.
Primary country and city: India (IN) || Bangalore
Req ID: 765249","performance optimization, SDLC processes, ETL pipelines, Idocs, Sap Hana, Sap Bods, Data Modeling, Version Control Tools, Sap Ecc, Sql Scripting"
IN_Associate_GCP Data engineer_D&A_Advisory_Gurgaon,PwC India,2-5 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
FS X-Sector
Specialism
Data, Analytics & AI
Management Level
Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary Senior Associate - GCP Data Engineer
Responsibilities
Role : Sr. Associate
Exp : 2-5 years
PwC India is seeking a talented GCP Data Engineer to join our team in Mumbai or Gurgaon. The ideal candidate will have 2-5 years of experience with a strong focus on GCP services, data engineering, and analytics. This role offers an exciting opportunity to work on cutting-edge projects for global clients while leveraging your expertise in cloud technologies and data management.
Key Responsibilities:
GCP Service Implementation:
Design, develop, and maintain data solutions using GCP services, with a particular focus on Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler
Implement and optimize data lakes and data warehouses on GCP platforms.
Data Pipeline Development:
Create and maintain efficient ETL processes using PySpark and other relevant tools.
Develop scalable and performant data pipelines to process large volumes of data.
Implement data quality checks and monitoring systems to ensure data integrity.
Database Management:
Proficient with SQL and NoSQL databases, optimizing queries and database structures for performance.
Design and implement database schemas that align with business requirements and data models.
Performance Optimization:
Continuously monitor and optimize the performance of data processing jobs and queries.
Implement best practices for cost optimization in AWS environments.
Troubleshoot and resolve performance bottlenecks in data pipelines and analytics processes.
Collaboration and Documentation:
Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
Develop and maintain comprehensive documentation for data architectures, processes, and best practices.
Participate in code reviews and contribute to the team's knowledge base.
Required Qualifications:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
2-5 years of experience in data engineering, with a focus on GCP technologies.
Strong hands-on experience with GCP services, particularly Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler
Proficiency in Python and PySpark for data processing and analysis.
Extremely good with SQL/PL SQL
Demonstrated ability to optimize data pipelines and queries for performance.
Strong problem-solving skills and attention to detail.
Preferred Skills:
GCP certifications
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with data modeling and data warehouse concepts.
Innovation thinking and creativity in solution delivery
Mandatory Skill Sets
GCP Cloud Services/Python/PySpark
Preferred Skill Sets
GCP Cloud Services/Python/PySpark
Years Of Experience Required
2-5 years
Education Qualification
BE/BTech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Good Clinical Practice (GCP)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis, Intellectual Curiosity, Java (Programming Language), Market Development + 7 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Big Query, Data Proc, GCP Cloud Services, Cloud Scheduler, Cloud Composer, Cloud Run, Data Flow, Pyspark, Pl Sql, Sql, Workflow, Python"
IN_Associate_GCP Data engineer_D&A_Advisory_Gurgaon,PwC India,2-5 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
FS X-Sector
Specialism
Data, Analytics & AI
Management Level
Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary Senior Associate - GCP Data Engineer
Responsibilities
Role : Sr. Associate
Exp : 2-5 years
PwC India is seeking a talented GCP Data Engineer to join our team in Mumbai or Gurgaon. The ideal candidate will have 2-5 years of experience with a strong focus on GCP services, data engineering, and analytics. This role offers an exciting opportunity to work on cutting-edge projects for global clients while leveraging your expertise in cloud technologies and data management.
Key Responsibilities:
GCP Service Implementation:
Design, develop, and maintain data solutions using GCP services, with a particular focus on Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler
Implement and optimize data lakes and data warehouses on GCP platforms.
Data Pipeline Development:
Create and maintain efficient ETL processes using PySpark and other relevant tools.
Develop scalable and performant data pipelines to process large volumes of data.
Implement data quality checks and monitoring systems to ensure data integrity.
Database Management:
Proficient with SQL and NoSQL databases, optimizing queries and database structures for performance.
Design and implement database schemas that align with business requirements and data models.
Performance Optimization:
Continuously monitor and optimize the performance of data processing jobs and queries.
Implement best practices for cost optimization in AWS environments.
Troubleshoot and resolve performance bottlenecks in data pipelines and analytics processes.
Collaboration and Documentation:
Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
Develop and maintain comprehensive documentation for data architectures, processes, and best practices.
Participate in code reviews and contribute to the team's knowledge base.
Required Qualifications:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
2-5 years of experience in data engineering, with a focus on GCP technologies.
Strong hands-on experience with GCP services, particularly Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler
Proficiency in Python and PySpark for data processing and analysis.
Extremely good with SQL/PL SQL
Demonstrated ability to optimize data pipelines and queries for performance.
Strong problem-solving skills and attention to detail.
Preferred Skills:
GCP certifications
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with data modeling and data warehouse concepts.
Innovation thinking and creativity in solution delivery
Mandatory Skill Sets
GCP Cloud Services/Python/PySpark
Preferred Skill Sets
GCP Cloud Services/Python/PySpark
Years Of Experience Required
2-5 years
Education Qualification
BE/BTech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Good Clinical Practice (GCP)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis, Intellectual Curiosity, Java (Programming Language), Market Development + 7 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Big Query, Data Proc, GCP Cloud Services, Cloud Scheduler, Cloud Composer, Cloud Run, Data Flow, Pyspark, Pl Sql, Sql, Workflow, Python"
Senior Data Engineer,HCLTech,6-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role : Senior Data Engineer.
Experience : 6 to 10 years.
Location : Bengaluru/Chennai/Hyderabad/Pune/Delhi NCR.
Notice Period : Immediate to 45 Days.
Business domain knowledge : SaaS, SFDC, NetSuite.
Areas we support : Product, Finance (ARR reporting), GTM, Marketing, Sales.
Tech Stack : Fivetran, Snowflake, DBT, Tableau, GitHub.
What we are looking for:
1. SQL and data modeling at intermediate level - write complex SQL queries, build data model and experience with data transformation.
2. Problem solver: Person who can weed through ambiguity of the ask.
3. Bias for Action: Asks questions, reaches out to stakeholders, comes up with solutions.
4. Communication: Effectively communicates with stakeholder and team members.
5.Documentation: Can create BRD.
6. Someone well versed in Finance (ARR reporting) and/or GTM (sales and marketing) would be an added advantage.
7. Experience in SAAS, NetSuite and Salesforce will be a plus.
8. Independent, self-starter, motivated and experience with working in an onsite/offshore environment.","dbt, snowflake, Fivetran, Github, Data Transformation, Tableau, Data Modeling, Sql"
Data Engineer,Bristlecone,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Summary
JOB DESCRIPTION
We are looking for a skilled and motivated GCP Data Engineer to join our team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and systems on the Google Cloud Platform. You will work closely with analytics, data science, and business teams to deliver reliable and efficient data solutions that power decision-making and innovation.
Responsibilities
Key Responsibilities:
Design, implement, and optimize scalable datapipelines on GoogleCloudPlatform ( GCP ) .
Build and maintain ETLworkflows using GCP services such as CloudDataflow , CloudDataproc , and BigQuery .
Develop data models and schemas that support analytics and reporting needs.
Collaborate with stakeholders to gather business requirements and translate them into technical solutions.
Ensure data quality, security, and compliance with organizational policies.
Monitor and troubleshoot data pipeline performance and reliability.
Work with APIs and external data sources to integrate data into the cloud environment.
Stay updated with advancements in GCP services and data engineering best practices.
Qualifications
Required Skills and Qualifications:
Strong proficiency in GoogleCloudPlatform services, including BigQuery , CloudStorage , Pub / Sub , and CloudComposer .
Experience with programming languages like Python , Java , or SQL for data processing and transformation.
Knowledge of datamodeling , datawarehousing , and databasedesign .
Familiarity with CI / CDpipelines and workflow orchestration tools (e.g., ApacheAirflow ).
Understanding of IAMpolicies and data security best practices in GCP.
Strong analytical and problem-solving skills.
Effective communication and teamwork abilities.
Preferred Qualifications
GCP certifications such as ProfessionalDataEngineer or AssociateCloudEngineer .
Experience with Terraform or CloudDeploymentManager for infrastructure automation.
Exposure to machinelearning workflows and tools like AIPlatform .
Familiarity with DevOpsconcepts and containerization tools like Docker and Kubernetes .
Experience working with large-scale, distributed systems and big data frameworks like ApacheSpark .
About Us
ABOUT US
Bristlecone is the leading provider of AI-powered application transformation services for the connected supply chain. We empower our customers with speed, visibility, automation, and resiliency to thrive on change.
Our transformative solutions in Digital Logistics, Cognitive Manufacturing, Autonomous Planning, Smart Procurement and Digitalization are positioned around key industry pillars and delivered through a comprehensive portfolio of services spanning digital strategy, design and build, and implementation across a range of technology platforms.
Bristlecone is ranked among the top ten leaders in supply chain services by Gartner. We are headquartered in San Jose, California, with locations across North America, Europe and Asia, and over 2,500 consultants. Bristlecone is part of the $19.4 billion Mahindra Group.
Equal Opportunity Employer
Bristlecone is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status .
Information Security Responsibilities
Understand and adhere to Information Security policies, guidelines and procedure, practice them for protection of organizational data and Information System.
Take part in information security training and act while handling information.
Report all suspected security and policy breach to InfoSec team or appropriate authority (CISO).
Understand and adhere to the additional information security responsibilities as part of the assigned job role.","Java, BigQuery, Docker, Terraform, Kubernetes, Python, Sql"
(IND) Software Engineer III CRM Data Engineer,Walmart Global Tech India,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Position Summary...
Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales. Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities. Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.
What you'll do...
Team and Position Summary: This role is on our Marketplace Seller Acquisition and Onboarding team where we own the Marketplace Engineering team is at the forefront of building core platforms and services to enable Walmart to deliver vast selection at competitive prices and with best-in-class seller onboarding experience by enabling third-party sellers to list, sell and manage their products to our customers on walmart.com. We do this by managing the entire seller lifecycle, monitoring customer experience, and delivering high-value insights to our sellers to help them plan their assortment, price, inventory. The team also actively collaborates with partner platform teams to ensure we continue to deliver the best experience to our sellers and our customers. This role will be focused on the Marketplace. Position Responsibilities: You want a challenge Come join a team that is merging digital and physical, building real-time systems at scale, responding quicker to changes. Youll sweep us off our feet if you
Act as the Senior Software Engineer for the team, taking ownership of technical projects and solutions.
Lead by example, setting technical standards and driving overall technical architecture and design.
Mentor junior developers, enhancing their skills and understanding of Android development.
Desire to keep up with technology trends
Encouraging others to grow and be curious. Provide technical leadership in every stage of the development process, from design to deployment, ensuring adherence to best practices.
Have the desire to learn
Maintain and improve application performance, compatibility, and responsiveness across various devices and Android versions.
Drive for engineering and operational excellence, delivering high-quality solutions and processes.
Youll make an impact byAs a Senior Software Engineer for Walmart, youll have the opportunity to Apply and/or develop CRM solutions to develop efficient and scalable models at Walmart scale. Through this role you have an opportunity to develop intuitive software that meets and exceeds the needs of the customer and the company. You also get to collaborate with team members to develop best practices and requirements for the software. In this role it would be important for you to professionally maintain all codes and create updates regularly to address the customers and companys concerns. You will show your skills in analyzing and testing programs/products before formal launch to ensure flawless performance. Troubleshooting coding problems quickly and efficiently will offer you a chance to grow your skills in a high-pace, high-impact environment. Software security is of prime importance and by developing programs that monitor sharing of private information, you will be able to add tremendous credibility to your work. You will also be required to seek ways to improve the software and its effectiveness. Adhere to Company policies, procedures, mission, values, and standards of ethics and integrityWhat youll do
Lead a team of senior Individual Contributors
Leads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery and hand-offs; interacting with project manager to provinput on project plan; and providing leadership to the project team.
Supports business objectives by collaborating with business partners to identify opportunities; addressing high-priority initiatives (for example, business strategy, technical feasibility, implementation alternatives); identifying short- and long-term solutions; and leading cross-functional partnership.
Utilizes industry research to improve Wal-Marts technology environment by analyzing industry best practices; bench marking industry against internal processes and solutions; researching or influencing future industry solutions for fit with internal needs; and defining software development guidelines, standards and processes.
Position Requirements:Preferred skills: Agentforce, Salesforce-managed models Gen AI, LLMsMinimum qualifications:
3-5 years of Software Engineer experience with Salesforce.com platfrom knowledge and good to have knowledge on Salesforce-managed models Gen AI, LLMs
Proven working experience as a CRM Data Engineer with a minimum of 3 years in the field.
Strong programming skills in Scala and experience with Spark for data processing and analytics
Familiarity with Google Cloud Platform (GCP) services such as Big Query, GCS, Dataproc, Pub/Sub, etc.
Experience with data modeling, data integration, and ETL processes
Excellent working experience in Salesforce with Apex, Visual Force, Lightning, andForce.com
Strong Knowledge of Sales cloud, Service cloud, experience cloud (community cloud)
Experience in Application customization and development, including Lightning pages, Lightning Web Components, Aura Components, Apex (classes and triggers).
Experience in integration of salesforce with other systems using Salesforce APIs, SOAP, Rest API etc.
Proficient with Microsoft Visual Studio, Salesforce Lightning Design System, and the Salesforce development lifecycle.
Knowledge of Tools such as Data-Loader, ANT, Workbench, GIT, Bit-Bucket Version Control.
knowledge of deployment activities (CI/CD) between the Salesforce environments.
Knowledge of high-quality professional software engineering practices for agile software development cycle, including coding standards, code reviews, source control management, build processes, testing, and deployment.
Fundamental knowledge of design patterns
Experience in communicating effectively with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.
Mentoring the team members to meet the clients needs and holding them accountable for high standards of delivery.
Being able to understand and relate technology integration scenarios and be able to apply these learnings in complex troubleshooting scenarios.
RESPONSIBILITIES:
Writing and reviewing great quality code
Understanding functional requirements thoroughly and analysing the clients needs in the context of the project
Envisioning the overall solution for defined functional and non-functional requirements, and being able to define technologies, patterns, and frameworks to realize it.
Determining and implementing design methodologies and tool sets
Enabling application development by coordinating requirements, schedules, and activities.
Being able to lead/support UAT and production roll outs.
Creating, understanding, and validating WBS and estimated effort for given module/task, and being able to justify it.
Addressing issues promptly, responding positively to setbacks and challenges with a mindset of continuous improvement
Giving constructive feedback to the team members and setting clear expectations.
Helping the team in troubleshooting and resolving of complex bugs
Coming up with solutions to any issue that is raised during code/design review and being able to justify the decision taken.
Carrying out POCs to make sure that suggested design/technologies meet the requirements.
About Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionals within the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, hybrid work We use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives. Benefits Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more. Equal Opportunity Employer: Walmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being respectful of all people.
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Minimum Qualifications:Option 1: Bachelor's degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.
Option 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Certification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master's degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area
Primary Location...
4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2156629","Big Query, Salesforce APIs, Aura Components, Data-Loader, Pub Sub, Lightning Web Components, Lightning pages, LLMs, Bit-Bucket, force.com, Salesforce Lightning Design System, Gen AI, workbench, Salesforce-managed models, GCS, Ant, Microsoft Visual Studio, Visual Force, APEX, Scala, Salesforce.com, Soap, Rest Api, Dataproc, Git, Service Cloud, Lightning, Spark, Sales Cloud, Community Cloud"
Data Engineer,S&P Global Market Intelligence,4-6 Years,,India,Banking/Accounting/Financial Services,"About the Role:
09
S&P Global Commodity Insights
The Team: ThePlatform Solutions groupsupports all development requirements for two of S&P's specialist business teams within the wider Commodity Insights group: Fuel, Chemicals and Resource Solutions, and Gas, Power and Climate Solutions. Our work includes all aspects of creation of, and ongoing support for, our business lines data flows, databases, analyst modelling solutions and workflows, new apps, new client-facing products, and many other work areas besides. Our highly experienced team members are based in locations worldwide to support these S&P teams across the globe.
The Impact: As part of the CI division, the FCRS and GPCS business lines have a wide customer base and are experiencing continuing strong growth. We are expanding our team to ensure we can continue to offer these business areas maximum support with their wide-ranging development requirements.
What's in it for you: The new team member will work in a fast-paced environment, working with colleagues worldwide on a varied range of interesting and high-impact projects. Our teams use the latest software and development processes, including a number of proprietary AI tools.
Responsibilities
Design, develop, and maintain scalable ETL/ELT pipelines.
Optimize and automate data ingestion, transformation, and storage processes.
Work with structured and unstructured data sources, ensuring data quality and consistency.
Develop and maintain data models, warehouses, and databases.
Collaborate with cross-functional teams to support data-driven decision-making.
Ensure data security, privacy, and compliance with industry standards.
Troubleshoot and resolve data-related issues in a timely manner.
Monitor and improve system performance, reliability, and scalability.
Stay up-to-date with emerging data technologies and recommend improvements to our data architecture and engineering practices.
What We're Looking For:
4 - 6 years Experience working with Unified analytics platforms like Databricks
Proficiency in database management systems (e.g., SQL, NoSQL, Oracle, MySQL, PostgreSQL).
Strong programming skills in languages such as Python, Java, or Scala.
Experience with big data technologies (e.g., Hadoop, Big data processing engines, Message broker systems).
Familiarity with cloud data platforms (e.g., AWS, Azure, Google Cloud).
Strong concepts/experience of designing and developing ETL architectures.
Strong RDBMS concepts and SQL development skills
Strong knowledge of data modeling and mapping
Experience with Data Integration from multiple data sources
About Company Statement:
S&P Global delivers essential intelligence that powers decision making. We provide the world's leading organizations with the right data, connected technologies and expertise they need to move ahead. As part of our team, you'll help solve complex challenges that equip businesses, governments and individuals with the knowledge to adapt to a changing economic landscape.
S&P Global Commodity Insights enables organizations to create long-term, sustainable value with data and insights for a complete view on the global energy and commodities markets.
We're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.
S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.
For more information, visit .
What's In It For You
Our Purpose:
Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology-the right combination can unlock possibility and change the world.
Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.
Our People:
We're more than 35,000 strong worldwide-so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.
From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.
Our Values:
Integrity, Discovery, Partnership
At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.
Benefits:
We take care of you, so you can take care of business. We care about our people. That's why we provide everything you-and your career-need to thrive at S&P Global.
Our benefits include:
Health & Wellness: Health care coverage designed for the mind and body.
Flexible Downtime: Generous time off helps keep you energized for your time on.
Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.
Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.
Family Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.
Beyond the Basics: From retail discounts to referral incentive awards-small perks can make a big difference.
For more information on benefits by country visit:
Global Hiring and Opportunity at S&P Global:
At S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.
-----------------------------------------------------------
Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.
If you need an accommodation during the application process due to a disability, please send an email to: and your request will be forwarded to the appropriate person.
US Candidates Only: The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -
-----------------------------------------------------------
20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority - Ratings - (Strategic Workforce Planning)","Big data processing engines, Message broker systems, Oracle, Databricks, Sql, ELT, Data Modeling, Java, Hadoop, Google Cloud, Etl, AWS, Data Integration, MySQL, Nosql, Python, Azure, Scala, PostgreSQL"
AWS Data Engineer,KPMG India,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"KPMG in India, a professional services firm, is the Indian member firm affiliated with KPMG International and was established in September 1993. Our professionals leverage the global network of firms, providing detailed knowledge of local laws, regulations, markets, and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, and Vadodara.
KPMG in India offers services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed, and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.
KPMG Advisory professionals provide advice and assistance to enable companies, intermediaries, and public sector bodies to mitigate risk, improve performance, and create value. KPMG firms provide a wide range of Risk Advisory and Financial Advisory Services that can help clients respond to immediate needs as well as put in place the strategies for the longer term.
Job Title: Consultant, Senior Consultant
Skills: AWS Data Engineer
Location: Bengaluru & Mumbai
Role & Responsibility
Evaluating, developing, maintaining and testing data engineering solutions for Data Lake and advanced analytics projects.
Implement processes and logic to extract, transform, and distribute data across one or more data stores from a wide variety of sources
Distil business requirements and translate into technical solutions for data systems including data warehouses, cubes, marts, lakes, ETL integrations, BI tools or other components.
Creation and support of data pipelines built on AWS technologies including Glue, Redshift, EMR, Kinesis and Athena
Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
Optimize data integration platform to provide optimal performance under increasing data volumes
Support the data architecture and data governance function to continually expand their capabilities
Experience in development of Solution Architecture for Enterprise Data Lakes (applicable for AM/Manager level candidates)
Should have exposure to client facing roles
Strong communication, inter-personal and team management skills
Education Qualification and Experience
Proficient in any object-oriented/ functional scripting languages: Pyspark, Python etc.
Experience in using AWS SDKs for creating data pipelines ingestion, processing and orchestration.
Hands on experience in working with big data on AWS environment including cleaning/transforming/cataloguing/mapping etc.
Good understanding of AWS components, storage (S3) & compute services (EC2)
Hands on experience in AWS managed services (Redshift, Lambda, Athena) and ETL (Glue).
Experience in migrating data from on-premise sources (e.g. Oracle, API-based, data extracts) into AWS storage (S3)
Experience in setup of data warehouse using Amazon Redshift, creating Redshift clusters and perform data analysis queries
Experience in ETL and data modelling on AWS ecosystem components - AWS Glue, Redshift, DynamoDB
Experience in setting up AWS Glue to prepare data for analysis through automated ETL processes.
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Hands on experience with AWS CLI, Linux tools and shell scripts
Certifications on AWS will be an added plus.
BE/BTech/MCA
4+ years of strong experience in 3-4 of the above-mentioned skills.
Equal employment opportunity information
KPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their colour, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability, or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavour for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","AWS DMS, ETL Glue, AWS SDKs, Pyspark, data engineering, Linux Tools, Dynamodb, AWS Glue, Big Data, AWS Data Pipeline, Shell Scripts, amazon emr, Aws Cli, Python, AWS"
AWS Data Engineer - Consultant,KPMG India,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description
About KPMG in India
KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Jaipur, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.
KPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.
AWS Data Engineering
Equal employment opportunity information
KPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.
Qualifications
B.E","data engineering, AWS"
Celonis Data Engineer - Analyst,KPMG India,1-3 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
About KPMG in India
KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.
KPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.
The person will work on a variety of projects in a highly collaborative, fast-paced environment. The person will be responsible for software development activities of KPMG, India. Part of the development team, he/she will work on the full life cycle of the process, develop code and unit testing. He/she will work closely with Technical Architect, Business Analyst, user interaction designers, and other software engineers to develop new product offerings and improve existing ones. Additionally, the person will ensure that all development practices are in compliance with KPMG's best practices policies and procedures. This role requires quick ramp up on new technologies whenever required.
Responsibilities
Role Celonis Data E ngineer
Location Bangalore
Experience 1 to 2 Years
Key Responsibilities :-
At least 1+ years of experience in databases, data integration, and data modeling.
Minimum 1 years of experience in Celonis process mining.
Strong skills in SQL and PQL.
Experience with cloud-based data technologies.
Ability to lead teams and drive project completion as per business requirements.
Experience in managing the Celonis platform and optimizing APC consumption.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Equal Opportunity Employer
KPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","Celonis process mining, cloud-based data technologies, PQL, Databases, Data Modeling, Data Integration, Sql"
AWS Data Engineer - Consultant,KPMG India,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description
About KPMG in India
KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Jaipur, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.
KPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.
AWS Data Engineering
Equal employment opportunity information
KPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.
Qualifications
B.E","data engineering, AWS"
Data Engineer,SPN Globe,5-10 Years,,"Nagpur, Pune",Login to check your skill match score,"SPN Globe is a specialized IT recruitment and staffing firm. We are the sourcing partner of various software companies on Permanent and Contract to Hire Demands. Also working with few top notch (CMMI-5 level) clients, PAN India clients and major city clients such as Hyderabad, Bengaluru, Pune, Mumbai, Nagpur and NCR.
Please find the position details below:
Company: IT Company
Role: Data Engineer
Type: Permanent (No third-party payroll)
Location: Pune, Nagpur
Joining: 0 to 30 days / Immediate Joiners
Experience: 5 to 10 years
Mandatory Skills: Data Engineering, Python, SQL, Snowflake, Gitlab
Apply immediately to grab it. Email your resume at [HIDDEN TEXT]
Also, immediately refer this opportunity to your friends.","snowflake, Gitlab, Data Engineer, Python, Sql"
Data Engineer - Bangalore,Dayworks Private Limited,9-13 Years,INR 35 - 60 LPA,Bengaluru,Login to check your skill match score,"This role is for one of Weekday's clients
Salary range: Rs 3500000 - Rs 6000000 (ie INR 35-60 LPA)
Min Experience: 9 years
Location: Bengaluru
JobType: full-time
About the Role:
We are seeking a highly experienced and innovative Senior Data Engineer to join our growing data team. As a Data Engineer, you will be responsible for designing, developing, and maintaining robust data pipelines and scalable architectures that support our business intelligence, analytics, and data-driven decision-making initiatives. You will work closely with data scientists, analysts, product managers, and other stakeholders to ensure that our data infrastructure is efficient, reliable, and aligned with business goals.
This is a strategic role ideal for someone who thrives in a fast-paced environment, has deep experience in data engineering best practices, and is passionate about leveraging data to drive impact at scale.
Key Responsibilities:
Data Architecture & Pipeline Development:
Design and build highly scalable, efficient, and secure data pipelines for batch and real-time data processing.
Develop and maintain ETL/ELT processes to extract, transform, and load data from multiple sources into data warehouses and data lakes.
Data Modeling & Warehousing:
Create and maintain optimized data models that support advanced analytics and reporting.
Design and implement data warehousing solutions using modern data storage technologies.
Data Quality & Governance:
Ensure high levels of data availability, quality, and integrity through the implementation of robust data validation, monitoring, and governance practices.
Partner with compliance and data governance teams to enforce data security and privacy policies.
Collaboration & Cross-functional Partnership:
Work closely with data scientists, analysts, and business teams to understand data requirements and provide reliable data solutions.
Collaborate with DevOps and infrastructure teams to automate deployment and manage cloud-based data environments.
Tooling & Performance Optimization:
Implement monitoring tools and optimize performance of data pipelines and database systems.
Stay updated with the latest trends in data engineering, and evaluate new tools and technologies for adoption.
Required Qualifications:
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or a related field.
9+ years of hands-on experience in Data Engineering, with a deep understanding of scalable data pipeline architecture.
Proficient in at least one programming language such as Python, Java, or Scala.
Strong experience with ETL/ELT frameworks, data orchestration tools (e.g., Apache Airflow, DBT), and workflow management.
Solid experience working with cloud data platforms (e.g., AWS, Azure, GCP) and data storage solutions (e.g., Snowflake, Redshift, BigQuery).
Expertise in SQL and data modeling for both OLAP and OLTP systems.
Familiarity with distributed systems, streaming technologies (Kafka, Spark), and containerization (Docker, Kubernetes) is a plus.
Preferred Skills:
Experience in a fast-paced startup or enterprise data team.
Exposure to big data technologies and real-time data processing.
Strong analytical thinking and problem-solving skills.
Excellent communication and project management abilities.",data engineering
Data Engineer - Python/SQL,Xpetize Technology Solutions Private Limited,3-5 Years,,Itanagar,"Information Technology, Information Services","Skills : Data engineer
Location : Remote
Experience : 4+ years
Notice : Immediate only
Key Skills
Data Engineering Expertise : Bring 3+ years of experience in building data pipelines and managing a secure, modern data stack. This includes CDC streaming ingestion using tools like Debezium into a Hudi data lake that supports AI/ML workloads and a curated Redshift data warehouse.
AWS Cloud Proficiency : At least 3 years of experience working with AWS cloud infrastructure, including Kafka (MSK), Spark / AWS Glue, and infrastructure as code (IaC) using Terraform.
Strong Coding Skills : Write and review high-quality, maintainable code that enhances the reliability and scalability of our data platform. We use Python, SQL, and dbt extensively, and you should be comfortable leveraging third-party frameworks to accelerate development.
Data Lake Development : Prior experience building data lakes on S3 using Apache Hudi with Parquet, Avro, JSON, and CSV file formats.
Workflow Automation : Build and manage multi-stage workflows using serverless Lambdas and AWS Step Functions to automate and orchestrate data processing pipelines.
Data Governance Knowledge : Familiarity with data governance practices, including data quality, lineage, and privacy, as well as experience using cataloging tools to enhance discoverability and compliance.
CI/CD Best Practices : Experience developing and deploying data pipeline solutions using CI/CD best practices to ensure reliability and scalability.
Data Integration Tools : Working knowledge of tools such as Stitch and Segment CDP for integrating diverse data sources into a cohesive ecosystem.
Analytical and ML Tools Expertise : Knowledge and practical experience with Athena, Redshift, or Sagemaker Feature Store to support analytical and machine learning workflows is a definite bonus!","cdc, S3, Machine Learning, Artificial Intelligence, Aws Cloud, Csv, AWS Glue, Kafka, Json, Avro, Redshift, Sql, Data Pipeline, Terraform, Spark, Python, Coding Skills"
"Data Engineer, AVP",NatWest Markets,16-18 Years,,Bengaluru,Banking,"Join us as a Data Engineer
We're looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure
Day-to-day, you'll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights
If you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you
We're offering this role at associate vice president level
What you'll do Your daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.
We ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.
You ll also be responsible for:
Driving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions
Participating in the data engineering community to deliver opportunities to support our strategic direction
Carrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists
Building advanced automation of data engineering pipelines through the removal of manual stages
Leading on the planning and design of complex products and providing guidance to colleagues and the wider team when required
The skills you ll needTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.
We ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.
You ll also need:
Experience inDevOps practice andcloud technologies specifically AWS
Good knowledge of snowflake and capable of creating practical and scalable data solutions using the snowflake platform to meet business needs
Experience in business intelligence and data engineering, including data warehousing, delivery, and operations.
Hours
45
Job Posting Closing Date:
23/05/2025","Analytical Skills, Forecasting, Data Analysis, Banking, Business Strategy, Business Management, Cost Control"
Data Engineer,R Systems International,7-10 Years,,"Noida, Mumbai, Pune",Information Technology,"Candidate should have strong technical capabilities, particularly in the following areas:
SQL: Expertise in writing stored procedures, complex queries, and optimizing performance.
Power BI development :Experience of developing complex power bi reports
Dax :Complex Dax queries to meet business needs
Paginated Reports (Power BI): Experience in designing and building paginated reports using Power BI Report Builder.
Healthcare domain experience - Added advantahe
Excellent Communication skill
Role
Candidate should have strong technical capabilities, particularly in the following areas:
SQL: Expertise in writing stored procedures, complex queries, and optimizing performance.
Power BI development :Experience of developing complex power bi reports
Dax :Complex Dax queries to meet business needs
Paginated Reports (Power BI): Experience in designing and building paginated reports using Power BI Report Builder.
Healthcare domain experience - Added advantahe
Excellent Communication skill","Power Bi, Dax, Sql, Ssrs, Tsql"
Data Engineer,R Systems International,7-12 Years,,Delhi NCR,Information Technology,"Roles and Responsibilities
Design, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.
Develop complex ETL processes using Python scripts and SQL queries to process large datasets stored in Azure Data Lake Storage.
Collaborate with cross-functional teams to gather requirements for new data warehousing solutions and implement scalable architectures on Azure Data Warehouse.
Troubleshoot issues related to ADF pipeline failures, Azure Synapse connections, and SQL queries.","Adf, Power Bi, Azure, Sql, Etl"
Senior Data Engineer - Immediate joiners,R Systems International,7-12 Years,,"Noida, Pune, Chennai",Information Technology,"Hands-on experience ofAzure data services, including Azure Data Factory, Azure Data Lake, Databricks and Spark.
Experience in ETL Development.
Experience of developing and servicing data via API endpoints, and Azure API Management.
Python 3.x, with 6+ years programming experience that includes:
Tool usage, such as Jupyter Notebooks.
Virtual environments.
Automated testing through feature files.
CI/CD implementations.
Experience in git and branching strategies.
Good Understanding of ADF/ADB github integration.
Experience of batch scheduling.
Experience in bash scripting.
Experience using SOAP UI / Postman / Fiddler for API testing.
Experience of incident and change management processes.","Azure, Adf, Api, Python, Adb, Sql"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Noida,Information Technology,"Key Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)
Familiarity with DataHub, DataMesh, and security best practices is a plus
Strong problem-solving and debugging mindset
Ability to work under pressure in a fast-paced environment.
Excellent communication and collaboration skills.
Ownership, customer orientation, and a bias for actionKey Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Bengaluru,Information Technology,"Key Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)
Familiarity with DataHub, DataMesh, and security best practices is a plus
Strong problem-solving and debugging mindset
Ability to work under pressure in a fast-paced environment.
Excellent communication and collaboration skills.
Ownership, customer orientation, and a bias for actionKey Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer,Mondelez,2-9 Years,,Mumbai,Food and Beverage,"How you will contribute
You will:
Execute the business analytics agenda in conjunction with analytics team leaders
Work with best-in-class external partners who leverage analytics tools and processes
Use models/algorithms to uncover signals/patterns and trends to drive long-term business performance
Execute the business analytics agenda using a methodical approach that conveys to stakeholders what business analytics will deliver
What you will bring
A desire to drive your future and accelerate your career and the following experience and knowledge:
Using data analysis to make recommendations to analytic leaders
Understanding in best-in-class analytics practices
Knowledge of Indicators (KPIs) and scorecards
Knowledge of BI tools like Tableau, Excel, Alteryx, R, Python, etc. is a plus
Are You Ready to Make It Happen at Mondel z International
Join our Mission to Lead the Future of Snacking. Make It with Pride
In This Role
Asa DaaSData Engineer, you will have the opportunity to design and build scalable, secure, and cost-effective cloud-based data solutions. You will develop andmaintaindata pipelines to extract, transform, and load data into data warehouses or data lakes, ensuring data quality and validation processes tomaintaindata accuracy and integrity. You will ensure efficient data storage and retrieval foroptimalperformance, and collaborate closely with data teams, product owners, and other stakeholders to stay updated with the latest cloud technologies and best practices.
Role & Responsibilities:
Design and Build:Develop and implement scalable, secure, and cost-effective cloud-based data solutions.
Manage Data Pipelines:Develop andmaintaindata pipelines to extract, transform, and load data into data warehouses or data lakes.
Ensure Data Quality:Implement data quality and validation processes to ensure data accuracy and integrity.
Optimize Data Storage:Ensure efficient data storage and retrieval foroptimalperformance.
Collaborate and Innovate:Work closely with data teams, product owners, and stay updated with the latest cloud technologies and best practices.
Technical Requirements:
Programming:Python
Database:SQL, PL/SQL, Postgres SQL,Bigquery, Stored Procedure / Routines.
ETL & Integration:AecorSoft, Talend,DBT, Databricks(Optional),Fivetran.
Data Warehousing:SCD, Schema Types, Data Mart.
Visualization:PowerBI(Optional), Tableau (Optional), Looker.
GCP Cloud Services:Big Query, GCS.
Supply Chain:IMS + Shipment functional knowledge good to have.
Supporting Technologies:Erwin, Collibra,Data Governance,Airflow.
Soft Skills:
Problem-Solving:The ability toidentifyand solve complex data-related challenges.
Communication:Effective communication skills to collaborate with Product Owners, analysts, and stakeholders.
Analytical Thinking:Thecapacityto analyze data and draw meaningful insights.
Attention to Detail:Meticulousness in data preparation and pipeline development.
Adaptability:The ability to stay updated with emerging technologies and trends in the dataengineering field.","business analytics, DaaS Data Engineer, cloud-based data solutions, Manage Data Pipelines, Bi Tools, Sql"
"Data Engineer, AVP",NatWest Markets,16-18 Years,,Gurugram,Banking,"Join us as a Data Engineer
We're looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure
Day-to-day, you'll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights
If you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you
We're offering this role at associate vice president level
What you'll do Your daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.
We ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.
You ll also be responsible for:
Driving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions
Participating in the data engineering community to deliver opportunities to support our strategic direction
Carrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists
Building advanced automation of data engineering pipelines through the removal of manual stages
Leading on the planning and design of complex products and providing guidance to colleagues and the wider team when required
The skills you ll needTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.
We ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.
You ll also need:
Experience inDevOps practice andcloud technologies specifically AWS
Good knowledge of snowflake and capable of creating practical and scalable data solutions using the snowflake platform to meet business needs
Experience in business intelligence and data engineering, including data warehousing, delivery, and operations.
Hours
45
Job Posting Closing Date:
23/05/2025","Data Analysis, Analytical Skills, Forecasting, Banking, Business Strategy, Business Management, Cost Control"
Consultant - Data Engineer,Aliqan Services,5-8 Years,,Bengaluru,Information Technology,"As a Consultant - Data Engineer, you will be responsible for:
Working with large datasets, including terabyte-scale and growing data
Utilizing various technologies and tools associated with databases and big data
Designing and modeling data to meet business requirements
Developing complex ETL processes from concept to implementation
Optimizing system performance and tuning for scalability
Collaborating with business and technical teams to effectively communicate and solve problems
Candidate Qualifications:
To be considered for this position, you should have:
5+ years of data engineering or general software development experience
Experience with DWH, Snowflake, SQL, Python, and any cloud platform
Familiarity with AWS Data Ecosystem, including AWS S3 and AWS Lambda
Proficiency in writing complex SQL queries across large datasets
Strong software engineering principles and functional programming skills in Python or equivalent
Excellent problem-solving abilities and the ability to work in a fast-paced environment
Required Skills:
Bigdata
Snowflake
ETL (any tool)
DWH
Spark
Python
SQL","snowflake, snowflake, Dwh, Spark, Bigdata, Python, Etl, Dwh, Spark, Bigdata, Python, Etl"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Gurugram,Information Technology,"Key Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)
Familiarity with DataHub, DataMesh, and security best practices is a plus
Strong problem-solving and debugging mindset
Ability to work under pressure in a fast-paced environment.
Excellent communication and collaboration skills.
Ownership, customer orientation, and a bias for actionKey Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Pune,Information Technology,"Key Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)
Familiarity with DataHub, DataMesh, and security best practices is a plus
Strong problem-solving and debugging mindset
Ability to work under pressure in a fast-paced environment.
Excellent communication and collaboration skills.
Ownership, customer orientation, and a bias for actionKey Responsibilities
Ensure platform uptime and application health as per SLOs/KPIs
Monitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.
Debug and resolve complex production issues, performing root cause analysis
Automate routine tasks and implement self-healing systems
Design and maintain dashboards, alerts, and operational playbooks
Participate in incident management, problem resolution, and RCA documentation
Own and update SOPs for repeatable processes
Collaborate with L3 and Product teams for deeper issue resolution
Support and guide L1 operations team
Conduct periodic system maintenance and performance tuning
Respond to user data requests and ensure timely resolution
Address and mitigate security vulnerabilities and compliance issues Technical Skillset
Hands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger
Strong Linux fundamentals and scripting (Python, Shell)
Experience with Apache NiFi, Airflow, Yarn, and Zookeeper
Proficient in monitoring and observability tools: ELK Stack, Prometheus, Loki
Working knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines
Strong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer - Supply Chain,Mondelez,2-5 Years,,Mumbai,Food and Beverage,"Are You Ready to Make It Happen at Mondel z International
Join our Mission to Lead the Future of Snacking. Make It With Pride.
Together with analytics team leaders you will support our business with excellent data models to uncover trends that can drive long-term business results.
How you will contribute
You will:
Execute the business analytics agenda in conjunction with analytics team leaders
Work with best-in-class external partners who leverage analytics tools and processes
Use models/algorithms to uncover signals/patterns and trends to drive long-term business performance
Execute the business analytics agenda using a methodical approach that conveys to stakeholders what business analytics will deliver
What you will bring
A desire to drive your future and accelerate your career and the following experience and knowledge:
Using data analysis to make recommendations to analytic leaders
Understanding in best-in-class analytics practices
Knowledge of Indicators (KPIs) and scorecards
Knowledge of BI tools like Tableau, Excel, Alteryx, R, Python, etc. is a plus
Are You Ready to Make It Happen at Mondel z International
Join our Mission to Lead the Future of Snacking. Make It with Pride
In This Role
As a DaaS Data Engineer, you will have the opportunity to design and build scalable, secure, and cost-effective cloud-based data solutions. You will develop and maintain data pipelines to extract, transform, and load data into data warehouses or data lakes, ensuring data quality and validation processes to maintain data accuracy and integrity. You will ensure efficient data storage and retrieval for optimal performance, and collaborate closely with data teams, product owners, and other stakeholders to stay updated with the latest cloud technologies and best practices.
Role & Responsibilities:
Design and Build:Develop and implement scalable, secure, and cost-effective cloud-based data solutions.
Manage Data Pipelines:Develop and maintain data pipelines to extract, transform, and load data into data warehouses or data lakes.
Ensure Data Quality:Implement data quality and validation processes to ensure data accuracy and integrity.
Optimize Data Storage:Ensure efficient data storage and retrieval for optimal performance.
Collaborate and Innovate:Work closely with data teams, product owners, and stay updated with the latest cloud technologies and best practices to remain current in the field.
Technical Requirements:
Programming:Python, PySpark, Go/Java
Database:SQL, PL/SQL
ETL & Integration:DBT, Databricks + DLT, AecorSoft, Talend,Informatica/Pentaho/Ab-Initio,Fivetran.
Data Warehousing:SCD, Schema Types, Data Mart.
Visualization:Databricks Notebook, PowerBI, Tableau, Looker.
GCP Cloud Services:Big Query, GCS, Cloud Function, PubSub, Dataflow, DataProc, Dataplex.
AWS Cloud Services:S3, Redshift, Lambda, Glue, CloudWatch, EMR, SNS, Kinesis.
Supporting Technologies:Graph Database/Neo4j, Erwin, Collibra, Ataccama DQ, Kafka, Airflow.
Experience with RGM.ai product would have an added advantage.
Soft Skills:
Problem-Solving:The ability to identify and solve complex data-related challenges.
Communication:Effective communication skills to collaborate with Product Owners, analysts, and stakeholders.
Analytical Thinking:The capacity to analyse data and draw meaningful insights.
Attention to Detail:Meticulousness in data preparation and pipeline development.
Adaptability:The ability to stay updated with emerging technologies and trends in the data engineering field.","SCD, Schema Types, dbt, Databricks + DLT, DaaS Data Engineer, Data Mart, Python, Sql"
Senior Data Engineer,Tech SMC Square,4-9 Years,,Bengaluru,Outsourcing,"4-5 years of experience in Data Engineering projects (Med to Small Data Lake / Data Warehousing preferred).
3+ years of experience in Python (Must Have) / Kafka programming (Preferred) / PySpark (Nice to Have) /
2+ years of working experience with AWS Services like S3 (Must Have), Lambda (Must Have), Redshift (Preferred), Glue (Nice to Have), Data Brew (Nice to Have)
Must Have:
3+ years of experience in performing data analysis, data ingestion and data integration. o 3+ years of experience in ETL (Extraction, Transformation & Loading) or ELT and architecting data systems.
3+ years of experience with schema design, data modelling and SQL queries. o Strong Database experience (Oracle/PostgreSQL/SQL Server/Redshift). o Design and Develop scalable Data warehouse solutions, ETL/ELT pipelines in AWS cloud environment.
Experience in data workflow management.
Knowledge on CI/CD (Must Have) and Terraform technologies (Nice to Have).
Preferred / Nice To Have:
Knowledge of advanced statistics and experience with statistical data analysis systems (scikit-learn, Pandas, R) is added advantage.
Education background:
BE / B Tech / MS in Computer Science, Information Technology, or equivalent degree in related discipline","SQL/Data Modeling, ETL/ELT, CI/CD, AWS (S3/Lambda), Data Warehousing, Python"
Lead Data Engineer,Tech SMC Square,10-20 Years,,Bengaluru,Outsourcing,"8+ years of experience in data engineering, machine learning, AI, data warehousing, and data management Projects Med to Small.
3+ years in a leadership role, responsible for the direct oversight of a team of data engineers (offshore) where you oversaw day-to-day activities, attained delivery metrics, and provided technical coaching to staff (Must Have)
Data processing programming using SQL, Python, and similar tools (Must Have).
Solid experience with cloud-based data tools and platforms - AWS (Must Have), Azure or, Google (Nice To Have)
Deep hands-on experience using databases and related technologies in a fast-paced business environment with large-scale and complex datasets including design and optimize queries, create data structures, and develop data models (Must Have)
Be comfortable with building effective cross-team collaborative working relationships with people at all levels of the organization. (Must Have)
Experience in enterprise level data platforms involving implementation of end-to-end data pipelines. (Must Have)
Experience with column-oriented database technologies, Redshift (Must Have) Big Query or Vertica (Nice to Have), and traditional database systems SQL Server and Oracle (Must Have), MySQL (Nice to Have).
Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Matillion (Must Havebut can be learned), Lambda (Preferred), Spark (Preferred), AWS Glue ETL (Nice to Have), Spark Streaming (Nice to Have), AWS Glue DataBrew (Nice to Have), Kinesis Data Firehose (Nice to Have)
Logical programming in Python (Must Have), Spark (Preferred), PySpark (Nice to Have), Java(Nice to Have), Javascript(Nice to Have), and/or Scala (Nice to Have).
Cloud-native data platform design with a focus on streaming and event-driven architectures (Preferred)","Oracle), SQL & Python, Data Warehousing (Redshift, Data Pipeline Architecture, data engineering, SQL Server, Aws Cloud"
Snowflake Data Engineer,Sysintelli Inc,5-10 Years,INR 20 - 30 LPA,Bengaluru,"Consulting, Staffing Agency","Position: SnapLogic Developer
Experience: 5+ Years
Location: Remote
Budget: 30-35 LPA
Availability to Join: On or before 30th January
Key Responsibilities:
Design, develop, and manage robust data integration workflows using SnapLogic.
Work extensively with Snowflake to manage and optimize data pipelines and warehouse operations.
Write efficient SQL queries to process, transform, and analyze large datasets.
Leverage Python for scripting, data processing, and automation tasks as required.
Collaborate with cross-functional teams to gather requirements and deliver solutions.
Troubleshoot and optimize existing workflows for better performance and scalability.
Ensure data security, integrity, and compliance with best practices.
Required Skills and Qualifications:
SnapLogic Expertise: Hands-on experience in developing and managing SnapLogic pipelines.
Snowflake Knowledge: Strong proficiency in working with Snowflake databases.
SQL Proficiency: Advanced SQL skills for complex queries and transformations.
Python Skills: Experience in scripting and automation using Python.
5+ years of experience in data integration, ETL, and related fields.
Strong problem-solving skills and the ability to work independently in a remote environment.
Preferred Skills:
Knowledge of data modeling and database optimization techniques.
Experience with cloud platforms like AWS, Azure, or Google Cloud.
Familiarity with DevOps practices and CI/CD pipelines.","snowflake, Snaplogic, Python, Sql"
Data Engineer ( Hiring For one of our client),Vibrantminds Technologies,Fresher,INR 4.5 - 5 LPA,Pune,"Database, Data Mining, Data Visualization, Big Data, Data Integration","Job description
Position:Data Engineer
Experience:Fresher
Job Location:Pune
Internship Duration:06 Months
Approx. Stipend During Internship:Upto Rs. 11,000/- Per Month.
Approx. Package Post Internship:Upto Rs. 4,50,000/- Per Annum
(Based on performance during the internship, with a potential CTC of 4.5 LPA)
Job Location:Pune
Educational Criteria:
BE / BTech (All Branches)
Pass out Year 2023&2024 Batch Only
70% in Graduation
Job Description:
We are looking for data engineers who have the right attitude, aptitude, skills, empathy, compassion, and hunger for learning. Build products in the data analytics space. A passion for shipping high-quality data products, interest in the data products space; curiosity about the bigger picture of building a company, product development and its people. Bachelors degree in Computer Science, Engineering, Statistics, or a related field.
Roles and Responsibilities
Develop and maintain ETL pipelines using Pyspark.
Understand Pyspark concepts, performance optimization techniques and governance tools
Understanding data from various systems to the Enterprise Data
Warehouse/Data Lake/Data Mesh.
Collaborate cross-functionally to design effective data solutions
Monitor, troubleshoot, and optimize pipeline performance and data quality
Maintain high coding standards and produce thorough documentation.
Skills Required:
Experience with big data technologies (Python, PySpark, etc.).
Exposure to building robust and resilient data pipelines which are scalable,
Fault tolerant and reliable in terms of data movement.
Strong understanding in Python, Pyspark.
Knowledge of any one cloud platform (AWS, GCP, Azure).
Strong understanding of SQL and NoSQL technologies.
Basic knowledge of data warehousing and ETL/ELT processes.
Excellent written and verbal communication skills.Role & responsibilities
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:B.Tech/B.E. in Electronics/Telecommunication, Information Technology, Mechanical, Computers, Electrical
Key Skills
Skills highlighted with are preferred keyskills","Scripting, Hadoop, Linux, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Data Engineer,Calsoft,8-13 Years,,"Bengaluru, Kolkata, Pune",Software,"Role & responsibilities
8+ years experienceson relevant field below (Internship, prototype, and personal projects won't be counted)
Coding is required. (Ideally Python or Java)
Own end to end lifecycle (From development to deployment to production environment)
Experience in building or deploying solution in the cloud.
EitherCloud Native (Serverless): S3, Lambda, AWS Batch, ECS
OrCloud Agnostic:Kubernetes, Helm Chart, ArgoCD, Prometeus, Grafana.
CICD experience:Github action or Jenkin.
Infrastructure as code: e.g., Terraform
And experience inat least one of this focus area:
Big Data:Building Big data pipeline or Platform to process petabytes of data: (PySpark, Hudi, Data Lineage, AWS Glue, AWS EMR, Kafka, Schema Registry)
OrGraphDB: Ingesting and consuming data in Graph Database such as Neo4J, AWS Neptune, JanusGraph or DGraph
Preferred candidate profile
Specifically highlight Kafka expertise - include details like:
Experience with Kafka cluster management and configuration
Stream processing with Kafka Streams or KSQL
Schema Registry implementation and management
Kafka Connect for data integration
Put significant focus on PySpark skills:
Experience building and optimizing PySpark jobs for batch processing
Stream processing with Spark Structured Streaming
Familiarity with Delta Lake, Hudi, or Iceberg for lakehouse implementation
Highlight data engineering skills that complement these technologies:
Data pipeline design and implementation
Experience with data quality, validation, and lineage tracking
Performance optimization for large-scale data processing","Sql, Python, Data Warehousing, Etl, AWS"
Data Engineer,Ford,5-7 Years,,Chennai,Automotive,"Were seeking a highly skilled and experienced Full Stack Data Engineer to play a pivotal role in the development and maintenance of our Enterprise Data Platform.
In this role, you'll be responsible for designing, building, and optimizing scalable data pipelines within our Google Cloud Platform (GCP) environment.
You'll work with GCP Native technologies like BigQuery, Dataflow, and Pub/Sub, ensuring data governance, security, and optimal performance.
This is a fantastic opportunity to leverage your full-stack expertise, collaborate with talented teams, and establish best practices for data engineering at Ford.
Bachelors degree in Computer Science, Information Technology, Information Systems, Data Analytics, or a related field (or equivalent combination of education and experience).
5-7 years of experience in Data Engineering or Software Engineering, with at least 2 years of hands-on experience building and deploying cloud-based data platforms (GCP preferred).
Strong proficiency in SQL, Java, and Python, with practical experience in designing and deploying cloud-based data pipelines using GCP services like BigQuery, Dataflow, and DataProc.
Solid understanding of Service-Oriented Architecture (SOA) and microservices, and their application within a cloud data platform.
Experience with relational databases (e.g., PostgreSQL, MySQL), NoSQL databases, and columnar databases (e.g., BigQuery).
Knowledge of data governance frameworks, data encryption, and data masking techniques in cloud environments.
Familiarity with CI/CD pipelines, Infrastructure as Code (IaC) tools like Terraform and Tekton, and other automation frameworks.
Excellent analytical and problem-solving skills, with the ability to troubleshoot complex data platform and microservices issues.
Experience in monitoring and optimizing cost and compute resources for processes in GCP technologies (e.g., BigQuery, Dataflow, Cloud Run, DataProc).
A passion for data, innovation, and continuous learning.
Data Pipeline Architect Builder:Spearhead the design, development, and maintenance of scalable data ingestion and curation pipelines from diverse sources. Ensure data is standardized, high-quality, and optimized for analytical use. Leverage cutting-edge tools and technologies, including Python, SQL, and DBT/Dataform, to build robust and efficient data pipelines.
End-to-End Integration Expert:Utilize your full-stack skills to contribute to seamless end-to-end development, ensuring smooth and reliable data flow from source to insight.
GCP Data Solutions Leader: Leverage your deep expertise in GCP services (BigQuery, Dataflow, Pub/Sub, Cloud Functions, etc.) to build and manage data platforms that not only meet but exceed business needs and expectations.
Data Governance Security Champion: Implement and manage robust data governance policies, access controls, and security best practices, fully utilizing GCPs native security features to protect sensitive data.
Data Workflow Orchestrator: Employ Astronomer and Terraform for efficient data workflow management and cloud infrastructure provisioning, championing best practices in Infrastructure as Code (IaC).
Performance Optimization Driver: Continuously monitor and improve the performance, scalability, and efficiency of data pipelines and storage solutions, ensuring optimal resource utilization and cost-effectiveness.
Collaborative Innovator: Collaborate effectively with data architects, application architects, service owners, and cross-functional teams to define and promote best practices, design patterns, and frameworks for cloud data engineering.
Automation Reliability Advocate: Proactively automate data platform processes to enhance reliability, improve data quality, minimize manual intervention, and drive operational efficiency.
Effective Communicator: Clearly and transparently communicate complex technical decisions to both technical and non-technical stakeholders, fostering understanding and alignment.
Continuous Learner: Stay ahead of the curve by continuously learning about industry trends and emerging technologies, proactively identifying opportunities to improve our data platform and enhance our capabilities.
Business Impact Translator: Translate complex business requirements into optimized data asset designs and efficient code, ensuring that our data solutions directly contribute to business goals.
Documentation Knowledge Sharer: Develop comprehensive documentation for data engineering processes, promoting knowledge sharing, facilitating collaboration, and ensuring long-term system maintainability.","Pub/Sub, BigQuery, data engineering, Terraform, DataFlow, Python, Sql"
Data Engineer,E Solutions,5-10 Years,,Ghaziabad,"Information Technology, Information Services","Proficiency in Python, Java, or SQL for data processing and automation.
Hands-on experience with cloud platforms (especially Azure, Snowflake, and Databricks).
Expertise in big data technologies (such as Apache Spark, Hadoop, or Kafka).
Deep understanding of data modeling, performance tuning, and optimization.
Familiarity with transformer-based architecture, Generative AI, and LLM data workflows
Leadership & Collaboration:
Proven ability to lead complex data engineering projects.
Strong mentorship skills and experience guiding junior engineers.
Excellent communication and teamwork skills to collaborate with cross-functional teams.","Data Processing, Transformers, Llm, Java, Big Data Technologies, Azure, Python, Sql"
Data Engineer,Alight,8-13 Years,,Gurugram,Information Technology,"Our story
At Alight, we believe a company s success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to Be Alight.
Our Values:
Champion People- be empathetic and help create a place where everyone belongs.
Grow with purpose -Be inspired by our higher calling of improving lives.
Be Alight -act with integrity, be real and empower others.
It s why we re so driven to connect passion with purpose. Alight helps clients gain a benefits advantage while building a healthy and financially secure workforce by unifying the benefits ecosystem across health, wealth, wellbeing, absence management and navigation.
With a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.
Join our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.
Learn more atcareers.alight.com.
Alight is seeking a skilled and passionate Data Engineer to join our team. As the Data Engineer, you will be a member of a team responsible for various stages of data pipelines development, including understanding business requirements, coding, testing, documentation, deployment, and production support. As part of the Data Analytics team, you will focus on delivering high-quality enterprise caliber systems on Alight s Data Lake. Your primary role will involve participating in full life-cycle data ingestion and data architecting development projects.
Qualifications:
Knowledge & Experience:
5+ years of data integration, data warehousing or data conversion experience.
3+ years of data modeling experience.
3+ years of Informatica IICS experience.
3+ years working with AWS Redshift.
2+ years AWS step functions, Lambda, Glue, and other Cloud Data Lake Services
1+ SSIS / SSRS experience
Excellent analytical and critical thinking skills
Strong interpersonal skills with the ability to work effectively with diverse and remote teams
Experience in agile processes and development task estimation
Strong sense of responsibility for deliverables
Ability to work in a small team with moderate supervision
Responsibility Areas:
Design data ingestion solutions for small to medium complexity requirements independently, adhering to existing standards
Develop high-priority and highly complex solutions for systems based on functional specifications, detailed design, maintainability, and coding and efficiency standards, working independently.
Estimate and evaluate risks, and prioritize technical tasks based on requirements
Collaborate actively with Data Engineering Lead, Product Owners, Quality Assurance, and stakeholders to ensure high-quality project delivery
Conduct formal code reviews to ensure compliance with standards
Utilize appropriately system design, development, and process standards
Create, maintain, and publish system-level documentation, including system diagrams, with minimal guidance
Ensure clarity, conciseness, and completeness of requirements before starting development, collaborating with Business Analysts and stakeholders to evaluate feasibility. Take primary accountability for meeting non-functional requirements.
Alight requires all virtual interviews to be conducted on video.
Flexible Working
So that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and Top 100 Company for Remote Jobs 5 years in a row.
Benefits
We offer programs and plans for a healthy mind, body, wallet and life because it s important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as several voluntary benefit options.
By applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight s employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.
Our commitment to Inclusion
We celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.
At Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.
Authorization to work in the Employing Country
Applicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.
Note, this job description does not restrict managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.
We offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.
DISCLAIMER:
Nothing in this job description restricts managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.
.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:Any Graduate
PG:Any Postgraduate","Manager Quality Assurance, Analytical, Data Conversion, Production Support, Coding, Data Modeling, Ssrs, Informatica, Project Delivery, SSIS"
Data Engineer,Kanini Software Solutions,5-10 Years,,"Bengaluru, Chennai, Pune",Software,"Job DescriptionWant to join KANINI
We are looking for aData Engineer who can build a robust database and its architecture. In thisrole, you will assess a wide range of requirements and apply relevant databasetechniques to create a sustainable data architecture before you begin theimplementation process and develop the database from scratch.
You areall set to:
Develop, maintain,evaluate, and test big data solutions. You will be involved in data engineeringactivities like creating pipelines/workflows for Source-to-Target Data Mapping amongothers.
You are someone who can:
You will be involved in the design of datasolutions using Hadoop based technologies along with Hadoop, Azure, HDInsightfor Cloudera based Data Late using Scala Programming.
Liaise and be part of our extensive GCP community,contributing in the knowledge exchange learning programme of the platform.
Be required to showcase your GCP Data engineeringexperience when communicating with business team on their requirements, turningthese into technical data solutions.
Be required to build and deliver Data solutionsusing GCP products and offerings.
Hands on and deep experience working with GoogleData Products (e.g. Big Query, Dataflow, Dataproc, AI Building Blocks, Looker,Cloud Data Fusion, Dataprep, etc.).
Experience in Spark /Scala / Python/Java / Kafka.
Responsible to Ingest data from files, streams anddatabases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT
Develop programs in Scala and Python as part ofdata cleaning and processing
Responsible to design and develop distributed, highvolume, high velocity multi-threaded event processing systems
Develop efficient software code for multiple usecases leveraging Python and Big Data technologies for various use cases builton the platform
Provide high operational excellence guaranteeinghigh availability and platform stability
Implement scalable solutions to meet theever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka,any Cloud computing etc
You bring in:
Minimum 4+ years of experience in Big Datatechnologies
Good to have experience in Cloud, GCP, AWS,Azure Data Engineering with background in Spark/Python/Scala / Java.
Proficient in any of the programming languages -Python, Scala or Java
Mandatory experience in Mid to Expert Levelprogramming capabilities in a large-scale enterprise
In-depth experience in modern data platformcomponents such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc
Experience with Distributed Versioning Controlenvironments such as GIT
Familiarity with development tools - experience oneither IntelliJ / Eclipse / VSCode IDE, Build Tool Maven
Demonstrated experience in modern API platformdesign including how modern UI s are built consuming services / APIs.
Experience on Azure cloud including Data Factory,Databricks, Data Lake Storage is highly preferred.
Solid experience in all phases of Software DevelopmentLifecycle - plan, design, develop, test, release, maintain and support, decommission.
Your qualificationis:
B.E/B.Tech/M.C.A/MSc(preferably in Computer Science/IT)
Role:Data Engineer
Industry Type:Software Product
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development","Cloud Computing, Gcp, Cloud, Scala, Spark, Eclipse, Software Development Life Cycle, Data Architecture, Big Data, Python"
Data Engineer,Qentelli Solutions Private Limited,4-9 Years,,Hyderabad,"Information Technology, Information Services","Role & responsibilities
Must Haves Skills
Extensive experience as Data Engineer with Python Language and Cloud Technologies (AWS preferably).
Experience in Automating ETL process/Pipelines and AWS Data & Infrastructure with Python.
Extensive experience with AWS components like S3, Athena, EMR, Glue, Redshift, Kinesis and SageMaker.
Extensive Experience with SQL/Unix/Linux scripting.
Developing/testing Experience on Cloud/On Prem ETL Technologies (Ab Initio, AWS Glue, Informatica, Alteryx).
Experience in Data migration from Onprem to Cloud is Plus.
Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data Science.
Extensive experience in DevOps/Data Ops space.
Having experience in Data Science platforms like SageMaker/Machine Learning Studio/ H2O is plus.
Work Description SDET Python, AWS, Unix and ETL.
Work with business stakeholders, Business Systems Analysts and Developers to ensure delivery of Data Applications.
Building Automation Frameworks using Python.
Designing and managing the data workflows using Python during development and deployment of data products
Design, development of Reports and dashboards.
Analyzing and evaluating data sources, data volume, and business rules.
Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.
Interpret and analyses data from various source systems to support data integration and data reporting needs.
Experience in testing Database Application to validate source to destination data movement and transformation.
Work with team leads to prioritize business and information needs.
Develop and summarize Data Quality analysis and dashboards.
Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.
Execute testing of data analytic and data integration on time and within budget.
Troubleshoot & determine best resolution for data issues and anomalies
Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platforms","Numpy, Amazon Web Services, Pandas, Python, AWS"
Data Engineer,Anblicks Solutions,8-13 Years,,Ahmedabad,Cloud Data Services,"Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise in Azure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake) along with big data processing, real-time analytics, and cloud data integration.
Key Responsibilities:
1. Data Pipeline Development ETL/ELT
Design and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.
Implement ETL/ELT workflows for structured and unstructured data processing.
Optimize data ingestion, transformation, and storage strategies.
2. Cloud Data Architecture Integration
Develop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).
Work with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.
3. Database Management Optimization
Design and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).
Optimize SQL queries and indexing strategies for performance.
Implement data partitioning, compression, and caching for efficiency.
4. Data Governance, Security Compliance
Ensure data quality, lineage, and governance with tools like Purview.
Implement role-based access control (RBAC), encryption, and security policies.
Ensure compliance with GDPR, HIPAA, and ISO 27001 regulations.
5. Monitoring Performance Tuning
Use Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.
Automate data pipeline testing and validation.
6. Collaboration Documentation
Document data models, pipeline architectures, and data workflows.
Technical Skills:
Cloud Data Services: Azure Data Factory, Azure Synapse, Databricks, SSIS, BigQuery.
ETL Data Pipelines: Apache Spark, Python, SQL.
Big Data Processing: Delta Lake, Parquet, Hadoop, Event Hubs.
Database Management: SQL Server, Cosmos DB.
Security Compliance: RBAC, Data Masking, Encryption, Purview.
Scripting Automation: Python, PowerShell, Terraform for IaC.","Cloud Architecture, Azure Data Factory, Data Governance, Performance Tuning"
Data Engineer,Genzeon Corporation,2-5 Years,,Pune,Information Technology,"Genzeon is looking for an experienced Full Stack Data Engineer to join our team in Pune. In this role, you will be instrumental in shaping our data architecture, developing full-stack data solutions, and driving innovation in data processing and analytics. This position offers an exciting opportunity to work with a team of passionate professionals dedicated to leveraging data for impactful decisions and products.
Responsibilities
Data Architecture:Design and build robust, scalable data architectures that support both operational and analytical use cases.
ETL Development:Develop and maintain ETL processes to gather data from various sources, ensuring data quality and consistency.
Data Modeling:Create and optimize data models to support efficient data storage, retrieval, and analysis.
API Development:Design and implement APIs for data ingestion, processing, and retrieval.
Data Visualization:Develop dashboards and reports to visualize complex datasets in a user-friendly manner.
Cloud Solutions:Work with cloud technologies to deploy and maintain data solutions.
Collaboration:Work closely with data scientists, analysts, and other engineers to integrate data solutions into company products and services.
Innovation:Stay updated with the latest trends and technologies in data engineering and propose innovative solutions to improve existing systems.
Requirements
Education:Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Experience:Proven experience as a Full Stack Data Engineer or in a similar role.
Technical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.
Data Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).
Front-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.
Cloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.
Analytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.
Communication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.
Benefits
Competitive salary and benefits package.
Dynamic and innovative work environment.
Opportunities for professional growth and development.
Flexible working hours and supportive team culture.
Must Have Skills:
Technical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.
Data Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).
Front-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.
Cloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.
Analytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.
Communication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.","Java, python, Spark, hadoop, Sql, aws"
Data Engineer,TechnoGen,6-9 Years,,Hyderabad,"Consulting, Information Services","Python Proficiency: Strong understanding of Python, with practical coding experience
AWS:Comprehensive knowledge of AWS services and their applications
Airflow: creating and managing Airflow DAG scheduling.
Unix & SQL: Solid command of Unix commands, shell scripting, and writing efficient SQL scripts
Analytical & Troubleshooting Skills: Exceptional ability to analyze data and resolve complex issues.
Development Tasks: Proven capability to execute a variety of development activities with efficiency
Insurance Domain Knowledge:Familiarity with the Insurance sector is highly advantageous.
Production Data Management: Significant experience in managing and processing production data
Work Schedule Flexibility:Open to working in any shift, including 24/7 support, as require","Airflow, Trouble Shooting, Core Python, Sql, AWS, data engineering, Data Management"
Data Engineer,E Solutions,5-10 Years,,Noida,"Information Technology, Information Services","Proficiency in Python, Java, or SQL for data processing and automation.
Hands-on experience with cloud platforms (especially Azure, Snowflake, and Databricks).
Expertise in big data technologies (such as Apache Spark, Hadoop, or Kafka).
Deep understanding of data modeling, performance tuning, and optimization.
Familiarity with transformer-based architecture, Generative AI, and LLM data workflows
Leadership & Collaboration:
Proven ability to lead complex data engineering projects.
Strong mentorship skills and experience guiding junior engineers.
Excellent communication and teamwork skills to collaborate with cross-functional teams.","Data Processing, Transformers, Llm, Java, Big Data Technologies, Python, Sql, Azure"
Data Engineer,IDESLABS,5-7 Years,,Pune,"Recruiting, Staffing Agency","Job description
5+ years of relevant experience in the field of Data Engineering
Advance skills in big data technologies like Hadoop, Python, Spark, SQL.
Must have experience building data APIs.
Bachelor s in Computer Science or related disciplines
Knowledge of Data Structures and Algorithm.
Strong Python programming skills with ability to implement OOPs and functional programming. Knowledge of Scala/Java would be plus.
Strong knowledge on RDBMS and NoSQL databases with the ability to implement them from scratch. Knowledge of Graph databases will be a plus.
Strong expertise in building & optimizing data pipelines, architectures, and data sets.
Experience working with different file formats like Parquet, ORC, Avro, RC, etc.
Experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc.
Knowledge and experience of using orchestration frameworks like Airflow, Oozie, Luigi, etc.
Experience using Spark, and building jobs using Python/Scala/Java.
Experience or Knowledge building stream processing platforms using Spark Streaming, Storm, etc. Knowledge of Kafka/Flink+Beam would be plus.
Knowledge of building REST API end points for data consumption.
Experience in building scalable data pipelines for both real time and batch using best practices in data modeling, ETL/ELT processes utilizing varioud technologies such as Spark, Kafka, Presto, SAP HANA, Airflow, informatica.
Perform Data analysis using Python, complex SQLs, and other tools.
Perform root cause analysis of issues from platform standpoint on Kubernetes, Containers, Hadoop, Spark, Hive, Presto
Excellent oral and written communication is a must.
Preferred
Masters in Computer Science or related disciplines
Experience building self-service tools for analytics would be plus.
Knowledge of ELK stack would be a plus.
Knowledge of implementing CI/CD on the pipelines is a plus.
Knowledge of Containerization (Docker/Kubernetes) will be plus.
Experience working with one of the popular Public Cloud based platforms is preferred","Airflow, Java, Python, Docker, Presto, Sap Hana, Spark, Kafka"
Data Engineer,Kanini Software Solutions,1-4 Years,,"Coimbatore, Bengaluru",Software,"Job DescriptionWant to join KANINI
We are looking for aData Engineer who can build a robust database and its architecture. In thisrole, you will assess a wide range of requirements and apply relevant databasetechniques to create a sustainable data architecture before you begin theimplementation process and develop the database from scratch.
You areall set to:
Develop, maintain,evaluate, and test big data solutions. You will be involved in data engineeringactivities like creating pipelines/workflows for Source-to-Target Data Mapping amongothers.
You are someone who can:
You will be involved in the design of datasolutions using Hadoop based technologies along with Hadoop, Azure, HDInsightfor Cloudera based Data Late using Scala Programming.
Liaise and be part of our extensive GCP community,contributing in the knowledge exchange learning programme of the platform.
Be required to showcase your GCP Data engineeringexperience when communicating with business team on their requirements, turningthese into technical data solutions.
Be required to build and deliver Data solutionsusing GCP products and offerings.
Hands on and deep experience working with GoogleData Products (e.g. Big Query, Dataflow, Dataproc, AI Building Blocks, Looker,Cloud Data Fusion, Dataprep, etc.).
Experience in Spark /Scala / Python/Java / Kafka.
Responsible to Ingest data from files, streams anddatabases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT
Develop programs in Scala and Python as part ofdata cleaning and processing
Responsible to design and develop distributed, highvolume, high velocity multi-threaded event processing systems
Develop efficient software code for multiple usecases leveraging Python and Big Data technologies for various use cases builton the platform
Provide high operational excellence guaranteeinghigh availability and platform stability
Implement scalable solutions to meet theever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka,any Cloud computing etc
You bring in:
Minimum 4+ years of experience in Big Datatechnologies
Good to have experience in Cloud, GCP, AWS,Azure Data Engineering with background in Spark/Python/Scala / Java.
Proficient in any of the programming languages -Python, Scala or Java
Mandatory experience in Mid to Expert Levelprogramming capabilities in a large-scale enterprise
In-depth experience in modern data platformcomponents such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc
Experience with Distributed Versioning Controlenvironments such as GIT
Familiarity with development tools - experience oneither IntelliJ / Eclipse / VSCode IDE, Build Tool Maven
Demonstrated experience in modern API platformdesign including how modern UI s are built consuming services / APIs.
Experience on Azure cloud including Data Factory,Databricks, Data Lake Storage is highly preferred.
Solid experience in all phases of Software DevelopmentLifecycle - plan, design, develop, test, release, maintain and support, decommission.
Your qualificationis:
B.E/B.Tech/M.C.A/MSc(preferably in Computer Science/IT)
Role:Data Engineer
Industry Type:Software Product
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development","Cloud Computing, Gcp, Cloud, Scala, Spark, Eclipse, Software Development Life Cycle, Data Architecture, Big Data, Python"
Data Engineer - Senior,Cummins India Limited,8-10 Years,,Pune,Manufacturing,"Although the role category specified in the GPP is Remote, the requirement is for Hybrid.
Key Responsibilities:
Design and Automation: Deploy distributed systems for ingesting and transforming data from various sources (relational, event-based, unstructured).
Data Quality and Integrity: Implement frameworks to monitor and troubleshoot data quality and integrity issues.
Data Governance: Establish processes for managing metadata, access, and retention for internal and external users.
Data Pipelines: Build reliable, efficient, scalable, and quality data pipelines with monitoring and alert mechanisms using ETL/ELT tools or scripting languages.
Database Structure: Design and implement physical data models to optimize database performance through efficient indexing and table relationships.
Optimization and Troubleshooting: Optimize, test, and troubleshoot data pipelines.
Large Scale Solutions: Develop and operate large-scale data storage and processing solutions using distributed and cloud-based platforms (e.g., Data Lakes, Hadoop, Hbase, Cassandra, MongoDB, Accumulo, DynamoDB).
Automation: Use modern tools and techniques to automate common, repeatable, and tedious data preparation and integration tasks.
Infrastructure Renovation: Renovate data management infrastructure to drive automation in data integration and management.
Agile Development: Ensure the success of critical analytics initiatives using agile development technologies such as DevOps, Scrum, Kanban.
Team Development: Coach and develop less experienced team members.
External Qualifications and Competencies
Qualifications:
College, university, or equivalent degree in a relevant technical discipline, or equivalent experience required. Licensing may be required for compliance with export controls or sanctions regulations.
Competencies:
System Requirements Engineering: Translate stakeholder needs into verifiable requirements; establish acceptance criteria; track requirements status; assess impact of changes.
Collaboration: Build partnerships and work collaboratively to meet shared objectives.
Communication: Develop and deliver communications that convey a clear understanding of the unique needs of different audiences.
Customer Focus: Build strong customer relationships and deliver customer-centric solutions.
Decision Quality: Make good and timely decisions to keep the organization moving forward.
Data Extraction: Perform ETL activities from various sources using appropriate tools and technologies.
Programming: Create, write, and test computer code, test scripts, and build scripts to meet business, technical, security, governance, and compliance requirements.
Quality Assurance Metrics: Apply measurement science to assess solution outcomes using ITOM, SDLC standards, tools, metrics, and KPIs.
Solution Documentation: Document information and solutions to enable improved productivity and effective knowledge transfer.
Solution Validation Testing: Validate configuration item changes or solutions using SDLC standards, tools, and metrics.
Data Quality: Identify, understand, and correct data flaws to support effective information governance.
Problem Solving: Solve problems using systematic analysis processes; implement robust, data-based solutions; prevent problem recurrence.
Values Differences: Recognize the value of different perspectives and cultures.
Additional Responsibilities Unique to this Position
Skills:
ETL/Data Engineering Solution Design and Architecture: Expert level.
SQL and Data Modeling: Expert level (ER Modeling and Dimensional Modeling).
Team Leadership: Ability to lead a team of data engineers.
MSBI (SSIS, SSAS): Experience required.
Databricks (Pyspark) and Python: Experience required.
Additional Skills: Snowflake, Power BI, Neo4j (good to have).
Communication: Good communication skills.
Preferred Experience:
8+ years of overall experience.
5+ years of relevant experience in data engineering.
Knowledge of the latest technologies and trends in data engineering.
Technologies: Familiarity with analyzing complex business systems, industry requirements, and data regulations.
Big Data Platform: Design and development using open source and third-party tools.
Tools: SPARK, Scala/Java, Map-Reduce, Hive, Hbase, Kafka.
SQL: Proficiency in SQL query language.
Cloud-Based Implementation: Experience with clustered compute cloud-based implementations.
Large File Movement: Experience developing applications requiring large file movement for cloud environments.
Analytical Solutions: Experience in building analytical solutions.
IoT Technology: Intermediate experience preferred.
Agile Software Development: Intermediate experience preferred.
Role:Data Engineer
Industry Type:Industrial Equipment / Machinery
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Any Specialization
PG:Any Postgraduate","snowflake, Pyspark, Databricks, Python, Sql"
Big Data Engineer (Java & Spark),Synechron Technologies Private Limited,5-10 Years,,Pune,Information Technology,"As a Big Data Engineer, you will be responsible for translating application storyboards and use cases into functional applications, designing and building efficient and reliable Java code. You will ensure optimal performance, quality, and responsiveness of applications, while also identifying bottlenecks and devising solutions. Your role will involve developing high-performance, low-latency components to run Spark clusters and collaborating with global teams to propose best practices and standards.
Technical Skills:
Programming Languages: Strong Java experience (8+ years) with Java 1.8 or higher; solid understanding of object-oriented programming and design patterns.
Big Data Technologies: Experience with HDFS, Hive, HBase, Apache Spark, and Kafka.
Data Processing: Proficient in processing data using Hive, Impala, and HBase; capable of performing analysis on large data sets.
APIs and Architecture: Experience in building self-service platform-agnostic data access APIs; knowledge of service-oriented architecture and data standards like JSON, Avro, and Parquet.
Analytical Skills: Experience in building advanced analytical models based on business context; strong analytical and problem-solving skills.
Development Practices: Familiarity with Agile/Scrum methodologies, SCMs like Git, and tools like JIRA; strong understanding of unit testing and SDLC activities.
Scripting and Databases: Experience with Linux shell scripting and RDBMS/NoSQL databases; good knowledge of database principles, practices, and SQL development (preferably with Oracle).
Performance Tuning: Experience in application performance tuning and troubleshooting in the Big Data domain.
Other Technologies: Familiarity with cloud and container technologies, build tools such as Maven, and continuous integration tools like Jenkins or Team City is a plus.
Experience:
5-10 years of experience in software development, with strong Java and Big Data technology expertise.
Demonstrated ability to design solutions and mentor other developers within the team.
Proven experience in working with large data volumes and logical data structures.
Day-to-Day Activities:
Translate application storyboards and use cases into functional applications.
Design, build, and maintain reliable and efficient Java code.
Ensure optimal performance, quality, and responsiveness of applications.
Identify and resolve bottlenecks and bugs.
Develop high-performance components for Spark clusters.
Collaborate with global teams to propose best practices and standards.
Test software prototypes and facilitate handover to the operations team.
Process data using Hive, Impala, and HBase, and conduct analysis on large datasets.
Mentor and guide team members in technical skills and best practices.
Qualifications:
Bachelor s or Master s degree in Computer Science, Information Technology, or a related field.
Optional: Familiarity with Arcadia Tool for Analytics.
Soft Skills:
Excellent analytical and problem-solving abilities.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Ability to mentor and guide junior developers.
Strong attention to detail and ability to work under pressure.
Creative thinking and initiative in proposing improvements and innovations.","Troubleshooting, Performance Tuning, RDBMS, Linux, Shell scripting, Data structures, Json, Oracle, Sdlc"
Data Engineer - Senior,Cummins India Limited,6-8 Years,,Pune,Manufacturing,"Job Summary:
Leads projects for the design, development, and maintenance of a data and analytics platform. Effectively and efficiently processes, stores, and makes data available to analysts and other consumers. Works with key business stakeholders, IT experts, and subject-matter experts to plan, design, and deliver optimal analytics and data science solutions. Works on one or many product teams at a time. Though the role category is generally listed as Remote, this specific position is designated as Hybrid.
Key Responsibilities:
Business Alignment & CollaborationPartner with the Product Owner to align data solutions with strategic goals and business requirements.
Data Pipeline Development & Management Design, develop, test, and deploy scalable data pipelines for efficient data transport into Cummins Digital Core (Azure DataLake, Snowflake) from various sources (ERP, CRM, relational, event-based, unstructured).
Architecture & Standardization Ensure compliance with AAI Digital Core and AAI Solutions Architecture standards for data pipeline design and implementation.
Automation & Optimization Design and automate distributed data ingestion and transformation systems, integrating ETL/ELT tools and scripting languages to ensure scalability, efficiency, and quality.
Data Quality & Governance Implement data governance processes, including metadata management, access control, and retention policies, while continuously monitoring and troubleshooting data integrity issues.
Performance & Storage Optimization Develop and implement physical data models, optimize database performance (indexing, table relationships), and operate large-scale distributed/cloud-based storage solutions (Data Lakes, Hadoop, HBase, Cassandra, MongoDB, Accumulo, DynamoDB).
Innovation & Tool Evaluation Conduct proof-of-concept (POC) initiatives, evaluate new data tools, and provide recommendations for improvements in data management and integration.
Documentation & Best Practices Maintain standard operating procedures (SOPs) and data engineering documentation to support consistency and efficiency.
Agile Development & Automation Use Agile methodologies (DevOps, Scrum, Kanban) to drive automation in data integration, preparation, and infrastructure management, reducing manual effort and errors.
Coaching & Team Development Provide guidance and mentorship to junior team members, fostering skill development and knowledge sharing.
External Qualifications and Competencies
Competencies:
System Requirements Engineering:Translates stakeholder needs into verifiable requirements, tracks status, and assesses impact changes.
Collaborates:Builds partnerships and works collaboratively with others to meet shared objectives.
Communicates Effectively:Delivers multi-mode communications tailored to different audiences.
Customer Focus:Builds strong customer relationships and provides customer-centric solutions.
Decision Quality:Makes good and timely decisions that drive the organization forward.
Data Extraction:Performs ETL activities from various sources using appropriate tools and technologies.
Programming:Develops, tests, and maintains code using industry standards, version control, and automation tools.
Quality Assurance Metrics:Measures and assesses solution effectiveness using IT Operating Model (ITOM) standards.
Solution Documentation:Documents knowledge gained and communicates solutions for improved productivity.
Solution Validation Testing:Validates configurations and solutions to meet customer requirements using SDLC best practices.
Data Quality:Identifies, corrects, and manages data flaws to support effective governance and decision-making.
Problem Solving:Uses systematic analysis to determine root causes and implement robust solutions.
Values Differences:Recognizes and leverages the value of diverse perspectives and cultures.
Education, Licenses, Certifications:
Bachelor's degree in a relevant technical discipline, or equivalent experience required.
This position may require licensing for compliance with export controls or sanctions regulations.
Additional Responsibilities Unique to this Position
Preferred Experience:
Technical Expertise Intermediate experience in data engineering with hands-on knowledge of SPARK, Scala/Java, MapReduce, Hive, HBase, Kafka, and SQL.
Big Data & Cloud Solutions Proven ability to design and develop Big Data platforms, manage large datasets, and implement clustered compute solutions in cloud environments.
Data Processing & Movement Experience developing applications requiring large-scale file movement and utilizing various data extraction tools in cloud-based environments.
Business & Industry Knowledge Familiarity with analyzing complex business systems, industry requirements, and data regulations to ensure compliance and efficiency.
Analytical & IoT Solutions Experience building analytical solutions with exposure to IoT technology and its integration into data engineering processes.
Agile Development Strong understanding of Agile methodologies, including Scrum and Kanban, for iterative development and deployment.
Technology Trends Awareness of emerging technologies and trends in data engineering, with a proactive approach to innovation and continuous learning.
Technical Skills:
Programming Languages:Proficiency in Python, Java, and/or Scala.
Database Management:Expertise in SQL and NoSQL databases.
Big Data Technologies:Hands-on experience with Hadoop, Spark, Kafka, and similar frameworks.
Cloud Services:Experience with Azure, Databricks, and AWS platforms.
ETL Processes:Strong understanding of Extract, Transform, Load (ETL) processes.
Data Replication:Working knowledge of replication technologies like Qlik Replicate is a plus.
API Integration:Experience working with APIs to consume data from ERP and CRM systems.
Role:Data Engineer
Industry Type:Industrial Equipment / Machinery
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development","snowflake, Pyspark, Databricks, Python, Sql, Azure, Hadoop"
"Sr. Big Data Engineer (Java, Apache Spark and Data Architecture)",Synechron Technologies Private Limited,7-9 Years,,Pune,Software,"Job Summary:
We are looking for a highly skilled Sr. Big Data Engineer professional to join our team. This role will focus on designing, developing, and maintaining large-scale data processing systems. The ideal candidate will have extensive experience in Big Data technologies, Java programming, and a strong understanding of data architecture principles. You will be responsible for translating functional requirements into robust data solutions, collaborating with global teams, and mentoring junior developers.
Overall Responsibilities:
Translate application storyboards and use cases into functional applications.
Design, build, and maintain efficient, reusable, and reliable Java code.
Ensure optimal performance, quality, and responsiveness of applications.
Identify bottlenecks and bugs, and devise appropriate solutions.
Develop high-performance and low-latency components to run Spark clusters.
Interpret functional requirements into design approaches suitable for the Big Data platform.
Collaborate with global teams across various locations to deliver high-quality solutions.
Propose best practices and standards, and ensure smooth handover to the operations team.
Conduct testing of software prototypes and facilitate the transfer to operational teams.
Process data using Hive, Impala, and HBase.
Analyze large data sets to derive actionable insights.
Technical Skills:
Core Skills:
Solid understanding of object-oriented programming and design patterns.
Strong experience with Java (8+ years) using Java 1.8 or higher.
Proficiency in working with large data volumes and logical data structures.
Big Data Technologies:
Experience with HDFS, Hive, HBase, Apache Spark, and Kafka.
Familiarity with building self-service platform-agnostic data access APIs.
Understanding of service-oriented architecture and data standards (JSON, Avro, Parquet).
Analytical Skills:
Experience in building advanced analytical models based on business context.
Strong systems analysis, design, and architecture fundamentals.
Development Tools:
Familiarity with Agile/Scrum methodologies.
Experience with source control management tools like GIT and project management tools like JIRA.
Basic proficiency in Linux shell scripting.
Understanding of RDMS and NoSQL databases.
Additional Skills:
Application performance tuning and troubleshooting in the Big Data domain.
Ability to write reliable, manageable, and high-performance code.
Knowledge of database principles, SQL development (preferably with Oracle).
Familiarity with concurrency patterns and multithreading in Java.
Understanding of domain design concepts, JDBC, and RESTful services.
Optional:
Familiarity with Arcadia Tool for Analytics.
Understanding of cloud and container technologies.
Experience with build tools such as Maven and continuous integration tools like Jenkins/TeamCity.
Experience:
Minimum of 8 years of experience in software development, with a focus on Big Data technologies.
Proven experience in mentoring and guiding other developers within a team.
Demonstrated analytical and problem-solving skills.
Day-to-Day Activities:
Participate in daily stand-up meetings and sprint planning sessions.
Collaborate with cross-functional teams to understand business requirements.
Write, test, and deploy scalable software solutions.
Conduct code reviews and provide feedback to team members.
Stay updated with the latest trends and advancements in Big Data technologies.
Provide technical support and mentorship to team members.
Qualifications:
Bachelor s or Master s degree in Computer Science, Information Technology, or a related field.
Soft Skills:
Excellent written and verbal communication skills.
Strong collaboration and teamwork abilities.
Exceptional problem-solving and analytical skills.
Ability to adapt to new technologies and changing requirements.
Strong time management and prioritization skills.","Big Data Engineer, Java, Spark"
Data Engineer,Amazon Development Centre (India) Private Limited,5-10 Years,,Hyderabad,Software,"Skill Data Engineer
Role T3, T2
Key responsibility
Data Engineer Must have 5+ years of experience in below mentioned skills. Must Have Big Data Concepts , Python(Core Python- Able to write code), SQL, Shell Scripting, AWS S3 Good to Have Event-driven/AWA SQS, Microservices, API Development, Kafka, Kubernetes, Argo, Amazon Redshift, Amazon Aurora","Data Engineer, Sql, AWS"
Aws Data Engineer,Amazon Development Centre (India) Private Limited,3-7 Years,,Bengaluru,Software,"Cloud and AWS Expertise:
In-depth knowledge of AWS services related to data engineering: EC2, S3, RDS, DynamoDB, Redshift, Glue, Lambda, Step Functions, Kinesis, Iceberg, EMR, and Athena.
Strong understanding of cloud architecture and best practices for high availability and fault tolerance.
Data Engineering Concepts:
Expertise in ETL/ELT processes, data modeling, and data warehousing.
Knowledge of data lakes, data warehouses, and big data processing frameworks like Apache Hadoop and Spark.
Proficiency in handling structured and unstructured data.
Programming and Scripting:
Proficiency in Python, Pyspark and SQLfor data manipulation and pipeline development.
Expertise in working with data warehousing solutions like Redshift.","Aws Data Engineer, Hadoop, Redshift"
Data Engineer,Cummins India Limited,4-8 Years,,Pune,Manufacturing,"Job Summary:
Supports, develops and maintains a data and analytics platform. Effectively and efficiently process, store and make data available to analysts and other consumers. Works with the Business and IT teams to understand the requirements to best leverage the technologies to enable agile data delivery at scale.
Key Responsibilities:
Implements and automates deployment of our distributed system for ingesting and transforming data from various types of sources (relational, event-based, unstructured). Implements methods to continuously monitor and troubleshoot data quality and data integrity issues. Implements data governance processes and methods for managing metadata, access, retention to data for internal and external users.
Develops reliable, efficient, scalable and quality data pipelines with monitoring and alert mechanisms that combine a variety of sources using ETL/ELT tools or scripting languages. Develops physical data models and implements data storage architectures as per design guidelines.
Analyzes complex data elements and systems, data flow, dependencies, and relationships in order to contribute to conceptual physical and logical data models. Participates in testing and troubleshooting of data pipelines.
Develops and operates large scale data storage and processing solutions using different distributed and cloud based platforms for storing data (e.g. Data Lakes, Hadoop, Hbase, Cassandra, MongoDB, Accumulo, DynamoDB, others). Uses agile development technologies, such as DevOps, Scrum, Kanban and continuous improvement cycle, for data driven application.
Good Understanding of ETL framework and technologies
Technologies - Databricks, Pyspark, Python, Snowflake, SQL
Good to have skills - Power BI, Neo4j, Matillion
Good communication skills
Role:Data Engineer
Industry Type:Industrial Equipment / Machinery
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development","snowflake, Pyspark, Databricks, Python, Sql"
Data Engineer 2,Bread Financial,2-5 Years,,Bengaluru,Financial Services,"The Data Engineer II works on different projects of data engineering to support the use cases, data ingestion pipeline and identify potential process or data quality issues. The team also supports marketing analytic teams with analytical tools that enable our analytics and business communities to do their job easier, faster and smarter. The team brings together data from different internal external partners and builds a curated Marketing analytics focused data tools ecosystem. The Data Engineer plays a crucial role in building this ecosystem depending on the Marketing analytics communities need.
Essential Job Functions
Collaboration - Collaborates with internal/external stakeholders to manage data logistics - including data specifications, transfers, structures, and rules. Collaborates with business users, business analysts and technical architects in transforming business requirements into analytical workbenches, tools and dashboards reflecting usability best practices and current design trends. Demonstrates analytical, interpersonal and professional communication skills. Learns quickly and works effectively individually and as part of a team.
Process Improvement - Accesses, extracts, and transforms Credit and Retail data from a variety of sources of all sizes (including client marketing databases, 2nd and 3rd party data) using Hadoop, Spark, SQL, Big data technologies etc. Provide automation help to analytical teams around data centric needs using orchestration tools, SQL and possibly other big data/cloud solutions for efficiency improvement.
Project Support - Supports Sr. Specialist and Specialist in new analytical proof of concepts and tool exploration projects. Effectively manages time and computing resources in order to deliver on time/correctly on concurrent projects. Involved in creating POCs to ingest and process streaming data using Spark and HDFS.
Data and Analytics - Answers and trouble shoots questions about data sets and analytical tools; Develops, maintains and enhances new and existing analytics tools/Frameworks to support internal customers/consumers. Ingests data from different sources, processes it according to the requirement document in order to store data to Hive or NoSQL database or different warehousing solutions. Manages data coming from different sources, involved in HDFS maintenance, and loading of structured and unstructured data. Applies knowledge in Agile Scrum methodology that leverages the Client BigData platform and used version control tool Git. Imports and exports data using Sqoop from HDFS to RDBMS and vice-versa. Demonstrates an understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management. Creates POCs to ingest and process streaming data using Spark and HDFS. Work on back-end using Scala, Python and Spark to perform several aggregation logics.
Technical Skills - Expert in writing complicated SQL Queries and database analysis for good performance. Experience working with python or Scala, Spark, Hadoop, Hive, Oozie, Sqoop, HDFS, Impala, Shell Scripts, Microsoft Azure Services like ADLS/Blob Storage solutions, Azure DataFactory, Azure Functions and Databricks. Utilize basic knowledge of Rest API for designing networked applications.
Reports to: Lead or above
Working Conditions/Physical Requirements: Normal office environment
Direct Reports: 0
Minimum Requirements:
Degree Required: Bachelor's Degree
Area of Study: Computer Science, Engineering
Years of Work Experience Required: Two to five years or more
Type / focus of work experience required: Data Analytics","Python, Data Analytics, Sql, Azure Cloud, Big Data, Rest Api"
Job | Immediately Hiring for AWS Data Engineer @ IT Company,SPN Globe,5-10 Years,INR 25 - 30.5 LPA,Pune,Login to check your skill match score,"Job | Immediately Hiring for AWS Data Engineer @ IT Company
SPN Globe is a specialized IT recruitment and staffing firm. We are the sourcing partner of various software companies on Permanent and Contract to Hire Demands. Also working with few top notch (CMMI-5 level) clients, PAN India clients and major city clients such as Hyderabad, Bengaluru, Pune, Mumbai, Nagpur and NCR.
Please find the position details below:
Company: IT Company
Role: AWS Data Engineer
Type: Permanent (No third-party payroll)
Location: Pune
Joining: 0 to 30 days / Immediate Joiners
Experience: 5 to 10 years
TECHNICAL SKILLS: Python,Spark, Scala, AWS, ETL,AWS Services
Apply immediately to grab it. Email your resume at [HIDDEN TEXT]
Also, immediately refer this opportunity to your friends.","Aws Services, Scala, Spark, Python, AWS, Etl"
Data Engineer ( CDP),MResult Technologies Private Limited,3-7 Years,,Remote,Login to check your skill match score,"Job Role: CDP Data Engineer
Why MResult
Founded in 2004, MResult is a global digital solutions partner trusted by leading Fortune 500 companies in industries such as pharma & healthcare, retail, and BFSI. MResult's expertise in data and analytics, data engineering, machine learning, AI, and automation help companies streamline operations and unlock business value. As part of our team, you will collaborate with top minds in the industry to deliver cutting-edge solutions that solve real-world challenges.
What We Offer:
At MResult, you can leave your mark on projects at the world's most recognized brands, access opportunities to grow and upskill, and do your best work with the flexibility of hybrid work models. Great work is rewarded, and leaders are nurtured from within.
Our values Agility, Collaboration, Client Focus, Innovation, and Integrity are woven into our culture, guiding every decision.
Website:https://mresult.com/
LinkedIn: https://www.linkedin.com/company/mresult/
What This Role Requires
In the role of CDP Data Engineer you will be a key contributor to MResult's mission of empowering our clients with data-driven insights and innovative digital solutions. Each day brings exciting challenges and growth opportunities. Here is what you will do:
Design, develop, and implement solutions using Customer Data Platform (CDP) to manage and analyze customer data.
Collaborate with cross-functional teams to understand business requirements and translate
them into technical solutions.
Integrate CDP with various data sources and ensure seamless data flow and accuracy.
Develop and maintain data pipelines, ensuring data is collected, processed, and stored efficiently.
Create and manage customer profiles, segments, and audiences within the CDP.
Implement data governance and security best practices to protect customer data.
Monitor and optimize the performance of the CDP infrastructure.
Provide technical support and troubleshooting for CDP-related issues.
Stay updated with the latest trends and advancements in CDP technology and best practices.
Key Skills to Succeed in This Role:
Overall experience 3-6 yrs
Experience in Customer Insights Data.
Experience in customer insights journey.
Experience in ADLS, ADF & Synapse is a must.
Experience in data verse, Power platform and Snowflake.
Manage, Master, and Maximize with MResult
MResult is an equal-opportunity employer committed to building an inclusive environment free of discrimination and harassment.
Take the next step in your career with MResult where your ideas help shape the future.","Customer Data Platform, Cdp, ADLS, Adf, Azure, Azure Synapse"
Data engineer Manager,Yotta Techports Private Limited,10-15 Years,INR 25 - 40 LPA,Hyderabad,Software,"Responsibilities:
Lead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.
Collaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.
Drive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.
Define and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.
Stay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.
Assume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.
Demonstrate proficiency in AWS services such as:
Expertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.
Experience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.
Knowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.
Proficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.
Demonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:
Deep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.
Hands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.
Ability to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.
Manage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.
Qualifications:
Bachelor's degree in computer science, Engineering, or a related field; advanced degree preferred.
10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.
Proficiency in programming languages such as Python, Java, or Scala, and expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).
Strong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).
Experience with data architecture design, data modeling, and optimization techniques.
Excellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.
Proven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.","Team Development, snowflake, Teambuilding, Stakeholder Management, amazon emr, Amazon Redshift, Aws S3, AWS Glue, Databricks, Aws Ec2"
Big Data Engineer,Robotics Technologies,10-15 Years,INR 11 - 16 LPA,"Gurugram, Hyderabad, Kolkata","Meeting Software, Private Cloud, Presentation Software, Computer Vision, Consumer Software, Operating Systems, Video Conferencing","We are seeking a highly skilled Big Data Engineer with 10-15 years of experience to join our team in India. The ideal candidate will be responsible for designing and implementing robust big data solutions, ensuring data is available and accessible for analytics and decision-making.
Responsibilities
Design, develop, and maintain scalable data pipelines and architectures for big data processing.
Collaborate with data scientists and analysts to understand data needs and provide timely data access.
Implement data integration processes and ETL workflows to ensure data quality and integrity.
Monitor and optimize performance of big data solutions and infrastructure.
Work with various big data technologies such as Hadoop, Spark, and Kafka.
Ensure data security and compliance with relevant regulations.
Stay updated with emerging technologies and propose enhancements to the existing systems.
Skills and Qualifications
10-15 years of experience in Big Data technologies and frameworks.
Proficiency in programming languages such as Java, Scala, or Python.
Strong experience with big data tools like Hadoop, Spark, Hive, and Kafka.
Knowledge of data modeling, ETL processes, and data warehousing concepts.
Experience with cloud platforms (AWS, Azure, Google Cloud) and their big data services.
Strong analytical and problem-solving skills with attention to detail.
Ability to work independently and collaboratively in a fast-paced environment.","Nosql, Hadoop, Data Modeling, Cloud Services, Spark, Kafka, Data Warehousing, Sql, Python, Etl"
Big Data Engineer,Robotics Technologies,3-7 Years,INR 12 - 44 LPA,"Ahmedabad, Bengaluru, Chennai","Cyber Security, Unified Communications, Data Center Automation, CMS, Messaging","Description
We are seeking a skilled Big Data Engineer to join our team in India. The ideal candidate will be responsible for designing, building, and maintaining scalable data processing systems to handle large volumes of data. You will work closely with data scientists and analysts to ensure that our data infrastructure meets the needs of our organization.
Responsibilities
Design and implement scalable data pipelines and architecture for big data solutions.
Analyze and process large datasets to extract valuable insights and support decision-making.
Collaborate with data scientists and analysts to optimize data processing and storage solutions.
Ensure data quality and integrity through monitoring and troubleshooting data pipelines.
Stay updated with emerging big data technologies and trends to enhance existing systems.
Skills and Qualifications
3-7 years of experience in big data technologies such as Hadoop, Spark, and Kafka.
Proficient in programming languages such as Java, Scala, or Python.
Strong understanding of database systems including SQL and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with data modeling and ETL processes.
Familiarity with cloud platforms like AWS, Azure, or Google Cloud.
Knowledge of data warehousing solutions and tools like Snowflake or Redshift.
Ability to work with large datasets and perform data processing using tools like Hive or Pig.","ETL Processes, Nosql, Hadoop, Data Modeling, Spark, Kafka, Data Warehousing, Sql, Python, AWS"
Data Engineer,Commonwealth Bank,5-7 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title:Data Engineer-Big Data
Location:Bengaluru
Business & Team:RM & FS Data Engineering
Impact & contribution:
As a Senior Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you're ahead of trends and work at the forefront of Big Data and Data warehouse technologies.
Which is why we're the perfect fit for you. Here, you'll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers most complex data-centric problems.
To us, data is everything. It is what powers our cutting-edge features and it's the reason we can provide seamless experiences for millions of customers from app to branch.
We're responsible for CommBank's key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We work across the Cloudera Hadoop Big Data, Teradata Group Data Warehouse and Ab Initio platforms.
Roles & Responsibilities:
Passionate about building next generation data platforms and data pipeline solution across the bank.
Enthusiastic, be able to contribute and learn from wider engineering talent in the team.
Ready to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.
Capable to create both technology blueprints and engineering roadmaps, for a multi- year data transformational journey.
Can lead and drive a culture where quality, excellence and openness are championed.
Constantly thinking outside the box and breaking boundaries to solve complex data problems.
Have experience in providing data driven solutions that source data from various enterprise data platform into Cloudera Hadoop Big Data environment, using technologies like Spark, MapReduce, Hive, Sqoop, Kafka transform and process the source data to produce data assets and transform and egression to other data platforms like Teradata or RDBMS system.
Are experienced in building effective and efficient Big Data and Data Warehouse frameworks, capabilities, and features, using common programming language (Scala, Java, or Python), with proper data quality assurance and security controls.
Are experienced in providing data driven solutions in the Cloud to build various enterprise data platform into AWS platform using technologies like S3, EMR, Glue, Iceberg, Kinesis or MSK/Kafka transform and process the data to produce data assets for Redshift and DocumentDB.
Are confident in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.
Can collaborate, co-create and contribute to existing Data Engineering practices in the team.
Have experience and responsible for data security and data management.
Have a natural drive to educate, communicate and coordinate with different internal stakeholders.
Essential Skills:
Preferably with at least 5+ years of hands-on experience in a Data Engineering role.
Experience in designing, building, and delivering enterprise-wide data ingestion, data integration and data pipeline solutions using common programming language (Scala, Java, or Python) in a Big Data and Data Warehouse platform.
Experience in building data solution in Hadoop platform, using Spark, Hive,MapReduce, Sqoop, Kafka and various ETL frameworks for distributed data storage and processing. Preferably with at least 5+ years of hands-on experience.
Experience in building data solution using AWS Cloud technology (EMR, Glue, Iceberg, Kinesis, MSK/Kafka, Redshift, DocumentDB, S3, etc.). Preferably with 2+ years of hands-on experience and certified AWS Data Engineer.
Strong Unix/Linux Shell scripting and programming skills in Scala, Java, or Python.
Proficient in SQL scripting, writing complex SQLs for building data pipelines.
Experience in working in Agile teams, including working closely with internal business stakeholders.
Familiarity with data warehousing and/or data mart build experience in Teradata, Oracle or RDBMS system is a plus.
Certification on Cloudera CDP, Hadoop, Spark, Teradata, AWS, Ab Initio is a plus.
Experience in Ab Initio software products (GDE, Co Operating System, Express It, etc.) is a plus.
Educational Qualifications: B.Tech and above
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 05/06/2025","MSK, DocumentDB, Glue, Teradata, Iceberg, Emr, Sql, Java, Hadoop, Kafka, Linux, Sqoop, Hive, Mapreduce, Big Data, Scala, S3, Unix, Cloudera, Kinesis, Ab Initio, AWS, Python, Redshift, Spark"
Data Engineer,InfoDriven Solutions Private Limited,2-4 Years,INR 12 - 20 LPA,Thiruvananthapuram / Trivandrum,Login to check your skill match score,"Job Type: Perm
Exp:2-4yrs
Work Location: Trivandrum
Work From Office
Required Qualifications
Bachelor's degree in Computer Science, Engineering, or a related field.
2+ years of experience in data engineering or related roles.
Proficiency in SQL and programming languages such as Python, Java, or Scala.
Experience with big data tools (e.g., Hadoop, Spark, Kafka). Familiarity with cloud platforms (AWS, GCP, Azure) and data warehousing solutions (e.g., Redshift, BigQuery, Snowflake).
Understanding of data modeling, ETL pipelines, and workflow orchestration tools (e.g., Airflow, Luigi).
Strong problem-solving skills and attention to detail","snowflake, Etl Design, BigQuery, Hadoop, Spark, Kafka, Data Engineer, Redshift, Etl, Java, Java, python"
Data Engineer,Robotics Technologies,10-15 Years,INR 11 - 16 LPA,"Gurugram, Hyderabad, Bengaluru","Meeting Software, Enterprise Applications, Presentation Software, Data Visualization, Presentations, Operating Systems","We are seeking a highly skilled Data Engineer with 10-15 years of experience to join our dynamic team in India. The ideal candidate will be responsible for designing and maintaining scalable data architecture, building data pipelines, and ensuring the availability and integrity of data for analysis and reporting.
Responsibilities
Design, construct, install, and maintain large-scale processing systems and data pipelines.
Develop data set processes for data modeling, mining, and production.
Analyze and organize data to provide insights and support business decisions.
Collaborate with data scientists and other stakeholders to understand data needs and deliver solutions.
Ensure data quality and integrity by implementing data validation checks and troubleshooting issues.
Optimize and improve data delivery architecture and processes for efficiency.
Skills and Qualifications
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Strong proficiency in programming languages such as Python, Java, or Scala.
Experience with big data technologies such as Hadoop, Spark, or Kafka.
In-depth knowledge of SQL and NoSQL databases like MySQL, PostgreSQL, MongoDB, or Cassandra.
Familiarity with data warehousing solutions such as Amazon Redshift, Google BigQuery, or Snowflake.
Understanding of ETL (Extract, Transform, Load) processes and tools.
Ability to work with cloud platforms like AWS, Azure, or Google Cloud Platform.","Nosql, Hadoop, Data Modeling, Spark, Kafka, Data Warehousing, Python, Sql, Etl, AWS"
Data Engineer,Robotics Technologies,3-7 Years,INR 18 - 54.5 LPA,"Noida, Delhi NCR, Pune","Information Technology, Sales Automation, Information and Communications Technology (ICT), Reputation","Description
We are seeking a skilled Data Engineer to join our team in India. The ideal candidate will be responsible for designing and maintaining data systems that support our organization's data needs. You will work with various stakeholders to ensure data is collected, stored, and processed efficiently.
Responsibilities
Design, develop, and maintain scalable data pipelines and ETL processes.
Collaborate with data scientists and analysts to understand data requirements and ensure data quality.
Implement data models and maintain data architecture to support analytics and reporting needs.
Monitor and optimize data performance and data storage solutions.
Ensure data security and compliance with relevant regulations.
Skills and Qualifications
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
3-7 years of experience in data engineering or a related field.
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).
Experience with big data technologies (e.g., Hadoop, Spark) and data warehousing solutions (e.g., Redshift, Snowflake).
Strong programming skills in Python, Java, or Scala.
Experience with data pipeline orchestration tools (e.g., Apache Airflow, Luigi).
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services.","Nosql, Hadoop, Data Modeling, Cloud Services, Spark, Apache Kafka, Data Warehousing, Python, Sql, Etl"
Data Engineer,G Jayaprabu ( Proprietor of JP Professional Recruiter & Services),3-6 Years,INR 8 - 9 LPA,Chennai,IT Management,"We are hiring Data Engineer
Industry Type- US Based IT Company
Location - Guindy, Chennai,
Experience - 36 years
Salary - 9LPA
Responsibilities:
* Build and maintain scalable data pipelines and ETL processes for BI needs.
* Optimize data models and ensure clean, reliable, and well-structured data for Power BI.
* Integrate data from various internal systems (web apps, databases, accounting platforms).
* Collaborate with BI analysts and managers to meet reporting requirements.
* Strong SQL development and performance tuning skills.
* Hands-on experience with Power BI dataset structuring and integration.
* Familiarity with Python for ETL, automation, or data cleaning tasks.
* Knowledge of APIs and scripting for data ingestion
Suitable candidates can contact
WhatsApp 9042537746","Power Bi, Data Engineer, Sql, Etl"
Senior Data Engineer,Anblicks Solutions,5-9 Years,,Ahmedabad,Cloud Data Services,"Key Responsibilities
Design and develophigh-performance, scalable data pipelinesusingPySpark, SQL, and cloud platforms.
Optimizedistributed data processingfor improved efficiency and reliability.
Implementbest practices in data modeling, architecture, and governance.
Ensuredata integrity, security, and compliancewithin cloud environments.
Automatedata workflows, monitor pipeline performance, and troubleshoot issues proactively.
Providetechnical leadership and mentorshipto junior engineers, conduct code reviews, and establish best practices.
Required Skills & Experience
4+ years of hands-on experienceinData Engineeringwith expertise inPython, PySpark,Spark, SQL, and Cloud platforms.
Strong proficiency inSnowflake, Databricks, or Microsoft Fabric.
Solid experience indata architecture, modeling, and pipeline optimization.
Hands-on experience withworkflow orchestration tools(e.g.,Airflow, DBT).
Proven ability tomentor and leadjunior engineers while driving best practices in data engineering.","Modeling, python, data engineering, databricks, Pyspark, Data Architecture"
Data Engineer II,Tekion,3-5 Years,,Chennai,Software,"Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.
Should be able to design (high level / low level) software solutions for the new requirements.
Coding independently and with other team members with proper software industry standard best practices.
Collaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.
Qualifications:
B.E/MTech in computer science
3 - 5 yearsof relevant work experience.
Experience in building scalable products with preferably big data.
ExcellentPythoncoding skills (Mandatory)
Experience inApache spark, Data Lakeand other Big data technologies.
Experience in either Data Warehouses or Relational Database is mandatory.
Experience inAWScloud
Mandatory Skills: Python , Spark","Spark, Data Lake, Apache Spark, Data Engineer, Python, Aws Cloud"
GCP Data Engineer,IDESLABS,4-8 Years,,Pune,"Recruiting, Staffing Agency","Job description
To ensure successful initiation, planning, execution, control and completion of the project by guiding team members on technical aspects, conducting reviews of technical documents and artefacts
Lead project development, production support and maintenance activities
Fill and ensure timesheets are completed, as is the invoicing process, on or before the deadline
Lead the customer interface for the project on an everyday basis, proactively addressing any issues before they are escalated
Create functional and technical specification documents
Track open tickets/ incidents in queue and allocate tickets to resources and ensure that the tickets are closed within the deadlines
Ensure analysts adhere to SLA s/KPI s/OLA s
Ensure that all in the delivery team, including self, are constantly thinking of ways to do things faster, better or in a more economic manner
Lead and ensure project is in compliance with Software Quality Processes and within timelines
Review functional and technical specification documents
Serve as the single point of contact for the team to the project stakeholders
Promote team work, motivate, mentor and develop subordinates
Provide application production support as per process/RACI (Responsible, Accountable, Consulted and Informed) Matrix","Project Managment, Crm, Qa"
Data Engineer,FCS Software Solutions,6-11 Years,,Delhi,Information Technology,"Description:
Seeking someone who has experience/knowledge in manufacturing. SQL, Power BI and DAX calculations are a must for skillsets for this position. Candidates do not require to have a degree, as long as they have the required skillset/experiences from the job description. We are looking for a Data Engineer with MFG experience for a staff augmentation role.
These are the requirements for the role:
Expert in Azure Data Factory
Proven experience in Data Modelling for Manufacturing data sources.
Proficient SQL design
+5 years of experience in Data engineering roles
Prove experience in PBI: Dashboarding, DAX calculations, Star scheme development and semantic model building
Manufacturing knowledge
Experience with GE PPA as data source is desirable
API dev Knowledge
Python skills","DAX Calculations, Azure Data Factory, Power Bi, Sql, Python"
Data Engineer,Alight,8-13 Years,,Gurugram,Information Technology,"Our story
At Alight, we believe a company s success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to Be Alight.
Our Values:
Champion People- be empathetic and help create a place where everyone belongs.
Grow with purpose -Be inspired by our higher calling of improving lives.
Be Alight -act with integrity, be real and empower others.
It s why we re so driven to connect passion with purpose. Alight helps clients gain a benefits advantage while building a healthy and financially secure workforce by unifying the benefits ecosystem across health, wealth, wellbeing, absence management and navigation.
With a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.
Join our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.
Learn more atcareers.alight.com.
Alight is seeking a skilled and passionate Data Engineer to join our team. As the Data Engineer, you will be a member of a team responsible for various stages of data pipelines development, including understanding business requirements, coding, testing, documentation, deployment, and production support. As part of the Data Analytics team, you will focus on delivering high-quality enterprise caliber systems on Alight s Data Lake. Your primary role will involve participating in full life-cycle data ingestion and data architecting development projects.
Qualifications:
Knowledge & Experience:
5+ years of data integration, data warehousing or data conversion experience.
3+ years of data modeling experience.
3+ years of Informatica IICS experience.
3+ years working with AWS Redshift.
2+ years AWS step functions, Lambda, Glue, and other Cloud Data Lake Services
1+ SSIS / SSRS experience
Excellent analytical and critical thinking skills
Strong interpersonal skills with the ability to work effectively with diverse and remote teams
Experience in agile processes and development task estimation
Strong sense of responsibility for deliverables
Ability to work in a small team with moderate supervision
Responsibility Areas:
Design data ingestion solutions for small to medium complexity requirements independently, adhering to existing standards
Develop high-priority and highly complex solutions for systems based on functional specifications, detailed design, maintainability, and coding and efficiency standards, working independently.
Estimate and evaluate risks, and prioritize technical tasks based on requirements
Collaborate actively with Data Engineering Lead, Product Owners, Quality Assurance, and stakeholders to ensure high-quality project delivery
Conduct formal code reviews to ensure compliance with standards
Utilize appropriately system design, development, and process standards
Create, maintain, and publish system-level documentation, including system diagrams, with minimal guidance
Ensure clarity, conciseness, and completeness of requirements before starting development, collaborating with Business Analysts and stakeholders to evaluate feasibility. Take primary accountability for meeting non-functional requirements.
Alight requires all virtual interviews to be conducted on video.
Flexible Working
So that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and Top 100 Company for Remote Jobs 5 years in a row.
Benefits
We offer programs and plans for a healthy mind, body, wallet and life because it s important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as several voluntary benefit options.
By applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight s employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.
Our commitment to Inclusion
We celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.
At Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.
Authorization to work in the Employing Country
Applicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.
Note, this job description does not restrict managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.
We offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.
DISCLAIMER:
Nothing in this job description restricts managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.
.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:Any Graduate
PG:Any Postgraduate","Manager Quality Assurance, Analytical, Data Conversion, Production Support, Coding, Data Modeling, Ssrs, Informatica, Project Delivery, SSIS"
Gcp Data Engineer,Tredence Analytics Solutions Private Limited,6-11 Years,,Bengaluru,Software,"Role:- GCP Data Engineer/GCP Architect
Experience:-6+ Years
Location :- Pan India
Role & responsibilities
Cloud Skills:
Strong experience with Google Cloud Platform (GCP), particularly BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Functions, and Cloud Composer.
Understanding of cloud architecture and scalability best practices.
Programming & Scripting:
Proficiency in Python, SQL, Java, or Scala for data manipulation and pipeline development.
Knowledge of shell scripting and automation tools.
Data Engineering:
Experience in designing and implementing ETL/ELT workflows and data pipelines.
Familiarity with data modeling, schema design, and optimization techniques.
Big Data & Analytics:
Experience with distributed computing and big data tools (Apache Hadoop, Spark, or similar frameworks).
Proficient in querying and managing large datasets in BigQuery.
Collaboration Tools:
Experience with tools like Git, JIRA, or similar for version control and project management.
Soft Skills:
Strong problem-solving and analytical skills.
Excellent communication skills to interact with technical and non-technical teams.
Preferred Qualifications:
Experience in machine learning or data science applications.
Certification in Google Cloud Professional Data Engineer or equivalent.
Knowledge of data visualization tools like Google Data Studio, Tableau, or Power BI.","Gcp Data Engineer, Git, JIRA, ELT"
Data Engineer - MDM/PIM/Syndigo/Attacama/Informatica,Reflections Info Systems,6-11 Years,,"Chennai, Pune",IT Management,"Responsibilities include:
Work as part of a team to develop Data and Analytics solutions.
Participate in the development of cloud data warehouses, data as a service, business intelligence solutions
Ability to provide solutions that are forward-thinking in data integration.
Deliver a quality product.
Developing Modern Data Warehouse solutions using Azure or AWS Stack
Certifications :
Bachelor s degree in computer science & engineering or equivalent demonstrable experience
Desirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.
Primary Skills :
6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions
Programming experience in Scala or Python, SQL
Min 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks
Working knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse
Demonstrated analytical and problem-solving skills
Excellent written and verbal skills (English)
Secondary Skills :
Familiar with Agile Practices
Familiar with Version control platforms GIT, CodeCommit etc.
Problem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.
Ownership
Proactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
GCP Data Engineer,Nihilent,5-7 Years,,Pune,Information Technology,"Unnesting of JSON files in Google BigQuery
Google Cloud Run (using container services, python)
Hands on experience in GCP services like composer, cloud function, cloud run
Experience in developing CI/CD pipelines.
Experience in DBT scripts, docker files and knowledge on datavault is a plus.
Strong technical knowledge and hands on experience of python or java
Role: Data Engineer
Industry Type: IT Services & Consulting
Department: Data Science & Analytics
Employment Type: Full Time, Permanent
Role Category: Data Science & Machine Learning
Education
UG: Any Graduate
PG: Any Postgraduate","Gcp, Cloud, Json, Python"
Data Engineer Architect,TechnoGen,10-19 Years,,Hyderabad,"Consulting, Information Services","Basic Qualifications
Bachelors degree in computer science, engineering or a related field
Data: 8+ years of experience with data analytics and warehousing inInvestment& Finance Domain
SQL: Deep knowledge of SQL and query optimization
ELT: Good understanding of ELT methodologies and tools
Troubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues
Communication: Excellent communication, problem solving and organizational and analytical skills
Able to work independently and to provide leadership to small teams of developers
3+ years of coding and scripting (Python, Java, Scala) and design experience.","Java, Scala, Data Analytics, Sql, Python, data engineering, Data Architecture"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Delhi, Bengaluru, Chennai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS.
This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives.
Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.
User will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training
Demonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.
Possess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.
Optimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.
Develop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.
Monitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.
Stay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.
Collaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.
Ensure data privacy and security compliance in accordance with industry standards and regulations.
Qualifications we seek in you:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Strong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.
Strong understanding of data warehousing concepts, ETL processes, and data modeling.
Strong understanding of S3 and code-based scripting to move large volumes of data across application storage layers
Familiarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.
Excellent problem-solving, analytical, and critical thinking skills.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Preferred Qualifications/ skills
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.
Experience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.
Familiarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.
A continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Data Processing, Cloud Computing, data engineering, Machine Learning, SSIS, Python, AWS"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,Pune,Software,"Key Responsibilities:
- Design, develop, and maintain data pipelines using Python, SQL, and Kedro
- Implement serverless solutions using AWS Lambda and Step Functions
- Develop and manage data workflows in Azure and AWS cloud environments
- Create integrations between data systems and Power Platform (Power Apps, Power Automate)
- Design, develop, and maintain APIs for data exchange and integration
- Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
- Optimize data storage and retrieval processes for improved performance
- Collaborate with cross-functional teams to understand data requirements and provide solutions
- API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","data engineering, Database Design, Cloud Services, Data Extraction, Azure, Sql, Python"
Data Engineer,Testingxperts Inc,0-3 Years,,Chandigarh,Information Technology,"Job Details
Data Engineer with 0-1 years of experience in Power BI, sql, Python.
It is 6 Months internship post to that fulltime, All 2years bond .
Certification in relevant field best to have
Job Type: Full-time
Fixed shift: Monday to Friday
Morning shift
Work Location: In person,","Powerbi, Power Bi, Python, Sql"
Associate Data Engineer- GET,XenonStack,0-1 Years,,Mohali,Information Technology,"Job Responsibilities -
Develop, construct, test and maintain Data Platform Architectures
Align Data Architecture with business requirements
Liaising with coworkers and clients to elucidate the requirements for each task.
Scalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analyzed quickly by BI AI Teams .
Reformulating existing frameworks to optimize their functioning.
Transforming Raw Data into InSights for manipulation by Data Scientists.
Ensuring that your work remains backed up and readily accessible to relevant coworkers.
Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.
Technical Requirements-
Experience of Python, Java/Scala
Great Statistical / SQL based Analytical Skills
Experience of Data Analytics Architectural Design Patterns for Batch , Event Driven and Real-Time Analytics Use Cases
Understanding of Data warehousing, ETL tools, machine learning, Data EPIs
Excellent in Algorithms and Data Systems
Understanding of Distributed System for Data Processing and Analytics
Familiarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores .
Professional Attributes-
Excellent communication skills
Attention to detail
Analytical mind and Problem Solving Aptitude
Strong Organizational skills","Consulting, Analytical, Architectural Design, Machine Learning, Big Data Analytics"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS.
This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives.
Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.
User will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training
Demonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.
Possess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.
Optimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.
Develop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.
Monitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.
Stay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.
Collaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.
Ensure data privacy and security compliance in accordance with industry standards and regulations.
Qualifications we seek in you:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Strong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.
Strong understanding of data warehousing concepts, ETL processes, and data modeling.
Strong understanding of S3 and code-based scripting to move large volumes of data across application storage layers
Familiarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.
Excellent problem-solving, analytical, and critical thinking skills.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Preferred Qualifications/ skills
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.
Experience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.
Familiarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.
A continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Data Processing, Cloud Computing, data engineering, Machine Learning, SSIS, Python, AWS"
Data Engineer II,Tekion,3-5 Years,,Chennai,Software,"Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.
Should be able to design (high level / low level) software solutions for the new requirements.
Coding independently and with other team members with proper software industry standard best practices.
Collaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.
Qualifications:
B.E/MTech in computer science
3 - 5 yearsof relevant work experience.
Experience in building scalable products with preferably big data.
ExcellentPythoncoding skills (Mandatory)
Experience inApache spark, Data Lakeand other Big data technologies.
Experience in either Data Warehouses or Relational Database is mandatory.
Experience inAWScloud
Mandatory Skills: Python , Spark","Spark, Data Lake, Apache Spark, Data Engineer, Python, Aws Cloud"
Aws Data Engineer,Coditas Technologies,3-7 Years,,Pune,Software,"We are looking for data engineers who have the right attitude, aptitude, skills, empathy,
compassion, and hunger for learning. Build products in the data analytics space. A passion
for shipping high-quality data products, interest in the data products space; curiosity about
the bigger picture of building a company, product development and its people.
Roles and Responsibilities
Develop and manage robust ETL pipelines using Apache Spark (Scala)
Understand park concepts, performance optimization techniques and governance
tools
Develop a highly scalable, reliable, and high-performance data processing pipeline to
extract, transform and load data from various systems to the Enterprise Data
Warehouse/Data Lake/Data Mesh
Collaborate cross-functionally to design effective data solutions
Implement data workflows utilizing AWS Step Functions for efficient orchestration.
Leverage AWS Glue and Crawler for seamless data cataloging and automation
Monitor, troubleshoot, and optimize pipeline performance and data quality
Maintain high coding standards and produce thorough documentation. Contribute to
high-level (HLD) and low-level (LLD) design discussions
Technical Skills
Minimum 3 years of progressive experience building solutions in Big Data
environments.
Have a strong ability to build robust and resilient data pipelines which are scalable,
fault tolerant and reliable in terms of data movement.
3+ years of hands-on expertise in Python, Spark and Kafka.
Strong command of AWS services like EMR, Redshift, Step Functions, AWS Glue, and
AWS Crawler.
Strong hands on capabilities on SQL and NoSQL technologies.
Sound understanding of data warehousing, modeling, and ETL concepts
Familiarity with High-Level Design (HLD) and Low-Level Design (LLD) principles
Excellent written and verbal communication skills.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate","Time Management, Backend, Analytical, Business Strategy, Data Processing, Actuarial, Big Data, Data Analytics, Apache, Sql"
Gcp Data Engineer,Tredence Analytics Solutions Private Limited,6-11 Years,,Bengaluru,Software,"Role:- GCP Data Engineer/GCP Architect
Experience:-6+ Years
Location :- Pan India
Role & responsibilities
Cloud Skills:
Strong experience with Google Cloud Platform (GCP), particularly BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Functions, and Cloud Composer.
Understanding of cloud architecture and scalability best practices.
Programming & Scripting:
Proficiency in Python, SQL, Java, or Scala for data manipulation and pipeline development.
Knowledge of shell scripting and automation tools.
Data Engineering:
Experience in designing and implementing ETL/ELT workflows and data pipelines.
Familiarity with data modeling, schema design, and optimization techniques.
Big Data & Analytics:
Experience with distributed computing and big data tools (Apache Hadoop, Spark, or similar frameworks).
Proficient in querying and managing large datasets in BigQuery.
Collaboration Tools:
Experience with tools like Git, JIRA, or similar for version control and project management.
Soft Skills:
Strong problem-solving and analytical skills.
Excellent communication skills to interact with technical and non-technical teams.
Preferred Qualifications:
Experience in machine learning or data science applications.
Certification in Google Cloud Professional Data Engineer or equivalent.
Knowledge of data visualization tools like Google Data Studio, Tableau, or Power BI.","Gcp Data Engineer, Git, JIRA, ELT"
Data Engineer,FCS Software Solutions,6-11 Years,,Delhi,Information Technology,"Description:
Seeking someone who has experience/knowledge in manufacturing. SQL, Power BI and DAX calculations are a must for skillsets for this position. Candidates do not require to have a degree, as long as they have the required skillset/experiences from the job description. We are looking for a Data Engineer with MFG experience for a staff augmentation role.
These are the requirements for the role:
Expert in Azure Data Factory
Proven experience in Data Modelling for Manufacturing data sources.
Proficient SQL design
+5 years of experience in Data engineering roles
Prove experience in PBI: Dashboarding, DAX calculations, Star scheme development and semantic model building
Manufacturing knowledge
Experience with GE PPA as data source is desirable
API dev Knowledge
Python skills","DAX Calculations, Azure Data Factory, Power Bi, Sql, Python"
Data Engineer - MDM/PIM/Syndigo/Attacama/Informatica,Reflections Info Systems,6-11 Years,,"Chennai, Pune",IT Management,"Responsibilities include:
Work as part of a team to develop Data and Analytics solutions.
Participate in the development of cloud data warehouses, data as a service, business intelligence solutions
Ability to provide solutions that are forward-thinking in data integration.
Deliver a quality product.
Developing Modern Data Warehouse solutions using Azure or AWS Stack
Certifications :
Bachelor s degree in computer science & engineering or equivalent demonstrable experience
Desirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.
Primary Skills :
6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions
Programming experience in Scala or Python, SQL
Min 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks
Working knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse
Demonstrated analytical and problem-solving skills
Excellent written and verbal skills (English)
Secondary Skills :
Familiar with Agile Practices
Familiar with Version control platforms GIT, CodeCommit etc.
Problem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.
Ownership
Proactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
Sr. Data Engineer,Anblicks Solutions,8-13 Years,,Ahmedabad,Cloud Data Services,"Designing Data Models: Utilize Kimball data modeling techniques and Data Vault 2.0 methodologies to create and maintain robust data models.
Data Integration: Use Fivetran for seamless data extraction and loading, and Azure Data Factory (ADF) for orchestrating data workflows.
Data Warehousing: Manage and optimize data storage in Snowflake, ensuring efficient data retrieval and processing.
Data Transformation: Develop and maintain transformation scripts using DBT (Data Build Tool) to convert raw data into actionable insights.
Maintaining Data Documentation: Ensure all data processes and models are well-documented and easily understandable.
Communicating Results: Present data insights to stakeholders through visual representations and reports.
Collaborating: Work closely with data analysts, data engineers, and business executives to align data strategies with business goals.
Skills Required
Technical Skills: Proficiency in SQL, Python, and data visualization tools like Tableau or Power BI.
Analytical Skills: Ability to interpret complex data and provide actionable insights.
Communication Skills: Strong ability to explain technical concepts to non-technical stakeholders.
Problem-Solving: Aptitude for identifying issues within data and developing solutions.","Problem Solving, Stakeholder Management, Data Visualisation, Sql, Python, Scripting"
Azure Data Engineer,SRS Infoway,6-11 Years,,"Ahmedabad, Bengaluru, Chennai",Information Technology,"Looking for a Data Engineer with 6+ years of experience in Azure, including ADF, Azure Databricks, Python, and PySpark. Strong expertise in designing, developing, and implementing data pipelines in a cloud-based environment.","Azure Data Factory, Pyspark, Azure Databricks, Python, Sql"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Bengaluru, Chennai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.
User will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training
Demonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.
Possess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.
Optimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.
Develop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.
Monitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.
Stay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.
Collaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.
Ensure data privacy and security compliance in accordance with industry standards and regulations.
Qualifications we seek in you:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Strong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.
Strong understanding of data warehousing concepts, ETL processes, and data modeling.
Strong understanding of S3 and code-based scripting to move large volumes of data across application storage layers
Familiarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.
Excellent problem-solving, analytical, and critical thinking skills.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Preferred Qualifications/ skills
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.
Experience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.
Familiarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.
A continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Cloud Computing, Artificial Intelligence, Pytorch, Big Data Technologies, Aws, Tensorflow"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Key Responsibilities:
- Design, develop, and maintain data pipelines using Python, SQL, and Kedro
- Implement serverless solutions using AWS Lambda and Step Functions
- Develop and manage data workflows in Azure and AWS cloud environments
- Create integrations between data systems and Power Platform (Power Apps, Power Automate)
- Design, develop, and maintain APIs for data exchange and integration
- Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
- Optimize data storage and retrieval processes for improved performance
- Collaborate with cross-functional teams to understand data requirements and provide solutions
- API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Database Design, data engineering, Cloud Services, Data Extraction, Azure, Sql, Python"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Delhi, Hyderabad",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.
User will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training
Demonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.
Possess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.
Optimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.
Develop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.
Monitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.
Stay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.
Collaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.
Ensure data privacy and security compliance in accordance with industry standards and regulations.
Qualifications we seek in you:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Strong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.
Strong understanding of data warehousing concepts, ETL processes, and data modeling.
Strong understanding of S3 and code-based scripting to move large volumes of data across application storage layers
Familiarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.
Excellent problem-solving, analytical, and critical thinking skills.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Preferred Qualifications/ skills
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.
Experience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.
Familiarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.
A continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Cloud Computing, Artificial Intelligence, Pytorch, Big Data Technologies, Aws, Tensorflow"
Azure Data Engineer,Future Focus Infotech,5-10 Years,,Hyderabad,Information Technology,"Greetings from Future Focus Infotech!!!
We have multiple opportunities Azure Data Engineer (F2F interview on 17th May (Saturday)
Exp: 5+yrs
Location : Hyderabad
Job Type- This is a Permanent position with Future Focus Infotech Pvt Ltd & you will be deputed with our client.
A small glimpse about Future Focus Infotech Pvt Ltd. (Company URL: www.focusinfotech.com)
If you are interested in above opportunity, send updated CV and below information to [HIDDEN TEXT]","Azure Data Factory, Databricks, Azure"
Data Engineer-Microsoft Azure Databricks,Dynpro,12-14 Years,,"Hyderabad, Bengaluru, Chennai",Information Technology,"Job Description
As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data projects and collaborating with cross-functional teams to drive data initiatives.
Roles & Responsibilities:
Serve as a Subject Matter Expert (SME) and manage the team to deliver results.
Take responsibility for team-level decisions and performance.
Collaborate with multiple teams and contribute to strategic technical decisions.
Provide scalable and effective solutions for both team-specific and cross-team challenges.
Lead the design and development of data solutions and infrastructure.
Implement and optimize data pipelines for efficient and scalable data processing.
Ensure data quality, consistency, and integrity throughout the data lifecycle.
Partner with stakeholders to gather requirements and deliver data-driven solutions.
Professional & Technical Skills:
Must-Have Skills:
Proficiency inMicrosoft Azure Databricks
Strong understanding ofcloud-based data engineeringconcepts
Experience withbig data technologiessuch asHadoopandSpark
Solid knowledge ofdata modelinganddatabase designprinciples
Hands-on experience withSQLandNoSQLdatabases.
Experience Required:
Minimum of5 yearsof experience working withMicrosoft Azure Data bricks
Educational Qualifications:
15 years of full-time education
Bachelors or Masters degree inComputer Science, Engineering, Data Science, or a related field
Additional Information:
This role isbased in our Mumbai office
Candidates should be comfortable working in acollaborative, fast-paced environment","Sme, Nosql, Azure, Sql"
Sr. Data Engineer,MITS Solution Private Limited,6-10 Years,INR 20 - 22.5 LPA,"Noida, Bengaluru, Pune",Information Technology,"Job description
Job Position: Sr Data Engineer
Experience: 6+ years
Immediate joiners
Mode: Hybrid (3 days a week),
Location: Bangalore, Pune, New Mumbai, Noida, Chennai, Hyderabad, Chennai, Bhopal
Job description:
Data engineering development & testing with experience in Spark, Scala, and SQL.
Experienced big data developer with hands on Spark & Scala development skills.Ideal candidate would be a big data engineer passionate about data, data modelling, data enrichment for deriving deep insights accessible via Visualization dashboards or third party tools.
Must have skills
Must be a Big Data engineer and passionate about data.
Hands on Hive, Scala, Druid experience plus optionally Python
Hands on Experience in big data lake house like Databricks
Working knowledge and hands-on experience in known Big Data platforms and Hadoop tools & technologies
Experience with extraction, enrichment, and export large volumes data.
Experience in map-reduce and well-known big data algorithms.
Database development experience with a solid understanding of core database concepts, relational database design ODS and DWH
Peer reviews, Unit testing, deployment to production working with several teams to deliver big data solutions.
Experience in Finance big data projects will be a plus
Role:DBA / Data warehousing - Other
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:DBA / Data warehousing
Education
UG:B.Tech/B.E. in Computers","Hive, Hadoop, Scala, Big Data"
Data Engineer,IDESLABS,5-7 Years,,Pune,"Recruiting, Staffing Agency","Job description
5+ years of relevant experience in the field of Data Engineering
Advance skills in big data technologies like Hadoop, Python, Spark, SQL.
Must have experience building data APIs.
Bachelor s in Computer Science or related disciplines
Knowledge of Data Structures and Algorithm.
Strong Python programming skills with ability to implement OOPs and functional programming. Knowledge of Scala/Java would be plus.
Strong knowledge on RDBMS and NoSQL databases with the ability to implement them from scratch. Knowledge of Graph databases will be a plus.
Strong expertise in building & optimizing data pipelines, architectures, and data sets.
Experience working with different file formats like Parquet, ORC, Avro, RC, etc.
Experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc.
Knowledge and experience of using orchestration frameworks like Airflow, Oozie, Luigi, etc.
Experience using Spark, and building jobs using Python/Scala/Java.
Experience or Knowledge building stream processing platforms using Spark Streaming, Storm, etc. Knowledge of Kafka/Flink+Beam would be plus.
Knowledge of building REST API end points for data consumption.
Experience in building scalable data pipelines for both real time and batch using best practices in data modeling, ETL/ELT processes utilizing varioud technologies such as Spark, Kafka, Presto, SAP HANA, Airflow, informatica.
Perform Data analysis using Python, complex SQLs, and other tools.
Perform root cause analysis of issues from platform standpoint on Kubernetes, Containers, Hadoop, Spark, Hive, Presto
Excellent oral and written communication is a must.
Preferred
Masters in Computer Science or related disciplines
Experience building self-service tools for analytics would be plus.
Knowledge of ELK stack would be a plus.
Knowledge of implementing CI/CD on the pipelines is a plus.
Knowledge of Containerization (Docker/Kubernetes) will be plus.
Experience working with one of the popular Public Cloud based platforms is preferred","Airflow, Java, Python, Docker, Presto, Sap Hana, Spark, Kafka"
Azure Data Engineer,SRS Infoway,6-11 Years,,"Delhi NCR, Pune",Information Technology,"Looking for a Data Engineer with 6+ years of experience in Azure, including ADF, Azure Databricks, Python, and PySpark. Strong expertise in designing, developing, and implementing data pipelines in a cloud-based environment.","Azure Data Factory, Pyspark, Azure Databricks, Python, Sql"
Azure Data Engineer,SRS Infoway,6-11 Years,,"Ahmedabad, Bengaluru, Chennai",Information Technology,"Looking for a Data Engineer with 6+ years of experience in Azure, including ADF, Azure Databricks, Python, and PySpark. Strong expertise in designing, developing, and implementing data pipelines in a cloud-based environment.","Azure Data Factory, Pyspark, Azure Databricks, Python, Sql"
Lead Data Engineer,NXP Semiconductors,7-12 Years,,Bengaluru,Semiconductor,"Proven experience as a Data Engineer
Hands on experience in ETL design and development concepts (7+ years)
Experience with AWS and Azure cloud platforms and their data service offerings
Proficiency in SQL, PySpark, Python
Experience with GitHub, GitLab, CI/CD
Knowledge of advanced analytic concepts including AI/ML
Strong problem-solving skills and ability to work in a fast-paced and collaborative environment
Excellent oral and written communication skills
Preferred Skills Qualifications:
Experience with Agile / DevOps
Proficiency in SQL (Databricks, Teradata)
Experience with DBT
Experience with Dataiku platform, including administration","data engineering, Azure, Sql, Python, Etl, AWS"
Lead Data Engineer-Databricks,Anblicks Solutions,10-14 Years,,"Ahmedabad, Hyderabad",Cloud Data Services,"Role & responsibilities
Lead Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise inAzure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake)along with big data processing, real-time analytics, and cloud data integration and Team Leading Experience.
Key Responsibilities:
1. Data Pipeline Development & ETL/ELT
Design and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.
Implement ETL/ELT workflows for structured and unstructured data processing.
Optimize data ingestion, transformation, and storage strategies.
2. Cloud Data Architecture & Integration
Develop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).
Work with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.
3. Database Management & Optimization
Design and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).
Optimize SQL queries and indexing strategies for performance.
Implement data partitioning, compression, and caching for efficiency.
4. Data Governance, Security & Compliance
Ensure data quality, lineage, and governance with tools like Purview.
Implement role-based access control (RBAC), encryption, and security policies.
Ensure compliance with GDPR, HIPAA, and ISO 27001 regulations.
5. Monitoring & Performance Tuning
Use Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.
Automate data pipeline testing and validation.
6. Collaboration & Documentation
Document data models, pipeline architectures, and data workflows.
Immediate joiners are preferred.","data models, log analytics, Data Processing, Azure, Database Management, elt"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Bengaluru,Software,"Position Summary:
We are looking for candidates with hands on experience in Big Data or Cloud Technologies.
Must have technical Skills
7 to 10 Years of experience
Expertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have
Good and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of PySpark (SparkSQL) Must Have
Good knowledge of Shell script & Python Good to Have
Good knowledge of SQL Good to Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Good Communication Skills Must Have
*Data Ingestion, Processing and Orchestration knowledge
Roles & Responsibilities
Lead technical implementation of Data Warehouse modernization projects for Impetus
Design and development of applications on Cloud technologies
Lead technical discussions with internal & external stakeholders
Resolve technical issues for team
Ensure that team completes all tasks & activities as planned
Code Development
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Aviation, Information Technology, Computers
PG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Pune,Software,"Position Summary:
We are looking for candidates with hands on experience in Big Data or Cloud Technologies.
Must have technical Skills
7 to 10 Years of experience
Expertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have
Good and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of PySpark (SparkSQL) Must Have
Good knowledge of Shell script & Python Good to Have
Good knowledge of SQL Good to Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Good Communication Skills Must Have
*Data Ingestion, Processing and Orchestration knowledge
Roles & Responsibilities
Lead technical implementation of Data Warehouse modernization projects for Impetus
Design and development of applications on Cloud technologies
Lead technical discussions with internal & external stakeholders
Resolve technical issues for team
Ensure that team completes all tasks & activities as planned
Code Development
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Aviation, Information Technology, Computers
PG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer - Azure Fabric,Kanini Software Solutions,8-10 Years,,"Bengaluru, Chennai, Pune",Software,"We are looking for a Lead Data Engineer with expertise inAzure Fabric, Data Architecture, and ETL Pipelines. The ideal candidate will design and implement scalable data solutions, ensuring efficient data processing, governance, and analytics on the Azure cloud platform.
Key Responsibilities:
Design & develop data pipelines using Azure Fabric, Data Factory, and Synapse Analytics.
Implement ETL/ELT workflows for structured & unstructured data processing.
Optimize SQL queries, data modeling, and performance tuning.
Ensure data security, governance, and compliance using best practices.
Collaborate with cross-functional teams for data-driven insights and analytics.
Required Skills:
Azure Fabric, Azure Synapse, Azure Data Factory
ETL, SQL, Data Pipeline Design, Data Architecture
Big Data & Analytics, Data Governance, Performance Optimization
CI/CD, Azure DevOps, Git
Role:Data Science & Analytics - Other
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Analytics - Other
Education
UG:B.Tech/B.E. in Any Specialization
PG:Any Postgraduate","Architecture, Fabric, data engineering, Data Pipeline, Azure, Sql, Etl"
Lead Data Engineer,Coditas Technologies,7-11 Years,,Pune,Software,"We are looking for a Tech Lead expertise in Advanced Big Data Technology Stack. The person should be a hands-on technical expert in building and deploying applications using Big Data technologies. The person should have progressive experience in building highly scalable distributed systems. The person should have the ability to build a high-performance strong technical team that adheres to the strong quality standard of application development.
Roles and responsibilities
With over 7 yrs. of hands-on experience with Data Technologies
Requirement analysis and assess the technical feasibility of proposed solutions.
Act as the technical specialist in designing and recommending architecture for application development/feature development
Working on designing the application architecture and estimating effort in delivering features for new requirements
Implement data ingestion and transform pipeline for analytics and Dashboard reporting for business
Work in designing large-scale distributed computing applications using tools like Spark, Kafka, Hive, etc.
Performing validations, reconciliation, and consolidations for the imported data, Data migration, and data generation.
Effectively communicate with the Engineering Managers and Stakeholders to set the right expectations.
Technical Skills
Minimum 5 years of progressive experience building solutions in Big Data environments
Have a strong ability to build robust and resilient data pipelines that are scalable, fault-tolerant, and reliable in terms of data movement
Hands-on experience of Apache Spark with Python for batch and stream processing
Should know experience in batch and stream data processing
Exposure to working on projects across multiple domains
Hands-on experience in Apache Kafka
Strong hands-on capabilities in SQL and NoSQL technologies
Hands-on experience with AWS services like S3, DMS, Redshift, Glue, Lambda, Kinesis, MSK, etc. is must have or similar services of either Azure/GCP
Strong analytical/quantitative skills and comfortable working with very large sets of data.
Excellent written and verbal communication skills
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate","Airflow, Data Ingestion, Data Pipeline, Data Modeling, Pyspark, Data Architecture, Data Warehousing, Data Governance, Python, Sql, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Gurugram,Software,"Position Summary:
We are looking for candidates with hands on experience in Big Data or Cloud Technologies.
Must have technical Skills
7 to 10 Years of experience
Expertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have
Good and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of PySpark (SparkSQL) Must Have
Good knowledge of Shell script & Python Good to Have
Good knowledge of SQL Good to Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Good Communication Skills Must Have
*Data Ingestion, Processing and Orchestration knowledge
Roles & Responsibilities
Lead technical implementation of Data Warehouse modernization projects for Impetus
Design and development of applications on Cloud technologies
Lead technical discussions with internal & external stakeholders
Resolve technical issues for team
Ensure that team completes all tasks & activities as planned
Code Development
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Aviation, Information Technology, Computers
PG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Data Engineer - Azure,MITS Solution Private Limited,5-9 Years,INR 15 - 18 LPA,"Chennai, Bengaluru, Pune",Information Technology,"Job description
JD:
If you are an extraordinary developer and who loves to push the boundaries to solve complex business problems using creative solutions, then we wish to talk with you. As a Data Engineer -Azure, you will work in the Technology team that helps deliver our Data Engineering offerings at large scale to our Fortune clients worldwide. The role is responsible for innovating, building and maintaining technology services.
RESPONSIBILITIES:
Be an integral part of large scale client business development and delivery engagements
Develop the software and systems needed for end-to-end execution on large projects
Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions
Build the knowledge base required to deliver increasingly complex technology projects
Team handling, problem solving, project management and communication skills & creative thinking.
Work Mode:Hybrid work mode.
skills-CICD, python, Pyspark, Data Engineer Azure
QUALIFICATIONS:
A bachelors degree in Computer Science or related field with 6-10 years of technology experience
Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space
Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks
Database programming using any flavours of SQL
Expertise in relational and dimensional modelling, including big data technologies
Exposure across all the SDLC process, including testing and deployment
Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.
Good knowledge of Python and Spark are required
Good understanding of how to enable analytics using cloud technology and ML Ops
Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus
Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)
Characteristics of a forward thinker and self-starter
Ability to work with a global team of consulting professionals across multiple projects
Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems
Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to succeed.
EDUCATION:
B.E/B.Tech/M.Tech in Computer Science or related technical degree OR Equivalent.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Computers
PG:M.Tech in Computers","Pyspark, Data Engineer"
Senior Engineer (Data Engineer),Velotio Technologies,4-6 Years,,Pune,Software,"Job description
About Velotio:
Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.
Design and build scalable data infrastructure with efficiency, reliability, and consistency to meet rapidly growing data needs
Build the applications required for optimal extraction, cleaning, transformation, and loading data from dis","B2B SaaS, IoT Machine Learning space, data engineering"
Data Engineer Sr Associate,TechnoGen,5-8 Years,,Hyderabad,"Consulting, Information Services","Design, build and maintain complex ELT/ETL jobs that deliver business value.
Extract, transform and load data from various sources including databases, APIs, and flat files using IICS or Python/SQL.
Translate high-level business requirements into technical specs
Conduct unit testing, integration testing, and system testing of data integration solutions to ensure accuracy and quality
Ingest data from disparate sources into the data lake and data warehouse
Cleanse and enrich data and apply adequate data quality controls
Provide technical expertise and guidance to team members on Informatica IICS/IDMC and data engineering best practices to guide the future development of company Data Platform
Develop re-usable tools to help streamline the delivery of new projects
Collaborate closely with other developers and provide mentorship
Evaluate and recommend tools, technologies, processes and reference
Architectures
Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements
Participate in code reviews and ensure all solutions are lined to architectural and requirement specifications and provide feedback on code quality, design, and performance","ELT, Etl, data engineering, Data Integration, Api, Python, Sql, data quality control"
Data Engineer Sr Associate,TechnoGen,5-8 Years,,Hyderabad,"Consulting, Information Services","Python Proficiency: Strong understanding of Python, with practical coding experience
AWS:Comprehensive knowledge of AWS services and their applications
Airflow: creating and managing Airflow DAG scheduling.
Unix & SQL: Solid command of Unix commands, shell scripting, and writing efficient SQL scripts
Analytical & Troubleshooting Skills: Exceptional ability to analyze data and resolve complex issues.
Development Tasks: Proven capability to execute a variety of development activities with efficiency
Insurance Domain Knowledge:Familiarity with the Insurance sector is highly advantageous.
Production Data Management: Significant experience in managing and processing production data
Work Schedule Flexibility:Open to working in any shift, including 24/7 support, as requireRole & responsibilities","Airflow, Core Python, Sql, Aws, data engineering, Unix"
Data Engineer - OPS,Tredence Analytics Solutions Private Limited,9-14 Years,,Bengaluru,Software,"Job Description: Manager/Tech Lead (Data Engineering)
Location:Bangalore
Experience:9 to 14 years
Role Overview:
We are looking for a highly skilled Manager/Tech Lead, to lead and manage a team of 15 Data Engineers. This position requires a strong technical background, exceptional team management skills, and expertise in data engineering tools and techniques. The role involves end-to-end ownership of data pipelines, integration, and delivery, with a focus on cost and project optimization.
Key Responsibilities:
Team Management:
Manage and mentor a team of 15 Data Engineers, ensuring professional growth and skill enhancement.
Allocate tasks effectively, monitor performance, and address any issues proactively.
Technical Leadership:
Drive the development and optimization of data pipelines using Databricks, Data Lakes, Azure Data Factory, Delta Live Tables, Python, SQL, and PySpark.
Implement robust data integration, data quality, and data observability frameworks.
Project & Delivery Management:
Collaborate daily with clients, production teams, and delivery teams to align on goals and resolve challenges.
Ensure timely and efficient project delivery, maintaining high standards of quality and performance.
Cost & Process Optimization:
Identify opportunities for cost optimization in existing projects.
Streamline workflows to improve efficiency and reduce redundancies.
Stakeholder Management:
Act as a liaison between technical teams and stakeholders, ensuring clear communication of requirements and deliverables.
Manage expectations and provide updates on project progress and risks.
Tech Migration & Innovation:
Lead technology migration initiatives to ensure the adoption of modern tools and platforms.
Stay updated with industry trends and recommend best practices for data engineering.
Skills & Qualifications:
Mandatory Technical Skills:
Databricks, Data Lakes, Azure Data Factory, Delta Live Tables, Python, SQL, PySpark.
Expertise in building and managing scalable data pipelines and integrations.
Strong understanding of data quality and observability frameworks.
Management Skills:
Proven experience in leading and managing large teams (15+ members).
Strong project management and delivery skills.
Excellent stakeholder management and communication abilities.
Other Requirements:
Experience in driving delivery for tech teams and managing daily client interactions.
Knowledge of cost optimization and tech migration best practices.","Pyspark, Data Engineer, Sql, Python"
Senior Data Engineer,Egon Zehnder,8-13 Years,,Gurugram,Information Technology,"Job Summary
As a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.
Executing end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.
The Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.
The Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.
The Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.
Identifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.
Collaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.
Experience & Key Competencies
Engineering Degree or equivalent.
3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.
Experience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.
Proven experience in building ETL/ELT pipelines, preferably from SQL to Azure services
Architecture experience of making ETL operations such as Medallion Architecture etc.
Familiarity with CI/CD practices for data pipelines and version control using Git
Experience in integrating new/replacing vendor products in existing ecosystem.
Experience in migrating data structured, unstructured data to Data lake
Experience or understanding of performance engineering both at system level and database level.
Experience or understanding of Data Governance, Data Quality, Data Issue Management.
Work closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications
Ensure compliance with all regulations, policies, and procedures.
Escalate issues/risks pro-actively to appropriate stakeholders.
Regularly communicate status and challenges to team members and management.
Self-driven with keenness to master, suggest and work with different technologies & toolsets.
Excellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers
Excellent and resourceful problem-solving skills, adaptable and willingness to learn.
Good analysis skills - to be able to join the dots across multiple applications and interfaces between them.
Skill Set
Experience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.
Experience in Data factory and Data Lake technologies.
Rich Experience in data modelling techniques and creating various data models.
Experience in Azure cloud services and architecture patterns.
Understanding of RESTful APIs for data distribution.
Understanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments
Excellent oral and written communication with an ability to articulate complex systems to multiple teams.
Self-motivation and the ability to work under minimal supervision
Benefits Highlights:
5 Days working in a Fast-paced work environment
Work directly with the senior management team
Reward and Recognition
Employee friendly policies
Personal development and training
Health Benefits, Accident Insurance
Potential Growth for you
We will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.","Data Management, Data Quality, Version Control, postgresql, mysql, Sql"
Data Engineer Sr Associate,TechnoGen,6-10 Years,,Hyderabad,"Consulting, Information Services","Role & responsibilities
Bachelors degree in computer science, engineering, or a related field. Master's degree preferred.
Data: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.
SQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.
ELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC
Migration Experience: Experience Informatica on prem to IICS/IDMC migration
Cloud: 5+ years experience working in AWS cloud environment
Python: 5+ years of hands-on experience of development with Python
Workflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)
Advanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka
Troubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues
Communication: Excellent communication, problem-solving and organizational and analytical skills
Able to work independently and to provide leadership to small teams of developers.
Reporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)
Experience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms.","Airflow, data engineering, Aws Cloud, Tableau, Data Analytics, Apache, Sql, Python"
Data Engineer,Qentelli Solutions Private Limited,4-9 Years,,Hyderabad,"Information Technology, Information Services","Role & responsibilities
Must Haves Skills
Extensive experience as Data Engineer with Python Language and Cloud Technologies (AWS preferably).
Experience in Automating ETL process/Pipelines and AWS Data & Infrastructure with Python.
Extensive experience with AWS components like S3, Athena, EMR, Glue, Redshift, Kinesis and SageMaker.
Extensive Experience with SQL/Unix/Linux scripting.
Developing/testing Experience on Cloud/On Prem ETL Technologies (Ab Initio, AWS Glue, Informatica, Alteryx).
Experience in Data migration from Onprem to Cloud is Plus.
Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data Science.
Extensive experience in DevOps/Data Ops space.
Having experience in Data Science platforms like SageMaker/Machine Learning Studio/ H2O is plus.
Work Description SDET Python, AWS, Unix and ETL.
Work with business stakeholders, Business Systems Analysts and Developers to ensure delivery of Data Applications.
Building Automation Frameworks using Python.
Designing and managing the data workflows using Python during development and deployment of data products
Design, development of Reports and dashboards.
Analyzing and evaluating data sources, data volume, and business rules.
Should be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.
Interpret and analyses data from various source systems to support data integration and data reporting needs.
Experience in testing Database Application to validate source to destination data movement and transformation.
Work with team leads to prioritize business and information needs.
Develop and summarize Data Quality analysis and dashboards.
Knowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.
Execute testing of data analytic and data integration on time and within budget.
Troubleshoot & determine best resolution for data issues and anomalies
Has deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platforms","Numpy, Amazon Web Services, Pandas, Python, AWS"
Senior Azure Data Engineer,Mindera,7-12 Years,,Bengaluru,Software,"Job description
Experience in distributed computing (spark) and software development.
Experience in spark-scala.
Experience in Data Engineering.
Experience in Python
Experience of working on Cloud environments (Azure).","Azure, Python, Sql"
Google Cloud Platform Data Engineer,Ford,8-13 Years,,Chennai,Automotive,"We are seeking a skilled and experienced GCP Data Engineer to be part of Material Logistics IT team.
This individual will design, develop, and maintain our data pipelines and data warehousing solutions on Google Cloud Platform for Ford's ambitious command center and logistics service layer Initiatives.
You will be responsible for ensuring the efficient and reliable processing and storage of large datasets and architecting, building, and maintaining our cloud infrastructure and ensuring its high availability and scalability.
Qualifications:
Bachelor s degree in computer science or a related field.
8+ years of experience in software development, with at least 5 years on Google cloud technologies.
Expertise in Data ingestion and storage, Data Transformation and processing, Data analysis and visualization, Performance optimization and cost management, and a deep understanding, designing, data modeling, data integration, data warehousing and managing large-scale data storage system with below services: Big Query, Cloud Run, Cloud Function, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage and Data Studio, Dataflow, Dataproc, Cloud Build etc.,
Strong experience with infrastructure-as-code (IaC) tools like Terraform or Cloud Deployment Manager and Expertise in Tekton pipeline.
Deep understanding of GCP services, including Compute Engine, Kubernetes Engine (GKE), Cloud Storage, Cloud SQL, and Cloud Networking.
Extensive experience with DevOps practices and tools.
Expertise in Data encryption, Identity and Access Management and Cloud Security.
Key Responsibilities:
Work effectively with fellow software engineers, product owners and other technical experts to deliver curated data products.
Demonstrate technical knowledge and communication skills with the ability to advocate for well-designed solution.
Design, develop, and deploy data pipelines using Dataflow, Dataproc, and other relevant GCP services.
Build and maintain data warehouses using BigQuery.
Implement data quality and validation processes.
Develop and maintain data schemas and models.
Collaborate with software engineers and other stakeholders to meet business requirements.
Manage and monitor GCP infrastructure using tools like Cloud Monitoring and Cloud Logging.
Implement security best practices for cloud-based applications.
Mentor and guide junior developers.","Data Analysis, Logistics, data engineering, Cloud security, Gcp, Data Modeling, Sql"
Senior Data Engineer,Launch IT Consulting,8-13 Years,,Hyderabad,Advertising,"Be a part of our success story. Launch offers talented and motivated people the opportunity to do the best work of their lives in a dynamic and growing company. Through competitive salaries, outstanding benefits, internal advancement opportunities, and recognized community involvement, you will have the chance to create a career you can be proud of. Your new trajectory starts here at Launch.
What we are looking for: designing and building ETL pipelines, with a focus on Azure Data Factory and Microsoft Fabric services.
Mandatory Skills:
8+ years of hands-on experience designing and building ETL pipelines, with a focus on Azure Data Factory and Microsoft Fabric services.
Expertise in implementing CI/CD pipelines and automating deployment processes using Azure DevOps.
Hands-on experience migrating data from on-premises databases to Azure Cloud environments, including Managed Instances.
Proven track record of implementing version control systems and managing data pipeline architecture.
Strong SQL skills for developing and optimizing queries, stored procedures, and database performance.
Familiarity with Delta Lake, Synapse Analytics, or other MS Fabric-specific technologies.
Experience working in large, agile teams to deliver data solutions in an iterative, collaborative environment.
Preferred Skills:
Knowledge of Microsoft Fabric components such as Dataflows, Data Pipelines, and integration with Power BI for seamless analytics delivery.
Understanding of data security practices, including data encryption and role-based access control in Azure.
Experience with event-driven architectures using Azure Event Hubs or similar tools.
Familiarity with Data Ops principles to streamline pipeline monitoring and management.
Excellent problem-solving skills and ability to quickly adapt to evolving project requirements.","Azure, Sql, Etl, Power Bi, Ci"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,"Overview:
We are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.
Key Responsibilities:
Provide support and expertise in the domain of Data Engineer.
Collaborate with cross-functional teams to achieve business goals.
Ensure timely delivery of services and maintain high-quality standards.
Required Qualifications:
Proven experience in a relevant field or position.
Strong understanding of the responsibilities and tools associated with the role.
Excellent problem-solving and communication skills.
Preferred Qualifications:
Certifications or training relevant to Data Engineer.
Experience working in a fast-paced environment or large organizations.","data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,"Overview:
We are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.
Key Responsibilities:
Provide support and expertise in the domain of Data Engineer.
Collaborate with cross-functional teams to achieve business goals.
Ensure timely delivery of services and maintain high-quality standards.
Required Qualifications:
Proven experience in a relevant field or position.
Strong understanding of the responsibilities and tools associated with the role.
Excellent problem-solving and communication skills.
Preferred Qualifications:
Certifications or training relevant to Data Engineer.
Experience working in a fast-paced environment or large organizations.","data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Data Engineer,Veeva Systems,4-9 Years,,Mumbai,Cloud Data Services,"Veeva is looking for a hard-working, collaborative expert in data pipelines, ETL tools, and warehousing. You will be creating high-quality solutions to complex problems around data acquisition and integration into existing systems and data sets. If you enjoy working in a fast-paced environment with a variety of exciting challenges to solve, this is the role for you.
What You'll Do
Design and build reusable and configurable data pipelines using a mix of standard cloud-based and proprietary data platforms.
Work with a data scientist to develop AI/ML proofs of concept into full implementations.
Create documentation and provide informal training for data analysts on the configuration and use of tools and pipelines.
Work with other engineering and product teams to understand proprietary platforms and provide input and feedback.
Provide guidance to more junior data engineers on best practices.
Work with security teams to ensure that all servers, platforms, and other resources meet security requirements.
Requirements
BS degree in Computer Science, Engineering, or a related subject.
4+ years of experience in Data Engineering roles.
Experience developing sophisticated data pipelines in cloud-based environments (e.g., AWS) using scalable data processing tools (e.g., Apache Spark).
Data modeling experience.
Demonstrated ability to work with others, particularly providing guidance to other data engineers.
Ability to communicate around complex ideas and topics in English with both technical and non-technical individuals.
Nice to Have
Familiarity with Agile methodologies.
DevOps skills, especially CI/CD experience.
Configuring and maintaining cloud-based cluster computing resources and orchestration systems (e.g., EC2 instances, Kubernetes clusters, Elastic Beanstalk).
Perks & Benefits
Flexible work from anywhere policy.","Data Processing, San, Data Modeling, Cloud, Spark, Agile"
AWS Data Engineer Manager,Wipzo Systech Private Limited,10-15 Years,INR 30 - 45 LPA,"Gurugram, Hyderabad, Pune","Information Technology, Information Services","Role: AWS Data Engineering Manager
Experience: 10+ Years
Location: Pune, Gurgaon, Hyderabad, Bangalore (Hybrid)
Our Values: Passion, Continuous Learning, Adaptability, Teamwork, Customer Centricity, Reliability
Job Summary:
We are seeking a highly motivated and experienced AWS Engineer to join our MarTech team in the NFL. This position requires an individual with AWS cloud experience and ambition to continually keep up with best practices when it comes to cloud development. The successful candidate must be able to seek out requirements and create best-in-class cloud-native solutions. The engineer must always create solutions that are repeatable, scalable and well-governed. They will deploy and rigorously test solutions to ensure they are robust and secure. The engineer will create and maintain diagrams associated with solutions deployed into production.
Must have:
4-6 years of experience with AWS tech stack(S3, Glue, Redshift, Athena, Lambda,CloudWatch, SQS, IAM roles, CloudTrail).
3-5 years ofSQL, Python & Pyspark programming experience.
Experience working withETL Tools.
Experience with CDC mechanisms for database sources.
Experience building distributed architecture-based systems, especially handling large data volumes and real-time distribution.
Initiative and problem-solving skills when working independently.
Expertise in building high-performance, highly scalable, cloud-based applications.
Experience with SQL and No-SQL databases.
Good collaboration and communication skills, highly self-driven, and take ownership.
Experience in Writing well-documented, Clean, and Effective codes is a must.
Good to have:
AWS Cloud Certifications.
Knowledge and experience in designing and developing RESTful services.
1-3 years of experience inDBTwith Data Modeling, Airflow,MWAA,SQL,Jinja templating, and packages/macros to build robust, performant, and reliable data transformation and feature extraction pipelines.
1-2 years of experience in Airbyte building ingestion modules for streaming, batch.
Good experience building Real-Time streaming data pipelines with Kafka, Kinesis etc.
Familiarity with Big Data Design Patterns, modeling, and architecture.
Working knowledge of DevOps methodologies, including designing CI/CD pipelines.
Good understanding of Data warehousing & Data Lake solutions concepts.
Responsibilities:
Create and maintain scalable, robust AWS architecture.
Develop API-based, CDC, batch, and real-time data pipelines for structured and unstructured datasets.
Enable integration with third-party systems as needed.
Ensure solutions are repeatable and scalable across the organization.
Work with client teams to gather requirements, develop solutions, and deploy them.
Provide robust solution documentation for a wide audience.
Collaborate with data professionals to bring applications to life, meeting business needs.
Prioritize data protection and cloud security in all deliverables.
Education:
BE/B.Tech/MS/M.Tech/ME from reputed institute.
Every individual comes with a different set of skills and qualities so even if you dont tick all the boxes for the role today, we urge you to apply as there might be a suitable/unique role for you tomorrow!
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:Any Graduate
PG:Any Postgraduate
Doctorate:Any Doctorate","Data Bricks, Glue, Athena, CloudTrail, Pyspark, S3, Kafka, Cloudwatch, Redshift, Sql, Lambda, Kinesis, Docker, Sqs, Iam, Data Engineer, Python, Kubernetes, Etl, Aws"
Azure Data Engineer,Future Focus Infotech,4-8 Years,,Pune,Information Technology,"Overview:
We are seeking a highly motivated and detail-oriented individual to join our team as a Azure Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.
Key Responsibilities:
Provide support and expertise in the domain of Azure Data Engineer.
Collaborate with cross-functional teams to achieve business goals.
Ensure timely delivery of services and maintain high-quality standards.
Required Qualifications:
Proven experience in a relevant field or position.
Strong understanding of the responsibilities and tools associated with the role.
Excellent problem-solving and communication skills.
Preferred Qualifications:
Certifications or training relevant to Azure Data Engineer.
Experience working in a fast-paced environment or large organizations.","Data Pipelines, Azure Data Factory, Databricks, Azure, Sql, Etl"
Senior Data Engineer,Launch IT Consulting,8-13 Years,,Hyderabad,Advertising,"Developing and maintaining data models that define the structure, relationships, and constraints of data within enterprise data model
Creating and implementing data pipelines and ETL processes to move and transform data from various sources into data storage solutions like data warehouses or data lakes
Implementing measures to ensure the accuracy, integrity, and consistency of data throughout the ETL process
Ensuring ETL processes are efficient, scalable, and optimized for performance
Mandatory Skills:
8+ years for Data Modeling and ETL Development Experience
Experience with dimensional data modeling and star schema
Strong SQL skills experience
Experience with ETL Development using Python and Amazon Glue
Experience loading data into the Amazon Redshift and Amazon S3
Apache Iceberg
Preferred Skills:
Spark
Amazon EMR
Apache Kafka or Apache Flink","Sql, Python, Etl, S3, Emr"
Data Engineer II,Trimble,3-6 Years,,Hyderabad,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.
Key Responsibilities:
Design and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.
Develop, optimize, and maintain data pipelines using AWS services.
Implement robust ETL/ELT processes and event-driven data ingestion.
Establish and enforce data governance policies, ensuring data quality, security, and compliance.
Optimize cloud resources for performance, availability, and cost-efficiency.
Partner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.
Identify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.
Our current tech-stack:
AWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.
DevOps: Cloudformation, Terraform, Git, CodeBuild
Database: Redshift, PostgreSQL, DynamoDB, Athena
Language: Bash, Python, SQL
Qualifications:
Bachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.
Expertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.
Extensive experience indata and cloud engineeringroles.
Expertise inAWSplatforms, including data services.
Strong competence inETL processes,data warehousing, and big data technologies.
Advanced skills inscripting,Python,SQL, and infrastructure automation tools.
Familiarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).
Experience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
Data Engineer,Anblicks Solutions,8-13 Years,,Ahmedabad,Cloud Data Services,"Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise in Azure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake) along with big data processing, real-time analytics, and cloud data integration.
Key Responsibilities:
1. Data Pipeline Development ETL/ELT
Design and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.
Implement ETL/ELT workflows for structured and unstructured data processing.
Optimize data ingestion, transformation, and storage strategies.
2. Cloud Data Architecture Integration
Develop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).
Work with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.
3. Database Management Optimization
Design and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).
Optimize SQL queries and indexing strategies for performance.
Implement data partitioning, compression, and caching for efficiency.
4. Data Governance, Security Compliance
Ensure data quality, lineage, and governance with tools like Purview.
Implement role-based access control (RBAC), encryption, and security policies.
Ensure compliance with GDPR, HIPAA, and ISO 27001 regulations.
5. Monitoring Performance Tuning
Use Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.
Automate data pipeline testing and validation.
6. Collaboration Documentation
Document data models, pipeline architectures, and data workflows.
Technical Skills:
Cloud Data Services: Azure Data Factory, Azure Synapse, Databricks, SSIS, BigQuery.
ETL Data Pipelines: Apache Spark, Python, SQL.
Big Data Processing: Delta Lake, Parquet, Hadoop, Event Hubs.
Database Management: SQL Server, Cosmos DB.
Security Compliance: RBAC, Data Masking, Encryption, Purview.
Scripting Automation: Python, PowerShell, Terraform for IaC.","Cloud Architecture, Azure Data Factory, Data Governance, Performance Tuning"
Data Engineer II,Trimble,3-6 Years,,Chennai,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.
Key Responsibilities:
Design and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.
Develop, optimize, and maintain data pipelines using AWS services.
Implement robust ETL/ELT processes and event-driven data ingestion.
Establish and enforce data governance policies, ensuring data quality, security, and compliance.
Optimize cloud resources for performance, availability, and cost-efficiency.
Partner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.
Identify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.
Our current tech-stack:
AWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.
DevOps: Cloudformation, Terraform, Git, CodeBuild
Database: Redshift, PostgreSQL, DynamoDB, Athena
Language: Bash, Python, SQL
Qualifications:
Bachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.
Expertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.
Extensive experience indata and cloud engineeringroles.
Expertise inAWSplatforms, including data services.
Strong competence inETL processes,data warehousing, and big data technologies.
Advanced skills inscripting,Python,SQL, and infrastructure automation tools.
Familiarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).
Experience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
ML/Data Engineer,Ca One Tech,2-7 Years,,"Hyderabad, Bengaluru",Information Technology,"Preferred Qualifications
Vertex AI
Jupyter notebooks
ML model Development
Kubeflow
Data proc
Data Pipelines (Data flow, Pub/sub)
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development","model development, Proc"
Data Engineer IV,Varite India Private Limited,2-5 Years,,Gurugram,"Information Technology, Information Services","VARITE is a global staffing and IT consulting company providing technical consulting and team augmentation services to Fortune 500 companies in the USA, UK, Canada, and India.
Client Overview: Our client is a leading American technological research and consulting firm based in Stamford, Connecticut. They conduct cutting-edge research on technology and provide insights through private consulting, executive programs, and conferences. With clients including large corporations, government agencies, technology companies, and investment firms, the client serves over 12,000 organizations across more than 100 countries. The company has a workforce of 15,000 employees.
Role Overview: We are seeking a talented Data Engineer IV to join the People Analytics Data Engineering team for our client. This role will involve the development, automation, testing, and integration of critical HR data assets to prepare the business for the future of work. As a Data Engineer, you will be responsible for building and supporting data platforms, working with data warehousing, dimensional modeling, and ETL/ELT pipelines.
Location: Gurgaon, Haryana
Contract Duration: 3 months
Start Date: 1st January 2025
Experience Required: 2+ years
Key Responsibilities:
Develop and maintain data pipelines and automation processes using Azure Data Factory.
Implement ETL/ELT processes focused on data warehousing, dimensional modeling, and incremental loads.
Develop cloud-based data solutions within Azure infrastructure.
Write complex ANSI SQL queries, including SELECT, DML, DDL, and optimization techniques like indexing and partitioning.
Track project and development progress using industry-standard DevOps tools (e.g., Jira).
Ensure seamless integration and automation for HR data assets, preparing the business for future workforce analytics needs.
Qualifications:
Bachelor's or Master's degree in Computer Science, Computer Engineering, Engineering, Data Science, or a related technical field.
2+ years of experience in data engineering and data warehousing.
Proficient in ANSI SQL, including advanced techniques like partitioning, indexing, and query optimization.
Hands-on experience with Azure Data Factory for data pipelines, orchestration, and mapping.
Experience with ETL/ELT processes specific to dimensional modeling concepts and incremental data loads.
Familiarity with project tracking in DevOps tools (e.g., Jira).
Preferred Skills:
Experience with Data Bricks development.
Knowledge of the HR/Workforce/People domain (e.g., headcount, hires, terms).
Familiarity with Workday source system and its integrations.
Experience with Business Intelligence tools like Power BI or Tableau.
Experience with Visier integrations.
Click apply for this exciting opportunity with a leading research and consulting firm.","Azure Data Factory, Azure Cloud, Ansi Sql"
IT - Data Engineer _ AWS,Systechcorp Inc,2-6 Years,,"Kolkata, Mumbai",Software,"Work with practice SMEs to understand the client s challenges and how Genpact solves for their challenges
Understand our right to play and value articulation
Liaison with practice team to drive product/offering messaging by translating technical nuances to strong client messages
Productize the offering with the right value articulation to make it client ready
Ensure every offering has all the GTM readiness collaterals for client readiness
Establish strong relationship with practice leaders to be their trusted advisor for offerings
Ability to quickly research industry and competitor s offerings and ability to incorporate it in our value messaging
Work with Knowledge Management team to drive strategic placement of the offerings in our infrastructure","IT, data engineering, Cloud, Aws"
Expert Data Engineer,Ciklum,2-6 Years,,Chennai,Information Technology,"About the Role
As an Expert Data Engineer, you will be part of a cross-functional development team dedicated to building innovative, scalable data solutions that drive tomorrow's experiences. You will design and maintain large-scale data infrastructure, work closely with analysts and data scientists, and contribute to internal technical communities.
Responsibilities
Build, deploy, and maintain mission-critical analytics solutions for processing large-scale data.
Develop components for data ingestion, real-time streaming, batch processing, and ETL across various storage platforms.
Own parts of the engineering infrastructure; improve platform quality, performance, and maintainability.
Collaborate with other engineers to share knowledge and continuously learn new technologies.
Ensure solutions meet requirements for functionality, scalability, reliability, and availability.
Support end-to-end solutions, performing development, QA, and DevOps roles as needed.
Partner with business analysts and data scientists to understand and support use cases.
Participate in unit activities, conferences, and promote best practices.
Contribute to sales efforts, customer meetings, and digital services engagement.
Requirement
Technical Experience
5+ years of experience coding in SQL, Java, Python, or Scala, with solid computer science fundamentals.
3+ years of experience leading production deployments of large-scale backend data systems.
2+ years of hands-on experience with tools like Hadoop, MapReduce, Pig, Hive, Impala, Spark, Kafka, Storm, and databases like HBase, Cassandra.
3+ years working with cloud data platforms: AWS, Azure, or GCP.
Data Engineering Expertise
Strong knowledge of Data Governance, including data quality, security, and compliance standards.
Deep understanding of Data Warehousing design, optimization, and implementation.
Experience with SQL and MPP databases (e.g., Vertica, Netezza, Greenplum).
Familiarity with data quality testing, automation, and visualization tools.
Experience in BI report and dashboard design (e.g., Power BI, Tableau).
Engineering Practices
Experience with Agile methodologies (e.g., SCRUM).
Strong grasp of the software development lifecycle, including coding standards, version control, code reviews, CI/CD, and operations.
Proven ability to lead development teams: managing backlogs, peer review processes, and maintaining development standards.
Education
Bachelor's degree in Computer Science or Engineering from a top-tier university; Master's preferred.
Desirable Skills
Experience in data science or machine learning projects.
Proficiency in backend development and deployment.
Knowledge of CI/CD pipelines and related tools.
Understanding of enterprise-level data analytics.
Experience with platforms like Databricks, Snowflake.
Familiarity with Kubernetes and container orchestration.","GCP., Java, Azure, Sql, Python, AWS, Machine Learning, Data Science"
Data Engineer,Testingxperts Inc,0-3 Years,,Chandigarh,Information Technology,"Job Details
Data Engineer with 0-1 years of experience in Power BI, sql, Python.
It is 6 Months internship post to that fulltime, All 2years bond .
Certification in relevant field best to have
Job Type: Full-time
Fixed shift: Monday to Friday
Morning shift
Work Location: In person,","Powerbi, Power Bi, Python, Sql"
Associate Data Engineer- GET,XenonStack,0-1 Years,,Mohali,Information Technology,"Job Responsibilities -
Develop, construct, test and maintain Data Platform Architectures
Align Data Architecture with business requirements
Liaising with coworkers and clients to elucidate the requirements for each task.
Scalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analyzed quickly by BI AI Teams .
Reformulating existing frameworks to optimize their functioning.
Transforming Raw Data into InSights for manipulation by Data Scientists.
Ensuring that your work remains backed up and readily accessible to relevant coworkers.
Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.
Technical Requirements-
Experience of Python, Java/Scala
Great Statistical / SQL based Analytical Skills
Experience of Data Analytics Architectural Design Patterns for Batch , Event Driven and Real-Time Analytics Use Cases
Understanding of Data warehousing, ETL tools, machine learning, Data EPIs
Excellent in Algorithms and Data Systems
Understanding of Distributed System for Data Processing and Analytics
Familiarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores .
Professional Attributes-
Excellent communication skills
Attention to detail
Analytical mind and Problem Solving Aptitude
Strong Organizational skills","Consulting, Analytical, Architectural Design, Machine Learning, Big Data Analytics"
Senior Engineer (Data Engineer),Velotio Technologies,2-7 Years,,Bengaluru,Software,"Job description
About Velotio:
Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.
Design and build scalable data infrastructure with efficiency, reliability, and consistency to meet rapidly growing data needs
Build the applications required for optimal extraction, cleaning, transformation, and loading data from dis","B2B SaaS, IoT Machine Learning space, data engineering"
GCP Data Engineer,Kognivera It Solutions Private Limited,2-5 Years,,Bengaluru,Software,"About the Role: We are seeking a Lead GCP Data Engineer who can demonstrate a broad range of technical skills across Google Cloud Platform (GCP) and third-party technologies. As a pivotal member of our team, you will be responsible for designing robust data architectures, developing efficient data pipelines, and optimizing data processing workflows on GCP. Your expertise will be instrumental in driving our data engineering initiatives forward, ensuring scalability, reliability, and performance.
Key Responsibilities
Data Architecture Design: Design scalable and efficient data architectures on GCP that meet the organization's data processing and analysis requirements. Collaborate closely with data scientists, business analysts, and stakeholders to define effective data models and structures.
Data Pipeline Development: Develop and implement data pipelines using GCP services such as Google Cloud Storage, Big Query, Dataflow, and Pub/Sub. Ensure data quality, reliability, and governance throughout the data lifecycle.
Data Transformation and Integration: Utilize technologies like Apache Beam, Apache Spark, and Cloud Dataproc to transform and integrate data from diverse sources. Perform data cleansing, aggregation, enrichment, and normalization to support downstream applications and analytics.
Performance Optimization: Optimize data processing workflows to enhance performance and efficiency. Monitor pipelines, identify bottlenecks, and implement optimizations such as improved data partitioning, sharding, and leveraging GCP's autoscaling capabilities.
Continuous Improvement: Stay abreast of advancements in data engineering and cloud technologies. Explore and implement new GCP features and services to enhance data processing capabilities and drive innovation within the team.
Research and Innovation: Conduct research on emerging data engineering technologies, tools, and best practices. Evaluate new methodologies to improve data engineering processes and bring innovative solutions to the organization.
Task Automation: Automate data engineering tasks using scripting, workflows, or tools like Cloud Composer and Cloud Functions. Streamline data ingestion, transformation, monitoring, and other operational processes to improve efficiency and reduce manual effort.
Attributes & Competencies
Education: BE/BTech, MTech, or MCA.
Experience: Minimum 5+ years in development/migration projects, with at least 3 years focused on GCP. Experience in GCP-based Big Data deployments (batch/real-time) using Big Query, Big Table, Google Cloud Storage, Pub/Sub, Data Fusion, Dataflow, Dataproc, and Airflow.
Technical Skills
Google Certified Professional Cloud Architect with experience automating and orchestrating workloads on GCP or other public clouds.
Proficiency in at least one configuration management system (Chef, Puppet, Ansible, Salt, etc.).
Strong programming skills in Python, Go, with expertise in Git and Git workflows.
Demonstrated proficiency in CI/CD tools such as Jenkins, TeamCity, or Spinnaker.","Data Modeling, Sql, Python, Etl"
Data Engineer,Systechcorp Inc,2-5 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Senior Data Engineer
Skills:
Azure Cloud technologies
Mandatory
Synapse (DataFlow, Pipeline (including web call), Notebook (Pyspark))
SQL expertise
Key vault
Blob Storage
Preferable
Logic Apps
Function App (C#)
API Management
GIT","Synapse, Key Vault, Blob Storage, data engineering, Pyspark, Azure, Sql"
Data Engineer,Systechcorp Inc,2-5 Years,,"Delhi, Bengaluru, Chennai",Software,"Senior Data Engineer
Skills:
Azure Cloud technologies
Mandatory
Synapse (DataFlow, Pipeline (including web call), Notebook (Pyspark))
SQL expertise
Key vault
Blob Storage
Preferable
Logic Apps
Function App (C#)
API Management
GIT","Synapse, Key Vault, Blob Storage, data engineering, Pyspark, Azure, Sql"
Data Engineer,Velotio Technologies,2-7 Years,,Bengaluru,Software,"Job description
Design and build scalable data infrastructure with efficiency, reliability, and consistency to meet rapidly growing data needs
Build the applications required for optimal extraction, cleaning, transformation, and loading data from disparate data sources and formats using the latest big data technologies
Building ETL/ELT pipelines and work with other data infrastructure components, like Data Lakes, Data Warehouses and BI/reporting/analytics tools
Work with various cloud services like AWS, GCP, Azure to implement highly available, horizontally scalable data processing and storage systems and automate manual processes and workflows
Implement processes and systems to monitor data quality, to ensure data is always accurate, reliable, and available for the stakeholders and other business processes that depend on it
Work closely with different business units and engineering teams to develop a long-term data platform architecture strategy and thus foster data-driven decision-making practices across the organization
Help establish and maintain a high level of operational excellence in data engineering
Evaluate, integrate, and build tools to accelerate Data Engineering, Data Science, Business Intelligence, Reporting, and Analytics as needed
Focus on building test-driven development by writing unit/integration tests
Contribute to design documents and engineering wiki
You will enjoy this role if you...
Like building elegant well-architected software products with enterprise customers
Want to learn to leverage public cloud services & cutting-edge big data technologies, like Spark, Airflow, Hadoop, Snowflake, and Redshift
Work collaboratively as part of a close-knit team of geeks, architects, and leads
Desired Skills & Experience:
2+ years of data engineering or equivalent knowledge and ability
2+ years software engineering or equivalent knowledge and ability
Strong proficiency in at least one of the following programming languages: Python, Scala, or Java
Experience designing and maintaining at least one type of database (Object Store, Columnar, In-memory, Relational, Tabular, Key-Value Store, Triple-store, Tuple-store, Graph, and other related database types)
Good understanding of star/snowflake schema designs
Extensive experience working with big data technologies like Spark, Hadoop, Hive
Experience building ETL/ELT pipelines and working on other data infrastructure components like BI/reporting/analytics tools
Experience working with workflow orchestration tools like Apache Airflow, Oozie, Azkaban, NiFi, Airbyte, etc.
Experience building production-grade data backup/restore strategies and disaster recovery solutions
Hands-on experience with implementing batch and stream data processing applications using technologies like AWS DMS, Apache Flink, Apache Spark, AWS Kinesis, Kafka, etc.
Knowledge of best practices in developing and deploying applications that are highly available and scalable
Experience with or knowledge of Agile Software Development methodologies
Excellent problem-solving and troubleshooting skills
Process-oriented with excellent documentation skills","Business Intelligence, Java, Sql, Python, Aws"
Data Engineer 2,Bread Financial,2-5 Years,,Bengaluru,Financial Services,"The Data Engineer II works on different projects of data engineering to support the use cases, data ingestion pipeline and identify potential process or data quality issues. The team also supports marketing analytic teams with analytical tools that enable our analytics and business communities to do their job easier, faster and smarter. The team brings together data from different internal external partners and builds a curated Marketing analytics focused data tools ecosystem. The Data Engineer plays a crucial role in building this ecosystem depending on the Marketing analytics communities need.
Essential Job Functions
Collaboration - Collaborates with internal/external stakeholders to manage data logistics - including data specifications, transfers, structures, and rules. Collaborates with business users, business analysts and technical architects in transforming business requirements into analytical workbenches, tools and dashboards reflecting usability best practices and current design trends. Demonstrates analytical, interpersonal and professional communication skills. Learns quickly and works effectively individually and as part of a team.
Process Improvement - Accesses, extracts, and transforms Credit and Retail data from a variety of sources of all sizes (including client marketing databases, 2nd and 3rd party data) using Hadoop, Spark, SQL, Big data technologies etc. Provide automation help to analytical teams around data centric needs using orchestration tools, SQL and possibly other big data/cloud solutions for efficiency improvement.
Project Support - Supports Sr. Specialist and Specialist in new analytical proof of concepts and tool exploration projects. Effectively manages time and computing resources in order to deliver on time/correctly on concurrent projects. Involved in creating POCs to ingest and process streaming data using Spark and HDFS.
Data and Analytics - Answers and trouble shoots questions about data sets and analytical tools; Develops, maintains and enhances new and existing analytics tools/Frameworks to support internal customers/consumers. Ingests data from different sources, processes it according to the requirement document in order to store data to Hive or NoSQL database or different warehousing solutions. Manages data coming from different sources, involved in HDFS maintenance, and loading of structured and unstructured data. Applies knowledge in Agile Scrum methodology that leverages the Client BigData platform and used version control tool Git. Imports and exports data using Sqoop from HDFS to RDBMS and vice-versa. Demonstrates an understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management. Creates POCs to ingest and process streaming data using Spark and HDFS. Work on back-end using Scala, Python and Spark to perform several aggregation logics.
Technical Skills - Expert in writing complicated SQL Queries and database analysis for good performance. Experience working with python or Scala, Spark, Hadoop, Hive, Oozie, Sqoop, HDFS, Impala, Shell Scripts, Microsoft Azure Services like ADLS/Blob Storage solutions, Azure DataFactory, Azure Functions and Databricks. Utilize basic knowledge of Rest API for designing networked applications.
Reports to: Lead or above
Working Conditions/Physical Requirements: Normal office environment
Direct Reports: 0
Minimum Requirements:
Degree Required: Bachelor's Degree
Area of Study: Computer Science, Engineering
Years of Work Experience Required: Two to five years or more
Type / focus of work experience required: Data Analytics","Python, Data Analytics, Sql, Azure Cloud, Big Data, Rest Api"
"Sr. Associate Technical Consultant, Data Engineer",AHEAD,2-5 Years,,Gurugram,Information Technology,"Data Engineer
(Internally known as a Sr. Associate Technical Consultant)
AHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.
The Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.
Responsibilities
A Data Engineer should be able to build, operationalize and monitor data processing systems
Create robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Implement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases
Engineers and supports data structures including but not limited to SQL and NoSQL databases
Engineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)
Leverages the right tools for the right job to deliver testable, maintainable, and modern data solutions
Respond to customer/team inquiries and assist in troubleshooting and resolving challenges
Works with other scrum team members to estimate and deliver work inside of a sprint
Research data questions, identifies root causes, and interacts closely with business users and technical resources
Qualifications
3+ years of professional technical experience
3+ years of hands-on Data Warehousing
3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake
2+ years of programming languages such as Python
3+ years of experience working in cloud environments (Azure)
2 years of experience in Redshift
Strong client-facing communication and facilitation skills
Key Skills
Python, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP
Why AHEAD
Through our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.
We fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.
USA Employment Benefits include
Medical, Dental, and Vision Insurance
401(k)
Paid company holidays
Paid time off
Paid parental and caregiver leave
Plus more! See benefits https://www.aheadbenefits.com/ for additional details.
The compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Azure, python, Spark, Redshift, hadoop"
Lead AWS Data Engineer (Python & Scala),iLink Digital,8-13 Years,,Bengaluru,Financial Services,"Develop & Optimize Data Pipelines Architect, build, and enhance scalable data pipelines for high-performance processing.
Troubleshoot & Sustain Identify, diagnose, and resolve data pipeline issues to ensure operational efficiency.
Data Architecture & Storage Design efficient data storage and retrieval strategies using Postgres, Redshift, and other databases.
CI/CD Pipeline Management Implement and maintain continuous integration and deployment strategies for smooth workflow automation.
Scalability & Performance Tuning Ensure the robustness of data solutions while optimizing performance at scale.
Collaboration & Leadership Work closely with cross-functional teams to ensure seamless data flow and lead engineering best practices.
Security & Reliability Establish governance protocols and ensure data integrity across all pipelines.
Technical Skills Required:
Programming: Expert in Python and Scala
Big Data Technologies: Proficient in Spark, Kafka
DevOps & Cloud Infrastructure: Strong understanding of Kubernetes
SQL & Database Management: Skilled in SQL administration, Postgres, Redshift
CI/CD Implementation: Experience in automating deployment processes for efficient workflow","cd, python, Scala, Ci, Big Data Technologies, Database Manager, Sql"
Data Engineer,Genzeon Corporation,2-5 Years,,Pune,Information Technology,"Genzeon is looking for an experienced Full Stack Data Engineer to join our team in Pune. In this role, you will be instrumental in shaping our data architecture, developing full-stack data solutions, and driving innovation in data processing and analytics. This position offers an exciting opportunity to work with a team of passionate professionals dedicated to leveraging data for impactful decisions and products.
Responsibilities
Data Architecture:Design and build robust, scalable data architectures that support both operational and analytical use cases.
ETL Development:Develop and maintain ETL processes to gather data from various sources, ensuring data quality and consistency.
Data Modeling:Create and optimize data models to support efficient data storage, retrieval, and analysis.
API Development:Design and implement APIs for data ingestion, processing, and retrieval.
Data Visualization:Develop dashboards and reports to visualize complex datasets in a user-friendly manner.
Cloud Solutions:Work with cloud technologies to deploy and maintain data solutions.
Collaboration:Work closely with data scientists, analysts, and other engineers to integrate data solutions into company products and services.
Innovation:Stay updated with the latest trends and technologies in data engineering and propose innovative solutions to improve existing systems.
Requirements
Education:Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Experience:Proven experience as a Full Stack Data Engineer or in a similar role.
Technical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.
Data Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).
Front-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.
Cloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.
Analytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.
Communication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.
Benefits
Competitive salary and benefits package.
Dynamic and innovative work environment.
Opportunities for professional growth and development.
Flexible working hours and supportive team culture.
Must Have Skills:
Technical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.
Data Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).
Front-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.
Cloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.
Analytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.
Communication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.","Java, python, Spark, hadoop, Sql, aws"
Senior Data Engineer,NXP Semiconductors,3-8 Years,,Bengaluru,Semiconductor,"You will work with Product Owners, Architects, Data Scientists, and other stakeholders to design, build and maintain ETL/ ELT pipelines, data pipelines and jobs, combining data from multiple source systems into one or multiple target systems.
Solutions delivered must adhere to EBI and IT architectural principles pertaining to capacity planning, performance management, data security, data privacy, lifecycle management and regulatory compliance.
Assisting the Operational Support team with analysis and investigation of issues is also expected, as needed.
Required Skills and Qualifications
Proven experience as a Data Engineer
Hands on experience in ETL design and development concepts (3+ years)
Experience with AWS and Azure cloud platforms and their data service offerings
Proficiency in SQL, PySpark, Python
Experience with GitHub, GitLab, CI/CD
Knowledge of advanced analytic concepts including AI/ML
Strong problem-solving skills and ability to work in a fast-paced and collaborative environment
Excellent oral and written communication skills
Preferred Skills Qualifications:
Experience with Agile / DevOps
Proficiency in SQL (Databricks, Teradata)
Experience with DBT","ETL/ELT, data engineering, Azure, Python, Sql, AWS"
Data Engineer / Senior Data Engineer,Infec Services Private Limited,3-12 Years,,Hyderabad,Software,"Job Descriptions:
Job Title: Data Engineer / Senior Data Engineer
Job Type: Contract To Hire(3 Months)
Yrs of Experience:3 to 5 yrs / 6 to 12 yrs
Notice Period: Immediate - 15 Days
Primary Skill Set: Data Engineer, GCP,DBT, Data flow, Composer, Python, SQL, Microservices
What are we required to do
Design, build, and maintain scalable and robust data pipelines for processing large volumes of data.
Develop ETL (Extract, Transform, Load) processes to ingest, clean, and transform data from various sources.
Implement and optimize data storage solutions, including data lakes and warehouses, using GCP services like BigQuery, Cloud Storage, and Dataflow.
Integrate data from multiple data sources, ensuring data consistency and quality.
Develop and maintain automated data processing workflows, monitoring, and alerting systems.
Manage and maintain metadata, data catalogs, and data lineage documentation.
Work closely with data scientists, analysts, and other stakeholders to understand data requirements and provide appropriate solutions.
Collaborate with cross-functional teams to ensure data availability and reliability for various use cases.
Provide technical support and guidance to other team members on data engineering best practices.
Optimize data processing pipelines for performance, scalability, and cost-efficiency.
Monitor data pipeline performance, troubleshoot issues, and implement necessary improvements.
Ensure data security and compliance with relevant regulations and best practices.
What you'll need
experience in data engineering or related roles.
Proven experience with Python for data processing, scripting, and automation.
Hands-on experience with Google Cloud Platform (GCP) services, including BigQuery, Cloud Storage, Dataflow, and Pub/Sub.
Strong understanding of data modeling, ETL processes, and data warehousing concepts.
Strong problem-solving skills and attention to detail.
Excellent communication skills, with the ability to explain complex technical concepts to non-technical stakeholders.
Ability to work independently and as part of a team in a fast-paced environment.","Data Engineer, Sql, Python, Microservices"
Senior Data Engineer,Ciklum,3-7 Years,,Chennai,Information Technology,"About the Role
As a Senior Data Engineer, you will be part of a cross-functional development team engineering experiences of tomorrow. This is an ideal role for professionals passionate about data, innovation, and working in a dynamic, international environment.
Responsibilities
Build and prototype scalable data pipelines and develop new API integrations to handle increasing data volume and complexity.
Write reusable, testable, and efficient Python code for data ingestion and integration.
Use SQL and Python to analyze data and derive insights in collaboration with business stakeholders.
Work on multiple projects simultaneously, adapting to shifting priorities in a fast-paced environment.
Apply creative problem-solving approaches to complex data challenges, continuously improving data strategies and execution.
Contribute to design, code, configurations, and documentation for components managing data ingestion, real-time streaming, batch processing, ETL, and data storage.
Help maintain and improve engineering infrastructure, identifying and addressing gaps to boost platform quality, robustness, maintainability, and performance.
Collaborate with engineering teams to ensure solutions meet functional, performance, availability, scalability, and reliability standards.
Perform development, QA, and DevOps tasks as needed to ensure end-to-end ownership of solutions.
Work closely with business analysts and data scientists to support their use cases.
Participate in team and unit activities, contribute to community building, and represent the company in conferences and best practice initiatives.
Support sales activities, customer engagements, and digital services.
Understand and contribute to cloud infrastructure design and implementation.
Requirements
Proven experience as a data engineer or in a similar role focused on data integration and management.
Strong programming skills in Python, including working with APIs and automation.
Solid foundation in SQL and relational database design.
Excellent problem-solving skills with an algorithmic mindset.
Strong communication skills and ability to collaborate in a team-oriented environment.
Experience building and maintaining robust, scalable data pipelines.
Hands-on experience with batch and streaming data processing systems.
Familiarity with best practices in software development, including documentation, testing, and performance optimization.
Demonstrated ability to work independently and take initiative in high-impact projects.
Understanding of cloud infrastructure and deployment.
Desirable
Experience with cloud platforms (e.g., AWS, Google Cloud Platform, Azure).
Knowledge of data warehousing concepts and ETL methodologies.
Familiarity with version control tools such as Git.","SQL and Python, cloud platforms, ETL methodologies, Google Cloud Platform, Data Engineer, Azure, AWS"
Senior Data Engineer - Visa,Foray Software,4-6 Years,,Bengaluru,Consulting,"Bachelor s Degree in Computer Science, Computer Engineering or related technical field required. Master s Degree or other advanced degree preferred.
4-6+ years of total experience of which 2+ years of relevant experience in Big Data platforms.
Strong analytical, problem solving and communication/articulation skills.
3+ years of experience with big data and the Hadoop ecosystem (Spark, HDFS, Hive, Sqoop, Hudi, Parquet, Apache Nifi and Kafka).
Hands On in Scala/Spark
Python is a plus
Hands on knowledge of Oracle, MS-SQL databases.
Experience with job schedulers such as CA or AutoSys.
Experience with source code control systems (e.g. Git, Jenkins, Artifactory).
Experience with platforms such as Tableau and At Scale is a plus.","Analytical, Git, MS SQL, Spark, Tableau, Big Data, Oracle, Apache, Python"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Mumbai, Pune",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.
User will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training
Demonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.
Possess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.
Optimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.
Develop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.
Monitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.
Stay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.
Collaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.
Ensure data privacy and security compliance in accordance with industry standards and regulations.
Qualifications we seek in you:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Strong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.
Strong understanding of data warehousing concepts, ETL processes, and data modeling.
Strong understanding of S3 and code-based scripting to move large volumes of data across application storage layers
Familiarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.
Excellent problem-solving, analytical, and critical thinking skills.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Preferred Qualifications/ skills
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.
Experience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.
Familiarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.
A continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Cloud Computing, Artificial Intelligence, Pytorch, Big Data Technologies, Aws, Tensorflow"
GCP Data Engineer,Nihilent,5-7 Years,,Pune,Information Technology,"Unnesting of JSON files in Google BigQuery
Google Cloud Run (using container services, python)
Hands on experience in GCP services like composer, cloud function, cloud run
Experience in developing CI/CD pipelines.
Experience in DBT scripts, docker files and knowledge on datavault is a plus.
Strong technical knowledge and hands on experience of python or java
Role: Data Engineer
Industry Type: IT Services & Consulting
Department: Data Science & Analytics
Employment Type: Full Time, Permanent
Role Category: Data Science & Machine Learning
Education
UG: Any Graduate
PG: Any Postgraduate","Gcp, Cloud, Json, Python"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS.
This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives.
Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.
User will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training
Demonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.
Possess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.
Optimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.
Develop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.
Monitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.
Stay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.
Collaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.
Ensure data privacy and security compliance in accordance with industry standards and regulations.
Qualifications we seek in you:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Strong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.
Strong understanding of data warehousing concepts, ETL processes, and data modeling.
Strong understanding of S3 and code-based scripting to move large volumes of data across application storage layers
Familiarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.
Excellent problem-solving, analytical, and critical thinking skills.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Preferred Qualifications/ skills
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.
Experience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.
Familiarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.
A continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Data Processing, Cloud Computing, data engineering, Machine Learning, SSIS, Python, AWS"
"Data Engineer (Pandas, SQL, Snowflake)",Franklin Templeton,4-6 Years,,Hyderabad,Financial Services,"This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. We re looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale.
The ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.
What is the Data Engineer responsible forServe as a hands - on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office:
Collaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.
Lead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.
Build, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.
Develop dashboards and data visualizations that support strategic business decisions.
Stay current with emerging trends in data engineering and help implement best practices within the team.
Mentor and support junior engineers, fostering a culture of learning and technical excellence.
What ideal qualifications, skills & experience would help someone to be successfulBachelor s or master s degree in computer science, data science, engineering, or a related field:
4+ years of experience in data engineering.
Strong Pandas & SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).
Deep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.
Proficiency in at least one programming language - Python preferred.
Self-starter with a passion for learning new tools and technologies.
Strong communication skills and a collaborative, ownership-driven mindset.","snowflake, data engineering, Pandas, Sql, Python"
Data Engineer,Anblicks Solutions,5-10 Years,,Hyderabad,Cloud Data Services,"Must have 5+ years of experience in Python programming.
Extensive experience in handling complex projects.
Good leadership skills and ability to lead projects independently.
Good communication and teamwork skills.
Good analytical skills and proficiency in Web based development technologies.
Expertise in Agile development methodology.
Strong problem-solving skills.
Strong knowledge in Design patterns, Security, Performance tuning and App monitoring.
Experience in API Design, security patterns, Microservices architecture.
Must have experience in building scalable, low latency based web application using Flask, FastAPI or similar.
Experience in building python-based SDKs.
Experience in python based ORM (Object Relational Mapper) frameworks.
Knowledge in distributed in-memory systems such as Redis, MemCache, GemFire or similar.
Knowledge in SQL database and no-SQL database.
Understanding on building Production grade application using WSGI/ASGI frameworks such as Gunicorn, Uvicorn etc.
Experience of Jenkins, CICD pipeline, Logging framework, Splunk, ELK stack is good to have.
Knowledge and experience with virtualization and cloud platforms (Kubernetes, AWS, OpenShift, Docker Containers)
Familiarity with version control system such as Git.
Knowledge on developing test suites, application debugging skills.
Good to have:
Understanding on front-end technologies including JavaScript, HTML5, Angular.
Understanding on Event-driven programming using Python.
Knowledge of Generative AI technologies, LLMs, Embeddings, building RAG based solutions.
Understanding on HDFS, Hive, Impala, PySpark and other components of BigData systems.","web based development, app monitoring, python, Sql, Python Programming, Performance Tuning"
Data Engineer,Systechcorp Inc,2-5 Years,,Pune,Software,"Senior Data Engineer
Skills:
Azure Cloud technologies
Mandatory
Synapse (DataFlow, Pipeline (including web call), Notebook (Pyspark))
SQL expertise
Key vault
Blob Storage
Preferable
Logic Apps
Function App (C#)
API Management
GIT","Synapse, Key Vault, Blob Storage, data engineering, Pyspark, Azure, Sql"
Data Engineer,Impetus Technologies,3-5 Years,,"Bengaluru, Noida, Pune",Software,"Must have technical Skills
3-5 Years of experience
Expertise and hands-on experience on Python Must Have
Expertise knowledge on SparkQL/Spark Dataframe Must Have
Good knowledge of SQL Good to Have
Good knowledge of Shell script Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Good knowledge of Cloud- Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Roles & Responsibilities
Selected candidate will work on Data Warehouse modernization projects and will responsible for the following activities.
Develop programs/scripts in Python/Java + SparkSQL/Spark Dataframe or Python/Java + Cloud native SQL like RedshiftSQL/SnowSQL etc.
Validation of scripts
Performance tuning
Data ingestion from source to target platform
Job orchestration
Role:Big Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Any Specialization, Any Graduate","data engineering, Data Pipeline, Pyspark, Big Data, Data Warehousing, Python, Sql, Data Transformation"
Data Engineer,Coditas Technologies,3-6 Years,,Pune,Software,"We are looking for data engineers who have the right attitude, aptitude, skills, empathy,
compassion, and hunger for learning. Build products in the data analytics space. A passion
for shipping high-quality data products, interest in the data products space; curiosity about
the bigger picture of building a company, product development and its people.
Roles and Responsibilities
Develop and manage robust ETL pipelines using Apache Spark (Scala)
Understand park concepts, performance optimization techniques and governance tools
Develop a highly scalable, reliable, and high-performance data processing pipeline to extract, transform and load data from various systems to the Enterprise Data Warehouse/Data Lake/Data Mesh
Collaborate cross-functionally to design effective data solutions
Implement data workflows utilizing AWS Step Functions for efficient orchestration.
Leverage AWS Glue and Crawler for seamless data cataloging and automation
Monitor, troubleshoot, and optimize pipeline performance and data quality
Maintain high coding standards and produce thorough documentation. Contribute to high-level (HLD) and low-level (LLD) design discussions
Technical Skills
Minimum 3 years of progressive experience building solutions in Big Data environments.
Have a strong ability to build robust and resilient data pipelines which are scalable, fault tolerant and reliable in terms of data movement.
3+ years of hands-on expertise in Python, Spark and Kafka.
Strong command of AWS services like EMR, Redshift, Step Functions, AWS Glue, and AWS Crawler.
Strong hands on capabilities on SQL and NoSQL technologies.
Sound understanding of data warehousing, modeling, and ETL concepts
Familiarity with High-Level Design (HLD) and Low-Level Design (LLD) principles
Excellent written and verbal communication skills.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate","Airflow, Athena, Python, Sql, S3, Aws Lambda, Amazon Ec2, Amazon Redshift, Pyspark, AWS Glue, Azure"
Data Engineer Sr Associate,TechnoGen,6-10 Years,,Hyderabad,"Consulting, Information Services","Role & responsibilities
Bachelors degree in computer science, engineering, or a related field. Master's degree preferred.
Data: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.
SQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.
ELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC
Migration Experience: Experience Informatica on prem to IICS/IDMC migration
Cloud: 5+ years experience working in AWS cloud environment
Python: 5+ years of hands-on experience of development with Python
Workflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)
Advanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka
Troubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues
Communication: Excellent communication, problem-solving and organizational and analytical skills
Able to work independently and to provide leadership to small teams of developers.
Reporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)
Experience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms.","Airflow, data engineering, Aws Cloud, Tableau, Data Analytics, Apache, Sql, Python"
Senior Data Engineer,Egon Zehnder,8-13 Years,,Gurugram,Information Technology,"Job Summary
As a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.
Executing end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.
The Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.
The Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.
The Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.
Identifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.
Collaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.
Experience & Key Competencies
Engineering Degree or equivalent.
3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.
Experience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.
Proven experience in building ETL/ELT pipelines, preferably from SQL to Azure services
Architecture experience of making ETL operations such as Medallion Architecture etc.
Familiarity with CI/CD practices for data pipelines and version control using Git
Experience in integrating new/replacing vendor products in existing ecosystem.
Experience in migrating data structured, unstructured data to Data lake
Experience or understanding of performance engineering both at system level and database level.
Experience or understanding of Data Governance, Data Quality, Data Issue Management.
Work closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications
Ensure compliance with all regulations, policies, and procedures.
Escalate issues/risks pro-actively to appropriate stakeholders.
Regularly communicate status and challenges to team members and management.
Self-driven with keenness to master, suggest and work with different technologies & toolsets.
Excellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers
Excellent and resourceful problem-solving skills, adaptable and willingness to learn.
Good analysis skills - to be able to join the dots across multiple applications and interfaces between them.
Skill Set
Experience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.
Experience in Data factory and Data Lake technologies.
Rich Experience in data modelling techniques and creating various data models.
Experience in Azure cloud services and architecture patterns.
Understanding of RESTful APIs for data distribution.
Understanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments
Excellent oral and written communication with an ability to articulate complex systems to multiple teams.
Self-motivation and the ability to work under minimal supervision
Benefits Highlights:
5 Days working in a Fast-paced work environment
Work directly with the senior management team
Reward and Recognition
Employee friendly policies
Personal development and training
Health Benefits, Accident Insurance
Potential Growth for you
We will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.","Data Management, Data Quality, Version Control, postgresql, mysql, Sql"
Data Engineer - MDM/PIM/Syndigo/Attacama/Informatica,Reflections Info Systems,6-11 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru",IT Management,"Responsibilities include:
Work as part of a team to develop Data and Analytics solutions.
Participate in the development of cloud data warehouses, data as a service, business intelligence solutions
Ability to provide solutions that are forward-thinking in data integration.
Deliver a quality product.
Developing Modern Data Warehouse solutions using Azure or AWS Stack
Certifications :
Bachelor s degree in computer science & engineering or equivalent demonstrable experience
Desirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.
Primary Skills :
6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions
Programming experience in Scala or Python, SQL
Min 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks
Working knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse
Demonstrated analytical and problem-solving skills
Excellent written and verbal skills (English)
Secondary Skills :
Familiar with Agile Practices
Familiar with Version control platforms GIT, CodeCommit etc.
Problem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.
Ownership
Proactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
Aws Data Engineer,SRS Infoway,8-10 Years,,"Ahmedabad, Bengaluru, Chennai",Information Technology,"Looking for AWS & MuleSoft developers with expertise in ETL, APIs, AWS Glue, Lambda, SNS/SQS, and RDS for cloud and event-driven architecture. Must be available during North America hours and capable of managing vendor interactions. Strong integration skills required.","Manufacturing Industry, Aws Lambda, Sqs, Sns, AWS Glue, Amazon Rds"
Senior Azure Data Engineer,Mindera,7-12 Years,,Bengaluru,Software,"Job description
Experience in distributed computing (spark) and software development.
Experience in spark-scala.
Experience in Data Engineering.
Experience in Python
Experience of working on Cloud environments (Azure).","Azure, Python, Sql"
Data Engineer II,Tekion,3-5 Years,,Chennai,Software,"Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.
Should be able to design (high level / low level) software solutions for the new requirements.
Coding independently and with other team members with proper software industry standard best practices.
Collaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.
Qualifications:
B.E/MTech in computer science
3 - 5 yearsof relevant work experience.
Experience in building scalable products with preferably big data.
ExcellentPythoncoding skills (Mandatory)
Experience inApache spark, Data Lakeand other Big data technologies.
Experience in either Data Warehouses or Relational Database is mandatory.
Experience inAWScloud
Mandatory Skills: Python , Spark","Spark, Data Lake, Apache Spark, Data Engineer, Python, Aws Cloud"
Data Engineer Supervisor/Manager,Ford,12-20 Years,,Chennai,Automotive,"We are seeking an experienced Manager to lead a team responsible for the development and maintenance of our Connected Vehicle enterprise data and data products. The ideal candidate will have a strong technical background in data and/or software engineering, along with proven leadership and management skills. This role requires the ability to design and code streaming solutions, prioritize team tasks, make timely decisions, run results-focused meetings, and guide the team to deliver high-quality results. The leader must be knowledgeable in data governance, customer consent, and security standards.
Responsibilities:
Lead and mentor a high-performing team of local and remote engineers.
Prioritize team workload, allocate tasks effectively, and ensure team members have the resources to succeed.
Provide technical expertise and guidance to the team.
Evaluate and mentor adherence to coding standards, best practices, and architectural guidelines.
Oversee the design, development, maintenance, scalability, reliability, and performance of the connected vehicle data platform pipelines and architecture. Contribute to the long-term strategic direction of the Connected Vehicle Data Platform with a focus on enterprise use.
Communicate decisions effectively and transparently to internal and external customers.
Accurately and routinely track all prioritized work in JIRA to support both financial delivery tracking
Enforce and ensure data quality, data governance, and security standards.
Lead implementation and delivery of various business customers requests and logic into the data assets with optimized design and code development.
Identify and consolidate common tasks across teams to improve efficiency and reduce redundancy.
Stay updated on industry trends and emerging technologies to inform technical decisions.
Qualifications Required:
Bachelor's degree in computer science, Information Technology, Information Systems, or Data Analytics.
5 years of progressive responsibilities in a complex streaming data environment.
5+ years experience leading a software/data engineering team.
5+ years of experience in Big Data Environments or expertise with Big Data tools.
Expertise in Google Cloud Platform and Services for End-to-End Data Engineering
Expert knowledge and hands on experience in DevOps and SDLC.
Monitor and optimize cost and compute for processes in GCP technologies (e.g., BigQuery, Dataflow, Cloud Run, DataProc).
Manage and scale serverless applications and clusters, optimizing resource utilization, and implementing monitoring and logging strategies.
Expertise in streaming technologies (Kafka, Pub/Sub) and OpenShift, managing high-throughput topics, message ordering, and ensuring data consistency and durability.
Even better you may have:
Expertise in other public cloud environments: Amazon Web Services, Microsoft Azure, etc.
Hands on experience on AI Engineering","Devops, data engineering, Gcp, Kafka, DataFlow, AWS"
Senior Data Engineer,Ciklum,8-13 Years,,Chennai,Information Technology,"About the Role
As a Senior Data Engineer, you will become part of a cross-functional development team focused on engineering the data experiences of tomorrow. This role is ideal for someone passionate about scalable data systems, eager to solve complex problems, and ready to work in a dynamic environment.
Responsibilities
Build and prototype scalable data pipelines to handle increasing data volume and complexity.
Develop and maintain API integrations for efficient data ingestion and transformation.
Write reusable, testable, and efficient Python code for data integration tasks.
Use SQL and Python to extract insights from data in collaboration with business stakeholders.
Manage multiple concurrent projects, adapting to shifting priorities in a fast-paced setting.
Solve complex challenges creatively to continuously improve data strategies and systems.
Contribute to the design, codebase, configurations, and documentation for components handling data ingestion, real-time streaming, batch processing, ETL, and storage.
Continuously enhance engineering infrastructure by identifying gaps and improving platform robustness, maintainability, and speed.
Collaborate with engineering teams to ensure that solutions meet customer expectations in functionality, performance, scalability, and reliability.
Take end-to-end responsibility, contributing across development, QA, and DevOps roles.
Partner with business analysts and data scientists to understand their use cases and support them effectively.
Participate in unit activities and community-building initiatives, contribute to conferences, and promote best practices.
Support sales efforts by participating in customer meetings and providing technical insights on digital services.
Demonstrate understanding and hands-on application of cloud infrastructure design and implementation.
Requirements
Proven experience as a Data Engineer or in a similar role with a focus on data integration and management.
Strong programming skills in Python, particularly in API interactions and automation.
Solid foundation in SQL and relational database design.
Strong algorithmic thinking and problem-solving skills.
Effective communication and collaboration abilities in a team environment.
Hands-on experience with designing and maintaining data ingestion, transformation, and loading pipelines across various storage systems.
Exposure to data streaming tools and batch processing frameworks.
Ability to troubleshoot and improve engineering infrastructure and deployment environments.
End-to-end ownership mindset, including development, testing, and DevOps.
Experience working with stakeholders across business and technical functions.
Active contribution to team knowledge sharing, technical community engagement, and continuous learning.
Understanding of cloud infrastructure and its components (e.g., storage, compute, networking).
Desirable
Experience with cloud platforms such as AWS, Google Cloud Platform, or Azure.
Familiarity with data warehousing and ETL techniques.
Proficiency with version control systems like Git.","SQL and Python, cloud platforms, ETL methodologies, Google Cloud Platform, Data Engineer, Azure, AWS"
Senior AWS Data Engineer,iLink Digital,5-10 Years,,Bengaluru,Financial Services,"We are seeking a highly skilled Data Engineer with5-7 years of experience to develop, sustain, troubleshoot, and monitor data pipelines.The ideal candidate should be proficient in Python and SQL, with expertise in databases including Redshift, Aurora, and MySQL.This role requires a strong analytical mindset, problem-solving skills, and the ability to work collaboratively in a dynamic environment.
Key Responsibilities:
Develop, optimize, and maintain data pipelines to ensure smooth data flow.
Troubleshoot and resolve issues related to data pipelines and database performance.
Monitor data systems to ensure reliability, accuracy, and efficiency.
Collaborate with cross-functional teams to enhance data-driven decision-making.
Technical Skills Required:
Programming: Proficient in Python and SQL
Databases: Strong knowledge of Redshift, Aurora and MySQL
Data Engineering: Experience in building and maintaining scalable data solutions","aurora, python, Redshift, mysql, Sql, data engineering"
Data Engineer Supervisor,Ford,9-13 Years,,Chennai,Automotive,"We are seeking an experienced Manager to lead a team responsible for the development and maintenance of our Connected Vehicle enterprise data and data products. The ideal candidate will possess a strong technical background in data and/or software engineering, coupled with proven leadership and management skills.
This role requires the ability to design and code streaming solutions, prioritize team tasks, make timely decisions, run results-focused meetings, and guide the team to deliver high-quality results. The successful leader must be knowledgeable in data governance, customer consent, and security standards.
Qualifications Required:
Bachelor's degree in Computer Science, Information Technology, Information Systems, or Data Analytics.
5 years of progressive responsibilities in a complex streaming data environment.
5+ years experience leading a software/data engineering team.
5+ years of experience in Big Data Environments or expertise with Big Data tools.
Expertise in Google Cloud Platform and Services for End-to-End Data Engineering.
Expert knowledge and hands-on experience in DevOps and SDLC.
Ability to monitor and optimize cost and compute for processes in GCP technologies (e.g., BigQuery, Dataflow, Cloud Run, DataProc).
Experience managing and scaling serverless applications and clusters, optimizing resource utilization, and implementing monitoring and logging strategies.
Expertise in streaming technologies (Kafka, Pub/Sub) and OpenShift, managing high-throughput topics, message ordering, and ensuring data consistency and durability.
Even better you may have:
Expertise in other public cloud environments: Amazon Web Services, Microsoft Azure, etc.
Hands-on experience in AI Engineering.
Responsibilities:
Lead and mentor a high-performing team of local and remote engineers.
Prioritize team workload, effectively allocate tasks, and ensure team members have the necessary resources to succeed.
Provide technical expertise and guidance to the team; evaluate and mentor adherence to coding standards, best practices, and architectural guidelines.
Oversee the design, development, maintenance, scalability, reliability, and performance of the connected vehicle data platform pipelines and architecture.
Contribute to the long-term strategic direction of the Connected Vehicle Data Platform with a focus on enterprise use.
Communicate decisions effectively and transparently to internal and external customers.
Accurately and routinely track all prioritized work in JIRA to support financial delivery tracking.
Enforce and ensure data quality, data governance, and security standards.
Lead implementation and delivery of various business customer requests and logic into the data assets with optimized design and code development.
Identify and consolidate common tasks across teams to improve efficiency and reduce redundancy.
Stay updated on industry trends and emerging technologies to inform technical decisions.","Architecture, Gcp, Cloud, Data Governance, JIRA, Sdlc, data engineering"
Senior Data Engineer,ThoughtFocus,5-8 Years,,Hyderabad,Information Technology,"Key Responsibilities:
Database Administration Maintenance
Install, configure, and maintain database management systems (DBMS) such as MySQL, PostgreSQL, SQL Server, Oracle, or MongoDB.
Ensure database security, backup, and disaster recovery strategies are in place.
Monitor database performance and optimize queries, indexing, and storage.
Apply patches, updates, and upgrades to ensure system stability and security.
Database Design Development
Design and implement database schemas, tables, and relationships based on business requirements.
Develop and optimize stored procedures, functions, and triggers.
Implement data partitioning, replication, and sharding strategies for scalability.
Performance Tuning Optimization
Analyze slow queries and optimize database performance using indexing, caching, and tuning techniques.
Conduct database capacity planning and resource allocation.
Monitor and troubleshoot database-related issues, ensuring minimal downtime.
Security Compliance
Implement role-based access control (RBAC) and manage user permissions.
Ensure databases comply with security policies, including encryption, auditing, and GDPR/HIPAA regulations.
Conduct regular security assessments and vulnerability scans.
Collaboration Automation
Work closely with developers, system administrators, and DevOps teams to integrate databases with applications.
Automate database management tasks using scripts and tools.
Document database configurations, processes, and best practices.
Required Skills Qualifications:
Experience:4+ years of experience in database administration, engineering, or related fields.
Education:Bachelor s or Master s degree in Computer Science, Information Technology, or related disciplines.
Technical Skills:
Strong knowledge of SQL and database optimization techniques.
Hands-on experience with at least one major RDBMS (MySQL, PostgreSQL, SQL Server, Oracle).
Experience with NoSQL databases (MongoDB, Cassandra, DynamoDB) is a plus.
Proficiency in database backup, recovery, and high availability solutions (Replication, Clustering, Mirroring).
Familiarity with scripting languages (Python, Bash, PowerShell) for automation.
Experience with cloud-based database solutions (AWS RDS, Azure SQL, Google Cloud Spanner).
Preferred Qualifications:
Experience with database migration and cloud transformation projects.
Knowledge of CI/CD pipelines and DevOps methodologies for database management.
Familiarity with big data technologies like Hadoop, Spark, or Elasticsearch.","Stored procedures, Database Design, Performance Tuning, RDBMS, MySQL, Automation"
Sr. Data Engineer,E Zest,3-7 Years,,Bengaluru,Information Technology,"Roles & responsibilities
Data Architecture Design: Designing and implementing data architectures that support data integration, transformation, and storage, ensuring scalability and high performance
Data Pipeline Development: Building and maintaining data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses or data lakes
Big Data Technologies: Working with big data technologies like Hadoop, Spark, and Kafka to process and manage large volumes of data efficiently
Data Modeling: Designing data models and schemas to support business intelligence, analytics, and reporting requirements
Data Quality and Governance: Implementing data quality checks and data governance processes to ensure data accuracy, consistency, and compliance with data policies
Performance Optimization: Identifying performance bottlenecks in data processing workflows and implementing optimizations to improve data pipeline efficiency
Data Security: Implementing data security measures to protect sensitive data and ensure compliance with data privacy regulations","Performance Optimization, Data Architecture Design, Data Pipeline Development, Data Modeling, Big Data Technologies, Data Security"
Data Engineer II,Trimble,3-6 Years,,Chennai,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.
Key Responsibilities:
Design and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.
Develop, optimize, and maintain data pipelines using AWS services.
Implement robust ETL/ELT processes and event-driven data ingestion.
Establish and enforce data governance policies, ensuring data quality, security, and compliance.
Optimize cloud resources for performance, availability, and cost-efficiency.
Partner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.
Identify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.
Our current tech-stack:
AWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.
DevOps: Cloudformation, Terraform, Git, CodeBuild
Database: Redshift, PostgreSQL, DynamoDB, Athena
Language: Bash, Python, SQL
Qualifications:
Bachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.
Expertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.
Extensive experience indata and cloud engineeringroles.
Expertise inAWSplatforms, including data services.
Strong competence inETL processes,data warehousing, and big data technologies.
Advanced skills inscripting,Python,SQL, and infrastructure automation tools.
Familiarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).
Experience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,"Overview:
We are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.
Key Responsibilities:
Provide support and expertise in the domain of Data Engineer.
Collaborate with cross-functional teams to achieve business goals.
Ensure timely delivery of services and maintain high-quality standards.
Required Qualifications:
Proven experience in a relevant field or position.
Strong understanding of the responsibilities and tools associated with the role.
Excellent problem-solving and communication skills.
Preferred Qualifications:
Certifications or training relevant to Data Engineer.
Experience working in a fast-paced environment or large organizations.","data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,"Overview:
We are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.
Key Responsibilities:
Provide support and expertise in the domain of Data Engineer.
Collaborate with cross-functional teams to achieve business goals.
Ensure timely delivery of services and maintain high-quality standards.
Required Qualifications:
Proven experience in a relevant field or position.
Strong understanding of the responsibilities and tools associated with the role.
Excellent problem-solving and communication skills.
Preferred Qualifications:
Certifications or training relevant to Data Engineer.
Experience working in a fast-paced environment or large organizations.","data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Delhi, Bengaluru, Chennai",Software,"Key Responsibilities:
- Design, develop, and maintain data pipelines using Python, SQL, and Kedro
- Implement serverless solutions using AWS Lambda and Step Functions
- Develop and manage data workflows in Azure and AWS cloud environments
- Create integrations between data systems and Power Platform (Power Apps, Power Automate)
- Design, develop, and maintain APIs for data exchange and integration
- Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
- Optimize data storage and retrieval processes for improved performance
- Collaborate with cross-functional teams to understand data requirements and provide solutions
- API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Database Design, data engineering, Cloud Services, Data Extraction, Azure, Sql, Python"
AWS Data Engineer,IDESLABS,4-7 Years,,Pune,"Recruiting, Staffing Agency","Job description
The Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams
The Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives
The role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies
This role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems
Design and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala","Data Management, Sdlc, Scala, Data Architecture, Spark"
Data Engineer II,Trimble,3-6 Years,,Hyderabad,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.
Key Responsibilities:
Design and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.
Develop, optimize, and maintain data pipelines using AWS services.
Implement robust ETL/ELT processes and event-driven data ingestion.
Establish and enforce data governance policies, ensuring data quality, security, and compliance.
Optimize cloud resources for performance, availability, and cost-efficiency.
Partner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.
Identify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.
Our current tech-stack:
AWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.
DevOps: Cloudformation, Terraform, Git, CodeBuild
Database: Redshift, PostgreSQL, DynamoDB, Athena
Language: Bash, Python, SQL
Qualifications:
Bachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.
Expertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.
Extensive experience indata and cloud engineeringroles.
Expertise inAWSplatforms, including data services.
Strong competence inETL processes,data warehousing, and big data technologies.
Advanced skills inscripting,Python,SQL, and infrastructure automation tools.
Familiarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).
Experience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
Lead Data Engineer,NXP Semiconductors,7-12 Years,,Bengaluru,Semiconductor,"Proven experience as a Data Engineer
Hands on experience in ETL design and development concepts (7+ years)
Experience with AWS and Azure cloud platforms and their data service offerings
Proficiency in SQL, PySpark, Python
Experience with GitHub, GitLab, CI/CD
Knowledge of advanced analytic concepts including AI/ML
Strong problem-solving skills and ability to work in a fast-paced and collaborative environment
Excellent oral and written communication skills
Preferred Skills Qualifications:
Experience with Agile / DevOps
Proficiency in SQL (Databricks, Teradata)
Experience with DBT
Experience with Dataiku platform, including administration","data engineering, Azure, Sql, Python, Etl, AWS"
Lead Data Engineer-Databricks,Anblicks Solutions,10-14 Years,,"Ahmedabad, Hyderabad",Cloud Data Services,"Role & responsibilities
Lead Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise inAzure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake)along with big data processing, real-time analytics, and cloud data integration and Team Leading Experience.
Key Responsibilities:
1. Data Pipeline Development & ETL/ELT
Design and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.
Implement ETL/ELT workflows for structured and unstructured data processing.
Optimize data ingestion, transformation, and storage strategies.
2. Cloud Data Architecture & Integration
Develop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).
Work with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.
3. Database Management & Optimization
Design and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).
Optimize SQL queries and indexing strategies for performance.
Implement data partitioning, compression, and caching for efficiency.
4. Data Governance, Security & Compliance
Ensure data quality, lineage, and governance with tools like Purview.
Implement role-based access control (RBAC), encryption, and security policies.
Ensure compliance with GDPR, HIPAA, and ISO 27001 regulations.
5. Monitoring & Performance Tuning
Use Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.
Automate data pipeline testing and validation.
6. Collaboration & Documentation
Document data models, pipeline architectures, and data workflows.
Immediate joiners are preferred.","data models, log analytics, Data Processing, Azure, Database Management, elt"
Lead Data Engineer - Azure Fabric,Kanini Software Solutions,8-10 Years,,"Bengaluru, Chennai, Pune",Software,"We are looking for a Lead Data Engineer with expertise inAzure Fabric, Data Architecture, and ETL Pipelines. The ideal candidate will design and implement scalable data solutions, ensuring efficient data processing, governance, and analytics on the Azure cloud platform.
Key Responsibilities:
Design & develop data pipelines using Azure Fabric, Data Factory, and Synapse Analytics.
Implement ETL/ELT workflows for structured & unstructured data processing.
Optimize SQL queries, data modeling, and performance tuning.
Ensure data security, governance, and compliance using best practices.
Collaborate with cross-functional teams for data-driven insights and analytics.
Required Skills:
Azure Fabric, Azure Synapse, Azure Data Factory
ETL, SQL, Data Pipeline Design, Data Architecture
Big Data & Analytics, Data Governance, Performance Optimization
CI/CD, Azure DevOps, Git
Role:Data Science & Analytics - Other
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Analytics - Other
Education
UG:B.Tech/B.E. in Any Specialization
PG:Any Postgraduate","Architecture, Fabric, data engineering, Data Pipeline, Azure, Sql, Etl"
Lead Data Engineer,Coditas Technologies,7-11 Years,,Pune,Software,"We are looking for a Tech Lead expertise in Advanced Big Data Technology Stack. The person should be a hands-on technical expert in building and deploying applications using Big Data technologies. The person should have progressive experience in building highly scalable distributed systems. The person should have the ability to build a high-performance strong technical team that adheres to the strong quality standard of application development.
Roles and responsibilities
With over 7 yrs. of hands-on experience with Data Technologies
Requirement analysis and assess the technical feasibility of proposed solutions.
Act as the technical specialist in designing and recommending architecture for application development/feature development
Working on designing the application architecture and estimating effort in delivering features for new requirements
Implement data ingestion and transform pipeline for analytics and Dashboard reporting for business
Work in designing large-scale distributed computing applications using tools like Spark, Kafka, Hive, etc.
Performing validations, reconciliation, and consolidations for the imported data, Data migration, and data generation.
Effectively communicate with the Engineering Managers and Stakeholders to set the right expectations.
Technical Skills
Minimum 5 years of progressive experience building solutions in Big Data environments
Have a strong ability to build robust and resilient data pipelines that are scalable, fault-tolerant, and reliable in terms of data movement
Hands-on experience of Apache Spark with Python for batch and stream processing
Should know experience in batch and stream data processing
Exposure to working on projects across multiple domains
Hands-on experience in Apache Kafka
Strong hands-on capabilities in SQL and NoSQL technologies
Hands-on experience with AWS services like S3, DMS, Redshift, Glue, Lambda, Kinesis, MSK, etc. is must have or similar services of either Azure/GCP
Strong analytical/quantitative skills and comfortable working with very large sets of data.
Excellent written and verbal communication skills
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate","Airflow, Data Ingestion, Data Pipeline, Data Modeling, Pyspark, Data Architecture, Data Warehousing, Data Governance, Python, Sql, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Gurugram,Software,"Position Summary:
We are looking for candidates with hands on experience in Big Data or Cloud Technologies.
Must have technical Skills
7 to 10 Years of experience
Expertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have
Good and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of PySpark (SparkSQL) Must Have
Good knowledge of Shell script & Python Good to Have
Good knowledge of SQL Good to Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Good Communication Skills Must Have
*Data Ingestion, Processing and Orchestration knowledge
Roles & Responsibilities
Lead technical implementation of Data Warehouse modernization projects for Impetus
Design and development of applications on Cloud technologies
Lead technical discussions with internal & external stakeholders
Resolve technical issues for team
Ensure that team completes all tasks & activities as planned
Code Development
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Aviation, Information Technology, Computers
PG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Bengaluru,Software,"Position Summary:
We are looking for candidates with hands on experience in Big Data or Cloud Technologies.
Must have technical Skills
7 to 10 Years of experience
Expertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have
Good and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of PySpark (SparkSQL) Must Have
Good knowledge of Shell script & Python Good to Have
Good knowledge of SQL Good to Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Good Communication Skills Must Have
*Data Ingestion, Processing and Orchestration knowledge
Roles & Responsibilities
Lead technical implementation of Data Warehouse modernization projects for Impetus
Design and development of applications on Cloud technologies
Lead technical discussions with internal & external stakeholders
Resolve technical issues for team
Ensure that team completes all tasks & activities as planned
Code Development
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Aviation, Information Technology, Computers
PG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Pune,Software,"Position Summary:
We are looking for candidates with hands on experience in Big Data or Cloud Technologies.
Must have technical Skills
7 to 10 Years of experience
Expertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have
Good and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of PySpark (SparkSQL) Must Have
Good knowledge of Shell script & Python Good to Have
Good knowledge of SQL Good to Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Good to Have
Automation approach - Good to Have
Good Communication Skills Must Have
*Data Ingestion, Processing and Orchestration knowledge
Roles & Responsibilities
Lead technical implementation of Data Warehouse modernization projects for Impetus
Design and development of applications on Cloud technologies
Lead technical discussions with internal & external stakeholders
Resolve technical issues for team
Ensure that team completes all tasks & activities as planned
Code Development
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Aviation, Information Technology, Computers
PG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Delhi, Hyderabad",Software,"Key Responsibilities:
Design, develop, and maintain data pipelines using Python, SQL, and Kedro
Implement serverless solutions using AWS Lambda and Step Functions
Develop and manage data workflows in Azure and AWS cloud environments
Create integrations between data systems and Power Platform (Power Apps, Power Automate)
Design, develop, and maintain APIs for data exchange and integration
Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
Optimize data storage and retrieval processes for improved performance
Collaborate with cross-functional teams to understand data requirements and provide solutions
API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer with BI and DWH - Ikrux,Ikrux Solutions (Opc) Private Limited,15-16 Years,,"Bengaluru, Pune",Information Technology,"Job description
Data Analytics and AIQuality AssuranceArtificial IntelligenceApplication Development and Software EngineeringCloud EngineeringCapability BuildingDevopsCRMSynapse-360
Need different solutions
Ikrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.
industries Overview
Banking, financial services and insurance (BFSI)FintechMediaGlobal Capability CentersTechnologyHealthcareE-CommerceOil and Gas
Need different solutions
Ikrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.
Job Category:AWS GlueAzureBusiness IntelligenceData EngineerData IngestionData ModellingData WarehousingPower BISSISTableau
Job Type:Full Time
Job Location:BangalorePune
We are seeking a highly experienced BI Architect and Developer with 6-15+ years of expertise in business intelligence, data visualization, and data engineering. The ideal candidate should hold a Bachelor s degree in Computer Science, Information Systems, Data Science, or a related field. Proficiency in Power BI and Tableau is essential, along with a strong background in ETL processes and tools such as SSIS. Advanced knowledge of SQL Server for query writing and database management is required, complemented by skills in exploratory data analysis using Python and familiarity with the CRISP-DM model.
The candidate should have experience working with diverse data models and databases, including Snowflake, Postgres, Redshift, and MongoDB. Expertise in visualization tools like Power BI, QuickSight, Plotly, and Dash is necessary. A strong programming foundation in Python is essential, particularly in data manipulation and analysis using Pandas, NumPy, and PySpark, as well as data serialization with JSON, CSV, and Parquet. Experience with cloud services like AWS S3, AWS Lambda, and Azure SDK is a plus. Proficiency in workflow orchestration tools such as Airflow and automation of ETL pipelines is required. Additional skills in interacting with REST APIs, web scraping, and version control for collaborative projects will be an advantage.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate
PG:Any Postgraduate","Workflow, Application Development, Json, Automation, Quality Assurance"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Mumbai, Pune",Software,"Key Responsibilities:
Design, develop, and maintain data pipelines using Python, SQL, and Kedro
Implement serverless solutions using AWS Lambda and Step Functions
Develop and manage data workflows in Azure and AWS cloud environments
Create integrations between data systems and Power Platform (Power Apps, Power Automate)
Design, develop, and maintain APIs for data exchange and integration
Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
Optimize data storage and retrieval processes for improved performance
Collaborate with cross-functional teams to understand data requirements and provide solutions
API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Bengaluru, Chennai",Software,"Key Responsibilities:
Design, develop, and maintain data pipelines using Python, SQL, and Kedro
Implement serverless solutions using AWS Lambda and Step Functions
Develop and manage data workflows in Azure and AWS cloud environments
Create integrations between data systems and Power Platform (Power Apps, Power Automate)
Design, develop, and maintain APIs for data exchange and integration
Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
Optimize data storage and retrieval processes for improved performance
Collaborate with cross-functional teams to understand data requirements and provide solutions
API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer,Mindera,4-9 Years,,Bengaluru,Software,"Job description
We are looking for an experienced Data Engineer to join our team.
Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.
As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.
This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.
You re great at
Python
Cloud - AWS/GCP
SQL
Airflow
Data testing
Snowflake","Gcp, Python, Sql, Aws"
Data Engineer,Mindera,4-9 Years,,Bengaluru,Software,"Job description
We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you ll create algorithms and conduct statistical analysis. Overall, you ll strive for efficiency by aligning data systems with business goals.
To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.
If you are detail-oriented, with excellent organizational skills and experience in this field, we d like to hear from you.
Implement/support new data solutions in datalake/datawarehouse built on snowflake.
Develop and design data pipelines using python, spark and snowflake.
Implement infrastructure as code using cloudformation and terraform.","Gcp, Python, Sql, Aws"
Data Engineer,Mindera,4-9 Years,,Bengaluru,Software,"Job description
Requirements
Implement/support new data solutions in datalake/datawarehouse built on snowflake.
Develop and design data pipelines using python, spark and snowflake.
Implement infrastructure as code using cloudformation and terraform.
Design and Implement Continuous Integration/Continuous Deployments pipelines.
Perform Data Modelling using downstream requirements.
Develop transformation scripts using advanced SQL and DBT.
Create reports/dashboard using business intelligence tools such as looker and tableau.
Write test cases/scenarios to ensure incident free production release.
Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
Identify areas of improvements in data pipelines, snowflake, infrastructure etc Conduct peer reviews of the code.
Debug production and development issues and provide support to colleagues where necessary.
Perform data quality checks to ensure quality of the data exposed to the end users.
Perform production deployments and perform a post-production support and validation (We follow a You build, you run it philosophy)
Build strong relationships with team, peers and stakeholders.
Contributes to overall data platform implementation.
Proficient in SQL/Python/Spark
Exposure to DBT would be preferable
Experience in AWS services such as Glue, Lambda, S3, DynamoDB, RDS, Kinesis, ECS/Fargate.
Experience working with modern data platforms such as redshift or snowflake
Experience working with BI tools such as looker and tableau
Experience working with docker
Proficient in data modelling.","Gcp, Python, Sql, Aws"
Data Engineer- MDM/PIM/Atacama/Informatica,Reflections Info Systems,6-10 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Chennai",IT Management,"Responsibilities include:
Work as part of a team to develop Data and Analytics solutions.
Participate in the development of cloud data warehouses, data as a service, business intelligence solutions
Ability to provide solutions that are forward-thinking in data integration.
Deliver a quality product.
Developing Modern Data Warehouse solutions using Azure or AWS Stack
Certifications :
Bachelor s degree in computer science & engineering or equivalent demonstrable experience
Desirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.
Primary Skills :
6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions
Programming experience in Scala or Python, SQL
Min 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake
Min 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks
Working knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse
Demonstrated analytical and problem-solving skills
Excellent written and verbal skills (English)
Secondary Skills :
Familiar with Agile Practices
Familiar with Version control platforms GIT, CodeCommit etc.
Problem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.
Ownership
Proactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Delhi, Hyderabad",Software,"Key Responsibilities:
Design, develop, and maintain data pipelines using Python, SQL, and Kedro
Implement serverless solutions using AWS Lambda and Step Functions
Develop and manage data workflows in Azure and AWS cloud environments
Create integrations between data systems and Power Platform (Power Apps, Power Automate)
Design, develop, and maintain APIs for data exchange and integration
Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
Optimize data storage and retrieval processes for improved performance
Collaborate with cross-functional teams to understand data requirements and provide solutions
API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer with BI and DWH - Ikrux,Ikrux Solutions (Opc) Private Limited,15-16 Years,,"Bengaluru, Pune",Information Technology,"Job description
Data Analytics and AIQuality AssuranceArtificial IntelligenceApplication Development and Software EngineeringCloud EngineeringCapability BuildingDevopsCRMSynapse-360
Need different solutions
Ikrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.
industries Overview
Banking, financial services and insurance (BFSI)FintechMediaGlobal Capability CentersTechnologyHealthcareE-CommerceOil and Gas
Need different solutions
Ikrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.
Job Category:AWS GlueAzureBusiness IntelligenceData EngineerData IngestionData ModellingData WarehousingPower BISSISTableau
Job Type:Full Time
Job Location:BangalorePune
We are seeking a highly experienced BI Architect and Developer with 6-15+ years of expertise in business intelligence, data visualization, and data engineering. The ideal candidate should hold a Bachelor s degree in Computer Science, Information Systems, Data Science, or a related field. Proficiency in Power BI and Tableau is essential, along with a strong background in ETL processes and tools such as SSIS. Advanced knowledge of SQL Server for query writing and database management is required, complemented by skills in exploratory data analysis using Python and familiarity with the CRISP-DM model.
The candidate should have experience working with diverse data models and databases, including Snowflake, Postgres, Redshift, and MongoDB. Expertise in visualization tools like Power BI, QuickSight, Plotly, and Dash is necessary. A strong programming foundation in Python is essential, particularly in data manipulation and analysis using Pandas, NumPy, and PySpark, as well as data serialization with JSON, CSV, and Parquet. Experience with cloud services like AWS S3, AWS Lambda, and Azure SDK is a plus. Proficiency in workflow orchestration tools such as Airflow and automation of ETL pipelines is required. Additional skills in interacting with REST APIs, web scraping, and version control for collaborative projects will be an advantage.
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate
PG:Any Postgraduate","Workflow, Application Development, Json, Automation, Quality Assurance"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Mumbai, Pune",Software,"Key Responsibilities:
Design, develop, and maintain data pipelines using Python, SQL, and Kedro
Implement serverless solutions using AWS Lambda and Step Functions
Develop and manage data workflows in Azure and AWS cloud environments
Create integrations between data systems and Power Platform (Power Apps, Power Automate)
Design, develop, and maintain APIs for data exchange and integration
Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
Optimize data storage and retrieval processes for improved performance
Collaborate with cross-functional teams to understand data requirements and provide solutions
API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Bengaluru, Chennai",Software,"Key Responsibilities:
Design, develop, and maintain data pipelines using Python, SQL, and Kedro
Implement serverless solutions using AWS Lambda and Step Functions
Develop and manage data workflows in Azure and AWS cloud environments
Create integrations between data systems and Power Platform (Power Apps, Power Automate)
Design, develop, and maintain APIs for data exchange and integration
Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).
Optimize data storage and retrieval processes for improved performance
Collaborate with cross-functional teams to understand data requirements and provide solutions
API Integration and data extraction from Sharepoint
Required Skills and Experience:
Expert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.
Good Knowledge on integration like Integration using API, XML with ADF, Logic App,
Azure Functions, AWS Step functions, AWS Lambda Functions, etc
Strong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.
Having knowledge and experience in Scala, Java and R would be a good to have skill.
Experience with Kedro framework for data engineering pipelines
Expertise in AWS services, particularly Lambda and Step Functions
Proven experience with Power Platform (Power Apps, Power Automate) and Dataverse.
Strong understanding of API development and integration
Experience in database design and management
Excellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer,Saxon Global INC,4-9 Years,,Bengaluru,"Information Technology, Software, Information Services","Job description
Contributes to the design of information infrastructure, and data management processes
Develop good understanding of how data will flow & stored through an organization across multiple applications such as CRM, Broker & Sales tools, Finance, HR etc
Required Candidate profile
Min 3 yrs using Azure(preferred), Python, Kafka, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc.
should have L2 Support, L3 Support.","Data Validation, Azure, python, Data Management, Kafka, Data Engineer"
Lead Engineer (Data Engineer),Velotio Technologies,7-10 Years,,Bengaluru,Software,"ob description
Design, develop, and maintain robust and scalable data pipelines that ingest, transform, and load data from various sources into data warehouse.
Collaborate with business stakeholders to understand data requirements and translate them into technical solutions.
Implement data quality checks and monitoring to ensure data accuracy and integrity.
Optimize data pipelines for performance and efficiency.
Troubleshoot and resolve data pipeline issues.
Stay up-to-date with emerging technologies and trends in data engineering.
Qualifications
Bachelor s or Master s degree in Computer Science, Engineering, or a related field.
7+ years of experience in data engineering or a similar role.
Strong proficiency in SQL and at least one programming language (e.g., Python, Java).
Experience with data pipeline tools and frameworks
Experience with cloud-based data warehousing solutions (Snowflake).
Experience with AWS Kinesis, SNS, SQS
Excellent problem-solving and analytical skills.
Strong communication and interpersonal skills.
Desired Skills & Experience:
Data pipeline architecture
Data warehousing
ETL (Extract, Transform, Load)
Data modeling
SQL
Python or Java or Go
Cloud computing
Business intelligence","Business Intelligence, Java, Cloud Computing, Sql, Python"
Lead/Staff Data Engineer,Tableau Software,8-13 Years,,Hyderabad,Information Technology,"About the Role:
Data Engineering at the Heart of AI Marketing
Join theMarketing AI/ML Algorithms and Applicationsteam a powerhouse team within Salesforce s marketing organization, directly influencing how wemarket our vast product portfolio to a global customer base (including 90% of the Fortune 500).
In this role, you'll:
Drive state-of-the-art ML solutionsthat supercharge our internal marketing platforms
Help shapecustomer engagement strategiesat massive scale
Work on AI/ML-powered personalization, propensity modeling, recommender systems & more
Bring yourdata engineering expertiseto fuel real business impact at global scale
What You'll Do
Architect and implement scalable data platforms & pipelinesfor ML model development and production
Build and manage Feature Storesto enable predictive modeling and analytics
Designreal-time and batch ETL systems, integrating complex customer data from multiple sources
Collaborate withAnalytics, Data Science, and Marketing teamsto deliver actionable insights
Create robust data infrastructure formodel deployment, monitoring & tuning
Ensuredata quality, governance, and compliance, aligned with Salesforces AI Trust framework
Lead and mentor other engineers, promotinginnovation and continuous learning
What You'll Need
8+ yearsof hands-on experience building enterprise-scale data platforms in the cloud
Expert-level skills inSQL, Python, PySpark/Spark, Hive, Presto, Kafka
Experience withcloud platforms (AWS, GCP, Azure)and tools likeDBT, Airflow, Snowflake, Redshift
Strong grasp ofdata modeling, pipelines, and large-scale data architecture
Familiarity withAI/ML model pipelines, CDPs, and Customer 360 initiatives
Passion for enablingdata-driven decision-makingin aB2B marketing context
Bonus: Familiarity withSalesforce productsor CRM data
Why This Role Stands Out
DriveAI-powered marketingat the #1 CRM company in the world
Collaborate with some of the bestData Scientists and Marketing experts
Influence how Salesforceconnects with millions of global customers
Be part of a team that s pioneering the future ofAgentic AI + Marketing
Own your impactnot just as an engineer, but as a leader and innovator
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via thisAccommodations Request Form .","Salesforce, Business Strategy, CRM, Analytics, Monitoring, Predictive Modeling, Data Modeling, Gcp, Sql, Python"
Module Lead Data Engineer/GCP Module Lead Database Engineer,Impetus Technologies,3-6 Years,,"Noida, Indore",Software,"We are looking for GCP Data Engineer and SQL Programmer with good working experience on PostgreSQL, PL/SQL programming experience and following technical skills
PL/SQL and PostgreSQL programming - Ability to write complex SQL Queries, Stored Procedures.
Migration - Working experience in migrating Database structure and data from Oracle to Postgres SQL preferably on GCP Alloy DB or Cloud SQL
Working experience on Cloud SQL/Alloy DB
Working experience to tune autovacuum in postgresql.
Working experience on tuning Alloy DB / PostgreSQL for better performance.
Working experience on Big Query, Fire Store, Memory Store, Spanner and bare metal setup for PostgreSQL
Ability to tune the Alloy DB / Cloud SQL database for better performance
Experience on GCP Data migration service
Working experience on MongoDB
Working experience on Cloud Dataflow
Working experience on Database Disaster Recovery
Working experience on Database Job scheduling
Working experience on Database logging techniques
Knowledge of OLTP And OLAP
Desirable: GCP Database Engineer Certification
Other Skills:-
Out of the Box Thinking
Problem Solving Skills
Ability to make tech choices (build v/s buy)
Performance management (profiling, benchmarking, testing, fixing)
Enterprise Architecture
Project management/Delivery Capabilty/ Quality Mindset
Scope management
Plan (phasing, critical path, risk identification)
Schedule management / Estimations
Leadership skills
Other Soft Skills
Learning ability
Innovative / Initiative
Develop, construct, test, and maintain data architectures
Migrate Enterprise Oracle database from On Prem to GCP cloud autovacuum in postgresql
Ability to tune autovacuum in postgresql.
Working on tuning Alloy DB / PostgreSQL for better performance.
Performance Tuning of PostgreSQL stored procedure code and queries
Converting Oracle stored procedure queries to PostgreSQL stored procedures Queries
Creating Hybrid data store with Datawarehouse and No SQL GCP solutions along with PostgreSQL.
Migrate Oracle Table data from Oracle to Alloy DB
Leading the database team
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:Any Graduate
PG:Any Postgraduate","Project Management, Stored Procedures, Disaster Recovery, Scheduling, Data Warehousing, Sql, Data Migration, OLAP, Oracle, Performance Tuning"
"Sr. Associate Technical Consultant, Data Engineer",AHEAD,3-5 Years,,Gurugram,Information Technology,"Data Engineer
(Internally known as a Sr. Associate Technical Consultant)
AHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.
The Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.
Responsibilities
A Data Engineer should be able to build, operationalize and monitor data processing systems
Create robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Implement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases
Engineers and supports data structures including but not limited to SQL and NoSQL databases
Engineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)
Leverages the right tools for the right job to deliver testable, maintainable, and modern data solutions
Respond to customer/team inquiries and assist in troubleshooting and resolving challenges
Works with other scrum team members to estimate and deliver work inside of a sprint
Research data questions, identifies root causes, and interacts closely with business users and technical resources
Qualifications
3+ years of professional technical experience
3+ years of hands-on Data Warehousing
3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake
2+ years of programming languages such as Python
3+ years of experience working in cloud environments (Azure)
2 years of experience in Redshift
Strong client-facing communication and facilitation skills
Key Skills
Python, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP
Why AHEAD
Through our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.
We fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.
USA Employment Benefits include
Medical, Dental, and Vision Insurance
401(k)
Paid company holidays
Paid time off
Paid parental and caregiver leave
Plus more! See benefits https://www.aheadbenefits.com/ for additional details.
The compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Azure, python, Spark, Redshift, hadoop"
"Technical Consultant, Data Engineer",AHEAD,3-5 Years,,Gurugram,Information Technology,"Data Engineer (Internally known as a Technical Consultant)
AHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.
The Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.
Responsibilities:
A Data Engineer should be able to build, operationalize and monitor data processing systems
Create robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Implement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases
Engineers and supports data structures including but not limited to SQL and NoSQL databases
Engineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)
Leverages the right tools for the right job to deliver testable, maintainable, and modern data solutions
Respond to customer/team inquiries and assist in troubleshooting and resolving challenges
Works with other scrum team members to estimate and deliver work inside of a sprint
Research data questions, identifies root causes, and interacts closely with business users and technical resources
Qualifications:
3+ years of professional technical experience
3+ years of hands-on Data Warehousing
3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake
2+ years of programming languages such as Python
3+ years of experience working in cloud environments (Azure)
2 years of experience in Redshift
Strong client-facing communication and facilitation skills
Key Skills:
Python, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP
Why AHEAD:
Through our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.
We fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.
USA Employment Benefits include
Medical, Dental, and Vision Insurance
401(k)
Paid company holidays
Paid time off
Paid parental and caregiver leave
Plus more! See benefits https://www.aheadbenefits.com/ for additional details.
The compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Nosql, Hadoop, Spark, Data Warehouse, Python, Sql"
Big Data Engineer - Xebia,Foray Software,3-6 Years,,Bengaluru,Consulting,"Implementation experience on building large scale data applications from scratch (initial stages)
Programming experience in Python or Java
Good experience of deploying applications on AWS and usage of its services
Must have experience with Hadoop Distributed File System (HDFS), Amazon Simple Storage Service (S3)
Must have good experience on SQL
Data organization in Data Lake (experience in Delta Lake or Databricks is added advantage)
Detailed understanding of Data pipeline creation
Detailed experience of Data ingestion
Techno functional experience working with technical team (data engineering / data science) and the business (functional) teams","Delta, Deployment, HDFS, Techno-functional, Hadoop, Data Science, Big Data, Sql, Python, AWS"
Data Engineer OpenData Commercial,Veeva Systems,3-7 Years,,Mumbai,Cloud Data Services,"We are seeking a skilled Data Engineer to design and build reusable and configurable data pipelines using a mix of standard cloud-based and proprietary data platforms. This role involves collaborating with data scientists to develop AI/ML proofs of concept into full implementations, creating documentation, and providing informal training to data analysts. You will also work closely with other engineering and product teams and provide guidance to more junior data engineers.
What You'll Do:
Design and build reusable and configurable data pipelines using a mix of standard cloud-based and proprietary data platforms.
Work with a data scientist to develop AI/ML proofs of concept into full implementations.
Create documentation and provide informal training for data analysts on the configuration and use of tools and pipelines.
Work with other engineering and product teams to understand proprietary platforms and provide input and feedback.
Provide guidance to more junior data engineers on best practices.
Work with security teams to ensure that all servers, platforms, and other resources meet security requirements.
Requirements:
BS degree in Computer Science, Engineering, or a related subject.
4+ years of experience in Data Engineering roles.
Experience developing sophisticated data pipelines in cloud-based environments (e.g., AWS) using scalable data processing tools (e.g., Apache Spark).
Data modeling experience.
Demonstrated ability to work with others, particularly providing guidance to other data engineers.
Ability to communicate around complex ideas and topics in English with both technical and non-technical individuals.
Nice to Have:
Familiarity with Agile methodologies.
DevOps skills, especially CI/CD experience.
Configuring and maintaining cloud-based cluster computing resources and orchestration systems (e.g., EC2 instances, Kubernetes clusters, Elastic Beanstalk).","Data Pipelines, CI/CD, data engineering, Data Modeling, Apache Spark, AWS"
"Technical Consultant, Data Engineer",AHEAD,3-5 Years,,Gurugram,Information Technology,"Data Engineer (Internally known as a Technical Consultant)
AHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.
The Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.
Responsibilities:
A Data Engineer should be able to build, operationalize and monitor data processing systems
Create robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset
Implement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases
Engineers and supports data structures including but not limited to SQL and NoSQL databases
Engineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)
Leverages the right tools for the right job to deliver testable, maintainable, and modern data solutions
Respond to customer/team inquiries and assist in troubleshooting and resolving challenges
Works with other scrum team members to estimate and deliver work inside of a sprint
Research data questions, identifies root causes, and interacts closely with business users and technical resources
Qualifications:
3+ years of professional technical experience
3+ years of hands-on Data Warehousing
3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake
2+ years of programming languages such as Python
3+ years of experience working in cloud environments (Azure)
2 years of experience in Redshift
Strong client-facing communication and facilitation skills
Key Skills:
Python, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP
Why AHEAD:
Through our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.
We fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.
USA Employment Benefits include
Medical, Dental, and Vision Insurance
401(k)
Paid company holidays
Paid time off
Paid parental and caregiver leave
Plus more! See benefits https://www.aheadbenefits.com/ for additional details.
The compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Nosql, Hadoop, Spark, Data Warehouse, Python, Sql"
Module Lead Data Engineer/GCP Module Lead Database Engineer,Impetus Technologies,3-6 Years,,"Noida, Indore",Software,"We are looking for GCP Data Engineer and SQL Programmer with good working experience on PostgreSQL, PL/SQL programming experience and following technical skills
PL/SQL and PostgreSQL programming - Ability to write complex SQL Queries, Stored Procedures.
Migration - Working experience in migrating Database structure and data from Oracle to Postgres SQL preferably on GCP Alloy DB or Cloud SQL
Working experience on Cloud SQL/Alloy DB
Working experience to tune autovacuum in postgresql.
Working experience on tuning Alloy DB / PostgreSQL for better performance.
Working experience on Big Query, Fire Store, Memory Store, Spanner and bare metal setup for PostgreSQL
Ability to tune the Alloy DB / Cloud SQL database for better performance
Experience on GCP Data migration service
Working experience on MongoDB
Working experience on Cloud Dataflow
Working experience on Database Disaster Recovery
Working experience on Database Job scheduling
Working experience on Database logging techniques
Knowledge of OLTP And OLAP
Desirable: GCP Database Engineer Certification
Other Skills:-
Out of the Box Thinking
Problem Solving Skills
Ability to make tech choices (build v/s buy)
Performance management (profiling, benchmarking, testing, fixing)
Enterprise Architecture
Project management/Delivery Capabilty/ Quality Mindset
Scope management
Plan (phasing, critical path, risk identification)
Schedule management / Estimations
Leadership skills
Other Soft Skills
Learning ability
Innovative / Initiative
Develop, construct, test, and maintain data architectures
Migrate Enterprise Oracle database from On Prem to GCP cloud autovacuum in postgresql
Ability to tune autovacuum in postgresql.
Working on tuning Alloy DB / PostgreSQL for better performance.
Performance Tuning of PostgreSQL stored procedure code and queries
Converting Oracle stored procedure queries to PostgreSQL stored procedures Queries
Creating Hybrid data store with Datawarehouse and No SQL GCP solutions along with PostgreSQL.
Migrate Oracle Table data from Oracle to Alloy DB
Leading the database team
Role:Data Engineer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:Any Graduate
PG:Any Postgraduate","Project Management, Stored Procedures, Disaster Recovery, Scheduling, Data Warehousing, Sql, Data Migration, OLAP, Oracle, Performance Tuning"
Big Data Engineer - Xebia,Foray Software,3-6 Years,,Gurugram,Consulting,"Implementation experience on building large scale data applications from scratch (initial stages)
Programming experience in Python or Java
Good experience of deploying applications on AWS and usage of its services
Must have experience with Hadoop Distributed File System (HDFS), Amazon Simple Storage Service (S3)
Must have good experience on SQL
Data organization in Data Lake (experience in Delta Lake or Databricks is added advantage)
Detailed understanding of Data pipeline creation
Detailed experience of Data ingestion
Techno functional experience working with technical team (data engineering / data science) and the business (functional) teams","Delta, Deployment, HDFS, Techno-functional, Hadoop, Data Science, Big Data, Sql, Python, AWS"
"DATA ENGINEER - AWS , PYTHON , TERRAFORM",Augusta Infotech,6-11 Years,,Gurugram,"Consulting, Information Services","The Technical Specialist Data Services is responsible for working on data pipelines and engineering project delivery and / or production support activities from a technical perspective. This role would typically include day to day activities creating and supporting data pipelines, deployments including solving production and pre-production issues, working on change, incidents, problems and service improvement or engineering initiatives.
This role demands strong technical skills with a focus to ensure maximum uptime of the production infrastructure. Working as a member of a global database support team the successful candidate will play a pro-active role in the support of Emerging Services estate. The role requires Expert level skills in AWS/Azure Cloud devops which includes working on Terraform, Lambda (Python and Java) , Building micro services and CICD pipelines. Were looking for a candidate who can support Technologies like NiFi , Spark and Kafka. Working knowledge on Database, Oracle Golden Gate will be an advantage.
The role will involve regular BAU (Business As Usual) support and will involve 24 x 7 On-Call support and weekend shift working.
Key Responsibilities
The key responsibilities of this role are:
Design and develop secure data pipelines.
24x7 support for Production Emerging Data Services estate. This includes all Production, Disaster Recovery, Emergency Break Fix, User Acceptance Testing and Development environments.
Strategize data migration from Data Centre to Cloud includes Batch and CDC migration.
Automation and process improvement.
Rapid response to address high severity (severity 1, 2 and 3H) incidents.
Support and delivery of assigned project work.
Pro-active issue detection and resolution as well as service monitoring.
Off business hours support on rotation basis.
Collaborate with other Business and Technology teams.
Experience and Qualifications Required
Excellent Cloud Devops Engineer who can work and support CICD Pipeline using Terraform, cloud formation.
Working experience on Lambda, microservices, API.
Data migration using NiFi, Spark, Kafka and Oracle Golden-Gate from on-prem to Cloud.
Working experience with Snowflake will be an addon.
Support of engineering pipelines, monitoring and housekeeping, upgrade and patching, troubleshooting, root cause analysis and performance tuning.
Proficient in ITIL processes.
Interpersonal Skills
A good team player.
Good written and verbal communication skills, together with the ability to communicate with management and other business and technical groups.
Good analytical and, problem solving skills.
Ability to adapt to changing business needs (flexible) and learning new skills quickly.
Customer focused strong service ethic.
Ability to work on own initiative with minimal direction.
Technical skills
Above 6 Years of progressive working knowledge as Devops data engineer with technologies including but not restricted to:
NiFi
Kafka
Big Data - Spark
Hands-on experience on Cloud (AWS/Azure)
Automation/Scripting expertise in Terraform, Lambda (Python/Java), Cloud formation
Performance tuning with Data Pipelines
Architecting Data migration Solution (Nice to Have)
Basic knowledge of Snowflake, Oracle, SQL Server (Nice to have)","Python, Sql, Terraform, Data Warehousing, AWS"
Big Data Engineer - Xebia,Foray Software,3-6 Years,,Bengaluru,Consulting,"Implementation experience on building large scale data applications from scratch (initial stages)
Programming experience in Python or Java
Good experience of deploying applications on AWS and usage of its services
Must have experience with Hadoop Distributed File System (HDFS), Amazon Simple Storage Service (S3)
Must have good experience on SQL
Data organization in Data Lake (experience in Delta Lake or Databricks is added advantage)
Detailed understanding of Data pipeline creation
Detailed experience of Data ingestion
Techno functional experience working with technical team (data engineering / data science) and the business (functional) teams","Delta, Deployment, HDFS, Techno-functional, Hadoop, Data Science, Big Data, Sql, Python, AWS"
Associate Data Engineer- GET,XenonStack,0-1 Years,,Chennai,Information Technology,"Job Responsibilities -
Develop, construct, test and maintain Data Platform Architectures
Align Data Architecture with business requirements
Liaising with coworkers and clients to elucidate the requirements for each task.
Scalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analyzed quickly by BI AI Teams .
Reformulating existing frameworks to optimize their functioning.
Transforming Raw Data into InSights for manipulation by Data Scientists.
Ensuring that your work remains backed up and readily accessible to relevant coworkers.
Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.
Technical Requirements-
Experience of Python, Java/Scala
Great Statistical / SQL based Analytical Skills
Experience of Data Analytics Architectural Design Patterns for Batch , Event Driven and Real-Time Analytics Use Cases
Understanding of Data warehousing, ETL tools, machine learning, Data EPIs
Excellent in Algorithms and Data Systems
Understanding of Distributed System for Data Processing and Analytics
Familiarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores .
Professional Attributes-
Excellent communication skills
Attention to detail
Analytical mind and Problem Solving Aptitude
Strong Organizational skills","Consulting, Analytical, Architectural Design, Machine Learning, Big Data Analytics"
GCP Data Engineer,SP Staffing Services Private Limited,6-12 Years,,"Hyderabad, Bengaluru, Pune",Software,"We are seeking an experienced GCP Data Engineer proficient in data processing frameworks, programming languages, and GCP services. The ideal candidate will have hands-on experience with cloud-based solutions, particularly GCP services such as BigQuery, Dataflow, and Spanner. Strong problem-solving abilities, a solid understanding of data security, and expertise in ETL processes are essential.
Key Responsibilities:
Develop and implement data processing solutions using frameworks such as Apache Beam (Data Flow) and Kafka.
Work with GCP services like BigQuery, Dataflow, and Spanner to optimize data pipelines and workflows.
Design and model data structures for efficient data storage and retrieval.
Oversee ETL processes to ensure seamless data extraction, transformation, and loading.
Collaborate with cross-functional teams to address data engineering challenges and improve data systems.
Ensure scalability and security of data infrastructure while maintaining high performance.
Leverage Apache Airflow or similar tools to manage workflows and pipelines.
Key Requirements:
Proficiency in programming languages like Python, Java, or Scala.
Expertise in Apache Beam, Kafka, and GCP services such as BigQuery, Dataflow, and Spanner.
Experience in data modeling, database design, and ETL processes.
Strong problem-solving abilities and the ability to manage complex data engineering tasks.
Familiarity with cloud storage solutions and data security best practices.
Understanding of scalability principles in cloud-based data engineering environments.","Data Engineer, Etl, Gcp"
GCP Data Engineer!,TechnoGen,5-10 Years,,Hyderabad,"Consulting, Information Services","Description
We are seeking a skilled GCP Data Engineer to join our dynamic team in India. The ideal candidate will have extensive experience in designing, building, and maintaining data pipelines and data processing systems on Google Cloud Platform. The role involves collaborating with cross-functional teams to ensure that our data infrastructure meets the needs of the organization.
Responsibilities
Design and implement data processing systems on Google Cloud Platform (GCP).
Develop and maintain scalable data pipelines using tools such as Dataflow, Dataproc, and BigQuery.
Collaborate with data scientists and analysts to understand data requirements and ensure data availability.
Monitor and optimize data pipelines for performance and cost efficiency.
Ensure data quality and compliance with data governance standards.
Implement security best practices for data storage and access.
Skills and Qualifications
5-10 years of experience in data engineering or related field.
Strong proficiency in Google Cloud Platform services, particularly BigQuery, Dataflow, and Cloud Storage.
Experience with SQL and NoSQL databases, including data modeling and ETL processes.
Proficiency in programming languages such as Python, Java, or Go.
Familiarity with data warehousing concepts and data architecture principles.
Knowledge of data visualization tools and techniques.
Ability to work collaboratively in a team environment and communicate effectively with stakeholders.","ETL Processes, Cloud Storage, BigQuery, Data Modeling, Sql, Python, data engineering"
GCP Data Engineer,Nihilent,5-7 Years,,Chennai,Information Technology,"Unnesting of JSON files in Google BigQuery
Google Cloud Run (using container services, python)
Hands on experience in GCP services like composer, cloud function, cloud run
Experience in developing CI/CD pipelines.
Experience in DBT scripts, docker files and knowledge on datavault is a plus.
Strong technical knowledge and hands on experience of python or java
Role: Data Engineer
Industry Type: IT Services & Consulting
Department: Data Science & Analytics
Employment Type: Full Time, Permanent
Role Category: Data Science & Machine Learning
Education
UG: Any Graduate
PG: Any Postgraduate","Gcp, Cloud, Json, Python"
GCP Data Engineer,Impetus Technologies,4-7 Years,,Bengaluru,Software,"We need GCP engineers for capacity building;
- The candidate should have extensive production experience (1-2 Years ) in GCP, Other cloud experience would be a strong bonus.
- Strong background in Data engineering 2-3 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
- Exposure to enterprise application development is a must
Roles and Responsibilities
4-7 years of IT experience range is preferred.
Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Big Query, GCS- At least 4 of these Services.
Good to have knowledge on Cloud Composer, Cloud SQL, Big Table, Cloud Function.
Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOPs.
Good hands on expertise on either Python or Java programming.
Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
Experience with technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
Technical ability to become certified in required GCP technical certifications.
Role:Data Engineer
Industry Type:Software Product
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Any Specialization
PG:M.Com in Any Specialization, MCA in Any Specialization, M.Tech in Any Specialization","CloudGen, Engine, Java, Devops, Hive, Gcp, Pyspark, Iam, Spark, Big Data Technologies, Sql"
IT - Data Engineer _ AWS,Systechcorp Inc,2-6 Years,,"Delhi, Hyderabad",Software,"Work with practice SMEs to understand the client s challenges and how Genpact solves for their challenges
Understand our right to play and value articulation
Liaison with practice team to drive product/offering messaging by translating technical nuances to strong client messages
Productize the offering with the right value articulation to make it client ready
Ensure every offering has all the GTM readiness collaterals for client readiness
Establish strong relationship with practice leaders to be their trusted advisor for offerings
Ability to quickly research industry and competitor s offerings and ability to incorporate it in our value messaging
Work with Knowledge Management team to drive strategic placement of the offerings in our infrastructure","IT, data engineering, Cloud, Aws"
Azure Data Engineer,Trigent Software Private Limited,5-10 Years,,"Delhi, Bengaluru, Chennai",Software,"Detailed JD *(Roles and Responsibilities)
Experience in Databricks, PL/SQL, PySpark, Python, and Azure Data Factory (ADF).
Experience in designing, developing, and maintaining data pipelines and data streams.
Experience in moving/transforming data across layers (Bronze, Silver, Gold) using ADF, Python, and PySpark.
Experience in working with stakeholders to understand their data needs and provide solutions.
Experience in collaborating with other teams to ensure data quality and consistency.
Experience in developing and maintaining data models and data dictionaries.
Optimize ETL processes for performance and scalability.
Experience in developing and maintaining data integrity and accuracy
Experience in developing and maintaining data governance policies and procedures.
Experience in developing and maintaining data security policies and procedures.
Good understanding of deployment processes
Ability to manage customer handling
Flexible for support and maintenance type of project
Mandatory skills*
Databricks, PySpark, Python, Azure Data Factory (ADF)","Azure Data Factory, Pyspark, Databricks, Python, Etl"
Big Data Engineer,Coditas Technologies,2-5 Years,,Pune,Software,"We are looking for people who have the right attitude, aptitude, skills, empathy, compassion, and hunger for learning.Built products in the data analytics space, either frontend/backend/cloud. A passion for shipping high-quality products, interest in the data products space, curiosity about the bigger picture of building a company, product development, people, and product
Roles and Responsibilities
We are looking for a savvy Data Engineering professional to join the newly formed Data Engineering team
We are looking for Big Data specialists who have proven skills on working large scale data systems
The hire will be responsible for building and optimizing data pipeline architectures, as well as optimizing data flow and collection for multiple source systems
The ideal candidate should be experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up
Have got strong ability to build robust and resilient data pipelines which are fault tolerant and reliable in terms of data movement
Should have knowledge of experience on batch and stream data processing
Create end to end data products and productionize them in cloud/in-house servers
Technical Skills
Minimum 2-4 years of progressive experience building solutions in Big Data environments
Should have solid hands on experience on Big Data technologies like Hadoop, HBase, Hive, Pig, Oozie, MapReduce, Yarn, HDFS, Zookeeper
Hands on knowledge of Apache Spark with Java/Scala for batch and stream processing will be highly preferred
Knowledge of Apache Kafka will be added advantage but not mandatory
Strong hands on capabilities on SQL and NoSQL technologies
Should be able to build performant, fault tolerant, scalable solutions
Excellent written and verbal communication skills
What is in it for you
A clear career path with a hybrid of technical track and management track to grow in the company
In the technical track, the candidate will be developing sophisticated actuarial and analytics skill
Communicating with clients and manage the track, the candidate will develop skills sets in business strategy and build strong tech products
You should have
Excellent problem solving and analytical skills
Fluency in written and communication skills in English
Good time-management skills
Should be a quick learner with ability to learn new tools/technology quickly
Good analytical and problem-solving skills
Intellectual and analytical curiosity
Expertise in data analytics and providing data driven insights that help in taking business decisions
The traits of a self-motivated, independent and detail-oriented team player
Role:Back End Developer
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:Any Graduate
PG:Any Postgraduate","Time Management, Backend, Analytical, Business Strategy, Data Processing, Actuarial, Big Data, Data Analytics, Apache, Sql"
Data Engineer - Insights & Analytics,Philips,10-12 Years,,"Bengaluru, India",Hospitals/Healthcare/Diagnostics,"Job Description
Job title:
Data Engineer - Insights & Analytics
Your role:
Engage with the Enterprise Informatics leadership team to understand their challenges and translate them into actionable analytics solutions.
Perform advanced analytics, data mapping, and reporting to support initiatives and commercial operations projects.
Ensure timely delivery of high-quality analysis and reports.
Stay updated on the latest insights and analytics developments implement machine learning and AI solutions in key use cases.
Analyze multiple business flows and data models, ensuring the application of relevant analytics methodologies and standards.
Develop and support Insights & Analytics solutions, including maintaining existing EI CommOps business-owned reports.
You're the right fit if:
Your have proven technical skills in PowerBI, Azure DataBricks, Advanced Excel + Macros experience with Salesforce and SAP is preferred.
You should have successfully integrated and deployed AI technologies, including machine learning models and Large Language Models (LLMs), into production environments, demonstrating significant business impact.
You are proficient in one or two key comercial data domains (e.g., OIT, Sales, Services, SSOP, Pricing,...)
You should possess a solid foundation in Python, which is crucial for developing and optimizing analytics solutions in our projects.
You have at least 10 years of relevant experience in roles such as Business Analytics Specialist, Data Analyst, or Business Process Expert, with a strong understanding of commercial processes.
You have expertise in designing, building, and maintaining scalable ETL pipelines to support data transformation and structuring for business intelligence.
You should exhibit a problem-solving mindset with experience in root cause analysis and lean principles.
You are eager to work with global teams.
You demonstrate excellent analytical thinking and the ability to interpret large volumes of information, synthesise insights, and effectively communicate analyses and proposals to various audiences, including senior management.
You hold a Bacheloru2019s degree in Computer Science, Information Management, Data Science, Statistics, or a related field a Masteru2019s degree is preferred.
How we work together
We believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.
Onsite roles require full-time presence in the companyu2019s facilities.
Field roles are most effectively done outside of the companyu2019s main facilities, generally at the customersu2019 or suppliersu2019 locations.
Indicate if this role is an office/field/onsite role.
About Philips
We are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.
u2022 Learn more about .
u2022 Discover .
u2022 Learn more about .
If youu2019re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion .
#LI-EU #LI-Hybrid #LI-PHILIN","Salesforce, AI technologies, ETL pipelines, machine learning models, SAP, Macros, Powerbi, Azure Databricks, Advanced Excel, Python"
GCP Data Engineer,Impetus Technologies,4-7 Years,,Bengaluru,Software,"We need GCP engineers for capacity building;
- The candidate should have extensive production experience (1-2 Years ) in GCP, Other cloud experience would be a strong bonus.
- Strong background in Data engineering 2-3 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
- Exposure to enterprise application development is a must
Roles and Responsibilities
4-7 years of IT experience range is preferred.
Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Big Query, GCS- At least 4 of these Services.
Good to have knowledge on Cloud Composer, Cloud SQL, Big Table, Cloud Function.
Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOPs.
Good hands on expertise on either Python or Java programming.
Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
Experience with technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
Technical ability to become certified in required GCP technical certifications.
Role:Data Engineer
Industry Type:Software Product
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:Software Development
Education
UG:B.Tech/B.E. in Any Specialization
PG:M.Com in Any Specialization, MCA in Any Specialization, M.Tech in Any Specialization","CloudGen, Engine, Java, Devops, Hive, Gcp, Pyspark, Iam, Spark, Big Data Technologies, Sql"
GCP Data Engineer!,TechnoGen,5-10 Years,,Hyderabad,"Consulting, Information Services","Description
We are seeking a skilled GCP Data Engineer to join our dynamic team in India. The ideal candidate will have extensive experience in designing, building, and maintaining data pipelines and data processing systems on Google Cloud Platform. The role involves collaborating with cross-functional teams to ensure that our data infrastructure meets the needs of the organization.
Responsibilities
Design and implement data processing systems on Google Cloud Platform (GCP).
Develop and maintain scalable data pipelines using tools such as Dataflow, Dataproc, and BigQuery.
Collaborate with data scientists and analysts to understand data requirements and ensure data availability.
Monitor and optimize data pipelines for performance and cost efficiency.
Ensure data quality and compliance with data governance standards.
Implement security best practices for data storage and access.
Skills and Qualifications
5-10 years of experience in data engineering or related field.
Strong proficiency in Google Cloud Platform services, particularly BigQuery, Dataflow, and Cloud Storage.
Experience with SQL and NoSQL databases, including data modeling and ETL processes.
Proficiency in programming languages such as Python, Java, or Go.
Familiarity with data warehousing concepts and data architecture principles.
Knowledge of data visualization tools and techniques.
Ability to work collaboratively in a team environment and communicate effectively with stakeholders.","ETL Processes, Cloud Storage, BigQuery, Data Modeling, Sql, Python, data engineering"
Data Engineer,Ford Motor Company,4-6 Years,,"Chennai, India",Automotive/Automobile/Ancillaries,"JOB DESCRIPTION
This role requires a combination of software engineer, data enginner and an ML developer where you're expected to build solutions integrating data pipelines & ML models. It also requires you to have basic knowledge and (preferably) hands-on experience on building back-end services(APIs) which integrate data & applications.
RESPONSIBILITIES
Key Responsibilities:
Data Pipeline Development:Design, develop, and maintain scalable and reliable batch data pipelines using Python and Java, leveraging industry standard frameworks like Data proc and Dataflow.
ELT Implementation:Implement efficient data extraction, transformation, and loading processes to move data from various sources into data warehouses, data lakes, or other storage systems.
Backend Development:Develop and maintain backend services (potentially in Python or Java) that interact with data stores, trigger pipelines, and/or serve data via APIs.
Data Storage Interaction:Work with various data storage technologies, including relational databases (SQL), data lakes (GCS, Big Query), and data warehouses (BigQuery).
Performance Optimization:Analyze and optimize the performance of data pipelines and related services to ensure efficiency and cost-effectiveness when dealing with large datasets.
Monitoring and Reliability:Implement monitoring, logging, and alerting for data pipelines and services to ensure their health, reliability, and data quality. Troubleshoot and resolve production issues.
Collaboration:Collaborate effectively with Product Owners, Data Scientists, Data Engineers, MLOps Engineers, and other engineering teams to understand requirements and deliver integrated solutions.
Code Quality & Testing:Write clean, well-tested, and maintainable code. Participate in code reviews.
Technical Contribution:Contribute to architectural discussions and help define technical approaches for data and AI-related projects.
AI/ML Integration (Additional responsibility):Work closely with Data Scientists to operationalize machine learning models. This includes building the infrastructure and code (incl. chatbots) to integrate models into data pipelines or backend services for training data preparation, inference, or prediction serving.
Operational support: Handle tickets (incidents/requests) for data pipelines/chatbot applications & work with product owners/business customers to track the tickets to closure within pre-defined SLAs.
QUALIFICATIONS
Required Skills and Qualifications:
4+ years of professional experience in software development.
Strong proficiency and hands-on experience in both Python(Must-have) and Java(Nice to have).
Experience building and maintaining data pipelines (batch or streaming) preferably on Cloud platforms(especially GCP).
Experience with at least one major distributed data processing framework (e.g., DBT, DataForm, Apache Spark, Apache Flink, or similar).
Experience with workflow orchestration tools (e.g., Apache Airflow, Qlik replicate etc).
Experience working with relational databases (SQL) and understanding of data modeling principles.
Experience with cloud platforms (Preferably GCP. AWS or Azure will also do) and relevant data services (e.g., BigQuery, GCS, Data Factory, Dataproc, Dataflow, S3, EMR, Glue etc.).
Experience with data warehousing concepts and platforms (BigQuery, Snowflake, Redshift etc.).
Understanding of concepts related to integrating or deploying machine learning models into production systems.
Experience working in an Agile development environment & hands-on in any Agile work management tool(Rally, JIRA etc.).
Experience with version control systems, particularly Git.
Solid problem-solving, debugging, and analytical skills.
Excellent communication and collaboration skills.
Experience working in a production support team (L2/L3) for operational support.
Preferred Skills and Qualifications (Nice to Have):
Familiarity with data quality and data governance concepts.
Experience building and consuming APIs (REST, gRPC) related to data or model serving.
Bachelor's or Master's degree in Computer Science, Engineering, Data Science, or a related field.","Data Storage Interaction, snowflake, AI ML Integration, DataForm, Data Pipeline Development, Glue, ELT Implementation, GCS, Monitoring and Reliability, Performance Optimization, dbt, Sql, Data Factory, Emr, Java, DataFlow, Backend Development, BigQuery, Dataproc, Git, S3, Apache Airflow, Apache Flink, Apache Spark, Python, Redshift"
Associate Data Engineer,Commonwealth Bank,3-5 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title:Associate Data Engineer-Big Data
Location:Bengaluru
Business & Team:RM & FS Data Engineering
Impact & contribution:
As a Associate Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you're ahead of trends and work at the forefront of Big Data and Data warehouse technologies.
Which is why we're the perfect fit for you. Here, you'll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers most complex data-centric problems.
To us, data is everything. It is what powers our cutting-edge features and it's the reason we can provide seamless experiences for millions of customers from app to branch.
We're responsible for CommBank's key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We work across the Cloudera Hadoop Big Data, Teradata Group Data Warehouse and Ab Initio platforms.
Roles & Responsibilities:
Passionate about building next generation data platforms and data pipeline solution across the bank.
Enthusiastic, be able to contribute and learn from wider engineering talent in the team.
Ready to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.
Capable to create both technology blueprints and engineering roadmaps, for a multi- year data transformational journey.
Can lead and drive a culture where quality, excellence and openness are championed.
Constantly thinking outside the box and breaking boundaries to solve complex data problems.
Are experienced in providing data driven solutions that source data from various enterprise data platform into Cloudera Hadoop Big Data environment, using technologies like Spark, MapReduce, Hive, Sqoop, Kafka transform and process the source data to produce data assets and transform and egression to other data platforms like Teradata or RDBMS system.
Are confident in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.
Can collaborate, co-create and contribute to existing Data Engineering practices in the team.
Have experience and responsible for data security and data management.
Have a natural drive to educate, communicate and coordinate with different internal stakeholders.
Essential Skills:
Preferably with at least 3+ years of hands-on experience in a Data Engineering role.
Experience in designing, building, and delivering enterprise-wide data ingestion, data integration and data pipeline solutions using common programming language (Scala, Java, or Python) in a Big Data and Data Warehouse platform.
Experience in building data solution in Hadoop platform, using Spark, MapReduce, Sqoop, Kafka and various ETL frameworks for distributed data storage and processing. Preferably with at least 3+ years of hands-on experience.
Experience in building data solution using AWS Cloud technology (EMR, Glue, Iceberg, Kinesis, MSK/Kafka, Redshift, DocumentDB, S3, etc.). Preferably with 1+ years of hands-on experience and certified AWS Data Engineer.
Strong Unix/Linux Shell scripting and programming skills in Scala, Java, or Python.
Proficient in SQL scripting, writing complex SQLs for building data pipelines.
Experience in working in Agile teams, including working closely with internal business stakeholders.
Familiarity with data warehousing and/or data mart build experience in Teradata, Oracle or RDBMS system is a plus.
Certification on Cloudera CDP, Hadoop, Spark, Teradata, AWS, Ab Initio is a plus.
Experience in Ab Initio software products (GDE, Co Operating System, Express It, etc.) is a plus.
Educational Qualifications: B.Tech and above
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 05/06/2025","DocumentDB, MSK, Teradata, Iceberg, Glue, Java, S3, Hadoop, Scala, Kafka, Big Data, Emr, Redshift, Sql, Mapreduce, Hive, Kinesis, Sqoop, Spark, Cloudera, Ab Initio, Python, AWS"
Senior Data Engineer (AI/ML),Global Payments,5-7 Years,,"Pune, India",Banking/Accounting/Financial Services,"Every day, Global Payments makes it possible for millions of people to move money between buyers and sellers using our payments solutions for credit, debit, prepaid and merchant services. Our worldwide team helps over 3 million companies, more than 1,300 financial institutions and over 600 million cardholders grow with confidence and achieve amazing results. We are driven by our passion for success and we are proud to deliver best-in-class payment technology and software solutions. Join our dynamic team and make your mark on the payments technology landscape of tomorrow.
RESPONSIBILITIES
Design, develop, implement, test, and maintain scalable and efficient data pipelines for large scale structured and unstructured datasets, including document, image, and event data used in GenAI and ML use cases..
Collaborate closely with data scientists, AI/ML engineers, MLOps and Product Owners to understand data requirements and ensure data availability and quality.
Build and optimize data architectures for both batch and real-time processing.
Develop and maintain data warehouses and data lakes to store and manage large volumes of structured and unstructured data.
Implement data validation and monitoring processes to ensure data integrity.
Implement and manage vector databases (eg. pgVector, Pinecone, FAISS, etc) and embedding pipelines to support retrieval-augmented architectures.
Support data sourcing and ingestion strategies, including API, data lakes, and message queues
Enforce data quality, lineage, observability, and governance standards for AI workloads
Work with cross-functional IT and business teams in an Agile environment to deliver successful data solutions.
Help foster a data-driven culture via information sharing, design for scalability, and operational efficiency.
Stay updated with the latest trends and best practices in data engineering and big data technologies.
Must Haves:
Bachelor's degree in Computer Science, Engineering, or a related field.
5+ years of experience in data engineering or a related field.
Experience with the following:
Strong programming skills in Python (and optionally Scala or Java) with Spark, Airflow, or similar orchestration tools.
Deep experience with cloud data platforms (eg Databricks, GCP BigQuery, Snowflake, AWS Glue)
Strong familiarity with LLM workflows, RAG solutions, embeddings, reranking, and vector search concepts.
Proficiency in SQL and experience with data modeling for AI/ML use cases
Experience with NoSQL databases (MongoDB, Cassandra, or similar).
Knowledge of containerization technologies like Docker and orchestration systems like Kubernetes.
Cloud platform experience GCP, AWS or Azure are acceptable.
Understanding of responsible AI principles as applied to data sourcing and processing
Excellent problem-solving and analytical skills.
Excellent communication and collaboration skills.
Bonus Attributes:
Experience with real-time data processing frameworks (Kafka, Flink, etc.).
Experience working with data scientists on machine learning projects.
Experience supporting generative AI model training or inference in production environments
Knowledge of LLM and integration with foundation AI platforms (eg AWS Bedrock, Google VertexAI, Snowflake Cortex, Azure OpenAI)
Hands-on exposure and understanding of LangChain, LangGraph, CrewAI, or similar orchestration frameworks
Familiarity with machine learning frameworks (TensorFlow, PyTorch, scikit-learn).
Experience with CI/CD tools and practices.
Knowledge of data governance and data security best practices.
Certifications in data engineering or cloud technologies.
Abilities:
Ability to work with a high level of initiative, accuracy, and attention to detail.
Ability to prioritize multiple assignments effectively. Ability to meet established deadlines.
Ability to successfully, efficiently, and professionally interact with staff and customers.
Excellent organization skills.
Critical thinking ability ranging from moderately to highly complex.
Flexibility in meeting the business needs of the customer and the company.
Ability to work creatively and independently with latitude and minimal supervision.
Ability to utilize experience and judgment in accomplishing assigned goals.
Experience in navigating organizational structure.
Global Payments Inc. is an equal opportunity employer. Global Payments provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including pregnancy), national origin, ancestry, age, marital status, sexual orientation, gender identity or expression, disability, veteran status, genetic information or any other basis protected by law. If you wish to request reasonable accommodations related to applying for employment or provide feedback about the accessibility of this website, please contact .","snowflake, Airflow, scikit-learn, Flink, reranking, embeddings, LLM workflows, vector search, RAG solutions, Spark, Databricks, Sql, Java, Tensorflow, Kafka, BigQuery, Pytorch, Cassandra, Gcp, Scala, AWS Glue, AWS, Kubernetes, Python, Azure, Docker, MongoDB"
Senior Data Engineer (AI/ML),Global Payments,5-7 Years,,"Pune, India",Banking/Accounting/Financial Services,"Every day, Global Payments makes it possible for millions of people to move money between buyers and sellers using our payments solutions for credit, debit, prepaid and merchant services. Our worldwide team helps over 3 million companies, more than 1,300 financial institutions and over 600 million cardholders grow with confidence and achieve amazing results. We are driven by our passion for success and we are proud to deliver best-in-class payment technology and software solutions. Join our dynamic team and make your mark on the payments technology landscape of tomorrow.
RESPONSIBILITIES
Design, develop, implement, test, and maintain scalable and efficient data pipelines for large scale structured and unstructured datasets, including document, image, and event data used in GenAI and ML use cases..
Collaborate closely with data scientists, AI/ML engineers, MLOps and Product Owners to understand data requirements and ensure data availability and quality.
Build and optimize data architectures for both batch and real-time processing.
Develop and maintain data warehouses and data lakes to store and manage large volumes of structured and unstructured data.
Implement data validation and monitoring processes to ensure data integrity.
Implement and manage vector databases (eg. pgVector, Pinecone, FAISS, etc) and embedding pipelines to support retrieval-augmented architectures.
Support data sourcing and ingestion strategies, including API, data lakes, and message queues
Enforce data quality, lineage, observability, and governance standards for AI workloads
Work with cross-functional IT and business teams in an Agile environment to deliver successful data solutions.
Help foster a data-driven culture via information sharing, design for scalability, and operational efficiency.
Stay updated with the latest trends and best practices in data engineering and big data technologies.
Must Haves:
Bachelor's degree in Computer Science, Engineering, or a related field.
5+ years of experience in data engineering or a related field.
Experience with the following:
Strong programming skills in Python (and optionally Scala or Java) with Spark, Airflow, or similar orchestration tools.
Deep experience with cloud data platforms (eg Databricks, GCP BigQuery, Snowflake, AWS Glue)
Strong familiarity with LLM workflows, RAG solutions, embeddings, reranking, and vector search concepts.
Proficiency in SQL and experience with data modeling for AI/ML use cases
Experience with NoSQL databases (MongoDB, Cassandra, or similar).
Knowledge of containerization technologies like Docker and orchestration systems like Kubernetes.
Cloud platform experience GCP, AWS or Azure are acceptable.
Understanding of responsible AI principles as applied to data sourcing and processing
Excellent problem-solving and analytical skills.
Excellent communication and collaboration skills.
Bonus Attributes:
Experience with real-time data processing frameworks (Kafka, Flink, etc.).
Experience working with data scientists on machine learning projects.
Experience supporting generative AI model training or inference in production environments
Knowledge of LLM and integration with foundation AI platforms (eg AWS Bedrock, Google VertexAI, Snowflake Cortex, Azure OpenAI)
Hands-on exposure and understanding of LangChain, LangGraph, CrewAI, or similar orchestration frameworks
Familiarity with machine learning frameworks (TensorFlow, PyTorch, scikit-learn).
Experience with CI/CD tools and practices.
Knowledge of data governance and data security best practices.
Certifications in data engineering or cloud technologies.
Abilities:
Ability to work with a high level of initiative, accuracy, and attention to detail.
Ability to prioritize multiple assignments effectively. Ability to meet established deadlines.
Ability to successfully, efficiently, and professionally interact with staff and customers.
Excellent organization skills.
Critical thinking ability ranging from moderately to highly complex.
Flexibility in meeting the business needs of the customer and the company.
Ability to work creatively and independently with latitude and minimal supervision.
Ability to utilize experience and judgment in accomplishing assigned goals.
Experience in navigating organizational structure.
Global Payments Inc. is an equal opportunity employer. Global Payments provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including pregnancy), national origin, ancestry, age, marital status, sexual orientation, gender identity or expression, disability, veteran status, genetic information or any other basis protected by law. If you wish to request reasonable accommodations related to applying for employment or provide feedback about the accessibility of this website, please contact .","snowflake, Airflow, scikit-learn, Flink, reranking, embeddings, LLM workflows, vector search, RAG solutions, NoSQL databases, Spark, Databricks, Sql, Java, Tensorflow, Kafka, BigQuery, Pytorch, Cassandra, Gcp, Scala, AWS Glue, AWS, Kubernetes, Python, Azure, Docker, MongoDB"
Senior Data Engineer,Commonwealth Bank,7-10 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization
At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title: SeniorData Engineer
Location: Bangalore
Business & Team:
The Business Banking Technology Domain works in an Agile methodology with our business banking business to plan, prioritise and deliver on high value technology objectives with key results that meet our regulatory obligations and protect the community.
You will work within the VRM Crew that is working on initiatives such as Gen AI based cash flow coach to provide relevant data to our regulators
Impact & Contribution:
As a Senior Data Engineer and database engineer you will be creating and managing the cloud databases and data pipelines that underpin our decoupled cloud architecture and API first approach. You have proven expertise in database design, data ingestion, transformation, data writing, scheduling and query management within a cloud environment.
You will have proven experience and expertise in working with AWS Cloud Infrastructure Engineers, Software/API Developers to design, develop, deploy and operate data services and solutions that underpin a cloud ecosystem. You will take ownership and accountability of functional and non-functional design and work within a team of Engineers to create innovative solutions that unlock value and modernise technology designs.
You will role model continuous improvement mindset in the team, and in your project interactions, by taking technical ownership of key assets, including roadmaps and technical direction of data services running on our AWS environments.
Roles & Responsibilities:
Can design and implement databases for data integration in the enterprise
Can performance tune applications from a database code and design perspective
Can automate data ingestion and transformation processes using scheduling tools.
Monitor and troubleshoot data pipelines to ensure reliability and performance.
Can design application logical database requirements and implement physical solutions
Can collaborate with business and technical teams in order to design and build critical databases and data pipelines
Essential Skills:
Minimum 7 to 10 years of experience
AWS Data products such as AWS Glue and AWS EMR
Oracle and AWS Aurora RDS such as PostgreSQL
AWS S3 ingestion, transformation and writing to databases
Proficiency in programming languages like Python, Scala or Java for developing data ingestion and transformation scripts.
Strong knowledge of SQL for writing, optimizing, and debugging queries.
Familiarity with database design, indexing, and normalization principles.
Understanding of data formats (JSON, CSV, XML) and techniques for converting between them. Ability to handle data validation, cleaning, and transformation.
Proficiency in automation tools and scripting (e.g., bash scripting, cron jobs) for scheduling and monitoring data processes.
Experience with version control systems (e.g., Git) for managing code and collaboration.
Education Qualifications:
Bachelor's degree in engineering in Computer Science/Information Technology.
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 29/06/2025","AWS Data products such as AWS Glue and AWS EMR, Experience with version control systems e.g. Git, Oracle and AWS Aurora RDS such as PostgreSQL, Strong knowledge of SQL, AWS S3 ingestion transformation and writing to databases"
Data Engineer,PayPal,2-4 Years,,"Chennai, India",Banking/Accounting/Financial Services,"The Company
PayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.
We operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.
We offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.
Our beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.
Job Summary:
Meet Your Team
At the heart of our fintech innovation lies the Data Engineering & Analytics Team - a tight-knit group of engineers, data scientists, and analysts building the real-time intelligence layer that powers everything from fraud detection to personalized financial recommendations.
You'll work alongside team members who bring experience from top-tier tech, finance, and environments, and who value clean architecture, reproducible data science, and high-velocity experimentation.
We don't just crunch numbers - we build the data backbone that makes real-time decisioning possible across millions of user interactions.
Whether it's building low-latency pipelines, optimizing streaming systems, or unlocking insights from noisy data, we're driven by impact - and we're looking for someone who's excited to build with us.
What do you need to know about the role
As a Data Engineer in our team, you'll play a critical role in building and maintaining real-time and near real-time data pipelines that transform raw data into meaningful, trustworthy datasets for our stakeholders across the organization. This role involves backend development primarily in Java, with a focus on building scalable, low-latency systems that support timely and accurate decision-making.
You'll solve both technical and domain challenges - from optimizing data ingestion and transformation, to aligning with evolving compliance and regulatory needs. You'll collaborate closely with engineering peers, compliance analysts, and product managers to deliver solutions that are audit-ready, scalable, and aligned with our broader data strategy.
This is a high-impact role that offers deep exposure to the company's products, platforms, and data governance practices - an excellent opportunity to shape how compliance data is consumed in real time and help ensure the organization remains both agile and compliant.
Job Description:
Your way to impact
At PayPal, Backend Software Engineers are the architects of our global payment platform.You'll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers experiences and our company's success.
Your day-to-day
As a Software Engineer - Backendyou'll contribute to building robust backend systems. You'll collaborate closely with experienced engineers to learn and grow your skills.
Develop and maintain backend components.
Write clean, efficient code adhering to coding standards.
Participate in code reviews and provide feedback.
What do you need to Bring
Bachelor's degree in Computer Science or related field.
2+ years of backend development experience.Strong foundation in programming concepts and data structures.
Proficiency in at least one backend language (Java, Python, Ruby on Rails)
Proficiency in back-end development utilizing Java EE technologies (Java, application servers, servlet containers, JMS, JPA, Spring MVC, Hibernate)
Strong understanding of web services and Service-Oriented Architecture (SOA) standards, including REST, OAuth, and JSON, with experience in Java environments.
Experience with ORM (Object-Relational Mapper) tools, working within Java-based solutions like Hibernate.
Experience with databases (SQL, NoSQL)
Preferred Qualifications
Experience with large-scale, high-performance systems.
Knowledge of the payment processing industry and relevant regulations.
Experience with cloud platforms (AWS, GCP, Azure).
Contributions to open-source projects.
We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.
Preferred Qualification:
Subsidiary:
PayPal
Travel Percent:
0
For the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.
Our Benefits:
At PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.
We have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.
Who We Are:
to learn more about our culture and community.
Commitment to Diversity and Inclusion
PayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at.
Belonging at PayPal:
Our employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.
Any general requests for consideration of your skills, please .
We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.","Java application servers, Java EE technologies, servlet containers, AWS, Sql, Java, Ruby On Rails, Oauth, Json, Hibernate, Spring MVC, Jpa, Nosql, Python, Azure, Gcp, REST, Jms"
Senior Data Engineer,NielsenIQ,7-9 Years,,India,IT/Computers - Software,"Job Description
Senior Data Engineer
Mission of the Role
You are a passionate and skilled data engineer who enjoys building scalable, production-grade data pipelines that enable machine learning and AI at scale. You thrive on solving complex data challenges and understand how critical a robust data pipeline is to the success of any predictive model.
As a Senior Data Engineer on the Forecasting team, you will be responsible for designing, implementing, and maintaining the data pipelines that power deep neural network models used for global sales and revenue forecasting. You will work closely with ML engineers, data scientists, and product stakeholders to build automated, secure, and cost-efficient pipelines using GCP-native services.
You take pride in writing clean, testable, and production-ready code, and are comfortable operating in cloud environments. You know how to make data pipelines observable, recoverable, and performant. You are also comfortable taking architectural ownership and contributing to long-term platform improvements.
You will:
Design and build robust, scalable, and maintainable data pipelines for training and inference of deep learning models used in forecasting and other Machine Learning based algorithms.
Develop and orchestrate end-to-end workflows using GCP-native tools such as Cloud Workflows, Cloud Run, BigQuery, Cloud Functions, and Pub/Sub.
Automate and maintain the pipeline lifecycle, including data extraction, transformation, loading (ETL/ELT), and triggering model runs.
Monitor and Optimise performance, cost-efficiency, and data freshness across all stages of the pipeline.
Collaborate with data scientists to productionise AI/ML models and integrate them seamlessly into business workflows.
Own the data ingestion, validation, and transformation logic powering the forecasting models, ensuring high-quality, consistent input data.
Handle deployment, versioning, and configuration of pipeline components using Docker, Git, and CI/CD practices.
Implement logging, monitoring, and alerting to ensure reliability and traceability in a high-scale environment.
Review code, mentor junior engineers, and help define best practices in our evolving data engineering stack.
Qualifications
You have:
7+ years of experience in data engineering or backend engineering roles.
Strong expertise in Python and SQL, with experience building production-grade data pipelines.
Solid understanding of Docker, Git, and shell scripting in Linux environments.
Hands-on experience with GCP services
Experience in building, deploying, and maintaining data workflows that feed AI/ML models.
Familiarity with model lifecycle management and infrastructure challenges in ML pipelines.
Proficiency in setting up CI/CD pipelines for data and ML systems using GitLab CI, Cloud Build, or similar tools.
Exposure to Java for backend services or pipeline components (even if not primary language).
A proactive, collaborative mindset and strong communication skills across engineering and data science teams.
Nice to have:
Exposure to forecasting or time series modelling pipelines.
Experience with event-driven architectures.
Familiarity with infrastructure-as-code tools like Terraform
Understanding of data quality frameworks and observability tools
Knowledge of model versioning tools and experiment tracking systems
Additional Information
Why Join us
You'll be working on a high-impact platform that forecasts consumer demand and drives business-critical decisions globally
You'll be part of a tight-knit, collaborative, and innovative Forecasting & AI team within a leading global data science organisation
You'll have access to top-tier infrastructure, GCP services, and challenging real-world problems in predictive modelling
Flexible working hours, remote-friendly culture, and strong focus on personal and professional growth
Competitive compensation and performance-based bonuses
Our Benefits
Flexible working environment
Volunteer time off
LinkedIn Learning
Employee-Assistance-Program (EAP)
About NIQ
NIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights-delivered with advanced analytics through state-of-the-art platforms-NIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.
For more information, visit NIQ.com
Want to keep up with our latest updates
Follow us on: | | |
Our commitment to Diversity, Equity, and Inclusion
NIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center:","Cloud Run, Cloud Workflows, GCP-native tools, Cloud Functions, Git, BigQuery, Sql, Python, Docker"
Associate Analyst - Data Engineer,PepsiCo,4-6 Years,,"Hyderabad, India",Food Processing & Packaged Food,"Overview
PepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo's global business scale to enable business insights, advanced analytics, and new product development. PepsiCo's Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.
What PepsiCo Data Management and Operations does:
Maintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.
Responsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset
Work cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.
Increase awareness about available data and democratize access to it across the company.
As a data engineer, you will be the key technical expert building PepsiCo's data productsto drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developingvery large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.
Responsibilities
Act as a subject matter expert across different digital projects.
Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance, and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to productionalize data science models.
Define and manage SLA's for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Qualifications
4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.
3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience in Azure Log Analytics
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus.
Experience with version control systems like Github and deployment & CI tools.
Working knowledge of agile development, including DevOps and DataOps concepts.
B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.
Skills, Abilities, Knowledge:
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Strong change manager. Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong organizational and interpersonal skills comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals.","Data Analytics tools, snowflake, Deequ, SQL optimization, Great Expectations, MPP database technology, Data Lake Infrastructure, Synapse, Apache Griffin, ELT, Azure Data Factory, Data Warehousing, Pyspark, Azure Machine Learning, Etl, Azure Databricks, Azure Log Analytics, Python, Azure, Scala, Redshift"
Azure Databricks Lead (Sr. Data Engineer),Tiger Analytics,5-7 Years,,"Chennai, India",Internet/E-commerce,"Job Description
Who we are
Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer full-stack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow.
Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence.
We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.
Curious about the role What your typical day would look like
Role Overview:
We are seeking street-smart and technically strong Senior Data Engineers / Leads who can take ownership of designing and developing cutting-edge data and AI platforms using Azure-native technologies and Databricks. You will play a critical role in building scalable data pipelines, modern data architectures, and intelligent analytics solutions.
Key Responsibilities:
Design and implement scalable, metadata-driven frameworks for data ingestion, quality, and transformation across both batch and streaming datasets.
Develop and optimize end-to-end data pipelines to process structured and unstructured data, enabling the creation of analytical data products.
Build robust exception handling, logging, and monitoring mechanisms for better observability and operational support.
Take ownership of complex modules and lead the development of critical data workflows and components.
Provide guidance to data engineers and peers on best practices.
Collaborate with cross-functional teams-including business consultants, data architects & scientists, and application developers-to deliver impactful analytics solutions.
Job Requirement
5+ years of overall technical experience, with a minimum of 2 years of hands-on experience with Microsoft Azure and Databricks.
Proven experience delivering at least one end-to-end Data Lakehouse solution on Azure Databricks using the Medallion Architecture.
Strong working knowledge of the Databricks ecosystem, including: PySpark, Notebooks, Structured Streaming, Unity Catalog, Delta Live Tables, Workflows, and SQL Warehouse.
Advanced programming, unit testing, and debugging skills in Python and SQL.
Hands-on experience with Azure-native services such as: Azure Data Factory, ADLS Gen2, Azure SQL Database, and Event Hub.
Solid understanding of data modeling techniques, including both Dimensional and Third Normal Form (3NF) models.
Exposure to developing LLM/Generative AI-powered applications.
Must have excellent understanding of CI/CD workflows using Azure DevOps.
Bonus: Knowledge of Azure infrastructure, including provisioning, networking, security, and governance.
Educational Background:
Bachelor's degree (B.E/B.Tech) in Computer Science, Information Technology, or a related field from a reputed institute (preferred).
You are important to us, let's stay connected!
Every individual comes with a different set of skills and qualities so even if you don't tick all the boxes for the role today we urge you to apply as there might be a suitable/unique role for you tomorrow. We are an equal- opportunity employer. Ourdiverse and inclusive culture and values guide us to listen, trust, respect, and encourage people to grow the way they desire, packages are among the best in industry.","Azure SQL Database, CI CD workflows, ADLS Gen2, Delta Live Tables, Unity Catalog, Event Hub, Structured Streaming, Databricks, Sql, Azure Data Factory, Workflows, Pyspark, Microsoft Azure"
Enterprise Data Operations Analyst - Data Engineer,PepsiCo,7-9 Years,,"Hyderabad, India",Food Processing & Packaged Food,"Overview
As a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.
Responsibilities
Act as a subject matter expert across different digital projects.
Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance, and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to productionalize data science models.
Define and manage SLA's for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Qualifications
7+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience in Azure Log Analytics
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus.
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.","Azure Machine Learning tools, Deequ, SQL optimization, Great Expectations, Data Lake Infrastructure, Apache Griffin, Statistical ML techniques, CI tools, Data Analytics tools, Azure Log Analytics, ELT, Data Warehousing, Azure Data Factory, Github, Pyspark, Data Modelling, Etl, Azure Databricks, Powerbi, Python, Kubernetes, Azure, Scala"
Lead Data Engineer,Komhar Infotech Private Limited,8-10 Years,,Hyderabad,Software,"We are looking for aLeadData Engineerto join our data engineering team. The ideal candidate will have strong experience in IBM Infosphere DataStage, ETL development, and leading end-to-end data integration projects. The role requires hands-on development, team leadership, and close collaboration with business and technical stakeholders.
Key Responsibilities:
Lead the design, development, and implementation of ETL processes using IBM DataStage.
Collaborate with business analysts and data architects to understand data requirements.
Optimize, maintain, and troubleshoot existing DataStage jobs and data pipelines.
Ensure high-quality deliverables through code reviews, unit testing, and documentation.
Mentor junior team members and provide technical leadership.
Work closely with QA and DevOps teams for deployment and release management.
Participate in project planning, estimation, and status reporting activities.
Required Skills:
Strong hands-on experience with IBM Infosphere DataStage.
Solid understanding of ETL concepts, data warehousing, and data modeling.
Proficient in SQL and working with large relational databases (Oracle, DB2, etc.).
Experience with performance tuning and troubleshooting of ETL jobs.
Familiarity with data quality, data governance, and master data management practices.
Excellent communication and problem-solving skills.
Preferred Skills:
Experience with cloud-based ETL tools or hybrid cloud environments (e.g., AWS, Azure).
Knowledge of scheduling tools like Control-M, Autosys.
Exposure to Agile/Scrum methodologies.
Knowledge in shell scripting.
DataStage, PySpark, Hive, Impala & Snowflake
Work Location: Hyderabad
Work Mode: WFO",snowflake
Senior Data engineer,Pago Analytics India Private Limited,5-6 Years,INR 13 - 15 LPA,Hyderabad,Login to check your skill match score,"Data Engineer - Remote
Overview:
A highly skilled Data Engineer with 5+ years of experience specializing in cloud data architecture and integration. Expertise in leveraging AWS cloud services to build scalable data pipelines, ensuring seamless integration with cloud-based ERP systems like Microsoft Dynamics, Salesforce, and Oracle Fusion. Adept at handling large-scale data processing, transformation, and storage solutions.
*Mandatory Skillsets:*
AWS Cloud Services (S3, Redshift, Lambda, Glue, RDS, Athena)
Python & PySpark (Data Transformation, Big Data Processing, Automation)
ETL Process Development & Optimization
*ERP System Integration (Microsoft Dynamics 365, Salesforce, Oracle Fusion)*
Data Pipeline Development & Workflow Automation
SQL & NoSQL Query Optimization
Data Modelling & Data Transformation
Data Validation, Cleansing & Quality Assurance
Performance Monitoring & Troubleshooting
Project Management & Documentation","Salesforce, Oracle Fusion)*, *ERP System Integration (Microsoft Dynamics 365"
Enterprise Data Operations Analyst - Data Engineer,PepsiCo,7-9 Years,,"Hyderabad, India",Food Processing & Packaged Food,"Overview
As a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.
Responsibilities
Act as a subject matter expert across different digital projects.
Oversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.
Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.
Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.
Responsible for implementing best practices around systems integration, security, performance, and data management.
Empower the business by creating value through the increased adoption of data, data science and business intelligence landscape.
Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.
Evolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.
Develop and optimize procedures to productionalize data science models.
Define and manage SLA's for data products and processes running in production.
Support large-scale experimentation done by data scientists.
Prototype new approaches and build solutions at scale.
Research in state-of-the-art methodologies.
Create documentation for learnings and knowledge transfer.
Create and audit reusable packages or libraries.
Qualifications
7+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.
4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).
2+ years in cloud data engineering experience in Azure.
Fluent with Azure cloud services. Azure Certification is a plus.
Experience in Azure Log Analytics
Experience with integration of multi cloud services with on-premises technologies.
Experience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse or Snowflake.
Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.
Experience with Statistical/ML techniques is a plus.
Experience with building solutions in the retail or in the supply chain space is a plus.
Understanding of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
B Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.","Azure Machine Learning tools, Deequ, SQL optimization, Great Expectations, Data Lake Infrastructure, Apache Griffin, Statistical ML techniques, CI tools, Data Analytics tools, Azure Log Analytics, ELT, Data Warehousing, Azure Data Factory, Github, Pyspark, Data Modelling, Etl, Azure Databricks, Powerbi, Python, Kubernetes, Azure, Scala"
Lead Data Engineer,Komhar Infotech Private Limited,8-10 Years,,Hyderabad,Software,"We are looking for aLeadData Engineerto join our data engineering team. The ideal candidate will have strong experience in IBM Infosphere DataStage, ETL development, and leading end-to-end data integration projects. The role requires hands-on development, team leadership, and close collaboration with business and technical stakeholders.
Key Responsibilities:
Lead the design, development, and implementation of ETL processes using IBM DataStage.
Collaborate with business analysts and data architects to understand data requirements.
Optimize, maintain, and troubleshoot existing DataStage jobs and data pipelines.
Ensure high-quality deliverables through code reviews, unit testing, and documentation.
Mentor junior team members and provide technical leadership.
Work closely with QA and DevOps teams for deployment and release management.
Participate in project planning, estimation, and status reporting activities.
Required Skills:
Strong hands-on experience with IBM Infosphere DataStage.
Solid understanding of ETL concepts, data warehousing, and data modeling.
Proficient in SQL and working with large relational databases (Oracle, DB2, etc.).
Experience with performance tuning and troubleshooting of ETL jobs.
Familiarity with data quality, data governance, and master data management practices.
Excellent communication and problem-solving skills.
Preferred Skills:
Experience with cloud-based ETL tools or hybrid cloud environments (e.g., AWS, Azure).
Knowledge of scheduling tools like Control-M, Autosys.
Exposure to Agile/Scrum methodologies.
Knowledge in shell scripting.
DataStage, PySpark, Hive, Impala & Snowflake
Work Location: Hyderabad
Work Mode: WFO",snowflake
Senior Data Engineer,TalentBasket,5-8 Years,INR 20.5 - 25 LPA,"Cochin / Kochi / Ernakulam, Thiruvananthapuram / Trivandrum",Login to check your skill match score,"Job Description
Job Title: Senior Data Engineer Data Quality, Ingestion & API Development
Job Overview
We are seeking an experienced Senior Data Engineer to lead the development of a scalable data
ingestion framework while ensuring high data quality and validation. The successful candidate
will also be responsible for designing and implementing robust APIs for seamless data
integration. This role is ideal for someone with deep expertise in building and managing big data
pipelines using modern AWS-based technologies, and who is passionate about driving quality
and efficiency in data processing systems.
Key Responsibilities
Data Ingestion Framework:
o Design & Development: Architect, develop, and maintain an end-to-end data
ingestion framework that efficiently extracts, transforms, and loads data from
diverse sources.
o Framework Optimization: Use AWS services such as AWS Glue, Lambda,
EMR, ECS , EC2 and Step Functions to build highly scalable, resilient, and
automated data pipelines.
Data Quality & Validation:
o Validation Processes: Develop and implement automated data quality checks,
validation routines, and error-handling mechanisms to ensure the accuracy and
integrity of incoming data.
o Monitoring & Reporting: Establish comprehensive monitoring, logging, and
alerting systems to proactively identify and resolve data quality issues.
API Development:
o Design & Implementation: Architect and develop secure, high-performance
APIs to enable seamless integration of data services with external applications
and internal systems.
o Documentation & Best Practices: Create thorough API documentation and
establish standards for API security, versioning, and performance optimization.
Collaboration & Agile Practices:
o Cross-Functional Communication: Work closely with business stakeholders,
data scientists, and operations teams to understand requirements and translate
them into technical solutions.
o Agile Development: Participate in sprint planning, code reviews, and agile
ceremonies, while contributing to continuous improvement initiatives and CI/CD
pipeline development (using tools like GitLab).
Required Qualifications
Experience & Technical Skills:
o Professional Background: At least 5 years of relevant experience in data
engineering with a strong emphasis on analytical platform development.
o Programming Skills: Proficiency in Python and/or PySpark, SQL for
developing ETL processes and handling large-scale data manipulation.
o AWS Expertise: Extensive experience using AWS services including AWS Glue,
Lambda, Step Functions, and S3 to build and manage data ingestion frameworks.
o Data Platforms: Familiarity with big data systems (e.g., AWS EMR, Apache
Spark, Apache Iceberg) and databases like DynamoDB, Aurora, Postgres, or
Redshift.
o API Development: Proven experience in designing and implementing RESTful
APIs and integrating them with external and internal systems.
o CI/CD & Agile: Hands-on experience with CI/CD pipelines (preferably with
GitLab) and Agile development methodologies.
Soft Skills:
o Strong problem-solving abilities and attention to detail.
o Excellent communication and interpersonal skills with the ability to work
independently and collaboratively.
o Capacity to quickly learn and adapt to new technologies and evolving business
requirements.
Preferred Qualifications
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
Experience with additional AWS services such as Kinesis, Firehose, and SQS.
Familiarity with data lakehouse architectures and modern data quality frameworks.
Prior experience in a role that required proactive data quality management and API-
driven integrations in complex, multi-cluster environments.create a catchy linkedin post for this JD also mention the email to contact section to this post","Analytical Thinking, Python and/or PySpark, AWS EMR, communication, Interpersonal, AWS Step Functions, ECS / EC2, Problem-solving, Aws Lambda, Sql, AWS Glue"
Opening for Lead/Principal Data engineer- Bangalore,Hireatease Consulting Private Limited,7-14 Years,INR 40 - 55.5 LPA,Bengaluru,Information Technology,"Accountabilities
Bridge business needs with technical solutions by leading IT application design and implementation.
Collaborate with team members to define and deliver requirements, translating them into detailed specifications.
Optimize performance and mitigate risks through improvements in design and processes.
Advise on industry trends and standard methodologies to enhance performance and business outcomes.
Provide strategic direction and guidance to IT teams and business units.
Chip in to Data & Software Engineering standards and best practices.
Research new technologies to boost system performance and scalability.
Lead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering etc.) and work with AI/GenAI leads to foster collaboration and innovation.
Lead technical teams in design, architecture, and innovation, resolving complex issues.
Ensure platform stability, scalability, and simplicity.
Foster continuous improvement and innovation.
Be responsible for technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.
Essential Skills/Experience
9+ years in software and data engineering.
Strong problem-solving with a focus on high-quality solutions.
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python","Data Brick, Governance, Modeling, Py Spark, Python"
Azure data Engineer,Xpetize Technology Solutions Private Limited,8-13 Years,INR 15 - 22 LPA,Remote,Login to check your skill match score,"Design and develop warehouse solutions using Azure Synapse Analytics, ADLS, ADF, Databricks, Power BI, Azure Analysis Services
Should be proficient in SSIS, SQL and Query optimization.
Should have worked in onshore offshore model managing challenging scenarios.
Expertise in working with large amounts of data (structured and unstructured), building data pipelines for ETL workloads and generate insights utilizing Data Science, Analytics.
Expertise in Azure, AWS cloud services, and DevOps/CI/CD frameworks.
Ability to work with ambiguity and vague requirements and transform them into deliverables.
Good combination of technical and interpersonal skills with strong written and verbal communication; detail-oriented with the ability to work independently.
Drive automation efforts across the data analytics team utilizing Infrastructure as Code (IaC) using Terraform, Configuration Management, and Continuous Integration (CI) / Continuous Delivery (CD) tools such as Jenkins.
Help build define architecture frameworks, best practices & processes. Collaborate on Data warehouse architecture and technical design discussions.
Expertise in Azure Data factory and should be familiar with building pipelines for ETL projects.
Expertise in SQL knowledge and experience working with relational databases.
Expertise in Python and ETL projects
Experience in data bricks will be of added advantage.
Should have expertise in data life cycle, data ingestion, transformation, data loading, validation, and performance tuning.","Azure Data Factory, Azure Data Lake, Ssis"
Data Engineer,Freelancer Riddhi Govind Nighojkar,5-10 Years,INR 5 - 16 LPA,Remote,Login to check your skill match score,"Snowflake with DBTExperience: 5-10 YearsMandatory skills: Candidate should be on Data Engineer background,Snowflake, SQL, Looker(All mandatory)Good to have: PythonSkills:1. Expertise in building data platforms & warehouses with Snowflake covering all aspects of data engineering practices.2. Expertise in using ETL tools like DBT etc.3. Expert at writing and optimizing SQL queries.4. Knowledge of AWS/Azure and various services related to data engineering is a must.5. Should have worked on developing and optimizing data pipelines (spark code).6. Should have experience with NFRs for data platforms like data quality, governance, security and compliance aspects.","snowflake, looker, Data Warehousing, Sql, Python Language"
GCP Data Engineer,Growel Softech Private Limited,8-10 Years,INR 25 - 30 LPA,"Mumbai City, Bengaluru, Chennai",Login to check your skill match score,"Description
We are seeking a skilled GCP Data Engineer with 8-10 years of experience to join our dynamic team in India. The ideal candidate will be responsible for designing and implementing robust data processing systems on Google Cloud Platform, ensuring seamless data flow and high-quality data management.
Responsibilities
Design and implement data processing systems on Google Cloud Platform (GCP) using tools like BigQuery, Dataflow, and Pub/Sub.
Develop and maintain ETL processes to ensure efficient data ingestion and transformation.
Collaborate with data scientists and analysts to understand data requirements and provide solutions.
Monitor and optimize the performance of data pipelines and storage solutions.
Ensure data quality and integrity by implementing validation and testing strategies.
Participate in architecture discussions and contribute to the design of data solutions.
Skills and Qualifications
8-10 years of experience in data engineering or a related field.
Proficiency in Google Cloud Platform services such as BigQuery, Dataflow, and Pub/Sub.
Strong programming skills in languages such as Python, Java, or Scala.
Experience with SQL and data modeling techniques.
Familiarity with data warehousing concepts and methodologies.
Knowledge of data pipeline orchestration tools like Apache Airflow or similar.
Understanding of data governance and security best practices.
Ability to work collaboratively in a team environment and communicate effectively with stakeholders.","ETL Processes, Pub/Sub, Cloud Storage, BigQuery, Data Modeling, Terraform, Apache Beam, DataFlow, Sql, Python"
Sr. DATA Engineer,Number 11,6-15 Years,INR 22 - 28.5 LPA,Pune,Login to check your skill match score,"We are hiring for our client NCS (Singapore based organization).
Skills required -SQL , Python, ETL,Pyspak
Key Responsibilities:
Design, build, and optimize scalable#SQL-based data pipelines
Develop and manage#ETL workflowsacross varied data sources
Collaborate with cross-functional teams (data analysts, scientists, engineers)
Write clean#Pythonscripts for automation and data manipulation
Utilize#PySparkfor big data processing and distributed workflows
Monitor and troubleshoot pipeline performance and ensure data reliability
Maintain data governance, security, and integrity standards
Required Skills & Qualifications:
68 yearsof experience in data engineering or related fields
Expert-level proficiency in #SQL complex queries, tuning, stored procedures
Strong understanding of#ETL processesanddata warehousing
Proficiency in#Pythonscripting
Hands-on experience with#PySparkor other distributed frameworks
Familiarity with#PostgreSQL, #MySQL, #SQLServer, or other relational DBs
Excellent analytical and problem-solving skills","Py Spark, Sql, Python, Etl"
Big Data Engineer,Robotics Technologies,10-15 Years,INR 10 - 15.5 LPA,"Hyderabad, Bengaluru, Kolkata","Sales Automation, Embedded Software, Artificial Intelligence, Embedded Systems, Presentation Software, Web Development, File Sharing","We are seeking a seasoned Big Data Engineer with 10-15 years of experience to join our dynamic team in India. The ideal candidate will have a strong background in designing and implementing data processing systems and will be responsible for managing large-scale data pipelines.
Responsibilities
Design and implement scalable data pipelines to process large datasets.
Develop and maintain ETL processes to extract, transform, and load data from various sources.
Optimize and tune data processing systems for maximum performance and efficiency.
Collaborate with data scientists and analysts to understand data requirements and deliver solutions.
Ensure data quality and integrity across all data systems.
Implement data security and compliance measures in line with industry standards.
Monitor and troubleshoot data pipeline performance and issues.
Skills and Qualifications
10-15 years of experience in Big Data technologies such as Hadoop, Spark, and Kafka.
Proficiency in programming languages such as Java, Scala, or Python.
Experience with data warehousing solutions like Amazon Redshift, Google BigQuery, or Snowflake.
Strong knowledge of SQL and NoSQL databases.
Familiarity with cloud platforms like AWS, Azure, or Google Cloud Platform.
Experience with containerization and orchestration tools such as Docker and Kubernetes.
Understanding of data modeling and data architecture best practices.","Cloud Platforms, Nosql, Hadoop, Etl Tools, Data Modeling, Spark, Kafka, Data Warehousing, Sql, Python"
Senior Data engineer,Pago Analytics India Private Limited,5-6 Years,INR 13 - 15 LPA,Hyderabad,Login to check your skill match score,"Data Engineer - Remote
Overview:
A highly skilled Data Engineer with 5+ years of experience specializing in cloud data architecture and integration. Expertise in leveraging AWS cloud services to build scalable data pipelines, ensuring seamless integration with cloud-based ERP systems like Microsoft Dynamics, Salesforce, and Oracle Fusion. Adept at handling large-scale data processing, transformation, and storage solutions.
*Mandatory Skillsets:*
AWS Cloud Services (S3, Redshift, Lambda, Glue, RDS, Athena)
Python & PySpark (Data Transformation, Big Data Processing, Automation)
ETL Process Development & Optimization
*ERP System Integration (Microsoft Dynamics 365, Salesforce, Oracle Fusion)*
Data Pipeline Development & Workflow Automation
SQL & NoSQL Query Optimization
Data Modelling & Data Transformation
Data Validation, Cleansing & Quality Assurance
Performance Monitoring & Troubleshooting
Project Management & Documentation","Salesforce, Oracle Fusion)*, *ERP System Integration (Microsoft Dynamics 365"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements
Job Requirements
Role/ Job Title: Data Engineer - Gen AI
Function/ Department: Data & Analytics
Place of Work: Mumbai
Job Purpose
The data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.
Job & Responsibilities
Build data engineering pipeline focused on unstructured data pipelines
Conduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.
Design, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.
Work on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability
Drive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets
A suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.
Creating Technical Design Documentation of the projects/pipelines
Good skills in technical debugging of the code in case of issues. Also, working with git for code versioning
Education Qualification
Graduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)
Post-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA
Experience Range : 5-10 years of relevant experience","big data infrastructure, HDFS, Hive, Dynamodb, Api, MongoDB, HBase, Yarn, Mapreduce, Etl"
Data Engineer,MResult,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Why MResult
Founded in 2004, MResult is a global digital solutions partner trusted by leading Fortune 500 companies in industries such as pharma & healthcare, retail, and BFSI. MResult's expertise in data and analytics, data engineering, machine learning, AI, and automation help companies streamline operations and unlock business value. As part of our team, you will collaborate with top minds in the industry to deliver cutting-edge solutions that solve real-world challenges.
What We Offer:
At MResult, you can leave your mark on projects at the world's most recognized brands, access opportunities to grow and upskill, and do your best work with the flexibility of hybrid work models. Great work is rewarded, and leaders are nurtured from within.
Our values Agility, Collaboration, Client Focus, Innovation, and Integrity are woven into our culture, guiding every decision.
What This Role Requires:
In the role of Data Engineer, you will be a key contributor to MResult's mission of empowering our clients with data-driven insights and innovative digital solutions. Each day brings exciting challenges and growth opportunities. Here is what you will do:
Roles & Responsibilities:
Project solutioning, including scoping, and estimation.
Data sourcing, investigation, and profiling.
Prototyping and design thinking.
Developing data pipelines & complex data workflows.
Actively contribute to project documentation and playbook, including but not limited to physical models, conceptual models, data dictionaries and data cataloguing.
Key Skills to Succeed in This Role:
Overall 6+ years of experience, 3+ years of hands-on experience in working with Python in building data pipelines and processes.
Proficiency in SQL programming, including the ability to create and debug stored procedures, functions, and views.
3+ years of hands-on experience delivering data lake/data warehousing projects. Innovation Insights.
Experience in working with cloud native SQL and NoSQL database platforms. Snowflake experience is desirable.
Experience in AWS services EC2, EMR, RDS, Spark is preferred.
Solid understanding of Scrum/Agile is preferred and working knowledge of CI/CD, GitHub MLflow.
Manage, Master, and Maximize with MResult
MResult is an equal-opportunity employer committed to building an inclusive environment free of discrimination and harassment.
Take the next step in your career with MResult where your ideas help shape the future.","Cloud Native SQL, snowflake, MLflow, Nosql, Github, Data Lake, Data Warehousing, Python, Sql"
Data Engineer,Cardinal Health,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Department Overview:
augmented Intelligence (augIntel) builds automation, analytics and artificial intelligence solutions that drive success for Cardinal Health by creating material savings, efficiencies and revenue growth opportunities. The team drives business innovation by leveraging emerging technologies and turning them into differentiating business capabilities.
Job Overview:
Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of Google Cloud Platform data and analytics services in combination with technologies like Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, Cloud BigQuery, Cloud PubSub, Cloud Functions, Airflow.
Responsibilities:
Designing and implementing data transformation, ingestion and curation functions on GCP cloud using GCP native or custom programming
Designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Python. Optimizing data pipelines for performance and cost for large scale data lakes.
Desired Qualifications:
Bachelor's degree preferred or equivalent work experience.
5+ years of engineering experience in , Data Analytics and Data Integration related fields.
3+ years of experience writing complex SQL queries, stored procedures, etc.
1+ years of hands-on GCP experience in Data Engineering and Cloud Analytics solutions.
Experience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery.
Agile development skills and experience.
Experience with CI/CD pipelines such as Concourse, Jenkins.
Google Cloud Platform certification is a plus.
Perks and benefits
Variable pay, WiFi reimbursement, Cab facilities, shift allowance (1PM-10PM)","Airflow, Cloud Functions, Concourse, Cloud Dataflow, Cloud DataProc, CI CD pipelines, Cloud PubSub, Cloud BigQuery, Google Cloud Platform, Sql, Jenkins, Spark, Apache Beam, Python"
Data Engineer,NTT DATA North America,Fresher,,"Bengaluru, India",Login to check your skill match score,"Req ID: 321499
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties:
Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.
Work closely with Data modeller to ensure data models support the solution design
Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.
Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.
Develop documentation and artefacts to support projects
Minimum Skills Required:
ADF
Fivetran (orchestration & integration)
SQL
Snowflake DWH
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com
NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Snowflake DWH, Fivetran, Adf, Sql"
Data Engineer,Knight Frank India,2-5 Years,,"Mumbai, India",Real Estate,"Responsibilities
To work with the multiple teams to extend the team's data integration, modelling and reporting capabilities
Develop and Maintain Scalable Data Pipelines: Design and implement robust data pipelines using Azure Data Bricks (ADB) and Microsoft Fabric to efficiently process and transform large datasets, ensuring data quality and timely delivery across various business units.
Provide solution and integration development requirements, functional specs, design, custom development, integration, testing, and deployment
Leverage Azure DevOps and Git to establish and maintain Cl/CD pipelines for automating the deployment
and monitoring of data integration workflows, ensuring smooth updates and version control across environments.
To be able to work with a degree of independence on projects
To ensure easy access to data for the wider team, collaborating with stakeholders and Knight Frank teams on projects, analysing and cleansing complex datasets, and advising on best practices for data integration and modelling.
Develop and maintain scalable RESTful APIs for seamless data exchange, leveraging JSON for efficient data ingestion, transformation, and integration across systems, ensuring high performance and reliability.
Experience
Demonstrated 2-3 years of strong experience using T-SQL.
More than 3 years of working experience in developing Data warehouses.
More than 3 years of experience in building ETL/ELT pipelines, serving both on-prem and cloud-based data warehouses.
Working as a data engineer in Azure based projects for not less than 2 years, having an in-depth understanding of the following:
ETL Pipelines developed with Azure Data Bricks & Azure Synapse pipelines
Strong proficiency in Python and PySpark for developing efficient ETL workflows, data processing, and automation of data pipelines in distributed environments.
Hands-on experience with Azure DevOps, Git, and Cl/CD pipeline automation for continuous integration
and delivery of data workflows and infrastructure deployments.
Demonstrated working experience that requires understanding of underlying data structures and different integration methods and uses them to develop quality solutions within agreed timelines.
Demonstrated working with multiple stakeholders across multiple disciplines to understand their data
requirements and enhancing their current architecture.
Having strong troubleshooting and error-handling skillset, along with a keen inclination towards process automation.
Proactive to remain up to date with latest technologies and techniques.
Demonstrated experience in working effectively as part of a team, managing/mentoring individuals - all along showing a strong collaborative mindset and resourcefulness.
Systems
Strong experience with the following
T-SQL & Data Modelling
Azure Data Bricks
SQL Server
Data Security & Governance
Cl/CD & DevOps
REST API & JSON
Experience of the following desirable
Power BI
SSIS
Microsoft Fabric
Azure Synapse
Particular Aptitudes/Skills Required
Highly organized, systematic and adaptable with an excellent attention to detail, with the ability to recognize the relative importance of software issues and to prioritize work effectively
Ability to communicate clearly and deal with others at all levels in a polite, professional, friendly and helpful manner, both face to face, by email and on the telephone and always maintain a good working relationship
Demonstrated an ability to work on multiple projects simultaneously
Ability to work with all members of the team, in a professional and yet dynamic and creative environment, fostering an open, friendly and constructive working relationship with all members of the team
Ability to work within a high-pressure environment, balance priorities and remain calm under pressure
The successful candidate will be flexible, self-motivated, organized and pro-active with excellent computing and administration skills and the ability to adapt to a wide range of tasks. They will also have a hands on attitude and possess the necessary skills, manner and experience to provide an effective support service to the department/office
Desire to learn new technologies and continuously develop new skills and expertise
Ability to work out of hours to deliver application upgrades in agreed maintenance windows","Data Security Governance, Rest Api, Azure Synapse, T-sql, Azure Data Bricks, Pyspark, SQL Server, Json, Python, Azure DevOps"
Data Engineer,Cardinal Health,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Department Overview:
augmented Intelligence (augIntel) builds automation, analytics and artificial intelligence solutions that drive success for Cardinal Health by creating material savings, efficiencies and revenue growth opportunities. The team drives business innovation by leveraging emerging technologies and turning them into differentiating business capabilities.
Job Overview:
Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of Google Cloud Platform data and analytics services in combination with technologies like Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, Cloud BigQuery, Cloud PubSub, Cloud Functions, Airflow.
Responsibilities:
Designing and implementing data transformation, ingestion and curation functions on GCP cloud using GCP native or custom programming
Designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Python. Optimizing data pipelines for performance and cost for large scale data lakes.
Desired Qualifications:
Bachelor's degree preferred or equivalent work experience.
5+ years of engineering experience in , Data Analytics and Data Integration related fields.
3+ years of experience writing complex SQL queries, stored procedures, etc.
1+ years of hands-on GCP experience in Data Engineering and Cloud Analytics solutions.
Experience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery.
Agile development skills and experience.
Experience with CI/CD pipelines such as Concourse, Jenkins.
Google Cloud Platform certification is a plus.
Perks and benefits
Variable pay, WiFi reimbursement, Cab facilities, shift allowance (1PM-10PM)","Airflow, Cloud Functions, Concourse, Cloud Dataflow, Cloud DataProc, CI CD pipelines, Cloud PubSub, Cloud BigQuery, Google Cloud Platform, Sql, Jenkins, Spark, Apache Beam, Python"
Data Engineer,NTT DATA North America,Fresher,,"Bengaluru, India",Login to check your skill match score,"Req ID: 321499
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties:
Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.
Work closely with Data modeller to ensure data models support the solution design
Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.
Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.
Develop documentation and artefacts to support projects
Minimum Skills Required:
ADF
Fivetran (orchestration & integration)
SQL
Snowflake DWH
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com
NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Snowflake DWH, Fivetran, Adf, Sql"
Data engineer,Qloron Pvt Ltd,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Senior Data Engineer
Experience: Minimum 5+ Years
Employment Type: Full-Time
Job Summary
We are seeking a skilled and experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in Power BI development, data modeling, and SQL, with the ability to translate business requirements into actionable insights. You will be responsible for end-to-end dashboard development, managing junior developers, and optimizing performance across reports and dashboards.
Key Responsibilities
Analyze business requirements and translate them into data models and reporting solutions.
Perform GAP analysis between existing data models and business needs.
Design and model efficient Power BI schemas and architecture.
Transform and prepare data using Power BI, SQL, and ETL tools.
Develop complex DAX formulas, measures, and calculated columns for analytics.
Create visually appealing and functional Power BI reports and dashboards.
Write SQL queries and stored procedures to retrieve and manage data effectively.
Design robust Power BI solutions aligned with business objectives.
Lead and guide a team of Power BI developers, ensuring high-quality deliverables.
Integrate data from multiple sources into Power BI for holistic analysis.
Optimize the performance of Power BI dashboards and reports.
Collaborate with business stakeholders to align deliverables with strategic goals.
Required Skills
Minimum 5+ years of hands-on experience with Power BI development.
Strong proficiency in DAX, Power Query, and Power BI Service.
Excellent command of SQL, including stored procedures.
Proven experience in data modeling, data transformation, and ETL processes.
Strong understanding of data warehousing concepts (mandatory).
Experience working with multiple data sources and integrating them within Power BI.
Leadership capabilities to manage and mentor junior developers.
Solid communication and stakeholder management skills.
Preferred Qualifications
Bachelor's Degree in Computer Science, Information Technology, or equivalent.
Knowledge of Data Engineering concepts is a plus.
Experience with cloud platforms such as Azure or AWS is advantageous.","Data Modeling, Power Bi, Data Warehousing, Power Query, Dax, Sql, Etl"
Data Engineer,Knight Frank India,2-5 Years,,"Mumbai, India",Real Estate,"Responsibilities
To work with the multiple teams to extend the team's data integration, modelling and reporting capabilities
Develop and Maintain Scalable Data Pipelines: Design and implement robust data pipelines using Azure Data Bricks (ADB) and Microsoft Fabric to efficiently process and transform large datasets, ensuring data quality and timely delivery across various business units.
Provide solution and integration development requirements, functional specs, design, custom development, integration, testing, and deployment
Leverage Azure DevOps and Git to establish and maintain Cl/CD pipelines for automating the deployment
and monitoring of data integration workflows, ensuring smooth updates and version control across environments.
To be able to work with a degree of independence on projects
To ensure easy access to data for the wider team, collaborating with stakeholders and Knight Frank teams on projects, analysing and cleansing complex datasets, and advising on best practices for data integration and modelling.
Develop and maintain scalable RESTful APIs for seamless data exchange, leveraging JSON for efficient data ingestion, transformation, and integration across systems, ensuring high performance and reliability.
Experience
Demonstrated 2-3 years of strong experience using T-SQL.
More than 3 years of working experience in developing Data warehouses.
More than 3 years of experience in building ETL/ELT pipelines, serving both on-prem and cloud-based data warehouses.
Working as a data engineer in Azure based projects for not less than 2 years, having an in-depth understanding of the following:
ETL Pipelines developed with Azure Data Bricks & Azure Synapse pipelines
Strong proficiency in Python and PySpark for developing efficient ETL workflows, data processing, and automation of data pipelines in distributed environments.
Hands-on experience with Azure DevOps, Git, and Cl/CD pipeline automation for continuous integration
and delivery of data workflows and infrastructure deployments.
Demonstrated working experience that requires understanding of underlying data structures and different integration methods and uses them to develop quality solutions within agreed timelines.
Demonstrated working with multiple stakeholders across multiple disciplines to understand their data
requirements and enhancing their current architecture.
Having strong troubleshooting and error-handling skillset, along with a keen inclination towards process automation.
Proactive to remain up to date with latest technologies and techniques.
Demonstrated experience in working effectively as part of a team, managing/mentoring individuals - all along showing a strong collaborative mindset and resourcefulness.
Systems
Strong experience with the following
T-SQL & Data Modelling
Azure Data Bricks
SQL Server
Data Security & Governance
Cl/CD & DevOps
REST API & JSON
Experience of the following desirable
Power BI
SSIS
Microsoft Fabric
Azure Synapse
Particular Aptitudes/Skills Required
Highly organized, systematic and adaptable with an excellent attention to detail, with the ability to recognize the relative importance of software issues and to prioritize work effectively
Ability to communicate clearly and deal with others at all levels in a polite, professional, friendly and helpful manner, both face to face, by email and on the telephone and always maintain a good working relationship
Demonstrated an ability to work on multiple projects simultaneously
Ability to work with all members of the team, in a professional and yet dynamic and creative environment, fostering an open, friendly and constructive working relationship with all members of the team
Ability to work within a high-pressure environment, balance priorities and remain calm under pressure
The successful candidate will be flexible, self-motivated, organized and pro-active with excellent computing and administration skills and the ability to adapt to a wide range of tasks. They will also have a hands on attitude and possess the necessary skills, manner and experience to provide an effective support service to the department/office
Desire to learn new technologies and continuously develop new skills and expertise
Ability to work out of hours to deliver application upgrades in agreed maintenance windows","Data Security Governance, Rest Api, Azure Synapse, T-sql, Azure Data Bricks, Pyspark, SQL Server, Json, Python, Azure DevOps"
Data Engineer,PwC Acceleration Centers,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"ole: Associate
Tower: Data, Analytics & Specialist Managed Service
Experience: : 3 -5.5 years
Key Skills: AWS , Snowflake, DBT
Educational Qualification: BE / B Tech / ME / M Tech / MBA
Work Location: Bangalore
Job Description
As a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self-awareness, personal strengths, and address development areas.
Flexible to work in stretch opportunities/assignments.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Ticket Quality and deliverables review, Status Reporting for the project.
Adherence to SLAs, experience in incident management, change management and problem management.
Seek and embrace opportunities which give exposure to different situations, environments, and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Demonstrate leadership capabilities by working, with clients directly and leading the engagement.
Work in a team environment that includes client interactions, workstream management, and cross-team collaboration.
Good team player, take up cross competency work and contribute to COE activities.
Escalation/Risk management.
Position Requirements:
Required Skills:
AWS Cloud Engineer:
Job description:
Candidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas:
Should have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms.
Should have minimum 1-3 years of Operate/Managed Services/Production Support Experience
Should have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc.
Designing and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes.
Should have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc.
Design, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS.
Work together with data scientists and analysts to understand the needs for data and create effective data workflows.
Implementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.
Improve the scalability, efficiency, and cost-effectiveness of data pipelines.
Monitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage.
Implementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Should have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools
Scaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments.
Should have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc.
Should have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc.
Should have Strong communication, problem solving, quantitative and analytical abilities.
Nice to have:
AWS certification
Managed Services- Data, Analytics & Insights Managed Service
At PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients better.
Within our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client's enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC's Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today's dynamic business environment.
Within our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective.
As a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.
Kindly share your resume to [HIDDEN TEXT]
Regards,
Mirunalini MJ","AWS DMS, Prefect, snowflake, dbt, Aws Lambda, Hadoop, Collibra, Pyspark, AWS Glue, Informatica, Sql, Spark, Python, AWS"
Data Engineer,Tredence Inc.,1-3 Years,,"Bengaluru, India",Login to check your skill match score,"Data Engineer
Experience Level: 12 months to 16 months only (Excluding internship)
Job location- Bengaluru, Chennai, Gurgaon, Kolkata, Pune, Hyderabad
B.Tech/BE/ME/M.Tech- 2023 & 2024 Grads only. Rest can avoid please
MUST HAVE
Minimum of 12 months of Data Engineering experience.
Strong technical knowledge of tools like Azure Data Factory/Databricks/GCP/Snowflake, SQL, Python
. Experience in collaborating with business stakeholders to identify and meet data requirements
Experience in using Azure services and tools to ingest, egress, and transform data from multiple sources
Delivered ETL/ELT solutions including data extraction, transformation, cleansing, data integration and data management
Implemented batch & near real time data ingestion pipelines
Experience in working on Event-driven cloud platform for cloud services and apps, Data integration for building and managing pipeline, Data warehouse running on serverless infrastructure, Workflow orchestration using Azure Cloud Data Engineering components Databricks, Synapse, etc.
Excellent written and oral communication skills
GOOD TO HAVE Proven ability to work with large cross functional teams with good communication skills.
Experience on Cloud migration methodologies and processes
Azure Certified Data Engineer
Exposure to Azure Dev ops and Github
Ability to drive customer calls independently
Ability to take ownership and work with team","snowflake, Workflow orchestration, Azure services, Event-driven cloud platform, Synapse, Azure Cloud components, Databricks, Data Integration, Sql, ELT, Azure Data Factory, Data Management, Etl, Python, Gcp"
Data Engineer,Catalyst Clinical Research,3-5 Years,,India,Login to check your skill match score,"Catalyst Clinical Research provides customizable solutions to the biopharmaceutical and biotechnology industries through Catalyst Oncology, a full-service oncology CRO,andmulti-therapeutic global functional and CRO services through Catalyst Flex. The company's customer-centric flexible service model, innovative technology, expert team members, and global presence advance clinical studies. Visit CatalystCR.com
The Data Engineer is a key member of the Data Engineering Team responsible for performing tasks related to all aspects of DataOps lifecycle. The products and services you build will enable analytical applications, AI products, and integrations enterprise systems. You--along with your teammates-- will work with the internal and external stakeholders to turn requirements into solutions that will drive better decision making through the organization. You will develop marts, warehouses, models, and logic that contain and distill the intricate innerworkings of Catalyst through data; you will work with the Associate Director, Enterprise Data Architecture to ensure this fit within the overall analytics framework.
Design, build, and maintain scalable data pipelines using Databricks Delta, and structured streaming with Delta Live Tables.
Manage Unity Catalog for efficient data governance across multiple domains, regions, and end users.
Develop and manage transformations in dbt and Databricks using Medallion/Multi-Hop Architecture, ensuring best practices in code quality and data modeling.
Manage DAG workflows using systems like Airflow or Databricks Workflows to optimize data processing tasks.
Build upon our CI/CD strategies on GitLab/GitActions for automated testing and deployment of data artifacts.
Collaborate with cross-functional teams across different regions to ensure highly-availability and quality data integration.
Education: B.S. or M.S. Computer Science, Engineering, Economics, Mathematics, related field, or relevant experience
Experience
3+ years of Data Engineering experience, including Webhooks, API, ELT/ETL, Data Lakehouse Architecture, and Event-Driven Architectures.
3+ years of Data Architecture experience, including data modeling for semantic layers, normalization forms and OBT.
3+ years of experience with cloud computing technologies (Azure, AWS, GCP)
3+ years of experience with the Databricks Data Intelligence platform
Working knowledge of UX design methods as it relates to analytical and AI platforms.
Prior experience with project management tools such as JIRA.
Required Certifications: N/A
Required Skills;
Proficient in Python or PySpark
Proficient in SQL/ Spark SQL
Solid understanding of cloud computing environments like (Azure, AWS, GCP)
Knowledge of Big Data technologies (Spark)
Solid exposure to Delta Live Tables and Databricks Workflows.
Solid understanding of Data Lakehouse design
Solid understanding of code modularization strategies for Jupyter notebook -style coding
Adept with data structures such as delta, parquet, YAML, XML, JSON, and HTML
Proficient in the administration of the Databricks platform
Strong organizational, problem-solving, and analytical skills
Ability to manage priorities and workflow.
Proven ability to handle multiple projects and meet deadlines.
Strong interpersonal skills.
Ability to deal effectively with a diversity of individuals at all organizational levels.
Commitment to excellence and high standards.
Creative, flexible, and innovative team player.
Ability to work independently and as a member of various teams and committees.
Nice To Have
Demonstrated experience working with MLFlow, create feature stores, develop using vector databases, or graph databases.
Familiarity with SDTM, FHIR, HL7 & SMART.
Working Hours
Every day: 1:30 PM - 9:00 PM IST
OR
Monday, Wednesday, Friday: 2:30 PM - 10:30 PM IST
Tuesday, Thursday: 9:00 AM - 5:00 PM IST
Note: Working hours may vary based on individual seniority, business demand, and ability to work independently. This will be evaluated on a case-by-case basis.
PI265776752","Airflow, GitLab GitActions, SQL Spark SQL, dbt, Big Data technologies Spark, Delta Live Tables, Python or PySpark, Data Lakehouse design, Databricks"
Data Engineer,ThoughtSpot,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"We are seeking an experienced Data Engineer to join our AI search team. As a Senior Data Engineer at ThoughtSpot, you will be responsible for designing, building, and maintaining the data infrastructure that powers our analytics platform and underlying Natural Language Search engine . You will work closely with data scientists, software engineers, and product managers to ensure our data systems are robust, scalable, and efficient. We have a rapidly expanding list of happy customers who love our product and we're growing to serve even more
What You'll Do
Design, develop, and maintain scalable data pipelines to process large volumes of data from various sources.
Working closely with our product teams and ML engineers to build features that directly impact the robustness and accuracy of our search infrastructure and NLS engine
Ensure data quality and consistency through rigorous testing and validation processes.
Monitor and troubleshoot data pipeline performance and resolve any issues
What You Bring
6+ years of experience in python, building data infra and pipelines for product companies
Proficiency in programming languages such as Python and data processing libraries such as Pandas and Numpy. Experience with Java is good to have.
Experience building and maintaining large data pipelines, data infrastructure, (Kubernetes preferred) and Machine learning infrastructure
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases/warehouses
Working knowledge of message queuing, stream processing, and highly scalable data stores
Experience working with data tools: Spark, Kafka, etc.
Experience with data pipeline and workflow management tools like Dagster or Airflow with code and no-code ETL implementations
Experience with cloud services such as AWS, GCP, Azure etc
Knowledge of data warehousing concepts and experience with databases like Snowflake, Redshift with experience with data model design, and optimization for data warehousing
Understanding of data governance, data quality, data anonymization and security best practices.
Advanced knowledge of development good practices such as testing, code reviews and git.
Experience with monitoring, tracing tools and alerting tools.
Strong problem-solving skills and the ability to work independently and as part of a team.
You love building and leading exceptional teams in a fast-paced, entrepreneurial environment. You have a strong bias for action and being resourceful
You are obsessed with the customer experience and uncompromising engineering standards
Bring amazing problem-solving skills and an ability to identify, quantify, debug, and remove bottlenecks and functional issues
Great communication skills, both verbal and written, and an interest in working with a diverse set of peers and customers
Alignment with ThoughtSpot Values
What makes ThoughtSpot a great place to work
ThoughtSpot is the experience layer of the modern data stack, leading the industry with our AI-powered analytics and natural language search. We hire people with unique identities, backgrounds, and perspectivesthis balance-for-the-better philosophy is key to our success. When paired with our culture of Selfless Excellence and our drive for continuous improvement (2% done), ThoughtSpot cultivates a respectful culture that pushes norms to create world-class products. If you're excited by the opportunity to work with some of the brightest minds in the business and make your mark on a truly innovative company, we invite you to read more about our mission, and apply to the role that's right for you.
ThoughtSpot for All
Building a diverse and inclusive team isn't just the right thing to do for our people, it's the right thing to do for our business. We know we can't solve complex data problems with a single perspective. It takes many voices, experiences, and areas of expertise to deliver the innovative solutions our customers need. At ThoughtSpot, we continually celebrate the diverse communities that individuals cultivate to empower every Spotter to bring their whole authentic self to work. We're committed to being real and continuously learning when it comes to equality, equity, and creating space for underrepresented groups to thrive. Research shows that in order to apply for a job, women feel they need to meet 100% of the criteria while men usually apply after meeting 60%. Regardless of how you identify, if you believe you can do the job and are a good match, we encourage you to apply.","Airflow, Machine Learning Infrastructure, Data Anonymization, Alerting Tools, Tracing Tools, snowflake, Dagster, Security Best Practices, Kafka, Redshift, Sql, Data Quality, Numpy, Pandas, Gcp, Spark, Data Warehousing, Data Governance, Azure, Python, Kubernetes, AWS"
Data Engineer,Aryng,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Welcome! You made it to the job description page!
Aryng is looking for a Data Engineer with experience in developing enterprise-class distributed data engineering solutions on the cloud. We are seeking an entrepreneurial and technology-proficient Data Engineer who is an expert in the implementation of a large-scale, highly efficient data platform, batch, and real-time pipelines and tools for Aryng clients. This role is based out of India. You will work closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength is creative and effective problem-solving.
Key Responsibilities:
Should have implement asynchronous data ingestion, high volume stream data processing, and real-time data analytics using various Data Engineering Techniques
Implement application components using Cloud technologies and infrastructure
Assist in defining the data pipelines and able to identify bottlenecks to enable the adoption of data management methodologies
Implementing cutting edge cloud platform solutions using the latest tools and platforms offered by GCP, AWS, and Azure. (AWS is preferred)
Functional capabilities: Requirement gathering, Client Mgt, team handling, Program delivery, Project Management (Project Estimation, Scope of Project, Agile methodology).
Requirements
3-5 years of data engineering experience is a must
3+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers. AWS is preferred.
Should be comfortable working and interacting with clients
2+ years experience in Python
Must be strong in SQL and its concepts
Experience in Big Query, Snowflake, Redshift, DBT
Strong understanding of data warehousing, data lake, and cloud concepts
Excellent communication and presentation skills
Excellent problem-solving skills, highly proactive and self-driven
Consulting background is a big plus
Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or a related area of study
Working knowledge of Airflow is preferred
Good to have:
Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
Experience in Tableau, Looker, or other BI tools is preferred
Availability:
Available to join immediately
This role requires mandatory overlap hours with clients in the US from 8 am to 1 pm PST.
Benefits
Direct Client Access
Flexible work hours
Rapidly Growing Company
Awesome work culture
Learn From Experts
Work-life Balance
Competitive Salary
Executive Presence
End to End Problem Solving
50%+ Tax Benefit
100% Remote company
Flat Hierarchy
Opportunity to become a thought leader
Why Join Aryng: Click on the Youtube link","Big Query, Airflow, dbt, snowflake, Data Lake, Data Warehousing, Redshift, Sql, Python"
Data Engineer,Rakuten Symphony,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"About Us
All people need connectivity.
The Rakuten Group is reinventing telecom by greatly reducing cost, rewarding big users not penalizing them, empowering more people and leading the human centric AI future.
The mission is to connect everybody and enable all to be.
Rakuten. Telecom Invented.
Job Description
Job Title - Data Engineer
Location - Bangalore (Onsite)
Why should you choose us
Rakuten Symphony is a Rakuten Group company, that provides global B2B services for the mobile telco industry and enables next-generation, cloud-based, international mobile services. Building on the technology Rakuten used to launch Japan's newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Let's build the future of mobile telecommunications together!
What will you do
Our Data Platform team is building a world-class autonomous data service platform to cater services such as data lake as a service, database as a service, data transformation as a service and AI services.
We are looking for a Data Engineer to help us build functional systems for our data platform services.
For our autonomous data platform services as a Data Engineer you will be responsible for the end-to-end research and development of relevant features for the platform following the product lifecycle of the offerings.
If you have in-depth knowledge in Spark , NiFi and other distributed systems, we'd like to meet you.
Ultimately, you will develop various self-managed and scalable services of the data platform which will be offered as cloud services to the end users.
Roles And Responsibilities
Work experience as a Data Engineer or similar software engineering role (3-8 years)
Good knowledge of NiFi, Spark and distributed eco systems, knowledge of Kubernetes application development, how to make K8 centric applications is a plus
Expertise in development using Java/Scala Not python
Must be able to quickly design and implement tolerant and highly available pipelines using distributed eco systems and should have hands-on experience with any NoSQL DB, Presto/Trino, NiFi, and Airflow Casandra
Sound knowledge of Spring frame work and spring frame work related solutions for web application development
Problem-solving attitude, tinkering approach, and creative thinking
Our Commitment To You
Rakuten Group's mission is to contribute to society by creating value through innovation and entrepreneurship. By providing high-quality services that help our users and partners grow,
We aim to advance and enrich society.
To fulfill our role as a Global Innovation Company, we are committed to maximizing both corporate and shareholder value.
Job Requirement
Responsibilities
Research and comparative analysis
System architecture design
Implement integrations
Deploy updates and fixes
Perform root cause analysis for production errors
Investigate and resolve technical issues
Architecture and support documentation
Requirements
Work experience as a Data Engineer or similar software engineering role (3-8 years)
Good knowledge of NiFi, Spark and distributed eco systems, knowledge of Kubernetes application development, how to make K8 centric applications is a plus
Expertise in development using Java/Scala Not python
Must be able to quickly design and implement tolerant and highly available pipelines using distributed eco systems and should have hands-on experience with any NoSQL DB, Presto/Trino, NiFi, and Airflow Casandra
Sound knowledge of Spring frame work and spring frame work related solutions for web application development
Problem-solving attitude, tinkering approach, and creative thinking","Airflow, NoSQL DB, Trino, NiFi, Java, Presto, Scala, Spark, Kubernetes"
Data Engineer,Ideas2IT Technologies,Fresher,,"Chennai, India",Login to check your skill match score,"Job Role
As a Data Engineer, you'll build and maintain data pipelines and architectures.Responsibilities include optimizing databases and ETL processes, using Python or SQL,and collaborating with data teams for informed decision-making.
Why Choose Ideas2IT
Ideas2IT has all the good attributes of a product startup and a services company. Since we launch our products, you will have ample opportunities to learn and contribute. However, single-product companies stagnate in the technologies they use. In our multiple product initiatives and customer-facing projects, you will have the opportunity to work on various technologies.
AGI is going to change the world. Big companies like Microsoft are betting heavily on this (see here and here). We are following suit. As a Data Engineer, exclusively focus on engineering data pipelines for complex products
What's in it for you
A robust distributed platform to manage a self-healing swarm of bots onunreliable network / compute
Large-scale Cloud-Native applications
Document Comprehension Engine leveraging RNN and other latest OCR techniques
Completely data-driven low-code platform
You will leverage cutting-edge technologies like Blockchain, IoT, and Data Science as you work on projects for leading Silicon Valley startups.
Your role does not start or end with just Java development; you will enjoy the freedom to share your suggestions on the choice of tech stacks across the length of the project
If there is a certain technology you would like to explore, you can do your Technical PoCs
Work in a culture that values capability over experience and continuous learning as a core tenet
Here's what you'll bring
Proficiency in SQL and experience with database technologies (e.g., MySQL, PostgreSQL, SQL Server).Experience in any one of the cloud environments AWS, Azure
Experience with data modeling, data warehousing, and building ETL pipelines.
Experience building large-scale data pipelines and data-centric applications using any distributed storage platform
Experience in data processing tools like Pandas, pyspark.
Experience in cloud services like S3, Lambda, SQS, Redshift, Azure Data Factory, ADLS, Function Apps, etc.
Expertise in one or more high-level languages (Python/Scala)
Ability to handle large-scale structured and unstructured data from internal and third-party sources
Ability to collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision-making across the organization
Experience with data visualization tools like PowerBI, Tableau
Experience in containerization technologies like Docker , Kubernetes","Function Apps, ADLS, S3, Scala, PostgreSQL, SQL Server, Pyspark, Tableau, Redshift, Sql, Lambda, Azure Data Factory, Pandas, Docker, Powerbi, MySQL, Sqs, Azure, Kubernetes, Python, AWS"
Data Engineer,DePronto Infotech,2-4 Years,,"Mumbai, India",Login to check your skill match score,"Company Description
DePronto Infotech is a Software Development Company operating in India and UK, specializing in high-performance front-end technologies, API Development, Data Engineering services, and Customized application development focused on Analytics and Automation with respect to Risk & Compliance domain. With over 15+ years of experience in the BFSI domain, DePronto Infotech partners with reputed clients for governance, risk, and compliance solutions. The company emphasizes employee development to keep pace with new trends and technologies, aiming to offer economical business solutions through innovation. The firm's vision is to become the preferred provider of technology solutions for its clients.
Role Description
This is a full-time remote role for a Data Engineer. The Data Engineer will be responsible for designing, developing, and managing robust data solutions with a solid foundation in SQL. Day-to-day tasks will include data modeling, Extract Transform Load (ETL) processes, data warehousing, and data analytics. The Data Engineer will work closely with stakeholders to understand data requirements and ensure data integrity and quality.
Key Responsibilities:
Design, create, and maintain:
SQL queries, scripts, views, stored procedures, functions, and triggers to support analytics and operational workflows.
Data models for OLTP/OLAP systems using best practices (e.g., normalization, indexing, partitioning).
ETL/ELT pipelines using SQL and orchestration tools (e.g., Airflow, dbt).
Optimize SQL queries and database performance through indexing, partitioning, and query refactoring.
Ensure data integrity, accuracy, and consistency across systems.
Collaborate with data analysts, engineers, and business users to understand requirements and translate them into technical solutions.
Participate in schema evolution, version control of SQL code, and automated deployments (CI/CD).
Monitor data pipelines and implement automated alerts and logging.
Mandatory Skills:
Advanced SQL (window functions, CTEs, joins, subqueries, dynamic SQL)
Strong experience with:
Stored Procedures, Functions, Triggers
Views and Materialized Views
Writing and optimizing complex queries
Relational databases: PostgreSQL, MySQL, or SQL Server
Data modeling for transactional and analytical systems
ETL/ELT processes and orchestration using tools like Airflow, dbt, or Apache NiFi
Version control with Git
Experience working with large datasets and performance tuning
Preferred Skills / Modern Tech Stack Exposure:
Cloud Platforms: AWS (Redshift, RDS, Athena, S3), GCP (BigQuery), Azure SQL
Data Lakes and Delta Lake technologies
NoSQL exposure: MongoDB, DynamoDB (bonus)
Python or Scala for scripting data workflows
Familiarity with event-driven data architectures (Kafka, Kinesis)
Experience with DataOps, CI/CD pipelines, and Infrastructure as Code (e.g., Terraform)
Exposure to BI tools: Looker, Power BI, Tableau
Experience with data catalogs, data quality tools, and observability frameworks
Qualifications:
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
2+ years of experience in a Data Engineering or similar role with strong SQL focus.
Strong problem-solving and communication skills.
Ability to work independently and in a cross-functional team.
Nice to Have:
Certification in cloud data engineering (e.g., AWS Data Analytics Specialty, Google Professional Data Engineer)
Knowledge of data privacy and compliance frameworks (GDPR, HIPAA)","Airflow, Looker, dbt, PostgreSQL, Kafka, Data Modeling, Tableau, Data Analytics, ELT, Kinesis, Terraform, MySQL, Python, AWS, Power Bi, Scala, Dynamodb, SQL Server, Data Warehousing, Sql, Git, Gcp, MongoDB, Azure, Etl"
Data Engineer,Chiselon Technologies Pvt Ltd,Fresher,,"Chennai, India",Login to check your skill match score,"Qualifications and Skills
Proficiency in Python, SQL, and Snowflake (Mandatory skill) for developing robust data solutions.
Expertise in working with big data frameworks and technologies for managing and processing large datasets.
Experience with Microsoft Azure for cloud-based data storage and processing solutions.
Strong understanding of data modeling principles to design efficient and scalable data architectures.
Hands-on experience with ETL and ELT processes to ensure seamless data integration and transformation.
Proficiency in PySpark for distributed data processing and analysis across large datasets.
Solid problem-solving skills with the ability to troubleshoot complex data-related issues effectively.
Excellent communication skills to collaborate with cross-functional teams and stakeholders efficiently.
Roles and Responsibilities
Design, construct, and maintain scalable data pipelines to support various business needs and analytics.
Collaborate with data scientists and analysts to understand data requirements and deliver suitable solutions.
Optimize and monitor data workflows to ensure high performance and reliability of data systems.
Work with engineering teams to integrate data solutions seamlessly with existing systems and applications.
Implement best practices in data management, including data quality, integrity, and security measures.
Stay updated with industry trends and advancements to continually improve data engineering processes.
Deliver data-driven insights and recommendations to drive strategic business decisions.
Troubleshoot and resolve data-related issues promptly to maintain system integrity and performance.","Big Data frameworks, snowflake, Data modeling principles, Pyspark, Microsoft Azure, Python, Sql, ELT, Etl"
Data Engineer,Alight Solutions,5-7 Years,,"Gurugram, India",Login to check your skill match score,"Our story
At Alight, we believe a company's success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to Be Alight.
Our Values:
Champion People be empathetic and help create a place where everyone belongs.
Grow with purpose Be inspired by our higher calling of improving lives.
Be Alight act with integrity, be real and empower others.
It's why we're so driven to connect passion with purpose. Alight helps clients gain a benefits advantage while building a healthy and financially secure workforce by unifying the benefits ecosystem across health, wealth, wellbeing, absence management and navigation.
With a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.
Join our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.
Learn more at careers.alight.com .
Alight is seeking a skilled and passionate Data Engineer to join our team. As the Data Engineer, you will be a member of a team responsible for various stages of data pipelines development, including understanding business requirements, coding, testing, documentation, deployment, and production support. As part of the Data Analytics team, you will focus on delivering high-quality enterprise caliber systems on Alight's Data Lake. Your primary role will involve participating in full life-cycle data ingestion and data architecting development projects.
Qualifications:
Knowledge & Experience:
5+ years of data integration, data warehousing or data conversion experience.
3+ years of data modeling experience.
3+ years of Informatica IICS experience.
3+ years working with AWS Redshift.
2+ years AWS step functions, Lambda, Glue, and other Cloud Data Lake Services
1+ SSIS / SSRS experience
Excellent analytical and critical thinking skills
Strong interpersonal skills with the ability to work effectively with diverse and remote teams
Experience in agile processes and development task estimation
Strong sense of responsibility for deliverables
Ability to work in a small team with moderate supervision
Responsibility Areas:
Design data ingestion solutions for small to medium complexity requirements independently, adhering to existing standards
Develop high-priority and highly complex solutions for systems based on functional specifications, detailed design, maintainability, and coding and efficiency standards, working independently.
Estimate and evaluate risks, and prioritize technical tasks based on requirements
Collaborate actively with Data Engineering Lead, Product Owners, Quality Assurance, and stakeholders to ensure high-quality project delivery
Conduct formal code reviews to ensure compliance with standards
Utilize appropriately system design, development, and process standards
Create, maintain, and publish system-level documentation, including system diagrams, with minimal guidance
Ensure clarity, conciseness, and completeness of requirements before starting development, collaborating with Business Analysts and stakeholders to evaluate feasibility. Take primary accountability for meeting non-functional requirements.
Alight requires all virtual interviews to be conducted on video.
Flexible Working
So that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and Top 100 Company for Remote Jobs 5 years in a row.
Benefits
We offer programs and plans for a healthy mind, body, wallet and life because it's important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as several voluntary benefit options.
By applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight's employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.
Our commitment to Inclusion
We celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.
At Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.
Authorization to work in the Employing Country
Applicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.
Note, this job description does not restrict management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.
We offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.
DISCLAIMER:
Nothing in this job description restricts management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.","Data Conversion, AWS step functions, Cloud Data Lake Services, Informatica IICS, Aws Redshift, Ssrs, Data Warehousing, Data Modeling, Data Integration, SSIS"
Data Engineer,Azoon Tech Consulting LLC,Fresher,,"Hyderabad, India",Login to check your skill match score,"Join a World-Class AI Startup at the Forefront of Agent Technology
We're a stealth-mode AI startup, founded by globally recognized executive leaders with a proven history of building game-changing technologies. We're now building a world-class engineering team to shape the future of AI agentsand this is your chance to get in at the ground floor.
We offer a rare opportunity to work on cutting-edge AI infrastructure, learn fast, and grow alongside some of the best minds in the industry. This isn't just a jobit's a chance to build something meaningful and be rewarded for your impact.
What's in it for you:
Work directly on the next generation of AI agent technology
Collaborate in a high-impact, fast-paced environment
Flexible location options with a hybrid/remote-friendly culture
We're looking for engineers with hands-on expertise in:
AWS Cloud, Apache Spark, Airflow, Python, and FastAPI
Framework and system design
Apache Spark performance tuning and optimization
If you're excited about building from scratch, solving deep technical problems, and making a real impactyou'll thrive here.
Qualifications
Bachelor's in Computer Science","Airflow, Apache Spark performance tuning and optimization, Framework and system design, Apache Spark, Aws Cloud, FastAPI, Python"
Data Engineer,Aditi Consulting,5-8 Years,,"Bengaluru, India",Login to check your skill match score,"We are hiring Data Engineer for Bangalore Location
Location: Bangalore
Experience : 5 8 Yrs
Working Model : Hybrid
EXPERIENCE:
5 - 8 years preferred experience in a data engineering role.
Minimum of 4 years of preferred experience in Azure data services (Data Factory, Databricks, ADLS, SQL DB, etc.)
EDUCATION:
Minimum Bachelor's Degree in Computer Science, Computer Engineering or in STEM Majors (Science, Technology, Engineering, and
Math)
SKILLS/REQUIREMENTS:
Strong working knowledge of Databricks, ADF.
Expertise working with databases and SQL.
Strong working knowledge of code management and continuous integrations systems (Azure DevOps or Github)
preferred
Familiarity with Agile delivery methodologies
Familiarity with NoSQL databases (such as MongoDB) preferred.
Any experience on IoT Data Standards like Project Haystack, Brick Schema, Real Estate Core is an added advantage
Ability to multi-task and reprioritize in a dynamic environment.
Outstanding written and verbal communication skills","Sql, Databricks, Github, Azure DevOps"
Data Engineer,Klodev,6-10 Years,,"Coimbatore, India",Login to check your skill match score,"Design, develop, and maintain data pipelines and ETL processes using PySpark or Scala.
Work extensively with Python to build scalable data solutions.
Should be strong in SQL
Develop and manage data workflows in Azure Data Factory (ADF).
Implement and optimize data solutions on cloud platforms such as Azure or AWS.
Ensure data integrity, performance, and security across data pipelines.
Collaborate with data scientists, analysts, and other stakeholders to support business needs.
Lead and mentor a team of junior data engineers (if applicable).
Requirements
6 to 10 years of experience in data engineering or related roles.
Strong hands-on experience with PySpark or Scala.
Proficiency in Python for data processing and automation.
Experience working with Azure Data Factory (ADF).
Hands-on experience with cloud platforms (Azure or AWS).
Strong understanding of ETL processes, data warehousing, and big data technologies.
Experience in leading a team is a plus.","ETL processes, Scala, Pyspark, Big Data Technologies, Data Warehousing, Azure, Python, Sql, AWS"
Data Engineer,TELUS Digital,4-6 Years,,"Noida, India",Login to check your skill match score,"Requirements
Description and Requirements
Exploring bleeding-edge technologies, tools and frameworks to experiment with and build better products for existing customers.
Evaluating areas of improvement with technical products built and implementing ideas which will make us better than yesterday.
Collaborating with developers to work on technical designs and develop code, configurations, and scripts to enhance the development lifecycle and integrate systems.
Collaborate proactively and respectfully with our team and customers
Develop tools and integrations to support other developers in building products.
Take solutions from concept to production by writing code, configurations, and scripts.
Improve existing platforms or implement new features for any of our products.
Create comprehensive documentation for implemented solutions, including implementation details and usage instructions.
Promote our culture of focus, flow, and joy to gain developers support for our solution
Qualifications
Build Data pipelines required for optimal extraction, anonymization, and transformation of data from a wide variety of data sources using SQL, NoSQL and AWS big data technologies. Streaming Batch
Work with stakeholders including the Product Owners, Developers and Data scientists to assist with data-related technical issues and support their data infrastructure needs.
Ensure that data is secure and separated following corporate compliance and data governance policies.
Take ownership of existing ETL scripts, maintain and rewrite them in modern data transformation tools whenever needed.
Being an automation advocate for data transformation, cleaning and reporting tools.
You are proficient in developing software from idea to production
You can write automated test suites for your preferred language
You have frontend development experience with frameworks such as React.js/Angular
You have backend development experience building and integrating with REST APIs and Databases using languages such as Java Spring, JavaScript on Node.js, Flask on Python.
You have experience with cloud-native technologies, such as Cloud Composer, Dataflow, Dataproc, BigQuery, GKE, Cloud run, Docker, Kubernetes, and Terraform.
You have used cloud platforms such as Google Cloud/AWS for application hosting.
You have used and understand CI/CD best practices with tools such as GitHub Actions, GCP Cloud Build.
You have experience with YAML and JSON for configuration.
You are up-to-date on the latest trends in AI Technology
Additional Job Description
Great-to-haves
4+ years of experience as a data or software engineer
4+ years of experience in SQL and Python
2+ years of experience with ELT/ETL platforms (Airflow, DBT, Apache Beam, PySpark, Airbyte)
2+ years of experience with BI reporting tools (Looker, Metabase, Quicksight, PowerBI, Tableau)
Extensive knowledge of the Google Cloud Platform, specifically the Google Kubernetes Engine
Experience with GCP cloud data related services ( Dataflow, GCS, Datastream, Data Fusion, Data Application, BigQuery, Data Flow, Data Proc, Dataplex, PubSub, CloudSQL, BigTable)
Experience in health industry an asset
Expertise in Python, Java
Interest in PaLM, LLM usage and LLMOps
Familiarity with LangFuse or Backstage plugins or GitHub Actions
Strong experience with GitHub beyond source control
Familiarity with monitoring, alerts, and logging solutions
Join us on this exciting journey to make Generative AI accessible to all and create a positive impact with technology.
EEO Statement
At TELUS Digital, we enable customer experience innovation through spirited teamwork, agile thinking, and a caring culture that puts customers first. TELUS Digital is the global arm of TELUS Corporation, one of the largest telecommunications service providers in Canada. We deliver contact center and business process outsourcing (BPO) solutions to some of the world's largest corporations in the consumer electronics, finance, telecommunications and utilities sectors. With global call center delivery capabilities, our multi-shore, multi-language programs offer safe, secure infrastructure, value-based pricing, skills-based resources and exceptional customer service - all backed by TELUS, our multi-billion dollar telecommunications parent.
Equal Opportunity Employer
At TELUS Digital, we are proud to be an equal opportunity employer and are committed to creating a diverse and inclusive workplace. All aspects of employment, including the decision to hire and promote, are based on applicants qualifications, merits, competence and performance without regard to any characteristic related to diversity.","Airflow, GCP Cloud Build, Cloud Run, GitHub Actions, Cloud Composer, dbt, GKE, Airbyte, Looker, Metabase, Yaml, Pyspark, Tableau, Json, Angular, Nosql, Javascript, Terraform, Docker, Flask, react.js, Python, AWS, Java, BigQuery, Node.js, Dataproc, Sql, Spring, Quicksight, Powerbi, Apache Beam, DataFlow, Kubernetes"
Data Engineer I,FedEx ACC,2-5 Years,,"Hyderabad, India",Login to check your skill match score,"Responsible for designing and implementing data pipelines, ensuring data quality and accessibility, creating data models, managing ETL processes, optimizing databases, collaborating with stakeholders, utilizing big data tools, implementing real-time data processing, ensuring data governance and security, documenting data architecture, and delivering data-driven insights.
Design and implement data pipelines to collect, process, and store data efficiently.
Ensure data quality, integrity, and accessibility.
Create data models to support business analytics and reporting.
Work with data scientists, analysts, and other stakeholders to understand data needs, and design schemas.
Build and manage ETL (Extract, Transform, Load) processes for data integration and automate data ingestion and transformation tasks.
Administer and optimize databases (both SQL and NoSQL). Monitor database performance and troubleshoot issues.
Collaborate with software engineers to integrate data systems with applications.
Utilize big data tools for processing large datasets, such as Hadoop or Spark.
Implement distributed systems for real-time data processing.
Ensure data governance and security through compliance with data governance policies and data security standards.
Implement data access controls and encryption techniques.
Document data architecture, pipelines, and processes.
Provide reports and insights based on data analysis.
Education: Bachelors degree or equivalent in Computer Science, MIS, or similar discipline.
Accreditation: Specific business accreditation for Business Intelligence.
Experience: Relevant work experience in data engineering based on the following number of years:
Associate: Prior experience not required
Standard I: Two (2) years
Standard II: Three (3) years
Senior I: Four (4) years
Senior II: Five (5) years
Knowledge, Skills And Abilities
Fluency in English
Analytical Skills
Accuracy & Attention to Detail
Numerical Skills
Planning & Organizing Skills
Presentation Skills
Preferred Qualifications
Pay Transparency:
Pay
Additional Details:
FedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.
All qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.
Our Company
FedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World's Most Admired Companies by Fortune magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.
Our Philosophy
The People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.
Our Culture
Our culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970's. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today's global marketplace.","Real-time data processing, Data pipelines, data models, Nosql, Data Security, Data Quality, Data Governance, Sql"
Data Engineer - I,slice,1-3 Years,,"Bengaluru, India",Login to check your skill match score,"About Us
slice, India's leading consumer payments and credit platform, has now merged with North East Small Finance Bank (NESFB), marking a significant step into the banking sector. Trusted by over 18 million Indians, we are on a mission to build the most loved bank in Indiaoffering simple, transparent, and customer-first banking.
About the role:
As a Data Engineer - I, you will play a pivotal role in our mission to drive innovation and excellence in our software solutions. You will be part of a talented team of engineers and work on challenging projects that will expand your horizons and elevate your skills to new heights. Reporting directly to the Engineering Manager, you will be entrusted with crucial responsibilities and given the autonomy to shape the future of our products.
Note: We are inviting Backend Developers working in Data teams of any scalable product companies with similar skillset to apply. This is ahardcore coding role that requires strong technical skills and deep problem-solving abilities.
What You'll Work On:
Trino query governance and abuse prevention
AI-based anomaly detection using Apache Flink
Designing and maintaining self-healing, SLA-driven pipelines
Building an LLMOps platform using Ray and MLflow
Creating zero-SQL tooling for pipeline automation
Optimizing performance and cost across Spark jobs and EKS clusters
Building a central governance layer and organizational data maps
You Should Have:
1-3 years of strong programming experience in Python (PySpark) and/or Golang
Hands-on experience with Kubernetes (EKS) and REST API development using Django or FastAPI
Proficiency in distributed systems and data processing frameworks like Spark and Flink
Experience with Trino, Redpanda, Ray, and MLflow
Working knowledge of Delta Lake on S3 and AWS big data services like EMR and Glue
Familiarity with Superset, Apache Ranger, and low-latency data stores
Strong problem-solving skills and a deep understanding of scalable cloud-native infrastructure
Life at slice:
Life so good, you'd think we're kidding
Competitive salaries. Period.
An extensive medical insurance that looks out for our employees & their dependants. We'll love you and take care of you, our promise.
Flexible working hours. Just don't call us at 3AM, we like our sleep schedule.
Tailored vacation & leave policies so that you enjoy every important moment in your life.
A reward system that celebrates hard work and milestones throughout the year. Expect a gift coming your way anytime you kill it here.
Learning and upskilling opportunities. Seriously, not kidding.
Good food, games, and a cool office to make you feel like home.
An environment so good, you'll forget the term colleagues can't be your friends.","Apache Ranger, MLflow, Ray, Flink, EKS, AWS big data services, Superset, Glue, Trino, Redpanda, Delta Lake, Golang, Pyspark, Emr, Django, Rest API Development, Spark, FastAPI, Kubernetes, Python"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role
At BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.
We are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.
Engineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.
Key Responsibilities
Design, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.
Develop data transformation using DBT (Data Build Tool) with SQL or Python.
Design and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.
Develop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.
Ensure data quality and integrity through automated testing and validation using tools like Great Expectations.
Implement all observability requirements in the data pipeline.
Optimize data workflows for performance and scalability.
Performs code and design reviews for tasks done by other team members.
Works with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.
Works on the development of technical standards for the product and platform.
Collaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.
Provide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.
Document data engineering processes and best practices whenever required.
Required Skills And Qualifications
Must have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.
Exposure on leading complex software projects.
Strong programming skills in Python.
Experience with Apache Airflow or any other orchestration framework for data orchestration.
Proficiency in DBT for data transformation and modeling.
Experience with data quality validation tools like Great Expectations or any other similar tools.
Strong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.
Experience with cloud-based data warehouse platform like Snowflake.
Experience working on NoSQL databases like Elasticsearch and MongoDB.
Experience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.
Experience on Cloud platforms like AWS and/or Azure.
Experience working with backend microservices and APIs using Java or C#.
Exposure on message-oriented middleware technologies like Kafka is a plus.
Ability to work collaboratively in a team environment.
Need to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.
Experience with Financial Services application is a plus.
Effective communication skills, both written and verbal.
Bachelor's or master's degree in computer science, Engineering, or a related field.
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
Lead Azure Data Engineer,Aspire Systems,7-9 Years,,"Chennai, India",Login to check your skill match score,"Job Title: Lead Azure Data Engineer
Experience: 7+ Years
Location: Chennai / Bangalore /Hyderabad /Kochi
About Aspire Systems
Aspire Systems is a global technology services firm offering end-to-end digital transformation and product engineering solutions. We are currently seeking an experienced Lead Azure Data Engineer to join our Data & Analytics team and help build scalable, secure, and intelligent data platforms.
Role Overview
As an Azure Data Engineer, you will be responsible for creating, managing, and optimizing data lakes and pipelines using Azure technologies. You will work with internal teams to build high-performance data platforms that support enterprise-grade analytics and reporting.
Key Responsibilities
Design and build data lakes and lakehouses from scratch, and configure existing systems.
Develop and manage Azure Data Factory (ADF) and Synapse pipelines for data ingestion and transformation.
Work on data security implementation across storage and data movement layers.
Use Python, PySpark, and Spark SQL for data transformation and orchestration.
Create and maintain CI/CD pipelines for data workflows.
Understand and work with various file formats: JSON, Parquet, CSV, Excel, and both structured and unstructured datasets.
Collaborate with internal teams to support application integration and performance optimization.
Provide user support and documentation where required.
Key Skills Required
Expertise in Azure Data Lake, Lakehouse Architecture, Synapse Analytics, Databricks, and T-SQL.
Strong experience in ETL/ELT pipelines, working with large volumes of data.
Proficient with SQL Server, Synapse DB, and data warehousing concepts.
Understanding of metadata management, data catalogs, DWH, MPP, OLTP, and OLAP systems.
Familiar with Source Control tools like Git, TFS, or SVN.
Knowledge of Azure Data Fabric, Microsoft Purview, and MDM tools is a plus.
Experience working with non-functional requirements and performance tuning.
Strong communication skills and the ability to work effectively in a collaborative team environment.
Why Aspire
Work on cutting-edge Azure Data & AI solutions.
Collaborate with a global team on enterprise-scale transformation projects.
Growth opportunities into Data Architect or Tech Lead roles.
Continuous learning and certification support.
Interested candidates can share their resume with [HIDDEN TEXT]","MDM tools, OLAP systems, Synapse DB, Microsoft Purview, Azure Data Fabric, ETL ELT pipelines, Synapse Analytics, Azure Data Lake Lakehouse Architecture, data catalogs, Spark SQL, T-sql, Metadata Management, Pyspark, SQL Server, Mpp, Dwh, Databricks, Data Warehousing Concepts, Python, Oltp"
IN_Senior Associate_Azure Data Engineer_Data & Analytics_Advisory_PAN India,PwC India,8-13 Years,,"Ahmedabad, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Analyses current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics Services.
Provide technical leadership and thought leadership as a senior member of the Analytics Practice in areas such as data access & ingestion, data processing, data integration, data modeling, database design & implementation, data visualization, and advanced analytics.
Engage and collaborate with customers to understand business requirements/use cases and translate them into detailed technical specifications.
Develop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL.
Maintain best practice standards for the development or cloud-based data warehouse solutioning including naming standards.
Designing and implementing highly performant data pipelines from multiple sources using Apache Spark and/or Azure Databricks
Integrating the end-to-end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is always maintained
Working with other members of the project team to support delivery of additional project components (API interfaces)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Integrate Databricks with other technologies (Ingestion tools, Visualization tools).
Proven experience working as a data engineer
Highly proficient in using the spark framework (python and/or Scala)
Extensive knowledge of Data Warehousing concepts, strategies, methodologies.
Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably in Databricks).
Hands on experience designing and delivering solutions using Azure including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
Experience in designing and hands-on development in cloud-based analytics solutions.
Expert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required.
Designing and building of data pipelines using API ingestion and Streaming ingestion methods.
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential.
Thorough understanding of Azure Cloud Infrastructure offerings.
Strong experience in common data warehouse modeling principles including Kimball.
Working knowledge of Python is desirable
Experience developing security models.
Databricks & Azure Big Data Architecture Certification would be plus
Mandatory Skill Sets
ADE, ADB, ADF
Preferred Skill Sets
ADE, ADB, ADF
Years Of Experience Required
8-13 Years
Education Qualification
BE, B.Tech, MCA, M.Tech
a
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Android Debug Bridge (ADB), Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure SQL Data Warehouse, Dev-Ops processes, Microsoft Azure Data Analytics Services, Infrastructure as code, Azure Stream Analytics, API ingestion, Azure App Service, Streaming ingestion, Scala, Apache Spark, Azure Databricks, Azure Synapse, Azure Cosmos DB, Azure Data Factory, Data Pipeline, Azure Data Lake, Data Warehousing, Python"
"IN_Senior Associate_Data Engineer(PySpark,Python)_Data & Analytics_Advisory_Bangalore",PwC India,4-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities:
Roles & Responsibilities
Deliver projects integrating data flows within and across technology systems.
Lead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.
Design end-to-end job flows that span across systems, including quality checks and controls.
Create technology delivery plans to implement system changes.
Perform data analysis, data profiling, and data sourcing in relational and Big Data environments.
Convert functional requirements into logical and physical data models.
Assist in ETL development, testing, and troubleshooting ETL issues.
Troubleshoot data issues and work with data providers for resolution; provide L3 support when needed.
Design and develop ETL workflows using modern coding and testing standards.
Participate in agile ceremonies and actively drive towards team goals.
Collaborate with a global team of technologists.
Lead with ideas and innovation.
Manage communication and partner with end users to design solutions.
Required Skills:
Must have: Total experience required 4-10 years (relevant experience minimum 5 years)
5 years of project experience in Python/Shell scripting in Data Engineering (experience in building and optimizing data pipelines, architectures, and data sets with large data volumes).
3+ years of experience in PySpark scripting, including the architecture framework of Spark.
3-5 years of strong experience in database development (Snowflake/ SQL Server/Oracle/Sybase/DB2) in designing schema, complex procedures, complex data scripts, query authoring (SQL), and performance optimization.
Strong understanding of Unix environment and batch scripting languages (Shell/Python).
Strong knowledge of Big Data/Hadoop platform.
Strong engineering skills with the ability to understand existing system designs and enhance or migrate them.
Strong logical data modeling skills within the Financial Services domain.
Experience in data integration and data conversions.
Strong collaboration and communication skills.
Strong organizational and planning skills.
Strong analytical, profiling, and troubleshooting skills.
Good To Have:
Experience with ETL tools (e.g Informatica, Azure Data Factory) and pipelines across disparate sources is a plus.
Experience working with Databricks is a plus.
Familiarity with standard Agile & DevOps methodology & tools (Jenkins, Sonar, Jira).
Good understanding of developing ETL processes using Informatica or other ETL tools.
Experience working with Source Code Management solutions (e.g., Git).
Knowledge of Investment Management Business.
Experience with job scheduling tools (e.g., Autosys).
Experience with data visualization software (e.g., Tableau).
Experience with data modeling tools (e.g., Power Designer).
Basic familiarity with using metadata stores to maintain a repository of Critical Data Elements. (e.g. Collibra)
Familiarity with XML or other markup languages.
Mandatory Skill Sets:
ETL,Python/Shell scripting , building pipelines,pyspark, database, sql
Preferred Skill Sets:
informatica, hadoop, databricks, collibra
Years Of Experience Required:
4 to 10 years
Education Qualification:
Graduate Engineer or Management Graduate
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration
Degrees/Field Of Study Preferred:
Certifications (if blank, certifications not specified)
Required Skills
Extract Transform Load (ETL), Python (Programming Language), Structured Query Language (SQL)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment, Performance Management Software + 16 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","power designer, snowflake, Sybase, Sonar, Hadoop, Collibra, Pyspark, SQL Server, Tableau, Jira, Informatica, Sql, Jenkins, Git, Azure Data Factory, DB2, Shell scripting, Databricks, Oracle, Python, Etl"
IN_Manager_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Manager
Job Description & Summary
At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.
In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary:
We are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.
Key Responsibilities:
Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.
Implement data ingestion and transformation processes to facilitate efficient data warehousing.
Utilize cloud services to enhance data processing capabilities:
AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.
Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.
GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.
Optimize Spark job performance to ensure high efficiency and reliability.
Stay proactive in learning and implementing new technologies to improve data processing frameworks.
Collaborate with cross-functional teams to deliver robust data solutions.
Work on Spark Streaming for real-time data processing as necessary.
Qualifications
7-10 years of experience in data engineering with a strong focus on cloud environments.
Proficiency in PySpark or Spark is mandatory.
Proven experience with data ingestion, transformation, and data warehousing.
In-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP):
Demonstrated ability in performance optimization of Spark jobs.
Strong problem-solving skills and the ability to work independently as well as in a team.
Cloud Certification (AWS, Azure, or GCP) is a plus.
Familiarity with Spark Streaming is a bonus
Preferred Skills:
Python, Pyspark, SQL with (AWS or Azure or GCP)
Mandatory Skill Sets:
Python, Pyspark, SQL with (AWS or Azure or GCP)
Years Of Experience Required:
7-10 years
Education Qualification:
BE/BTECH, ME/MTECH, MBA, MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Bachelor of Engineering, Master of Engineering, Bachelor of Technology
Degrees/Field Of Study Preferred:
Certifications (if blank, certifications not specified)
Required Skills
Microsoft SQL Server, PySpark, Python (Programming Language)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation + 24 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","GCP Dataflow, Pyspark, AWS Glue, Sql, Azure Data Factory, Spark Streaming, Gcp, Spark, Databricks, Azure, Python, AWS"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Bachelor of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline + 38 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql, Etl"
Associate - Data Engineer,BlackRock,5-8 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role
At BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.
We are seeking a highly skilled and motivated Senior level Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.
Engineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.
Key Responsibilities
Design, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.
Develop data transformation using DBT (Data Build Tool) with SQL or Python.
Develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.
Ensure data quality and integrity through automated testing and validation using tools like Great Expectations.
Implement all observability requirements in the data pipeline.
Optimize data workflows for performance and scalability.
Monitor and troubleshoot data pipeline issues, ensuring timely resolution.
Document data engineering processes and best practices whenever required.
Develop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.
Required Skills And Qualifications
Must have 5 to 8 years of experience in data engineering, with a focus on building data pipelines.
Strong programming skills in Python.
Experience with Apache Airflow or any other orchestration framework for data orchestration.
Proficiency in DBT for data transformation and modeling.
Experience with data quality validation tools like Great Expectations or any other similar tools.
Strong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.
Experience with cloud-based data warehouse platform like Snowflake.
Experience working on NoSQL databases like Elasticsearch and MongoDB.
Experience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.
Experience on Cloud platforms like AWS and/or Azure.
Experience working with backend microservices and APIs using Java or C#.
Ability to work collaboratively in a team environment.
Need to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.
Experience with Financial Services application is a plus.
Effective communication skills, both written and verbal.
Bachelor's or master's degree in computer science, Engineering, or a related field.
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Python, Kubernetes, AWS"
IN_Senior Associate_ Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Design, develop, and maintain scalable data pipelines using Azure data services such as Azure Data Factory, Azure Databricks, and Apache Spark.
Implement efficient Extract, Transform, Load (ETL) processes to move and transform data across various sources.
Knowledge about data warehousing concepts
Extensive working experience in Azure Databricks
Utilize Azure SQL Database, Azure Blob Storage, Azure Data Lake Storage, and other Azure data services to store and retrieve data.
Performance optimization and troubleshooting capabilities
Experience in working in pharma and/or healthcare clients will be a plus Technology: SQL, ADF, Azure Databricks, ADLS, Synapse, PySpark Desired Candidate Profile:
B.E./B.Tech. (preferably in Computer Science) or MCA or Statistics/Applied Mathematics
Experience in designing solutions with IaaS, PaaS, and SaaS.
Proficient in data modeling and ETL pipeline implementation.
Strong knowledge of Azure services.
Proven track record in business development and client relationship management.
Excellent communication and interpersonal skills.
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Technology, Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
GCP Dataflow, Good Clinical Practice (GCP)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Blob Storage, Azure SQL Database, Azure Data Lake Storage, Paas, Saas, Apache Spark, Iaas, Data Modeling, Azure Databricks, Sql, Azure Data Factory, Etl"
IN_Senior Associate_Azure Data Engineer_Data & Analytics_Advisory_PAN India,PwC India,8-13 Years,,"Ahmedabad, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Analyses current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics Services.
Provide technical leadership and thought leadership as a senior member of the Analytics Practice in areas such as data access & ingestion, data processing, data integration, data modeling, database design & implementation, data visualization, and advanced analytics.
Engage and collaborate with customers to understand business requirements/use cases and translate them into detailed technical specifications.
Develop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL.
Maintain best practice standards for the development or cloud-based data warehouse solutioning including naming standards.
Designing and implementing highly performant data pipelines from multiple sources using Apache Spark and/or Azure Databricks
Integrating the end-to-end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is always maintained
Working with other members of the project team to support delivery of additional project components (API interfaces)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Integrate Databricks with other technologies (Ingestion tools, Visualization tools).
Proven experience working as a data engineer
Highly proficient in using the spark framework (python and/or Scala)
Extensive knowledge of Data Warehousing concepts, strategies, methodologies.
Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably in Databricks).
Hands on experience designing and delivering solutions using Azure including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
Experience in designing and hands-on development in cloud-based analytics solutions.
Expert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required.
Designing and building of data pipelines using API ingestion and Streaming ingestion methods.
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential.
Thorough understanding of Azure Cloud Infrastructure offerings.
Strong experience in common data warehouse modeling principles including Kimball.
Working knowledge of Python is desirable
Experience developing security models.
Databricks & Azure Big Data Architecture Certification would be plus
Mandatory Skill Sets
ADE, ADB, ADF
Preferred Skill Sets
ADE, ADB, ADF
Years Of Experience Required
8-13 Years
Education Qualification
BE, B.Tech, MCA, M.Tech
a
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Android Debug Bridge (ADB), Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure SQL Data Warehouse, Dev-Ops processes, Microsoft Azure Data Analytics Services, Infrastructure as code, Azure Stream Analytics, API ingestion, Azure App Service, Streaming ingestion, Scala, Apache Spark, Azure Databricks, Azure Synapse, Azure Cosmos DB, Azure Data Factory, Data Pipeline, Azure Data Lake, Data Warehousing, Python"
Azure data engineer,HCLTech,7-9 Years,,"Chennai, India",Login to check your skill match score,"We are seeking an experienced Azure Data Engineer to design, implement, and manage data solutions on the Microsoft Azure platform. The ideal candidate will have extensive experience with Azure services, including Azure Functions, Azure Synapse, Azure Data Factory.
Locations - Chennai
Experience - 7 to 9 Yea
rs
Required Skills and Qualificatio
ns:
Strong expertise in Azure services, including Azure Functions, Azure Synapse, Azure Data Factory, and API Managem
ent.Proficiency in designing and implementing APIs for data integration and acc
ess.Hands-on experience with ETL processes, data modeling, and data warehous
ing.Knowledge of programming languages such as Python, SQL, or
C#.Familiarity with big data technologies like Azure Databricks or Apache Spark is a p
lus.Excellent problem-solving and analytical ski
lls.Strong communication and collaboration abilit
ies.
If interested, please share profile at shraddha_rathor@hcltech.com with below deta
ils -
Total Expe
rienceRelevant Experience as Azure Data En
gineerNotice
Per
iodC
TCECTCCurrent Lo
cationPreferred Lo
cation","ETL processes, Azure Synapse, Azure Data Factory, Azure Functions, Apache Spark, Azure Databricks, Data Modeling, Sql, Python"
IN-Senior Associate_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.
In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary: A career within PWC
Responsibilities:
Job Title: Cloud Data Engineer (AWS/Azure/Databricks/GCP)
Experience:3-8 years in Data Engineering
Job Description:
We are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.
Key Responsibilities:
- Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.
Implement data ingestion and transformation processes to facilitate efficient data warehousing.
Utilize cloud services to enhance data processing capabilities:
-AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.
-Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.
-GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.
Optimize Spark job performance to ensure high efficiency and reliability.
Stay proactive in learning and implementing new technologies to improve data processing frameworks.
Collaborate with cross-functional teams to deliver robust data solutions.
Work on Spark Streaming for real-time data processing as necessary.
Qualifications:
3-8 years of experience in data engineering with a strong focus on cloud environments.
Proficiency in PySpark or Spark is mandatory.
Proven experience with data ingestion, transformation, and data warehousing.
In-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP):
Demonstrated ability in performance optimization of Spark jobs.
Strong problem-solving skills and the ability to work independently as well as in a team.
Cloud Certification (AWS, Azure, or GCP) is a plus.
Familiarity with Spark Streaming is a bonus.
Mandatory skill sets:
Python, Pyspark, SQL with (AWS or Azure or GCP)
Preferred skill sets:
Python, Pyspark, SQL with (AWS or Azure or GCP)
Years of experience required:
3-8 years
Education qualification:
BE/BTECH, ME/MTECH, MBA, MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Technology, Master of Engineering, Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Node.js
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation + 19 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Data Ingestion, Pyspark, Sql, Spark Streaming, Data Pipeline, Gcp, Spark, Databricks, Data Warehousing, Azure, Data Transformation, AWS"
"IN_Senior Associate_Data Engineer(PySpark,Python)_Data & Analytics_Advisory_Bangalore",PwC India,4-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities:
Roles & Responsibilities
Deliver projects integrating data flows within and across technology systems.
Lead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.
Design end-to-end job flows that span across systems, including quality checks and controls.
Create technology delivery plans to implement system changes.
Perform data analysis, data profiling, and data sourcing in relational and Big Data environments.
Convert functional requirements into logical and physical data models.
Assist in ETL development, testing, and troubleshooting ETL issues.
Troubleshoot data issues and work with data providers for resolution; provide L3 support when needed.
Design and develop ETL workflows using modern coding and testing standards.
Participate in agile ceremonies and actively drive towards team goals.
Collaborate with a global team of technologists.
Lead with ideas and innovation.
Manage communication and partner with end users to design solutions.
Required Skills:
Must have: Total experience required 4-10 years (relevant experience minimum 5 years)
5 years of project experience in Python/Shell scripting in Data Engineering (experience in building and optimizing data pipelines, architectures, and data sets with large data volumes).
3+ years of experience in PySpark scripting, including the architecture framework of Spark.
3-5 years of strong experience in database development (Snowflake/ SQL Server/Oracle/Sybase/DB2) in designing schema, complex procedures, complex data scripts, query authoring (SQL), and performance optimization.
Strong understanding of Unix environment and batch scripting languages (Shell/Python).
Strong knowledge of Big Data/Hadoop platform.
Strong engineering skills with the ability to understand existing system designs and enhance or migrate them.
Strong logical data modeling skills within the Financial Services domain.
Experience in data integration and data conversions.
Strong collaboration and communication skills.
Strong organizational and planning skills.
Strong analytical, profiling, and troubleshooting skills.
Good To Have:
Experience with ETL tools (e.g Informatica, Azure Data Factory) and pipelines across disparate sources is a plus.
Experience working with Databricks is a plus.
Familiarity with standard Agile & DevOps methodology & tools (Jenkins, Sonar, Jira).
Good understanding of developing ETL processes using Informatica or other ETL tools.
Experience working with Source Code Management solutions (e.g., Git).
Knowledge of Investment Management Business.
Experience with job scheduling tools (e.g., Autosys).
Experience with data visualization software (e.g., Tableau).
Experience with data modeling tools (e.g., Power Designer).
Basic familiarity with using metadata stores to maintain a repository of Critical Data Elements. (e.g. Collibra)
Familiarity with XML or other markup languages.
Mandatory Skill Sets:
ETL,Python/Shell scripting , building pipelines,pyspark, database, sql
Preferred Skill Sets:
informatica, hadoop, databricks, collibra
Years Of Experience Required:
4 to 10 years
Education Qualification:
Graduate Engineer or Management Graduate
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration
Degrees/Field Of Study Preferred:
Certifications (if blank, certifications not specified)
Required Skills
Extract Transform Load (ETL), Python (Programming Language), Structured Query Language (SQL)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment, Performance Management Software + 16 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","power designer, snowflake, Sybase, Sonar, Hadoop, Collibra, Pyspark, SQL Server, Tableau, Jira, Informatica, Sql, Jenkins, Git, Azure Data Factory, DB2, Shell scripting, Databricks, Oracle, Python, Etl"
IN-Manager_Azure Data Engineer_Data Analytics_Advisory_Bangalore,PwC India,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Operations
Management Level
Manager
Job Description & Summary
At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.
In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary: A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Responsibilities
Must have:
Candidates with a minimum of 5 years of relevant experience for 10-12 years of total experience (Architect / Managerial level).
Deep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.
Must be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)
Strong troubleshooting and debugging skills.
Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.
Experience in writing complex PySpark queries to perform data analysis.
Good To Have
Good to have certifications: Apache Spark 3.0
Experience in any one visualization tool like Power BI, tableau etc.
Understanding of Spark Architecture landscape
Mandatory Skill Sets
Spark, Pyspark, Azure
Preferred Skill Sets
Spark, Pyspark, Azure
Years Of Experience Required
8-12yrs
Education Qualification
B.Tech / M.Tech / MBA / MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Master of Engineering, Bachelor of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation + 23 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Available for Work Visa Sponsorship
Government Clearance Required
Job Posting End Date","Pyspark scripting, Data Bricks, Synapse, Stored Procedures, Azure Analysis Services, Data Factory, Power Bi, Tableau, Python Scripting, Github, Advanced Sql, Azure DevOps"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role
At BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.
We are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.
Engineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.
Key Responsibilities
Design, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.
Develop data transformation using DBT (Data Build Tool) with SQL or Python.
Design and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.
Develop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.
Ensure data quality and integrity through automated testing and validation using tools like Great Expectations.
Implement all observability requirements in the data pipeline.
Optimize data workflows for performance and scalability.
Performs code and design reviews for tasks done by other team members.
Works with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.
Works on the development of technical standards for the product and platform.
Collaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.
Provide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.
Document data engineering processes and best practices whenever required.
Required Skills And Qualifications
Must have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.
Exposure on leading complex software projects.
Strong programming skills in Python.
Experience with Apache Airflow or any other orchestration framework for data orchestration.
Proficiency in DBT for data transformation and modeling.
Experience with data quality validation tools like Great Expectations or any other similar tools.
Strong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.
Experience with cloud-based data warehouse platform like Snowflake.
Experience working on NoSQL databases like Elasticsearch and MongoDB.
Experience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.
Experience on Cloud platforms like AWS and/or Azure.
Experience working with backend microservices and APIs using Java or C#.
Exposure on message-oriented middleware technologies like Kafka is a plus.
Ability to work collaboratively in a team environment.
Need to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.
Experience with Financial Services application is a plus.
Effective communication skills, both written and verbal.
Bachelor's or master's degree in computer science, Engineering, or a related field.
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
Associate Data Engineer,Bristlecone,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Summary
JOB DESCRIPTION
We are looking for a motivated and detail-oriented Junior Data Engineer to join our team. The ideal candidate will assist in building, maintaining, and optimizing scalable data pipelines and systems for data processing and analytics. This role is perfect for individuals who are eager to learn and grow in the field of data engineering while working on exciting, data-driven projects.
Responsibilities
Key Responsibilities:
Assist in designing and developing data pipelines to ingest, transform, and load data into data warehouses or data lakes.
Work with ETLprocesses to automate data workflows and ensure data integrity.
Collaborate with the team to troubleshoot and resolve data-related issues.
Support the implementation of data models, schemas, and storage solutions.
Optimize data processing workflows for performance and scalability.
Maintain documentation for data systems, pipelines, and processes.
Stay updated with emerging data engineering tools and technologies.
Qualifications
Required Skills and Qualifications:
Knowledge of programming languages such as Python , SQL , or Java for data processing.
Familiarity with relational databases (e.g., MySQL , PostgreSQL ) and data warehousing concepts.
Basic understanding of cloud platforms (e.g., AWS , GCP , Azure ) for data storage and processing.
Familiarity with big data tools like Spark , Hadoop , or Kafka is a plus.
Strong analytical and problem-solving skills.
Ability to work collaboratively in a team environment.
Eagerness to learn and adapt to new technologies and methodologies.
Preferred Qualifications
Exposure to data visualization tools like PowerBI , Tableau , or Looker .
Understanding of APIs and data integration techniques.
Knowledge of version control systems like Git .
Experience with workflow orchestration tools like ApacheAirflow .
Relevant certifications in cloud platforms or data engineering tools are a plus.
About Us
ABOUT US
Bristlecone is the leading provider of AI-powered application transformation services for the connected supply chain. We empower our customers with speed, visibility, automation, and resiliency to thrive on change.
Our transformative solutions in Digital Logistics, Cognitive Manufacturing, Autonomous Planning, Smart Procurement and Digitalization are positioned around key industry pillars and delivered through a comprehensive portfolio of services spanning digital strategy, design and build, and implementation across a range of technology platforms.
Bristlecone is ranked among the top ten leaders in supply chain services by Gartner. We are headquartered in San Jose, California, with locations across North America, Europe and Asia, and over 2,500 consultants. Bristlecone is part of the $19.4 billion Mahindra Group.
Equal Opportunity Employer
Bristlecone is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status .
Information Security Responsibilities
Understand and adhere to Information Security policies, guidelines and procedure, practice them for protection of organizational data and Information System.
Take part in information security training and act while handling information.
Report all suspected security and policy breach to InfoSec team or appropriate authority (CISO).
Understand and adhere to the additional information security responsibilities as part of the assigned job role.","Looker, Java, Hadoop, Apis, PostgreSQL, Kafka, Tableau, Sql, Git, Gcp, Powerbi, MySQL, Spark, Azure, Python, AWS"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role
At BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.
We are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.
Engineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.
Key Responsibilities
Design, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.
Develop data transformation using DBT (Data Build Tool) with SQL or Python.
Design and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.
Develop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.
Ensure data quality and integrity through automated testing and validation using tools like Great Expectations.
Implement all observability requirements in the data pipeline.
Optimize data workflows for performance and scalability.
Performs code and design reviews for tasks done by other team members.
Works with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.
Works on the development of technical standards for the product and platform.
Collaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.
Provide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.
Document data engineering processes and best practices whenever required.
Required Skills And Qualifications
Must have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.
Exposure on leading complex software projects.
Strong programming skills in Python.
Experience with Apache Airflow or any other orchestration framework for data orchestration.
Proficiency in DBT for data transformation and modeling.
Experience with data quality validation tools like Great Expectations or any other similar tools.
Strong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.
Experience with cloud-based data warehouse platform like Snowflake.
Experience working on NoSQL databases like Elasticsearch and MongoDB.
Experience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.
Experience on Cloud platforms like AWS and/or Azure.
Experience working with backend microservices and APIs using Java or C#.
Exposure on message-oriented middleware technologies like Kafka is a plus.
Ability to work collaboratively in a team environment.
Need to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.
Experience with Financial Services application is a plus.
Effective communication skills, both written and verbal.
Bachelor's or master's degree in computer science, Engineering, or a related field.
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Bachelor of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline + 38 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql"
Lead Azure Data Engineer,Aspire Systems,7-9 Years,,"Chennai, India",Login to check your skill match score,"Job Title: Lead Azure Data Engineer
Experience: 7+ Years
Location: Chennai / Bangalore /Hyderabad /Kochi
About Aspire Systems
Aspire Systems is a global technology services firm offering end-to-end digital transformation and product engineering solutions. We are currently seeking an experienced Lead Azure Data Engineer to join our Data & Analytics team and help build scalable, secure, and intelligent data platforms.
Role Overview
As an Azure Data Engineer, you will be responsible for creating, managing, and optimizing data lakes and pipelines using Azure technologies. You will work with internal teams to build high-performance data platforms that support enterprise-grade analytics and reporting.
Key Responsibilities
Design and build data lakes and lakehouses from scratch, and configure existing systems.
Develop and manage Azure Data Factory (ADF) and Synapse pipelines for data ingestion and transformation.
Work on data security implementation across storage and data movement layers.
Use Python, PySpark, and Spark SQL for data transformation and orchestration.
Create and maintain CI/CD pipelines for data workflows.
Understand and work with various file formats: JSON, Parquet, CSV, Excel, and both structured and unstructured datasets.
Collaborate with internal teams to support application integration and performance optimization.
Provide user support and documentation where required.
Key Skills Required
Expertise in Azure Data Lake, Lakehouse Architecture, Synapse Analytics, Databricks, and T-SQL.
Strong experience in ETL/ELT pipelines, working with large volumes of data.
Proficient with SQL Server, Synapse DB, and data warehousing concepts.
Understanding of metadata management, data catalogs, DWH, MPP, OLTP, and OLAP systems.
Familiar with Source Control tools like Git, TFS, or SVN.
Knowledge of Azure Data Fabric, Microsoft Purview, and MDM tools is a plus.
Experience working with non-functional requirements and performance tuning.
Strong communication skills and the ability to work effectively in a collaborative team environment.
Why Aspire
Work on cutting-edge Azure Data & AI solutions.
Collaborate with a global team on enterprise-scale transformation projects.
Growth opportunities into Data Architect or Tech Lead roles.
Continuous learning and certification support.
Interested candidates can share their resume with [HIDDEN TEXT]","MDM tools, OLAP systems, Synapse DB, Microsoft Purview, Azure Data Fabric, ETL ELT pipelines, Synapse Analytics, Azure Data Lake Lakehouse Architecture, data catalogs, Spark SQL, T-sql, Metadata Management, Pyspark, SQL Server, Mpp, Dwh, Databricks, Data Warehousing Concepts, Python, Oltp"
IN_Senior Associate_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,3-8 Years,,"Mumbai, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.
In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary: A career within PWC
Responsibilities
Job Title: Cloud Data Engineer (AWS/Azure/Databricks/GCP) Experience:3-8 years in Data Engineering Job Description: We are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions. Key Responsibilities: - Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP. - Implement data ingestion and transformation processes to facilitate efficient data warehousing. - Utilize cloud services to enhance data processing capabilities: - AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS. - Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus. - GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion. - Optimize Spark job performance to ensure high efficiency and reliability.
Stay proactive in learning and implementing new technologies to improve data processing frameworks. - Collaborate with cross-functional teams to deliver robust data solutions. - Work on Spark Streaming for real-time data processing as necessary. Qualifications: - 3-8 years of experience in data engineering with a strong focus on cloud environments. - Proficiency in PySpark or Spark is mandatory. - Proven experience with data ingestion, transformation, and data warehousing. - In-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP): - Demonstrated ability in performance optimization of Spark jobs. - Strong problem-solving skills and the ability to work independently as well as in a team. - Cloud Certification (AWS, Azure, or GCP) is a plus. - Familiarity with Spark Streaming is a bonus.
Mandatory Skill Sets
Python, Pyspark, SQL with (AWS or Azure or GCP)
Preferred Skill Sets
Python, Pyspark, SQL with (AWS or Azure or GCP)
Years Of Experience Required
3-8 years
Education Qualification
BE/BTECH, ME/MTECH, MBA, MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Node.js
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation + 19 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","GCP Dataflow, Azure Data Factory, Gcp, Pyspark, AWS Glue, Spark, Azure, Sql, Python, AWS"
AWS Data Engineer,Zensar Technologies,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Hi Applicants,
We have an urgent demand for AWS Data Engineer. Please go through the Job Description below and share your resume if you feel interested.
Requirement for AWS Data Engineer
Experience-5+ Years
Location-Hyderabad
Notice Period-Immediate
Job Description
Key Responsibilities
Design, Develop, and Maintain Redshift Data Warehouses:
Architect, build, and optimize Redshift clusters for high performance and scalability.
Design and implement data models, schemas, and ETL/ELT pipelines.
Develop and maintain complex SQL queries and stored procedures.
Ensure data quality and integrity within the Redshift environment.
Data Loading and Transformation:
Develop and implement efficient data loading strategies from various sources (e.g., databases, files, APIs).
Design and implement data transformation and cleansing processes.
Optimize data loading performance and minimize data latency.
Performance Tuning and Optimization:
Monitor and analyze Redshift cluster performance.
Identify and troubleshoot performance bottlenecks.
Implement performance tuning strategies, such as indexing, partitioning, and query optimization.
AWS Integration:
Leverage other AWS services, such as S3, Glue, EMR, and Lambda, to enhance data warehousing solutions.
Integrate Redshift with other cloud-based applications and services.
Security and Compliance:
Implement and maintain data security measures, including access control, encryption, and data masking.
Ensure compliance with relevant data privacy regulations (e.g., GDPR, CCPA).
Collaboration and Communication:
Collaborate with data analysts, data scientists, and business stakeholders to understand their data needs.
Communicate technical information effectively to both technical and non-technical audiences.
Required Skills
Strong proficiency in SQL
Experience with AWS Redshift
Knowledge of data warehousing concepts and best practices
Experience with ETL/ELT processes
Experience with data modeling and schema design
Understanding of cloud computing concepts and AWS ecosystem
Experience with scripting languages (e.g., Python, Shell)
Strong analytical and problem-solving skills
Desired Skills
Experience with AWS services like S3, Glue, EMR, Lambda
Experience with data visualization tools (e.g., Tableau, Power BI)
Experience with data quality and validation tools
Experience with Agile development methodologies
AWS certifications (e.g., AWS Certified Data Analytics - Specialty)","ETL ELT processes, cloud computing concepts, AWS ecosystem, data modeling and schema design, data warehousing concepts and best practices, Aws Redshift, Sql"
Lead Azure Data Engineer,Aspire Systems,7-9 Years,,"Chennai, India",Login to check your skill match score,"Job Title: Lead Azure Data Engineer
Experience: 7+ Years
Location: Chennai / Bangalore /Hyderabad /Kochi
About Aspire Systems
Aspire Systems is a global technology services firm offering end-to-end digital transformation and product engineering solutions. We are currently seeking an experienced Lead Azure Data Engineer to join our Data & Analytics team and help build scalable, secure, and intelligent data platforms.
Role Overview
As an Azure Data Engineer, you will be responsible for creating, managing, and optimizing data lakes and pipelines using Azure technologies. You will work with internal teams to build high-performance data platforms that support enterprise-grade analytics and reporting.
Key Responsibilities
Design and build data lakes and lakehouses from scratch, and configure existing systems.
Develop and manage Azure Data Factory (ADF) and Synapse pipelines for data ingestion and transformation.
Work on data security implementation across storage and data movement layers.
Use Python, PySpark, and Spark SQL for data transformation and orchestration.
Create and maintain CI/CD pipelines for data workflows.
Understand and work with various file formats: JSON, Parquet, CSV, Excel, and both structured and unstructured datasets.
Collaborate with internal teams to support application integration and performance optimization.
Provide user support and documentation where required.
Key Skills Required
Expertise in Azure Data Lake, Lakehouse Architecture, Synapse Analytics, Databricks, and T-SQL.
Strong experience in ETL/ELT pipelines, working with large volumes of data.
Proficient with SQL Server, Synapse DB, and data warehousing concepts.
Understanding of metadata management, data catalogs, DWH, MPP, OLTP, and OLAP systems.
Familiar with Source Control tools like Git, TFS, or SVN.
Knowledge of Azure Data Fabric, Microsoft Purview, and MDM tools is a plus.
Experience working with non-functional requirements and performance tuning.
Strong communication skills and the ability to work effectively in a collaborative team environment.
Why Aspire
Work on cutting-edge Azure Data & AI solutions.
Collaborate with a global team on enterprise-scale transformation projects.
Growth opportunities into Data Architect or Tech Lead roles.
Continuous learning and certification support.
Interested candidates can share their resume with [HIDDEN TEXT]","MDM tools, OLAP systems, Synapse DB, Microsoft Purview, Azure Data Fabric, ETL ELT pipelines, Synapse Analytics, Azure Data Lake Lakehouse Architecture, data catalogs, Spark SQL, T-sql, Metadata Management, Pyspark, SQL Server, Mpp, Dwh, Databricks, Data Warehousing Concepts, Python, Oltp"
"IN_Senior Associate_Data Engineer(PySpark,Python)_Data & Analytics_Advisory_Bangalore",PwC India,4-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities:
Roles & Responsibilities
Deliver projects integrating data flows within and across technology systems.
Lead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.
Design end-to-end job flows that span across systems, including quality checks and controls.
Create technology delivery plans to implement system changes.
Perform data analysis, data profiling, and data sourcing in relational and Big Data environments.
Convert functional requirements into logical and physical data models.
Assist in ETL development, testing, and troubleshooting ETL issues.
Troubleshoot data issues and work with data providers for resolution; provide L3 support when needed.
Design and develop ETL workflows using modern coding and testing standards.
Participate in agile ceremonies and actively drive towards team goals.
Collaborate with a global team of technologists.
Lead with ideas and innovation.
Manage communication and partner with end users to design solutions.
Required Skills:
Must have: Total experience required 4-10 years (relevant experience minimum 5 years)
5 years of project experience in Python/Shell scripting in Data Engineering (experience in building and optimizing data pipelines, architectures, and data sets with large data volumes).
3+ years of experience in PySpark scripting, including the architecture framework of Spark.
3-5 years of strong experience in database development (Snowflake/ SQL Server/Oracle/Sybase/DB2) in designing schema, complex procedures, complex data scripts, query authoring (SQL), and performance optimization.
Strong understanding of Unix environment and batch scripting languages (Shell/Python).
Strong knowledge of Big Data/Hadoop platform.
Strong engineering skills with the ability to understand existing system designs and enhance or migrate them.
Strong logical data modeling skills within the Financial Services domain.
Experience in data integration and data conversions.
Strong collaboration and communication skills.
Strong organizational and planning skills.
Strong analytical, profiling, and troubleshooting skills.
Good To Have:
Experience with ETL tools (e.g Informatica, Azure Data Factory) and pipelines across disparate sources is a plus.
Experience working with Databricks is a plus.
Familiarity with standard Agile & DevOps methodology & tools (Jenkins, Sonar, Jira).
Good understanding of developing ETL processes using Informatica or other ETL tools.
Experience working with Source Code Management solutions (e.g., Git).
Knowledge of Investment Management Business.
Experience with job scheduling tools (e.g., Autosys).
Experience with data visualization software (e.g., Tableau).
Experience with data modeling tools (e.g., Power Designer).
Basic familiarity with using metadata stores to maintain a repository of Critical Data Elements. (e.g. Collibra)
Familiarity with XML or other markup languages.
Mandatory Skill Sets:
ETL,Python/Shell scripting , building pipelines,pyspark, database, sql
Preferred Skill Sets:
informatica, hadoop, databricks, collibra
Years Of Experience Required:
4 to 10 years
Education Qualification:
Graduate Engineer or Management Graduate
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration
Degrees/Field Of Study Preferred:
Certifications (if blank, certifications not specified)
Required Skills
Extract Transform Load (ETL), Python (Programming Language), Structured Query Language (SQL)
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment, Performance Management Software + 16 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","power designer, snowflake, Sybase, Sonar, Hadoop, Collibra, Pyspark, SQL Server, Tableau, Jira, Informatica, Sql, Jenkins, Git, Azure Data Factory, DB2, Shell scripting, Databricks, Oracle, Python, Etl"
IN-Manager_Azure Data Engineer_Data Analytics_Advisory_Bangalore,PwC India,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Operations
Management Level
Manager
Job Description & Summary
At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.
In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary: A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Responsibilities
Must have:
Candidates with a minimum of 5 years of relevant experience for 10-12 years of total experience (Architect / Managerial level).
Deep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.
Must be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)
Strong troubleshooting and debugging skills.
Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.
Experience in writing complex PySpark queries to perform data analysis.
Good To Have
Good to have certifications: Apache Spark 3.0
Experience in any one visualization tool like Power BI, tableau etc.
Understanding of Spark Architecture landscape
Mandatory Skill Sets
Spark, Pyspark, Azure
Preferred Skill Sets
Spark, Pyspark, Azure
Years Of Experience Required
8-12yrs
Education Qualification
B.Tech / M.Tech / MBA / MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Master of Engineering, Bachelor of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation + 23 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Available for Work Visa Sponsorship
Government Clearance Required
Job Posting End Date","Pyspark scripting, Data Bricks, Synapse, Stored Procedures, Azure Analysis Services, Data Factory, Power Bi, Tableau, Python Scripting, Github, Advanced Sql, Azure DevOps"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements
Job Requirements
Role/ Job Title: Data Engineer - Gen AI
Function/ Department: Data & Analytics
Place of Work: Mumbai
Job Purpose
The data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.
Job & Responsibilities
Build data engineering pipeline focused on unstructured data pipelines
Conduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.
Design, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.
Work on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability
Drive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets
A suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.
Creating Technical Design Documentation of the projects/pipelines
Good skills in technical debugging of the code in case of issues. Also, working with git for code versioning
Education Qualification
Graduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)
Post-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA
Experience Range : 5-10 years of relevant experience","Data Ingestion, Data engineering pipeline, Big data infrastructure, Technical debugging, HDFS, Data Consumption, Technical Design Documentation, Unstructured data pipelines, Dynamodb, Data Architecture, HBase, Yarn, Mapreduce, Git, Hive, MongoDB, Data Transformation"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role
At BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.
We are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.
Engineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.
Key Responsibilities
Design, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.
Develop data transformation using DBT (Data Build Tool) with SQL or Python.
Design and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.
Develop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.
Ensure data quality and integrity through automated testing and validation using tools like Great Expectations.
Implement all observability requirements in the data pipeline.
Optimize data workflows for performance and scalability.
Performs code and design reviews for tasks done by other team members.
Works with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.
Works on the development of technical standards for the product and platform.
Collaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.
Provide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.
Document data engineering processes and best practices whenever required.
Required Skills And Qualifications
Must have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.
Exposure on leading complex software projects.
Strong programming skills in Python.
Experience with Apache Airflow or any other orchestration framework for data orchestration.
Proficiency in DBT for data transformation and modeling.
Experience with data quality validation tools like Great Expectations or any other similar tools.
Strong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.
Experience with cloud-based data warehouse platform like Snowflake.
Experience working on NoSQL databases like Elasticsearch and MongoDB.
Experience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.
Experience on Cloud platforms like AWS and/or Azure.
Experience working with backend microservices and APIs using Java or C#.
Exposure on message-oriented middleware technologies like Kafka is a plus.
Ability to work collaboratively in a team environment.
Need to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.
Experience with Financial Services application is a plus.
Effective communication skills, both written and verbal.
Bachelor's or master's degree in computer science, Engineering, or a related field.
Our Benefits
To help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.
Our hybrid work model
BlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.
About BlackRock
At BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.
This mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.
For additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock
BlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Bachelor of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline + 38 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql, Etl"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
bn*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql, Etl"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql, Etl"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
ben*Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Requirment gathering and analysis.
PAN India
10
Senior Associate
3-7
Experience with different databases like Synapse, SQL DB, Snowflake etc.
Design and implement data pipelines using Azure Data Factory, Databricks, Synapse
Create and manage Azure SQL Data Warehouses and Azure Cosmos DB databases
Extract, transform, and load (ETL) data from various sources into Azure Data Lake Storage
Implement data security and governance measures
Monitor and optimize data pipelines for performance and efficiency
Troubleshoot and resolve data engineering issues
Provide optimized solution for any problem related to data engineering
Ability to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.
Strong knowledge on Databricks, Delta tables
Technology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks
Mandatory Skill Sets
Azure DE
Preferred Skill Sets
Azure DE
Years Of Experience Required
4-8
Education Qualification
Btech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql, Etl"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements
Job Requirements
Role/ Job Title: Data Engineer - Gen AI
Function/ Department: Data & Analytics
Place of Work: Mumbai
Job Purpose
The data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.
Job & Responsibilities
Build data engineering pipeline focused on unstructured data pipelines
Conduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.
Design, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.
Work on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability
Drive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets
A suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.
Creating Technical Design Documentation of the projects/pipelines
Good skills in technical debugging of the code in case of issues. Also, working with git for code versioning
Education Qualification
Graduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)
Post-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA
Experience Range : 5-10 years of relevant experience","big data infrastructure, HDFS, Hive, Dynamodb, Api, MongoDB, HBase, Yarn, Mapreduce, Etl"
Senior Data Engineer,IDFC FIRST Bank,6-8 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements
Job Requirements
Role/ Job Title: Senior Data Engineer
Business: New Age
Function/ Department: Data & Analytics
Place of Work: Mumbai/Bangalore
Roles & Responsibilities
Minimum 6 years of Data Engineering experience and 3 years in large scale Data Lake ecosystem
Proven expertise in SQL, Spark Python, Scala, Hadoop ecosystem,
Have worked on multiple TBs/PBs of data volume from ingestion to consumption
Work with business stakeholders to identify and document high impact business problems and potential solutions
First-hand experience with the complete software development life cycle including requirement analysis, design, development, deployment, and support
Advanced understanding of Data Lake/Lakehouse architecture and experience/exposure to Hadoop (cloudera,hortonworks) and AWS
Work on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer. Versed with API and its usability
A suitable candidate will also be proficient Spark, Spark Streaming, AWS, and EMR
A suitable candidate will also demonstrate machine learning experience and experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc.
The candidate will additionally demonstrate substantial experience and a deep knowledge of data mining techniques, relational, and non-relational databases.
Advanced skills in technical debugging of the architecture in case of issues
Creating Technical Design Documentation (HLD/LLD) of the projects/pipelines
Secondary Responsibilities
Ability to work independently and handle your own development effort.
Excellent oral and written communication skills Learn and use internally available analytic technologies
Identify key performance indicators and establish strategies on how to deliver on these key points for analysis solutions
Use educational background in data engineering and perform data mining analysis
Work with BI analysts/engineers to create prototypes, implementing traditional classifiers and determiners, predictive and regressive analysis points
Engage in the delivery and presentation of solutions
Managerial & Leadership Responsibilities
Lead moderately complex initiatives within Technology and contribute to large scale data processing framework initiatives related to enterprise strategy deliverables
Build and maintain optimized and highly available data pipelines that facilitate deeper analysis and reporting
Review and analyze moderately complex business, operational or technical challenges that require an in-depth evaluation of variable factors
Oversee the data integration work, including integrating a data model with datalake, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis
Resolve moderately complex issues and lead teams to meet data engineering deliverables while leveraging solid understanding of data information policies, procedures and compliance requirements
Collaborate and consult with colleagues and managers to resolve data engineering issues and achieve strategic goals
Key Success Metrics
Ensure timely deliverables. Spot Data fixes. Lead technical aspects of the projects. Error free deliverables.","HDFS, Hadoop, Scala, Emr, HBase, Yarn, Sql, Mapreduce, Hive, Spark, Oozie, Python, AWS"
IN_Senior Associate_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,3-8 Years,,"Mumbai, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Senior Associate
Job Description & Summary
At PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.
In data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary: A career within PWC
Responsibilities
Job Title: Cloud Data Engineer (AWS/Azure/Databricks/GCP) Experience:3-8 years in Data Engineering Job Description: We are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions. Key Responsibilities: - Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP. - Implement data ingestion and transformation processes to facilitate efficient data warehousing. - Utilize cloud services to enhance data processing capabilities: - AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS. - Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus. - GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion. - Optimize Spark job performance to ensure high efficiency and reliability.
Stay proactive in learning and implementing new technologies to improve data processing frameworks. - Collaborate with cross-functional teams to deliver robust data solutions. - Work on Spark Streaming for real-time data processing as necessary. Qualifications: - 3-8 years of experience in data engineering with a strong focus on cloud environments. - Proficiency in PySpark or Spark is mandatory. - Proven experience with data ingestion, transformation, and data warehousing. - In-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP): - Demonstrated ability in performance optimization of Spark jobs. - Strong problem-solving skills and the ability to work independently as well as in a team. - Cloud Certification (AWS, Azure, or GCP) is a plus. - Familiarity with Spark Streaming is a bonus.
Mandatory Skill Sets
Python, Pyspark, SQL with (AWS or Azure or GCP)
Preferred Skill Sets
Python, Pyspark, SQL with (AWS or Azure or GCP)
Years Of Experience Required
3-8 years
Education Qualification
BE/BTECH, ME/MTECH, MBA, MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Node.js
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation + 19 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","GCP Dataflow, Azure Data Factory, Gcp, Pyspark, AWS Glue, Spark, Azure, Sql, Python, AWS"
Data Engineer-DIG,TresVista,3-7 Years,,"Bengaluru, India",Login to check your skill match score,"Roles and Responsibilities:
Interacting with the client (internal or external) to understand their problems and work on solutions that address their needs
Driving projects and working closely with a team of individuals to ensure proper requirements are identified, useful user stories are created, and work is planned logically and efficiently to deliver solutions that support changing business requirements
Managing the various activities within the team, strategizing how to approach tasks, creating timelines and goals, distributing information/tasks to the various team members
Conducting meetings, documenting, and communicating findings effectively to clients, management and cross-functional teams
Creating Ad-hoc reports for multiple internal requests across departments
Automating the process using data transformation tools
Prerequisites
Strong analytical, problem-solving, interpersonal, and communication skills
Advanced knowledge of DBMS, Data Modelling along with advanced querying capabilities using SQL Working experience in cloud technologies (GCP/ AWS/Azure/Snowflake)
Prior experience in building and deploying ETL/ELT pipelines using CI/CD, and orchestration tools such as Apache Airflow, GCP workflows, etc.
Proficiency in Python for building ETL/ELT processes and data modeling
Proficiency in Reporting and Dashboards creation using Power BI/Tableau
Knowledge in building ML models and leveraging Gen AI for modern architectures.
Experience working with version control platforms like GitHub
Familiarity with IaC tools like Terraform and Ansible is good to have
Stakeholder Management and client communication experience would be preferred
Experience in the Financial Services domain will be an added plus
Experience in Machine Learning tools and techniques will be good to have
Experience
3-7 years
Education
BTech/MTech/BE/ME/MBA in Analytics
Compensation
The compensation structure will be as per industry standards","Gen AI, GCP workflows, snowflake, Github, Data Modelling, Machine Learning, Power Bi, Tableau, Sql, ELT, Apache Airflow, Gcp, Terraform, Ansible, Dbms, Azure, Python, Etl, AWS"
Data Engineer 3 (Hyderabad),Hyland,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Overview
Overview
Hyland Software is widely known as a great company to work for and a great company to do business with. Being a leader in providing software solution for managing content, processes, and cases for organizations across the globe we enabled more than 20,000 organizations to digitalize their workplaces and transform their operations.
Currently we are looking for the position of Data Engineer 3
The Data Engineer 3 is responsible for managing data pipelines and developing analytical solutions. This position requires technical expertise to design, build, and implement data solutions within a collaborative environment.
What You Will Be Doing
Design, build, and maintain scalable data pipelines to clean and transform data so that it is ready for analysis.
Develop end-to-end solutions from data extraction to operational deployment.
Collaborate with data scientists, data analysts, and business stakeholders to understand data requirements and deliver data solutions.
Implement data quality checks and monitoring to ensure data accuracy, consistency, and reliability.
Create and maintain thorough documentation for data pipelines architectures and processes.
Ensure data storage and processing meet security standards and comply with relevant regulations.
Troubleshoot and resolve data-related issues in a timely manner.
Operate as a trusted advisor on issues and trends, advising on best practices in data engineering and big data technologies.
Evaluate the team's models and suggest enhancements as needed promote best practices for modeling and create scalable operational needs.
Create and maintain thorough documentation for data pipelines architectures and processes.
Mentor coach train and provide feedback to other team members; provide feedback to leadership on abilities of team.
Comply with all corporate and departmental privacy and data security policies and practices, including but not limited to, Hyland's Information Systems Security Policy.
What Will Make You Successful
Bachelor's degree or equivalent experience
Proven experience with data engineering including working with data warehouses ETL
Experience in Data and ML Pipelines orchestration
Proficiency in SQL Python and at least one data processing framework (e.g. Spark Flink TensorFlow PyTorch)
Knowledge of machine learning concepts and experience applying them in real-world use cases.
Experience with Snowflake or Microsoft Fabric
At least 5 years experience as a Data Engineer with experience in development and maintenance support
Excellent collaboration skills applied successfully within team as well as with all levels of employees in other areas
Excellent critical thinking and problem solving skills
Excellent ability to use original thinking to translate goals into the implementation of new ideas and design solutions
Self-motivated with the ability to manage projects to completion independently
Able to thrive in a fast paced deadline driven environment
Excellent attention to detail
Excellent ability to handle sensitive information with discretion and tact
Excellent ability to establish rapport and gain the trust of others; effective at gaining consensus
Ability to work independently and in a team environment
Ability to coach mentor and provide feedback to team members in a timely manner
Ability to provide guidance and support to developing team members
Up to 5% travel time required
Hyland's Offering
We're proud of our culture and take employee engagement seriously. By listening to employees feedback, we're able to provide meaningful benefits and programs to our workforce.
Learning & Development - development budget (used for certifications, conferences etc..), tuition assistance program, 4,000+ self-paced online courses, instructor-led webinars, mentorship programs, structured on-boarding experience full of trainings, dedicated Learning & Development department supporting our employees.
R&D focus cutting edge technologies, constant modernization efforts, dynamic and innovative environment, dedicated R&D Education Services department to help you grow.
Work-life balance culture flexible work environment and working hours (we are working in task-based system!), possibility to work from home, we value trust, and we believe efficiency does not depend on your actual location, however we would like to spend time together in the office!
Well-being - private medical healthcare, life insurance, gym reimbursement, psychologist & dietician consultation, wellness manager care, constant wellbeing programs
Community Engagement Volunteer time off (12h/year), Hylanders for Hylanders relief found, Mission fit giving, Dolars-for-doers matching gift programs.
Diversity & Inclusion employee resource groups, inclusion benefits and policies
Niceties & Events snacks and beverages, employee referral program, birthday, baby gifts and employee programs
If you would like to join the company where honesty, integrity and fairness lie in the bottom of values, where people are truly passionate about technology and dedicated to their work connect with us!
We are committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, marital status, gender identity or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for employment, verify identity and maintain employment statistics on applicants.","snowflake, Data and ML Pipelines orchestration, Microsoft Fabric, Flink, Tensorflow, Pytorch, Spark, Python, Sql, Etl"
Data Engineer,Louisa AI,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"About Louisa AI
Louisa AI, a cutting-edge institutional revenue enablement platform, was originally developed at Goldman Sachs and became an independent entity in 2023. It utilizes AI to maximize revenues by mapping the expertise and relationships within organizations, primarily serving financial institutions. Louisa AI emphasizes connecting people through AI, not replacing them, leveraging relationship graphs and news integration to enhance revenue generation and connections based on expertise, relationships, and relevant information.
Responsibilities
As a data engineer on the Louisa team, you will have the opportunity to:
Data Architecture And Design
Design and implement scalable and efficient data models for storage and retrieval.
Develop and maintain data architecture, ensuring optimal performance and data integrity.
ETL Development
Create and optimize ETL processes to extract, transform, and load data from various sources into the data warehouse.
Implement data transformation logic to ensure data consistency and quality.
Programming And Scripting
Utilize programming languages (e.g., Python, Java) and scripting languages for data processing and manipulation.
Develop custom code and scripts for data extraction, transformation, and loading tasks.
Data Quality And Governance
Implement data quality checks and measures to ensure accuracy and reliability.
Adhere to data governance and security policies, ensuring compliance with relevant regulations.
Minimum Qualifications
Bachelor's degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
Minimum 5 years of relevant development experience.
Strong proficiency in programming languages (e.g., Python, Java).
Experience with big data technologies like Spark, Hive, etc and orchestration tool like airflow.
Experience with Data warehousing tools like Snowflake/Big Query/Databricks SQL.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Extra Awesome
Technical expertise with data models, data mining, and segmentation techniques
Comfortable multi-tasking, managing multiple stakeholders and working in a global team.","Big Query, Airflow, snowflake, Java, Sql, Hive, Gcp, Spark, Databricks, Azure, Python, AWS"
Data Engineer,Synechron Technologies Pvt. Ltd.,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"Greetings,
We have immediate opportunity for Data Engineer 5-10 years
Synechron Mumbai/ Pune/ Bangalore/ Chennai/ Kolkata/ Gurgaon
Job Role: Data Engineer
Job Location: Mumbai/ Pune/ Bangalore/ Chennai/ Kolkata/ Gurgaon
About Synechron
We began life in 2001 as a small, self-funded team of technology specialists. Since then, we've grown our organization to 14,500+ people, across 58 offices, in 21 countries, in key global markets.
Innovative tech solutions for business
We're now a leading global digital consulting firm, providing innovative technology solutions for business. As a trusted partner, we're always at the forefront of change as we lead digital optimization and modernization journeys for our clients.
Customized end-to-end solutions
Our expertise in AI, Consulting, Data, Digital, Cloud & DevOps and Software Engineering, delivers customized, end-to-end solutions that drive business value and growth.
Job Description:
Software Requirements:
Strong proficiency in SQL
Experience with HIVE
Familiarity with SQOOP
Knowledge of SPARK is a plus
Experience in performance optimization techniques
Proficiency in on-premises solutions, especially with Azure
Familiarity with Cloudera and HortonWorks platforms
Overall Responsibilities:
Design, develop, and maintain data pipelines and ETL processes to support data analytics and reporting needs.
Ensure data integrity and quality throughout the data lifecycle.
Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.
Optimize database performance and troubleshoot issues related to data storage and retrieval.
Stay updated on industry trends and emerging technologies to continuously enhance data architecture and operations.
Technical Skills:
Database Technologies:
SQL (Must-have)
HIVE (Must-have)
SQOOP (Must-have)
SPARK (Preferred)
Performance Optimization:
Techniques for optimizing data processing and query performance.
Cloud Technologies:
Azure (on-premises focus)
Big Data Platforms:
Cloudera
HortonWorks
Experience:
4.5+ years of experience in data engineering or related field.
Proven experience with database management and data warehousing.
Familiarity with big data technologies and cloud-based solutions is an advantage.
Day-to-Day Activities:
Develop and maintain data pipelines from various sources to databases.
Monitor data flows and troubleshoot issues in real time.
Collaborate with stakeholders to gather requirements and deliver actionable insights.
Conduct data quality checks and performance optimizations on existing systems.
Participate in team meetings to discuss project progress and share knowledge.
For more information on the company, please visit our website or LinkedIn community.
If you find this this opportunity interesting kindly share your updated profile on [HIDDEN TEXT]
With below details (Mandatory)
Total Experience
Experience as Data Engineer-
Experience in Spark-
Experience in Hive-
Experience in SQL
Current CTC-
Expected CTC-
Notice period-
Current Location-
Ready to relocate to Mumbai-
If you had gone through any interviews in Synechron before If Yes when
Regards,
Bansi Hindocha
[HIDDEN TEXT]","Hive, Sqoop, Hortonworks, Spark, Cloudera, Azure, Sql"
Tech Lead- Data Engineer,Thought Frameworks,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"INBOX : Tech Lead- Data Engineer @ [HIDDEN TEXT]
Position : Tech Lead Data Engineer
Exp : 8+ Yrs
Job Location : Gurugram (3 days work from office)
Mandate skill : Snowflake, Python, SQL, ETL/ELT with Asset Management experience (Finance domain)
All the skill will reflect in profile project and tech skill.
NP : with 15 - 20 days
Work Type : Full Time Hire
Job Description :
Key Responsibilities:
Data Pipeline Development: Design, build, and maintain robust ELT (Extract, Load, Transform) pipelines using Snowflake to support data ingestion, integration, and transformation.
Technical Leadership: Lead offshore development teams in implementing best practices for data engineering and ELT development.
Data Integration: Collaborate with stakeholders to understand data sources and integration requirements, ensuring seamless connectivity and data flow between systems.
Performance Optimization: Optimize data pipelines for performance, scalability, and reliability, including query tuning and resource management within Snowflake.
Data Quality Assurance: Implement and monitor data validation procedures to ensure data accuracy and consistency across systems.
Collaboration and Communication: Work closely with project managers, data architects, and business analysts to align project milestones and deliverables with business goals.
Documentation: Create and maintain detailed documentation of data pipelines, data flow diagrams, and transformation logic.
Issue Resolution: Troubleshoot and resolve issues related to data pipelines, including job failures and performance bottlenecks.
Compliance and Security: Ensure all data management processes comply with data governance policies and regulatory requirements in financial services.
Required Qualifications:
Bachelor's degree in Computer Science, Information Technology, or a related field.
5+ years of experience in data engineering with a strong focus on ELT processes and data pipeline development.
Hands-on experience with Snowflake cloud data platform, including data sharing, secure views, and performance optimization.
Proficiency in SQL and familiarity with data integration and ETL/ELT tools.
Experience managing and collaborating with offshore development teams.
Strong problem-solving skills and the ability to work independently to meet deadlines.
Excellent communication skills for effectively interacting with technical and non-technical stakeholders.
Preferred Qualifications:
Certifications in Snowflake or relevant data technologies.
Experience in the financial services sector with an understanding of data security and compliance requirements.
Familiarity with cloud platforms (e.g., AWS, Azure) and data orchestration tools (e.g., Apache Airflow).
Tech Lead- Data Engineer
Experience with scripting languages such as Python or JavaScript for data transformation.
Knowledge of data visualization tools (e.g., Tableau, Power BI).","snowflake, Data Pipeline Development, Performance Optimization, Data Integration, Sql, ELT, Etl, Python, data quality assurance"
Data Engineer,Bounteous,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Data Engineer
Location: Bangalore (On-Site)
Start Date: 1st week of June (immediate joiners or candidates serving notice period only)
Experience: 5 10 years (candidates with less than 5Years)
Required Skills:
Python
PySpark
AWS (EMR)
Falcon Framework
Key Responsibilities:
Build and optimize large-scale data pipelines on AWS EMR
Develop and maintain PySpark jobs for data processing and analytics
Implement data ingestion workflows using the Falcon framework
Collaborate with stakeholders to deliver robust, scalable data solutions","AWS EMR, Falcon Framework, Pyspark, Python"
Data Engineer,Kavi India,2-4 Years,,India,Login to check your skill match score,"We are seeking a detail-oriented and highly motivated Data Engineer to join our growing Data &
Analytics team. In this role, you will be responsible for designing, building, and maintaining robust
data pipelines and infrastructure that power insights across the organization. You'll work closely with
data scientists, analysts, and engineers to ensure the integrity, accessibility, and scalability of our data
systems.
Responsibilities:
Design, develop, and maintain scalable data pipelines and ETL processes.
Build and optimize data architecture to ensure data quality and consistency.
Integrate data from diverse internal and external sources.
Collaborate with cross-functional teams to define data requirements and deliver solutions.
Implement best practices for data governance, security, and compliance.
Monitor pipeline performance and perform real-time troubleshooting of data issues.
Participate in code reviews and contribute to documentation and standards.
Required Qualifications & Skills:
Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical
experience).
Minimum of 2 years of professional experience in data engineering or software development.
Solid understanding of SQL and proficiency in at least one programming language, such as
Python, Java, or Scala.
Practical experience building and maintaining data pipelines using tools like Apache Airflow or
dbt.
Hands-on experience with cloud platforms (AWS, GCP, Azure) and data warehousing solutions
(Redshift, BigQuery, Snowflake).
Familiarity with big data technologies and frameworks, including Spark, Kafka, and Hadoop.
Demonstrated ability to solve complex problems with a strong focus on detail.
Experience implementing CI/CD practices for data workflows.
Working knowledge of data modeling principles and schema design.
Exposure to machine learning pipelines or real-time analytics systems is a plus.","snowflake, dbt, Sql, Apache Airflow, Data Modeling, Java, Hadoop, Schema Design, Kafka, BigQuery, AWS, Python, Azure, Gcp, Scala, Spark, Redshift"
Principal Data Engineer,MakeMyTrip,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Principal Data Engineer
Experience: Must have 8+ years of experience
About Role:
We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.
About us:
Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.
Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.
GO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.
About the team:
MakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.
Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.
There are two key responsibilities of Data Engineering team:
One to develop the platform for data capture, storage, processing, serving and querying.
Second is to develop data products starting from;
o personalization & recommendation platform
o customer segmentation & intelligence
o data insights engine for persuasions and
o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users
We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.
Technology experience:
Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture
Extensive experience in data modeling and database design
At least 6+ years of hands-on experience in Spark/BigData Tech stack
Stream processing engines Spark Structured Streaming/Flink
Analytical processing on Big Data using Spark
At least 6+ years of experience in Scala
Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems
At least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform
At least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc
Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies
Should have experience in architecting and implementing domain centric big data solutions
Ability to frame architectural decisions and provide technology leadership & direction
Excellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Senior Data Engineer,Wavicle Data Solutions,6-8 Years,INR 17.45 - 22.78 LPA,India,Login to check your skill match score,"We are seeking a highly skilled Senior Azure Databricks Data Engineer to design, develop, and optimize data solutions on Azure. The ideal candidate will have expertise in Azure Data Factory (ADF), Databricks, SQL, Python, and experience working with SAP IS-Auto as a data source. This role involves data modeling, systematic layer modeling, and ETL/ELT pipeline development to enable efficient data processing and analytics.
Experience: 6+ years
Key Responsibilities:
Develop & Optimize ETL Pipelines: Build robust and scalable data pipelines using ADF, Databricks, and Python for data ingestion, transformation, and loading.
Data Modeling & Systematic Layer Modeling: Design logical, physical, and systematic data models for structured and unstructured data.
Integrate SAP IS-Auto: Extract, transform, and load data from SAP IS-Auto into Azure-based data platforms.
Database Management: Develop and optimize SQL queries, stored procedures, and indexing strategies to enhance performance.
Big Data Processing: Work with Azure Databricks for distributed computing, Spark for large-scale processing, and Delta Lake for optimized storage.
Data Quality & Governance: Implement data validation, lineage tracking, and security measures for high-quality, compliant data.
Collaboration: Work closely with business analysts, data scientists, and DevOps teams to ensure data availability and usability.
Required Skills:
Azure Cloud Expertise: Strong experience in Azure Data Factory (ADF), Databricks, and Azure Synapse.
Programming: Proficiency in Python for data processing, automation, and scripting.
SQL & Database Skills: Advanced knowledge of SQL, T-SQL, or PL/SQL for data manipulation.
SAP IS-Auto Data Handling: Experience integrating SAP IS-Auto as a data source into data pipelines.
Data Modeling: Hands-on experience in dimensional modeling, systematic layer modeling, and entity-relationship modeling.
Big Data Frameworks: Strong understanding of Apache Spark, Delta Lake, and distributed computing.
Performance Optimization: Expertise in query optimization, indexing, and performance tuning.
Data Governance & Security: Knowledge of RBAC, encryption, and data privacy standards.
Preferred Qualifications:
Experience with CI/CD for data pipelines using Azure DevOps.
Knowledge of Kafka/Event Hub for real-time data processing.
Experience with Power BI/Tableau for data visualization (not mandatory but a plus).","SAP IS-Auto, rbac, Delta Lake, Data Privacy Standards, Data Modeling, Apache Spark, Encryption, Sql, Databricks, Data Governance, Python, Etl"
Data Engineer Associate - Operate,PwC Acceleration Centers in India,2-5 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services.
Driven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities.
Skills
Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:
Apply a learning mindset and take ownership for your own development.
Appreciate diverse perspectives, needs, and feelings of others.
Adopt habits to sustain high performance and develop your potential.
Actively listen, ask questions to check understanding, and clearly express ideas.
Seek, reflect, act on, and give feedback.
Gather information from a range of sources to analyse facts and discern patterns.
Commit to understanding how the business works and building commercial awareness.
Learn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements.
Role: Associate
Tower: Data, Analytics & Specialist Managed Service
Experience: 2.0 - 5.5 years
Key Skills: AWS
Educational Qualification: BE / B Tech / ME / M Tech / MBA
Work Location: India.;l
Job Description
As a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self-awareness, personal strengths, and address development areas.
Flexible to work in stretch opportunities/assignments.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Ticket Quality and deliverables review, Status Reporting for the project.
Adherence to SLAs, experience in incident management, change management and problem management.
Seek and embrace opportunities which give exposure to different situations, environments, and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Demonstrate leadership capabilities by working, with clients directly and leading the engagement.
Work in a team environment that includes client interactions, workstream management, and cross-team collaboration.
Good team player, take up cross competency work and contribute to COE activities.
Escalation/Risk management.
Position Requirements
Required Skills:
AWS Cloud Engineer
Job description:
Candidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas:
Should have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms.
Should have minimum 1-3 years of Operate/Managed Services/Production Support Experience
Should have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc.
Designing and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes.
Should have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc.
Design, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS.
Work together with data scientists and analysts to understand the needs for data and create effective data workflows.
Implementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.
Improve the scalability, efficiency, and cost-effectiveness of data pipelines.
Monitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage.
Implementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Should have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools
Scaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments.
Should have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc.
Should have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc.
Should have Strong communication, problem solving, quantitative and analytical abilities.
Nice To Have
AWS certification
Managed Services- Data, Analytics & Insights Managed Service
At PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients better.
Within our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client's enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC's Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today's dynamic business environment.
Within our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective.
As a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.","AWS DMS, Prefect, dbt, snowflake, Aws Lambda, Hadoop, Collibra, Pyspark, AWS Glue, Informatica, Sql, Spark, Python, AWS"
Senior Data Engineer,DataRobot,5-7 Years,,India,Login to check your skill match score,"Job Description:
DataRobot delivers AI that maximizes impact and minimizes business risk. Our platform and applications integrate into core business processes so teams can develop, deliver, and govern AI at scale. DataRobot empowers practitioners to deliver predictive and generative AI, and enables leaders to secure their AI assets. Organizations worldwide rely on DataRobot for AI that makes sense for their business today and in the future.
Title: Senior Data Engineer (India)
About DataRobot
DataRobot delivers the industry-leading agentic AI applications and platform that maximize impact and minimize risk for your business.. DataRobot's enterprise AI platformdemocratizes data science with end-to-end automation for building, deploying, and managing machine learning models. This platform maximizes business value by delivering AI at scale and continuously optimizing performance over time. The company's proven combination of cutting-edge software and world-class AI implementation, training, and support services empowers any organization, regardless of size, industry, or resources, to drive better business outcomes with AI.
You will be responsible for the following:
Partner with internal customers and business analysts to understand business needs and build strong relationships with key stakeholders.
Develop, deploy, and support analytic data products, such as data marts, ETL jobs (extract/transform/load), functions (in Python/SQL/DBT) in a cloud data warehouse environment using Snowflake, Stitch/Fivetran/Airflow, AWS services (e.g., EC2, lambda, kinesis)
Navigate various data sources and efficiently locate data in a complex data ecosystem.
Work closely with our data analysts and data scientists to build data models and metrics to support their analytics needs.
Maintain and support deployed ETL pipelines and ensure data quality.
Develop monitoring and alerting systems to provide visibility into the health of data infrastructure, cloud applications, and data pipelines.
Partner with the IT enterprise applications and engineering teams on integration efforts between systems that impact data & Analytics
Requirements:
BA/BS preferred in a technical or engineering field
5-7 years of experience in a data engineering or data analyst role.
Strong understanding of data warehousing concepts, working experience with relational databases (Snowflake, Redshift, Postgres, etc.), and SQL.
Experience working with cloud providers like AWS, Azure, GCP, etc.
Solid programming foundations and proficiency in data-related languages like Python, Scala, and R.
Experience with DevOps workflows and tools like DBT, GitHub, Airflow, etc.
Experience with an infrastructure-as-code tool such as Terraform or CloudFormation
Excellent communication skills. Ability to effectively communicate with both technical and non-technical audiences
Knowledge of real-time stream technologies like AWS Firehose, Spark, etc.
Highly collaborative in working with teammates and stakeholders
AWS cloud certification is a plus
The talent and dedication of our employees are at the core of DataRobot's journey to be an iconic company. We strive to attract and retain the best talent by providing competitive pay and benefits with our employees well-being at the core. Here's what your benefits package may include depending on your location and local legal requirements: Medical, Dental & Vision Insurance, Flexible Time Off Program, Paid Holidays, Paid Parental Leave, Global Employee Assistance Program (EAP) and more!
DataRobot Operating Principles:
Wow Our Customers
Set High Standards
Be Better Than Yesterday
Be Rigorous
Assume Positive Intent
Have the Tough Conversations
Be Better Together
Debate, Decide, Commit
Deliver Results
Overcommunicate
Research shows that many women only apply to jobs when they meet 100% of the qualifications while many men apply to jobs when they meet 60%. At DataRobot we encourage ALL candidates, especially women, people of color, LGBTQ+ identifying people, differently abled, and other people from marginalized groups to apply to our jobs, even if you do not check every box. We'd love to have a conversation with you and see if you might be a great fit.
DataRobot is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. DataRobot is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities. Please see the United States Department of Labor's EEO poster and EEO poster supplement for additional information.
All applicant data submitted is handled in accordance with our Applicant Privacy Policy.","Airflow, Stitch, AWS Firehose, R, dbt, snowflake, Fivetran, Github, Cloudformation, Scala, Sql, Terraform, Spark, Python"
Data Engineer,AhinsaAI,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Data Engineer
Location: Bengaluru, India (Full-time)
At AhinsaAI, we're building a trusted foundation for enterprise AI - governed, ethical, and aligned. Our platform delivers loyal AI solutions designed with consent, transparency, and institutional oversight at their core. We're seeking a Data Engineer to architect scalable, secure, and privacy-preserving data infrastructure that powers our mission across regulated industries.
Key Responsibilities
Data Architecture: Design and implement modular, high-availability data systems optimized for AI/ML workflows, compliance, and scale.
Pipeline Development: Build and maintain robust ETL/ELT pipelines for structured and unstructured data ensuring integrity, traceability, and minimal latency.
Privacy-First Engineering: Embed data anonymization, encryption, and access controls into every layer of the data stack aligned with HIPAA, GDPR, and AI governance frameworks.
Automation & Reliability: Drive automation across ingestion, transformation, and validation processes using modern data engineering best practices.
Cross-Functional Collaboration: Partner with AI researchers, compliance officers, and product teams to deliver secure, compliant, and analytics-ready data systems.
What We're Looking For
Education: Bachelor's/Master's in Computer Science, Data Engineering, or a related field.
Experience: 3+ years of experience building data infrastructure in cloud-native, high-scale environments - preferably in fintech, healthtech, or AI startups.
Tech Stack: Expertise in Python and SQL; hands-on experience with Spark, Kafka, Airflow, or dbt; bonus: familiarity with Snowflake, BigQuery, Redshift, or Lakehouse architectures.
Cloud & Infra: Proficient with AWS/GCP/Azure; experience with Terraform, Docker, Kubernetes is a plus.
Mindset: Privacy-aware, systems-driven, and aligned with ethical tech practices. You think in terms of governance, not just performance.
Team Fit: Autonomous, reliable, and excited to build future-proof systems that power responsible AI at scale.
Why Join AhinsaAI
Build Trust-First AI: Design the data foundation for AI solutions rooted in transparency, alignment, and institutional trust.
Global Impact: Your infrastructure will support Fortune 500 clients deploying responsible AI across finance, healthcare, and public sectors.
Frontier Innovation: Work at the intersection of data engineering, AI alignment, and privacy-enhancing technologies.
High Ownership: Shape core systems with autonomy, purpose, and long-term impact.
How to Apply
If you're excited to build ethical data infrastructure for the age of intelligent systems - we'd love to hear from you.
Send your resume, a short note on why you're excited to join AhinsaAI, and any relevant links (GitHub, portfolio, case studies) to [HIDDEN TEXT]
Learn more: https://Ahinsa.ai","Airflow, Lakehouse architectures, snowflake, dbt, BigQuery, Kafka, Redshift, Sql, Gcp, Docker, Terraform, Spark, Azure, Kubernetes, Python, AWS"
Data Engineer,Knack Consulting Services Pvt Ltd.,7-12 Years,,"Delhi, India",Login to check your skill match score,"Job Title Data Engineer
Mandatory Skills - Azure/ AWS + Data Bricks + Pyspark
Years Of Experience 7 to 12 Years
Location- Hyderabad, Greater Noida, Gurgaon, Pune
Job Description
Must to Have Bachelor's degree in Computer Science, Information Technology, or a related field.
7+ years of experience in data engineering with a focus on Azure cloud services.
Proficiency in Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Azure SQL Database.
Strong experience with SQL, Python, or other scripting languages.
Familiarity with data modelling, ETL design, and big data tools such as Hadoop or Spark.
Experience with data warehousing concepts, data lakes, and data pipelines.
Understanding of data governance, data quality, and security best practices.
Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment.
Data factory, data bricks etc The ideal candidate will have extensive experience in data engineering, working with Azure cloud services, and designing and implementing scalable data solutions.
Candidate will play a crucial role in developing, optimizing, and maintaining data pipelines and architectures, ensuring data quality and availability across various platform.
Develop, and maintain data pipelines and ETL processes using Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Build and optimize data storage solutions using Azure Data Lake, Azure SQL Database, and Azure Cosmos DB.
Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.
Implement data quality checks, data governance, and security best practices across data platforms.
Monitor, troubleshoot, and optimize data workflows for performance and scalability.
Develop and maintain data models, data cataloguing, and metadata management.
Automate data integration and transformation processes using Azure DevOps and CI/CD pipelines.
Stay up-to-date with emerging Azure technologies and data engineering trends.
Skill- Good to have Azure certification (e.g., Microsoft Certified: Azure Data Engineer Associate) is a plus. Experience with Azure Logic Apps, Azure Functions, and API Management. Knowledge of Power BI, Tableau, or other data visualization tools","Azure SQL Database, Data Bricks, CI CD pipelines, Pyspark, Hadoop, Power Bi, Azure Databricks, Tableau, Sql, Azure Cosmos DB, Azure Data Factory, Azure Synapse Analytics, Spark, Azure Data Lake, Azure, Python, Azure DevOps, AWS"
Senior Principal Data Engineer,MakeMyTrip,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Senior Principal Data Engineer
Experience: Must have 10+ years of experience
About Role:
We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.
About us:
Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.
Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.
GO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.
About the team:
MakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.
Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.
There are two key responsibilities of Data Engineering team:
One to develop the platform for data capture, storage, processing, serving and querying.
Second is to develop data products starting from;
o personalization & recommendation platform
o customer segmentation & intelligence
o data insights engine for persuasions and
o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users
We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.
Technology experience:
Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture
Extensive experience in data modeling and database design
At least 6+ years of hands-on experience in Spark/BigData Tech stack
Stream processing engines Spark Structured Streaming/Flink
Analytical processing on Big Data using Spark
At least 6+ years of experience in Scala
Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems
At least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform
At least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc
Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies
Should have experience in architecting and implementing domain centric big data solutions
Ability to frame architectural decisions and provide technology leadership & direction
Excellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements
Job Requirements
Role/ Job Title: Data Engineer - Gen AI
Function/ Department: Data & Analytics
Place of Work: Mumbai
Job Purpose
The data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.
Job & Responsibilities
Build data engineering pipeline focused on unstructured data pipelines
Conduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.
Design, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.
Work on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability
Drive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets
A suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.
Creating Technical Design Documentation of the projects/pipelines
Good skills in technical debugging of the code in case of issues. Also, working with git for code versioning
Education Qualification
Graduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)
Post-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA
Experience Range : 5-10 years of relevant experience","Data Ingestion, Data engineering pipeline, Big data infrastructure, Technical debugging, HDFS, Data Consumption, Technical Design Documentation, Unstructured data pipelines, Dynamodb, Data Architecture, HBase, Yarn, Mapreduce, Git, Hive, MongoDB, Data Transformation"
Senior Azure Data Engineer,Landmark Group,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Designation Azure Data Engineer
Location Bengaluru, India
Top Skills
Cloud & Azure Technologies: Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, and Azure Databricks/Synapse for complex stored procedures
Programming & Scripting: Proficiency in Python, SQL (compulsory), and Scala; hands-on experience with PLSQL
Data Expertise: Data modelling, data warehousing, and data integration concepts
Big Data & Infrastructure: Familiarity with Apache Spark, Hadoop, and cloud infrastructure management
DevOps & Automation: Experience with Azure DevOps, PowerShell, and automation tools for deployment and monitoring
Data Governance & Security: Robust understanding of data governance practices, data quality management, and security principles
Soft Skills: Strategic planning, strong communication, collaboration, and problem-solving skills
Roles And Responsibilities
Design & Implementation: Architect, implement, and optimize end-to-end data solutions on the Azure ecosystem
ETL Pipeline Development: Build and maintain efficient ETL pipelines using Azure Data Factory while ensuring data quality and integration with Azure Data Lake
Stored Procedures & Analytics: Develop and fine-tune complex stored procedures within Azure Databricks or Synapse to enable advanced analytics on large-scale data (exceeding 50TB)
Data Management: Apply your expertise in SQL, PLSQL, and data modelling to manipulate, transform, and analyze data effectively
Collaboration: Work cross-functionally to support data-driven initiatives; utilize Azure Synapse Analytics to convert raw data into actionable insights
Optimization & Troubleshooting: Monitor and enhance data storage, processing, and retrieval mechanisms while addressing performance issues
Security & Governance: Enforce and maintain strong data governance, adherence to data security standards, and compliance with privacy regulations
Process Automation: Collaborate with DevOps teams to automate deployments, configurations, and monitoring using tools such as Azure DevOps and PowerShell
Qualification
Education: Bachelor's degree in Computer Science, Engineering, or a related field (advanced degree preferred)
Experience: 68 years of hands-on experience designing and implementing data engineering solutions in the Azure ecosystem
Technical Expertise:
Proficiency in developing solutions with Azure Data Factory, Azure Data Lake Storage, Azure Databricks, and Azure Synapse Analytics
Extensive experience in SQL (compulsory) and PLSQL
Strong background in data modelling, data warehousing, and data integration concepts
Familiarity with big data frameworks such as Apache Spark or Hadoop is required
Certifications: Relevant Azure certifications (e.g., Azure Data Engineer Associate or Azure Solutions Architect Expert) are preferred
Industry Experience: Experience in a retail setup is a plus
Employee Value Proposition Join the dynamic team at Landmark Digital and play a pivotal role in driving innovation through state-of-the-art data engineering practices. You'll work with the latest Azure technologies to design, deploy, and optimize robust data pipelines that directly influence strategic, data-driven decision-making. This role offers a collaborative environment that fosters continuous learning, career growth, and the opportunity to impact retail transformation on a global scale.
Powered by JazzHR
EBJ9dGj6Jf","Azure Data Lake Storage, Hadoop, PowerShell, Apache Spark, Azure Databricks, Data Warehousing, Sql, Azure Data Factory, Azure Synapse Analytics, Data Integration, Plsql, Data Modelling, Python, Azure DevOps"
DBT Data Engineer,Cognisol,6-8 Years,,"Chennai, India",Login to check your skill match score,"DBT Data Engineer -
Key Skillset-DBT,Python,SQL,AWS,pYSPARK
Years of Exp- 6 to 7 Years
Work Mode-Work From Home(Candidate should be available for 1st week of Onboarding @ Chennai Loc)
Shift Time-UK Shift time
Notice: Immediate to 15 days only
Placement Type: Contractual Position
Key Responsibilities
Data Pipeline Development: Design, implement, and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from various sources into cloud platforms like AWS.
Cloud Integration: Utilize AWS services such as Glue, Step Functions, Redshift, and Lambda to process and store data efficiently.
Data Transformation: Develop and maintain accurate data pipelines using Python and SQL, transforming data for aggregations, wrangling, quality control, and calculations.
Workflow Automation: Enhance end-to-end workflows with automation tools to accelerate data flow and pipeline management.
Collaboration: Work closely with business analysts, data scientists, and cross-functional teams to understand data requirements and develop solutions that support data-driven decision-making.
Qualifications
Educational Background: Bachelor's or Master's degree in Information Technology, Bioinformatics, Computer Science, or a related field.
Professional Experience: Several years of experience in data engineering, with hands-on expertise in:
Developing and managing large-scale ETL data pipelines on AWS.
Proficiency in Python and SQL for data pipeline development.
Utilizing AWS services such as Glue, Step Functions, Redshift, and Lambda.
Familiarity with tools like Docker, Linux Shell Scripting, Pandas, PySpark, and Numpy.
Soft Skills: Strong problem-solving abilities, excellent communication skills, and the capacity to work collaboratively in a dynamic environment.
Skills: etl,pipelines,dbt,python,aws,pyspark,pipeline,sql,cloud","Step Functions, dbt, Glue, Pyspark, Shell Scripting, Redshift, Sql, Lambda, Numpy, Pandas, Linux, Docker, Python, Etl, AWS"
Data Engineer MS Fabric_8+years,Zorba AI,8-10 Years,,India,Login to check your skill match score,"Company Overview
Zorba Consulting India is a leading consultancy firm focused on delivering innovative solutions and strategies to enhance business performance. With a commitment to excellence, we prioritize collaboration, integrity, and customer-centric values in our operations. Our mission is to empower organizations by transforming data into actionable insights and enabling data-driven decision-making. We are dedicated to fostering a culture of continuous improvement and supporting our team members professional development.
Role Responsibilities
Design and implement data pipelines using MS Fabric.
Develop data models to support business intelligence and analytics.
Manage and optimize ETL processes for data extraction, transformation, and loading.
Collaborate with cross-functional teams to gather and define data requirements.
Ensure data quality and integrity in all data processes.
Implement best practices for data management, storage, and processing.
Conduct performance tuning for data storage and retrieval for enhanced efficiency.
Generate and maintain documentation for data architecture and data flow.
Participate in troubleshooting data-related issues and implement solutions.
Monitor and optimize cloud-based solutions for scalability and resource efficiency.
Evaluate emerging technologies and tools for potential incorporation in projects.
Assist in designing data governance frameworks and policies.
Provide technical guidance and support to junior data engineers.
Participate in code reviews and ensure adherence to coding standards.
Stay updated with industry trends and best practices in data engineering.
Qualifications
8+ years of experience in data engineering roles.
Strong expertise in MS Fabric and related technologies.
Proficiency in SQL and relational database management systems.
Experience with data warehousing solutions and data modeling.
Hands-on experience in ETL tools and processes.
Knowledge of cloud computing platforms (Azure, AWS, GCP).
Familiarity with Python or similar programming languages.
Ability to communicate complex concepts clearly to non-technical stakeholders.
Experience in implementing data quality measures and data governance.
Strong problem-solving skills and attention to detail.
Ability to work independently in a remote environment.
Experience with data visualization tools is a plus.
Excellent analytical and organizational skills.
Bachelor's degree in Computer Science, Engineering, or related field.
Experience in Agile methodologies and project management.
Skills: etl processes,sql,python scripting,data integration,performance tuning,cloud technologies,data quality measures,data quality assurance,data modeling,cloud computing (azure, aws, gcp),python,ms fabric,data governance,databricks,data visualization tools,data warehousing","data visualization tools, MS Fabric, ETL processes, Performance Tuning, Data Warehousing, Data Modeling, Sql, Gcp, Data Governance, Azure, Python, AWS, Cloud Computing"
AWS Data Engineer,"GSPANN Technologies, Inc",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"About GSPANN
GSPANN is a global IT services and consultancy provider headquartered in Milpitas, California (U.S.A.). With five global delivery centers across the globe, GSPANN provides digital solutions that support the customer buying journeys of B2B and B2C brands worldwide.
With a strong focus on innovation and client satisfaction, GSPANN delivers cutting-edge solutions that drive business success and operational excellence. GSPANN helps retail, finance, manufacturing, and high-technology brands deliver competitive customer experiences and increased revenues through our solution delivery, technologies, practices, and operations for each client. For more information, visit www.gspann.com
JD for your reference:
GSPANN is looking for AWS Data Engineer. As we march ahead on a tremendous growth trajectory, we seek passionate and talented professionals to join our growing family.
Job Position-
AWS Data Engineer
Experience- 6+ years
Location- Bangalore
Skills- AWS+Redshift+Snowflake, SQL, Bigdata, StepFunction, Python/ PySpark, Airflow
Responsibilities
Actively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing, roll-out, and support.
Solve complex business problems by utilizing a disciplined development methodology.
Produce scalable, flexible, efficient, and supportable solutions using appropriate technologies.
Analyse the source and target system data. Map the transformation that meets the requirements.
Interact with the client and onsite coordinators during different phases of a project.
Design and implement product features in collaboration with business and Technology stakeholders.
Anticipate, identify, and solve issues concerning data management to improve data quality.
Clean, prepare, and optimize data at scale for ingestion and consumption.
Support the implementation of new data management projects and re-structure the current data architecture.
Implement automated workflows and routines using workflow scheduling tools.
Understand and use continuous integration, test-driven development, and production deployment frameworks.
Participate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards.
Analyze and profile data for the purpose of designing scalable solutions.
Troubleshoot straightforward data issues and perform root cause analysis to proactively resolve product issues.
Required Skills
6+ years experience developing Data and analytic solutions.
Experience building data lake solutions leveraging one or more of the following AWS, EMR, S3, Hive & PySpark
Experience with relational SQL
Experience with scripting languages such as Python
Experience with source control tools such as GitHub and related dev process
Experience with workflow scheduling tools such as Airflow
In-depth knowledge of AWS Cloud (S3, EMR, Databricks)
Has a passion for data solutions.
Has a strong problem-solving and analytical mindset
Working experience in the design, Development, and test of data pipelines.
Experience working with Agile Teams.
Able to influence and communicate effectively, both verbally and in writing, with team members and business stakeholders
Able to quickly pick up new programming languages, technologies, and frameworks.
Bachelor's Degree in computer science
Why Choose GSPANN
At GSPANN, we don't just serve our clientswe co-create. The GSPANNians are passionate technologists who thrive on solving the toughest business challenges, delivering trailblazing innovations for marquee clients. This collaborative spirit fuels a culture where every individual is encouraged to sharpen their skills, feed their curiosity, and take ownership to learn, experiment, and succeed.
We believe in celebrating each other's successesbig or smalland giving back to the communities we call home. If you're ready to push boundaries and be part of a close-knit team that's shaping the future of tech, we invite you to carry forward the baton of innovation with us.
Let's Co-Create the FutureTogether.
Discover Your Inner Technologist
Explore and expand the boundaries of tech innovation without the fear of failure.
Accelerate Your Learning
Shape your career while scripting the future of tech. Seize the ample learning opportunities to grow at a rapid pace.
Feel Included
At GSPANN, everyone is welcome. Age, gender, culture, and nationality do not matter here, what matters is YOU.
Inspire and Be Inspired
When you work with the experts, you raise your game. At GSPANN, you're in the company of marquee clients and extremely talented colleagues.
Enjoy Life
We love to celebrate milestones and victories, big or small. Ever so often, we come together as one large GSPANN family.
Give Back
Together, we serve communities. We take steps, small and large so we can do good for the environment, weaving in sustainability and social change in our endeavors.
We invite you to carry forward the baton of innovation in technology with us.
Let's Co-Create
GSPANN | Consulting Services, Technology Services, and IT Services Provider
GSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.
GSPANN | Consulting Services, Technology Services, and IT Services Provider
GSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.","Airflow, snowflake, StepFunction, Pyspark, Bigdata, Redshift, Python, Sql, AWS"
Sr. Data Engineer - Python Job,YASH Technologies,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"YASH Technologies is a leading technology integrator specializing in helping clients reimagine operating models, enhance competitiveness, optimize costs, foster exceptional stakeholder experiences, and drive business transformation.
At YASH, we're a cluster of the brightest stars working with cutting-edge technologies. Our purpose is anchored in a single truth bringing real positive changes in an increasingly virtual world and it drives us beyond generational gaps and disruptions of the future.
We are looking forward to hire AWS Professionals in the following areas :
Job Description
Data Engineering (DataEng)
IT would be great if we can find someone with prior data engineering experience in Palantir Foundry. It would be an added advantage if the candidate is familiar with the Workshop component within Palantir Foundry.
This Position will be Right to Hire
Experience 7+ years
Degree in computer science, engineering, or similar fields
Skill Set: AWS, Python, PySpark
Primary Responsibilities
Responsible for designing, developing, testing and supporting data pipelines and applications
Industrialize data feeds
Experience in working with cloud environments AWS
Creates data pipelines into existing systems
Experience with enforcing security controls and best practices to protect sensitive data within AWS data pipelines, including encryption, access controls, and auditing mechanisms.
Improves data cleansing and facilitates connectivity of data and applied technologies between both external and internal data sources.
Establishes a continuous quality improvement process and to systematically optimizes data quality
Translates data requirements from data users to ingestion activities
B.Tech/ B.Sc./M.Sc. in Computer Science or related field and 3+ years of relevant industry experience
Interest in solving challenging technical problems
Nice to have test driven development and CI/CD workflows
Knowledge of version control software such as Git and experience in working with major hosting services (e. g. Azure DevOps, Github, Bitbucket, Gitlab)
Nice to have in working with cloud environments such as AWSe especially creating serverless architectures and using infrastructure as code facilities such as CloudFormation/CDK, Terraform, ARM.
Hands-on experience in working with various frontend and backend languages (e.g., Python, R, Java, Scala, C/C++, Rust, Typescript
At YASH, you are empowered to create a career that will take you to where you want to go while working in an inclusive team environment. We leverage career-oriented skilling models and optimize our collective intelligence aided with technology for continuous learning, unlearning, and relearning at a rapid pace and scale.
Our Hyperlearning workplace is grounded upon four principles
Flexible work arrangements, Free spirit, and emotional positivity
Agile self-determination, trust, transparency, and open collaboration
All Support needed for the realization of business goals,
Stable employment with a great atmosphere and ethical corporate culture","R, Palantir Foundry, Java, Github, Rust, C, Cloudformation, Scala, Pyspark, CDK, Git, Terraform, Bitbucket, Typescript, Gitlab, Arm, Python, Azure DevOps, AWS"
GCP Senior Data Engineer,Xebia,4-6 Years,,India,Login to check your skill match score,"We are looking for a Senior Data Engineer with strong expertise in GCP, Databricks, and Airflow to design and implement a GCP Cloud Native Data Processing Framework. The ideal candidate will work on building scalable data pipelines and help migrate existing workloads to a modern framework.
Shift: 2 PM 11 PM
Work Mode: Hybrid (3 days a week) across Xebia locations
Notice Period: Immediate joiners or those with a notice period of up to 30 days
Key Responsibilities:
Design and implement a GCP Native Data Processing Framework leveraging Spark and GCP Cloud Services.
Develop and maintain data pipelines using Databricks and Airflow for transforming Raw Silver Gold data layers.
Ensure data integrity, consistency, and availability across all systems.
Collaborate with data engineers, analysts, and stakeholders to optimize performance.
Document standards and best practices for data engineering workflows.
Required Experience:
4+ years of experience in data engineering, architecture, and pipeline development.
Strong knowledge of GCP, Databricks, PySpark, and BigQuery.
Experience with Orchestration tools like Airflow, Dagster, or GCP equivalents.
Understanding of Data Lake table formats (Delta, Iceberg, etc.).
Proficiency in Python for scripting and automation.
Strong problem-solving skills and collaborative mindset.
How to Apply
If you're interested, please share your updated CV along with the following details to [HIDDEN TEXT]:
Full Name:
Total Experience:
Current CTC:
Expected CTC:
Current Location:
Preferred Location:
Notice Period / Last Working Day (if serving notice):
Primary Skill Set (Choose from above or mention any other relevant expertise):
Please apply only if you have not applied recently or are not currently in the interview process for any open roles at Xebia.
Looking forward to your response!
Best regards,
Vijay S
Assistant Manager - TAG
https://www.linkedin.com/in/vijay-selvarajan/","Orchestration tools, Airflow, GCP Cloud Services, BigQuery, Gcp, Pyspark, Spark, Databricks, Python"
Data Engineer II,Tekion Corp,3-5 Years,,"Chennai, India",Login to check your skill match score,"About Tekion:
Positively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.
Key Responsibilities:
Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.
Should be able to design (high level / low level) software solutions for the new requirements.
Coding independently and with other team members with proper software industry standard best practices.
Collaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.
Qualifications:
B.E/MTech in computer science
3 - 5 years of relevant work experience.
Experience in building scalable products with preferably big data.
Excellent Python coding skills (Mandatory)
Experience in Apache spark, Data Lake and other Big data technologies.
Experience in either Data Warehouses or Relational Database is mandatory.
Experience in AWS cloud
Mandatory Skills: Python , Spark
Tekion is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, victim of violence or having a family member who is a victim of violence, the intersectionality of two or more protected categories, or other applicable legally protected characteristics.
For more information on our privacy practices, please refer to our Applicant Privacy Notice here.","data warehouses, Relational Database, Apache Spark, Data Lake, Python, Aws Cloud, Big Data Technologies"
ML Data Engineer,S&P Global,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"About The Role
Grade Level (for internal use):
10
Responsibilities
To work closely with various stakeholders to collect, clean, model and visualise datasets.
To create data driven insights by researching, designing and implementing ML models to deliver insights and implement action-oriented solutions to complex business problems
To drive ground-breaking ML technology within the Modelling and Data Science team.
To extract hidden value insights and enrich accuracy of the datasets.
To leverage technology and automate workflows creating modernized operational processes aligning with the team strategy.
To understand, implement, manage, and maintain analytical solutions & techniques independently.
To collaborate and coordinate with Data, content and modelling teams and provide analytical assistance of various commodity datasets
To drive and maintain high quality processes and delivering projects in collaborative Agile team environments.
Requirements
7+ years of programming experience particularly in Python
4+ years of experience working with SQL or NoSQL databases.
1+ years of experience working with Pyspark.
University degree in Computer Science, Engineering, Mathematics, or related disciplines.
Strong understanding of big data technologies such as Hadoop, Spark, or Kafka.
Demonstrated ability to design and implement end-to-end scalable and performant data pipelines.
Experience with workflow management platforms like Airflow.
Strong analytical and problem-solving skills.
Ability to collaborate and communicate effectively with both technical and non-technical stakeholders.
Experience building solutions and working in the Agile working environment
Experience working with git or other source control tools
Strong understanding of Object-Oriented Programming (OOP) principles and design patterns.
Knowledge of clean code practices and the ability to write well-documented, modular, and reusable code.
Strong focus on performance optimization and writing efficient, scalable code.
Nice To Have
Experience working with Oil, gas and energy markets
Experience working with BI Visualization applications (e.g. Tableau, Power BI)
Understanding of cloud-based services, preferably AWS
Experience working with Unified analytics platforms like Databricks
Experience with deep learning and related toolkits: Tensorflow, PyTorch, Keras, etc.
About S&P Global Commodity Insights
At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.
We're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.
S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.
For more information, visit http://www.spglobal.com/commodity-insights.
What's In It For You
Our Purpose
Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.
Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.
Our People
We're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.
From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.
Our Values
Integrity, Discovery, Partnership
At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.
Benefits
We take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.
Our Benefits Include
Health & Wellness: Health care coverage designed for the mind and body.
Flexible Downtime: Generous time off helps keep you energized for your time on.
Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.
Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.
Family Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.
Beyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.
For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries
Inclusive Hiring And Opportunity At S&P Global
At S&P Global, we are committed to fostering an inclusive workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and equal opportunity, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.
Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.
If you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.
US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_ English_formattedESQA508c.pdf
IFTECH202.1 - Middle Professional Tier I (EEO Job Group)
Job ID: 314321
Posted On: 2025-04-23
Location: Hyderabad, Telangana, India","Airflow, Object-Oriented Programming, Pyspark, Hadoop, Kafka, Sql, Tensorflow, Nosql, Git, Pytorch, Spark, Keras, Python"
ML Data Engineer,S&P Global,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"About The Role
Grade Level (for internal use):
10
Responsibilities
To work closely with various stakeholders to collect, clean, model and visualise datasets.
To create data driven insights by researching, designing and implementing ML models to deliver insights and implement action-oriented solutions to complex business problems
To drive ground-breaking ML technology within the Modelling and Data Science team.
To extract hidden value insights and enrich accuracy of the datasets.
To leverage technology and automate workflows creating modernized operational processes aligning with the team strategy.
To understand, implement, manage, and maintain analytical solutions & techniques independently.
To collaborate and coordinate with Data, content and modelling teams and provide analytical assistance of various commodity datasets
To drive and maintain high quality processes and delivering projects in collaborative Agile team environments.
Requirements
7+ years of programming experience particularly in Python
4+ years of experience working with SQL or NoSQL databases.
1+ years of experience working with Pyspark.
University degree in Computer Science, Engineering, Mathematics, or related disciplines.
Strong understanding of big data technologies such as Hadoop, Spark, or Kafka.
Demonstrated ability to design and implement end-to-end scalable and performant data pipelines.
Experience with workflow management platforms like Airflow.
Strong analytical and problem-solving skills.
Ability to collaborate and communicate effectively with both technical and non-technical stakeholders.
Experience building solutions and working in the Agile working environment
Experience working with git or other source control tools
Strong understanding of Object-Oriented Programming (OOP) principles and design patterns.
Knowledge of clean code practices and the ability to write well-documented, modular, and reusable code.
Strong focus on performance optimization and writing efficient, scalable code.
Nice To Have
Experience working with Oil, gas and energy markets
Experience working with BI Visualization applications (e.g. Tableau, Power BI)
Understanding of cloud-based services, preferably AWS
Experience working with Unified analytics platforms like Databricks
Experience with deep learning and related toolkits: Tensorflow, PyTorch, Keras, etc.
About S&P Global Commodity Insights
At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.
We're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.
S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.
For more information, visit http://www.spglobal.com/commodity-insights.
What's In It For You
Our Purpose
Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.
Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.
Our People
We're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.
From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.
Our Values
Integrity, Discovery, Partnership
At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.
Benefits
We take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.
Our Benefits Include
Health & Wellness: Health care coverage designed for the mind and body.
Flexible Downtime: Generous time off helps keep you energized for your time on.
Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.
Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.
Family Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.
Beyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.
For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries
Inclusive Hiring And Opportunity At S&P Global
At S&P Global, we are committed to fostering an inclusive workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and equal opportunity, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.
Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.
If you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.
US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_ English_formattedESQA508c.pdf
IFTECH202.1 - Middle Professional Tier I (EEO Job Group)
Job ID: 314321
Posted On: 2025-04-23
Location: Hyderabad, Telangana, India","Airflow, Pyspark, Hadoop, Kafka, Sql, Tensorflow, Nosql, Git, Pytorch, Spark, Keras, Python"
Principal Data Engineer,Lenity Health,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role Overview
Location: Hebbal, Bengaluru
Work Mode: Monday to Friday | On-site
Compensation: 3035 LPA
Experience Level: Senior-Level (510 years)
About Lenity Health
The healthcare industry operates on systems so outdated, that it clogs the flow of innovation and efficient care coordination. These inefficiencies lead to lost opportunities in modernizing care delivery.
Lenity Health is a venture-backed, early stage startup with a deep subject matter expertise in US healthcare, redefining healthcare technology with AI at its core. We aim to coordinate care for all seniors in the United States, using AI agents and data driven care management tools.
We take pride in transforming healthcare delivery, improving health outcomes, and streamlining processes to reduce the cost of care. Our solutions are optimized to reduce inefficiencies and ensure patient interventions are made at the right time, for the right reasons.
If you are an inquisitive innovator with a passion to learn and make an impact, then Lenity is the place for you.
Position Summary
As Principal Data Engineer, you will lead the technical vision and implementation of Lenity's data platform for one of its customers. You'll architect and scale the cloud data infrastructure, build robust data pipelines, and ensure high-quality, compliant, and actionable data across all lines of businessclinical, operational, and financial.
You'll work closely with the leadership, analysts, clinicians, and product teams to deliver trusted data products that power decisions across the organization. This is a strategic, hands-on role for a senior technologist who thrives in a collaborative, fast-paced environment.
Responsibilities
Architect, build, and optimize Azure-based data platform, integrating data across EHRs, claims systems, and third-party healthcare APIs.
Lead development of ELT workflows using dbt, Airflow, and Azure Data Factory to deliver scalable, testable, and monitored data pipelines.
Develop and enforce standards for data modeling, transformation, and governance, with a focus on high performance, maintainability, and transparency.
Partner with analytics and operations teams to enable rapid insights across population health, provider performance, utilization, and cost-of-care datasets.
Ensure full compliance with HIPAA, HITECH, and internal security policies, working closely with compliance and security teams.
Guide integration of structured and unstructured data from EHRs, claims, lab results, FHIR/HL7 feeds, and SDoH data sources.
Implement and manage tools for data cataloging, lineage, and observability, driving improved trust in enterprise data assets.
Mentor and support a growing team of data engineers, enabling a strong engineering culture rooted in collaboration, accountability, and innovation.
Contribute to enterprise data strategy and roadmap, aligning engineering efforts with business priorities and growth initiatives.
Required Qualifications
510 years of experience in data engineering, with at least 3 years in a lead or principal-level role.
Proven expertise with dbt, Apache Airflow, and Azure Data Factory in production healthcare data environments.
Deep understanding of SQL, Python, and cloud-native data warehouse platforms such as Snowflake or Azure Synapse.
Demonstrated experience architecting and scaling data platforms on Azure or similar public cloud infrastructure.
Hands-on experience with healthcare data standards and formats including HL7, FHIR, X12 837/835, and EHR/claims data.
Strong knowledge of HIPAA compliance, PHI handling, and data governance in regulated healthcare settings.
Excellent communication and leadership skills with a track record of mentoring data teams and partnering cross-functionally.
Preferred Qualifications
Experience working in a Healthcare Organization.
Familiarity with Power BI, Looker, or other data visualization tools.
Exposure to real-time or event-driven data architecture (Kafka, Spark Streaming).
Knowledge of data cataloging tools (e.g., Alation, Microsoft Purview).
Strong opinions on CI/CD for data pipelines, Git-based workflows, and infrastructure-as-code.
Why Choose Lenity Health
Accelerate Your Career Access certifications, courses, and mentorship to sharpen your skills and grow professionally.
Comprehensive Health Benefits Enjoy full health coverage, including free annual dental and health check-ups, plus unlimited teleconsultations.
Beyond Insurance We offer not just group health insurance, but also term and accident insurance for added financial security.
Stay Active Get reimbursed for health club memberships to keep your body and mind energized.
Workplace Perks Enjoy a complimentary daily meal at the office to keep you fueled throughout the day.
Innovative Culture Collaborate with some of the brightest minds in health tech, where your ideas truly make an impact.
Ready to Make a Difference
Apply now and be part of a team that's transforming healthcare for millions of seniors across the U.S.
Apply via LinkedIn Easy Apply
Join us in reshaping senior healthcare with expertise, compassion, and innovation
#PrincipalDataEngineer #DataEngineeringJobs #HealthcareTech #AzureDataFactory #dbtJobs #Airflow #HealthcareData #OnsiteJobs #BangaloreTechJobs #AIinHealthcare #JoinLenityHealth #HiringNow #PlatformEngineering #DataPipelineJobs #TechForGood","FHIR, dbt, snowflake, X12, 835, 837, Sql, Apache Airflow, Azure Synapse, Azure Data Factory, Hl7, Python"
Lead AI/Data Engineer,Medtronic,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"At Medtronic you can begin a life-long career of exploration and innovation, while helping champion healthcare access and equity for all. You'll lead with purpose, breaking down barriers to innovation in a more connected, compassionate world.
A Day in the Life
As a Senior Data Engineer Techno-Manager, where you will lead the design, development, and implementation of scalable data solutions. In this role, you will provide technical leadership (90%) while also mentoring and guiding a team of Data Scientists and Power BI Developers (10%). You will work closely with cross-functional teams to enable data-driven decision-making and support advanced analytics initiatives.
Join our Global Finance Analytics Center of Excellence (COE) as a Senior Data Engineer Techno-Manager, where you will lead the design, development, and implementation of scalable data solutions. In this role, you will provide technical leadership (90%) while also mentoring and guiding a team of Data Scientists and Power BI Developers (10%). You will work closely with cross-functional teams to enable data-driven decision-making and support advanced analytics initiatives. This role requires an average of 2-3 days per week of overlapping work hours with the USA team to ensure seamless collaboration.
A Day in the Life
As a Senior Data Engineer Techno-Manager, you will:
Lead the end-to-end data engineering efforts, ensuring efficient data pipelines, ETL processes, and data governance practices.
Architect and optimize data solutions in Snowflake, Azure, and other cloud platforms for enterprise analytics and AI/ML models.
Collaborate with Data Scientists to design and implement scalable machine-learning pipelines.
Oversee Power BI development, ensuring efficient data modeling and visualization best practices.
Manage stakeholder expectations while delivering high-quality, reliable, and scalable data solutions.
Mentor and guide junior engineers, fostering best practices in coding, architecture, and data pipeline automation.
Ensure data integrity, security, and compliance while working with structured and unstructured data sources.
Work closely with business and IT teams to drive automation, self-service analytics, and cloud-based transformations.
Engage with USA teams for strategic discussions, project updates, and technical alignment .
As a People Manager, you will provide leadership, coaching, and career development opportunities to your team members. You will foster a culture of innovation, continuous learning, and collaboration, ensuring that team members have the resources and guidance needed to succeed in their roles. You will also facilitate communication between global teams and ensure that the team is aligned with business objectives.
This role requires 2-3 hours of overlap with USA teams, typically during early mornings or late evenings, to align with project requirements, attend stakeholder meetings, and ensure smooth collaboration across different time zones.
Must Have: Minimum Requirements
Bachelor's or Master's degree in Computer Science, Engineering, Data Science, or related field.
10+ years of experience in Data Engineering, Big Data, or Cloud Data Technologies.
Strong expertise in Snowflake, SQL, Python, and ETL processes.
Experience in Power BI (data modeling, DAX, performance optimization, and visualization best practices).
Cloud experience with Azure, AWS, or GCP, including data lakes, warehousing, and orchestration tools.
Experience with modern data stack (Databricks, Apache Spark, Airflow, etc.).
Exposure to AI/ML models and working with Data Scientists for productionizing models.
Strong problem-solving and communication skills with a global mindset.
Ability to balance technical depth with stakeholder engagement and people management.
Nice to Have
Experience in Snowflake performance tuning and cost optimization.
Hands-on experience with CI/CD pipelines for data engineering workflows.
Knowledge of APIs and integration with enterprise applications.
Prior experience leading small teams or mentoring engineers.
Familiarity with SAP, ERP, OneStream or other enterprise data sources.
Physical Job Requirements
The above statements are intended to describe the general nature and level of work being performed by employees assigned to this position, but they are not an exhaustive list of all the required responsibilities and skills of this position.
Benefits & Compensation
Medtronic offers a competitive Salary and flexible Benefits Package
A commitment to our employees lives at the core of our values. We recognize their contributions. They share in the success they help to create. We offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage.
This position is eligible for a short-term incentive called the Medtronic Incentive Plan (MIP).
About Medtronic
We lead global healthcare technology and boldly attack the most challenging health problems facing humanity by searching out and finding solutions.
Our Mission to alleviate pain, restore health, and extend life unites a global team of 95,000+ passionate people.
We are engineers at heart putting ambitious ideas to work to generate real solutions for real people. From the R&D lab, to the factory floor, to the conference room, every one of us experiments, creates, builds, improves and solves. We have the talent, diverse perspectives, and guts to engineer the extraordinary.
Learn more about our business, mission, and our commitment to diversity here","Airflow, snowflake, ETL processes, Power Bi, Apache Spark, Sql, Gcp, Databricks, Dax, Azure, Python, AWS"
Staff Data Engineer,Zinnia,6-8 Years,,"Pune, India",Login to check your skill match score,"Who We Are
Zinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.
Who You Are
As a seasoned Data Engineer specializing in data engineering, you bring extensive expertise in optimizing data workflows using various database tools like Oracle, BigQuery, and SQL Server. You possess a deep understanding of ELT/ETL processes, data integration, and have a strong command of Python for data manipulation and automation tasks. You will possess advanced expertise in working with data platforms like Google Big Query, DBT, Python, and Airflow. Responsible for designing and maintaining scalable ETL pipelines, optimizing complex data systems, and ensuring smooth data flow across different platforms. As a Senior Data Engineer, you will also be required to work collaboratively in a team and contribute to building data infrastructure that drives business insights
What You'll Do
Design, develop, and optimize complex ETL pipelines that integrate large data sets from various sources.
Build and maintain high-performance data models using Google BigQuery and DBT for data transformation.
Develop Python scripts for data ingestion, transformation, and automation.
Implement and manage data workflows using Apache Airflow for scheduling and orchestration.
Collaborate with data scientists, analysts, and other stakeholders to ensure data availability, reliability, and performance.
Troubleshoot and optimize data systems, identifying issues and resolving them proactively.
Work on cloud-based platforms, particularly AWS, to leverage scalability and storage options for data pipelines.
Ensure data integrity, consistency, and security across systems.
Take ownership of end-to-end data engineering tasks while mentoring junior team members.
Continuously improve processes and technologies for more efficient data processing and delivery.
Act as a key contributor to developing and supporting complex data architectures.
What You'll Need
Bachelor's degree in computer science, Information Technology, or a related field.
6+ years of hands-on experience in Data Engineering or related fields, with a strong background in building and optimizing data pipelines
Strong proficiency in Google Big Query, including designing and optimizing queries.
Advanced knowledge of DBT for data transformation and model management.
Proficiency in Python for data engineering tasks, including scripting, data manipulation, and automation.
Solid experience with Apache Airflow for workflow orchestration and task automation.
Extensive experience in building and maintaining ETL pipelines.
Familiarity with cloud platforms, particularly AWS (Amazon Web Services), including tools like S3, Lambda, Redshift, or Glue.
Java knowledge is a plus.
Excellent problem-solving and troubleshooting abilities.
Strong communication and collaboration skills with the ability to work effectively in a team environment.
Self-motivated, detail-oriented, and able to work with minimal supervision.
Ability to manage multiple priorities and deadlines in a fast-paced environment.
Experience with other cloud platforms (e.g., GCP, Azure) is a plus.
Knowledge of data warehousing best practices and architecture.
WHAT'S IN IT FOR YOU
At Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability","dbt, Google BigQuery, Apache Airflow, SQL Server, Oracle, Python, Etl, AWS"
Data Engineer/BI Engineer,canibuild,4-6 Years,,India,Login to check your skill match score,"About Us
Canibuild automates the residential construction industry's design, approval, and sales processes, allowing clients to answer Can I build this on this plot of land instantly. As a fast-growing SaaS platform backed by Australia's largest hedge fund, we serve clients across Australia, New Zealand, Canada, and the US.
Role Overview
We are seeking a highly skilled Data Engineer / BI Engineer with experience in designing ETL/ELT pipelines, managing databases, and developing dashboards using Power BI. The ideal candidate will be responsible for handling end-to-end data workflows, from data extraction and transformation to visualization and insights generation. This role also provides opportunities to upskill and contribute to AI-driven data initiatives, making it an excellent fit for professionals eager to grow in a modern data ecosystem.
Key Responsibilities
Design, develop, and maintain ETL/ELT pipelines using Python, Airflow, and SQL in an AWS environment
Manage and optimize data lake and data warehouse solutions on AWS
Develop and maintain data-driven dashboards and reports in Power BI, connecting to SQL Server and PostgreSQL Aurora databases
Extract and integrate data from third-party APIs to populate the data lake
Perform data profiling and source system analysis to ensure data quality and integrity
Collaborate with business stakeholders to capture and understand data requirements
Optimize SQL queries for performance and ensure efficient database operations
Implement best practices for data engineering, visualization, and database management
Participate in architectural decisions and contribute to the continuous improvement of data solutions
Follow agile and lean development practices for efficient project execution
Independently validate and assess the accuracy of data outputs before delivery, ensuring results align with business expectations
Proactively evaluate different technical approaches and suggest alternatives to ensure optimal outcomes within the broader system and business context
Stay updated on the latest AI and data engineering advancements, with opportunities to apply AI-powered solutions
Requirements
4+ years of experience in data engineering, ETL/ELT pipeline development, and database management
Strong expertise in SQL (T-SQL, MS SQL) with a focus on query optimization and database performance tuning
Proficiency in Python (including data-specific libraries such as Pandas, NumPy, etc.) and Airflow for ETL/ELT processes
Experience extracting and managing data from third-party APIs (REST, JSON)
Proven experience in designing and developing data warehouse solutions on AWS
Strong expertise in Power BI for data visualization, dashboard creation, and connecting to SQL Server/PostgreSQL Aurora
Familiarity with agile methodologies and a continuous improvement mindset
Demonstrated ability to think critically and evaluate multiple technical solutions in the broader context of system architecture and business goals
Excellent problem-solving skills and the ability to proactively identify and implement alternative solutions
Strong communication skills and ability to work collaboratively in a team-oriented environment
Willingness to upskill in AI-driven data solutions and contribute to AI-powered applications
Preferred Qualifications
Exposure to AWS cloud data services such as RedShift, Athena, Lambda, Glue, etc
Experience with additional BI tools like Tableau
Knowledge of data lake architectures and best practices
Experience with AI-driven data analytics, or integrating AI models with BI solutions
Benefits
Work with a talented team of data and AI professionals on meaningful, industry-transforming projects
Opportunity to gain hands-on experience with AI technologies and modern data engineering practices
Competitive salary, benefits, and opportunities for growth
A collaborative, fast-paced, and innovative culture that values initiative, ownership, and smart solutions
Flexible remote work opportunities","Airflow, Aurora, Data Warehouse, T-sql, Power Bi, PostgreSQL, Json, Sql, ELT, MS SQL, REST, Data Lake, Python, Etl"
Senior Microsoft Data Engineer,OneMagnify,Fresher,,"Chennai, India",Login to check your skill match score,"OneMagnify is a global performance marketing organization working at the intersection of brand marketing, technology, and analytics. The Company's core offerings accelerate business, amplify real-time results, and help set their clients apart from their competitors. OneMagnify partners with clients to design, implement and manage marketing and brand strategies using analytical and predictive data models that provide valuable customer insights to drive higher levels of sales conversion.
OneMagnify's commitment to employee growth and development extends far beyond typical approaches. We take great pride in fostering an environment where each of our 700+ colleagues can thrive and achieve their personal best. OneMagnify has been recognized as a Top Workplace, Best Workplace and Cool Workplace in the United States for 10 consecutive years and recently was recognized as a Top Workplace in India.
OneMagnify needs a Senior Snowflake Data Engineer to join our team. We blend brand marketing, technology, and analytics to boost business and client competitiveness.
At OneMagnify, we are dedicated to encouraging an environment where every individual can thrive and achieve their personal best. We have been consistently recognized as a Top Workplace, Best Workplace, and Cool Workplace in the United States for the past 10 years. Join our team and be a part of our outstanding culture!
Location: Chennai, India
Join OneMagnify and be part of a world-class team that competes at the highest level. Apply now and take your career to new heights!
Responsibilities:
Design and develop scalable and efficient data solutions leveraging both on-premise SQL Server and Azure Data Services.
Collaborate with cross-functional teams to define, implement, and optimize hybrid data architectures.
Ensure data performance, security, and integrity across SQL Server, Azure Synapse Analytics, Azure Data Factory, and Azure SQL Database.
Implement data migration and integration projects between on-premise SQL Server and Azure cloud environments.
Develop and maintain ETL pipelines using Azure Data Factory, SQL Server Integration Services (SSIS), Databricks, or Synapse Pipelines.
Provide technical mentorship and support to junior developers in both on-prem and cloud data environments.
Work closely with analytics and business intelligence teams to ensure effective data modeling, reporting, and governance across platforms.
Requirements:
Bachelor's degree in Information Technology, Computer Science, or a related field, or equivalent experience.
Proven experience in a Senior Data Engineer or similar role with expertise in both on-premise SQL Server and Azure cloud technologies.
Strong proficiency in SQL, database administration, and performance tuning across SQL Server and Azure databases.
Experience with Azure Synapse Analytics, Azure Data Factory, Azure SQL Database, and SQL Server Integration Services (SSIS).
Familiarity with data lakes, structured and unstructured data processing, and hybrid data architectures.
Knowledge of Azure security standard methodologies and data governance principles.
Strong problem-solving skills and attention to detail.
Ability to work in a fast-paced, collaborative environment.
Strong written and verbal communication skills.
Benefits
We offer a comprehensive benefits package including Medical Insurance, PF, Gratuity, paid holidays, and more.
About Us
Whether it's awareness, advocacy, engagement, or efficacy, we move brands forward with work that connects with audiences and delivers results. Through meaningful analytics, engaging communications and innovative technology solutions, we help clients tackle their most ambitious projects and overcome their biggest challenges.
We are an equal opportunity employer
We believe that Innovative ideas and solutions start with unique perspectives. That's why we're committed to providing every employee a workplace that's free of discrimination and intolerance. We're proud to be an equal opportunity employer and actively search for like-minded people to join our team","Azure SQL Database, Azure Data Services, ETL pipelines, Azure Data Factory, Azure Synapse Analytics, SQL Server, Data Governance, Databricks, Data Migration, Data Modeling"
Lead Data Engineer,M&G Global Services Private Limited,12-14 Years,,"Mumbai, India",Login to check your skill match score,"We are M&G Global Services Private Limited (formerly known as 10FA India Private Limited, and prior to that Prudential Global Services Private Limited). We are a fully owned subsidiary of the M&G plc group of companies, operating as a Global Capability Centre providing a range of value adding services to the Group since 2003. At M&G our purpose is to give everyone real confidence to put their money to work. As an international savings and investments business with roots stretching back more than 170 years, we offer a range of financial products and services through Asset Management, Life and Wealth. All three operating segments work together to deliver attractive financial outcomes for our clients, and superior shareholder returns.
M&G Global Services has rapidly transformed itself into a powerhouse of capability that is playing an important role in M&G plc's ambition to be the best loved and most successful savings and investments company in the world.
Our diversified service offerings extending from Digital Services (Digital Engineering, AI, Advanced Analytics, RPA, and BI & Insights), Business Transformation, Management Consulting & Strategy, Finance, Actuarial, Quants, Research, Information Technology, Customer Service, Risk & Compliance and Audit provide our people with exciting career growth opportunities. Through our behaviours of telling it like it is, owning it now, and moving it forward together with care and integrity; we are creating an exceptional place to work for exceptional talent.
Job Description
Job Title
Lead Data Engineer
Grade
2B
Level
Senior Manager Data
Job Function
Digital Transformation
Job Sub Function
Azure Data Engineering & DevOps & BI
Reports to
3B (VP Data Engineering)
Location
Mumbai
Business Area
M&G Global Services
Overall Job Purpose
To implement data engineering solutions using latest technologies available in Azure Cloud space conforming to the best in class design standard & agreed requirements to achieve business objective
Accountabilities/Responsibilities
Lead data engineering projects to build and operationalize data solutions for business using Azure services in combination with custom solutions Azure Data Factory, Azure Data Flows, Azure Databricks, Azure Data Lake Gen 2, Azure SQL etc
Proven experience on leading a team of data engineers providing technical guidance and ensuring alignment with agreed architectural principles
Experience in migrating on-premise data warehouses to data platforms on AZURE cloud
Designing and implementing data engineering, ingestion and transformation functions using ADF, Databricks
Proficient in Py-Spark
Experience in building Python based APIs on Azure Function Apps
Experience on Azure Logic apps
Experience in Lakehouse/Datawarehouse implementation using modern data platform architecture
Capacity Planning and Performance Tuning on ADF & Databricks pipelines
Support data visualization development using Power BI
Exposure across all the SDLC process, including testing and deployment
Experience in relational and dimensional modelling, including big data technologies
Experience in Azure DevOps Build CI/CD pipelines for ADF, ADLS, Databricks, Azure SQL DB etc
Experience of working in secured Azure environments using Azure KeyVaults, Service Principals, and Managed Identities
Good to have knowledge on Apigee (Googles API Management)
Understanding of data masking, encryption and other practices used in handling sensitive data
Ability to interact with Business for requirement gathering and query resolutions
Working on off shore office based development teams, collaborating within a team environment and participating in typical project lifecycle activities such as requirement analysis, testing and release
Develop Azure Data skills within the team through knowledge sharing sessions, articles, etc.
Adherence to organisations Risk & Controls requirements
Should have skills for Stakeholder management, process adherence, planning & documentationss
Key Stakeholder Management
Internal
Business Teams
Project Manager
Architects
Data Scientists
Team members
External
Knowledge, Skills, Experience & Educational Qualification
Knowledge & Skills:
Azure Data Factory,
Azure Data Lake Storage V2
Azure SQL
Azure DataBricks
Pyspark
Azure DevOps
Power BI Report
Technical leadership
Confidence & excellent communication
Experience:
Overall 12+ years of experience
5 + Experience on Azure data engineering
5 + experience of managing data deliveries
Educational Qualification:
Graduate/Post-graduate. Preferably with specialisation in Computer Science, Statistics, Mathematics, Data Science, Engineering or related discipline
Microsoft Azure certification (good to have)
M&G Behaviours relevant to all roles:
Inspire Others: support and encourage each other, creating an environment where everyone can contribute and succeed
Embrace Change: be open to change, willing to be challenged and able to adapt quickly and imaginatively to new ideas
Deliver Results: focus on performance, set high standards and deliver with energy and determination
Keep it simple: cut through complexity, keep the outcome in mind, keeping your approach simple and adapting your message to every audience
We have a diverse workforce and an inclusive culture at M&G Global Services, regardless of gender, ethnicity, age, sexual orientation, nationality, disability or long term condition, we are looking to attract, promote and retain exceptional people. We also welcome those who take part in military service and those returning from career breaks.","Azure Data Lake Storage V2, Azure Function Apps, Power Bi, Azure Sql, Azure Data Factory, Azure Databricks, Azure Logic Apps, Pyspark, Azure DevOps"
Data Engineer,GeMTech PARAS,2-5 Years,,"Delhi, India",Login to check your skill match score,"We're looking for a hands-on Data Engineer to manage and scale our data scraping pipelines across 60+ websites. The job involves handling OCR-processed PDFs, ensuring data quality, and building robust, self-healing workflows that fuel AI-driven insights.
You'll Work On:
Managing and optimizing Airflow scraping DAGs
Implementing validation checks, retry logic & error alerts
Cleaning and normalizing OCR text (Tesseract / Textract)
Handling deduplication, formatting, and missing data
Maintaining MySQL/PostgreSQL data integrity
Collaborating with ML engineers on downstream pipelines
What You Bring:
25 years of hands-on experience in Python data engineering
Experience with Airflow, Pandas, and OCR tools
Solid SQL skills and schema design (MySQL/PostgreSQL)
Comfort with CSVs and building ETL pipelines
Bonus: Scrapy or Selenium scraping experience","Airflow, OCR tools, ETL pipelines, Scrapy, Pandas, MySQL, PostgreSQL, Selenium, Python, Sql"
Senior Data Engineer Analyst,IMRIEL Technology Solutions Private Ltd,3-5 Years,,"Pune, India",Login to check your skill match score,"We are seeking a skilled Data Engineer with expertise in maintaining scalable semantic models using AtScale and cloud-based data warehouse platforms.
We at IMRIEL (An Allata Company) are looking for experienced and technically strong Analytics Data Engineers to design, build, and maintain scalable semantic models using AtScale and cloud-based data warehouse platforms. This role involves developing logical cubes, defining MDX-based business measures, and enabling governed, self-service BI consumption for enterprise analytics.
Experience:3 to 5 years.
Location:Vadodara & Pune
What you'll be doing:
Design and implement robust semantic data models using AtScale that abstract curated datasets into business-consumable layers.
Conduct a comprehensive POC to evaluate three potential semantic layer platforms: AtScale, Microsoft Fabric and Cube.dev. This includes assessing their performance, scalability, and integration with cloud-based platforms like Databricks, Snowflake, etc.
Develop and maintain logical cubes with calculated measures, dimension hierarchies, and drill-down paths to support self-service analytics.
Leverage MDX (Multidimensional Expressions) to define advanced business logic, KPIs, and aggregations aligned with enterprise reporting needs.
Configure and manage aggregate tables using AtScale Aggregate Designer, optimizing cube performance and reducing query latency.
Integrate semantic models with BI tools such as Power BI, Tableau, and Excel Pivot Tables, ensuring seamless end-user experiences.
Collaborate with data engineers to align semantic models with curated data sources, transformation views, and data pipelines.
Apply star and snowflake schema design to model fact and dimension tables, ensuring optimal structure for analytical workloads.
Implement Slowly Changing Dimensions (SCD Types 1 & 2) and maintain historical accuracy in reporting models.
Manage row-level security (RLS) and role-based access control (RBAC) policies within semantic layers for governed data access.
Participate in semantic model versioning, CI/CD-based deployments, and technical documentation.
Troubleshoot semantic layer performance issues using AtScale query logs, plan analysis, and catching strategies.
What you need:
Basic Skills:
Minimum 3 years of hands-on experience with AtScale, including building and maintaining semantic models, designing logical cubes, and implementing calculated measures using MDX. Proficiency in AtScale interface, modeling best practices, and performance tuning is essential.
Advanced experience in developing and optimizing DAX expressions for complex calculations in Power BI models, with a proven ability to translate these into new semantic layer technologies like AtScale or Cube.dev.
Strong experience with MDX, including creating calculated members, KPIs, and advanced expressions. Excellent SQL skills with the ability to write complex queries using joins, CTEs, window functions, and performance tuning.
Solid understanding of dimensional modeling. Ability to design fact/dimension tables using star/snowflake schemas, support SCD logic, and maintain model consistency.
Should be familiar with the Kimball methodology for dimensional modeling, including concepts like conformed dimensions, fact table granularity, and slowly changing dimensions, to design scalable and analytics-friendly data structures.
Hands-on experience with Snowflake, Redshift, or BigQuery. Familiarity with virtual warehouses, caching, clustering, partitioning, and compute-storage separation.
Experience implementing RLS and RBAC. Ability to define and enforce granular access controls within semantic models.
Strong grasp of OLAP concepts like query abstraction, drill-down/roll-up, and cube optimization. Understanding of business logic abstraction from physical data.
Skilled in using AtScale performance tools such as the Aggregate Designer, log analysis, and query optimization.
Proficient in managing model development lifecycle using Git, automation tools, and collaboration workflows with data/analytics teams.
Strong verbal and written communication to document models, explain logic, and coordinate with cross-functional teams.
Responsibilities:
Own the design, development, deployment, and maintenance of scalable, governed semantic models.
Implement complex MDX logic and optimized aggregate strategies to meet performance benchmarks.
Proven ability to design and implement scalable AtScale architectures, including the development of architectural blueprints and data flow diagrams.
Evaluate and implement the best semantic layer architecture for Power BI by leveraging tools like Microsoft Fabric or other modern BI accelerators to support self-service analytics.
Define business measures, hierarchies, and drill-down paths in semantic models aligned with enterprise KPIs.
Align semantic layers with upstream data transformations, curated datasets, and data warehouse architecture.
Enforce governance and security through robust RLS and RBAC implementations.
Continuously monitor, test, and tune semantic model performance using diagnostic tools and AtScale logging.
Ensure semantic layer reusability, consistency, and business-aligned metric standardization.
Collaborate with BI developers and analysts to understand reporting needs and validate model outputs.
Maintain documentation, data lineage, and business glossaries that support transparency and user adoption.
Contribute to reusable templates, modeling standards, and automation frameworks.
Nice-to-Have to have:
Experience with AtScale REST APIs for metadata-driven automation and CI/CD pipelines.
Familiarity with BI visualization platforms such as Power BI, Tableau, Looker, and Excel OLAP integration.
Scripting experience in Python, Shell, or YAML for configuration management or automation tasks.
Cloud certifications in Snowflake, Databricks, AWS, Azure, or Google Cloud Platform.
Exposure to metadata management, data cataloging, or enterprise data governance tools.
Personal Attributes:
High attention to detail with a focus on producing scalable, accurate, and governed semantic solutions.
Strong interpersonal and communication skills to collaborate effectively with technical and non-technical stakeholders.
Self-motivated and accountable, with the ability to take full ownership of deliverables.
Adaptability to evolving tools, data technologies, and enterprise analytics strategies.","MDX, kimball methodology, snowflake, AtScale, BigQuery, Power Bi, OLAP, Tableau, Redshift, Sql, Git, Excel, Dax"
Lead Snowflake Data Engineer,Ventra Health,7-9 Years,,"Chennai, India",Login to check your skill match score,"Ventra is a leading business solutions provider for facility-based physicians practicing anesthesia, emergency medicine, hospital medicine, pathology, and radiology. Focused on Revenue Cycle Management, Ventra partners with private practices, hospitals, health systems, and ambulatory surgery centers to deliver transparent and data-driven solutions that solve the most complex revenue and reimbursement issues, enabling clinicians to focus on providing outstanding care to their patients and communities.
Job Summary
We are seeking an experienced Lead Snowflake Data Engineer to join our Data & Analytics team. This role involves designing, implementing, and optimizing Snowflake-based data solutions while providing strategic direction and leadership to a team of junior and mid-level data engineers. The ideal candidate will have deep expertise in Snowflake, cloud data platforms, ETL/ELT processes, and Medallion data architecture best practices. The lead data engineer role has a strong focus on performance optimization, security, scalability, and Snowflake credit control and management. This is a tactical role requiring independent in-depth data analysis and data discovery to understand our existing source systems, fact and dimension data models, and implement an enterprise data warehouse solution in Snowflake.
Essential Functions And Tasks
Lead the design, development, and maintenance of a scalable Snowflake data solution serving our enterprise data & analytics team.
Architect and implement data pipelines, ETL/ELT workflows, and data warehouse solutions using Snowflake and related technologies.
Optimize Snowflake database performance, storage, and security.
Provide guidance on Snowflake best practices.
Collaborate with cross-functional teams of data analysts, business analysts, data scientists, and software engineers, to define and implement data solutions.
Ensure data quality, integrity, and governance across the organization.
Provide technical leadership and mentorship to junior and mid-level data engineers.
Troubleshoot and resolve data-related issues, ensuring high availability and performance of the data platform.
Education And Experience Requirements
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
7+ years of experience in-depth data engineering, with at least 3+ minimum years of dedicated experience engineering solutions in a Snowflake environment.
Tactical expertise in ANSI SQL, performance tuning, and data modeling techniques.
Strong experience with cloud platforms (preference to Azure) and their data services.
Proficiency in ETL/ELT development using tools such as Azure Data Factory, dbt, Matillion, Talend, or Fivetran.
Hands-on experience with scripting languages like Python for data processing.
Strong understanding of data governance, security, and compliance best practices.
Snowflake SnowPro certification; preference to the engineering course path.
Experience with CI/CD pipelines, DevOps practices, and Infrastructure as Code (IaC).
Knowledge of streaming data processing frameworks such as Apache Kafka or Spark Streaming.
Familiarity with BI and visualization tools such as PowerBI.
Knowledge, Skills, And Abilities
Familiarity working in an agile scum team, including sprint planning, daily stand-ups, backlog grooming, and retrospectives.
Ability to self-manage large complex deliverables and document user stories and tasks through Azure Dev Ops.
Personal accountability to committed sprint user stories and tasks.
Strong analytical and problem-solving skills with the ability to handle complex data challenges.
Ability to read, understand, and apply state/federal laws, regulations, and policies.
Ability to communicate with diverse personalities in a tactful, mature, and professional manner.
Ability to remain flexible and work within a collaborative and fast paced environment.
Understand and comply with company policies and procedures.
Strong oral, written, and interpersonal communication skills.
Strong time management and organizational skills.
Ventra Health
Equal Employment Opportunity (Applicable only in the US)
Ventra Health is an equal opportunity employer committed to fostering a culturally diverse organization. We strive for inclusiveness and a workplace where mutual respect is paramount. We encourage applications from a diverse pool of candidates, and all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, or veteran status. We will provide reasonable accommodations to qualified individuals with disabilities, as needed, to assist them in performing essential job functions.
Recruitment Agencies
Ventra Health does not accept unsolicited agency resumes. Ventra Health is not responsible for any fees related to unsolicited resumes.
Solicitation of Payment
Ventra Health does not solicit payment from our applicants and candidates for consideration or placement.
Attention Candidates
Please be aware that there have been reports of individuals falsely claiming to represent Ventra Health or one of our affiliated entities Ventra Health Private Limited and Ventra Health Global Services. These scammers may attempt to conduct fake interviews, solicit personal information, and, in some cases, have sent fraudulent offer letters.
To protect yourself, verify any communication you receive by contacting us directly through our official channels. If you have any doubts, please contact us at [HIDDEN TEXT] to confirm the legitimacy of the offer and the person who contacted you. All legitimate roles are posted on https://ventrahealth.com/careers/.
Statement of Accessibility
Ventra Health is committed to making our digital experiences accessible to all users, regardless of ability or assistive technology preferences. We continually work to enhance the user experience through ongoing improvements and adherence to accessibility standards. Please review at https://ventrahealth.com/statement-of-accessibility/.","DevOps practices, Matillion, dbt, snowflake, Fivetran, ELT, Spark Streaming, Azure Data Factory, Powerbi, Apache Kafka, Data Governance, Ansi Sql, Talend, Python, Etl"
Data Engineer,Prachodayath Global Services Private Limited,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"Data Engineering is the core team for all decision making systems and responsible for creating bespoke data processing workflows to enable Mistplay AI products and advanced analytics. You will become a part of our core data engineering function focusing on driving operational excellence across all data stacks. You will work closely with our ML and Analytics teams to modernize our data platform and continuous innovation on data features. You will provide the technical ownership to help drive continuous improvement to our current data processing workflows and new lakehouse architecture.
Role :- Senior Data Engineer
Experience -7+
Work Mode - Onsite
Full-time, Permanent Role
Budget - 11-12 LPA
As a Senior Data Engineer II you will be working closely with engineering, operations, and product to deploy new applications on our data lakehouse, refactoring legacy elements and design new features/data pipelines. This position requires someone who is passionate about data architecture design, big data technologies, deep technical proficiency in distributed data processing, real-time streaming, a strong problem-solver, a team collaborator and has a growth mindset.
What You'll Do
Design and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.
Participate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.
Independently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.
Collaborate with development, AI, and data science teams to integrate data solutions into complex enterprise systems, ensuring seamless interoperability with existing platforms.
Monitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.
Analyze and interpret trends in complex data sets, utilizing visualization tools (e.g., Tableau, Power BI) to create dashboards and reports that tell compelling data stories.
Create and maintain standardized operational tools and reporting mechanisms to communicate data health and business performance to various audiences, including executives.
Maintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.
Acquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.
Automate data analyses and authoring pipelines using tools such as Kinesis, Airflow, Lambda, Databricks, DBT, and other relevant technologies.
Engage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.
Drive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.
What You'll Bring
Bachelor's degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.
7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.
Extensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.
Proficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.
Expertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.
Experience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).
Ability to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.
Strong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.
Exceptional attention to detail and professionalism in report and dashboard creation.
Excellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.
Outstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.
Interested candidates share your resume at [HIDDEN TEXT]","Airflow, Beam, dbt, Java, Apache Flink, Scala, Redis, Lambda, Git, Kinesis, Terraform, Spark, Databricks, Python, Kubernetes"
DBT Data Engineer - Remote,Cognisol,6-8 Years,,"Chennai, India",Login to check your skill match score,"Key Skillset-DBT,Python,SQL,AWS,pYSPARK
Years of Exp- 6 to 7 Years
Work Mode-Work From Home(Candidate should be available for 1st week of Onboarding @ Chennai Loc)
Shift Time-UK Shift time
Notice: Immediate to 15 days only
Placement Type: Contractual Position
Key Responsibilities
Data Pipeline Development: Design, implement, and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from various sources into cloud platforms like AWS.
Cloud Integration: Utilize AWS services such as Glue, Step Functions, Redshift, and Lambda to process and store data efficiently.
Data Transformation: Develop and maintain accurate data pipelines using Python and SQL, transforming data for aggregations, wrangling, quality control, and calculations.
Workflow Automation: Enhance end-to-end workflows with automation tools to accelerate data flow and pipeline management.
Collaboration: Work closely with business analysts, data scientists, and cross-functional teams to understand data requirements and develop solutions that support data-driven decision-making.
Qualifications
Educational Background: Bachelor's or Master's degree in Information Technology, Bioinformatics, Computer Science, or a related field.
Professional Experience: Several years of experience in data engineering, with hands-on expertise in:
Developing and managing large-scale ETL data pipelines on AWS.
Proficiency in Python and SQL for data pipeline development.
Utilizing AWS services such as Glue, Step Functions, Redshift, and Lambda.
Familiarity with tools like Docker, Linux Shell Scripting, Pandas, PySpark, and Numpy.
Soft Skills: Strong problem-solving abilities, excellent communication skills, and the capacity to work collaboratively in a dynamic environment.
Skills: etl,linux shell scripting,pyspark,docker,pipelines,sql,aws,redshift,glue,pipeline,cloud,python,lambda,dbt,step functions,pandas,numpy","dbt, Glue, Pyspark, Shell Scripting, Redshift, Sql, Lambda, Numpy, Pandas, Linux, Docker, Python, AWS"
Data Engineer-Specialized-Associate - Operate,PwC Acceleration Centers in India,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services.
KEY SKILLS - Mainframe, Teradata Datastage
Mainframe and Teradata DataStage Associate
Summary:.
Minimum Degree Required: Bachelor's degree in computer science/IT or relevant field
Degree Preferred: Master's degree in computer science/IT or relevant field
Minimum Years of Experience: 6 - 8 year(s)
Certifications Required: NA
Required Knowledge/Skills: (character count limit 5000) *PLEASE ONLY USE THIS FIELD IF THIS IS A MUST HAVE SKILL FOR APPLICANT*
Job Summary
We are seeking a skilled and experienced IT professional to join our team as a Mainframe and Teradata DataStage Associate. The successful candidate will be responsible for developing, maintaining, and optimizing ETL processes using IBM DataStage, as well as managing and supporting data operations on Mainframe and Teradata platforms.
Key Responsibilities
Design, develop, and implement ETL processes using IBM DataStage to support data integration and transformation requirements.
Manage and maintain data on Mainframe and Teradata systems, ensuring data integrity and performance optimization.
Collaborate with business analysts and stakeholders to understand data requirements and translate them into technical specifications.
Troubleshoot and resolve issues related to ETL processes and data management on Mainframe and Teradata platforms.
Monitor and tune the performance of ETL jobs and database queries to ensure optimal performance.
Develop and maintain documentation related to ETL processes, data flows, and system configurations.
Participate in code reviews and ensure adherence to best practices and coding standards.
Provide support for data migration and integration projects, ensuring timely and accurate data delivery.
Stay updated with the latest developments in Mainframe, Teradata, and DataStage technologies and recommend improvements.
Qualifications
Job Summary -
A career in our Managed Services team will provide you with an opportunity to collaborate with a wide array of teams to help our clients implement and operate new capabilities, achieve operational efficiencies, and harness the power of technology. Our Data, Testing & Analytics as a Service team brings a unique combination of industry expertise, technology, data management and managed services experience to create sustained outcomes for our clients and improve business performance. We empower companies to transform their approach to analytics and insights while building your skills in exciting new directions. Have a voice at our table to help design, build and operate the next generation of software and services that manage interactions across all aspects of the value chain.
Minimum Degree Required (BQ) *:
Bachelor's degree
Degree Preferred
Required Field(s) of Study (BQ):
Preferred Field(s) Of Study
Computer and Information Science, Management Information Systems
Minimum Year(s) of Experience (BQ) *: US
Certification(s) Preferred
Minimum of 1 year of experience
Preferred Skills (PQs)
Position Requirements:
Datasphere
Required Skills:
More than 2 years of hands-on experience in SAP Datasphere / DWC at least 1 full life cycle project implementation.
Work on development/maintenance of DWC Models, CDS Views, SQL Scripts SAC Stories
Should have experience in building complex models in SAP Datasphere/ DWC
Developing SAP Datasphere end-to-end Dataflows Design, build data flows, and develop chains to load and monitor Data Loading.
Knowledge in setting up the connections to Datasphere and from Datasphere.
Knowledge in handling the delta in Datasphere.
Unit testing the dataflows and reconciling the data to Source Systems.
Good exposure in troubleshooting data issues and provide workarounds in cases where there are product limitations.
Good exposure with Datasphere security setup, currency conversion.
Good knowledge in writing CDS Analytical Queries and S4HANA Embedded Analytics.
Good exposure in performance tuning of the models in the datasphere.
Good knowledge on Datasphere and Data Lake integration.
Good Knowledge on using the Database explorer and SAP Hana Cockpit through Datasphere.
Nice To Have
Good knowledge in either BW Modeling or HANA Modeling.
BW/4HANA And/or Native HANA (or HANA Cloud) modeling, including SQL Scripting, Graphical View-Modelling, SDA extraction.","DWC, SAP Datasphere, SAC Stories, Teradata, DataStage, Cds Views, Mainframe, Sql"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position
This role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.
Key Accountabilities
Assist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.
Work closely with senior engineers and analysts to gather requirements and build working prototypes.
Support data integration from internal, third-party, and public sources.
Participate in developing and maintaining Data Warehouse schemas.
Contribute to documentation and testing efforts to ensure data reliability.
Learn and apply data standards and governance practices as guided by the team.
Skills and Experience | Essential
Knowledge of data engineering concepts and data structures.
Exposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.
Understanding of ETL processes and data pipelines.
Ability to work collaboratively in an Agile/Kanban team environment.
Microsoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage
Skills and Experience | Desirable
Familiarity with Medallion Architecture principles.
Exposure to MS Purview or other data governance tools.
Understanding of data warehousing and reporting concepts.
Interest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
Senior Data Engineer (SQL and ADF),thinkbridge,5-7 Years,,India,Login to check your skill match score,"We are hiring a Senior Data Engineer with 5+ years of experience in SQL , who has good hands-on in ADF and ETL. Good communication is mandatory.
About thinkbridge
thinkbridge is how growth-stage companies can finally turn into tech disruptors. They get a new way there with world-class technology strategy, development, maintenance, and data science all in one place. But solving technology problems like these involves a lot more than code. That's why we encourage thinkers to spend 80% of their time thinking through solutions and 20% coding them. With an average client tenure of 4+ years, you won't be hopping from project to project here unless you want to. So, you really can get to know your clients and understand their challenges on a deeper level. At thinkbridge, you can expand your knowledge during work hours specifically reserved for learning. Or even transition to a completely different role in the organization. It's all about challenging yourself while you challenge small thinking.
thinkbridge is a place where you can:
Think bigger because you have the time, opportunity, and support it takes to dig deeper and tackle larger issues.
Move faster because you'll be working with experienced, helpful teams who can guide you through challenges, quickly resolve issues, and show you new ways to get things done.
Go further because you have the opportunity to grow professionally, add new skills, and take on new responsibilities in an organization that takes a long-term view of every relationship.
thinkbridge.. there's a new way there.
What is expected of you
As part of the job, you will be required to
Read everything in detail that comes your way.
Elicit, analyze, specify & validate business requirements.
Define & Document the engineering requirements including architecture, components, usable interfaces & other characteristics.
Place specific emphasis on technical and usability design.
Code and verify the solution.
Debug & squash bugs.
Keep stakeholders informed.
Keep up with the technology and the domain.
If your beliefs resonate with these, you are looking at the right place!
Accountability Finish what you started.
Communication Context-aware, pro-active and clean communication.
Outcome High throughput.
Quality High-quality work and consistency.
Ownership Go beyond.
Requirements
Must-Have:
Should have hands-on experience in writing SQL Queries and Stored Procedures.
Excellent Communication.
Should have experience in ADF (Azure Data Factory)
Good experience in building ETL Solutions for large datasets
Good to have:
Good to have SSIS Experience
Other Details :
Remote First
Flexible work hours
No loss of pay for pre-approved leaves
Family Insurance
Quarterly in-person Collaboration Week -WWW","Adf, SSIS, Sql, Etl"
Senior Data Engineer,Commonwealth Bank,10-13 Years,,"Bengaluru, India",Login to check your skill match score,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title: Sr Data Engineering
Location: Bangalore
Business & Team:
Technology Team is responsible for the world leading application of technology and operations across every aspect of CommBank, from innovative product platforms for our customers to essential tools within our business. We also use technology to drive efficient and timely processing, an essential component of great customer service.
CommBank is recognised as leading the industry in IT and operations with its world-class platforms and processes, agile IT infrastructure, and innovation in everything from payments to internet banking and mobile apps.
The Group Security (GS) team protects the Bank and our customers from cyber compromise, through proactive management of cyber security, privacy, and operational risk. Our team includes:
Cyber Strategy & Performance
Cyber Security Centre
Cyber Protection & Design
Cyber Delivery
Cyber Data Engineering
Cyber Data Security
Identity & Access Technology
The Group Security Senior Data Engineering team provides specialised data services and platforms for the CommBank group & is accountable for developing Group's data strategy, data policy & standards, governance and set requirements for data enablers/tools. The team is also accountable to facilitate a community of practitioners to share best practice and build data talent and capabilities.
Impact & contribution :-
To ensure the Group achieves a sustainable competitive advantage through data engineering, you will play a key role in supporting and executing the Group's data strategy.
We are looking for an experienced Data Engineer to join our Group Security Team, which is part of the wider Cyber Security Engineering practice. In this role, you will be responsible for setting up the Group Security Data Platform to ingest data from various organizations security telemetry data, along with additional data assets and data products. This platform will provide security controls and services leveraged across the Group.
Roles & Responsibilities
You will be expected to perform the following tasks in a manner consistent with CBA's Values and People Capabilities.
CORE RESPONSIBILITIES:
Possesses hands-on technical experience working in AWS. The individual should have knowledge about AWS services like EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, KMS, Step functions, SQS,SNS, Cloud Watch.
The individual should possess a robust set of technical and soft skills and be an excellent AWS Data Engineer with a focus on complex Automation and Engineering Framework development.
Being well-versed in Python is mandatory, and experience in developing complex frameworks using Python is required.
Passionate about Cloud/DevSecOps/Automation and possess a keen interest in solving complex problems systematically.
Drive the development and implementation of scalable data solutions and data pipelines using various AWS services.
Possess the ability to work independently and collaborate closely with team members and technology leads.
Exhibit a proactive approach, constantly seeking innovative solutions to complex technical challenges.
Can take responsibility for nominated technical assets related to areas of expertise, including roadmaps and technical direction.
Can own and develop technical strategy, overseeing medium to complex engineering initiatives.
Essential Skills:-
About 10-13 years of experience as a Data Engineering professional in a data-intensive environment.
The individual should have strong analytical and reasoning skills in the relevant area.
Proficiency in AWS cloud services, specifically EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, Step functions, SQS,SNS, Cloud Watch.
Excellent skills in Python-based framework development are mandatory.
Proficiency in SQL for efficient querying, managing databases, handling complex queries, and optimizing query performance.
Excellent automation skills are expected in areas such as
Automating the testing framework using tools such as PyPy, Pytest, and various test cases including unit, integration, functional tests, and mockups.
Automating the data pipeline and expediting tasks such as data ingestion and transformation.
API-based automated and integrated calls(REST, cURL, authentication & authorization, tokens, pagination, openApi, Swagger)
Implementing advanced engineering techniques and handling ad hoc requests to automate processes on demand.
Implementing automated and secured file transfer protocols like XCOM, FTP, SFTP, and HTTP/S
Experience with Terraform, Jenkins, Teracity and Artifactory is essential as part of DevOps. Additionally, Docker and Kubernetes are also considered.
Proficiency in building orchestration workflows using Apache Airflow.
Strong understanding of streaming data processing concepts, including event-driven architectures.
Familiarity with CI/CD pipeline development, such as Jenkins.
Extensive experience and understanding in Data Modelling, SCD Types, Data Warehousing, and ETL processes.
Excellent experience with GitHub or any preferred version control systems.
Expertise in data pipeline development using various data formats/types.
Mandatory knowledge and experience in big data processing using PySpark/Spark and performance optimizations of applications
Proficiency in handling various file formats (CSV, JSON, XML, Parquet, Avro, and ORC) and automating processes in the big data environment.
Ability to use Linux/Unix environments for development and testing.
Should be aware of security best practices to protect data and infrastructure, including encryption, tokenization, masking, firewalls, and security zones.
Well-structured documentation skills and the ability to create a well-defined knowledge base.
Certifications such as AWS Certified Data Analytics/Engineer/Developer Specialty or AWS Certified Solutions Architect.
Should be able to perform extreme engineering and design a robust, efficient, and cost-effective data engineering pipelines which are highly available and dynamically scalable on demand.
Enable the systems to effectively respond to high demands and heavy loads maintaining the high throughput and high I/O performance with no data loss
Own and lead E2E Data engineering life cycle right from Requirement gathering, design, develop, test, deliver and support as part of DevSecOPS process.
Must demonstrate skills and mindset to implement encryption methodologies like SSL/TLS and data encryption at rest and in transit and other data security best practices
Hands on work experience with data design tools like Erwin and demonstrate the capabilities of building data models, data warehouse, data lakes, data assets and data products
Must be able to constructively challenge the status quo and lead to establish data governance, metadata management, ask the right questions, design with right principles
Education Qualification :-
A Bachelor's or Master's degree in Engineering, specializing in Computer Science, Information Technology or relevant qualifications.
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through Sidekick to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 23/05/2025","Big Data Processing, Data Pipeline Development, Github, Data Modelling, Pyspark, Automation, Sql, Apache Airflow, Jenkins, Terraform, Docker, Data Warehousing, Kubernetes, Python, Etl, AWS"
Data Engineer - Mastery,Technogen India Pvt. Ltd.,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Note: More details below for DE, Investments Domain role.
Domain Key words : Capital Markets, Investment Banking
Technical Key words: Informatica, batch/shell scripting, data warehousing , SQL, ETL, Python (basic to intermediate)
Objectives of the role
Design, build and maintain complex ELT jobs that deliver business value
Translate high-level business requirements into technical specs
Ingest data from disparate sources into the data lake and data warehouse
Cleanse and enrich data and apply adequate data quality controls
Provide insight and direction to guide the future development of organization's data platform
Develop re-usable tools to help streamline the delivery of new projects
Collaborate closely with other developers and provide mentorship
Evaluate and recommend tools, technologies, processes and reference architectures
Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements
Basic Qualifications
Bachelor's degree in computer science, engineering or a related field
Data: 8+ years of experience with data analytics and warehousing in Investment & Finance Domain
SQL: Deep knowledge of SQL and query optimization
ELT: Good understanding of ELT methodologies and tools
Troubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues
Communication: Excellent communication, problem solving and organizational and analytical skills
Able to work independently and to provide leadership to small teams of developers
3+ years of coding and scripting (Python, Java, Scala) and design experience.
3+ years of experience with Spark framework.
Experience with Vertica or any Columnar Databases.
Strong data integrity, analytical and multitasking skills.
Preferred Qualifications
Master's degree in computer science or engineering or a related field
Cloud: Experience working in a cloud environment (e.g. AWS)
Python: Hands on experience developing with Python
Advanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka
Workflow: Good knowledge of orchestration and scheduling tools (e.g. Apache Airflow)
Reporting: Experience with data reporting (e.g. Microstrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)","Alation, Looker, Microstrategy, Data Warehousing, Apache Spark, Kafka, Tableau, Informatica, Sql, ELT, Apache Airflow, shell scripting, Spark, Vertica, Python, Etl"
Big Data Engineer (Freelancer),Soul AI,4-6 Years,,India,Login to check your skill match score,"About Us:
Soul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact.
We are looking for an experienced Big Data Engineer with at least 4 years of experience to design, develop, and manage large-scale data processing systems.
Key Responsibilities:
Build and maintain data pipelines for large datasets.
Design systems for real-time data processing.
Collaborate with data scientists and engineers to optimize data workflows.
Required Qualifications:
4+ years of experience as a Big Data Engineer.
Strong proficiency with big data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.
Experience with cloud platforms like AWS, GCP, or Azure.
Why Join Us
Competitive pay (1200/hour).
Flexible hours.
Remote opportunity.
NOTE: Pay will vary by project and typically is up to Rs. 1200 per hour (if you work an average of 3 hours every day - that could be as high as Rs. 108K per month) once you clear our screening process.
Shape the future of AI with Soul AI!","NoSQL databases, Gcp, Hadoop, Spark, Kafka, Azure, AWS"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description
Oncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.
Job Type:Full-time or Contract
Start Date: June 1st 2025
Role Description
This is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.
Job Description:
Candidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively
6+ Years of experience as a Data Engineer
Strong technical expertise in SQL and Snowflake is a must.
Strong knowledge of joins and common table expressions (CTEs)
Strong experience with Python
Strong expertise in ETL process and with various data model concepts
Knowledge of star schema and snowflake schema
Good to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue
Experience with Big Data Tools and technologies
Key Skills:
Good Understanding of data structures and data analysis using SQL
Knowledge of implementing ETL/ELT for data solutions end-to-end
Understanding requirements, and data solutions (ingest, storage, integration, processing)
Knowledge of analyzing data using SQL
Conducting End to End verification and validation for the entire application
Responsibilities:
Understand and translate business needs into data models supporting long-term solutions.
Perform reverse engineering of physical data models from databases and SQL scripts.
Analyze data-related system integration challenges and propose appropriate solutions.
Assist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
Senior Data Engineer,Ninja Van,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"About the Role:
We are looking for a highly skilled Senior Data Engineer to join our growing team. The ideal
candidate will have over 6 years of experience in building and managing data systems, with
expertise in Python, Spark, and Airflow. This role offers an exciting opportunity to work in a
product-driven environment and make a significant impact on our data architecture and pipeline.
Key Responsibilities:
Design, implement, and maintain scalable data pipelines and systems.
Work with Python, Spark, and Airflow to build robust and reliable data solutions.
Ensure the integrity and governance of code, focusing on best practices for deployment
and testing.
Handle database management with hands-on experience in MySQL, PostgreSQL,
DynamoDB, and MongoDB.
Collaborate with cross-functional teams to improve cloud-based data architectures,
primarily focusing on GCP, AWS, Oracle OCI, and Huawei Cloud.
Implement Change Data Capture (CDC) mechanisms and work on continuous data
integration and delivery.
Mentor and guide junior engineers, promoting best practices and knowledge sharing.
Support performance tuning, scalability, and system optimization efforts across the data
stack.
Key Requirements:
Experience: 6+ years with significant hands-on experience in Python, Spark, and
Airflow.
Databases: Proficiency in MySQL, PostgreSQL, DynamoDB, and MongoDB.
Cloud: Expertise with at least two of the following: GCP, AWS, Oracle OCI, or Huawei
Cloud.
CDC & Code Governance: Strong understanding and implementation of Change Data
Capture (CDC) and robust code governance.
Mentorship: Proven experience mentoring junior engineers, ensuring growth and
development within the team.
Tenure: Minimum 3 years in the current/previous company to ensure deep technical
expertise.
Preferred Experience: Candidates from product-based companies with proven impact
in their current role.","Airflow, Oracle OCI, Huawei Cloud, Gcp, MySQL, PostgreSQL, Spark, Dynamodb, MongoDB, Python, AWS"
Data Engineer,Aditi Consulting,5-8 Years,,"Bengaluru, India",Login to check your skill match score,"We are hiring Data Engineer for Bangalore Location
Location: Bangalore
Experience : 5 8 Yrs
Working Model : Hybrid
EXPERIENCE:
5 - 8 years preferred experience in a data engineering role.
Minimum of 4 years of preferred experience in Azure data services (Data Factory, Databricks, ADLS, SQL DB, etc.)
EDUCATION:
Minimum Bachelor's Degree in Computer Science, Computer Engineering or in STEM Majors (Science, Technology, Engineering, and
Math)
SKILLS/REQUIREMENTS:
Strong working knowledge of Databricks, ADF.
Expertise working with databases and SQL.
Strong working knowledge of code management and continuous integrations systems (Azure DevOps or Github)
preferred
Familiarity with Agile delivery methodologies
Familiarity with NoSQL databases (such as MongoDB) preferred.
Any experience on IoT Data Standards like Project Haystack, Brick Schema, Real Estate Core is an added advantage
Ability to multi-task and reprioritize in a dynamic environment.
Outstanding written and verbal communication skills","Sql, Databricks, Github, Azure DevOps"
Sr Data Bricks Data Engineer,Rainier Softech Solutions Pvt Ltd,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"Required Skills
6+ years as a data engineer with any data modelling tool (ADF ,Snowflake ,Google Big query, AWS Redshift , Data Bricks )
Experience in python and cloud analytics services.
2 years of experience in SQL and Data Bricks
Familiarity with any cloud DevOps & database CI/CD.
Stong in advanced SQL concepts.
Strong in ETL and Data Modelling concepts.
Good communication skills","Cloud analytics services, snowflake, Data Bricks, Advanced SQL concepts, Sql, Etl, Aws Redshift, Python, Adf, Data Modelling, Cloud Devops"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position
This role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.
Key Accountabilities
Assist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.
Work closely with senior engineers and analysts to gather requirements and build working prototypes.
Support data integration from internal, third-party, and public sources.
Participate in developing and maintaining Data Warehouse schemas.
Contribute to documentation and testing efforts to ensure data reliability.
Learn and apply data standards and governance practices as guided by the team.
Skills and Experience | Essential
Knowledge of data engineering concepts and data structures.
Exposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.
Understanding of ETL processes and data pipelines.
Ability to work collaboratively in an Agile/Kanban team environment.
Microsoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage
Skills and Experience | Desirable
Familiarity with Medallion Architecture principles.
Exposure to MS Purview or other data governance tools.
Understanding of data warehousing and reporting concepts.
Interest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
Data Engineer,PwC Acceleration Centers,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"ole: Associate
Tower: Data, Analytics & Specialist Managed Service
Experience: : 3 -5.5 years
Key Skills: AWS , Snowflake, DBT
Educational Qualification: BE / B Tech / ME / M Tech / MBA
Work Location: Bangalore
Job Description
As a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self-awareness, personal strengths, and address development areas.
Flexible to work in stretch opportunities/assignments.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Ticket Quality and deliverables review, Status Reporting for the project.
Adherence to SLAs, experience in incident management, change management and problem management.
Seek and embrace opportunities which give exposure to different situations, environments, and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Demonstrate leadership capabilities by working, with clients directly and leading the engagement.
Work in a team environment that includes client interactions, workstream management, and cross-team collaboration.
Good team player, take up cross competency work and contribute to COE activities.
Escalation/Risk management.
Position Requirements:
Required Skills:
AWS Cloud Engineer:
Job description:
Candidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas:
Should have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms.
Should have minimum 1-3 years of Operate/Managed Services/Production Support Experience
Should have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc.
Designing and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes.
Should have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc.
Design, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS.
Work together with data scientists and analysts to understand the needs for data and create effective data workflows.
Implementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.
Improve the scalability, efficiency, and cost-effectiveness of data pipelines.
Monitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage.
Implementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Should have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools
Scaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments.
Should have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc.
Should have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc.
Should have Strong communication, problem solving, quantitative and analytical abilities.
Nice to have:
AWS certification
Managed Services- Data, Analytics & Insights Managed Service
At PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients better.
Within our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client's enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC's Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today's dynamic business environment.
Within our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective.
As a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.
Kindly share your resume to [HIDDEN TEXT]
Regards,
Mirunalini MJ","AWS DMS, Prefect, snowflake, dbt, Aws Lambda, Hadoop, Collibra, Pyspark, AWS Glue, Informatica, Sql, Spark, Python, AWS"
Senior Data Engineer-ADF/Python,DecisionTree Analytics & Services,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Company Description
DecisionTree Analytics & Services is a global provider of advanced analytics and campaign management solutions based in Gurugram. We specialize in transforming raw data into scalable and smart insights to empower organizations to make data-driven decisions. Our solutions range from data integration and automation to advanced machine learning algorithms for pattern identification and decision acceleration.
Role Description
This is a full-time on-site role for a Senior Data Engineer-ADF/Python at DecisionTree Analytics & Services in Gurugram. The Senior Data Engineer will be responsible for tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.
Qualifications
Data Engineering and Data Modeling skills
Experience with Azure Data Factory
Data Warehousing and Data Analytics proficiency
Strong programming skills in languages like Python
Ability to work in a fast-paced, collaborative environment
Strong problem-solving and analytical skills","data engineering, Azure Data Factory, Data Modeling, Data Warehousing, Data Analytics, Python"
Senior Data Engineer-ADF/Python,DecisionTree Analytics & Services,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Company Description
DecisionTree Analytics & Services is a global provider of advanced analytics and campaign management solutions based in Gurugram. We specialize in transforming raw data into scalable and smart insights to empower organizations to make data-driven decisions. Our solutions range from data integration and automation to advanced machine learning algorithms for pattern identification and decision acceleration.
Role Description
This is a full-time on-site role for a Senior Data Engineer-ADF/Python at DecisionTree Analytics & Services in Gurugram. The Senior Data Engineer will be responsible for tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.
Qualifications
Data Engineering and Data Modeling skills
Experience with Azure Data Factory
Data Warehousing and Data Analytics proficiency
Strong programming skills in languages like Python
Ability to work in a fast-paced, collaborative environment
Strong problem-solving and analytical skills","data engineering, Azure Data Factory, Data Modeling, Data Warehousing, Data Analytics, Python"
Big Data Engineer (Freelancer),Soul AI,4-6 Years,,India,Login to check your skill match score,"About Us:
Soul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact.
We are looking for an experienced Big Data Engineer with at least 4 years of experience to design, develop, and manage large-scale data processing systems.
Key Responsibilities:
Build and maintain data pipelines for large datasets.
Design systems for real-time data processing.
Collaborate with data scientists and engineers to optimize data workflows.
Required Qualifications:
4+ years of experience as a Big Data Engineer.
Strong proficiency with big data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.
Experience with cloud platforms like AWS, GCP, or Azure.
Why Join Us
Competitive pay (1200/hour).
Flexible hours.
Remote opportunity.
NOTE: Pay will vary by project and typically is up to Rs. 1200 per hour (if you work an average of 3 hours every day - that could be as high as Rs. 108K per month) once you clear our screening process.
Shape the future of AI with Soul AI!","NoSQL databases, Gcp, Hadoop, Spark, Kafka, Azure, AWS"
Senior Data Engineer,United Airlines,2-5 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Achieving our goals starts with supporting yours. Grow your career, access top-tier health and wellness benefits, build lasting connections with your team and our customers, and travel the world using our extensive route network.
Come join us to create what's next. Let's define tomorrow, together.
Description
United's Digital Technology team designs, develops, and maintains massively scaling technology solutions brought to life with innovative architectures, data analytics, and digital solutions.
Find your future at United! We're reinventing what our industry looks like, and what an airline can be from the planes we fly to the people who fly them. When you join us, you're joining a global team of 100,000+ connected by a shared passion with a wide spectrum of experience and skills to lead the way forward.
Achieving our ambitions starts with supporting yours. Evolve your career and find your next opportunity. Get the care you need with industry-leading health plans and best-in-class programs to support your emotional, physical, and financial wellness. Expand your horizons with travel across the world's biggest route network. Connect outside your team through employee-led Business Resource Groups.
Create what's next with us. Let's define tomorrow together.
Job Overview And Responsibilities
Exciting opportunity to be a part of a brand new best-in-class data science & analytics team to create the world's first travel media network from the world's best and largest airline. An entrepreneurial and meticulous data engineer who builds underlying supporting data for measurement and reporting across United's Travel Media Network.
Design and build scalable and reliable data infrastructure and pipelines (ingestion, integration, ETL, real-time connectors) to support data for measurement and reporting
Build connections with relevant endpoints for data ingestion
Collaborate with data scientists, analysts, and other stakeholders to understand data needs and requirements
Ensure data quality, performance, and security across the entire data lifecycle; developing high-quality and well-documented data sets
Continuously improve data infrastructure and processes to increase efficiency, scalability, and reliability
Stay up to date with emerging technologies and best practices in data engineering
This position is offered on local terms and conditions. Expatriate assignments and sponsorship for employment visas, even on a time-limited visa status, will not be awarded.
Qualifications
What's needed to succeed (Minimum Qualifications):
Bachelor's degree in Computer Science, Engineering or a related field
2-5 years hands-on industry experience in Data Engineering (or equivalent quantitative job title)
Deep technical knowledge of data engineering; highly skilled in SQL, relational databases, big data
Skilled in development of data warehousing, data flow design and development, and ETL processes
Proficient with Python or Scala, Azure Data Factory, Synapse
Proficient with cloud-based data technologies such as AWS, Azure, and/or GCP
Familiarity with data visualization tools (Power BI, Tableau)
Strong collaborator with cross-functional teams from tech, design, and business and experience leading teams in an agile setting
Ability to communicate and explain data and its implications to various stakeholders
Ability to evaluate different options proactively and to solve problems in an innovative way, developing new solutions or combining existing methods to create new approaches
Must be legally authorized to work in India for any employer without sponsorship
Must be fluent in English and Hindi (written and spoken)
Successful completion of interview required to meet job qualification
Reliable, punctual attendance is an essential function of the position
What will help you propel from the pack (Preferred Qualifications):
Advanced computer engineering degree preferred
GGN00001979","data flow design, Relational Databases, ETL processes, Power Bi, Scala, Data Warehousing, Tableau, Sql, Azure Data Factory, Gcp, Big Data, Python, AWS"
Data Engineer - Mastery,Technogen India Pvt. Ltd.,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Note: More details below for DE, Investments Domain role.
Domain Key words : Capital Markets, Investment Banking
Technical Key words: Informatica, batch/shell scripting, data warehousing , SQL, ETL, Python (basic to intermediate)
Objectives of the role
Design, build and maintain complex ELT jobs that deliver business value
Translate high-level business requirements into technical specs
Ingest data from disparate sources into the data lake and data warehouse
Cleanse and enrich data and apply adequate data quality controls
Provide insight and direction to guide the future development of organization's data platform
Develop re-usable tools to help streamline the delivery of new projects
Collaborate closely with other developers and provide mentorship
Evaluate and recommend tools, technologies, processes and reference architectures
Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements
Basic Qualifications
Bachelor's degree in computer science, engineering or a related field
Data: 8+ years of experience with data analytics and warehousing in Investment & Finance Domain
SQL: Deep knowledge of SQL and query optimization
ELT: Good understanding of ELT methodologies and tools
Troubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues
Communication: Excellent communication, problem solving and organizational and analytical skills
Able to work independently and to provide leadership to small teams of developers
3+ years of coding and scripting (Python, Java, Scala) and design experience.
3+ years of experience with Spark framework.
Experience with Vertica or any Columnar Databases.
Strong data integrity, analytical and multitasking skills.
Preferred Qualifications
Master's degree in computer science or engineering or a related field
Cloud: Experience working in a cloud environment (e.g. AWS)
Python: Hands on experience developing with Python
Advanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka
Workflow: Good knowledge of orchestration and scheduling tools (e.g. Apache Airflow)
Reporting: Experience with data reporting (e.g. Microstrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)","Alation, Looker, Microstrategy, Data Warehousing, Apache Spark, Kafka, Tableau, Informatica, Sql, ELT, Apache Airflow, shell scripting, Spark, Vertica, Python, Etl"
Senior Data Engineer,USEReady,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"Role: Senior Data Engineer
Experience-3-8 yrs
Location: Bangalore, Gurgaon, Mohali, Pune
About the Role:
We are seeking a skilled and proactive Data Engineer with 3-8 years of hands-on experience in Snowflake, Python, Streamlit, and SQL, along with expertise in consuming REST APIs and working with modern ETL tools likeMatillion, Fivetran etc. The ideal candidate will have a strong foundation in data modeling, data warehousing, and data profiling, and will play a key role in designing and implementing robust data solutions that drive business insights and innovation.
Key Responsibilities:
Design, develop, and maintain data pipelines and workflows using Snowflake and an ETL tool (e.g., Matillion, dbt, Fivetran, or similar).
Develop data applications and dashboards using Python and Streamlit.
Create and optimize complex SQL queries for data extraction, transformation, and loading.
Integrate REST APIs for data access and process automation.
Perform data profiling, quality checks, and troubleshooting to ensure data accuracy and integrity.
Design and implement scalable and efficient data models aligned with business requirements.
Collaborate with data analysts, data scientists, and business stakeholders to understand data needs and deliver actionable solutions.
Implement best practices in data governance, security, and compliance.
Required Skills and Qualifications:
35 years of professional experience in a data engineering or development role.
Strong expertise in Snowflake, including performance tuning and warehouse optimization.
Proficient in Python, including data manipulation with libraries like Pandas.
Experience building web-based data tools using Streamlit.
Solid understanding and experience with RESTful APIs and JSON data structures.
Strong SQL skills and experience with advanced data transformation logic.
Experience with an ETL tool commonly used with Snowflake (e.g., dbt, Matillion, Fivetran, Airflow).
Hands-on experience in data modeling (dimensional and normalized), data warehousing concepts, and data profiling techniques.
Familiarity with version control (e.g., Git) and CI/CD processes is a plus.
Preferred Qualifications:
Experience working in cloud environments (AWS, Azure, or GCP).
Knowledge of data governance and cataloging tools.
Experience with agile methodologies and working in cross-functional teams.","Airflow, Matillion, CI CD, dbt, snowflake, Streamlit, Fivetran, Data Warehousing, Data Modeling, Sql, Git, Etl Tools, Rest Apis, Data Profiling, Python"
"Data Engineer with Databricks, Azure and Power BI (DAX) Skills",Sony India Software Centre,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Description
Total relevant experience in Data Engineering should be 6-8 yrs.
Must have good knowledge on Azure Databricks, Azure Datalake and Power BI including DAX skills.
Should have good Power BI Experience which includes -
Strong Data modelling skills.
Expert in Tabular model design.
Expert in writing complex DAX formulas.
Hands on experience in DAX optimization
Strong knowledge on the DWBI with SQL.
Experience of working with On-premise/cloud BI solution implementation.
Good knowledge in Python.
Good to have these skills in Databricks -
Understanding of AI capabilities.
Unity Catalog - Implementation
AI/BI Genie
DevOps
Power Automate (Good to have)
Department
Regional Apps - APCNJP - AP Platforms - G3A
Open Positions
1
Skills Required
Data Bricks, power bi, Azure Data Lake, SQL Development, Python
Role
Design, develop, and maintain data pipelines and ETL processes using Azure Databricks
Collaborate with business analysts to understand and meet business requirements for data and analytics
Develop and optimize data models and visualizations using Power BI (DAX) to present meaningful insights to stakeholders
Implement best practices for data governance, security, and compliance in Azure environments
Monitor and troubleshoot data pipelines, ensuring data integrity and reliability
Stay abreast of the latest trends and technologies in data engineering and analytics
Provide technical guidance and mentorship to junior members of the team
Location
Bengaluru
Education/Qualification
B Tech
Desirable Skills
Azure Databricks, Azure Datalake, Power BI with Dax, SQL, Python
Years Of Exp
6 to 8 years
Designation
Technical Specialist","Azure Datalake, Data Bricks, Power Bi, Azure Databricks, Dax, Sql, Python"
ETL Data Engineer,QTek Digital,6-9 Years,,"Hyderabad, India",Login to check your skill match score,"Job description
Company Description
QTek Digital is a data solutions provider specializing in custom solutions in data management, data warehouse, and data science. Our team of data scientists, data analysts, and data engineers work together to solve today's challenges and unlock future possibilities. As an employee-centric company, we prioritize engagement, empowerment, and enrichment of our employees.
Role Description
This is a full-time remote role for a BI ETL Engineer at QTek Digital. As a BI ETL Engineer, you will be responsible for day-to-day tasks such as data modeling, utilizing analytical skills, implementing data warehousing solutions, and executing Extract, Transform, Load (ETL) processes. This role requires strong problem-solving abilities and the ability to work independently.
Qualifications
6-9 years of experience with ETL and ELT pipeline build out using Pentaho, SSIS, FiveTran, Airbyte, or similar tools.
6-8 Years of experience in SQL and data manipulation languages
Strong Data Modeling, Dashboard, and Analytical Skills
Excellent understanding of data warehousing concepts, esp. Kimball design.
Experience with Pentaho and Airbyte administration will be a huge plus.
Strong skills in Data Modeling, Dashboard design, and Analytics
Experience in Data Warehousing and Extract Transform Load (ETL) processes
Strong problem-solving and troubleshooting skills
Excellent communication and collaboration skills
Ability to work independently and in a team
Bachelor's degree in computer science, Information Systems, or a related field
This role is based onsite in our Hyderabad Office.
The compensation range for this role is INR 5-19 Lakhs depending on a variety of factors including skills and experience.","Airbyte, FiveTran, Pentaho, Data Modeling, Data Warehousing, SSIS, Sql, ELT, Etl"
Lead Data Engineer - Future Detections,JPMorganChase,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Be an integral part of an agile team that's constantly pushing the envelope to enhance, build, and deliver top-notch technology products.
As a Lead Data Engineer at JPMorgan Chase within the Cybersecurity & Tech Controls , you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. Drive significant business impact through your capabilities and contributions, and apply deep technical expertise and problem-solving methodologies to tackle a diverse array of challenges that span multiple technologies and applications.
Job Responsibilities
Regularly provides technical guidance and direction to support the business and its technical teams, contractors, and vendors
Develops secure and high-quality production code, and reviews and debugs code written by others
Drives decisions that influence the product design, application functionality, and technical operations and processes
Serves as a function-wide subject matter expert in one or more areas of focus
Actively contributes to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle
Influences peers and project decision-makers to consider the use and application of leading-edge technologies
Adds to the team culture of diversity, equity, inclusion, and respect
Required Qualifications, Capabilities, And Skills
Formal training or certification on Data engineering concepts and 5+ years applied experience
Hands-on practical experience Big Data , large volume data transfer analysis.
Experience in spark/ iceberg / parquets.
Experience with cloud platforms like AWS and container technologies such as Kubernetes and Docker
Knowledge of software applications and technical processes with considerable in-depth knowledge in one or more technical disciplines (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)
Experience to tackle design and functionality problems independently with little to no oversight
Preferred Qualifications, Capabilities, And Skills
Practical cloud native experience
Computer Science, Computer Engineering, Mathematics, or a related technical field
ABOUT US
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Mobile, parquets, Data engineering concepts, iceberg, Machine Learning, Big Data, Artificial Intelligence, cloud, Docker, Spark, Kubernetes, AWS"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description
Oncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.
Job Type:Full-time or Contract
Start Date: June 1st 2025
Role Description
This is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.
Job Description:
Candidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively
6+ Years of experience as a Data Engineer
Strong technical expertise in SQL and Snowflake is a must.
Strong knowledge of joins and common table expressions (CTEs)
Strong experience with Python
Strong expertise in ETL process and with various data model concepts
Knowledge of star schema and snowflake schema
Good to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue
Experience with Big Data Tools and technologies
Key Skills:
Good Understanding of data structures and data analysis using SQL
Knowledge of implementing ETL/ELT for data solutions end-to-end
Understanding requirements, and data solutions (ingest, storage, integration, processing)
Knowledge of analyzing data using SQL
Conducting End to End verification and validation for the entire application
Responsibilities:
Understand and translate business needs into data models supporting long-term solutions.
Perform reverse engineering of physical data models from databases and SQL scripts.
Analyze data-related system integration challenges and propose appropriate solutions.
Assist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
"Senior Data Engineer (ELT/ELT, Python, Snowflake, AWS)",Reap,6-8 Years,,"Mumbai, India",Login to check your skill match score,"Role Overview
As part of a rapidly growing data team, we are looking to hire Senior Data Engineers to contribute to our dynamic and innovative projects. We are looking for someone who is ready for the challenge, is solutions-oriented, and thinks out of the box. The ideal candidate should be passionate about using data to drive impact across the organization and a collaborative mindset to work effectively within cross-functional teams. If you are enthusiastic about pushing boundaries, solving complex problems, and making a significant impact, we encourage you to apply and be part of our exciting journey in shaping the future of our products.
Responsibilities
Design and development: Design, develop, and deploy scalable ETL/ELT pipelines and APIs for ingestion and transformation. Implement data modeling best practices for optimal accessibility, flexibility and query performance. Implement data governance practices, including data security, privacy, and compliance, to ensure data integrity and regulatory compliance.
Collaboration: Work closely with cross-functional teams, including product managers, designers, and other engineers, to ensure seamless product development from concept to deployment. Influence product and cross-functional teams to identify data opportunities to drive impact.
Continuous learning: Stay updated with the latest industry trends and technologies, ensuring our tech stack remains modern and competitive.
Observability and Support: Build monitor and alerts for data pipelines monitoring, identify and resolve performance issues, troubleshoot data-related problems in collaboration with other teams, and ensure data platform SLAs are met.
To Be Successful You Will Need To Have
Experience: 6+ year work experience in data engineering and cloud platforms. Previous experience in a senior or lead engineering role.
Technical proficiency: Expertise in ETL, data modeling, and cloud data warehousing. Strong programming skills in Python, SQL, AWS, Snowflake and related tech stack. Hands-on experience with big data processing and API integrations.
Problem solving: Strong analytical and problem-solving skills, with a keen attention to detail and a passion for troubleshooting and debugging
Experience in Credit Cards, Payments and AWS certification would be an advantage.
Exposure to AI, machine learning, and predictive analytics is highly desirable.
Benefits
A Global & Dynamic Team
Remote Work Friendly
After submitting your application, please check your inbox for a confirmation email. If you don't see it, kindly check your spam or junk folder and adjust your settings to ensure future communication reaches your inbox. You can follow the steps here.","API Integrations, Big Data Processing, snowflake, Cloud Data Warehousing, Data Modeling, Python, Sql, Etl, AWS"
Senior Data Engineer,WebMD,4-6 Years,,"Navi Mumbai, Mumbai, India",Login to check your skill match score,"Position Requirements:
4+ years of experience with RDBMS databases such as Oracle, MSSQL or PostgreSQL
2+ years of experience with Pentaho Data Integration or any ETL tools such as Talend, Informatica, DataStage or HOP.
Working knowledge of orchestration tools such Oozie and Airflow
Experience working in both OLAP and OLTP environments
Experience working on-prem, not just cloud environments
Experience working with teams outside of IT (i.e. Application Developers, Business Intelligence, Finance, Marketing, Sales)
Experience managing or developing in the Hadoop ecosystem is preferred
Programming background with either Python, Scala, Java or C/C++ is a plus
Experience with Spark. PySpark, SparkSQL, Spark Streaming, etc
Strong in any of the Linux distributions, RHEL, CentOS or Fedora
Experience using reporting and Data Visualization platforms (Tableau, Pentaho BI) is good to have
Web analytics or Business Intelligence a plus
Understanding of Ad stack and data (Ad Servers, DSM, Programmatic, DMP, etc)","Airflow, Pentaho BI, DMP, Ad stack, Programmatic, DSM, Business Intelligence, Java, C, Hadoop Ecosystem, Scala, Pyspark, OLAP, Tableau, Sparksql, Spark Streaming, Web Analytics, Ad Servers, Spark, Oozie, Pentaho Data Integration, Python, Oltp"
Senior Data Engineer,Endowus,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"About us
Endowus is Asia's leading fee-only wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping everyone grow all their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.
Our clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones.
Our mission is clear: help people invest better so they can live easier today, and better tomorrow.
The team has deep domain knowledge in finance and technology, bringing together decades of experience from various banks and tech companies. We treasure our diversity in background and experience, and we look for people who share our beliefs in our mission.
About the Team
The Client Interactions & Insights Platform team builds and operates the scalable data platform that powers data analytics and business intelligence for better decision making at Endowus. Working with colleagues in Data Analytics, Growth, Marketing, and Operations teams, the data engineering team creates data solutions that provide them with performant, near real time access to internal & third party data and insights.
We are a full stack team that builds our systems using cloud native patterns and operates them with high standards of engineering & operational excellence.
About this role, responsibilities & ownership
Lead technical design, delivery, reliability & security of our core data platform.
Work closely with the Product team, other Engineering teams, and stakeholders in Data Analytics, Growth, Marketing, Operations, Compliance & IT Risk to achieve our business goals.
Strive for high levels of technical quality, reliability, and delivery efficiency.
Mentor and grow a small talented team of junior data engineers.
Requirements
Bachelors or above in Computer Science, a related field, or equivalent professional experience.
At least 6 years experience in designing and implementing highly scalable, distributed data collection, aggregation, and analysis systems built for handling large volumes of data in the cloud.
Significant hands-on experience developing data pipelines with Apache Spark with Scala
At least 2 years experience as a tech lead facing business users directly and leading technical initiatives
Significant hands-on experience building and optimising data pipelines for data collection, transformation, aggregation in Apache Flink or Apache Spark, using dependency and workflow management tools such as Airflow, operating in a public cloud environment like AWS, GCP or Azure.
Advanced SQL knowledge and strong experience working with relational and non-relational databases.
Experience integrating BI tools such as Tableau, Mode, Looker, etc.
Experience integrating data sources using REST and streaming protocols, especially using Kafka.
Experience with building systems & processes to handle data quality, data privacy, and data sovereignty requirements.
Experience with agile processes, testing, CI/CD, and production error/metrics monitoring.
Self-driven with a strong sense of ownership.
Comfortable with numbers and motivated by steep learning curves.
Has a strong product sense and is empathetic to customers experiences of using the product.
Preferred Skills & Experience
Domain experience in a B2C context is a strong plus.
Knowledge of finance, wealth, and trading domain.
Some exposure to CQRS / Event Sourcing patterns.
Experience with AWS or GCP, Cassandra, Kafka, Kubernetes, Terraform.
Our Investors, recognition, licensing
Founded in 2017, Endowus has raised a total of US$50 million in funding from investors such as UBS, EDBI, Prosus Ventures, Z Venture Capital, Samsung Ventures, Singtel Innov8, and global leading venture capital firms Lightspeed Venture Partners and SoftBank Ventures Asia.
Endowus leadership and growth have been recognised by the industry and it has attained numerous awards including, Singapore's Best Digital Wealth Management (Asia Asset Management's Best of the Best Awards 2024), Singapore's Best Digital Upgrade for enhancements made on the Endowus app (The Asset Triple A Digital Awards 2024), Singapore's Best Digital Wealth Management Experience (The Asset Triple A Digital Awards 2023), and Best WealthTech Solution 2023 (Asian Private Banker 9th Technology Awards). Endowus is also among the firms named in the World Economic Forum's Technology Pioneers 2023, LinkedIn Top Start-ups 2023 and Forbes 100 to Watch list for 2022.
The Endowus Group comprises Endowus licensed companies in Hong Kong and Singapore, as well as Hong Kong-based multi family office Carret Private. Endowus Group serves over a hundred thousand clients with content, advice and access. With group assets of over US$6 billion, it is one of the largest independent wealth managers in Asia. From a combination of 100% trailer fees rebates as direct cashback to clients, savings from the access to institutional share class and exclusive funds, Endowus has created more than US$40 million in savings per year for its clients.","Mode, Airflow, data sovereignty, agile processes, Looker, streaming protocols, Relational Databases, non-relational databases, Apache Flink, Scala, Apache Spark, Kafka, Tableau, Sql, Data Quality, REST, Gcp, Data Privacy, Azure, AWS"
Data Engineer,Xebia,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description:
We are looking for a skilled Data science Data Engineer to join our team, working on end-to-end data engineering and data science use cases. The ideal candidate will have strong expertise in Python or Scala, Spark (Databricks), and SQL, building scalable and efficient data pipelines on Azure.
Primary Skills:
Data Engineering & Cloud:
Proficiency in Azure Data Platform (Data Factory, Databricks).
Strong skills in SQL and [Python or Scala] for data manipulation.
Experience with ETL/ELT pipelines and data transformations.
Familiarity with Big Data technologies (Spark, Delta Lake, Parquet).
ML & MLOps Integration:
Experience supporting ML pipelines with efficient data workflows.
Knowledge of MLOps practices (CI/CD, model monitoring, versioning).
Data Optimization & Performance:
Expertise in data pipeline optimization and performance tuning.
Experience on feature engineering and model deployment.
Analytical & Problem-Solving:
Strong troubleshooting and problem-solving skills.
Experience with data quality checks and validation.
Nice-to-Have Skills:
Exposure to NLP, time-series forecasting, and anomaly detection.
Familiarity with data governance frameworks and compliance practices.
Understanding of retail, or workforce analytics.","Parquet, Azure Data Platform, Delta Lake, Data Factory, Scala, Sql, ELT, MLops, Spark, Databricks, Python, Etl"
Data Engineer,Louis Dreyfus Company,Fresher,,"Bengaluru, India",Login to check your skill match score,"Company Description
Louis Dreyfus Company is a leading merchant and processor of agricultural goods. Our activities span the entire value chain from farm to fork, across a broad range of business lines, we leverage our global reach and extensive asset network to serve our customers and consumers around the world. Structured as a matrix organization of six geographical regions and ten platforms, Louis Dreyfus Company is active in over 100 countries and employs approximately 18,000 people globally.
Job Description
Background: The Crop Monitor and Weather Desk initiatives are critical components of our data-driven strategy in the Fundamental Trading division. These projects rely heavily on robust data engineering to ensure accurate and timely data processing, directly impacting our trading decisions and overall performance.
Current Situation: An external developer who was integral to these projects left our service provider company EPAM. This departure has created a gap in our data engineering capabilities, which needs to be addressed promptly to maintain the continuity and quality of our operations.
Proposal: We propose to internalize this role by hiring a dedicated data engineer in our Bangalore office. This position will be unbudgeted for the current year but included in the 2025 budget.
Rationale:
Cost Efficiency: Internalizing the role in Bangalore will be more cost-effective compared to continuing with external service providers. The cost savings will be realized through reduced contractor fees and better control over project timelines and deliverables. Quality and Continuity: Having an in-house data engineer will ensure better alignment with our project goals and provide continuity in our data engineering efforts. This will lead to improved data quality and more reliable outputs for the Crop Monitor and Weather Desk projects. Strategic Location: Bangalore is a strategic location for our data engineering needs due to its rich talent pool. This will enable us to attract and retain top talent, further enhancing our capabilities.
Long-term Benefits: Internalizing this role will not only address the immediate gap but also strengthen our internal expertise and reduce dependency on external vendors in the long run
Additional Information
Additional Information for the job
Diversity & Inclusion
LDC is driven by a set of shared values and high ethical standards, with diversity and inclusion being part of our DNA. LDC is an equal opportunity employer committed to providing a working environment that embraces and values diversity, equity and inclusion.
LDC encourages diversity, supports local communities and environmental initiatives. We encourage people of all backgrounds to apply.
Sustainability
Sustainable value is at the heart of our purpose as a company.
We are passionate about creating fair and sustainable value, both for our business and for other value chain stakeholders: our people, our business partners, the communities we touch and the environment around us
What We Offer
We provide a dynamic and stimulating international environment, which will stretch and develop your abilities and channel your skills and expertise with outstanding career development opportunities in one of the largest and most solid private companies in the world.
We offer
A workplace culture that embraces diversity and inclusivity
Opportunities for Professional Growth and Development
Employee Recognition Program
Employee Wellness Programs - Confidential access to certified counselors for employee and eligible family members, along with monthly wellness awareness sessions.
Certified Great Place to Work",data engineering
GCP Lead Data Engineer,StatusNeo,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Role Overview:
The Data Engineer will focus on developing, maintaining, and optimizing data pipelines. You will work on BigQuery, and Incorta,
ensuring efficient data ingestion, transformation, and governance while integrating AI-driven automation.
Key Responsibilities:
Build and optimize data pipelines for ingestion, transformation, and storage using BigQuery, SQL Managed Services, and Incorta.
Implement AI-driven automation for data pipeline monitoring, performance tuning, and anomaly detection.
Ensure data governance, security, and compliance standards are met across all platforms.
Optimize data workflows for cost efficiency and scalability.
Collaborate with BI, AI, and application teams for seamless data access and analytics.
Integrate data from multiple sources including Salesforce, Oracle AMS, and other enterprise applications.
Required Skills:
Primary: BigQuery, ETL Development, SQL Managed Services, Incorta
Secondary: Data Pipeline Optimization, Cost Optimization, Pipeline Maintenance .","SQL Managed Services, Pipeline Maintenance, Cost Optimization, Data Pipeline Optimization, Incorta, BigQuery, Etl Development"
Staff Data Engineer,Revenera,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Revenera helps product executives build better products, accelerate time to value and monetize what matters. Revenera's leading solutions help software and technology companies drive top line revenue with modern software monetization, understand usage and compliance with software usage analytics, empower the use of open source with software composition analysis and deliver an excellent user experiencefor embedded, on premises, cloud, and SaaS products.
Staff Data Engineer, India
Revenera helps product executives build better products, accelerate time to value and monetize what matters. Revenera's leading solutions help software and technology companies drive top line revenue with modern software monetization, understand usage and compliance with software usage analytics, empower the use of open source with software composition analysis and deliver an excellent user experiencefor embedded, on-premises, cloud, and SaaS products.
Revenera's Monetization platform is the global standard for electronic software licensing and entitlement management. It helps you implement your digital business model, define packaging options for your software, manage customers and their use rights and deliver software and updates.
Revenera is looking for an experienced Data Engineering Manager to lead our team of Software and Data Engineers who build our Data Warehouse and Data Analytics platforms for FlexNet Operations.
Job Description
Experienced Senior Data Engineer design, build, and optimize scalable data pipelines and systems. You will be responsible for managing complex datasets, ensuring high data quality, and collaborating with cross-functional teams.
Responsibilities
Architect, develop, document and maintain robust data pipelines and ETL processes.
Design and implement data storage solutions (data warehouses) optimized for performance and cost.
Collaborate with PO/PM and other stakeholders to understand data needs and deliver solutions. Advising on and contributing to projects & delivery planning for all data engineering elements.
Implement and manage data quality, data governance, and metadata management frameworks.
Optimize data systems for scalability, reliability, and performance. Ensure compliance with data privacy and security regulations (GDPR, CCPA, etc.).
Mentor junior data engineers and lead technical initiatives.
Continually looking for innovative ways to make improvements in building a scalable data platform based on the latest trends and research.
Working closely with IT to ensure data security and cloud configuration remains at optimum performance at all times.
Required Skills And Qualifications
10-12 years of previous experience of working with large complex data engineering projects.
Experience with designing and developing ETL pipelines. In-depth understanding of data modelling, ETL processes, and data warehousing concepts.
Technical knowledge of data engineering techniques and concepts, including data ingestion, processing, and storage.
Expert-level proficiency in SQL. Working knowledge in Python, reactJS/NodeJS, Javascript, typescript Strong understanding of database management.
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and containerization technologies (e.g., Docker, Kubernetes) is a plus
Prior experience with cloud-based analytical tools like Snowflake is highly advantageous. Excellent communication and technical presentation skills. Able to work methodically under pressure and work to tight deadlines.
Ability to multi-task and work across a range of projects and issues with various timelines and priorities.
Able to work in a flexible and agile environment.
Exposure to real-time data streaming and event-driven architecture.
Knowledge of AI, Machine Learning workflows and Machine Learning Operations is a plus.
Revenera is proud to be an equal opportunity employer. Qualified applicants will be considered for open roles regardless of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by local/national laws, policies and/or regulations. Regarding disability, we encourage candidates requiring accommodations to please let us know by emailing [HIDDEN TEXT].","Event-driven architecture, Data Storage, Real-time data streaming, AI Machine Learning workflows, Data ingestion, ETL processes, Machine Learning Operations, Nodejs, Data Warehousing Concepts, Sql, Typescript, Reactjs, Javascript, Python"
Data Engineer - Apache Airflow,Ekfrazo Technologies Private Limited,4-6 Years,,"Mumbai, India",Login to check your skill match score,"Job Title: Data Engineer
Location: Mumbai- WFO
Experience: 4+ Years
Notice Period: Immediate Joiners Only
About the Role:
We are seeking a skilled and motivated Data Engineer to join our team in Mumbai. The ideal candidate will have a strong background in Python (OOPS), workflow orchestration using Airflow, and experience working with Azure cloud services and Snowflake. This role is best suited for someone passionate about building scalable data pipelines and backend applications or SDKs.
Key Responsibilities:
Design, build, and maintain scalable and robust data pipelines using Apache Airflow
Develop backend components and reusable SDKs in Python, with a strong emphasis on OOPS principles
Integrate and manage large-scale data workflows on Azure Cloud, leveraging services such as Data Factory, Blob Storage, and more
Work with Snowflake for data warehousing and analytics
Implement monitoring and data quality checks using tools like Great Expectations
Collaborate with cross-functional teams to understand business data needs and deliver high-quality solutions
Must-Have Skills:
4+ years of experience in Data Engineering or related roles
Hands-on experience with Apache Airflow
Strong Python programming skills, with emphasis on Object-Oriented Programming (OOPS) and backend application or SDK development
Proven experience working with Azure cloud platform, especially services relevant to data engineering (e.g., Data Factory, Blob Storage, Azure Functions)
Good understanding of Snowflake architecture, data loading/unloading, and query optimization
Strong analytical and problem-solving skills
Nice to Have:
Experience with Great Expectations or similar Data Quality frameworks
Prior work on building or maintaining Data Quality (DQ) frameworks
Advanced knowledge of Snowflake features like Snowpipe, Streams & Tasks, or Materialized Views","snowflake, Blob Storage, Great Expectations, Apache Airflow, Data Factory, Azure Cloud Services, Python"
"Senior Data Engineer ( python , Java and SQL , Data pipelines )",NielsenIQ,7-9 Years,,India,IT/Computers - Software,"Job Description
Senior Data Engineer
Mission of the Role
You are a passionate and skilled data engineer who enjoys building scalable, production-grade data pipelines that enable machine learning and AI at scale. You thrive on solving complex data challenges and understand how critical a robust data pipeline is to the success of any predictive model.
As a Senior Data Engineer on the Forecasting team, you will be responsible for designing, implementing, and maintaining the data pipelines that power deep neural network models used for global sales and revenue forecasting. You will work closely with ML engineers, data scientists, and product stakeholders to build automated, secure, and cost-efficient pipelines using GCP-native services.
You take pride in writing clean, testable, and production-ready code, and are comfortable operating in cloud environments. You know how to make data pipelines observable, recoverable, and performant. You are also comfortable taking architectural ownership and contributing to long-term platform improvements.
You will:
Design and build robust, scalable, and maintainable data pipelines for training and inference of deep learning models used in forecasting and other Machine Learning based algorithms.
Develop and orchestrate end-to-end workflows using GCP-native tools such as Cloud Workflows, Cloud Run, BigQuery, Cloud Functions, and Pub/Sub.
Automate and maintain the pipeline lifecycle, including data extraction, transformation, loading (ETL/ELT), and triggering model runs.
Monitor and Optimise performance, cost-efficiency, and data freshness across all stages of the pipeline.
Collaborate with data scientists to productionise AI/ML models and integrate them seamlessly into business workflows.
Own the data ingestion, validation, and transformation logic powering the forecasting models, ensuring high-quality, consistent input data.
Handle deployment, versioning, and configuration of pipeline components using Docker, Git, and CI/CD practices.
Implement logging, monitoring, and alerting to ensure reliability and traceability in a high-scale environment.
Review code, mentor junior engineers, and help define best practices in our evolving data engineering stack.
Qualifications
You have:
7+ years of experience in data engineering or backend engineering roles.
Strong expertise in Python and SQL, with experience building production-grade data pipelines.
Solid understanding of Docker, Git, and shell scripting in Linux environments.
Hands-on experience with GCP services
Experience in building, deploying, and maintaining data workflows that feed AI/ML models.
Familiarity with model lifecycle management and infrastructure challenges in ML pipelines.
Proficiency in setting up CI/CD pipelines for data and ML systems using GitLab CI, Cloud Build, or similar tools.
Exposure to Java for backend services or pipeline components (even if not primary language).
A proactive, collaborative mindset and strong communication skills across engineering and data science teams.
Nice to have:
Exposure to forecasting or time series modelling pipelines.
Experience with event-driven architectures.
Familiarity with infrastructure-as-code tools like Terraform
Understanding of data quality frameworks and observability tools
Knowledge of model versioning tools and experiment tracking systems
Additional Information
Why Join us
You'll be working on a high-impact platform that forecasts consumer demand and drives business-critical decisions globally
You'll be part of a tight-knit, collaborative, and innovative Forecasting & AI team within a leading global data science organisation
You'll have access to top-tier infrastructure, GCP services, and challenging real-world problems in predictive modelling
Flexible working hours, remote-friendly culture, and strong focus on personal and professional growth
Competitive compensation and performance-based bonuses
Our Benefits
Flexible working environment
Volunteer time off
LinkedIn Learning
Employee-Assistance-Program (EAP)
About NIQ
NIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights-delivered with advanced analytics through state-of-the-art platforms-NIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.
For more information, visit NIQ.com
Want to keep up with our latest updates
Follow us on: | | |
Our commitment to Diversity, Equity, and Inclusion
NIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center:","Cloud Run, Cloud Workflows, GCP-native tools, Pub Sub, Cloud Functions, Python, Sql, BigQuery, Shell scripting, Java, Docker, Git"
Data Engineer,Recro,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"What you will enjoy doing
Lead Technical Architecture & Development
o Design and develop enterprise-level data architecture and scalable solutions,
handling diverse data types (time-series, structured, semi-structured, and
unstructured) across Linde.
o Drive technical development of unified data platforms to enable data access,
governance, and security at scale.
o Create scalable, reusable, and standardized data interfaces for internal and external
data exchange, in full compliance with Linde's cybersecurity and data protection
policies.
Data Engineering & Automation
o Build and maintain robust data pipelines for extracting and processing data from
varied sources including ERP systems, iHistorian, IoT devices, web platforms, and
external APIs.
o Leverage modern data-engineering frameworks and cloud technologies (Python,
SQL, Spark, Delta Lake, Databricks, Azure) for large-scale, secure, and efficient data
processing.
o Automate workflows, optimize compute resources, and manage CI/CD pipelines
through DevOps practices to ensure reliable and high-performance data operations.
o Oversee platform operability and ensure 24/7 availability of data repositories to
support business continuity.
Collaboration & Execution
o Engage with business stakeholders to understand requirements and deliver fit-for-
purpose data solutions that address strategic and operational needs.
o Collaborate closely with IT and cross-functional digital teams to implement and
deliver data solutions in a coordinated manner.
o Interact with multi-disciplinary project teams to support successful end-to-end
project execution.
Innovation & Continuous Improvement
o Stay up to date with the latest trends in data technologies and actively contribute to
innovation initiatives.
o Propose and implement enhancements for performance, scalability, and
maintainability of existing data systems and platforms.
You'll be working in the Global Artificial Intelligence team, Linde's AI global corporate division
engaged with real business challenges and opportunities in multiple countries. Focus of this role is to support the AI team with extending existing and building new AI products for a vast amount of uses cases across Linde's business and value chain.You'll collaborate across different business and
corporate functions in international team composed of Project Managers, Data Scientists, Data and
Software Engineers in the AI team and others in the Linde's Global AI team.
What Makes You Great
Bachelor's degree in Computer Science or related Engineering areas with 3+ years of
experience in manufacturing settings to develop data-engineering solutions, tools and
software applications.
3+ years of experience evaluating and implementing data-engineering and software
technologies.
Experience in programming languages and frameworks: SQL, Python, Spark, Databricks
(Delta Lake)
Experience in data storages: SQL and NoSQL databases, Azure Data Lake Storage
Experience in developing data solutions, models, API and software applications using SQL,
Python and .NET
Working knowledge of Azure infrastructure management and resource deployment
Working knowledge of system networking and security
Working knowledge of data visualization tools: PowerBI, Grafana, and Tableau.
Perseverance and results driven attitude to achieve goals and objectives on time.
Strong analytical and problem-solving skills.
Strong communication and presentation skills.
Preferred:
Master's or PhD degree in Computer Science or related Engineering areas with 5 years of
experience in developing data-engineering solutions, tools and software applications.
Strong programming skills and demonstrated ability to work in complex software
developments.
Knowledge of machine learning theory with practical development experience.","external APIs, Azure Data Lake Storage, iHistorian, Delta Lake, ERP systems, web platforms, IoT devices, SQL and NoSQL databases, Databricks, Sql, .NET, Grafana, Tableau, Powerbi, Python, Azure, Spark"
Data Engineer,Avalara India,6-8 Years,,"Pune, India",Login to check your skill match score,"What You'll Do
We are seeking an experienced Lead Data Engineer with experience in the Data Engineering. We are looking for a background in ETL processes, data warehousing, data modeling, and hands-on expertise in SQL and Python. The ideal candidate will have exposure to cloud technologies and will play a key role in designing and managing scalable, high-performance data systems that support marketing and sales insights.
You will report to Manager- Data engineering
What Your Responsibilities Will Be
You will Design, develop, and maintain efficient ETL pipelines using DBT,Airflow to move and transform data from multiple sources into a data warehouse.
You will Lead the development and optimization of data models (e.g., star, snowflake schemas) and data structures to support reporting.
You will Leverage cloud platforms (e.g., AWS, Azure, Google Cloud) to manage and scale data storage, processing, and transformation processes.
You will Work with business teams, marketing, and sales departments to understand data requirements and translate them into actionable insights and efficient data structures.
You will Use advanced SQL and Python skills to query, manipulate, and transform data for multiple use cases and reporting needs.
You will Implement data quality checks and ensure that the data adheres to governance best practices, maintaining consistency and integrity across datasets.
You will Experience using Git for version control and collaborating on data engineering projects.
What You'll Need To Be Successful
Bachelor's degree with 6+ years of experience in Data Engineering.
ETL/ELT Expertise: experience in building, improving ETL/ELT processes.
Data Modeling: experience with designing and implementing data models such as star and snowflake schemas, and working with denormalized tables to optimize reporting performance.
Experience with cloud-based data platforms (AWS, Azure, Google Cloud)
SQL and Python Proficiency: Advanced SQL skills for querying large datasets and Python for automation, data processing, and integration tasks.
DBT Experience: Hands-on experience with DBT (Data Build Tool) for transforming and managing data models.
Good To Have Skills
Familiarity with AI concepts such as machine learning (ML), (NLP), and generative AI. Work with AI-driven tools and models for data analysis, reporting, and automation.
Oversee and implement DBT models to improve the data transformation process.
Experience in the marketing and sales domain, with lead management, marketing analytics, and sales data integration.
Familiarity with business intelligence reporting tools, Power BI, for building data models and generating insights.
How We'll Take Care Of You
Total Rewards
In addition to a great compensation package, paid time off, and paid parental leave, many Avalara employees are eligible for bonuses.
Health & Wellness
Benefits vary by location but generally include private medical, life, and disability insurance.
Inclusive culture and diversity
Avalara strongly supports diversity, equity, and inclusion, and is committed to integrating them into our business practices and our organizational culture. We also have a total of 8 employee-run resource groups, each with senior leadership and exec sponsorship.
Learn more about our benefits by region here: Avalara North America
What You Need To Know About Avalara
We're Avalara. We're defining the relationship between tax and tech.
We've already built an industry-leading cloud compliance platform, processing nearly 40 billion customer API calls and over 5 million tax returns a year, and this year we became a billion-dollar business. Our growth is real, and we're not slowing down until we've achieved our mission - to be part of every transaction in the world.
We're bright, innovative, and disruptive, like the orange we love to wear. It captures our quirky spirit and optimistic mindset. It shows off the culture we've designed, that empowers our people to win. Ownership and achievement go hand in hand here. We instill passion in our people through the trust we place in them.
We've been different from day one. Join us, and your career will be too.
We're An Equal Opportunity Employer
Supporting diversity and inclusion is a cornerstone of our company we don't want people to fit into our culture, but to enrich it. All qualified candidates will receive consideration for employment without regard to race, color, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law. If you require any reasonable adjustments during the recruitment process, please let us know.","Airflow, dbt, Git, Data Modeling, Data Warehousing, Azure, Google Cloud, Sql, Python, AWS, Etl"
Data Engineer,Lingaro,8-10 Years,,India,Login to check your skill match score,"Role: Data Engineer Lead Consultant
Location: India (Full Time-Remote)
Preference: Immediate Joiners
About Lingaro:
Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.
Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.
About
Data Engineering:Data e
ngineering involves the development of solutions for the collection, transformation, storage and management of data to support data-driven decision making and enable efficient data analysis by end users. It focuses on the technical aspects of data processing, integration, and delivery to ensure that data is accurate, reliable, and accessible in a timely manner. It also focuses on the scalability, cost-effectiveness, security, and supportability of the solution. Data engineering encompasses multiple toolsets and architectural concepts across on-premises and cloud stacks, including but not limited to data warehousing, data lakes, lake house, data mesh, and includes extraction, ingestion, and synchronization of structured and unstructured data across the data ecosystem. It also includes processing organization and orchestration, as well as performance optimization of data processing.Job Re
sponsibilities:Provid
e leadership and guidance to the data engineering team, including mentoring, coaching, and fostering a collaborative work environment. Set clear goals, assign tasks, and manage resources to ensure successful project delivery. Work closely with developers to support them and improve data engineering processes. Suppor
t team members with troubleshooting and resolving complex technical issues and challenges. Provid
e technical expertise and direction in data engineering, guiding the team in selecting appropriate tools, technologies, and methodologies. Stay updated with the latest advancements in data engineering and ensure the team follows best practices and industry standards. Collab
orate with stakeholders to understand project requirements, define scope, and create project plans. Suppor
t project managers to ensure that projects are executed effectively, meeting timelines, budgets, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Act as
a trusted advisor for the customer. Overse
e the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration. Align
coding standards, conduct code reviews to ensure proper code quality level. Identi
fy and introduce quality assurance processes for data pipelines and workflows. Optimi
ze data processing and storage for performance, efficiency and cost savings. Evalua
te and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Act as
main point of contact to other teams/contributors engaged in the project. Mainta
in technical documentation of the project, control validity and perform regular reviews of it. Ensure
compliance with security standards and regulations. Requi
r
ements:A bach
elor's or master's degree in Computer Science, Information Systems, or a related field is typically required. Additional certifications in cloud are advantageous. Minimu
m of 8 years of experience in data engineering or a related field. Strong
technical skills in data engineering, including proficiency in programming languages such as Python, SQL, Pyspark. Famili
arity with Azure cloud platform viz. Azure Databricks, Data Factory, Data Lake etc., and experience in implementing data solutions in a cloud environment. Expert
ise in working with various data tools and technologies, such as ETL frameworks, data pipelines, and data warehousing solutions. In-dep
th knowledge of data management principles and best practices, including data governance, data quality, and data integration. Excell
ent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues. Knowle
dge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects. Excell
ent communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams, stakeholders, and senior management. Contin
uous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies. Consul
ting exposure, with external customer focus mindset is preferred.Why
j
oin us: Stabl
e
employment. On the market since 2008, 1300+ talents currently on board in 7 global sites.100% remote.Flexibility regarding working hours.Full-time positionComprehensive online onboarding program with a Buddy from day 1.Cooperation with top-tier engineers and experts.Unlimited access to the Udemy learning platform from day 1.Certificate training programs. Lingarians earn 500+ technology certificates yearly.Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.Grow as we grow as a company. 76% of our managers are internal promotions.A diverse, inclusive, and values-driven community.Autonomy to choose the way you work. We trust your ideas.Create our community together. Refer your friends to receive bonuses.Activities to support your well-being and health.Plenty of opportunities to donate to charities and support the environment.","ETL frameworks, data pipelines, data warehousing solutions, Pyspark, Data Factory, Data Lake, Azure Databricks, Python, Sql"
Data Engineer,Anblicks,Fresher,,"Hyderabad, India",Login to check your skill match score,"Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise in Azure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake) along with big data processing, real-time analytics, and cloud data integration.
Key Responsibilities
Data Pipeline Development & ETL/ELT
Design and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.
Implement ETL/ELT workflows for structured and unstructured data processing.
Optimize data ingestion, transformation, and storage strategies.
Cloud Data Architecture & Integration
Develop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).
Work with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.
Database Management & Optimization
Design and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).
Optimize SQL queries and indexing strategies for performance.
Implement data partitioning, compression, and caching for efficiency.
Data Governance, Security & Compliance
Ensure data quality, lineage, and governance with tools like Purview.
Implement role-based access control (RBAC), encryption, and security policies.
Ensure compliance with GDPR, HIPAA, and ISO 27001 regulations.
Monitoring & Performance Tuning
Use Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.
Automate data pipeline testing and validation.
Collaboration & Documentation
Document data models, pipeline architectures, and data workflows.
Technical Skills
Cloud Data Services: Azure Data Factory, Azure Synapse, Databricks, SSIS, BigQuery.
ETL & Data Pipelines: Apache Spark, Python, SQL.
Big Data Processing: Delta Lake, Parquet, Hadoop, Event Hubs.
Database Management: SQL Server, Cosmos DB.
Security & Compliance: RBAC, Data Masking, Encryption, Purview.
Scripting & Automation: Python, PowerShell, Terraform for IaC.","Purview, Event Hubs, Parquet, Synapse, rbac, Delta Lake, Azure Data Services, Encryption, ELT, Terraform, Cosmos DB, Python, BigQuery, Hadoop, PowerShell, Apache Spark, SQL Server, SSIS, Sql, Azure Data Factory, Data Lake, Databricks, data masking, Etl"
Data Engineer -Pharma Commerical Domain,Predigle,3-5 Years,,"Chennai, India",Login to check your skill match score,"We are seeking an experiencedData Engineerwith a strong background inPharma Commercial Datato join our growing data team. The ideal candidate will have hands-on experience withAzure Databricks,Snowflake, and a deep understanding of pharmaceutical commercial datasets, including sales, claims, and HCP-level data.
Key Responsibilities
Design, build, and maintain scalable and efficient data pipelines focused onPharma Commercial datasets
Develop data integration workflows usingAzure Databricksand manage data warehousing inSnowflake.
Work closely with business stakeholders, analytics teams, and data scientists to ensure data solutions support strategic commercial initiatives.
Ensure data quality, consistency, and governance across all pharma commercial datasets.
Automate routine data processes and monitor pipeline performance for production stability.
Participate in architecture reviews and recommend improvements for performance and scalability.
Required Qualifications
Minimum3 years of hands-on experience with Pharma Commercial Data, including datasets like sales, prescription, claims, payer data, or CRM/HCP information.
Proven expertise inAzure DatabricksandSnowflake.
Strong proficiency inSQLandPythonfor data manipulation and transformation.
Solid understanding of data modeling, ETL/ELT frameworks, and cloud-based data engineering best practices.
Experience with data orchestration tools (e.g., Airflow, Azure Data Factory).
Ability to work independently and communicate effectively with business and technical teams.
Preferred Qualifications
Exposure to commercial analytics use cases like HCP segmentation, field force effectiveness, and targeting.
Familiarity with data privacy, HIPAA, and compliance regulations in the pharma domain.
Experience working in Agile teams and DevOps environments.
What We Offer
Opportunity to work on impactful commercial data initiatives in the pharmaceutical industry.
Competitive compensation and benefits.
Remote-friendly, flexible work culture.
Supportive team environment and continuous learning opportunities.
About Predigle:
Predigle, an EsperGroup company, is an American multinational organization focused on building a disruptive technology platform that revolutionizes the way businesses conduct their daily operations.
Predigle has grown rapidly to offer multiple products and services,As a growing startup, we offer an entrepreneurial work environment where ideas are valued, creativity is encouraged, and learning opportunities are immense.
https://espergroup.com
/ https://predigle.com/
https://www.linkedin.com/company/predigle/","Airflow, snowflake, data orchestration tools, Azure Data Factory, Azure Databricks, Python, Sql, Etl, ELT"
Data Engineer - Looker,"iitjobs, Inc.",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"We have an great opportunity for the role of Data Engineer- Looker.
Mandatory Skills : Looker Action, Looker Dashboarding, Looker Data Entry, LookML, SQL Queries.
Relevant Exp : 6+ Yrs
Job Summary:
We are seeking a skilled Data Engineer with deep expertise in Looker, including Looker Actions, Dashboarding, Data Entry, LookML, and SQL Queries. You will play a key role in designing, implementing, and optimizing Looker-based solutions that enable data visualization, accessibility, and actionable insights.
Key Responsibilities:
Design and develop Looker dashboards to provide actionable insights and data visualization for stakeholders.
Implement Looker Actions for seamless integration with workflows and business processes.
Manage and maintain data entry pipelines within Looker to ensure data accuracy and completeness.
Develop and maintain LookML models for efficient data analysis and exploration.
Write and optimize SQL queries to extract, transform, and load data into Looker.
Collaborate with data analysts and business teams to ensure solutions meet user needs.
Monitor Looker performance and troubleshoot any issues to ensure reliability.
Document workflows, LookML models, and dashboarding best practices.
Stay current with Looker updates and industry trends to implement new features effectively.
-Immediate Joiners to 15 days Preferred
-Job location- Remote
Thanks and Regards,
iitjobs, Inc.
Register for a global opportunity on the world's first & only Global Technology Job Portal: www.iitjobs.com
Download our app on the Apple App Store and Google Play Store!
Refer and earn 50,000!","Looker Data Entry, Looker Dashboarding, Looker Actions, Looker, LookML, Sql Queries"
Senior Data Engineer - Fabric,Anblicks,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Details
Job Summary:
We are seeking an experienced Senior Data Engineer with strong expertise in Microsoft Fabric to support our enterprise data modernization and analytics transformation initiatives. The ideal candidate will have a deep understanding of data pipelines, lakehouse architecture, Power BI, Synapse integration, and experience in modernizing legacy data systems to cloud-native solutions. This role is critical in building scalable, secure, and high-performing data solutions on the Microsoft ecosystem.
Key Responsibilities:
Design and implement data pipelines using Microsoft Fabric s Data Factory, Synapse Data Engineering, and OneLake components.
Build and maintain lakehouse architectures leveraging Delta Lake, Parquet, and OneLake within Microsoft Fabric.
Lead initiatives to modernize legacy ETL/ELT processes to cloud-native data pipelines.
Work closely with Data Architects, BI Developers, and Analysts to deliver scalable data models for analytics and reporting.
Optimize performance of Power BI datasets and reports through best practices in data modeling and DAX.
Implement data governance and security controls, including Microsoft Purview, role-based access, and lineage tracking.
Collaborate with cross-functional teams in cloud migration, especially from on-premises SQL/Oracle/Hadoop platforms to Microsoft Azure & Fabric.
Evaluate and implement CI/CD practices for data pipelines using Azure DevOps or GitHub Actions.
Required Skills & Qualifications:
Bachelor s/Master s degree in Computer Science, Information Systems, or related field.
8+ years of experience in data engineering
Strong hands-on experience with Microsoft Fabric components:
Data Factory
Lakehouse / OneLake
Synapse Data Engineering
Power BI
Experience with data modeling (star/snowflake) and performance tuning in Power BI.
Deep understanding of modern data architecture patterns including lakehouse, medallion architecture, and ELT frameworks.
Expertise in SQL, PySpark, T-SQL, DAX, and Power Query (M language).
Experience modernizing platforms from SSIS, Informatica, or Hadoop to cloud-native tools.
Familiarity with Azure ecosystem Azure Data Lake, Azure SQL DB, Azure Functions, Azure Synapse, Azure Data Factory.
Strong experience in CI/CD pipelines, preferably with Azure DevOps.
Familiarity with data security, GDPR, HIPAA, and enterprise data governance.
Preferred Qualifications:
Microsoft certifications such as:
Microsoft Certified: Fabric Analytics Engineer Associate
Azure Data Engineer Associate (DP-203)
Experience with DataOps and Agile delivery methods.
Knowledge of Machine Learning/AI integration with Fabric is a plus.
Hands-on with Notebooks in Microsoft Fabric using Python or Scala.
Soft Skills:
Strong analytical and problem-solving skills.
Excellent communication and stakeholder management capabilities.
Ability to lead projects, mentor junior engineers, and collaborate with cross-functional teams.","GitHub Actions, Lakehouse, Microsoft Fabric, OneLake, CI CD, Azure SQL DB, Synapse, Power Query M language, Sql, Data Factory, T-sql, Azure Data Factory, Pyspark, Power Bi, Azure Functions, Dax, Azure Synapse, Azure Data Lake, Azure DevOps"
Data engineer,Qloron Pvt Ltd,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Senior Data Engineer
Experience: Minimum 5+ Years
Employment Type: Full-Time
Job Summary
We are seeking a skilled and experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in Power BI development, data modeling, and SQL, with the ability to translate business requirements into actionable insights. You will be responsible for end-to-end dashboard development, managing junior developers, and optimizing performance across reports and dashboards.
Key Responsibilities
Analyze business requirements and translate them into data models and reporting solutions.
Perform GAP analysis between existing data models and business needs.
Design and model efficient Power BI schemas and architecture.
Transform and prepare data using Power BI, SQL, and ETL tools.
Develop complex DAX formulas, measures, and calculated columns for analytics.
Create visually appealing and functional Power BI reports and dashboards.
Write SQL queries and stored procedures to retrieve and manage data effectively.
Design robust Power BI solutions aligned with business objectives.
Lead and guide a team of Power BI developers, ensuring high-quality deliverables.
Integrate data from multiple sources into Power BI for holistic analysis.
Optimize the performance of Power BI dashboards and reports.
Collaborate with business stakeholders to align deliverables with strategic goals.
Required Skills
Minimum 5+ years of hands-on experience with Power BI development.
Strong proficiency in DAX, Power Query, and Power BI Service.
Excellent command of SQL, including stored procedures.
Proven experience in data modeling, data transformation, and ETL processes.
Strong understanding of data warehousing concepts (mandatory).
Experience working with multiple data sources and integrating them within Power BI.
Leadership capabilities to manage and mentor junior developers.
Solid communication and stakeholder management skills.
Preferred Qualifications
Bachelor's Degree in Computer Science, Information Technology, or equivalent.
Knowledge of Data Engineering concepts is a plus.
Experience with cloud platforms such as Azure or AWS is advantageous.","Data Modeling, Power Bi, Data Warehousing, Power Query, Dax, Sql, Etl"
"Manager 1, Domo Data Engineer",Kenvue,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
Manager Domo Data Engineer
Who We Are
At Kenvue , we realize the extraordinary power of everyday care. Built on over a century of heritage and rooted in science, we're the house of iconic brands - including Neutrogena, Aveeno, Tylenol, Listerine, Johnson's and BAND-AID Brand Adhesive Bandages that you already know and love. Science is our passion; care is our talent. Our global team is made up of 22,000 diverse and brilliant people, passionate about insights, innovation and committed to delivering the best products to our customers. With expertise and empathy, being a Kenvuer means having the power to impact the life of millions of people every day. We put people first, care fiercely, earn trust with science and solve with courage and have brilliant opportunities waiting for you! Join us in shaping our futureand yours. For more information, click here .
What You Will Do
As a Data Engineer in the Global Analytics team, you will play a crucial role in designing, building, and maintaining scalable data pipelines and data models. You will work with cutting-edge technologies such as Snowflake, Python, Databricks and Azure Services to enable data-driven insights and support various analytics initiatives across the organization.
Key Responsibilities
Data Pipeline Development- Design, develop, and maintain robust data pipelines to ingest, transform, and store data from diverse sources into Snowflake and DOMO ETL systems. other data storage solutions.
Strong understanding and experience (5+ years) of DOMO ETL and best practice. Experience of designing and implementing data marts, data lakes or data warehouses using Domo
Data Modeling- Collaborate with data analysts and business stakeholders to create and optimize data models that meet analytical requirements and support reporting needs.
ETL Processes- Implement ETL processes using DOMO and other tools to ensure data integrity, quality, and availability for analytics and reporting.
Collaboration- Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver relevant datasets for analysis.
Proactive approach to solution designing becoming a trusted advisor within the team.
Performance Optimization- Monitor and optimize the performance of data pipelines and data models, ensuring efficient processing and quick turnaround times.
Documentation- Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to support knowledge sharing and future development.
Cloud Infrastructure- Utilize Azure services to deploy and manage data solutions, ensuring security, scalability, and reliability.
Troubleshooting- Identify and resolve data-related issues, ensuring data quality and accuracy across all analytics initiatives.
What We Are Looking For
Required Qualifications-
Must have-
Bachelor's degree in computer science, Information Technology, Data Science, or a related field.
Proven experience (8+ years) as a Data Engineer or in a similar role, with a strong understanding of data engineering concepts and best practices with at least 2+ in DOMO data engineering
Proficiency in Python for data manipulation, pipeline development, and automation.
Experience with Snowflake, including data warehousing concepts and SQL for querying and managing data.
Familiarity with Databricks for data processing and analytics workflows.
Knowledge of Azure cloud services and architecture, particularly in relation to data storage and processing.
Experience with DBT (Data Build Tool) for transforming data and managing data models.
Strong analytical and problem-solving skills, with attention to detail and a commitment to data quality.
Excellent communication skills to collaborate effectively with technical and non-technical stakeholders.
Experience with data visualization tools (e.g., Tableau, Power BI) for creating reports and dashboards.
Familiarity with orchestration tools.
Knowledge of machine learning concepts and techniques.
Primary Location
Asia Pacific-India-Karnataka-Bangalore
Job Function
Digital Product Development
Job Qualifications
Nice to have-","snowflake, Azure Services, DBT Data Build Tool, Databricks, Sql, Power Bi, Python, Tableau, Domo"
Data Engineer,DELTACLASS TECHNOLOGY SOLUTIONS LIMITED,5-7 Years,,"Chennai, India",Login to check your skill match score,"Currently we are looking for Data Engieer role.
Responsibilities
Role :Data Engineer
Exp : 5+Yrs
Location: Hyderabad,Chennai
Notice : Immediate to 15 Days
JD
Data Engineer
Hyderabad/Chennai (Hybrid)
Full-time with Info Services
Requirement
BS or higher degree in Computer Science (or equivalent field)
3+ years of programming experience with Java and Python
Strong in writing SQL queries and understanding of Kafka, Spark/Flink.
Exposure to AWS Lambda, AWS Cloud Watch, Step Functions, EC2, Cloud Formation, Jenkins
3+ years of experience with Snowflake or Databricks
BS or higher degree in Computer Science (or equivalent field)
3+ years of programming experience with Java and Python
Strong in writing SQL queries and understanding of Kafka, Spark/Flink.
Exposure to AWS Lambda, AWS Cloud Watch, Step Functions, EC2, Cloud Formation, Jenkins
3+ years of experience with Snowflake or Databricks
BS or higher degree in Computer Science (or equivalent field)
3+ years of programming experience with Java and Python
Strong in writing SQL queries and understanding of Kafka, Spark/Flink.
Exposure to AWS Lambda, AWS Cloud Watch, Step Functions, EC2, Cloud Formation, Jenkins
3+ years of experience with Snowflake or Databricks
Qualifications
Any Graduation Qualification is Fine.
Skills: sql,cloud watch,,sql quries,ec2,snowflake,,spark,aws lambda,data engineer,snowflake,aws cloud watch,java,flink,step functions,jenkins,kafka,databricks,cloud formation,python,aws lamda,","Step Functions, Flink, AWS Cloud Watch, snowflake, Java, Aws Lambda, Cloud Formation, Kafka, Sql, Jenkins, Ec2, Spark, Databricks, Python"
Lead Snowflake Data Engineer,Ventra Health,7-9 Years,,"Chennai, India",Login to check your skill match score,"Ventra is a leading business solutions provider for facility-based physicians practicing anesthesia, emergency medicine, hospital medicine, pathology, and radiology. Focused on Revenue Cycle Management, Ventra partners with private practices, hospitals, health systems, and ambulatory surgery centers to deliver transparent and data-driven solutions that solve the most complex revenue and reimbursement issues, enabling clinicians to focus on providing outstanding care to their patients and communities.
Job Summary
We are seeking an experienced Lead Snowflake Data Engineer to join our Data & Analytics team. This role involves designing, implementing, and optimizing Snowflake-based data solutions while providing strategic direction and leadership to a team of junior and mid-level data engineers. The ideal candidate will have deep expertise in Snowflake, cloud data platforms, ETL/ELT processes, and Medallion data architecture best practices. The lead data engineer role has a strong focus on performance optimization, security, scalability, and Snowflake credit control and management. This is a tactical role requiring independent in-depth data analysis and data discovery to understand our existing source systems, fact and dimension data models, and implement an enterprise data warehouse solution in Snowflake.
Essential Functions And Tasks
Lead the design, development, and maintenance of a scalable Snowflake data solution serving our enterprise data & analytics team.
Architect and implement data pipelines, ETL/ELT workflows, and data warehouse solutions using Snowflake and related technologies.
Optimize Snowflake database performance, storage, and security.
Provide guidance on Snowflake best practices.
Collaborate with cross-functional teams of data analysts, business analysts, data scientists, and software engineers, to define and implement data solutions.
Ensure data quality, integrity, and governance across the organization.
Provide technical leadership and mentorship to junior and mid-level data engineers.
Troubleshoot and resolve data-related issues, ensuring high availability and performance of the data platform.
Education And Experience Requirements
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
7+ years of experience in-depth data engineering, with at least 3+ minimum years of dedicated experience engineering solutions in a Snowflake environment.
Tactical expertise in ANSI SQL, performance tuning, and data modeling techniques.
Strong experience with cloud platforms (preference to Azure) and their data services.
Proficiency in ETL/ELT development using tools such as Azure Data Factory, dbt, Matillion, Talend, or Fivetran.
Hands-on experience with scripting languages like Python for data processing.
Strong understanding of data governance, security, and compliance best practices.
Snowflake SnowPro certification; preference to the engineering course path.
Experience with CI/CD pipelines, DevOps practices, and Infrastructure as Code (IaC).
Knowledge of streaming data processing frameworks such as Apache Kafka or Spark Streaming.
Familiarity with BI and visualization tools such as PowerBI.
Knowledge, Skills, And Abilities
Familiarity working in an agile scum team, including sprint planning, daily stand-ups, backlog grooming, and retrospectives.
Ability to self-manage large complex deliverables and document user stories and tasks through Azure Dev Ops.
Personal accountability to committed sprint user stories and tasks.
Strong analytical and problem-solving skills with the ability to handle complex data challenges.
Ability to read, understand, and apply state/federal laws, regulations, and policies.
Ability to communicate with diverse personalities in a tactful, mature, and professional manner.
Ability to remain flexible and work within a collaborative and fast paced environment.
Understand and comply with company policies and procedures.
Strong oral, written, and interpersonal communication skills.
Strong time management and organizational skills.
Ventra Health
Equal Employment Opportunity (Applicable only in the US)
Ventra Health is an equal opportunity employer committed to fostering a culturally diverse organization. We strive for inclusiveness and a workplace where mutual respect is paramount. We encourage applications from a diverse pool of candidates, and all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, or veteran status. We will provide reasonable accommodations to qualified individuals with disabilities, as needed, to assist them in performing essential job functions.
Recruitment Agencies
Ventra Health does not accept unsolicited agency resumes. Ventra Health is not responsible for any fees related to unsolicited resumes.
Solicitation of Payment
Ventra Health does not solicit payment from our applicants and candidates for consideration or placement.
Attention Candidates
Please be aware that there have been reports of individuals falsely claiming to represent Ventra Health or one of our affiliated entities Ventra Health Private Limited and Ventra Health Global Services. These scammers may attempt to conduct fake interviews, solicit personal information, and, in some cases, have sent fraudulent offer letters.
To protect yourself, verify any communication you receive by contacting us directly through our official channels. If you have any doubts, please contact us at [HIDDEN TEXT] to confirm the legitimacy of the offer and the person who contacted you. All legitimate roles are posted on https://ventrahealth.com/careers/.
Statement of Accessibility
Ventra Health is committed to making our digital experiences accessible to all users, regardless of ability or assistive technology preferences. We continually work to enhance the user experience through ongoing improvements and adherence to accessibility standards. Please review at https://ventrahealth.com/statement-of-accessibility/.","DevOps practices, Matillion, dbt, snowflake, Fivetran, ELT, Spark Streaming, Azure Data Factory, Powerbi, Apache Kafka, Data Governance, Ansi Sql, Talend, Python, Etl"
Data Engineer-Specialized-Associate - Operate,PwC Acceleration Centers in India,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services.
KEY SKILLS - Mainframe, Teradata Datastage
Mainframe and Teradata DataStage Associate
Summary:.
Minimum Degree Required: Bachelor's degree in computer science/IT or relevant field
Degree Preferred: Master's degree in computer science/IT or relevant field
Minimum Years of Experience: 6 - 8 year(s)
Certifications Required: NA
Required Knowledge/Skills: (character count limit 5000) *PLEASE ONLY USE THIS FIELD IF THIS IS A MUST HAVE SKILL FOR APPLICANT*
Job Summary
We are seeking a skilled and experienced IT professional to join our team as a Mainframe and Teradata DataStage Associate. The successful candidate will be responsible for developing, maintaining, and optimizing ETL processes using IBM DataStage, as well as managing and supporting data operations on Mainframe and Teradata platforms.
Key Responsibilities
Design, develop, and implement ETL processes using IBM DataStage to support data integration and transformation requirements.
Manage and maintain data on Mainframe and Teradata systems, ensuring data integrity and performance optimization.
Collaborate with business analysts and stakeholders to understand data requirements and translate them into technical specifications.
Troubleshoot and resolve issues related to ETL processes and data management on Mainframe and Teradata platforms.
Monitor and tune the performance of ETL jobs and database queries to ensure optimal performance.
Develop and maintain documentation related to ETL processes, data flows, and system configurations.
Participate in code reviews and ensure adherence to best practices and coding standards.
Provide support for data migration and integration projects, ensuring timely and accurate data delivery.
Stay updated with the latest developments in Mainframe, Teradata, and DataStage technologies and recommend improvements.
Qualifications
Job Summary -
A career in our Managed Services team will provide you with an opportunity to collaborate with a wide array of teams to help our clients implement and operate new capabilities, achieve operational efficiencies, and harness the power of technology. Our Data, Testing & Analytics as a Service team brings a unique combination of industry expertise, technology, data management and managed services experience to create sustained outcomes for our clients and improve business performance. We empower companies to transform their approach to analytics and insights while building your skills in exciting new directions. Have a voice at our table to help design, build and operate the next generation of software and services that manage interactions across all aspects of the value chain.
Minimum Degree Required (BQ) *:
Bachelor's degree
Degree Preferred
Required Field(s) of Study (BQ):
Preferred Field(s) Of Study
Computer and Information Science, Management Information Systems
Minimum Year(s) of Experience (BQ) *: US
Certification(s) Preferred
Minimum of 1 year of experience
Preferred Skills (PQs)
Position Requirements:
Datasphere
Required Skills:
More than 2 years of hands-on experience in SAP Datasphere / DWC at least 1 full life cycle project implementation.
Work on development/maintenance of DWC Models, CDS Views, SQL Scripts SAC Stories
Should have experience in building complex models in SAP Datasphere/ DWC
Developing SAP Datasphere end-to-end Dataflows Design, build data flows, and develop chains to load and monitor Data Loading.
Knowledge in setting up the connections to Datasphere and from Datasphere.
Knowledge in handling the delta in Datasphere.
Unit testing the dataflows and reconciling the data to Source Systems.
Good exposure in troubleshooting data issues and provide workarounds in cases where there are product limitations.
Good exposure with Datasphere security setup, currency conversion.
Good knowledge in writing CDS Analytical Queries and S4HANA Embedded Analytics.
Good exposure in performance tuning of the models in the datasphere.
Good knowledge on Datasphere and Data Lake integration.
Good Knowledge on using the Database explorer and SAP Hana Cockpit through Datasphere.
Nice To Have
Good knowledge in either BW Modeling or HANA Modeling.
BW/4HANA And/or Native HANA (or HANA Cloud) modeling, including SQL Scripting, Graphical View-Modelling, SDA extraction.","DWC, SAP Datasphere, SAC Stories, Teradata, DataStage, Cds Views, Mainframe, Sql"
Data Engineer,Prachodayath Global Services Private Limited,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"Data Engineering is the core team for all decision making systems and responsible for creating bespoke data processing workflows to enable Mistplay AI products and advanced analytics. You will become a part of our core data engineering function focusing on driving operational excellence across all data stacks. You will work closely with our ML and Analytics teams to modernize our data platform and continuous innovation on data features. You will provide the technical ownership to help drive continuous improvement to our current data processing workflows and new lakehouse architecture.
Role :- Senior Data Engineer
Experience -7+
Work Mode - Onsite
Full-time, Permanent Role
Budget - 11-12 LPA
As a Senior Data Engineer II you will be working closely with engineering, operations, and product to deploy new applications on our data lakehouse, refactoring legacy elements and design new features/data pipelines. This position requires someone who is passionate about data architecture design, big data technologies, deep technical proficiency in distributed data processing, real-time streaming, a strong problem-solver, a team collaborator and has a growth mindset.
What You'll Do
Design and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.
Participate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.
Independently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.
Collaborate with development, AI, and data science teams to integrate data solutions into complex enterprise systems, ensuring seamless interoperability with existing platforms.
Monitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.
Analyze and interpret trends in complex data sets, utilizing visualization tools (e.g., Tableau, Power BI) to create dashboards and reports that tell compelling data stories.
Create and maintain standardized operational tools and reporting mechanisms to communicate data health and business performance to various audiences, including executives.
Maintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.
Acquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.
Automate data analyses and authoring pipelines using tools such as Kinesis, Airflow, Lambda, Databricks, DBT, and other relevant technologies.
Engage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.
Drive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.
What You'll Bring
Bachelor's degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.
7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.
Extensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.
Proficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.
Expertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.
Experience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).
Ability to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.
Strong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.
Exceptional attention to detail and professionalism in report and dashboard creation.
Excellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.
Outstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.
Interested candidates share your resume at [HIDDEN TEXT]","Airflow, Beam, dbt, Java, Apache Flink, Scala, Redis, Lambda, Git, Kinesis, Terraform, Spark, Databricks, Python, Kubernetes"
DBT Data Engineer - Remote,Cognisol,6-8 Years,,"Chennai, India",Login to check your skill match score,"Key Skillset-DBT,Python,SQL,AWS,pYSPARK
Years of Exp- 6 to 7 Years
Work Mode-Work From Home(Candidate should be available for 1st week of Onboarding @ Chennai Loc)
Shift Time-UK Shift time
Notice: Immediate to 15 days only
Placement Type: Contractual Position
Key Responsibilities
Data Pipeline Development: Design, implement, and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from various sources into cloud platforms like AWS.
Cloud Integration: Utilize AWS services such as Glue, Step Functions, Redshift, and Lambda to process and store data efficiently.
Data Transformation: Develop and maintain accurate data pipelines using Python and SQL, transforming data for aggregations, wrangling, quality control, and calculations.
Workflow Automation: Enhance end-to-end workflows with automation tools to accelerate data flow and pipeline management.
Collaboration: Work closely with business analysts, data scientists, and cross-functional teams to understand data requirements and develop solutions that support data-driven decision-making.
Qualifications
Educational Background: Bachelor's or Master's degree in Information Technology, Bioinformatics, Computer Science, or a related field.
Professional Experience: Several years of experience in data engineering, with hands-on expertise in:
Developing and managing large-scale ETL data pipelines on AWS.
Proficiency in Python and SQL for data pipeline development.
Utilizing AWS services such as Glue, Step Functions, Redshift, and Lambda.
Familiarity with tools like Docker, Linux Shell Scripting, Pandas, PySpark, and Numpy.
Soft Skills: Strong problem-solving abilities, excellent communication skills, and the capacity to work collaboratively in a dynamic environment.
Skills: etl,linux shell scripting,pyspark,docker,pipelines,sql,aws,redshift,glue,pipeline,cloud,python,lambda,dbt,step functions,pandas,numpy","dbt, Glue, Pyspark, Shell Scripting, Redshift, Sql, Lambda, Numpy, Pandas, Linux, Docker, Python, AWS"
AWS Data Engineer (DBT),Artefact,3-9 Years,,"Pune, India",Login to check your skill match score,"Job Title AWS Data Engineer + DBT (Data Build Tool)
1) Min Exp 3-5 years
2) Min Exp - 5-9 years
Key Responsibilities:
Work with cross-functional teams to build and maintain data pipelines and project infrastructure
Manage end-to-end data engineering tasks, including data ingestion, transformation, and integration with the data lake
Ensure data quality and perform data transformations within the AWS ecosystem
Support CDP-related initiatives, particularly involving AWS cloud services and popular marketing and analytics platforms
Required Skills and Experience:
AWS Data Engineering: Hands-on experience with AWS S3, Glue, Lambda, Redshift and other AWS services.
DBT: Experience with DBT to make activities accessible to people with data analytical skills.
SQL: Ability to write and optimize complex SQL queries
Python: Experience in Python for data processing and automation tasks
Version Control: Knowledge of tools like Jenkins and Bitbucket for CI/CD
Additional Skills:
Data Modelling: Experience in creating and maintaining data models for large-scale environments
Big Data Technologies: Familiarity with Databricks and other big data processing frameworks
Workflow Orchestration: Familiarity with Apache Airflow or AWS Step Functions for managing complex data workflows
Qualifications:
Proven experience in delivering data engineering projects, ideally within large organizations
Ability to take ownership of tasks and work independently in an individual contributor role
No strict educational background requirements, but strong hands-on experience in delivering projects with diverse teams is essential","AWS Step Functions, dbt, Apache Airflow, Jenkins, Bitbucket, Databricks, Sql, Python, Aws S3"
Senior Data Engineer (SQL and ADF),thinkbridge,5-7 Years,,India,Login to check your skill match score,"We are hiring a Senior Data Engineer with 5+ years of experience in SQL , who has good hands-on in ADF and ETL. Good communication is mandatory.
About thinkbridge
thinkbridge is how growth-stage companies can finally turn into tech disruptors. They get a new way there with world-class technology strategy, development, maintenance, and data science all in one place. But solving technology problems like these involves a lot more than code. That's why we encourage thinkers to spend 80% of their time thinking through solutions and 20% coding them. With an average client tenure of 4+ years, you won't be hopping from project to project here unless you want to. So, you really can get to know your clients and understand their challenges on a deeper level. At thinkbridge, you can expand your knowledge during work hours specifically reserved for learning. Or even transition to a completely different role in the organization. It's all about challenging yourself while you challenge small thinking.
thinkbridge is a place where you can:
Think bigger because you have the time, opportunity, and support it takes to dig deeper and tackle larger issues.
Move faster because you'll be working with experienced, helpful teams who can guide you through challenges, quickly resolve issues, and show you new ways to get things done.
Go further because you have the opportunity to grow professionally, add new skills, and take on new responsibilities in an organization that takes a long-term view of every relationship.
thinkbridge.. there's a new way there.
What is expected of you
As part of the job, you will be required to
Read everything in detail that comes your way.
Elicit, analyze, specify & validate business requirements.
Define & Document the engineering requirements including architecture, components, usable interfaces & other characteristics.
Place specific emphasis on technical and usability design.
Code and verify the solution.
Debug & squash bugs.
Keep stakeholders informed.
Keep up with the technology and the domain.
If your beliefs resonate with these, you are looking at the right place!
Accountability Finish what you started.
Communication Context-aware, pro-active and clean communication.
Outcome High throughput.
Quality High-quality work and consistency.
Ownership Go beyond.
Requirements
Must-Have:
Should have hands-on experience in writing SQL Queries and Stored Procedures.
Excellent Communication.
Should have experience in ADF (Azure Data Factory)
Good experience in building ETL Solutions for large datasets
Good to have:
Good to have SSIS Experience
Other Details :
Remote First
Flexible work hours
No loss of pay for pre-approved leaves
Family Insurance
Quarterly in-person Collaboration Week -WWW","Adf, SSIS, Sql, Etl"
ETL Data Engineer,QTek Digital,6-9 Years,,"Hyderabad, India",Login to check your skill match score,"Job description
Company Description
QTek Digital is a data solutions provider specializing in custom solutions in data management, data warehouse, and data science. Our team of data scientists, data analysts, and data engineers work together to solve today's challenges and unlock future possibilities. As an employee-centric company, we prioritize engagement, empowerment, and enrichment of our employees.
Role Description
This is a full-time remote role for a BI ETL Engineer at QTek Digital. As a BI ETL Engineer, you will be responsible for day-to-day tasks such as data modeling, utilizing analytical skills, implementing data warehousing solutions, and executing Extract, Transform, Load (ETL) processes. This role requires strong problem-solving abilities and the ability to work independently.
Qualifications
6-9 years of experience with ETL and ELT pipeline build out using Pentaho, SSIS, FiveTran, Airbyte, or similar tools.
6-8 Years of experience in SQL and data manipulation languages
Strong Data Modeling, Dashboard, and Analytical Skills
Excellent understanding of data warehousing concepts, esp. Kimball design.
Experience with Pentaho and Airbyte administration will be a huge plus.
Strong skills in Data Modeling, Dashboard design, and Analytics
Experience in Data Warehousing and Extract Transform Load (ETL) processes
Strong problem-solving and troubleshooting skills
Excellent communication and collaboration skills
Ability to work independently and in a team
Bachelor's degree in computer science, Information Systems, or a related field
This role is based onsite in our Hyderabad Office.
The compensation range for this role is INR 5-19 Lakhs depending on a variety of factors including skills and experience.","Airbyte, FiveTran, Pentaho, Data Modeling, Data Warehousing, SSIS, Sql, ELT, Etl"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description
Oncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.
Job Type:Full-time or Contract
Start Date: June 1st 2025
Role Description
This is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.
Job Description:
Candidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively
6+ Years of experience as a Data Engineer
Strong technical expertise in SQL and Snowflake is a must.
Strong knowledge of joins and common table expressions (CTEs)
Strong experience with Python
Strong expertise in ETL process and with various data model concepts
Knowledge of star schema and snowflake schema
Good to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue
Experience with Big Data Tools and technologies
Key Skills:
Good Understanding of data structures and data analysis using SQL
Knowledge of implementing ETL/ELT for data solutions end-to-end
Understanding requirements, and data solutions (ingest, storage, integration, processing)
Knowledge of analyzing data using SQL
Conducting End to End verification and validation for the entire application
Responsibilities:
Understand and translate business needs into data models supporting long-term solutions.
Perform reverse engineering of physical data models from databases and SQL scripts.
Analyze data-related system integration challenges and propose appropriate solutions.
Assist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
Data Engineer - Senior Associate,PwC Acceleration Centers in India,5-8 Years,,"Hyderabad, India",Login to check your skill match score,"At PwC, our people in data management focus on organising and maintaining data to enable accuracy and accessibility for effective decision-making. These individuals handle data governance, quality control, and data integration to support business operations. In data governance at PwC, you will focus on establishing and maintaining policies and procedures to optimise the quality, integrity, and security of data. You will be responsible for optimising data management processes and mitigate risks associated with data usage.
Focused on relationships, you are building meaningful client connections, and learning how to manage and inspire others. Navigating increasingly complex situations, you are growing your personal brand, deepening technical expertise and awareness of your strengths. You are expected to anticipate the needs of your teams and clients, and to deliver quality. Embracing increased ambiguity, you are comfortable when the path forward isn't clear, you ask questions, and you use these moments as opportunities to grow.
Skills
Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:
Respond effectively to the diverse perspectives, needs, and feelings of others.
Use a broad range of tools, methodologies and techniques to generate new ideas and solve problems.
Use critical thinking to break down complex concepts.
Understand the broader objectives of your project or role and how your work fits into the overall strategy.
Develop a deeper understanding of the business context and how it is changing.
Use reflection to develop self awareness, enhance strengths and address development areas.
Interpret data to inform insights and recommendations.
Uphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.
Job Description/Activities To Be Performed
Working comprehensively with Azure Data Factory and Azure Cloud technologies with multiple data source systems on regular basis.
Create and maintain optimal data pipelines in - On Premise SQL Server Infrastructure and Azure Cloud Environments.
Responsible for expanding and optimizing data and data pipeline architecture as well as optimizing data flow and collection for cross functional utilization.
Design, develop and deploy high volume ETL pipelines to manage complex and near-real time data collection
Expertise working with SQL Server 2005/2008 R2/2012 tools Such as Management Studio, Query Analyzer, SQL Profiler, SQL Agent, SSIS and SSRS.
Develop complex queries using sub queries and multiple joins.
Work with the data management team and project leads to assist with data related technical issues, data analysis and support infrastructure needs
Monitor Data Quality Controls/Remediate
Data governance, version management, CI/CD deployments
Job Qualifications
5 to 8 years of core hands on experience as a Data Engineer, developing, maintaining and optimising data pipelines in Azure and SQL Server infrastructure.
Hand on with MS-SQL Server 2012/2008 R2/2008/2005, SQL Server Enterprise Manager, Transact-SQL T-SQL
Core expertise in Azure Data Factory (ADF), and SQL Server Integration Services SSIS
Handle common database procedures such as upgrade, backup, recovery, migration, etc.
Advanced working knowledge and experience with relational databases and database administration/ management
Ability to analyse and anticipate client requests, interpret asks and act according to expectations
B Tech/M Tech M Sc (Math/ Stats) or equivalent from a premier institute
Must have 4 to 6 years of on-the-job experience of working in Data Engineer profiles","Azure Cloud technologies, Transact-SQL, SQL Agent, Management Studio, T-sql, Ssrs, Sql Server 2005, SSIS, Sql Server 2012, Sql Profiler, Azure Data Factory, Query Analyzer"
Senior Principal Data Engineer,MakeMyTrip,10-12 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Position: Senior Principal Data Engineer
Experience: Must have 10+ years of experience
About Role:
We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.
About us:
Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.
Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.
GO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.
About the team:
MakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.
Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.
There are two key responsibilities of Data Engineering team:
One to develop the platform for data capture, storage, processing, serving and querying.
Second is to develop data products starting from;
o personalization & recommendation platform
o customer segmentation & intelligence
o data insights engine for persuasions and
o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users
We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.
Technology experience:
Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture
Extensive experience in data modeling and database design
At least 6+ years of hands-on experience in Spark/BigData Tech stack
Stream processing engines Spark Structured Streaming/Flink
Analytical processing on Big Data using Spark
At least 6+ years of experience in Scala
Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems
At least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform
At least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc
Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies
Should have experience in architecting and implementing domain centric big data solutions
Ability to frame architectural decisions and provide technology leadership & direction
Excellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Senior Principal Data Engineer,MakeMyTrip,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Senior Principal Data Engineer
Experience: Must have 10+ years of experience
About Role:
We are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.
About us:
Nurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.
Post-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.
GO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.
About the team:
MakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.
Data Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.
There are two key responsibilities of Data Engineering team:
One to develop the platform for data capture, storage, processing, serving and querying.
Second is to develop data products starting from;
o personalization & recommendation platform
o customer segmentation & intelligence
o data insights engine for persuasions and
o the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users
We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.
Technology experience:
Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture
Extensive experience in data modeling and database design
At least 6+ years of hands-on experience in Spark/BigData Tech stack
Stream processing engines Spark Structured Streaming/Flink
Analytical processing on Big Data using Spark
At least 6+ years of experience in Scala
Hands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems
At least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform
At least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc
Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies
Should have experience in architecting and implementing domain centric big data solutions
Ability to frame architectural decisions and provide technology leadership & direction
Excellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Big Data Engineer-4+ Years,Cortex Consultants LLC,4-7 Years,,"Chennai, India",Login to check your skill match score,"Total IT / development experience of 3+ years
Experience in Spark (Scala-Spark or PySpark) developing Big Data applications on Hadoop, Hive and/or Kafka, HBase, MongoDB+B4+A3:B5d technology strategies
Exposure to deploying on Cloud platforms
At least 2 years of development experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Spark-Scala/PySpark
At least 2 years of development experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS
At least 2 years of developing applications in Agile with Monitoring, Build Tools, Version Control, Unit Test, Unix Shell Scripting, TDD, CI/CD, Change Management to support DevOps
GOOD-TO-HAVE
Banking domain knowledge
Hands-on experience in SAS toolset / statistical modelling migrating to Machine Learning models
Banking Risk, Fraud or Digital Marketing Machine Learning models and use cases
ETL / Data Warehousing, SQL and Data Modelling experience prior to Big Data experience Location Chennai / Bangalore / Pune Experience 4-7 Yrs
Skills: pyspark,ci/cd,machine learning,mongodb,data pipelines,unit test,data,unix shell scripting,hadoop,hive,kafka,learning,agile,spark,management,scala-spark,change management,machine learning models,a3,hbase,tdd,scala,big data","data pipelines, scala-spark, Management, ci cd, machine learning models, Hive, Machine Learning, Scala, Pyspark, change management, Unix Shell Scripting, Unit Test, Tdd, Spark, Kafka, hadoop, agile, mongodb, hbase, Big Data"
Data Engineer - Looker,"iitjobs, Inc.",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"We have an great opportunity for the role of Data Engineer- Looker.
Mandatory Skills : Looker Action, Looker Dashboarding, Looker Data Entry, LookML, SQL Queries.
Relevant Exp : 6+ Yrs
Job Summary:
We are seeking a skilled Data Engineer with deep expertise in Looker, including Looker Actions, Dashboarding, Data Entry, LookML, and SQL Queries. You will play a key role in designing, implementing, and optimizing Looker-based solutions that enable data visualization, accessibility, and actionable insights.
Key Responsibilities:
Design and develop Looker dashboards to provide actionable insights and data visualization for stakeholders.
Implement Looker Actions for seamless integration with workflows and business processes.
Manage and maintain data entry pipelines within Looker to ensure data accuracy and completeness.
Develop and maintain LookML models for efficient data analysis and exploration.
Write and optimize SQL queries to extract, transform, and load data into Looker.
Collaborate with data analysts and business teams to ensure solutions meet user needs.
Monitor Looker performance and troubleshoot any issues to ensure reliability.
Document workflows, LookML models, and dashboarding best practices.
Stay current with Looker updates and industry trends to implement new features effectively.
-Immediate Joiners to 15 days Preferred
-Job location- Remote
Thanks and Regards,
iitjobs, Inc.
Register for a global opportunity on the world's first & only Global Technology Job Portal: www.iitjobs.com
Download our app on the Apple App Store and Google Play Store!
Refer and earn 50,000!","Looker Data Entry, Looker Dashboarding, Looker Actions, Looker, LookML, Sql Queries"
Senior Data Engineer - Fabric,Anblicks,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Details
Job Summary:
We are seeking an experienced Senior Data Engineer with strong expertise in Microsoft Fabric to support our enterprise data modernization and analytics transformation initiatives. The ideal candidate will have a deep understanding of data pipelines, lakehouse architecture, Power BI, Synapse integration, and experience in modernizing legacy data systems to cloud-native solutions. This role is critical in building scalable, secure, and high-performing data solutions on the Microsoft ecosystem.
Key Responsibilities:
Design and implement data pipelines using Microsoft Fabric s Data Factory, Synapse Data Engineering, and OneLake components.
Build and maintain lakehouse architectures leveraging Delta Lake, Parquet, and OneLake within Microsoft Fabric.
Lead initiatives to modernize legacy ETL/ELT processes to cloud-native data pipelines.
Work closely with Data Architects, BI Developers, and Analysts to deliver scalable data models for analytics and reporting.
Optimize performance of Power BI datasets and reports through best practices in data modeling and DAX.
Implement data governance and security controls, including Microsoft Purview, role-based access, and lineage tracking.
Collaborate with cross-functional teams in cloud migration, especially from on-premises SQL/Oracle/Hadoop platforms to Microsoft Azure & Fabric.
Evaluate and implement CI/CD practices for data pipelines using Azure DevOps or GitHub Actions.
Required Skills & Qualifications:
Bachelor s/Master s degree in Computer Science, Information Systems, or related field.
8+ years of experience in data engineering
Strong hands-on experience with Microsoft Fabric components:
Data Factory
Lakehouse / OneLake
Synapse Data Engineering
Power BI
Experience with data modeling (star/snowflake) and performance tuning in Power BI.
Deep understanding of modern data architecture patterns including lakehouse, medallion architecture, and ELT frameworks.
Expertise in SQL, PySpark, T-SQL, DAX, and Power Query (M language).
Experience modernizing platforms from SSIS, Informatica, or Hadoop to cloud-native tools.
Familiarity with Azure ecosystem Azure Data Lake, Azure SQL DB, Azure Functions, Azure Synapse, Azure Data Factory.
Strong experience in CI/CD pipelines, preferably with Azure DevOps.
Familiarity with data security, GDPR, HIPAA, and enterprise data governance.
Preferred Qualifications:
Microsoft certifications such as:
Microsoft Certified: Fabric Analytics Engineer Associate
Azure Data Engineer Associate (DP-203)
Experience with DataOps and Agile delivery methods.
Knowledge of Machine Learning/AI integration with Fabric is a plus.
Hands-on with Notebooks in Microsoft Fabric using Python or Scala.
Soft Skills:
Strong analytical and problem-solving skills.
Excellent communication and stakeholder management capabilities.
Ability to lead projects, mentor junior engineers, and collaborate with cross-functional teams.","GitHub Actions, Lakehouse, Microsoft Fabric, OneLake, CI CD, Azure SQL DB, Synapse, Power Query M language, Sql, Data Factory, T-sql, Azure Data Factory, Pyspark, Power Bi, Azure Functions, Dax, Azure Synapse, Azure Data Lake, Azure DevOps"
Lead Data Engineer,M&G Global Services Private Limited,12-14 Years,,"Mumbai, India",Login to check your skill match score,"We are M&G Global Services Private Limited (formerly known as 10FA India Private Limited, and prior to that Prudential Global Services Private Limited). We are a fully owned subsidiary of the M&G plc group of companies, operating as a Global Capability Centre providing a range of value adding services to the Group since 2003. At M&G our purpose is to give everyone real confidence to put their money to work. As an international savings and investments business with roots stretching back more than 170 years, we offer a range of financial products and services through Asset Management, Life and Wealth. All three operating segments work together to deliver attractive financial outcomes for our clients, and superior shareholder returns.
M&G Global Services has rapidly transformed itself into a powerhouse of capability that is playing an important role in M&G plc's ambition to be the best loved and most successful savings and investments company in the world.
Our diversified service offerings extending from Digital Services (Digital Engineering, AI, Advanced Analytics, RPA, and BI & Insights), Business Transformation, Management Consulting & Strategy, Finance, Actuarial, Quants, Research, Information Technology, Customer Service, Risk & Compliance and Audit provide our people with exciting career growth opportunities. Through our behaviours of telling it like it is, owning it now, and moving it forward together with care and integrity; we are creating an exceptional place to work for exceptional talent.
Job Description
Job Title
Lead Data Engineer
Grade
2B
Level
Senior Manager Data
Job Function
Digital Transformation
Job Sub Function
Azure Data Engineering & DevOps & BI
Reports to
3B (VP Data Engineering)
Location
Mumbai
Business Area
M&G Global Services
Overall Job Purpose
To implement data engineering solutions using latest technologies available in Azure Cloud space conforming to the best in class design standard & agreed requirements to achieve business objective
Accountabilities/Responsibilities
Lead data engineering projects to build and operationalize data solutions for business using Azure services in combination with custom solutions Azure Data Factory, Azure Data Flows, Azure Databricks, Azure Data Lake Gen 2, Azure SQL etc
Proven experience on leading a team of data engineers providing technical guidance and ensuring alignment with agreed architectural principles
Experience in migrating on-premise data warehouses to data platforms on AZURE cloud
Designing and implementing data engineering, ingestion and transformation functions using ADF, Databricks
Proficient in Py-Spark
Experience in building Python based APIs on Azure Function Apps
Experience on Azure Logic apps
Experience in Lakehouse/Datawarehouse implementation using modern data platform architecture
Capacity Planning and Performance Tuning on ADF & Databricks pipelines
Support data visualization development using Power BI
Exposure across all the SDLC process, including testing and deployment
Experience in relational and dimensional modelling, including big data technologies
Experience in Azure DevOps Build CI/CD pipelines for ADF, ADLS, Databricks, Azure SQL DB etc
Experience of working in secured Azure environments using Azure KeyVaults, Service Principals, and Managed Identities
Good to have knowledge on Apigee (Googles API Management)
Understanding of data masking, encryption and other practices used in handling sensitive data
Ability to interact with Business for requirement gathering and query resolutions
Working on off shore office based development teams, collaborating within a team environment and participating in typical project lifecycle activities such as requirement analysis, testing and release
Develop Azure Data skills within the team through knowledge sharing sessions, articles, etc.
Adherence to organisations Risk & Controls requirements
Should have skills for Stakeholder management, process adherence, planning & documentationss
Key Stakeholder Management
Internal
Business Teams
Project Manager
Architects
Data Scientists
Team members
External
Knowledge, Skills, Experience & Educational Qualification
Knowledge & Skills:
Azure Data Factory,
Azure Data Lake Storage V2
Azure SQL
Azure DataBricks
Pyspark
Azure DevOps
Power BI Report
Technical leadership
Confidence & excellent communication
Experience:
Overall 12+ years of experience
5 + Experience on Azure data engineering
5 + experience of managing data deliveries
Educational Qualification:
Graduate/Post-graduate. Preferably with specialisation in Computer Science, Statistics, Mathematics, Data Science, Engineering or related discipline
Microsoft Azure certification (good to have)
M&G Behaviours relevant to all roles:
Inspire Others: support and encourage each other, creating an environment where everyone can contribute and succeed
Embrace Change: be open to change, willing to be challenged and able to adapt quickly and imaginatively to new ideas
Deliver Results: focus on performance, set high standards and deliver with energy and determination
Keep it simple: cut through complexity, keep the outcome in mind, keeping your approach simple and adapting your message to every audience
We have a diverse workforce and an inclusive culture at M&G Global Services, regardless of gender, ethnicity, age, sexual orientation, nationality, disability or long term condition, we are looking to attract, promote and retain exceptional people. We also welcome those who take part in military service and those returning from career breaks.","Azure Data Lake Storage V2, Azure Function Apps, Power Bi, Azure Sql, Azure Data Factory, Azure Databricks, Azure Logic Apps, Pyspark, Azure DevOps"
Data Engineer(AWS),goML,3-7 Years,,"Delhi, India",Login to check your skill match score,"About the Role
We're looking for a skilled Data Engineer (AWS) with a strong foundation in ETL, data warehousing, and integration to join our team at goML. In this remote role, you'll be responsible for delivering robust and scalable data solutions across multiple client projects.
Key Responsibilities
Lead the design and development of data solutions for various client engagements.
Apply development best practices (e.g., agile methodologies, unit testing, peer reviews) to ensure quality and timely delivery.
Collaborate across teamsfrom requirement gathering through deploymentto ensure smooth execution and handoff.
Translate business requirements into technical specifications and guide development teams for accurate implementation.
Create comprehensive project artifacts including solution designs, technical documentation, test plans, and deployment strategies.
Drive user onboarding and change management efforts during project rollouts.
Requirements
37 years of relevant experience, preferably in consulting or large-scale tech solution delivery.
Strong understanding of ETL processes, data warehousing, and data integration.
Proficient in SQL/PL SQL and database development.
Experience designing scalable and flexible data solutions based on business needs.
Bachelor's or master's in computer science, Information Systems, or related fields.
Hands-on experience with backend database systems (e.g., Oracle) and/or ETL tools (e.g., Informatica).
Hands on Experience in AWS.","ETL processes, backend database systems, Data Warehousing, Pl Sql, Etl Tools, Informatica, Data Integration, Oracle, Sql, Database Development, AWS"
"Manager 1, Domo Data Engineer",Kenvue,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
Manager Domo Data Engineer
Who We Are
At Kenvue , we realize the extraordinary power of everyday care. Built on over a century of heritage and rooted in science, we're the house of iconic brands - including Neutrogena, Aveeno, Tylenol, Listerine, Johnson's and BAND-AID Brand Adhesive Bandages that you already know and love. Science is our passion; care is our talent. Our global team is made up of 22,000 diverse and brilliant people, passionate about insights, innovation and committed to delivering the best products to our customers. With expertise and empathy, being a Kenvuer means having the power to impact the life of millions of people every day. We put people first, care fiercely, earn trust with science and solve with courage and have brilliant opportunities waiting for you! Join us in shaping our futureand yours. For more information, click here .
What You Will Do
As a Data Engineer in the Global Analytics team, you will play a crucial role in designing, building, and maintaining scalable data pipelines and data models. You will work with cutting-edge technologies such as Snowflake, Python, Databricks and Azure Services to enable data-driven insights and support various analytics initiatives across the organization.
Key Responsibilities
Data Pipeline Development- Design, develop, and maintain robust data pipelines to ingest, transform, and store data from diverse sources into Snowflake and DOMO ETL systems. other data storage solutions.
Strong understanding and experience (5+ years) of DOMO ETL and best practice. Experience of designing and implementing data marts, data lakes or data warehouses using Domo
Data Modeling- Collaborate with data analysts and business stakeholders to create and optimize data models that meet analytical requirements and support reporting needs.
ETL Processes- Implement ETL processes using DOMO and other tools to ensure data integrity, quality, and availability for analytics and reporting.
Collaboration- Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver relevant datasets for analysis.
Proactive approach to solution designing becoming a trusted advisor within the team.
Performance Optimization- Monitor and optimize the performance of data pipelines and data models, ensuring efficient processing and quick turnaround times.
Documentation- Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to support knowledge sharing and future development.
Cloud Infrastructure- Utilize Azure services to deploy and manage data solutions, ensuring security, scalability, and reliability.
Troubleshooting- Identify and resolve data-related issues, ensuring data quality and accuracy across all analytics initiatives.
What We Are Looking For
Required Qualifications-
Must have-
Bachelor's degree in computer science, Information Technology, Data Science, or a related field.
Proven experience (8+ years) as a Data Engineer or in a similar role, with a strong understanding of data engineering concepts and best practices with at least 2+ in DOMO data engineering
Proficiency in Python for data manipulation, pipeline development, and automation.
Experience with Snowflake, including data warehousing concepts and SQL for querying and managing data.
Familiarity with Databricks for data processing and analytics workflows.
Knowledge of Azure cloud services and architecture, particularly in relation to data storage and processing.
Experience with DBT (Data Build Tool) for transforming data and managing data models.
Strong analytical and problem-solving skills, with attention to detail and a commitment to data quality.
Excellent communication skills to collaborate effectively with technical and non-technical stakeholders.
Experience with data visualization tools (e.g., Tableau, Power BI) for creating reports and dashboards.
Familiarity with orchestration tools.
Knowledge of machine learning concepts and techniques.
Primary Location
Asia Pacific-India-Karnataka-Bangalore
Job Function
Digital Product Development
Job Qualifications
Nice to have-","snowflake, Azure Services, DBT Data Build Tool, Databricks, Sql, Power Bi, Python, Tableau, Domo"
Senior Data Engineer Analyst,IMRIEL Technology Solutions Private Ltd,3-5 Years,,"Pune, India",Login to check your skill match score,"We are seeking a skilled Data Engineer with expertise in maintaining scalable semantic models using AtScale and cloud-based data warehouse platforms.
We at IMRIEL (An Allata Company) are looking for experienced and technically strong Analytics Data Engineers to design, build, and maintain scalable semantic models using AtScale and cloud-based data warehouse platforms. This role involves developing logical cubes, defining MDX-based business measures, and enabling governed, self-service BI consumption for enterprise analytics.
Experience:3 to 5 years.
Location:Vadodara & Pune
What you'll be doing:
Design and implement robust semantic data models using AtScale that abstract curated datasets into business-consumable layers.
Conduct a comprehensive POC to evaluate three potential semantic layer platforms: AtScale, Microsoft Fabric and Cube.dev. This includes assessing their performance, scalability, and integration with cloud-based platforms like Databricks, Snowflake, etc.
Develop and maintain logical cubes with calculated measures, dimension hierarchies, and drill-down paths to support self-service analytics.
Leverage MDX (Multidimensional Expressions) to define advanced business logic, KPIs, and aggregations aligned with enterprise reporting needs.
Configure and manage aggregate tables using AtScale Aggregate Designer, optimizing cube performance and reducing query latency.
Integrate semantic models with BI tools such as Power BI, Tableau, and Excel Pivot Tables, ensuring seamless end-user experiences.
Collaborate with data engineers to align semantic models with curated data sources, transformation views, and data pipelines.
Apply star and snowflake schema design to model fact and dimension tables, ensuring optimal structure for analytical workloads.
Implement Slowly Changing Dimensions (SCD Types 1 & 2) and maintain historical accuracy in reporting models.
Manage row-level security (RLS) and role-based access control (RBAC) policies within semantic layers for governed data access.
Participate in semantic model versioning, CI/CD-based deployments, and technical documentation.
Troubleshoot semantic layer performance issues using AtScale query logs, plan analysis, and catching strategies.
What you need:
Basic Skills:
Minimum 3 years of hands-on experience with AtScale, including building and maintaining semantic models, designing logical cubes, and implementing calculated measures using MDX. Proficiency in AtScale interface, modeling best practices, and performance tuning is essential.
Advanced experience in developing and optimizing DAX expressions for complex calculations in Power BI models, with a proven ability to translate these into new semantic layer technologies like AtScale or Cube.dev.
Strong experience with MDX, including creating calculated members, KPIs, and advanced expressions. Excellent SQL skills with the ability to write complex queries using joins, CTEs, window functions, and performance tuning.
Solid understanding of dimensional modeling. Ability to design fact/dimension tables using star/snowflake schemas, support SCD logic, and maintain model consistency.
Should be familiar with the Kimball methodology for dimensional modeling, including concepts like conformed dimensions, fact table granularity, and slowly changing dimensions, to design scalable and analytics-friendly data structures.
Hands-on experience with Snowflake, Redshift, or BigQuery. Familiarity with virtual warehouses, caching, clustering, partitioning, and compute-storage separation.
Experience implementing RLS and RBAC. Ability to define and enforce granular access controls within semantic models.
Strong grasp of OLAP concepts like query abstraction, drill-down/roll-up, and cube optimization. Understanding of business logic abstraction from physical data.
Skilled in using AtScale performance tools such as the Aggregate Designer, log analysis, and query optimization.
Proficient in managing model development lifecycle using Git, automation tools, and collaboration workflows with data/analytics teams.
Strong verbal and written communication to document models, explain logic, and coordinate with cross-functional teams.
Responsibilities:
Own the design, development, deployment, and maintenance of scalable, governed semantic models.
Implement complex MDX logic and optimized aggregate strategies to meet performance benchmarks.
Proven ability to design and implement scalable AtScale architectures, including the development of architectural blueprints and data flow diagrams.
Evaluate and implement the best semantic layer architecture for Power BI by leveraging tools like Microsoft Fabric or other modern BI accelerators to support self-service analytics.
Define business measures, hierarchies, and drill-down paths in semantic models aligned with enterprise KPIs.
Align semantic layers with upstream data transformations, curated datasets, and data warehouse architecture.
Enforce governance and security through robust RLS and RBAC implementations.
Continuously monitor, test, and tune semantic model performance using diagnostic tools and AtScale logging.
Ensure semantic layer reusability, consistency, and business-aligned metric standardization.
Collaborate with BI developers and analysts to understand reporting needs and validate model outputs.
Maintain documentation, data lineage, and business glossaries that support transparency and user adoption.
Contribute to reusable templates, modeling standards, and automation frameworks.
Nice-to-Have to have:
Experience with AtScale REST APIs for metadata-driven automation and CI/CD pipelines.
Familiarity with BI visualization platforms such as Power BI, Tableau, Looker, and Excel OLAP integration.
Scripting experience in Python, Shell, or YAML for configuration management or automation tasks.
Cloud certifications in Snowflake, Databricks, AWS, Azure, or Google Cloud Platform.
Exposure to metadata management, data cataloging, or enterprise data governance tools.
Personal Attributes:
High attention to detail with a focus on producing scalable, accurate, and governed semantic solutions.
Strong interpersonal and communication skills to collaborate effectively with technical and non-technical stakeholders.
Self-motivated and accountable, with the ability to take full ownership of deliverables.
Adaptability to evolving tools, data technologies, and enterprise analytics strategies.","MDX, kimball methodology, snowflake, AtScale, BigQuery, Power Bi, OLAP, Tableau, Redshift, Sql, Git, Excel, Dax"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position
This role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.
Key Accountabilities
Assist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.
Work closely with senior engineers and analysts to gather requirements and build working prototypes.
Support data integration from internal, third-party, and public sources.
Participate in developing and maintaining Data Warehouse schemas.
Contribute to documentation and testing efforts to ensure data reliability.
Learn and apply data standards and governance practices as guided by the team.
Skills and Experience | Essential
Knowledge of data engineering concepts and data structures.
Exposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.
Understanding of ETL processes and data pipelines.
Ability to work collaboratively in an Agile/Kanban team environment.
Microsoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage
Skills and Experience | Desirable
Familiarity with Medallion Architecture principles.
Exposure to MS Purview or other data governance tools.
Understanding of data warehousing and reporting concepts.
Interest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
Senior Data Engineer,Commonwealth Bank,10-13 Years,,"Bengaluru, India",Login to check your skill match score,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.
Job Title: Sr Data Engineering
Location: Bangalore
Business & Team:
Technology Team is responsible for the world leading application of technology and operations across every aspect of CommBank, from innovative product platforms for our customers to essential tools within our business. We also use technology to drive efficient and timely processing, an essential component of great customer service.
CommBank is recognised as leading the industry in IT and operations with its world-class platforms and processes, agile IT infrastructure, and innovation in everything from payments to internet banking and mobile apps.
The Group Security (GS) team protects the Bank and our customers from cyber compromise, through proactive management of cyber security, privacy, and operational risk. Our team includes:
Cyber Strategy & Performance
Cyber Security Centre
Cyber Protection & Design
Cyber Delivery
Cyber Data Engineering
Cyber Data Security
Identity & Access Technology
The Group Security Senior Data Engineering team provides specialised data services and platforms for the CommBank group & is accountable for developing Group's data strategy, data policy & standards, governance and set requirements for data enablers/tools. The team is also accountable to facilitate a community of practitioners to share best practice and build data talent and capabilities.
Impact & contribution :-
To ensure the Group achieves a sustainable competitive advantage through data engineering, you will play a key role in supporting and executing the Group's data strategy.
We are looking for an experienced Data Engineer to join our Group Security Team, which is part of the wider Cyber Security Engineering practice. In this role, you will be responsible for setting up the Group Security Data Platform to ingest data from various organizations security telemetry data, along with additional data assets and data products. This platform will provide security controls and services leveraged across the Group.
Roles & Responsibilities
You will be expected to perform the following tasks in a manner consistent with CBA's Values and People Capabilities.
CORE RESPONSIBILITIES:
Possesses hands-on technical experience working in AWS. The individual should have knowledge about AWS services like EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, KMS, Step functions, SQS,SNS, Cloud Watch.
The individual should possess a robust set of technical and soft skills and be an excellent AWS Data Engineer with a focus on complex Automation and Engineering Framework development.
Being well-versed in Python is mandatory, and experience in developing complex frameworks using Python is required.
Passionate about Cloud/DevSecOps/Automation and possess a keen interest in solving complex problems systematically.
Drive the development and implementation of scalable data solutions and data pipelines using various AWS services.
Possess the ability to work independently and collaborate closely with team members and technology leads.
Exhibit a proactive approach, constantly seeking innovative solutions to complex technical challenges.
Can take responsibility for nominated technical assets related to areas of expertise, including roadmaps and technical direction.
Can own and develop technical strategy, overseeing medium to complex engineering initiatives.
Essential Skills:-
About 10-13 years of experience as a Data Engineering professional in a data-intensive environment.
The individual should have strong analytical and reasoning skills in the relevant area.
Proficiency in AWS cloud services, specifically EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, Step functions, SQS,SNS, Cloud Watch.
Excellent skills in Python-based framework development are mandatory.
Proficiency in SQL for efficient querying, managing databases, handling complex queries, and optimizing query performance.
Excellent automation skills are expected in areas such as
Automating the testing framework using tools such as PyPy, Pytest, and various test cases including unit, integration, functional tests, and mockups.
Automating the data pipeline and expediting tasks such as data ingestion and transformation.
API-based automated and integrated calls(REST, cURL, authentication & authorization, tokens, pagination, openApi, Swagger)
Implementing advanced engineering techniques and handling ad hoc requests to automate processes on demand.
Implementing automated and secured file transfer protocols like XCOM, FTP, SFTP, and HTTP/S
Experience with Terraform, Jenkins, Teracity and Artifactory is essential as part of DevOps. Additionally, Docker and Kubernetes are also considered.
Proficiency in building orchestration workflows using Apache Airflow.
Strong understanding of streaming data processing concepts, including event-driven architectures.
Familiarity with CI/CD pipeline development, such as Jenkins.
Extensive experience and understanding in Data Modelling, SCD Types, Data Warehousing, and ETL processes.
Excellent experience with GitHub or any preferred version control systems.
Expertise in data pipeline development using various data formats/types.
Mandatory knowledge and experience in big data processing using PySpark/Spark and performance optimizations of applications
Proficiency in handling various file formats (CSV, JSON, XML, Parquet, Avro, and ORC) and automating processes in the big data environment.
Ability to use Linux/Unix environments for development and testing.
Should be aware of security best practices to protect data and infrastructure, including encryption, tokenization, masking, firewalls, and security zones.
Well-structured documentation skills and the ability to create a well-defined knowledge base.
Certifications such as AWS Certified Data Analytics/Engineer/Developer Specialty or AWS Certified Solutions Architect.
Should be able to perform extreme engineering and design a robust, efficient, and cost-effective data engineering pipelines which are highly available and dynamically scalable on demand.
Enable the systems to effectively respond to high demands and heavy loads maintaining the high throughput and high I/O performance with no data loss
Own and lead E2E Data engineering life cycle right from Requirement gathering, design, develop, test, deliver and support as part of DevSecOPS process.
Must demonstrate skills and mindset to implement encryption methodologies like SSL/TLS and data encryption at rest and in transit and other data security best practices
Hands on work experience with data design tools like Erwin and demonstrate the capabilities of building data models, data warehouse, data lakes, data assets and data products
Must be able to constructively challenge the status quo and lead to establish data governance, metadata management, ask the right questions, design with right principles
Education Qualification :-
A Bachelor's or Master's degree in Engineering, specializing in Computer Science, Information Technology or relevant qualifications.
If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through Sidekick to submit a valid application. We're keen to support you with the next step in your career.
We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.
Advertising End Date: 23/05/2025","Big Data Processing, Data Pipeline Development, Github, Data Modelling, Pyspark, Automation, Sql, Apache Airflow, Jenkins, Terraform, Docker, Data Warehousing, Kubernetes, Python, Etl, AWS"
Senior Data Engineer (SQL and ADF),thinkbridge,5-7 Years,,India,Login to check your skill match score,"We are hiring a Senior Data Engineer with 5+ years of experience in SQL , who has good hands-on in ADF and ETL. Good communication is mandatory.
About thinkbridge
thinkbridge is how growth-stage companies can finally turn into tech disruptors. They get a new way there with world-class technology strategy, development, maintenance, and data science all in one place. But solving technology problems like these involves a lot more than code. That's why we encourage thinkers to spend 80% of their time thinking through solutions and 20% coding them. With an average client tenure of 4+ years, you won't be hopping from project to project here unless you want to. So, you really can get to know your clients and understand their challenges on a deeper level. At thinkbridge, you can expand your knowledge during work hours specifically reserved for learning. Or even transition to a completely different role in the organization. It's all about challenging yourself while you challenge small thinking.
thinkbridge is a place where you can:
Think bigger because you have the time, opportunity, and support it takes to dig deeper and tackle larger issues.
Move faster because you'll be working with experienced, helpful teams who can guide you through challenges, quickly resolve issues, and show you new ways to get things done.
Go further because you have the opportunity to grow professionally, add new skills, and take on new responsibilities in an organization that takes a long-term view of every relationship.
thinkbridge.. there's a new way there.
What is expected of you
As part of the job, you will be required to
Read everything in detail that comes your way.
Elicit, analyze, specify & validate business requirements.
Define & Document the engineering requirements including architecture, components, usable interfaces & other characteristics.
Place specific emphasis on technical and usability design.
Code and verify the solution.
Debug & squash bugs.
Keep stakeholders informed.
Keep up with the technology and the domain.
If your beliefs resonate with these, you are looking at the right place!
Accountability Finish what you started.
Communication Context-aware, pro-active and clean communication.
Outcome High throughput.
Quality High-quality work and consistency.
Ownership Go beyond.
Requirements
Must-Have:
Should have hands-on experience in writing SQL Queries and Stored Procedures.
Excellent Communication.
Should have experience in ADF (Azure Data Factory)
Good experience in building ETL Solutions for large datasets
Good to have:
Good to have SSIS Experience
Other Details :
Remote First
Flexible work hours
No loss of pay for pre-approved leaves
Family Insurance
Quarterly in-person Collaboration Week -WWW","Adf, SSIS, Sql, Etl"
ETL Data Engineer,QTek Digital,6-9 Years,,"Hyderabad, India",Login to check your skill match score,"Job description
Company Description
QTek Digital is a data solutions provider specializing in custom solutions in data management, data warehouse, and data science. Our team of data scientists, data analysts, and data engineers work together to solve today's challenges and unlock future possibilities. As an employee-centric company, we prioritize engagement, empowerment, and enrichment of our employees.
Role Description
This is a full-time remote role for a BI ETL Engineer at QTek Digital. As a BI ETL Engineer, you will be responsible for day-to-day tasks such as data modeling, utilizing analytical skills, implementing data warehousing solutions, and executing Extract, Transform, Load (ETL) processes. This role requires strong problem-solving abilities and the ability to work independently.
Qualifications
6-9 years of experience with ETL and ELT pipeline build out using Pentaho, SSIS, FiveTran, Airbyte, or similar tools.
6-8 Years of experience in SQL and data manipulation languages
Strong Data Modeling, Dashboard, and Analytical Skills
Excellent understanding of data warehousing concepts, esp. Kimball design.
Experience with Pentaho and Airbyte administration will be a huge plus.
Strong skills in Data Modeling, Dashboard design, and Analytics
Experience in Data Warehousing and Extract Transform Load (ETL) processes
Strong problem-solving and troubleshooting skills
Excellent communication and collaboration skills
Ability to work independently and in a team
Bachelor's degree in computer science, Information Systems, or a related field
This role is based onsite in our Hyderabad Office.
The compensation range for this role is INR 5-19 Lakhs depending on a variety of factors including skills and experience.","Airbyte, FiveTran, Pentaho, Data Modeling, Data Warehousing, SSIS, Sql, ELT, Etl"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description
Oncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.
Job Type:Full-time or Contract
Start Date: June 1st 2025
Role Description
This is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.
Job Description:
Candidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively
6+ Years of experience as a Data Engineer
Strong technical expertise in SQL and Snowflake is a must.
Strong knowledge of joins and common table expressions (CTEs)
Strong experience with Python
Strong expertise in ETL process and with various data model concepts
Knowledge of star schema and snowflake schema
Good to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue
Experience with Big Data Tools and technologies
Key Skills:
Good Understanding of data structures and data analysis using SQL
Knowledge of implementing ETL/ELT for data solutions end-to-end
Understanding requirements, and data solutions (ingest, storage, integration, processing)
Knowledge of analyzing data using SQL
Conducting End to End verification and validation for the entire application
Responsibilities:
Understand and translate business needs into data models supporting long-term solutions.
Perform reverse engineering of physical data models from databases and SQL scripts.
Analyze data-related system integration challenges and propose appropriate solutions.
Assist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
Big Data Engineer (Freelancer),Soul AI,4-6 Years,,India,Login to check your skill match score,"About Us:
Soul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact.
We are looking for an experienced Big Data Engineer with at least 4 years of experience to design, develop, and manage large-scale data processing systems.
Key Responsibilities:
Build and maintain data pipelines for large datasets.
Design systems for real-time data processing.
Collaborate with data scientists and engineers to optimize data workflows.
Required Qualifications:
4+ years of experience as a Big Data Engineer.
Strong proficiency with big data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.
Experience with cloud platforms like AWS, GCP, or Azure.
Why Join Us
Competitive pay (1200/hour).
Flexible hours.
Remote opportunity.
NOTE: Pay will vary by project and typically is up to Rs. 1200 per hour (if you work an average of 3 hours every day - that could be as high as Rs. 108K per month) once you clear our screening process.
Shape the future of AI with Soul AI!","NoSQL databases, Gcp, Hadoop, Spark, Kafka, Azure, AWS"
Lead Data Engineer - Future Detections,JPMorganChase,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Be an integral part of an agile team that's constantly pushing the envelope to enhance, build, and deliver top-notch technology products.
As a Lead Data Engineer at JPMorgan Chase within the Cybersecurity & Tech Controls , you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. Drive significant business impact through your capabilities and contributions, and apply deep technical expertise and problem-solving methodologies to tackle a diverse array of challenges that span multiple technologies and applications.
Job Responsibilities
Regularly provides technical guidance and direction to support the business and its technical teams, contractors, and vendors
Develops secure and high-quality production code, and reviews and debugs code written by others
Drives decisions that influence the product design, application functionality, and technical operations and processes
Serves as a function-wide subject matter expert in one or more areas of focus
Actively contributes to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle
Influences peers and project decision-makers to consider the use and application of leading-edge technologies
Adds to the team culture of diversity, equity, inclusion, and respect
Required Qualifications, Capabilities, And Skills
Formal training or certification on Data engineering concepts and 5+ years applied experience
Hands-on practical experience Big Data , large volume data transfer analysis.
Experience in spark/ iceberg / parquets.
Experience with cloud platforms like AWS and container technologies such as Kubernetes and Docker
Knowledge of software applications and technical processes with considerable in-depth knowledge in one or more technical disciplines (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)
Experience to tackle design and functionality problems independently with little to no oversight
Preferred Qualifications, Capabilities, And Skills
Practical cloud native experience
Computer Science, Computer Engineering, Mathematics, or a related technical field
ABOUT US
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Mobile, parquets, Data engineering concepts, iceberg, Machine Learning, Big Data, Artificial Intelligence, cloud, Docker, Spark, Kubernetes, AWS"
"Data Engineer with Databricks, Azure and Power BI (DAX) Skills",Sony India Software Centre,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Description
Total relevant experience in Data Engineering should be 6-8 yrs.
Must have good knowledge on Azure Databricks, Azure Datalake and Power BI including DAX skills.
Should have good Power BI Experience which includes -
Strong Data modelling skills.
Expert in Tabular model design.
Expert in writing complex DAX formulas.
Hands on experience in DAX optimization
Strong knowledge on the DWBI with SQL.
Experience of working with On-premise/cloud BI solution implementation.
Good knowledge in Python.
Good to have these skills in Databricks -
Understanding of AI capabilities.
Unity Catalog - Implementation
AI/BI Genie
DevOps
Power Automate (Good to have)
Department
Regional Apps - APCNJP - AP Platforms - G3A
Open Positions
1
Skills Required
Data Bricks, power bi, Azure Data Lake, SQL Development, Python
Role
Design, develop, and maintain data pipelines and ETL processes using Azure Databricks
Collaborate with business analysts to understand and meet business requirements for data and analytics
Develop and optimize data models and visualizations using Power BI (DAX) to present meaningful insights to stakeholders
Implement best practices for data governance, security, and compliance in Azure environments
Monitor and troubleshoot data pipelines, ensuring data integrity and reliability
Stay abreast of the latest trends and technologies in data engineering and analytics
Provide technical guidance and mentorship to junior members of the team
Location
Bengaluru
Education/Qualification
B Tech
Desirable Skills
Azure Databricks, Azure Datalake, Power BI with Dax, SQL, Python
Years Of Exp
6 to 8 years
Designation
Technical Specialist","Azure Datalake, Data Bricks, Power Bi, Azure Databricks, Dax, Sql, Python"
Senior Data Engineer,USEReady,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"Role: Senior Data Engineer
Experience-3-8 yrs
Location: Bangalore, Gurgaon, Mohali, Pune
About the Role:
We are seeking a skilled and proactive Data Engineer with 3-8 years of hands-on experience in Snowflake, Python, Streamlit, and SQL, along with expertise in consuming REST APIs and working with modern ETL tools likeMatillion, Fivetran etc. The ideal candidate will have a strong foundation in data modeling, data warehousing, and data profiling, and will play a key role in designing and implementing robust data solutions that drive business insights and innovation.
Key Responsibilities:
Design, develop, and maintain data pipelines and workflows using Snowflake and an ETL tool (e.g., Matillion, dbt, Fivetran, or similar).
Develop data applications and dashboards using Python and Streamlit.
Create and optimize complex SQL queries for data extraction, transformation, and loading.
Integrate REST APIs for data access and process automation.
Perform data profiling, quality checks, and troubleshooting to ensure data accuracy and integrity.
Design and implement scalable and efficient data models aligned with business requirements.
Collaborate with data analysts, data scientists, and business stakeholders to understand data needs and deliver actionable solutions.
Implement best practices in data governance, security, and compliance.
Required Skills and Qualifications:
35 years of professional experience in a data engineering or development role.
Strong expertise in Snowflake, including performance tuning and warehouse optimization.
Proficient in Python, including data manipulation with libraries like Pandas.
Experience building web-based data tools using Streamlit.
Solid understanding and experience with RESTful APIs and JSON data structures.
Strong SQL skills and experience with advanced data transformation logic.
Experience with an ETL tool commonly used with Snowflake (e.g., dbt, Matillion, Fivetran, Airflow).
Hands-on experience in data modeling (dimensional and normalized), data warehousing concepts, and data profiling techniques.
Familiarity with version control (e.g., Git) and CI/CD processes is a plus.
Preferred Qualifications:
Experience working in cloud environments (AWS, Azure, or GCP).
Knowledge of data governance and cataloging tools.
Experience with agile methodologies and working in cross-functional teams.","Airflow, Matillion, CI CD, dbt, snowflake, Streamlit, Fivetran, Data Warehousing, Data Modeling, Sql, Git, Etl Tools, Rest Apis, Data Profiling, Python"
"Senior Data Engineer (ELT/ELT, Python, Snowflake, AWS)",Reap,6-8 Years,,"Mumbai, India",Login to check your skill match score,"Role Overview
As part of a rapidly growing data team, we are looking to hire Senior Data Engineers to contribute to our dynamic and innovative projects. We are looking for someone who is ready for the challenge, is solutions-oriented, and thinks out of the box. The ideal candidate should be passionate about using data to drive impact across the organization and a collaborative mindset to work effectively within cross-functional teams. If you are enthusiastic about pushing boundaries, solving complex problems, and making a significant impact, we encourage you to apply and be part of our exciting journey in shaping the future of our products.
Responsibilities
Design and development: Design, develop, and deploy scalable ETL/ELT pipelines and APIs for ingestion and transformation. Implement data modeling best practices for optimal accessibility, flexibility and query performance. Implement data governance practices, including data security, privacy, and compliance, to ensure data integrity and regulatory compliance.
Collaboration: Work closely with cross-functional teams, including product managers, designers, and other engineers, to ensure seamless product development from concept to deployment. Influence product and cross-functional teams to identify data opportunities to drive impact.
Continuous learning: Stay updated with the latest industry trends and technologies, ensuring our tech stack remains modern and competitive.
Observability and Support: Build monitor and alerts for data pipelines monitoring, identify and resolve performance issues, troubleshoot data-related problems in collaboration with other teams, and ensure data platform SLAs are met.
To Be Successful You Will Need To Have
Experience: 6+ year work experience in data engineering and cloud platforms. Previous experience in a senior or lead engineering role.
Technical proficiency: Expertise in ETL, data modeling, and cloud data warehousing. Strong programming skills in Python, SQL, AWS, Snowflake and related tech stack. Hands-on experience with big data processing and API integrations.
Problem solving: Strong analytical and problem-solving skills, with a keen attention to detail and a passion for troubleshooting and debugging
Experience in Credit Cards, Payments and AWS certification would be an advantage.
Exposure to AI, machine learning, and predictive analytics is highly desirable.
Benefits
A Global & Dynamic Team
Remote Work Friendly
After submitting your application, please check your inbox for a confirmation email. If you don't see it, kindly check your spam or junk folder and adjust your settings to ensure future communication reaches your inbox. You can follow the steps here.","API Integrations, Big Data Processing, snowflake, Cloud Data Warehousing, Data Modeling, Python, Sql, Etl, AWS"
Senior Data Engineer,WebMD,4-6 Years,,"Navi Mumbai, Mumbai, India",Login to check your skill match score,"Position Requirements:
4+ years of experience with RDBMS databases such as Oracle, MSSQL or PostgreSQL
2+ years of experience with Pentaho Data Integration or any ETL tools such as Talend, Informatica, DataStage or HOP.
Working knowledge of orchestration tools such Oozie and Airflow
Experience working in both OLAP and OLTP environments
Experience working on-prem, not just cloud environments
Experience working with teams outside of IT (i.e. Application Developers, Business Intelligence, Finance, Marketing, Sales)
Experience managing or developing in the Hadoop ecosystem is preferred
Programming background with either Python, Scala, Java or C/C++ is a plus
Experience with Spark. PySpark, SparkSQL, Spark Streaming, etc
Strong in any of the Linux distributions, RHEL, CentOS or Fedora
Experience using reporting and Data Visualization platforms (Tableau, Pentaho BI) is good to have
Web analytics or Business Intelligence a plus
Understanding of Ad stack and data (Ad Servers, DSM, Programmatic, DMP, etc)","Airflow, Pentaho BI, DMP, Ad stack, Programmatic, DSM, Business Intelligence, Java, C, Hadoop Ecosystem, Scala, Pyspark, OLAP, Tableau, Sparksql, Spark Streaming, Web Analytics, Ad Servers, Spark, Oozie, Pentaho Data Integration, Python, Oltp"
Collibra Data Engineer (8 yrs Noida),Orbion Infotech,Fresher,,"Noida, India",Login to check your skill match score,"Tips: Provide a summary of the role, what success in the position looks like, and how this role fits into the organization overall.
Responsibilities
[Be specific when describing each of the responsibilities. Use gender-neutral, inclusive language.]
Example: Determine and develop user requirements for systems in production, to ensure maximum usability
Qualifications
[Some qualifications you may want to include are Skills, Education, Experience, or Certifications.]
Example: Excellent verbal and written communication skills
Skills: etl processes,sql,data modeling,python,business intelligence,data quality,dgc,communication skills,data governance,collibra","dgc, etl processes, business intelligence, Data Quality, Collibra, python, Data Governance, Data Modeling, Sql"
Azure Data Engineer,Dexian,6-8 Years,,"Chennai, India",Login to check your skill match score,"Job Description:
Excellent understanding on data architecture system (source, target, transformations, processing, etc.,) and migration b/w DB platforms
Hands-On Experience of 6+ years on Azure data analytics and Datawarehouse in Azure.
Must have hands-on experience in the Azure services like Azure Data Explorer, Azure Databricks, Azure Data factory, Azure Synapse Analytics and Azure Fabric etc
Must have strong hands-on experience with Python
Strong hands-onexperience in creating data pipeline monitors in the Azure environment.
Tools & Technology Experience preferred:
Object-oriented /object function scripting languages: Python
Data migration from on premise systems - RDBMS to Cloud Datawarehouse
Relational SQL and NoSQL databases, including Snowflake and PostgreSQL
Data pipeline using Azure stack
Azure cloud services: Datafactory, Databricks, SQL Datawarehouse
Shift: 2PM-11PM
Work Mode: WFO","Relational SQL, snowflake, NoSQL databases, Azure Data Explorer, Azure Fabric, Azure Data Factory, Azure Synapse Analytics, PostgreSQL, Azure Databricks, Python"
Senior Data Engineer,"NTT DATA, Inc.",5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Make an impact with NTT DATA
Join a company that is pushing the boundaries of what is possible. We are renowned for our technical excellence and leading innovations, and for making a difference to our clients and society. Our workplace embraces diversity and inclusion it's a place where you can grow, belong and thrive.
Your day at NTT DATA
The Senior Data Engineer is an advanced subject matter expert, accountable for the transformation of data into a structured format that can be easily analyzed in a query or report.
This role is responsible for developing structured data sets that can be reused or compliment by other data sets and reports.
This role analyzes the data sources and data structure and will design and develop data models to support the analytics requirements of the business which includes management / operational / predictive / data science capabilities.
What You'll Be Doing
Key Responsibilities:
Designs data models in a structured data format to enable analysis thereof.
Designs and develops scalable extract, transformation and loading (ETL) packages from the business source systems and the development of ETL routines to populate data from sources,
Participates in the transformation of object and data models into appropriate database schemas within design constraints.
Interprets installation standards to meet project needs and produces database components as required.
Directs test scenarios and is responsible for participating in thorough testing and validation to support the accuracy of data transformations.
Accountable for running data migrations across different databases and applications, for example MS Dynamics, Oracle, SAP and other ERP systems.
Works across multiple IT and business teams to define and implement data table structures and data models based on requirements.
Accountable for analysis, and development of ETL and migration documentation.
Works with various stakeholders to evaluate potential data requirements.
Accountable for the definition and management of scoping, requirements, definition, and prioritization activities for small-scale changes and assist with more complex change initiatives.
Networks with various stakeholders, contributing to the recommendation of improvements in automated and non-automated components of the data tables, data queries and data models.
Knowledge and Attributes:
Advanced knowledge of the definition and management of scoping requirements, definition and prioritization activities.
Advanced understanding of database concepts, object and data modelling techniques and design principles and conceptual knowledge of building and maintaining physical and logical data models.
Advanced expertise in Microsoft Azure Data Factory, SQL Analysis Server, SAP Data Services, SAP BTP.
Advanced understanding of data architecture landscape between physical and logical data models
Analytical mindset with excellent business acumen skills.
Problem-solving aptitude with the ability to communicate effectively, both written and verbal.
Ability to think strategically and build effective relationships at all levels within the organization.
Advanced expert in programing languages (Perl, bash, Shell Scripting, Python, etc.).
Academic Qualifications and Certifications:
Bachelor's degree or equivalent in computer science, software engineering, information technology, or a related field.
Relevant certifications preferred such as SAP, Microsoft Azure etc.
Certified Data Engineer, Certified Professional certification preferred.
Required Experience:
Advanced demonstrated experience in data engineering, data mining within a fast-paced environment.
Proficient in building modern data analytics solutions that delivers insights from large and complex data sets with multi-terabyte scale.
Advanced demonstrated experience with architecture and design of secure, highly available and scalable systems.
Advanced proficiency in automation, scripting and proven examples of successful implementation.
Advanced proficiency in scripting languages (Perl, bash, Shell Scripting, Python, etc.).
Advanced demonstrated experience with big data tools like Hadoop, Cassandra, Storm etc.
Advanced demonstrated experience in any applicable language, preferably .NET.
Advanced proficiency in SAP, SQL, MySQL databases and Microsoft SQL.
Advanced demonstrated experience working with data sets and ordering data through MS Excel functions, e.g. macros, pivots.
Workplace type:
Remote Working
About NTT DATA
NTT DATA is a $30+ billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long-term success. We invest over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure, and connectivity. We are also one of the leading providers of digital and AI infrastructure in the world. NTT DATA is part of NTT Group and headquartered in Tokyo.
Equal Opportunity Employer
NTT DATA is proud to be an Equal Opportunity Employer with a global culture that embraces diversity. We are committed to providing an environment free of unfair discrimination and harassment. We do not discriminate based on age, race, colour, gender, sexual orientation, religion, nationality, disability, pregnancy, marital status, veteran status, or any other protected category. Join our growing global team and accelerate your career with us. Apply today.","SAP SQL, Microsoft Azure Data Factory, Sap Data Services, Ms Excel, Storm, Hadoop, SAP BTP, Cassandra, Bash, Shell Scripting, Microsoft Sql, Sql, Perl, MySQL, Python"
Senior Data Engineer,Verint,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"At Verint, we believe customer engagement is the core of every global brand. Our mission is to help organizations discover opportunities previously only scarcely imagined by connecting work, data, and experiences enterprise wide. We hire innovators with the passion, creativity, and drive to answer constantly shifting market challenges and deliver impactful results for our customers. Our commitment to attracting and retaining a talented, diverse, and engaged team creates a collaborative environment that openly celebrates all cultures and affords personal and professional growth opportunities. Learn more at www.verint.com.
We are looking for Senior Data Engineer who love to solve complex problems across a full spectrum of latest technologies and take lead on POC, Technical design, being ahead of team. Java, Spring Boot, with DevOps/CloudOps Engineer with expertise in AWS services, Terraform, and Kubernetes. The ideal candidate will have to have a strong background in building and managing services on cloud infrastructure using best practices in DevOps and cloud technologies.
Essential Requirements:
Java +Spark
Total Experience 5+ yrs.
Bachelor's degree in computer science / software engineering (or similar)
Java with Spring boot
Depth and technical understanding of: data structures, I/O, multi-threading, Restful Web Services
Spark Experience
AWS Cloud Knowledge, Deployment, troubleshooting
Database knowledge (RDBS/NoSQL)
Strong debugging skills of performance/memory leaks/crash/Multi-threaded/Algorithms.
Experience with source control tools (GIT or similar)
Experience with compiling and continuous integration tools like: Eclipse/IntelliJ, Gradle/Maven, Jenkins, Artifactory
Excellent teamwork and communication skills.
High self-learning and self-managed abilities.
Integration/build/static code analysis tools
System wide and end to product understanding.
Leadership skills
Strong interpersonal skills, high communication skills.
Responsibilities:
Analyze and influence technical, system, and other user requirements.
Design SW solutions, considering large scale, high availability, security, robustness, performance, cloud, multi tenancy.
Develop highest standard code, complex solutions, in enterprise level application and architecture Feature leading - leading engineers working on a mutual feature through integrations, reviews, status reports, problem solving.
Work with cross functional stakeholders such as product management, additional SW engineers, architects and team leads.
Deep understanding of robustness implementations, performance and sizing. Deep understanding and advanced implementation of continuous Integration and delivery.
Strong understanding and oversee of all phases of the development life cycle, such as Automation, CICD, TDD, Integrations, Builds, Deployment.
Preferred Requirements:
Working Knowledge or Exposure to NLP, AI/ML
Data Warehousing Concepts
Working Knowledge or Exposure to Python
Working Knowledge or Exposure AWS SDK
Data Modelling
Large scale systems design exposure
If Interested, please send in your resumes to [HIDDEN TEXT]
Thanks
Neena","Java, Maven, Eclipse, Spring Boot, Artifactory, Intellij, Nosql, Jenkins, Git, Gradle, Terraform, Spark, Kubernetes, AWS"
Senior Data Engineer,MDA Edge,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"Job Summary:
We are seeking a results-driven Senior Data Engineer to join our engineering team. The ideal candidate will have hands-on expertise in data pipeline development, cloud infrastructure, and business intelligence (BI) support, with a strong command of the modern data stack.
In this role, you will be responsible for building scalable ETL/ELT workflows, managing data lakes and marts, and ensuring seamless data delivery to analytics and BI teams. The position requires deep technical proficiency in PostgreSQL, Python scripting, Apache Airflow, AWS (or other cloud environments), and modern data and BI tools.
Key Responsibilities:
PostgreSQL & Data Modeling:
Design and optimize complex SQL queries, stored procedures, and indexes.
Perform performance tuning and query plan analysis.
Contribute to schema design and data normalization.
Data Migration & Transformation:
Migrate data from multiple sources to cloud or operational data store (ODS) platforms.
Design schema mapping and implement transformation logic.
Ensure consistency, integrity, and accuracy in migrated data.
Python Scripting for Data Engineering:
Develop automation scripts for data ingestion, cleansing, and transformation.
Handle various file formats (JSON, CSV, XML), REST APIs, and cloud SDKs (e.g., Boto3).
Maintain reusable script modules for operational pipelines.
Data Orchestration with Apache Airflow:
Develop and manage Directed Acyclic Graphs (DAGs) for batch and stream workflows.
Implement retries, task dependencies, notifications, and failure handling.
Integrate Airflow with cloud services, data lakes, and data warehouses.
Cloud Platforms (AWS / Azure / GCP):
Manage data storage solutions (S3, GCS, Blob), compute services, and data pipelines.
Configure permissions, IAM roles, encryption, and logging for security.
Monitor and optimize cost and performance of cloud-based data operations.
Data Marts & Analytics Layer:
Design and manage data marts using dimensional models.
Build star and snowflake schemas to support BI and self-service analytics.
Implement incremental load strategies and partitioning for efficiency.
Modern Data Stack Integration:
Work with tools such as DBT, Fivetran, Redshift, Snowflake, BigQuery, and Kafka.
Support modular pipeline design and metadata-driven frameworks.
Ensure high availability and scalability of the data stack.
BI & Reporting Tools (Power BI / Superset / Supertech):
Collaborate with BI teams to design datasets and optimize queries.
Support dashboard development and reporting layers.
Manage access, data refresh schedules, and performance optimization for BI tools.
Required Skills & Qualifications:
4-6 years of hands-on experience in data engineering roles.
Strong SQL skills in PostgreSQL (query tuning, complex joins, stored procedures).
Advanced Python scripting skills for automation and ETL processes.
Proven experience with Apache Airflow (custom DAGs, error handling).
Solid understanding of cloud architecture, particularly AWS.
Experience with data marts and dimensional data modeling.
Exposure to modern data stack tools (DBT, Kafka, Snowflake, etc.).
Familiarity with BI tools such as Power BI, Apache Superset, or Supertech BI.
Knowledge of version control (Git) and CI/CD pipelines is a plus.
Excellent problem-solving and communication skills.","Supertech BI, dbt, snowflake, Fivetran, Apache Superset, BigQuery, Power Bi, PostgreSQL, Kafka, Redshift, Apache Airflow, Git, Python, AWS"
Data Engineer - Apache Airflow,Ekfrazo Technologies Private Limited,4-6 Years,,"Mumbai, India",Login to check your skill match score,"Job Title: Data Engineer
Location: Mumbai- WFO
Experience: 4+ Years
Notice Period: Immediate Joiners Only
About the Role:
We are seeking a skilled and motivated Data Engineer to join our team in Mumbai. The ideal candidate will have a strong background in Python (OOPS), workflow orchestration using Airflow, and experience working with Azure cloud services and Snowflake. This role is best suited for someone passionate about building scalable data pipelines and backend applications or SDKs.
Key Responsibilities:
Design, build, and maintain scalable and robust data pipelines using Apache Airflow
Develop backend components and reusable SDKs in Python, with a strong emphasis on OOPS principles
Integrate and manage large-scale data workflows on Azure Cloud, leveraging services such as Data Factory, Blob Storage, and more
Work with Snowflake for data warehousing and analytics
Implement monitoring and data quality checks using tools like Great Expectations
Collaborate with cross-functional teams to understand business data needs and deliver high-quality solutions
Must-Have Skills:
4+ years of experience in Data Engineering or related roles
Hands-on experience with Apache Airflow
Strong Python programming skills, with emphasis on Object-Oriented Programming (OOPS) and backend application or SDK development
Proven experience working with Azure cloud platform, especially services relevant to data engineering (e.g., Data Factory, Blob Storage, Azure Functions)
Good understanding of Snowflake architecture, data loading/unloading, and query optimization
Strong analytical and problem-solving skills
Nice to Have:
Experience with Great Expectations or similar Data Quality frameworks
Prior work on building or maintaining Data Quality (DQ) frameworks
Advanced knowledge of Snowflake features like Snowpipe, Streams & Tasks, or Materialized Views","snowflake, Blob Storage, Great Expectations, Apache Airflow, Data Factory, Azure Cloud Services, Python"
Data Engineer,ICICIDirect,6-12 Years,,"Navi Mumbai, Mumbai, India",Login to check your skill match score,"Job Responsibilities:
To work on Data Analytics to solve various business use cases.
Design and implement data pipelines to extract data from various sources (databases, flat files, APIs, streaming data, etc.)
Develop efficient ETL processes using tools like Apache Spark, Apache Sqoop, Python/SQL, Pyspark,
Good to have expertise/ knowledge in Apache Iceberg, Apace Ni-fi/ Apache Flink and Apache Kafka.
Basic Understanding of Data Pipeline design using PL/SQL jobs.
Provide seamless data Availability for data analysts, data scientists, and other stakeholders.
Develop various Business Requirements leveraging SQL and Python.
Stay updated with the latest industry trends and advancements in data analytics and AI, Data Lake, Delta Lake concepts.
Required Experience:
Over all 6 -12 years of experience in Data Engineering & Data Analytics.
Hands-on experience in coding with Python and SQL.
Experience in building ETL, ELT Pipelines using Python.
Strong expertise in Hadoop Ecosystem (Hive, Impala, Cloudera), Spark, Sqoop, Delta Lake, Data warehouse Concepts, RDBMS System, No SQL Databases, Apache Tools like Ni-fi, Kafka etc.
Experience in Cloudera data platform is preferred.
Required Qualification:
Bachelor's or Master's degree in Computer Science, IT or any Engineering field.","Apache Iceberg, No SQL Databases, ETL processes, Delta Lake, Apache Sqoop, Apache Flink, Apache Spark, Pl Sql, Impala, Sql, Data Warehouse Concepts, Hive, Hadoop Ecosystem, RDBMS, Apache Kafka, Cloudera, Python"
Senior Data Engineer,MDA Edge,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"Job Summary:
We are seeking a results-driven Senior Data Engineer to join our engineering team. The ideal candidate will have hands-on expertise in data pipeline development, cloud infrastructure, and business intelligence (BI) support, with a strong command of the modern data stack.
In this role, you will be responsible for building scalable ETL/ELT workflows, managing data lakes and marts, and ensuring seamless data delivery to analytics and BI teams. The position requires deep technical proficiency in PostgreSQL, Python scripting, Apache Airflow, AWS (or other cloud environments), and modern data and BI tools.
Key Responsibilities:
PostgreSQL & Data Modeling:
Design and optimize complex SQL queries, stored procedures, and indexes.
Perform performance tuning and query plan analysis.
Contribute to schema design and data normalization.
Data Migration & Transformation:
Migrate data from multiple sources to cloud or operational data store (ODS) platforms.
Design schema mapping and implement transformation logic.
Ensure consistency, integrity, and accuracy in migrated data.
Python Scripting for Data Engineering:
Develop automation scripts for data ingestion, cleansing, and transformation.
Handle various file formats (JSON, CSV, XML), REST APIs, and cloud SDKs (e.g., Boto3).
Maintain reusable script modules for operational pipelines.
Data Orchestration with Apache Airflow:
Develop and manage Directed Acyclic Graphs (DAGs) for batch and stream workflows.
Implement retries, task dependencies, notifications, and failure handling.
Integrate Airflow with cloud services, data lakes, and data warehouses.
Cloud Platforms (AWS / Azure / GCP):
Manage data storage solutions (S3, GCS, Blob), compute services, and data pipelines.
Configure permissions, IAM roles, encryption, and logging for security.
Monitor and optimize cost and performance of cloud-based data operations.
Data Marts & Analytics Layer:
Design and manage data marts using dimensional models.
Build star and snowflake schemas to support BI and self-service analytics.
Implement incremental load strategies and partitioning for efficiency.
Modern Data Stack Integration:
Work with tools such as DBT, Fivetran, Redshift, Snowflake, BigQuery, and Kafka.
Support modular pipeline design and metadata-driven frameworks.
Ensure high availability and scalability of the data stack.
BI & Reporting Tools (Power BI / Superset / Supertech):
Collaborate with BI teams to design datasets and optimize queries.
Support dashboard development and reporting layers.
Manage access, data refresh schedules, and performance optimization for BI tools.
Required Skills & Qualifications:
4-6 years of hands-on experience in data engineering roles.
Strong SQL skills in PostgreSQL (query tuning, complex joins, stored procedures).
Advanced Python scripting skills for automation and ETL processes.
Proven experience with Apache Airflow (custom DAGs, error handling).
Solid understanding of cloud architecture, particularly AWS.
Experience with data marts and dimensional data modeling.
Exposure to modern data stack tools (DBT, Kafka, Snowflake, etc.).
Familiarity with BI tools such as Power BI, Apache Superset, or Supertech BI.
Knowledge of version control (Git) and CI/CD pipelines is a plus.
Excellent problem-solving and communication skills.","Supertech BI, dbt, snowflake, Fivetran, Apache Superset, BigQuery, Power Bi, PostgreSQL, Kafka, Redshift, Apache Airflow, Git, Python, AWS"
Senior Data Engineer,Verint,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"At Verint, we believe customer engagement is the core of every global brand. Our mission is to help organizations discover opportunities previously only scarcely imagined by connecting work, data, and experiences enterprise wide. We hire innovators with the passion, creativity, and drive to answer constantly shifting market challenges and deliver impactful results for our customers. Our commitment to attracting and retaining a talented, diverse, and engaged team creates a collaborative environment that openly celebrates all cultures and affords personal and professional growth opportunities. Learn more at www.verint.com.
We are looking for Senior Data Engineer who love to solve complex problems across a full spectrum of latest technologies and take lead on POC, Technical design, being ahead of team. Java, Spring Boot, with DevOps/CloudOps Engineer with expertise in AWS services, Terraform, and Kubernetes. The ideal candidate will have to have a strong background in building and managing services on cloud infrastructure using best practices in DevOps and cloud technologies.
Essential Requirements:
Java +Spark
Total Experience 5+ yrs.
Bachelor's degree in computer science / software engineering (or similar)
Java with Spring boot
Depth and technical understanding of: data structures, I/O, multi-threading, Restful Web Services
Spark Experience
AWS Cloud Knowledge, Deployment, troubleshooting
Database knowledge (RDBS/NoSQL)
Strong debugging skills of performance/memory leaks/crash/Multi-threaded/Algorithms.
Experience with source control tools (GIT or similar)
Experience with compiling and continuous integration tools like: Eclipse/IntelliJ, Gradle/Maven, Jenkins, Artifactory
Excellent teamwork and communication skills.
High self-learning and self-managed abilities.
Integration/build/static code analysis tools
System wide and end to product understanding.
Leadership skills
Strong interpersonal skills, high communication skills.
Responsibilities:
Analyze and influence technical, system, and other user requirements.
Design SW solutions, considering large scale, high availability, security, robustness, performance, cloud, multi tenancy.
Develop highest standard code, complex solutions, in enterprise level application and architecture Feature leading - leading engineers working on a mutual feature through integrations, reviews, status reports, problem solving.
Work with cross functional stakeholders such as product management, additional SW engineers, architects and team leads.
Deep understanding of robustness implementations, performance and sizing. Deep understanding and advanced implementation of continuous Integration and delivery.
Strong understanding and oversee of all phases of the development life cycle, such as Automation, CICD, TDD, Integrations, Builds, Deployment.
Preferred Requirements:
Working Knowledge or Exposure to NLP, AI/ML
Data Warehousing Concepts
Working Knowledge or Exposure to Python
Working Knowledge or Exposure AWS SDK
Data Modelling
Large scale systems design exposure
If Interested, please send in your resumes to [HIDDEN TEXT]
Thanks
Neena","Java, Maven, Eclipse, Spring Boot, Artifactory, Intellij, Nosql, Jenkins, Git, Gradle, Terraform, Spark, Kubernetes, AWS"
Data Engineer,NAZZTEC,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Senior Databricks Data Engineer (AWS Platform)
Location: Hyderabad
Shift: B Shift (12 PM 10 PM IST)
Experience Required: 5+ Years
Job Summary:
We are seeking a Senior Databricks Data Engineer with strong experience in building and optimizing data pipelines and architectures on AWS using Databricks and PySpark. The ideal candidate will also have hands-on experience in Big Data technologies, real-time streaming, and CI/CD pipelines. This role demands client-facing experience and the ability to operate in a fast-paced environment while delivering high-quality data engineering solutions.
Key Responsibilities:
Design and develop scalable data engineering solutions using Databricks on AWS with PySpark and Databricks SQL.
Build and maintain data pipelines using Delta Lake for batch and streaming data.
Design and implement real-time data streaming applications using Kafka or Kinesis.
Develop and maintain ETL workflows and data warehouse architectures aligned with business goals.
Collaborate with data scientists, analysts, and stakeholders to ensure data quality and integrity.
Use Airflow to orchestrate complex data workflows.
Develop and maintain CI/CD pipelines using GIT, Jenkins, Docker, Kubernetes, and Terraform.
Write efficient and reusable code in Python, Java, or Scala.
Engage with clients regularly to gather requirements, provide updates, and build trusted relationships.
Required Skills & Experience:
5+ years of experience in Databricks engineering on AWS using PySpark, Databricks SQL, and Delta Lake.
5+ years of experience in ETL, Big Data/Hadoop, and data warehouse design/delivery.
2+ years of hands-on experience with Kafka or Kinesis for real-time data streaming.
4+ years of experience in programming languages such as Python, Java, or Scala.
Experience using Apache Airflow in at least one project for data pipeline orchestration.
At least 1 year of experience developing CI/CD pipelines with GIT, Jenkins, Docker, Kubernetes, Shell Scripting, and Terraform.
Professional Attributes:
Willingness to work in B Shift (12 PM 10 PM IST).
Strong client-facing skills with the ability to build trusted relationships.
Excellent problem-solving and critical-thinking abilities.
Strong communication and collaboration skills in cross-functional teams.
Preferred Qualifications:
AWS certifications or relevant cloud training.
Familiarity with DataOps or MLOps practices.
Experience in Agile or Scrum development methodologies.","Databricks SQL, data warehouse design, Delta Lake, Java, Hadoop, Scala, Pyspark, Kafka, Big Data, Apache Airflow, Jenkins, Git, Kinesis, Terraform, Docker, Databricks, Python, Kubernetes, Etl, AWS"
Python Data Engineer,Planful,4-6 Years,,"Hyderabad, India",Login to check your skill match score,"About Us
Planful is the pioneer of financial performance management cloud software. The Planful platform, which helps businesses drive peak financial performance, is used around the globe to streamline business-wide planning, budgeting, consolidations, reporting, and analytics. Planful empowers finance, accounting, and business users to plan confidently, close faster, and report accurately. More than 1,500 customers, including Bose, Boston Red Sox, Five Guys, Grafton Plc, Gousto, Specialized and Zappos rely on Planful to accelerate cycle times, increase productivity, and improve accuracy. Planful is a private company backed by Vector Capital, a leading global private equity firm. Learn more at planful.com.
About The Role
We are looking for self-driven, self-motivated, and passionate technical experts who would love to join us in solving the hardest problems in the EPM space. If you are capable of diving deep into our tech stack to glean through memory allocations, floating point calculations, and data indexing (in addition to many others), come join us.
Position Responsibilities
Shape financial time-series data: outlier detection/handling, missing-value imputation, techniques for small/limited datasets.
Develop & refine forecasting models (ARIMA, Prophet, LSTM/GRU).
Implement anomaly-detection solutions (Isolation Forest, One-Class SVM, statistical methods).
Profile & optimize Python code (vectorization, multiprocessing, cProfile).
Monitor model performance and iterate to improve accuracy.
Collaborate with data scientists and stakeholders to integrate solutions.
Required Skills And Experience
4+ years in a mid-level Data Engineer role, preferably in analytics or fintech.
Expert in Python (pandas, NumPy, SciPy, scikit-learn) with hands-on performance tuning.
Experience with Flask and Django for building and deploying Python services.
Familiarity with AI-assisted development tools and IDEs (Cursor, Windsurf) and modern editor integrations (VS Code + Cline).
Proficiency in C# for .NET integration tasks.
Demonstrated time-series forecasting expertise (ARIMA, Prophet, LSTM/GRU).
Strong data preprocessing skills: outlier treatment, imputation, handling sparse datasets.
Proficient in SQL for complex queries on large datasets.
Excellent analytical thinking, problem-solving, and communication skills.
Why Planful
Planful Exists To Enrich The World By Helping Our Customers And Our People Achieve Peak Performance. To Foster The Best In Class Work We're So Proud Of, We've Created a Best In Class Culture, Including
2 Volunteer days, Birthday PTO, and quarterly company Wellness Days
3 months supply of diapers and meal deliveries for the first month of your Maternity/Paternity leave
Annual Planful Palooza, our in-person, company-wide culture kickoff
Company-wide Mentorship program with Executive sponsorship of CFO and Managed solutions that drive our products and services forward. You will lead a team of talented engineers, collaborating closely with cross-functional teams across our India and North America leadership hubs to deliver high-quality software solutions that meet business objectives. This role offers an exciting opportunity to leverage your expertise in end to end product development to drive innovation, mentor junior team members, and contribute to the overall technical vision of the organization.","AI-assisted development tools, prophet, GRU, scikit-learn, LSTM, Isolation Forest, VS Code, One-Class SVM, Numpy, Sql, Django, Pandas, Python, Arima, Scipy, Flask"
Data Engineer,VARITE INC,5-7 Years,,India,Login to check your skill match score,"VARITE is looking for a qualified Data Engineerfor one of its clients.
WHAT THE CLIENT DOES
The company started out as a hardware/software vendor, but over time added more subscription-based services.
WHAT WE DO
Established in the Year 2000, VARITE is an award-winning minority business enterprise providing global consulting & staffing services to Fortune 1000 companies and government agencies. With 850+ global consultants, VARITE is committed to delivering excellence to its customers by leveraging its global experience and expertise in providing comprehensive scientific, engineering, technical, and non-technical staff augmentation and talent acquisition services.
HERE'S WHAT YOU'LL DO:
Job Details:
Title: Data Engineer
Location: India/Remote
Duration: 6 Months
Job Description: We are looking for a Data Engineer with MFG experience for a staff augmentation role.
These are the requirements for the role:
Expert in Azure Data Factory
Proven experience in Data Modelling for Manufacturing data sources
Proficient SQL design
+5 years of experience in Data engineering roles
Prove experience in PBI: Dashboarding, DAX calculations, Star scheme development and semantic model building
Manufacturing knowledge
Experience with GE PPA as data source is desirable
API dev Knowledge
Python skills
Location: Offshore with 3 up to 5 hours overlap with CST time zone
If this opportunity interests you, please respond by clicking on EasyApply.
Know someone who would be perfect for this role Refer them to us and if they are hired, you could be eligible for our employee referral bonus! Help us grow our team with top talent from your network.
VARITE is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status","PBI Dashboarding, Star scheme development, GE PPA, DAX calculations, Manufacturing knowledge, semantic model building, Data Modelling, Azure Data Factory, Python, Sql"
Senior Data Engineer,Azoon Tech Consulting LLC,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"About Us
We are building a next-generation data engineering platform designed to simplify complex data workflows, automate orchestration, and deliver high-performance pipelines at scale. Our stack includes Python, Apache Airflow, PySpark, and FastAPI, and we're looking for passionate engineers who want to shape the future of data engineering.
Role Overview
As a Principal Data Engineer, you will play a critical role in designing and building core components of our data platform. This is a hands-on leadership role where you'll drive architecture decisions, mentor engineers, and own complex data problems end-to-end. You'll collaborate closely with product and platform teams to define scalable, reliable, and high-performance data systems.
Key Responsibilities
Design and build scalable, reusable data engineering components and frameworks using Python, Airflow, and PySpark.
Lead the architecture of end-to-end data pipelines from ingestion and transformation to orchestration and monitoring.
Build and maintain FastAPI-based services to expose metadata, control plane, and developer APIs.
Drive best practices in software engineering and data architecture (code quality, testing, CI/CD, performance).
Mentor and guide a team of data and backend engineers.
Collaborate with product managers, designers, and other engineers to deliver features on time and at high quality.
Evaluate and introduce tools, frameworks, and technologies that improve the developer experience and platform scalability.
Requirements
Technical Skills:
10+ years of experience in software or data engineering, including experience building large-scale data platforms or products.
Expert-level Python skills with a strong software engineering background.
Deep expertise in Apache Airflow (or similar orchestration tools) and PySpark (or distributed data processing frameworks).
Strong understanding of API design and development using FastAPI or similar frameworks.
Experience with data modeling, schema design, and working with large-scale data (TB/PB scale).
Hands-on experience with cloud-native data platforms (AWS, GCP, or Azure).
Familiarity with containerization (Docker), CI/CD pipelines, and infrastructure-as-code is a plus.
Leadership & Communication:
Proven track record in leading technical design discussions and making architecture decisions.
Strong mentorship skills and the ability to drive a high-performing engineering culture.
Excellent communication skills, both written and verbal, with the ability to convey complex ideas to both technical and non-technical stakeholders.
Nice to Have
Experience building developer tools or internal platforms.
Exposure to modern data tools like dbt, Snowflake, Delta Lake, etc.
Open-source contributions in data or backend engineering.
What We Offer
Opportunity to lead and shape a cutting-edge data platform from the ground up.
Collaborative and product-minded engineering culture.
Competitive compensation, equity, and benefits.
Flexible remote/onsite working model.","API design and development, Infrastructure-as-code, Data Modeling, Schema Design, Apache Airflow, FastAPI, Python, Pyspark"
Sr. Data Engineer,Madison Logic,5-7 Years,,"Pune, India",Login to check your skill match score,"About Madison Logic:
Our team is reshaping B2B marketing and having fun in the process! When joining Madison Logic, you are committing to giving 100% and always striving for more. As a truly global company, we take pride in a diverse culture free from gender, racial, and other forms of bias.
Our Vision: We empower B2B organizations globally to convert their best accounts faster
Our Values:
URGENCY Lead with Action. Prioritize Follow-up.
ACCOUNTABILITY Don't Point Fingers. Take Responsibility.
INNOVATION Think Big. Innovate.
RESPECT Respect Customers. Respect Each Other.
INTEGRITY Act Ethically. Lead by Example.
At ML you will work with & learn from an incredible group of people who care about your success as much as they care about their own. Our team is at the heart of what we do and our success starts with you!
About the Role:
We are currently seeking a Senior Data Engineer to play a crucial role in shaping the future of our data and analytics capabilities. As a Senior Data Engineer at Madison Logic, your responsibilities will include designing, constructing, and deploying scalable data pipelines. Additionally, you will be responsible for developing APIs that will empower our machine learning products and features. Your expertise will be invaluable in refining data models across various components of our data infrastructure to accommodate the growing demands of data processing and analytics at Madison Logic.
In this highly collaborative position, you will closely collaborate with product, engineering, and data teams to achieve our business objectives.
Responsibilities:
Develop and maintain the core data pipelines, involving the creation of production-level SQL and Python code to fuel our platforms.
Adapt and enhance data models and data schemas to align with both business and engineering requirements.
Conduct data analysis to contribute to the enhancement of overall business performance.
Identify and select optimal data sources for specific analytical tasks.
Establish procedures for data mining, data modeling, and data production.
Collaborate with internal and external partners to address challenges and ensure successful outcomes.
Basic Qualifications:
On-site working at the ML physical office, 5-days per week required
Ability to work UK Shift Timing (11:00am 8:00pm Local Time) Required
Fluent in English language (verbal and written) and possessing a clear and concise communication style.
Educational Background: Possess a Bachelor's degree in computer science, statistics, or mathematics.
Programming Expertise: 5+ years of experience with Python, with the ability to write production-level code.
SQL Proficiency: 5+ years of experience in SQL, with excellent skills in navigating multiple data tables and comprehending data models.
Cloud Computing: 3+ years of hands-on experience with cloud computing services, particularly AWS (Amazon Web Services).
Data Architecture: Proven experience in designing data architectures, including Kafka, Data Warehouses and Operational Data Stores (ODS).
Cloud-Based Analytics: Possess a strong understanding of cloud-based analytics platforms, such as Snowflake and AWS SageMaker.
Data Workflow Management: Ideally, possess at least 1 year of experience with data workflow management tools, with Airflow experience being a plus.
Data Cleaning: Be skilled in data cleaning and standardization processes.
SQL Engine: Exhibit an excellent understanding of SQL engines and the capability to perform advanced performance tuning.
Desired Characteristics:
Self-Sufficient and proactive nature, able & comfortable figuring things out, resorting to escalation only when after exhausting all other options
Strong sense of urgency required
Exceptional communication skills, both verbal and written, with a knack for explaining complex concepts in a clear & concise manner across all levels and functions
Team members are encouraged to work collaboratively with an emphasis on results, not on hierarchy or titles.
India-Specific Benefits
5 LPA Medical Coverage
Life Insurance
Provident Fund Contributions
Learning & Development Stipend (Over-And-Above CTC)
Wellness Stipend (Over-And-Above CTC)
Transportation available for female team-members with shifts starting or ending between the hours of 9:30pm and 7:00am
Welcoming in-office environment (located within AWFIS co-working space, Amanora Mall)
Team members are encouraged to work collaboratively with an emphasis on results, not on hierarchy or titles.
Expected Compensation: (Dependent upon Experience)
Fixed CTC: 23,00,000 - 27,00,000 a year
Work Environment:
We offer a mix of in-office and hybrid working. Hybrid remote work arrangements are not available for all positions. Please refer to the job posting detail to determine what in-office requirements apply. Where applicable, hybrid WFH days work must be conducted from your home office located in a jurisdiction in which Madison Logic has the legal right to operate. WFH requires availability and responsiveness on a full-time basis from a distraction free environment with access to high-speed internet. Please inquire for more details.
Pay Transparency/Equity:
We are committed to paying our team equitably for their work, commensurate with their individual skills and experience. Salary Range and additional compensation, including discretionary bonuses and incentive pay, are determined by a rigorous review process taking into account the experience, education, certifications and skills required for the specific role, equity with similarly situated team members, as well as employer-verified region-specific market data provided by an independent 3rd party partner.
We will provide more information about our perks & benefits upon request.
Our Commitment to Diversity & Inclusion:
Madison Logic is proud to be an equal opportunity employer. We are committed to equal employment opportunity regardless of sex, race, color, religion, national origin, sexual orientation, age, marital status, disability, gender identity or Veteran status.
Privacy Disclosure:
All of the information collected in this form and/or by your application by submission of your online profile is necessary and relevant to the performance of the job applied for. We will process the information provided by you in this form, your CV (including physical and online resume profiles), by the referees you have noted, and by the educational institutions with whom we may undertake to verify your qualifications with, in accordance with our privacy policy and for recruitment purposes only.
For more information on how we process the information you have provided including relevant lawful bases (where relevant) please see our privacy policy which is available on our website ( https://www.madisonlogic.com/privacy/ ).","SQL engines, Airflow, AWS SageMaker, snowflake, Data Cleaning, data warehouses, Kafka, Python, Sql"
Sr Data Engineer,ADM,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
Your Responsibilities
Design, develop, and maintain comprehensive BI models using SAP Datasphere, ensuring alignment with business requirements and objectives.
Collaborate with business stakeholders to understand data needs and translate them into effective and scalable BI solutions.
Work closely with SAP BTP to leverage its capabilities for seamless integration, data orchestration, and advanced analytics.
Implement and enforce data governance best practices to ensure data accuracy, consistency, and compliance with regulatory standards.
Develop and maintain data quality measures to proactively identify and address issues in the BI models.
Optimize BI models for performance and responsiveness, considering scalability and efficiency in handling large datasets.
Collaborate with IT infrastructure teams to ensure the underlying systems support optimal BI performance.
Provide training and support to end-users on utilizing BI models effectively for reporting and analysis.
Collaborate with other teams to address and resolve user-reported issues promptly
Create and maintain comprehensive documentation for BI models, data integration processes, and best practices.
Keep documentation up to date with any changes or enhancements made to the BI environment.
Your Profile
Minimum of 8 years plus of experience in SAP BTP DataSphere and SAC with, SAP BW/4HANA
In-depth knowledge of SAP BTP DataSphere architecture, components, and capabilities
Strong proficiency in SAP BW/4HANA data modeling, data provisioning, and data transformation techniques.
Expertise in S/4HANA Analytics, including CDS Views Development, SAP HANA Cloud and SAP BW/4HANA.
Experience in the integration of SAP and non-SAP data sources
Excellent analytical, problem-solving, and communication skills.
#IncludingYou
Diversity, equity, inclusion and belonging are cornerstones of ADM's efforts to continue innovating, driving growth, and delivering outstanding performance. We are committed to attracting and retaining a diverse workforce and create welcoming, truly inclusive work environments environments that enable every ADM colleague to feel comfortable on the job, make meaningful contributions to our success, and grow their career. We respect and value the unique backgrounds and experiences that each person can bring to ADM because we know that diversity of perspectives makes us better, together.
For more information regarding our efforts to advance Diversity, Equity, Inclusion & Belonging, please visit our website here: Diversity, Equity and Inclusion | ADM.
About ADM
At ADM, we unlock the power of nature to provide access to nutrition worldwide. With industry-advancing innovations, a complete portfolio of ingredients and solutions to meet any taste, and a commitment to sustainability, we give customers an edge in solving the nutritional challenges of today and tomorrow. We're a global leader in human and animal nutrition and the world's premier agricultural origination and processing company. Our breadth, depth, insights, facilities and logistical expertise give us unparalleled capabilities to meet needs for food, beverages, health and wellness, and more. From the seed of the idea to the outcome of the solution, we enrich the quality of life the world over. Learn more at www.adm.com.
Req/Job ID
97488BR
Ref ID","SAP Datasphere, SAP BW 4HANA, Data provisioning, Integration of SAP and non-SAP data sources, CDS Views Development, SAP HANA Cloud, SAP BTP, Data Transformation, Data Modeling"
Data Engineer,Lingaro,8-10 Years,,India,Login to check your skill match score,"Role: Data Engineer Lead Consultant
Location: India (Full Time-Remote)
Preference: Immediate Joiners
About Lingaro:
Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.
Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.
About
Data Engineering:Data e
ngineering involves the development of solutions for the collection, transformation, storage and management of data to support data-driven decision making and enable efficient data analysis by end users. It focuses on the technical aspects of data processing, integration, and delivery to ensure that data is accurate, reliable, and accessible in a timely manner. It also focuses on the scalability, cost-effectiveness, security, and supportability of the solution. Data engineering encompasses multiple toolsets and architectural concepts across on-premises and cloud stacks, including but not limited to data warehousing, data lakes, lake house, data mesh, and includes extraction, ingestion, and synchronization of structured and unstructured data across the data ecosystem. It also includes processing organization and orchestration, as well as performance optimization of data processing.Job Re
sponsibilities:Provid
e leadership and guidance to the data engineering team, including mentoring, coaching, and fostering a collaborative work environment. Set clear goals, assign tasks, and manage resources to ensure successful project delivery. Work closely with developers to support them and improve data engineering processes. Suppor
t team members with troubleshooting and resolving complex technical issues and challenges. Provid
e technical expertise and direction in data engineering, guiding the team in selecting appropriate tools, technologies, and methodologies. Stay updated with the latest advancements in data engineering and ensure the team follows best practices and industry standards. Collab
orate with stakeholders to understand project requirements, define scope, and create project plans. Suppor
t project managers to ensure that projects are executed effectively, meeting timelines, budgets, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Act as
a trusted advisor for the customer. Overse
e the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration. Align
coding standards, conduct code reviews to ensure proper code quality level. Identi
fy and introduce quality assurance processes for data pipelines and workflows. Optimi
ze data processing and storage for performance, efficiency and cost savings. Evalua
te and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Act as
main point of contact to other teams/contributors engaged in the project. Mainta
in technical documentation of the project, control validity and perform regular reviews of it. Ensure
compliance with security standards and regulations. Requi
r
ements:A bach
elor's or master's degree in Computer Science, Information Systems, or a related field is typically required. Additional certifications in cloud are advantageous. Minimu
m of 8 years of experience in data engineering or a related field. Strong
technical skills in data engineering, including proficiency in programming languages such as Python, SQL, Pyspark. Famili
arity with Azure cloud platform viz. Azure Databricks, Data Factory, Data Lake etc., and experience in implementing data solutions in a cloud environment. Expert
ise in working with various data tools and technologies, such as ETL frameworks, data pipelines, and data warehousing solutions. In-dep
th knowledge of data management principles and best practices, including data governance, data quality, and data integration. Excell
ent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues. Knowle
dge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects. Excell
ent communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams, stakeholders, and senior management. Contin
uous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies. Consul
ting exposure, with external customer focus mindset is preferred.Why
j
oin us: Stabl
e
employment. On the market since 2008, 1300+ talents currently on board in 7 global sites.100% remote.Flexibility regarding working hours.Full-time positionComprehensive online onboarding program with a Buddy from day 1.Cooperation with top-tier engineers and experts.Unlimited access to the Udemy learning platform from day 1.Certificate training programs. Lingarians earn 500+ technology certificates yearly.Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.Grow as we grow as a company. 76% of our managers are internal promotions.A diverse, inclusive, and values-driven community.Autonomy to choose the way you work. We trust your ideas.Create our community together. Refer your friends to receive bonuses.Activities to support your well-being and health.Plenty of opportunities to donate to charities and support the environment.","ETL frameworks, data pipelines, data warehousing solutions, Pyspark, Data Factory, Data Lake, Azure Databricks, Python, Sql"
Snowflake Data Engineer,COVET IT INC,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"Hi,
Please go through the below requirements and let me know your interest and forward your resume along with your contact information to [HIDDEN TEXT]
Location :Bengaluru, KA
Experience : 7 to 10 years
Job Description:
Snowflake + DBT + data stage + strong in sql and basic knowledge in python and Kafka.
Agile methodology","dbt, snowflake, Agile Methodology, Kafka, Python, Sql, Data Stage"
Senior Data Engineer,MDA Edge,4-6 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Job Summary:
We are seeking a results-driven Senior Data Engineer to join our engineering team. The ideal candidate will have hands-on expertise in data pipeline development, cloud infrastructure, and business intelligence (BI) support, with a strong command of the modern data stack.
In this role, you will be responsible for building scalable ETL/ELT workflows, managing data lakes and marts, and ensuring seamless data delivery to analytics and BI teams. The position requires deep technical proficiency in PostgreSQL, Python scripting, Apache Airflow, AWS (or other cloud environments), and modern data and BI tools.
Key Responsibilities:
PostgreSQL & Data Modeling:
Design and optimize complex SQL queries, stored procedures, and indexes.
Perform performance tuning and query plan analysis.
Contribute to schema design and data normalization.
Data Migration & Transformation:
Migrate data from multiple sources to cloud or operational data store (ODS) platforms.
Design schema mapping and implement transformation logic.
Ensure consistency, integrity, and accuracy in migrated data.
Python Scripting for Data Engineering:
Develop automation scripts for data ingestion, cleansing, and transformation.
Handle various file formats (JSON, CSV, XML), REST APIs, and cloud SDKs (e.g., Boto3).
Maintain reusable script modules for operational pipelines.
Data Orchestration with Apache Airflow:
Develop and manage Directed Acyclic Graphs (DAGs) for batch and stream workflows.
Implement retries, task dependencies, notifications, and failure handling.
Integrate Airflow with cloud services, data lakes, and data warehouses.
Cloud Platforms (AWS / Azure / GCP):
Manage data storage solutions (S3, GCS, Blob), compute services, and data pipelines.
Configure permissions, IAM roles, encryption, and logging for security.
Monitor and optimize cost and performance of cloud-based data operations.
Data Marts & Analytics Layer:
Design and manage data marts using dimensional models.
Build star and snowflake schemas to support BI and self-service analytics.
Implement incremental load strategies and partitioning for efficiency.
Modern Data Stack Integration:
Work with tools such as DBT, Fivetran, Redshift, Snowflake, BigQuery, and Kafka.
Support modular pipeline design and metadata-driven frameworks.
Ensure high availability and scalability of the data stack.
BI & Reporting Tools (Power BI / Superset / Supertech):
Collaborate with BI teams to design datasets and optimize queries.
Support dashboard development and reporting layers.
Manage access, data refresh schedules, and performance optimization for BI tools.
Required Skills & Qualifications:
4-6 years of hands-on experience in data engineering roles.
Strong SQL skills in PostgreSQL (query tuning, complex joins, stored procedures).
Advanced Python scripting skills for automation and ETL processes.
Proven experience with Apache Airflow (custom DAGs, error handling).
Solid understanding of cloud architecture, particularly AWS.
Experience with data marts and dimensional data modeling.
Exposure to modern data stack tools (DBT, Kafka, Snowflake, etc.).
Familiarity with BI tools such as Power BI, Apache Superset, or Supertech BI.
Knowledge of version control (Git) and CI/CD pipelines is a plus.
Excellent problem-solving and communication skills.","dbt, snowflake, Supertech, Fivetran, Apache Superset, BigQuery, Power Bi, PostgreSQL, Kafka, Redshift, Apache Airflow, Git, Python, AWS"
Snowflake Data Engineer,Ventra Health,4-6 Years,,"Chennai, India",Login to check your skill match score,"Ventra is a leading business solutions provider for facility-based physicians practicing anesthesia, emergency medicine, hospital medicine, pathology, and radiology. Focused on Revenue Cycle Management, Ventra partners with private practices, hospitals, health systems, and ambulatory surgery centers to deliver transparent and data-driven solutions that solve the most complex revenue and reimbursement issues, enabling clinicians to focus on providing outstanding care to their patients and communities.
Job Summary
We are seeking a Snowflake Data Engineer to join our Data & Analytics team. This role involves designing, implementing, and optimizing Snowflake-based data solutions. The ideal candidate will have proven, hands-on data engineering expertise in Snowflake, cloud data platforms, ETL/ELT processes, and Medallion data architecture best practices. The data engineer role has a day-to-day focus on implementation, performance optimization and scalability. This is a tactical role requiring independent data analysis and data discovery to understand our existing source systems, fact and dimension data models, and implement an enterprise data warehouse solution in Snowflake. This role will take direction from the Lead Snowflake Data Engineer and Director of Data Engineering for their work while bringing their own domain expertise and experience.
Essential Functions And Tasks
Participate in the design, development, and maintenance of a scalable Snowflake data solution serving our enterprise data & analytics team.
Implement data pipelines, ETL/ELT workflows, and data warehouse solutions using Snowflake and related technologies.
Optimize Snowflake database performance
Collaborate with cross-functional teams of data analysts, business analysts, data scientists, and software engineers, to define and implement data solutions.
Ensure data quality, integrity, and governance.
Troubleshoot and resolve data-related issues, ensuring high availability and performance of the data platform.
Education And Experience Requirements
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
4+ years of experience in-depth data engineering, with at least 1+ minimum year(s) of dedicated experience engineering solutions in an enterprise scale Snowflake environment.
Tactical expertise in ANSI SQL, performance tuning, and data modeling techniques.
Strong experience with cloud platforms (preference to Azure) and their data services.
Experience in ETL/ELT development using tools such as Azure Data Factory, dbt, Matillion, Talend, or Fivetran.
Hands-on experience with scripting languages like Python for data processing.
Snowflake SnowPro certification; preference to the engineering course path.
Experience with CI/CD pipelines, DevOps practices, and Infrastructure as Code (IaC).
Knowledge of streaming data processing frameworks such as Apache Kafka or Spark Streaming.
Familiarity with BI and visualization tools such as PowerBI.
Knowledge, Skills, And Abilities
Familiarity working in an agile scum team, including sprint planning, daily stand-ups, backlog grooming, and retrospectives.
Ability to self-manage medium complexity deliverables and document user stories and tasks through Azure Dev Ops.
Personal accountability to committed sprint user stories and tasks
Strong analytical and problem-solving skills with the ability to handle complex data challenges
Ability to read, understand, and apply state/federal laws, regulations, and policies.
Ability to communicate with diverse personalities in a tactful, mature, and professional manner.
Ability to remain flexible and work within a collaborative and fast paced environment.
Understand and comply with company policies and procedures.
Strong oral, written, and interpersonal communication skills.
Strong time management and organizational skills.
Ventra Health
Equal Employment Opportunity (Applicable only in the US)
Ventra Health is an equal opportunity employer committed to fostering a culturally diverse organization. We strive for inclusiveness and a workplace where mutual respect is paramount. We encourage applications from a diverse pool of candidates, and all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, or veteran status. We will provide reasonable accommodations to qualified individuals with disabilities, as needed, to assist them in performing essential job functions.
Recruitment Agencies
Ventra Health does not accept unsolicited agency resumes. Ventra Health is not responsible for any fees related to unsolicited resumes.
Solicitation of Payment
Ventra Health does not solicit payment from our applicants and candidates for consideration or placement.
Attention Candidates
Please be aware that there have been reports of individuals falsely claiming to represent Ventra Health or one of our affiliated entities Ventra Health Private Limited and Ventra Health Global Services. These scammers may attempt to conduct fake interviews, solicit personal information, and, in some cases, have sent fraudulent offer letters.
To protect yourself, verify any communication you receive by contacting us directly through our official channels. If you have any doubts, please contact us at [HIDDEN TEXT] to confirm the legitimacy of the offer and the person who contacted you. All legitimate roles are posted on https://ventrahealth.com/careers/.
Statement of Accessibility
Ventra Health is committed to making our digital experiences accessible to all users, regardless of ability or assistive technology preferences. We continually work to enhance the user experience through ongoing improvements and adherence to accessibility standards. Please review at https://ventrahealth.com/statement-of-accessibility/.","Matillion, snowflake, dbt, Fivetran, ELT, Devops, Azure Data Factory, Spark Streaming, Powerbi, Apache Kafka, Ansi Sql, Talend, Azure, Python, Etl"
Senior Data Engineer,DataHavn,5-10 Years,,"Delhi, India",Login to check your skill match score,"Position: Senior Data Engineer
Experience Required: 5 10 years
Location: Noida
Employment Type: Full-time
Responsibilities:
* Data Pipeline Development:
Build and maintain scalable data pipelines to extract, transform, and load (ETL) data from various sources (e.g., databases, APIs, files) into data warehouses or data lakes.
* Data Infrastructure:
Design, implement, and manage data infrastructure components including data warehouses, data lakes, and data marts.
* Data Quality:
Ensure high data quality through data validation, cleansing, and standardization processes.
* Performance Optimization:
Optimize data pipelines and infrastructure for performance, scalability, and efficiency.
* Collaboration:
Work closely with data analysts, data scientists, and business stakeholders to understand data needs and translate them into technical requirements.
* Tool and Technology Selection:
Evaluate and select appropriate tools and technologies for data engineering (e.g., SQL, Python, Spark, Hadoop, cloud platforms).
* Documentation:
Create and maintain clear, comprehensive documentation for data pipelines, infrastructure, and processes.
Required Skills:
* Strong proficiency in SQL and at least one programming language (e.g., Python, Java).
* Hands-on experience with data warehousing and data lake technologies (e.g., Snowflake, AWS Redshift, Databricks).
* Solid understanding of cloud platforms (e.g., AWS, GCP, Azure) and their data services.
* Knowledge of data modeling and data architecture principles.
* Familiarity with ETL/ELT tools and frameworks.
* Excellent problem-solving and analytical skills.
* Ability to work independently as well as collaboratively in a team environment.
Preferred Qualifications:
* Experience with real-time data processing and streaming technologies (e.g., Kafka, Flink).
* Understanding of machine learning and AI concepts.
* Experience with data visualization tools (e.g., Tableau, Power BI).
* Certifications in cloud platforms or data engineering.","ETL ELT tools, snowflake, AWS GCP Azure, Java, Databricks, Redshift, Python, Sql, AWS"
Data Engineer-Dataiku/Python,InfoCepts,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Data Engineer (Python/Dataiku/Spark/SAS DI)
Location: Bangalore/Chennai
Purpose of the Position:
As a Data Engineer, this position requires candidate who are enthusiastic about specialized skills in any DE tools like Python/Dataiku/Spark/SAS DI and features. As a member of the team, you will help our clients, by building data pipelines that supports to progress on their Data and Analytics journey.
Key Result Areas and Activities:
Driving Data Engineering Initiatives:
Work with clients to understand and define the data initiatives based on business needs.
Review all project architectures/codes and contribute towards the development of the project delivery.
Data Integration and ETL Development:
Design, develop, and implement data integration solutions using Data Engineering tools like Python/Dataiku/Spark/SAS data integration.
Develop and maintain automated data pipelines for real-time and scheduled data ingestions.
Performance Tuning and Optimization:
Conduct performance tuning and optimization of queries and data models.
Optimize data workflows for efficiency and scalability.
Database Design and Implementation:
Design and implement database objects such as tables, views, schemas, and stored procedures.
Ensure data quality and consistency across the data warehouse.
Collaboration and Communication:
Work closely with cross-functional teams to gather requirements and translate them into technical specifications.
Collaborate seamlessly with clients across multiple geographies.
Problem Solving and Troubleshooting:
Troubleshoot and resolve data-related issues.
Apply strong analytical and problem-solving abilities to ensure data quality.
Drive the Project & Technical Team:
Work with client to gather the data requirements.
Provide required technical support to development team while delivering the code/project.
Manage end to project deliverables as technical lead/spoc for client and Infocepts.
Essential Skills:
Experience on data engineering tools like Python/Dataiku/Spark/SAS data integration.
Collate data from various data sources and implement business logic while loading into data mart layer.
Must have strong SQL experience and extract data from various databases and perform required analysis on data.
Should have BI knowledge to understand the analytics needs of business users.
Communication with business users to understand the requirements. Able to convert the business requirements into technical requirements.
Understanding of data modeling concepts, including relational and dimensional modeling.
Desirable Skills:
Good analytical skill to perform data analysis for different banking products data
Experience on any BI tools like Microstrategy, Power BI
Qualifications:
3+ years of experience to Design and develop efficient and scalable data integration/engineering solutions to support our organization's data analytics and reporting needs.
Technical certifications that attribute to continuous learning aspirations.
Qualities:
Influences and implements change with confidence and sound decisions.
Tackles problems head-on with a logical, systematic, and practical approach; follows up with developers.
Consults, writes, and presents persuasively.
Works effectively in self-organized, cross-functional teams.
Iterates based on new information, peer reviews, and feedback.
Collaborates seamlessly with clients across geographies.
Proficient in English (read/write/speak) and email communication.","Dataiku, Data Modeling, Bi Tools, SAS, Spark, Data Integration, Python, Sql, Etl Development, Database Design"
Azure Fabric Data Engineer,Adastra,6-8 Years,,India,Login to check your skill match score,"Job Summary:
We are seeking a highly skilledAzureFabricDataEngineerto lead agile, lab-style development efforts in collaboration directly with business stakeholders. This is a senior-level role requiring deep expertise inMicrosoftFabric,dataengineering, and a solid understanding ofbusiness intelligencepractices. The ideal candidate is a self-starter who can independently developdatasolutions, rapidly prototype, and iterate based on business feedback.
Job Description:
Design, develop, and deploydatapipelines and solutions withinMicrosoftFabric.
Collaborate closely with business users to rapidly develop and iterate ondata-driven prototypes.
Translate business requirements into technical specifications and scalabledatamodels.
Build and manageDataflows, Lakehouses, Pipelines, and NotebooksinFabric.
Ensuredataquality, reliability, and performance across solutions.
Supportdatavisualization needs and basic BI reporting (Power BI or equivalent).
Maintain thorough documentation of processes, architectures, and models.
Required Skills & Experience:
6+ years of experience inDataEngineeringwith a focus onAzureservices.
Hands-on experience withMicrosoftFabric(including Lakehouse,DataFactory (Pipelines), Notebooks, etc.).
Proficient inSQL,Python, andPower Query/M.
Strong understanding ofDataWarehousing,ETL/ELT, andLakehouse architecture.
Experience working inagile, business-facing development environments.
Familiarity withPower BIand building foundational reports/dashboards.
Immediate joiner would be an advantage
To apply for a job, please send your CV to [HIDDEN TEXT]
FRAUD ALERT: Be cautious of fake job postings and individuals posing as Adastra employees.
HOW TO VERIFY IT'S US:
Our employees will only use email addresses ending in @adastragrp.com. Any other domains, even if similar, are not legitimate.
We will never request any form of payment, including but not limited to fees, certification costs, or deposits.
Please reach out to [HIDDEN TEXT] only if you have any questions.","Lakehouse architecture, Power Query M, Power Bi, Python, Sql, ELT, Etl"
Sigma Computing - Data Engineer / Business Intelligence Engineer,Oncorre Inc,Fresher,,"Pune, India",Login to check your skill match score,"Company Description
Oncorre, Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey, USA. Established in 2007, the company provides cutting-edge engineering solutions for Fortune companies and Government Agencies. Oncorre's mission is to help enterprises accelerate the adoption of new technologies, resolve complex issues that arise during digital evolution, and promote ongoing innovation. We serve clients across the US with flexible onsite and offsite models tailored to their needs.
*** Experience in Sigma computing is must
Overview:
Oncorre is seeking a talented [Data Engineer/BI Engineer] to develop and optimize data solutions that empower business insights using Sigma Computing. You will work with modern cloud data platforms to build scalable pipelines, enable data exploration, and support analytics initiatives.
Key Responsibilities:
Design and develop data models, reports, and visualizations to support business insights.
Build and maintain robust data pipelines and workflows using cloud data platforms like Snowflake.
Collaborate with product, data, and engineering teams to gather requirements and implement solutions.
Optimize queries and data processes for performance and accuracy.
Assist in establishing data governance, security, and compliance standards.
Document data workflows, architecture, and best practices.
Stay informed about new data technologies and incorporate them into workflows.
Skills & Qualifications:
Experience with cloud data warehouses like Snowflake, Redshift, or BigQuery.
Proficiency in SQL, Python, or other relevant scripting languages.
Familiarity with ETL/ELT tools and data pipeline orchestration (e.g., Airflow).
Strong understanding of data modeling, analytics, and data visualization.
Excellent problem-solving, communication, and collaborative skills.
Experience working in a fast-paced, data-driven environment.","Airflow, snowflake, Sigma Computing, BigQuery, Redshift, Sql, Python, ELT, Etl"
Data Engineer/BI Engineer,canibuild,4-6 Years,,India,Login to check your skill match score,"About Us
Canibuild automates the residential construction industry's design, approval, and sales processes, allowing clients to answer Can I build this on this plot of land instantly. As a fast-growing SaaS platform backed by Australia's largest hedge fund, we serve clients across Australia, New Zealand, Canada, and the US.
Role Overview
We are seeking a highly skilled Data Engineer / BI Engineer with experience in designing ETL/ELT pipelines, managing databases, and developing dashboards using Power BI. The ideal candidate will be responsible for handling end-to-end data workflows, from data extraction and transformation to visualization and insights generation. This role also provides opportunities to upskill and contribute to AI-driven data initiatives, making it an excellent fit for professionals eager to grow in a modern data ecosystem.
Key Responsibilities
Design, develop, and maintain ETL/ELT pipelines using Python, Airflow, and SQL in an AWS environment
Manage and optimize data lake and data warehouse solutions on AWS
Develop and maintain data-driven dashboards and reports in Power BI, connecting to SQL Server and PostgreSQL Aurora databases
Extract and integrate data from third-party APIs to populate the data lake
Perform data profiling and source system analysis to ensure data quality and integrity
Collaborate with business stakeholders to capture and understand data requirements
Optimize SQL queries for performance and ensure efficient database operations
Implement best practices for data engineering, visualization, and database management
Participate in architectural decisions and contribute to the continuous improvement of data solutions
Follow agile and lean development practices for efficient project execution
Independently validate and assess the accuracy of data outputs before delivery, ensuring results align with business expectations
Proactively evaluate different technical approaches and suggest alternatives to ensure optimal outcomes within the broader system and business context
Stay updated on the latest AI and data engineering advancements, with opportunities to apply AI-powered solutions
Requirements
4+ years of experience in data engineering, ETL/ELT pipeline development, and database management
Strong expertise in SQL (T-SQL, MS SQL) with a focus on query optimization and database performance tuning
Proficiency in Python (including data-specific libraries such as Pandas, NumPy, etc.) and Airflow for ETL/ELT processes
Experience extracting and managing data from third-party APIs (REST, JSON)
Proven experience in designing and developing data warehouse solutions on AWS
Strong expertise in Power BI for data visualization, dashboard creation, and connecting to SQL Server/PostgreSQL Aurora
Familiarity with agile methodologies and a continuous improvement mindset
Demonstrated ability to think critically and evaluate multiple technical solutions in the broader context of system architecture and business goals
Excellent problem-solving skills and the ability to proactively identify and implement alternative solutions
Strong communication skills and ability to work collaboratively in a team-oriented environment
Willingness to upskill in AI-driven data solutions and contribute to AI-powered applications
Preferred Qualifications
Exposure to AWS cloud data services such as RedShift, Athena, Lambda, Glue, etc
Experience with additional BI tools like Tableau
Knowledge of data lake architectures and best practices
Experience with AI-driven data analytics, or integrating AI models with BI solutions
Benefits
Work with a talented team of data and AI professionals on meaningful, industry-transforming projects
Opportunity to gain hands-on experience with AI technologies and modern data engineering practices
Competitive salary, benefits, and opportunities for growth
A collaborative, fast-paced, and innovative culture that values initiative, ownership, and smart solutions
Flexible remote work opportunities","Airflow, Aurora, Data Warehouse, T-sql, Power Bi, PostgreSQL, Json, Sql, ELT, MS SQL, REST, Data Lake, Python, Etl"
Staff Data Engineer,Zinnia,6-8 Years,,"Pune, India",Login to check your skill match score,"Who We Are
Zinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.
Who You Are
As a seasoned Data Engineer specializing in data engineering, you bring extensive expertise in optimizing data workflows using various database tools like Oracle, BigQuery, and SQL Server. You possess a deep understanding of ELT/ETL processes, data integration, and have a strong command of Python for data manipulation and automation tasks. You will possess advanced expertise in working with data platforms like Google Big Query, DBT, Python, and Airflow. Responsible for designing and maintaining scalable ETL pipelines, optimizing complex data systems, and ensuring smooth data flow across different platforms. As a Senior Data Engineer, you will also be required to work collaboratively in a team and contribute to building data infrastructure that drives business insights
What You'll Do
Design, develop, and optimize complex ETL pipelines that integrate large data sets from various sources.
Build and maintain high-performance data models using Google BigQuery and DBT for data transformation.
Develop Python scripts for data ingestion, transformation, and automation.
Implement and manage data workflows using Apache Airflow for scheduling and orchestration.
Collaborate with data scientists, analysts, and other stakeholders to ensure data availability, reliability, and performance.
Troubleshoot and optimize data systems, identifying issues and resolving them proactively.
Work on cloud-based platforms, particularly AWS, to leverage scalability and storage options for data pipelines.
Ensure data integrity, consistency, and security across systems.
Take ownership of end-to-end data engineering tasks while mentoring junior team members.
Continuously improve processes and technologies for more efficient data processing and delivery.
Act as a key contributor to developing and supporting complex data architectures.
What You'll Need
Bachelor's degree in computer science, Information Technology, or a related field.
6+ years of hands-on experience in Data Engineering or related fields, with a strong background in building and optimizing data pipelines
Strong proficiency in Google Big Query, including designing and optimizing queries.
Advanced knowledge of DBT for data transformation and model management.
Proficiency in Python for data engineering tasks, including scripting, data manipulation, and automation.
Solid experience with Apache Airflow for workflow orchestration and task automation.
Extensive experience in building and maintaining ETL pipelines.
Familiarity with cloud platforms, particularly AWS (Amazon Web Services), including tools like S3, Lambda, Redshift, or Glue.
Java knowledge is a plus.
Excellent problem-solving and troubleshooting abilities.
Strong communication and collaboration skills with the ability to work effectively in a team environment.
Self-motivated, detail-oriented, and able to work with minimal supervision.
Ability to manage multiple priorities and deadlines in a fast-paced environment.
Experience with other cloud platforms (e.g., GCP, Azure) is a plus.
Knowledge of data warehousing best practices and architecture.
WHAT'S IN IT FOR YOU
At Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability","dbt, Google BigQuery, Apache Airflow, SQL Server, Oracle, Python, Etl, AWS"
Data Engineer II,Tekion Corp,3-5 Years,,"Chennai, India",Login to check your skill match score,"About Tekion:
Positively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.
Key Responsibilities:
Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.
Should be able to design (high level / low level) software solutions for the new requirements.
Coding independently and with other team members with proper software industry standard best practices.
Collaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.
Qualifications:
B.E/MTech in computer science
3 - 5 years of relevant work experience.
Experience in building scalable products with preferably big data.
Excellent Python coding skills (Mandatory)
Experience in Apache spark, Data Lake and other Big data technologies.
Experience in either Data Warehouses or Relational Database is mandatory.
Experience in AWS cloud
Mandatory Skills: Python , Spark
Tekion is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, victim of violence or having a family member who is a victim of violence, the intersectionality of two or more protected categories, or other applicable legally protected characteristics.
For more information on our privacy practices, please refer to our Applicant Privacy Notice here.","data warehouses, Relational Database, Apache Spark, Data Lake, Python, Aws Cloud, Big Data Technologies"
ML Data Engineer,S&P Global,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"About The Role
Grade Level (for internal use):
10
Responsibilities
To work closely with various stakeholders to collect, clean, model and visualise datasets.
To create data driven insights by researching, designing and implementing ML models to deliver insights and implement action-oriented solutions to complex business problems
To drive ground-breaking ML technology within the Modelling and Data Science team.
To extract hidden value insights and enrich accuracy of the datasets.
To leverage technology and automate workflows creating modernized operational processes aligning with the team strategy.
To understand, implement, manage, and maintain analytical solutions & techniques independently.
To collaborate and coordinate with Data, content and modelling teams and provide analytical assistance of various commodity datasets
To drive and maintain high quality processes and delivering projects in collaborative Agile team environments.
Requirements
7+ years of programming experience particularly in Python
4+ years of experience working with SQL or NoSQL databases.
1+ years of experience working with Pyspark.
University degree in Computer Science, Engineering, Mathematics, or related disciplines.
Strong understanding of big data technologies such as Hadoop, Spark, or Kafka.
Demonstrated ability to design and implement end-to-end scalable and performant data pipelines.
Experience with workflow management platforms like Airflow.
Strong analytical and problem-solving skills.
Ability to collaborate and communicate effectively with both technical and non-technical stakeholders.
Experience building solutions and working in the Agile working environment
Experience working with git or other source control tools
Strong understanding of Object-Oriented Programming (OOP) principles and design patterns.
Knowledge of clean code practices and the ability to write well-documented, modular, and reusable code.
Strong focus on performance optimization and writing efficient, scalable code.
Nice To Have
Experience working with Oil, gas and energy markets
Experience working with BI Visualization applications (e.g. Tableau, Power BI)
Understanding of cloud-based services, preferably AWS
Experience working with Unified analytics platforms like Databricks
Experience with deep learning and related toolkits: Tensorflow, PyTorch, Keras, etc.
About S&P Global Commodity Insights
At S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.
We're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.
S&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.
For more information, visit http://www.spglobal.com/commodity-insights.
What's In It For You
Our Purpose
Progress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.
Our world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.
Our People
We're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.
From finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.
Our Values
Integrity, Discovery, Partnership
At S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.
Benefits
We take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.
Our Benefits Include
Health & Wellness: Health care coverage designed for the mind and body.
Flexible Downtime: Generous time off helps keep you energized for your time on.
Continuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.
Invest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.
Family Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.
Beyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.
For more information on benefits by country visit: https://spgbenefits.com/benefit-summaries
Inclusive Hiring And Opportunity At S&P Global
At S&P Global, we are committed to fostering an inclusive workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and equal opportunity, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.
Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.
If you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.
US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_ English_formattedESQA508c.pdf
IFTECH202.1 - Middle Professional Tier I (EEO Job Group)
Job ID: 314321
Posted On: 2025-04-23
Location: Hyderabad, Telangana, India","Airflow, Object-Oriented Programming, Pyspark, Hadoop, Kafka, Sql, Tensorflow, Nosql, Git, Pytorch, Spark, Keras, Python"
Data Engineer MS Fabric_8+years,Zorba AI,8-10 Years,,India,Login to check your skill match score,"Company Overview
Zorba Consulting India is a leading consultancy firm focused on delivering innovative solutions and strategies to enhance business performance. With a commitment to excellence, we prioritize collaboration, integrity, and customer-centric values in our operations. Our mission is to empower organizations by transforming data into actionable insights and enabling data-driven decision-making. We are dedicated to fostering a culture of continuous improvement and supporting our team members professional development.
Role Responsibilities
Design and implement data pipelines using MS Fabric.
Develop data models to support business intelligence and analytics.
Manage and optimize ETL processes for data extraction, transformation, and loading.
Collaborate with cross-functional teams to gather and define data requirements.
Ensure data quality and integrity in all data processes.
Implement best practices for data management, storage, and processing.
Conduct performance tuning for data storage and retrieval for enhanced efficiency.
Generate and maintain documentation for data architecture and data flow.
Participate in troubleshooting data-related issues and implement solutions.
Monitor and optimize cloud-based solutions for scalability and resource efficiency.
Evaluate emerging technologies and tools for potential incorporation in projects.
Assist in designing data governance frameworks and policies.
Provide technical guidance and support to junior data engineers.
Participate in code reviews and ensure adherence to coding standards.
Stay updated with industry trends and best practices in data engineering.
Qualifications
8+ years of experience in data engineering roles.
Strong expertise in MS Fabric and related technologies.
Proficiency in SQL and relational database management systems.
Experience with data warehousing solutions and data modeling.
Hands-on experience in ETL tools and processes.
Knowledge of cloud computing platforms (Azure, AWS, GCP).
Familiarity with Python or similar programming languages.
Ability to communicate complex concepts clearly to non-technical stakeholders.
Experience in implementing data quality measures and data governance.
Strong problem-solving skills and attention to detail.
Ability to work independently in a remote environment.
Experience with data visualization tools is a plus.
Excellent analytical and organizational skills.
Bachelor's degree in Computer Science, Engineering, or related field.
Experience in Agile methodologies and project management.
Skills: etl processes,sql,python scripting,data integration,performance tuning,cloud technologies,data quality measures,data quality assurance,data modeling,cloud computing (azure, aws, gcp),python,ms fabric,data governance,databricks,data visualization tools,data warehousing","data visualization tools, MS Fabric, ETL processes, Performance Tuning, Data Warehousing, Data Modeling, Sql, Gcp, Data Governance, Azure, Python, AWS, Cloud Computing"
AWS Data Engineer,"GSPANN Technologies, Inc",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"About GSPANN
GSPANN is a global IT services and consultancy provider headquartered in Milpitas, California (U.S.A.). With five global delivery centers across the globe, GSPANN provides digital solutions that support the customer buying journeys of B2B and B2C brands worldwide.
With a strong focus on innovation and client satisfaction, GSPANN delivers cutting-edge solutions that drive business success and operational excellence. GSPANN helps retail, finance, manufacturing, and high-technology brands deliver competitive customer experiences and increased revenues through our solution delivery, technologies, practices, and operations for each client. For more information, visit www.gspann.com
JD for your reference:
GSPANN is looking for AWS Data Engineer. As we march ahead on a tremendous growth trajectory, we seek passionate and talented professionals to join our growing family.
Job Position-
AWS Data Engineer
Experience- 6+ years
Location- Bangalore
Skills- AWS+Redshift+Snowflake, SQL, Bigdata, StepFunction, Python/ PySpark, Airflow
Responsibilities
Actively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing, roll-out, and support.
Solve complex business problems by utilizing a disciplined development methodology.
Produce scalable, flexible, efficient, and supportable solutions using appropriate technologies.
Analyse the source and target system data. Map the transformation that meets the requirements.
Interact with the client and onsite coordinators during different phases of a project.
Design and implement product features in collaboration with business and Technology stakeholders.
Anticipate, identify, and solve issues concerning data management to improve data quality.
Clean, prepare, and optimize data at scale for ingestion and consumption.
Support the implementation of new data management projects and re-structure the current data architecture.
Implement automated workflows and routines using workflow scheduling tools.
Understand and use continuous integration, test-driven development, and production deployment frameworks.
Participate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards.
Analyze and profile data for the purpose of designing scalable solutions.
Troubleshoot straightforward data issues and perform root cause analysis to proactively resolve product issues.
Required Skills
6+ years experience developing Data and analytic solutions.
Experience building data lake solutions leveraging one or more of the following AWS, EMR, S3, Hive & PySpark
Experience with relational SQL
Experience with scripting languages such as Python
Experience with source control tools such as GitHub and related dev process
Experience with workflow scheduling tools such as Airflow
In-depth knowledge of AWS Cloud (S3, EMR, Databricks)
Has a passion for data solutions.
Has a strong problem-solving and analytical mindset
Working experience in the design, Development, and test of data pipelines.
Experience working with Agile Teams.
Able to influence and communicate effectively, both verbally and in writing, with team members and business stakeholders
Able to quickly pick up new programming languages, technologies, and frameworks.
Bachelor's Degree in computer science
Why Choose GSPANN
At GSPANN, we don't just serve our clientswe co-create. The GSPANNians are passionate technologists who thrive on solving the toughest business challenges, delivering trailblazing innovations for marquee clients. This collaborative spirit fuels a culture where every individual is encouraged to sharpen their skills, feed their curiosity, and take ownership to learn, experiment, and succeed.
We believe in celebrating each other's successesbig or smalland giving back to the communities we call home. If you're ready to push boundaries and be part of a close-knit team that's shaping the future of tech, we invite you to carry forward the baton of innovation with us.
Let's Co-Create the FutureTogether.
Discover Your Inner Technologist
Explore and expand the boundaries of tech innovation without the fear of failure.
Accelerate Your Learning
Shape your career while scripting the future of tech. Seize the ample learning opportunities to grow at a rapid pace.
Feel Included
At GSPANN, everyone is welcome. Age, gender, culture, and nationality do not matter here, what matters is YOU.
Inspire and Be Inspired
When you work with the experts, you raise your game. At GSPANN, you're in the company of marquee clients and extremely talented colleagues.
Enjoy Life
We love to celebrate milestones and victories, big or small. Ever so often, we come together as one large GSPANN family.
Give Back
Together, we serve communities. We take steps, small and large so we can do good for the environment, weaving in sustainability and social change in our endeavors.
We invite you to carry forward the baton of innovation in technology with us.
Let's Co-Create
GSPANN | Consulting Services, Technology Services, and IT Services Provider
GSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.
GSPANN | Consulting Services, Technology Services, and IT Services Provider
GSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.","Airflow, snowflake, StepFunction, Pyspark, Bigdata, Redshift, Python, Sql, AWS"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position
This role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.
Key Accountabilities
Assist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.
Work closely with senior engineers and analysts to gather requirements and build working prototypes.
Support data integration from internal, third-party, and public sources.
Participate in developing and maintaining Data Warehouse schemas.
Contribute to documentation and testing efforts to ensure data reliability.
Learn and apply data standards and governance practices as guided by the team.
Skills and Experience | Essential
Knowledge of data engineering concepts and data structures.
Exposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.
Understanding of ETL processes and data pipelines.
Ability to work collaboratively in an Agile/Kanban team environment.
Microsoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage
Skills and Experience | Desirable
Familiarity with Medallion Architecture principles.
Exposure to MS Purview or other data governance tools.
Understanding of data warehousing and reporting concepts.
Interest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
"Lead, Big Data Engineer",Qualys,10-12 Years,INR 40 - 52 LPA,"Pune, India",Login to check your skill match score,"Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!
Job Description:
We are seeking a talented Lead Big Data Engineer to deliver roadmap features of Unified Asset Inventory. This is a great opportunity to be an integral part of a team building Qualys next generation Micro-Services based platform processing over a 100 million transactions and terabytes of data per day, leverage open-source technologies, and work on challenging and business-impacting projects.
Responsibilities:
You will be building the Unified Asset Management product in the cloud
You will be building highly scalable Micro-services that interacts with Qualys Cloud Platform. Research, evaluate and adopt next generation technologies
Produce high quality software following good architecture and design principles that you and your team will find easy to work with in the future
This is a fantastic opportunity to be an integral part of a team building Qualys next generation platform using Big Data & Micro-Services based technology to process over billions of transactions data per day, leverage open-source technologies, and work on challenging and business-impacting initiatives.
Qualifications:
Bachelor's degree in computer science or equivalent
10+ years of total experience.
4+ years of relevant experience in design and architecture Big Data solutions using Spark
3+ years experience in working with engineering resources for innovation.
4+ years experience in understanding Big Data events flow pipeline.
3+ years experience in performance testing for large infrastructure.
3+ In depth experience in understanding various search solutions solr/elastic.
3+ years experience in Kafka
In depth experience in Data lakes and related ecosystems.
In depth experience of messing queue
In depth experience in giving requirements to build a scalable architecture for Big data and Micro-services environments.
In depth experience in understanding caching components or services
Knowledge in Presto technology.
Knowledge in Airflow.
Hands-on experience in scripting and automation
In depth understanding of RDBMS/NoSQL, Oracle , Cassandra , Kafka , Redis, Hadoop, lambda architecture, kappa , kappa ++ architectures with flink data streaming and rule engines
Experience in working with ML models engineering and related deployment.
Design and implement secure big data clusters to meet many compliances and regulatory requirements.
Experience in leading the delivery of large-scale systems focused on managing the infrastructure layer of the technology stack.
Strong experience in doing performance benchmarking testing for Big data technologies.
Strong troubleshooting skills.
Experience leading development life cycle process and best practices
Experience in Big Data services administration would be added value.
Experience with Agile Management (SCRUM, RUP, XP), OO Modeling, working on internet, UNIX, Middleware, and database related projects.
Experience mentoring/training the engineering community on complex technical issue.","Airflow, OO Modeling, Caching Components, Flink, ML Models Engineering, Elastic, Data Lakes, agile management, Xp, Micro-services, Performance Benchmarking, Scripting, Cassandra, Kafka, Nosql, RDBMS, Oracle, Hadoop, Solr, Big Data, Scrum, Middleware, Automation, Redis, UNIX, Rup, Lambda Architecture, Presto, Spark"
"Senior Data Engineer( Snowflake, DBT)","Enterprise Minds, Inc",5-7 Years,,India,Login to check your skill match score,"Hiring: Senior Data Engineer (Strong Snowflake, SQL, and DBT Expertise) | Remote
We are looking for a senior data engineer with strong expertise in Snowflake and SQL to join our team. If you have a passion for data engineering, this is your perfect role!
Location: Remote
Experience: 5+ Years
Employment Type: Full-Time
Key Responsibilities:
Develop and maintain scalable data pipelines using DBT and orchestrate workflows (ideally with Airflow).
Work with Snowflake to model data and optimize queries for performance.
Apply strong SQL skills to extract, transform, and load data.
Interpret and understand Python scripts used within data workflows.
Design data models with a clear understanding of relational database concepts (primary/foreign keys, normalization, etc.).
Implement unit testing and data quality tests using DBT best practices.
Collaborate using Git/GitHub and contribute to a CI/CD pipeline for version control and deployment.
Required Skills:
Strong SQL expertise (query optimization, joins, CTEs, etc.)
Experience with Snowflake or similar cloud data warehouses.
Working knowledge of Python (reading/debugging existing code is sufficient).
Deep understanding of DBT, including:
Project structure, models, macros, hooks, and YAML configuration
Data testing: generic, singular, custom, and unit tests
Proficiency in Git/GitHub for version control.
Familiarity with CI/CD workflows in data projects.
Nice to Have:
Experience with Apache Airflow for pipeline orchestration.
Understanding of financial data concepts (useful for data validation and table design).
Apply Now! Send your resume to [HIDDEN TEXT]","snowflake, dbt, Git, Sql, Apache Airflow, Python, Github"
"Senior Data Engineer ( python , Java and SQL , Data pipelines )",NielsenIQ,7-9 Years,,India,IT/Computers - Software,"Job Description
Senior Data Engineer
Mission of the Role
You are a passionate and skilled data engineer who enjoys building scalable, production-grade data pipelines that enable machine learning and AI at scale. You thrive on solving complex data challenges and understand how critical a robust data pipeline is to the success of any predictive model.
As a Senior Data Engineer on the Forecasting team, you will be responsible for designing, implementing, and maintaining the data pipelines that power deep neural network models used for global sales and revenue forecasting. You will work closely with ML engineers, data scientists, and product stakeholders to build automated, secure, and cost-efficient pipelines using GCP-native services.
You take pride in writing clean, testable, and production-ready code, and are comfortable operating in cloud environments. You know how to make data pipelines observable, recoverable, and performant. You are also comfortable taking architectural ownership and contributing to long-term platform improvements.
You will:
Design and build robust, scalable, and maintainable data pipelines for training and inference of deep learning models used in forecasting and other Machine Learning based algorithms.
Develop and orchestrate end-to-end workflows using GCP-native tools such as Cloud Workflows, Cloud Run, BigQuery, Cloud Functions, and Pub/Sub.
Automate and maintain the pipeline lifecycle, including data extraction, transformation, loading (ETL/ELT), and triggering model runs.
Monitor and Optimise performance, cost-efficiency, and data freshness across all stages of the pipeline.
Collaborate with data scientists to productionise AI/ML models and integrate them seamlessly into business workflows.
Own the data ingestion, validation, and transformation logic powering the forecasting models, ensuring high-quality, consistent input data.
Handle deployment, versioning, and configuration of pipeline components using Docker, Git, and CI/CD practices.
Implement logging, monitoring, and alerting to ensure reliability and traceability in a high-scale environment.
Review code, mentor junior engineers, and help define best practices in our evolving data engineering stack.
Qualifications
You have:
7+ years of experience in data engineering or backend engineering roles.
Strong expertise in Python and SQL, with experience building production-grade data pipelines.
Solid understanding of Docker, Git, and shell scripting in Linux environments.
Hands-on experience with GCP services
Experience in building, deploying, and maintaining data workflows that feed AI/ML models.
Familiarity with model lifecycle management and infrastructure challenges in ML pipelines.
Proficiency in setting up CI/CD pipelines for data and ML systems using GitLab CI, Cloud Build, or similar tools.
Exposure to Java for backend services or pipeline components (even if not primary language).
A proactive, collaborative mindset and strong communication skills across engineering and data science teams.
Nice to have:
Exposure to forecasting or time series modelling pipelines.
Experience with event-driven architectures.
Familiarity with infrastructure-as-code tools like Terraform
Understanding of data quality frameworks and observability tools
Knowledge of model versioning tools and experiment tracking systems
Additional Information
Why Join us
You'll be working on a high-impact platform that forecasts consumer demand and drives business-critical decisions globally
You'll be part of a tight-knit, collaborative, and innovative Forecasting & AI team within a leading global data science organisation
You'll have access to top-tier infrastructure, GCP services, and challenging real-world problems in predictive modelling
Flexible working hours, remote-friendly culture, and strong focus on personal and professional growth
Competitive compensation and performance-based bonuses
Our Benefits
Flexible working environment
Volunteer time off
LinkedIn Learning
Employee-Assistance-Program (EAP)
About NIQ
NIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights-delivered with advanced analytics through state-of-the-art platforms-NIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.
For more information, visit NIQ.com
Want to keep up with our latest updates
Follow us on: | | |
Our commitment to Diversity, Equity, and Inclusion
NIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center:","Cloud Run, Cloud Workflows, GCP-native tools, Pub Sub, Cloud Functions, Python, Sql, BigQuery, Shell scripting, Java, Docker, Git"
Data Engineer -Pharma Commerical Domain,Predigle,3-5 Years,,"Chennai, India",Login to check your skill match score,"We are seeking an experiencedData Engineerwith a strong background inPharma Commercial Datato join our growing data team. The ideal candidate will have hands-on experience withAzure Databricks,Snowflake, and a deep understanding of pharmaceutical commercial datasets, including sales, claims, and HCP-level data.
Key Responsibilities
Design, build, and maintain scalable and efficient data pipelines focused onPharma Commercial datasets
Develop data integration workflows usingAzure Databricksand manage data warehousing inSnowflake.
Work closely with business stakeholders, analytics teams, and data scientists to ensure data solutions support strategic commercial initiatives.
Ensure data quality, consistency, and governance across all pharma commercial datasets.
Automate routine data processes and monitor pipeline performance for production stability.
Participate in architecture reviews and recommend improvements for performance and scalability.
Required Qualifications
Minimum3 years of hands-on experience with Pharma Commercial Data, including datasets like sales, prescription, claims, payer data, or CRM/HCP information.
Proven expertise inAzure DatabricksandSnowflake.
Strong proficiency inSQLandPythonfor data manipulation and transformation.
Solid understanding of data modeling, ETL/ELT frameworks, and cloud-based data engineering best practices.
Experience with data orchestration tools (e.g., Airflow, Azure Data Factory).
Ability to work independently and communicate effectively with business and technical teams.
Preferred Qualifications
Exposure to commercial analytics use cases like HCP segmentation, field force effectiveness, and targeting.
Familiarity with data privacy, HIPAA, and compliance regulations in the pharma domain.
Experience working in Agile teams and DevOps environments.
What We Offer
Opportunity to work on impactful commercial data initiatives in the pharmaceutical industry.
Competitive compensation and benefits.
Remote-friendly, flexible work culture.
Supportive team environment and continuous learning opportunities.
About Predigle:
Predigle, an EsperGroup company, is an American multinational organization focused on building a disruptive technology platform that revolutionizes the way businesses conduct their daily operations.
Predigle has grown rapidly to offer multiple products and services,As a growing startup, we offer an entrepreneurial work environment where ideas are valued, creativity is encouraged, and learning opportunities are immense.
https://espergroup.com
/ https://predigle.com/
https://www.linkedin.com/company/predigle/","Airflow, snowflake, data orchestration tools, Azure Data Factory, Azure Databricks, Python, Sql, Etl, ELT"
Senior Data Engineer,NovusPlatform.io,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"We are hiring Data Platform Engineers for our Hyderabad Office (Hybrid Mode)
Role: Data Platform Engineer | 5 positions
Loc.: Hyderabad
Work Mode: Hybrid (3 days a week)
- 6 to 8 years of exp.
- immediate joiner
- For a Healthcare Data Analytics client
Must have Hands-on Skills:
Proficient with Python Programming, Scala, SQL, Redshift
AWS: Experience with the AWS ecosystem
Terraform: Good Knowledge of Infrastructure as a code using Terraform
Jenkins: CI/CD pipelines using Jenkins
Docker: Containerization with Docker
Apply at the earliest:
[HIDDEN TEXT]
#HiringNow
#DataPlatformEngineer
#DataJobs
#HyderabadJobs
#ImmediateJoiners
#PythonJobs
#DataEngineering
#ITJobsIndia","Jenkins, Docker, Terraform, Scala, Redshift, Sql, AWS, Python Programming"
Data Engineer,PayPay India,4-6 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Job Description
PayPay's growth is driving a rapid expansion of PayPay product teams and the need for a robust Data Engineering Platform to support our growing business needs is more critical than ever. The DaaS team's responsibility is to design, implement, and operate this platform using cutting edge technologies such as Spark, Hudi, Delta Lake, Scala, and AWS suite of data tools.
We are looking for talented Data Engineers to join our team and help us scale our platform across the organizations.
Main Responsibilities
Design, develop, and maintain scalable data ingestion pipelines using Databricks, Airflow, Kafka, AWS Lambda, and Terraform.
Optimize and manage large scale data pipelines to ensure high performance, reliability, and efficiency.
Implement data processing workflows using Delta Lake, Databricks, Python, and Scala.
Design and maintain Databricks Unity Catalog for effective data management and discovery.
Collaborate with cross-functional teams to ensure seamless data flow and integration across the organization.
Implement best practices for observability, data governance, security, and compliance.
Qualifications
4+ years experience as a Data Engineer or in a similar role.
Hands-on experience with Delta Lake, Hudi, Spark, and Scala.
Experience designing, building, and operating a DataLake or Data Warehouse.
Knowledge of Data Orchestration tools such as Airflow, Dagster, Prefect.
Strong expertise in AWS services, including Glue, Step Functions, Lambda, and EMR.
Familiarity with change data capture tools like Canal, Debezium, and Maxwell.
Experience with data warehousing tools like Databricks, Snowflake, and BigQuery.
Experience in at least one primary language (e.g. Scala, Python) and SQL (any variant).
Experience with data cataloging and metadata management using Databricks Unity Catalog, AWS Glue Data Catalog, or Lakeformation.
Proficiency in Terraform for infrastructure as code (IaC).
Strong problem-solving skills and ability to troubleshoot complex data issues.
Excellent communication and collaboration skills.
Ability to work in a fast-paced, dynamic environment and manage multiple tasks simultaneously.","snowflake, Databricks Unity Catalog, Canal, Airflow, maxwell, Data Catalog, Delta Lake, Hudi, Debezium, Lakeformation, Sql, Databricks, AWS Glue, Aws Lambda, Kafka, BigQuery, AWS, Python, Terraform, Scala, Spark"
