job_title,company_name,experience,salary,location,industry,job_description,skills
Asst./Dy Manager - Data engineer,Ajanta Pharma,5-7 Years,,Mumbai,"Manufacturing, Pharmaceutical",Ajanta Pharma is looking for Asst./Dy Manager - Data engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed\nReformulating existing frameworks to optimize their functioning\nTesting such structures to ensure that they are fit for use\nPreparing raw data for manipulation by data scientists\nDetecting and correcting errors in your work\nEnsuring that your work remains backed up and readily accessible to relevant coworkers\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs,"NoSQL Databases, Cloud Computing, Etl Tools, Data Modeling, Data Warehousing, Sql, Python"
Consultant - Sr.Data Engineer (DBT+Snowflake),Genpact,Fresher,,Bengaluru,IT/Computers - Hardware & Networking,"Ready to shape the future of work\n\n\n\nAt Genpact, we don&rsquot just adapt to change&mdashwe drive it. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.\n\n\n\nIf you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that&rsquos shaping the future, this is your moment.\n\n\n\nGenpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.\n\n\n\nInviting applications for the role ofConsultant -Sr.Data Engineer (DBT+Snowflake)\n\n\n\n!\n\n\n\nIn this role, the Sr.Data Engineer is responsible for providing technical direction and lead a group of one or more developer to address a goal.\n\n\n\n\n\n\n\n\n\n\n\n\nJob Description:\n\n\n\n\n\nDevelop, implement, and optimize data pipelines using Snowflake, with a focus on Cortex AI capabilities.\n\n\n\n\n\n\n\nExtract, transform, and load (ETL) data from various sources into Snowflake, ensuring data integrity and accuracy.\n\n\n\n\n\n\n\nImplement Conversational AI solutions using Snowflake Cortex AI to facilitate data interaction through ChatBot agents.\n\n\n\n\n\n\n\nCollaborate with data scientists and AI developers to integrate predictive analytics and AI models into data workflows.\n\n\n\n\n\n\n\nMonitor and troubleshoot data pipelines to resolve data discrepancies and optimize performance.\n\n\n\n\n\n\n\nUtilize Snowflake's advanced features, including Snowpark, Streams, and Tasks, to enable data processing and analysis.\n\n\n\n\n\n\n\nDevelop and maintain data documentation, best practices, and data governance protocols.\n\n\n\n\n\n\n\nEnsure data security, privacy, and compliance with organizational and regulatory guidelines.\n\n\n\n\n\n\n\n\n\n\nResponsibilities:\n\n\n\n\n\n. Bachelor&rsquos degree in Computer Science, Data Engineering, or a related field.\n\n\n\n\n\n\n\n. experience in data engineering, with experience working with Snowflake.\n\n\n\n\n\n\n\n. Proven experience in Snowflake Cortex AI, focusing on data extraction, chatbot development, and Conversational AI.\n\n\n\n\n\n\n\n. Strongproficiency in SQL, Python, and data modeling.\n\n\n\n\n\n\n\n. Experience with data integration tools (e.g., Matillion, Talend, Informatica).\n\n\n\n\n\n\n\n. Knowledge of cloud platforms such as AWS, Azure, or GCP.\n\n\n\n\n\n\n\n. Excellent problem-solving skills, with a focus on data quality and performance optimization.\n\n\n\n\n\n\n\n. Strong communication skills and the ability to work effectively in a cross-functional team.\n\n\n\n\n\n\n\nProficiency in using DBT's testing and documentation features to ensure the accuracy and reliability of data transformations.\n\n\n\n\n\n\n\nUnderstanding of data lineage and metadata management concepts, and ability to track and document data transformations using DBT's lineage capabilities.\n\n\n\n\n\n\n\nUnderstanding of software engineering best practices and ability to apply these principles to DBT development, including version control, code reviews, and automated testing.\n\n\n\n\n\n\n\nShould have experience building data ingestion pipeline.\n\n\n\n\n\n\n\nShould have experience with Snowflake utilities such as SnowSQL, SnowPipe, bulk copy, Snowpark, tables, Tasks, Streams, Time travel, Cloning, Optimizer, Metadata Manager, data sharing, stored procedures and UDFs, Snowsight.\n\n\n\n\n\n\n\nShould have good experience in implementing CDC or SCD type 2\n\n\n\n\n\n\n\nProficiency in working with Airflow or other workflow management tools for scheduling and managing ETL jobs.\n\n\n\n\n\n\n\nGood to have experience in repository tools like Github/Gitlab, Azure repo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQualifications/Minimum qualifications\n\n\n\n\n\nB.E./ Masters in Computer Science, Information technology, or Computer engineering or any equivalent degree with good IT experience and relevant of working experience as a Sr. Data Engineer with DBT+Snowflake skillsets\n\n\n\n\n\n\n\nSkill Matrix:\n\n\n\n\n\n\n\n\n\n\nDBT (Core or Cloud), Snowflake, AWS/Azure, SQL, ETL concepts, Airflow or any orchestration tools, Data Warehousing concepts\n\n\n\n\n\n\n\n\n\n\n\nWhy join Genpact\n\n\n\n\n\nBe a transformation leader - Work at the cutting edge of AI, automation, and digital innovation\n\n\n\n\n\n\n\nMake an impact - Drive change for global enterprises and solve business challenges that matter\n\n\n\n\n\n\n\nAccelerate your career - Get hands-on experience, mentorship, and continuous learning opportunities\n\n\n\n\n\n\n\nWork with the best - Join 140,000+ bold thinkers and problem-solvers who push boundaries every day\n\n\n\n\n\n\n\nThrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress\n\n\n\n\n\nCome join the tech shapers and growth makers at Genpact and take your career in the only direction that matters: Up.\n\n\n\nLet&rsquos build tomorrow together.\n\n\n\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Senior Data Engineer,Adidas,7-12 Years,,Gurugram,Sporting Goods,"Data Lakehouse & Data Product Engineer\nPurpose & Overall Relevance for the Organization:\nBe part of a team building a state-of-the-art data lakehouse and create data products which are ingested from heterogeneous source systems.\nOpportunity to work in leading-edge technology like Databricks.\nCollaborate with multiple stakeholders to drive the data roadmap by translating stories into data pipelines.\nLearn and build knowledge around data domains and business processes.\nSet the mindset to lifeblood for adidas data-driven strategy by enabling the data backbone, powering AI and BI solutions that produce unseen insights for the entire company.\nWhat You Bring:\nExpert in SAP data models:\nExpert in SAP ECC and SAP S4HANA data models and processes. SAP process knowledge in sales and distribution (SD) is preferred.\nSAP BW Development Experience:\nStrong background in SAP BW Development, with knowledge of SAP ECC and S4HANA versions and their integration with data warehouse solutions.\nExposure to Spark Architecture:\nFamiliarity with Spark, PySpark, and Python is key, especially the willingness to learn and create well-documented, maintainable, and production-ready solutions.\nStrong ETL Build Knowledge:\nKnowledge of Slowly Changing Dimension (SCD) and heterogeneous data harmonization methods.\nExperienced with SQL:\nFoundation in writing and interpreting SQL queries is essential, with the expectation to handle more complex queries.\nFamiliarity with Data Tools:\nFamiliar with tools such as Databricks and AWS, and have knowledge of different storage formats like Delta, Parquet, Avro, etc.\nExcellent Analytical and Team Leadership Skills:\nConvince with excellent analytical skills, ability to work in a team, as well as lead a team of engineers.\nRequisite Education and Experience / Minimum Qualifications:\nFour-year college or university degree with focus on Business Administration or IT or related areas, or equivalent combination of education and experience.\nProficient spoken and written command of English.\nAt least 7 years of experience in IT.\n5 years of experience in relevant area.\n2 years of experience in team management.\nCOURAGE: Speak up when you see an opportunity; step up when you see a need.\nOWNERSHIP: Pick up the ball. Be proactive, take responsibility and follow-through.\nINNOVATION: Elevate to win. Be curious, test and learn new and better ways of doing things.\nTEAMPLAY: Win together. Work collaboratively and cultivate a shared mindset.\nINTEGRITY: Play by the rules. Hold yourself and others accountable to our company's standards.\nRESPECT: Value all players. Display empathy, be inclusive and show dignity to all.","SAP BW Development, SAP S4HANA, Pyspark, Spark, Sql, Sap Ecc"
Data Engineer,Vichara Technologies,6-12 Years,,"Delhi, Delhi NCR",Consulting,Key deliverables:\nEnhance and maintain the MDM platform\nMonitor and optimize system performance\nTroubleshoot and resolve technical issues\nSupport production incident resolution\nRole responsibilities:\nDevelop and refactor Python and SQL code\nIntegrate REST APIs within MDM workflows\nUtilize Azure Databricks and ADF for ETL tasks\nWork on Markit EDM or Semarchy-based solutions,"Rest Api, Adf, Azure Databricks, Python, Sql"
Data Engineer,Vichara Technologies,6-12 Years,,Hyderabad,Consulting,Key deliverables:\nEnhance and maintain the MDM platform\nMonitor and optimize system performance\nTroubleshoot and resolve technical issues\nSupport production incident resolution\nRole responsibilities:\nDevelop and refactor Python and SQL code\nIntegrate REST APIs within MDM workflows\nUtilize Azure Databricks and ADF for ETL tasks\nWork on Markit EDM or Semarchy-based solutions,"Rest Api, Adf, Azure Databricks, Python, Sql"
Data Engineer 3,Vichara Technologies,7-12 Years,,Bengaluru,Consulting,"Key deliverables:\nEnhance and maintain the MDM platform to support business needs\nDevelop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow\nMonitor and improve system performance and troubleshoot data pipeline issues\nResolve production issues and ensure platform reliability\nRole responsibilities:\nCollaborate with data engineering and analytics teams for scalable solutions\nApply DevOps practices to streamline deployment and automation\nIntegrate cloud-native tools and services (AWS, Azure) with the data platform\nUtilize dbt and version control (Git) for data transformation and management","Airflow, snowflake, dbt, Python, Sql, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Hyderabad,Consulting,"Role Responsibilities:\nBuild scalable data pipelines and applications from scratch on AWS\nDesign and manage data ingestion processes and workflows\nCollaborate with functional and data science teams to deliver data solutions\nOptimize data storage and organization using HDFS, S3, and Delta Lake\nJob Requirements:\nStrong programming experience in Python or Java\nHands-on experience with AWS services and deployment\nProficiency in HDFS, S3, SQL, and data ingestion tools\nExposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Bengaluru,Consulting,"Role Responsibilities:\nBuild scalable data pipelines and applications from scratch on AWS\nDesign and manage data ingestion processes and workflows\nCollaborate with functional and data science teams to deliver data solutions\nOptimize data storage and organization using HDFS, S3, and Delta Lake\nJob Requirements:\nStrong programming experience in Python or Java\nHands-on experience with AWS services and deployment\nProficiency in HDFS, S3, SQL, and data ingestion tools\nExposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Senior Data Engineer,Eagleview,5-10 Years,,Bengaluru,Software,"Role & responsibilities\nDesign, develop, and optimize scalable data pipelines for ETL/ELT processes.\nDevelop and maintain Python-based data processing scripts and automation tools.\nWrite and optimize complex SQL queries (preferably in Snowflake) for data transformation and analytics.\nExperience with Jenkins or other CI/CD tools.\nExperience developing with Snowflake as the data platform.\nExperience with ETL/ELT tools (preferably Fivetran, dbt).\nImplement version control best practices using Git or other tools to manage code changes.\nCollaborate with cross-functional teams (analysts, product managers, and engineers) to understand business needs and translate them into technical data solutions.\nEnsure data integrity, security, and governance across multiple data sources.\nOptimize query performance and database architecture for efficiency and scalability.\nLead troubleshooting and debugging efforts for data-related issues.\nDocument data workflows, architectures, and best practices to ensure maintainability and knowledge sharing.\nPreferred candidate profile\n5+ years of experience in Data Engineering, Software Engineering, or a related field.\nBachelors or masters degree in computer science, Computer Engineering, or a related discipline\nHigh proficiency in SQL (preferably Snowflake) for data modeling, performance tuning, and optimization.\nStrong expertise in Python for data processing and automation.\nExperience with Git or other version control tools in a collaborative development environment.\nStrong communication skills and ability to collaborate with cross-functional teams for requirements gathering and solution design.\nExperience working with large-scale, distributed data systems and cloud data warehouse.","snowflake, dbt, fivetran, Git, python, Sql"
Azure Data Engineer,Zf Lifetec India,5-11 Years,,Pune,Manufacturing,"What you can look forward to as Azure Data Engineer\nEnd-to-End Ownership: Lead and drive analytics initiatives, ensuring high-quality deliverables with minimal oversight\nStructured Software Development: Apply engineering best practices, including CI/CD, version control, testing, and code reviews, to ensure robust, maintainable, and scalable solutions\nDesign, implement, and scale reliable and efficient data models and pipelines in Databricks, Azure Cloud, and other relevant platforms to support analytics and AI initiatives\nLead AI & ML initiatives, according to AI & automation roadmap of analytics team\nData Modeling & Optimization: Develop efficient data models, ensuring high performance for analytical workloads and business intelligence applications\nEnsure seamless integration of data products with BI tools, APIs, and other business applications, providing structured, well-documented, and high-quality datasets\nYour profile as Azure Data Engineer\nDegree in computer science, math or engineering with emphasis on machine learning / analytics or equivalent in experience\n5-11 years of experience in Azure Data engineering, Analytics or AI/ML integration\nStrong experience in Python, R and Spark (Pyspark is desirable)\nDeployment of ML models in production & Industry experience in agile development and DevOps Databases (ex. Oracle, AWS S3)\nBig Data Platforms (ex. Databricks, GCP, Cloudera) & Soft skills include working well in teams dealing with uncertainty and ambiguity, flexibility\nDeep understanding of structured software development practices (version control, CI/CD, testing) & Excellent problem-solving skills and ability to work independently on complex projects","R, Pyspark, Azure, Python, Devops"
Azure Data Engineer,Tiger Analytics,6-11 Years,,Hyderabad,Consulting,"Role Overview:\nWe are seeking street-smart and technically strong Senior Data Engineers / Leads who can take ownership of designing and developing cutting-edge data and AI platforms using Azure-native technologies and Databricks. You will play a critical role in building scalable data pipelines, modern data architectures, and intelligent analytics solutions.\nKey Responsibilities:\nDesign and implement scalable, metadata-driven frameworks for data ingestion, quality, and transformation across both batch and streaming datasets.\nDevelop and optimize end-to-end data pipelines to process structured and unstructured data, enabling the creation of analytical data products.\nBuild robust exception handling, logging, and monitoring mechanisms for better observability and operational support.\nTake ownership of complex modules and lead the development of critical data workflows and components.\nProvide guidance to data engineers and peers on best practices.\nCollaborate with cross-functional teamsincluding business consultants, data architects & scientists, and application developersto deliver impactful analytics solutions.\nRequired Qualifications:\n5+ yearsof overall technical experience, with a minimum of2 yearsof hands-on experience with Microsoft Azure and Databricks.\nProven experience delivering at least one end-to-endData Lakehousesolution on Azure Databricks using the Medallion Architecture.\nStrong working knowledge of theDatabricks ecosystem, including: PySpark, Notebooks, Structured Streaming, Unity Catalog, Delta Live Tables, Workflows, and SQL Warehouse.\nAdvancedprogramming, unit testing, and debugging skills in Python and SQL.\nHands-on experience withAzure-native servicessuch as: Azure Data Factory, ADLS Gen2, Azure SQL Database, and Event Hub.\nSolid understanding ofdata modeling techniques, including both Dimensional and Third Normal Form (3NF) models.\nExposure to developingLLM/Generative AI-powered applications.\nMust have excellent understanding ofCI/CD workflowsusing Azure DevOps.\nBonus: Knowledge of Azure infrastructure, including provisioning, networking, security, and governance.\nEducational Background:\nBachelor's degree (B.E/B.Tech) in Computer Science, Information Technology, or a related field from a reputed institute (preferred).","Azure Data Engineer, Pyspark, Azure Data Factory, Azure Data Lake, Spark, Azure Databricks, Sql, Python"
Data Engineer 3,Vichara Technologies,7-12 Years,,Pune,Consulting,"Key deliverables:\nEnhance and maintain the MDM platform to support business needs\nDevelop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow\nMonitor and improve system performance and troubleshoot data pipeline issues\nResolve production issues and ensure platform reliability\nRole responsibilities:\nCollaborate with data engineering and analytics teams for scalable solutions\nApply DevOps practices to streamline deployment and automation\nIntegrate cloud-native tools and services (AWS, Azure) with the data platform\nUtilize dbt and version control (Git) for data transformation and management","Airflow, snowflake, dbt, Python, Sql, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Gurugram,Consulting,"Role Responsibilities:\nBuild scalable data pipelines and applications from scratch on AWS\nDesign and manage data ingestion processes and workflows\nCollaborate with functional and data science teams to deliver data solutions\nOptimize data storage and organization using HDFS, S3, and Delta Lake\nJob Requirements:\nStrong programming experience in Python or Java\nHands-on experience with AWS services and deployment\nProficiency in HDFS, S3, SQL, and data ingestion tools\nExposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Data Engineer(Snowflake+AWS) job opening at GlobalData,GlobalData Publications Inc,4-6 Years,,Hyderabad,Information Services,"Responsibilities:\nDevelop and optimize Snowflake data warehousing solutions.\nImplement and manage AWS DevOps pipelines for continuous integration and delivery.\nCollaborate with cross-functional teams to gather and analyze requirements.\nPerform data analysis, modeling, and integration to ensure data quality and integrity.\nMonitor and troubleshoot Snowflake, and AWS DevOps performance issues.\nAutomate processes and workflows to improve efficiency and scalability.\nEnsure security and compliance best practices are followed.\nRequired Skills:\nExtensive experience in database development (SQL, NoSQL).\nProficiency in Snowflake development and data warehousing concepts, Snowpipe, and Time Travel.\nStrong knowledge of AWS services (EC2, S3, Lambda, RDS, etc.).\nExperience with AWS DevOps tools (CodePipeline, CodeBuild, CodeDeploy).\nProficiency in programming languages such as Python, Java or node.js\nStrong understanding of data modelling, ETL processes, and data integration.\nHands-on experience with CI/CD pipelines and automation.\nExcellent problem-solving and analytical skills.\nPreferred Skills:\nKnowledge of containerization and orchestration (Docker, Kubernetes).\nFamiliarity with data visualization tools (Tableau, Power BI).\nExperience with data governance and security practices.\nStrong communication and collaboration skills.\nAbility to work in a fast-paced, dynamic environment.","Aws, S3, Lambda, RDS, Ec2, Power Bi, Tableau"
Data Engineer 3,Vichara Technologies,7-12 Years,,"Delhi, Delhi NCR",Consulting,"Key deliverables:\nEnhance and maintain the MDM platform to support business needs\nDevelop data pipelines using Snowflake, Python, SQL, and orchestration tools like Airflow\nMonitor and improve system performance and troubleshoot data pipeline issues\nResolve production issues and ensure platform reliability\nRole responsibilities:\nCollaborate with data engineering and analytics teams for scalable solutions\nApply DevOps practices to streamline deployment and automation\nIntegrate cloud-native tools and services (AWS, Azure) with the data platform\nUtilize dbt and version control (Git) for data transformation and management","Airflow, snowflake, dbt, Python, Sql, AWS"
Senior Data Engineer - Visa,Foray Software Private Limited,3-7 Years,,Bengaluru,Consulting,"Role Responsibilities:\nBuild and manage data pipelines using Spark and Hadoop ecosystem\nDevelop scalable solutions with Hive, HDFS, Kafka, and Sqoop\nWrite code using Scala with exposure to Python\nMaintain jobs using schedulers like AutoSys and version control with Git\nJob Requirements:\n2+ years relevant experience in Big Data platforms\nHands-on with Oracle/MS-SQL databases\nExposure to data visualization platforms like Tableau or AtScale (preferred)\nStrong problem-solving and communication skills","Hive, Hadoop, Scala, Spark, Kafka"
Senior Data Engineer - Data Management,Accordion India,2-5 Years,,Hyderabad,Consulting,"What You will do:\nUnderstand the business requirements thoroughly to design and develop the BI architecture.\nDetermine business intelligence and data warehousing solutions that meet business needs.\nPerform data warehouse design and modelling according to established standards.\nWork closely with the business teams to arrive at methodologies to develop KPIs and Metrics.\nWork with Project Manager in developing and executing project plans within assigned schedule and timeline.\nDevelop standard reports and functional dashboards based on business requirements.\nEnsure to develop and deliver high quality reports in timely and accurate manner.\nConduct training programs and knowledge transfer sessions to junior developers when needed. Recommend improvements to provide optimum reporting solutions.\nIdeally, you have:\nUndergraduate degree (B.E/B.Tech.) from tier-1/tier-2 colleges are preferred.\n2 - 5 years of experience in related field.\nProven expertise in SSIS, SSAS and SSRS (MSBI Suite).\nIn-depth knowledge of databases (SQL Server, MySQL, Oracle etc.) and data warehouse (Azure Synapse, AWS Redshift, Google BigQuery, Snowflake etc.).\nIn-depth knowledge of business intelligence tools (any one of Power BI, Tableau, Qlik, DOMO, Looker etc.).\nGood understanding of Azure (Data Factory Pipelines, SQL Database Managed Instances, DevOps, Logic Apps, Analysis Services), AWS (Glue, Aurora Database, Dynamo Database, Redshift, QuickSight).\nProven abilities to take on initiative and be innovative.\nAnalytical mind with problem solving attitude.","Sql, Hadoop, Spark, Data Warehousing, Python, Etl, AWS"
Big Data Engineer - Xebia,Foray Software Private Limited,3-7 Years,,Pune,Consulting,"Role Responsibilities:\nBuild scalable data pipelines and applications from scratch on AWS\nDesign and manage data ingestion processes and workflows\nCollaborate with functional and data science teams to deliver data solutions\nOptimize data storage and organization using HDFS, S3, and Delta Lake\nJob Requirements:\nStrong programming experience in Python or Java\nHands-on experience with AWS services and deployment\nProficiency in HDFS, S3, SQL, and data ingestion tools\nExposure to Delta Lake or Databricks is an added advantage","HDFS, Data Lake, Python, Sql, AWS"
Senior Data Engineer,Zeta Cards,4-6 Years,,"Mumbai, India",Banking/Accounting/Financial Services,"About Zeta\n\nZeta is a Next-Gen Banking Tech company that empowers banks and fintechs to launch banking products for the future. It was founded by and Ramki Gaddipati in 2015.\nOur flagship processing platform - Zeta Tachyon - is the industry's first modern, cloud-native, and fully API-enabled stack that brings together issuance, processing, lending, core banking, fraud & risk, and many more capabilities as a single-vendor stack. 20M+ cards have been issued on our platform globally.\nZeta is actively working with the largest Banks and Fintechs in multiple global markets transforming customer experience for multi-million card portfolios.\nZeta has over 1700+ employees - with over 70% roles in R&D - across locations in the US, EMEA, and Asia. We raised $280 million at a $1.5 billion valuation from Softbank, Mastercard, and other investors in 2021.\nLearn more @, , ,\n\nAbout the Role\nIn this role, you'll design robust data models using SQL, dbt, and Redshift, while driving best practices across development, deployment, and monitoring. You'll also collaborate closely with product and engineering to ensure data quality and impactful delivery.\n\nResponsibilities\n\nCreate optimised data models with SQL, DBT and Redshift\nWrite functional and column level test for Models\nBuild reports from the data models\nCollaborate with product to clarify requirement and create design document\nGet design reviewed from Architect/Principal/Lead Engineer\nContribute to code reviews\nSet up and monitor Airflow DAGs\nSet up and use CI/CD pipelines\nLeverage Kubernetes operators for deployment automation\nEnsure data quality\nDrive best practices in Data models development, deployment, and monitoring\nMentor colleagues and contribute to team growth\n\nSkills\n\nStrong expertise in SQL for complex data querying and optimization\nHands-on experience with Apache Airflow for orchestration and scheduling\nSolid understanding of data modeling and data warehousing concepts\nExperience with dbt (Data Build Tool) for data transformation and modeling\nExposure to Amazon Redshift or other cloud data warehouses\nFamiliarity with CI/CD tools such as Jenkins\nExperience using Bitbucket for version control\nMonitoring and alerting using Grafana and Prometheus\nWorking knowledge of JIRA for agile project tracking\nFamiliarity with Kubernetes for deployment automation and orchestration\n\nExperience and Qualification\n\n4-6 years of relevant experience in data engineering\ns/Master's degree in engineering (computer science, information systems)","CI CD tools, dbt, Apache Airflow, Bitbucket, Amazon Redshift, Prometheus, Grafana, JIRA, Kubernetes, Sql"
Data Engineer,Vivotex India Private Limited,5-10 Years,INR 0.5 - 17 LPA,"Hyderabad, Bengaluru, Chennai",Login to check your skill match score,"location: Hyderabad,Bangalore,pune,Chennai\nSkill set: SQL, Python, PySpark, and experience with cloud platforms (e.g., Snowflake, AWS).\nClient: NTT Data (CTH Position)\nExperience 5+\nImmediate Joiners only","dbt, Snow Flake, Data Engineer, Data Warehouse, Sql"
Data Engineer,Robotics Technologies,4-13 Years,INR 6.5 - 24 LPA,"Gurugram, Delhi, Kolkata","Management Information Systems, Outsourcing, Cloud Data Services, Information Services","Description\nWe are seeking a skilled Data Engineer to join our team in India. The ideal candidate will be responsible for building and maintaining the infrastructure that supports our data-driven initiatives. You will work closely with cross-functional teams to ensure data is accessible, reliable, and secure.\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and architectures.\nEnsure data quality and reliability through comprehensive testing and validation.\nCollaborate with data scientists and analysts to understand data requirements and deliver necessary data solutions.\nOptimize and improve data storage and retrieval processes for efficiency.\nMonitor and troubleshoot data-related issues and performance bottlenecks.\nImplement data security and compliance measures as per industry standards.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong understanding of SQL and experience with relational databases like MySQL, PostgreSQL, or Oracle.\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nFamiliarity with cloud platforms like AWS, Google Cloud, or Azure.\nKnowledge of data warehousing solutions like Snowflake, Redshift, or BigQuery.\nExperience with ETL tools and frameworks.\nStrong analytical and problem-solving skills.\nAbility to work collaboratively in a team environment and communicate effectively.","Nosql, Hadoop, Data Modeling, Spark, Kafka, Data Warehousing, Sql, Python, Etl, AWS"
Principal Data Engineer,Dayworks Private Limited,8-12 Years,,Pune,Login to check your skill match score,"This role is for one of Weekday's clients\nMin Experience: 8 years\nLocation: Pune\nJobType: full-time\nTitle: Principal Data Engineer\nJob Description:\nJob Summary:\nLead the design, development, and optimization of enterprise-scale data pipelines and architectures to drive data-driven decision-making. Architect efficient data tables and schemas to enhance query performance, ensuring seamless integration with cross-functional teams\nKey Responsibilities:\nData Pipeline\nDesign and implement high-performance data pipelines using ETL processes, batch/streaming frameworks (e.g., Apache Spark, Airflow), and cloud platforms (AWS).\nOptimize data ingestion, transformation, and storage workflows to meet scalability, reliability, and latency requirements.\nData Architecture & Query Efficiency\nArchitect database schemas and dimensional models (e.g., star/snowflake) to maximize query performance and reduce latency.\nImplement indexing strategies, partitioning, and materialized views to optimize data retrieval.\nCollaboration & Governance\nPartner with data scientists, product managers, and engineers to align data infrastructure with business needs.\nEstablish data governance frameworks, ensuring compliance with security, privacy, and quality standards.\nMentor junior engineers and foster best practices in data engineering\nInnovation & Leadership\nStay ahead of industry trends (e.g., AI/ML integration, real-time analytics) and advocate for emerging technologies.\nLead technical strategy for data infrastructure, balancing innovation with operational stability.\nTechnical Expertise:\n8+ years of experience in data engineering, with a focus on ETL, data modelling, and cloud-based architectures.\nProficiency in SQL, Python, and tools like Spark and Snowflake.\nStrong knowledge of database design, dimensional modeling, and query optimization.\nLeadership & Collaboration\nProven ability to lead teams, mentor engineers, and drive technical initiatives.\nExperience translating business requirements into scalable technical solutions.\nPreferred qualification\nExpertise in machine learning model deployment or real-time analytics.\nFamiliarity with DevOps practices (CI/CD, Docker, Kubernetes).\nOur Core Values\nData Fanatics: Our edge is always found in the data\nPartner Obsessed: We are obsessed with partner success\nTeam of Doers: We have a bias for action\nGame Changers: We encourage innovation","SparkML, Data Modelling, Spark, Apache Spark"
Platform Data Engineer,Elitez Recruitment Services Private Limited,5-13 Years,INR 120 - 216 LPA,"Bengaluru, Chennai","Enterprise Software, Software Engineering, Software, Consumer Software, Information Services","Docker/Kubernetes Application Development and Kube Admin Skills (Incl. Helm)\nPhyton Development is important\nObservability Skills (Monitoring, and Logging ELK, Prometheus/Grafana preferably)\nAirflow 2.7 + (Both as a user level who develops DAGs, and has knowledge of Airflow internals including customizations)\nCICD (Azure DevOps)\nAny experience with Control-M is good\nKubernetes:\n- In depth mastery of core Kubernetes concepts, including Control Plane Architecture, Networking (CNI, Service, Ingress, NetworkPolicy), Storage (CSI, PV, PVC), Security (RBAC, Secrets), Scheduling, and Resource Management\n- Must have proven track record of successfully deploying and managing large-scale, mission-critical Kubernetes clusters in production environment\nObservability:\n- Mastery of core observability principles, including Metrics, Logs, Traces, Baggage, SLOs/SLIs, Alerting, Instrumentation, and Correlation.\n- Must have proven experience engineering/developing and implementing comprehensive observability solutions using the OpenTelemetry framework, resulting in significant reductions in Mean Time To Detection (MTTD) or Mean Time To Resolution (MTTR).\nFrameworks: Opentelemetry or equivalent\nAirflow:\n- Strong understanding of core Airflow architecture, including Control-M integration, DAG best practices, scheduling, operators, and executors.\n- Must have proven experience owning and stabilizing production Airflow environments, ensuring reliable execution of critical data workflows that support business operations.\nCoding & Scripting: (Technologies)\n- Proficiency in Python for automation, tooling, and systems development, coupled with strong Bash scripting skills.\n- Demonstrated experience building impactful internal tools or services that significantly enhance developer productivity and platform capabilities.\n- Demonstrated expertise in identifying and resolving highly complex, cross-functional issues within distributed systems.","Java, Golang, Graphql, Docker, Node.js, Rest Apis, Sql, Kubernetes, Microservices, AWS"
Data Engineer,Robotics Technologies,7-12 Years,INR 29 - 50 LPA,"Navi Mumbai, Mumbai City, Pune","Management Information Systems, Email, VoIP, Identity Management","Description\nWe are seeking a skilled Data Engineer to join our team in India. The ideal candidate will have extensive experience in designing, building, and maintaining scalable data pipelines and databases, ensuring high data quality and availability for analytical needs.\nResponsibilities\nDesign and implement data pipelines to support data transformation and analysis.\nEnsure data quality and integrity through automated testing and monitoring.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions.\nOptimize data storage and retrieval processes for efficiency and scalability.\nDevelop documentation for data processes and systems to ensure knowledge transfer and compliance.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n7-12 years of experience in data engineering or a related role.\nProficiency in programming languages such as Python, Java, or Scala.\nStrong experience with SQL and NoSQL databases (e.g., PostgreSQL, MongoDB).\nFamiliarity with data warehousing solutions (e.g., Amazon Redshift, Google BigQuery).\nExperience with ETL tools and frameworks (e.g., Apache NiFi, Talend).\nKnowledge of cloud platforms (AWS, Azure, Google Cloud) and their data services.\nUnderstanding of data modeling concepts and best practices.","Cloud Technologies, Data Modeling, Apis, Spark, Kafka, Data Warehousing, Sql, Python, Etl, AWS"
leaad data engineer,Teizo Soft Private Limited,10-13 Years,INR 26.5 - 34.5 LPA,Hyderabad,Login to check your skill match score,"Lead Data Engineer Hyderabad (Work from Office)\nSend profiles to [HIDDEN TEXT]\nRequired Skills & Experience: 10+ years of data engineering experience, with 2+ years in a technical leadership role.\nDeep expertise in FiveTran, Snowflake, and SQL development for data processing.\nStrong Python skills for data transformation, orchestration, and automation.\nExtensive experience in ETL pipeline design and data integration strategies.\nProven ability in dimensional modeling, star schema design, and data warehousing best practices.\nHands-on experience with business KPIs and reporting analytics to influence data model architecture.\nStrong leadership in mentoring Data Engineers and BI Engineers, providing architectural guidance.\nExceptional problem-solving abilities, communication skills, and stakeholder management.\nPreferred Skills:\nExperience with cloud platforms like AWS, Azure, or GCP for data solutions.\nKnowledge of big data technologies such as Apache Spark or Kafka.\nStrong grasp of data security and governance principles.\nLocation: Hyderabad Mode of Work: Work from Office Apply Now: [HIDDEN TEXT]\nHashtags: #DataEngineer #Snowflake #BI #Data #Python #FiveTran #SQL #ETL #CloudData #DataWarehousing #Leadership #Hyderabad #Hiring #Recruitment","Cloud Data Platforms (AWS, Hadoop Data Security & Governance, Data Engineering Leadership, SQL Query Optimization, Data Migration & Integration, Data Science & Analytics, Gcp, Big Data Technologies, Spark, Kafka, Azure"
Gcp Data Engineer,Telus International,3-8 Years,,Bengaluru,BPO,"Role & responsibilities.\nAs a Data Engineer, you'll focus on solving problems and creating value for business by building solutions that are reliable and scalable to work with the size and scope of the company. You will be tasked with creating a custom-built pipeline on GCP stack, and you will be part of teams that implement vendor sourced enterprise software, configuring that software, customizing it, and integrating with other internal systems.\nRequired Skills:\n3-10+ years of industry experience in software development, data engineering, business intelligence, or related field with experience in manipulating, processing, and extracting value from datasets.\nDesign, build and deploy internal applications to support our technology life cycle, collaboration and spaces, service delivery management, data and business intelligence among others.\nBuilding Modular code for multi usable pipeline or any kind of complex Ingestion Framework used to ease the job to load the data into Datalake or Data Warehouse from multiple sources.\nWork closely with analysts and business process owners to translate business requirements into technical solutions.\nCoding experience in scripting and languages (Python, SQL, PySpark).\nExpertise in Google Cloud Platform (GCP) technologies in the data warehousing space (BigQuery,Dataproc, GCP Workflows, Dataflow, Cloud Scheduler, Secret Manager,Batch, Cloud Logging, Cloud SDK, Google Cloud Storage, IAM, Vertex AI).\nMaintain highest levels of development practices including: technical design, solution development, systems configuration, test documentation/execution, issue identification and resolution, writing clean, modular and self-sustaining code, with repeatable quality and predictability.\nUnderstanding CI/CD Processes usingPulumi,Github, Cloud Build, Cloud SDK, Docker\nExperience withSAS/SQL Server/SSISis an added advantage.\nQualifications:\nBachelor's degree in Computer Science or related technical field, or equivalent practical experience.\nGCP Certified Data Engineer (preferred)\nExcellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences.","cd, dataflow cloud, python, Ci, Sas, Pyspark, Data Warehousing, Sql"
Data Engineer | Tealium IQ |Tag Management,Datamatics Global Services Limited,5-8 Years,,Bengaluru,Information Technology,"Job description\nExtensive experience in the process flows associated with tracking design and build.\nExtensive experience in Tealium IQ and the configuration of one or more digital analytics tools across websites and apps (ideally Amplitude but not essential).\nExtensive experience in the wider marketing stack (Google, Facebook, etc) and implementation of associated pixels.\nStrong understanding of privacy requirements where they relate to digital data, with experience in using OneTrust or similar Consent Management Platform.\nIdeally have knowledge of Front End coding languages (Javascript, HTML, CSS).\nExperienced on API integrations.\nStrong stakeholder manager, able to priorities and communicate accordingly in a fast-paced, product-led organization.\nExcellent Test Script and Documentation skills\nExperience working in more than one of the following service delivery functions; development, support, software design or environment configuration\nAbility to translate business goals into technology solutions\nExperience of working in a matrix managed environment, with mixed teams including contractors and outsourced vendors","Tag Management, Tealium IQ, Test Script, css, Html Scripting, Api Integration, Javascript"
Senior Azure Data Engineer,IQVIA,4-8 Years,,"Gurugram, Bengaluru, Cochin / Kochi / Ernakulam",Hospital,"Project Role:Azure date engineer\nWork Experience:4 to 8 Years\nWork location:Bangalore / Gurugram / Kochi\nWork Mode:Hybrid\nMust Have Skills:Azure Data engineer, SQL, Spark/Pyspark\nJob Overview:\nResponsible for the on-time completion of projects or components of large, complex projects for clients in the life sciences field. Identifies and elevates potential new business opportunities and assists in the sales process.\nSkills required:\nExperience in developing Azure components like Azure data factory, Azure data Bircks, Logic Apps, Functions\nDevelop efficient & smart data pipelines in migrating various sources on to Azure datalake\nProficient in working with Delta Lake, Parquet file formats\nDesigns, implements, and maintain the CI/CD pipelines, deploy, merge codes\nExpert in programming in SQL, Pyspark, Python\nCreation of databases on Azure data lake with best data warehousing practises\nBuild smart metadata databases and solutions, parameterization, configurations\nDevelop Azure frameworks, develops automated systems for deployment & monitoring\nHands-on experience in continuous delivery and continuous integration of CI/CD pipelines, CI/CD infrastructure and process troubleshooting.\nExtensive experience with version control systems like Git and their use in release management, branching, merging, and integration strategies\nEssential Functions:\nParticipates or leads teams in the design, development and delivery of consulting projects or components of larger, complex projects.\nReviews and analyzes client requirements or problems and assists in the development of proposals of cost effective solutions that ensure profitability and high client satisfaction.\nProvides direction and guidance to Analysts, Consultants, and where relevant, to Statistical Services assigned to engagement.\nDevelops detailed documentation and specifications.\nPerforms qualitative and/or quantitative analyses to assist in the identification of client issues and the development of client specific solutions.\nDesigns, structures and delivers client reports and presentations that are appropriate to the characteristics or needs of the audience.\nMay deliver some findings to clients.\nQualifications\nBachelor's Degree Req\nMaster's Degree Business Administration Pref\n4-8 years of related experience in consulting and/or life sciences industry Req.","Spark/Pyspark, Logic Apps, Azure Data engineer, Azure data Bircks, Azure Data Factory, Sql"
Senior Azure Data Engineer,IQVIA,4-8 Years,,"Gurugram, Bengaluru, Cochin / Kochi / Ernakulam",Hospital,"Project Role:Azure date engineer\nWork Experience:4 to 8 Years\nWork location:Bangalore / Gurugram / Kochi\nWork Mode:Hybrid\nMust Have Skills:Azure Data engineer, SQL, Spark/Pyspark\nJob Overview:\nResponsible for the on-time completion of projects or components of large, complex projects for clients in the life sciences field. Identifies and elevates potential new business opportunities and assists in the sales process.\nSkills required:\nExperience in developing Azure components like Azure data factory, Azure data Bircks, Logic Apps, Functions\nDevelop efficient & smart data pipelines in migrating various sources on to Azure datalake\nProficient in working with Delta Lake, Parquet file formats\nDesigns, implements, and maintain the CI/CD pipelines, deploy, merge codes\nExpert in programming in SQL, Pyspark, Python\nCreation of databases on Azure data lake with best data warehousing practises\nBuild smart metadata databases and solutions, parameterization, configurations\nDevelop Azure frameworks, develops automated systems for deployment & monitoring\nHands-on experience in continuous delivery and continuous integration of CI/CD pipelines, CI/CD infrastructure and process troubleshooting.\nExtensive experience with version control systems like Git and their use in release management, branching, merging, and integration strategies\nEssential Functions:\nParticipates or leads teams in the design, development and delivery of consulting projects or components of larger, complex projects.\nReviews and analyzes client requirements or problems and assists in the development of proposals of cost effective solutions that ensure profitability and high client satisfaction.\nProvides direction and guidance to Analysts, Consultants, and where relevant, to Statistical Services assigned to engagement.\nDevelops detailed documentation and specifications.\nPerforms qualitative and/or quantitative analyses to assist in the identification of client issues and the development of client specific solutions.\nDesigns, structures and delivers client reports and presentations that are appropriate to the characteristics or needs of the audience.\nMay deliver some findings to clients.\nQualifications\nBachelor's Degree Req\nMaster's Degree Business Administration Pref\n4-8 years of related experience in consulting and/or life sciences industry Req.","Spark/Pyspark, Logic Apps, Azure Data engineer, Azure data Bircks, Azure Data Factory, Sql"
Data Engineer,Infosys Limited,Fresher,,"Bengaluru, India",IT/Computers - Software,"Key Responsibilities:\nWe are seeking a skilled and detail oriented Data Warehouse Engineer to design build and maintain scalable data warehouse solutions\nYou will be responsible for developing efficient data pipelines integrating diverse data sources ensuring data accuracy and enabling high quality analytics to drive business decisions\nResponsibilities\nDesign develop and maintain data warehouse architectures and systems\nBuild robust ETL Extract Transform Load processes for structured and unstructured data sources\nOptimize data models database performance and storage solutions\nCollaborate with data analysts data scientists and business stakeholders to understand data requirements\nImplement data quality checks and ensure data governance best practices\nDevelop and maintain documentation related to data warehouse design data flow and processes\nMonitor system performance and proactively identify areas for improvement\nSupport ad hoc data requests and reporting needs\nStay up to date with emerging data technologies and industry best practices\nPreferred Skills:\nTechnology->ETL & Data Quality->ETL - Others,Technology->Database->Data Modeling,Technology->Data Management - DB->DB2,Technology->Data on Cloud-DataStore->Snowflake",
Data Engineer,Commonwealth Bank,4-6 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization\nAt CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title: Data Engineer\nLocation: Bangalore\nBusiness & Team:\nThe Business Banking Technology Domain works in an Agile methodology with our business banking business to plan, prioritise and deliver on high value technology objectives with key results that meet our regulatory obligations and protect the community.\nYou will work within the VRM Crew that is working on initiatives such as Gen AI based cash flow coach to provide relevant data to our regulators\nImpact & Contribution:\nAs a Data Engineer and database engineer you will be creating and managing the cloud databases and data pipelines that underpin our decoupled cloud architecture and API first approach. You have proven expertise in database design, data ingestion, transformation, data writing, scheduling and query management within a cloud environment.\nYou will have proven experience and expertise in working with AWS Cloud Infrastructure Engineers, Software/API Developers to design, develop, deploy and operate data services and solutions that underpin a cloud ecosystem. You will take ownership and accountability of functional and non-functional design and work within a team of Engineers to create innovative solutions that unlock value and modernise technology designs.\nYou will role model continuous improvement mindset in the team, and in your project interactions, by taking technical ownership of key assets, including roadmaps and technical direction of data services running on our AWS environments.\nRoles & Responsibilities:\nCan design and implement databases for data integration in the enterprise\nCan performance tune applications from a database code and design perspective\nCan automate data ingestion and transformation processes using scheduling tools.\nMonitor and troubleshoot data pipelines to ensure reliability and performance.\nCan design application logical database requirements and implement physical solutions\nCan collaborate with business and technical teams in order to design and build critical databases and data pipelines\nEssential Skills:\nMinimum 4 to 6 years of experience\nAWS Data products such as AWS Glue and AWS EMR\nOracle and AWS Aurora RDS such as PostgreSQL\nAWS S3 ingestion, transformation and writing to databases\nProficiency in programming languages like Python, Scala or Java for developing data ingestion and transformation scripts.\nStrong knowledge of SQL for writing, optimizing, and debugging queries.\nFamiliarity with database design, indexing, and normalization principles.\nUnderstanding of data formats (JSON, CSV, XML) and techniques for converting between them. Ability to handle data validation, cleaning, and transformation.\nProficiency in automation tools and scripting (e.g., bash scripting, cron jobs) for scheduling and monitoring data processes.\nExperience with version control systems (e.g., Git) for managing code and collaboration.\nEducation Qualifications:\nBachelor's degree in engineering in Computer Science/Information Technology.\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 29/06/2025","Experience with version control systems e.g. Git, Oracle and AWS Aurora RDS such as PostgreSQL, Strong knowledge of SQL, AWS S3 ingestion transformation and writing to databases, AWS Data products such as AWS Glue and AWS EMR"
Data Engineer - Manager,State Street Corporation,8-12 Years,,Bengaluru,Login to check your skill match score,"Technical Lead (overall 10+ years good development experience) with good hands-on experience in Python/PySpark with strong experience in Spark scala/PySpark and Spark SQL for big data processing and Databricks, AWS services,Kafka,Airflow.\nHands-on experience in SQL, PL/SQL, Unix Shell Scripting, Autosys.\nExperience with relational databases such as Oracle, PostgreSQL as well as NoSQL databases\nData integration techniques and tools including APIs and data streaming\nStrong analytical and problem solving skills, with the ability to identify and resolve issues in data pipelines and systems\nExperience with Agile software development methodology and version control systems like GitHub\nExpertise in Data Modeling and DB design with the skills in performance tuning\nSupport existing data platforms and products in production for any critical issues and provide resolution fixes\nHave good communication skill, and collaborate well with different teams/stakeholders and vendors\nExpected to spend 90% of the time on hands-on development, design and architecture and remaining 10% on guiding the team on technology and removing other impediments Capital Markets Projects experience preferred Provides advanced technical expertise in analyzing, designing, estimating, and developing software applications to project schedule. Oversees systems design and implementation of most complex design components. Creates project plans and deliverables and monitors task deadlines. Oversees, maintains and supports existing software applications. Provides subject matter expertise in reviewing, analyzing, and resolving complex issues. Designs and executes end to end system tests of new installations and/or software prior to release to minimize failures and impact to business and end users. Responsible for resolution, communication, and escalation of critical technical issues. Prepares user and systems documentation as needed. Identifies and recommends Industry best practices. Serves as a mentor to junior staff.\nAbility to lead a team of agile developers.\nWorked in a complex deadline driven projects with minimal supervision. Ability to architect/design/develop with minimum requirements by effectively.\n2+ experience on Apache Kafka streaming processingand Python scripting.\nEnd to end understanding of software architecture, design, development and implementation.\nMin 8+ years of good experience on AWS clouds and Data bricks,Airflow.\nExcellent work ethics with ability to work independently.\nExperience in Banking Domain preferred.","Airflow, NoSQL databases, Spark SQL, data streaming, Github, Apis, Aws Services, Data Modeling, PostgreSQL, Performance Tuning, Pyspark, Pl Sql, Kafka, Autosys, Db Design, Sql, Unix Shell Scripting, Spark, Databricks, Oracle, Python"
Senior Data Engineer,Commonwealth Bank,8-10 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title:Senior Data Engineer-Big Data\nLocation:Bengaluru\nBusiness & Team:RM & FS Data Engineering\nImpact & contribution:\nAs a Senior Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you're ahead of trends and work at the forefront of Big Data and Data warehouse technologies.\nWhich is why we're the perfect fit for you. Here, you'll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers most complex data-centric problems.\nTo us, data is everything. It is what powers our cutting-edge features and it's the reason we can provide seamless experiences for millions of customers from app to branch.\nWe're responsible for CommBank's key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We work across the Cloudera Hadoop Big Data, Teradata Group Data Warehouse and Ab Initio platforms.\nRoles & Responsibilities:\nPassionate about building next generation data platforms and data pipeline solution across the bank.\nEnthusiastic, be able to contribute and learn from wider engineering talent in the team.\nReady to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.\nCapable to create both technology blueprints and engineering roadmaps, for a multi- year data transformational journey.\nCan lead and drive a culture where quality, excellence and openness are championed.\nConstantly thinking outside the box and breaking boundaries to solve complex data problems.\nHave experience in providing data driven solutions that source data from various enterprise data platform into Cloudera Hadoop Big Data environment, using technologies like Spark, MapReduce, Hive, Sqoop, Kafka transform and process the source data to produce data assets and transform and egression to other data platforms like Teradata or RDBMS system.\nAre experienced in building effective and efficient Big Data and Data Warehouse frameworks, capabilities, and features, using common programming language (Scala, Java, or Python), with proper data quality assurance and security controls.\nAre experienced in providing data driven solutions in the Cloud to build various enterprise data platform into AWS platform using technologies like S3, EMR, Glue, Iceberg, Kinesis or MSK/Kafka transform and process the data to produce data assets for Redshift and DocumentDB.\nAre confident in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.\nCan collaborate, co-create and contribute to existing Data Engineering practices in the team.\nHave experience and responsible for data security and data management.\nHave a natural drive to educate, communicate and coordinate with different internal stakeholders.\nEssential Skills:\nPreferably with at least 8+ years of hands-on experience in a Data Engineering role.\nExperience in designing, building, and delivering enterprise-wide data ingestion, data integration and data pipeline solutions using common programming language (Scala, Java, or Python) in a Big Data and Data Warehouse platform.\nExperience in building data solution in Hadoop platform, using Spark, Hive,MapReduce, Sqoop, Kafka and various ETL frameworks for distributed data storage and processing. Preferably with at least 8+ years of hands-on experience.\nExperience in building data solution using AWS Cloud technology (EMR, Glue, Iceberg, Kinesis, MSK/Kafka, Redshift, DocumentDB, S3, etc.). Preferably with 2+ years of hands-on experience and certified AWS Data Engineer.\nStrong Unix/Linux Shell scripting and programming skills in Scala, Java, or Python.\nProficient in SQL scripting, writing complex SQLs for building data pipelines.\nExperience in working in Agile teams, including working closely with internal business stakeholders.\nFamiliarity with data warehousing and/or data mart build experience in Teradata, Oracle or RDBMS system is a plus.\nCertification on Cloudera CDP, Hadoop, Spark, Teradata, AWS, Ab Initio is a plus.\nExperience in Ab Initio software products (GDE, Co Operating System, Express It, etc.) is a plus.\nEducational Qualifications: B.Tech and above\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 05/06/2025","DocumentDB, MSK, Teradata, Iceberg, Glue, Java, S3, Hadoop, Scala, Kafka, Big Data, Emr, Redshift, Sql, Mapreduce, Hive, Kinesis, Sqoop, Spark, Cloudera, Ab Initio, Python, AWS"
Senor Data Engineer with Snowflake Experience.( 5+ Years of Exp),3Pillar Global,5-8 Years,,"Indi, India",Login to check your skill match score,"Distinguished Tech Innovator:3Pillar warmly extends an invitation for you to join an elite team of visionaries. Beyond software development, we are dedicated to engineering solutions that challenge conventional norms. Envision you: steering projects that redefine urban living, establish new media channels for enterprise companies, or drive innovation in healthcare. Your invaluable expertise will serve as the cornerstone in shaping the future direction of our role transcends the ordinary realms of coding it's about orchestrating technological marvels that disrupt industries. Seize this extraordinary opportunity to lead a team that is actively shaping the tech landscape for our clients, and sets global standards along the way.\nMinimum Qualification\nExperience: 5-8 years of experience in data engineering, DevOps, or a related technical field.\nProgramming & Scripting: Strong programming skills in Python and Linux Bash for automation and data workflows.\nFramework Proficiency: Hands-on experience with Luigi for orchestrating complex data workflows.\nData Processing & Storage: Expertise in Hadoop ecosystem tools and managing SQL databases for data storage and query optimization.\nAWS Cloud Services: In-depth knowledge of AWS EC2, S3, RDS, and EMR to deploy and manage data solutions.\nMonitoring & Alerting Tools: Familiarity with monitoring solutions for real-time tracking and troubleshooting of data pipelines.\nCommunication & Leadership: Proven ability to lead projects, communicate with stakeholders, and guide junior team members.\nAdditional experience\nData Architecture: Experience designing or optimizing data lake solutions.\nSecurity Practices: Understanding of data security practices, data governance, and compliance for secure data processing.\nAutomation & CI/CD: Familiarity with CI/CD tools to support automation of deployment and testing.\nBig Data Technologies: Knowledge of big data processing tools like Spark, Hive, or related AWS services.\nAdvanced Analytics: Background in analytics or data science to contribute to more data-driven decision-making.\nCross-Functional Collaboration: Experience collaborating with non-technical teams on business goals and technical solutions.\nRegards\nRajan\n3Pillar Global","Luigi, CI CD tools, SQL databases, Linux Bash, Hadoop ecosystem tools, Monitoring solutions, Aws Ec2, Hive, Python, Spark"
Senior Data Engineer,Swiss Re,12-14 Years,,"Hyderabad, India",Consulting/Advisory Services,"As a Senior Data Engineer, you will be responsible for designing and implementing complex data pipelines and analytics solutions to support key decision-making business processes in our Property & Casualty business domain. You will gain exposure to a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re Property & Casualty.\nYou will be expected to take end-to-end ownership of deliverables, gaining a full understanding of the Property & Casualty data and the business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Architects to understand requirements, formulate solutions, and evaluate the implementation effort.\nDesign, develop and maintain scalable data transformation pipelines.\nData model Design, Data architecture implementation\nWorking with Palantir platform for implementation\nEvaluate new capabilities of the analytics platform, develop prototypes and assist in drawing Development of single source of truth about our application landscape.\nCollaborate within a global development team to design and deliver solutions\nAssist stakeholders with data-related functional and technical issues\nWorking with data governance platform for data management and stewardship.\nAbout the Team:\nThis position is part of the Property & Casualty Data Integration and Analytics project within the Reinsurance Data office team under Data & Foundation. We are part of a global strategic initiative to make better use of our Property & Casualty data and to enhance our ability to make data driven decisions across the Property & Casualty reinsurance value chain.\nAbout You:\nYou enjoy the challenge of solving complex big data analytics problems using state-of-the-art technologies as part of a growing global team of data engineering professionals. You are a self-starter with strong problem-solving skills and capable of owning and implementing solutions from start to finish. Key qualifications include:\nBachelor's degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 12 years of experience working with large scale software systems\nAt least 6 years of experience in Pyspark and Proficient in designing Large Scale Data Engineering solutions\nMinimum of 2 years of experience with Palantir Foundry, including familiarity with tools such as code repositories and Workshop.\nProficient in SQL (Spark SQL preferred)\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with TypeScript/JavaScript/HTML/CSS a plus\nKnowledge of data management fundamentals and data warehousing principals\nDemonstrated strength in data modelling, ETL and storage/Data Lake development\nExperience with Scrum/Agile development methodologies\nKnowledge of Insurance Domain, Financial Industry or Finance function in other industries is a strong plus\nExperienced in working with a diverse multi-location team of internal and external professionals\nStrong analytical and problem-solving skills\nSelf-starter with a positive attitude and a willingness to learn\nAbility to manage own workload self-directed\nAbility and enthusiasm to work in a global and multicultural environment\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nKeywords:\nReference Code:133975","Data Lake development, Palantir Foundry, Pyspark, Spark SQL, Hadoop, CSS, Agile Development Methodologies, Sql, HTML, Typescript, Hive, Javascript, Spark, Etl"
Senior Data Engineer,Swiss Re,12-14 Years,,"Bengaluru, India",Consulting/Advisory Services,"As a Senior Data Engineer, you will be responsible for designing and implementing complex data pipelines and analytics solutions to support key decision-making business processes in our Property & Casualty business domain. You will gain exposure to a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re Property & Casualty.\nYou will be expected to take end-to-end ownership of deliverables, gaining a full understanding of the Property & Casualty data and the business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Architects to understand requirements, formulate solutions, and evaluate the implementation effort.\nDesign, develop and maintain scalable data transformation pipelines.\nData model Design, Data architecture implementation\nWorking with Palantir platform for implementation\nEvaluate new capabilities of the analytics platform, develop prototypes and assist in drawing Development of single source of truth about our application landscape.\nCollaborate within a global development team to design and deliver solutions\nAssist stakeholders with data-related functional and technical issues\nWorking with data governance platform for data management and stewardship.\nAbout the Team:\nThis position is part of the Property & Casualty Data Integration and Analytics project within the Reinsurance Data office team under Data & Foundation. We are part of a global strategic initiative to make better use of our Property & Casualty data and to enhance our ability to make data driven decisions across the Property & Casualty reinsurance value chain.\nAbout You:\nYou enjoy the challenge of solving complex big data analytics problems using state-of-the-art technologies as part of a growing global team of data engineering professionals. You are a self-starter with strong problem-solving skills and capable of owning and implementing solutions from start to finish. Key qualifications include:\nBachelor's degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 12 years of experience working with large scale software systems\nAt least 6 years of experience in Pyspark and Proficient in designing Large Scale Data Engineering solutions\nMinimum of 2 years of experience with Palantir Foundry, including familiarity with tools such as code repositories and Workshop.\nProficient in SQL (Spark SQL preferred)\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with TypeScript/JavaScript/HTML/CSS a plus\nKnowledge of data management fundamentals and data warehousing principals\nDemonstrated strength in data modelling, ETL and storage/Data Lake development\nExperience with Scrum/Agile development methodologies\nKnowledge of Insurance Domain, Financial Industry or Finance function in other industries is a strong plus\nExperienced in working with a diverse multi-location team of internal and external professionals\nStrong analytical and problem-solving skills\nSelf-starter with a positive attitude and a willingness to learn\nAbility to manage own workload self-directed\nAbility and enthusiasm to work in a global and multicultural environment\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nKeywords:\nReference Code:133972","Data Lake development, Palantir Foundry, Typescript, Sql, Scrum, Agile, HTML, CSS, Hadoop, Data Modelling, Pyspark, Etl, Hive, Spark SQL, Javascript, Spark"
Data Engineer,NTT Data,4-6 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324632\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","snowflake, Google DataProc, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Solr, Databricks, Java, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324609\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Azure Data Engineer, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,NTT Data,Fresher,,"Bengaluru, India",IT/Computers - Software,"Req ID:321499\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nJob Duties: . Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n. Work closely with Data modeller to ensure data models support the solution design\n. Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n. Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n. Develop documentation and artefacts to support projects\nMinimum Skills Required: . ADF\n. Fivetran (orchestration & integration)\n. SQL\n. Snowflake DWH\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.",
Data Engineer,NTT Data,Fresher,,"Bengaluru, India",IT/Computers - Software,"Req ID:321498\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nJob Duties: . Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n. Work closely with Data modeller to ensure data models support the solution design\n. Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n. Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n. Develop documentation and artefacts to support projects\nMinimum Skills Required: . ADF\n. Fivetran (orchestration & integration)\n. SQL\n. Snowflake DWH\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.",
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324653\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Azure Data Engineer, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,Nineleaps Technology Solutions Private Limited,2-4 Years,,Bengaluru,"Retail Technology, Health Care, Banking, Finance, Manufacturing, Logistics","We are looking for a Data Engineer to join our team.\nPrimary Responsibilities:\nExperience in Big data Distributed ecosystems:- Hadoop, Hive\nExcellent knowledge of HQL/PrestoQL:- Optimisations, complex aggregations, performance tuning\nExperience building data processing frameworks, and big data pipelines.\nSolid understanding of DWH architecture, ELT/ETL processes, and data structures\nBasic Understanding of Python:- ETL\nKeep the SLA of maintained datasets(Tier 1, 2) to above 99%. (On-call)\nMaintain the SLAs for Tier 3 tables as decided\nDesired Skills:\nStrongSQL Experience,\nHadoop,\nHive,\nSpark/PySpark,\nETL","Spike, Hive, Hadoop, Py Spark, Sql, Etl"
Data Engineer,Tech Tammina,2-5 Years,,Remote,Information Technology,"Must be strong with Python for ML pipelines specifically with Pytorch and scikit-learn AWS is required, building pipelines within Should have a background in LLM (langchain, agents, extensive prompt engineering)\nThe strong additional requirements below are required.\nResponsibilities:\nIngesting, structuring and analyzing a wide range of unstructured datasources\nDesigning, maintaining and orchestrating data pipelines in an AWS environment for production processing and training flows\nContinuously evaluate, analyze, test and improve the quality, privacy and performance of our data systems\nContribute across the product, where - from front-end UX and product design, API/systems architecture and ML processing/training\nMinimum Qualifications:\n3+ years of experience ingesting, analyzing and structuring a wide variety of datasources\nSignificant experience building and maintaining data pipelines in a production environment\nStrong database/SQL, python, pandas (or equivalent) experience\nPrior experience working in fast paced environments and tackling problems across the stack with quick iterations while maintaining a high quality bar.\nStrong Additional Qualifications:\nSignificant healthcare data experience\nLLM experience (langchain, agents, extensive prompt engineering)\nMLE Experience - pytorch, scikit-learn, etc.\nExtensive production AWS, container and/or data orchestration experience\nFullstack development experience (JS/TS/Node in particular)\nDemonstrated experience in similar roles in a startup or consultancy","Training, Product Design, Consultancy, Healthcare, Front End, Orchestration, Sql Database, Javascript, Python, AWS"
Data Modeler - Principal Data Engineer - R01548372,Brillio,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Principal Data Engineer\n\nPrimary Skills\n\nData Modeling (ER Studio/ER Win)\n\nSpecializationJob requirements\n\nAbout Brillio:\nBrillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.\nBrillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.\n10+ years of overall IT experience with more than 2+ years of experience in Snowflake with hands on experience in modelling\nIn-depth understanding of Data Lake/ODS, ETL concept and modeling structure principles.\nAbility to partner closely with the clients to deliver and drive Physical and logical model solutions\nExperience in Data warehousing - Dimensions, Facts, and Data modeling.\nExcellent communication and ability to lead teams.\nGood to have exposure to AWS eco systems.\nExcellent communication and ability to lead teams. Technical Skills\nData Modeling concepts (Advanced Level)\nModeling experience across large volume-based environments\nExperience with ER Studio\nOverall understanding of Database space (Databases, Data Warehouses, Reporting, Analytics)\nWell versed with Data Modeling platforms like SQLDBM etc\nStrong skills in Entity Relationship Modeling\nGood knowledge about Database Designing Administration, Advanced Data Modeling for Star Schema design\nGood understanding of Normalized Denormalized Star Snowflake design concepts SQL query development for investigation and analysis","Entity Relationship Modeling, SQL query development, snowflake, SQLDBM, Advanced Data Modeling, Star Schema Design, Data Modeling, Data Warehousing, Database Designing, Data Lake, Etl, Er Studio"
IN_Senior Associate_ AWS Data Engineer_Advisory Corporate_Advisory_ Bangalore,PwC India,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nSAP\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.\n\nThose in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nPython,\n\nPy Spark,\n\nSql,\n\nRedshift ,\n\nS3 ,\n\nCloud Watch,\n\nLambda,\n\nAWS Glue\n\nEMR\n\nStep Function\n\nDatabricks\n\nHaving knowledge on visulalization tool will add value\n\n\n\nExperience\n\nShould have worked in technical delivery of above services preferable in similar organizations and having good communication skills\n\nLocation\n\nBangalore\n\nAcademics\n\nShould be engineer (Preferably Comp Science)\n\nCertifications\n\nPreference of AWS Data Engineer Certification\n\nMandatory Skills: AWS Python pyspark Databricks\nPreferred Skills: AZURE\nYears of Experience: 5-7\n\nEducation Qualification-Btech MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Technology\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nAWS Devops\n\nOptional Skills\n\nAcceptance Test Driven Development (ATDD), Acceptance Test Driven Development (ATDD), Accepting Feedback, Active Listening, Analytical Thinking, API Management, Application Development, Application Frameworks, Application Lifecycle Management, Application Software, Business Process Improvement, Business Process Management (BPM), Business Requirements Analysis, C++ Programming Language, Client Management, Code Review, Coding Standards, Communication, Computer Engineering, Computer Science, Continuous Integration/Continuous Delivery (CI/CD), Creativity, Debugging, Embracing Change, Emotional Regulation + 30 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Step Function, S3, AWS Glue, Emr, Cloud Watch, Redshift, Sql, Lambda, Py Spark, Databricks, Python, AWS"
Data Engineer - SAP HANA and Snow Flake,Ericsson,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Join our Team\n\nAbout this opportunity:\n\nEricsson's Automation Chapter is seeking a highly motivated and self-driven Data Engineer and Senior Data Engineer with strong expertise in SAP HANA and SAP BODS. The ideal candidates will be focused on SAP-centric development and integration, ensuring that enterprise data flows are robust, scalable, and optimized for analytics consumption. You will collaborate with a high-performing team that builds and supports end-to-end data solutions aligned with our SAP ecosystem. You are adaptable and a flexible problem-solver with deep hands-on experience in HANA modeling and ETL workflows, capable of switching contexts across a range of projects with varying scale and complexity.\n\nWhat you bring:\n\nDesign, develop, and optimize SAP HANA objects such as Calculation Views, SQL Procedures, and Custom Functions.\nDevelop robust and reusable ETL pipelines using SAP BODS for both SAP and third-party system integration.\nEnable seamless data flow between SAP ECC and external platforms, ensuring accuracy and performance.\nCollaborate with business analysts, architects, and integration specialists to translate requirements into technical deliverables.\nTune and troubleshoot HANA and BODS jobs for performance, scalability, and maintainability.\nEnsure compliance with enterprise data governance, lineage, and documentation standards.\nSupport ongoing enhancements, production issues, and business-critical data deliveries.\n\nExperience\n\n8+ years of experience in SAP data engineering roles.\nStrong hands-on experience in SAP HANA (native development, modeling, SQL scripting).\nProficient in SAP BODS, including job development, data flows, and integration techniques.\nExperience working with SAP ECC data structures, IDOCs, and remote function calls.\nKnowledge of data warehouse concepts, data modeling, and performance optimization techniques.\nStrong debugging and analytical skills, with the ability to independently drive technical solutions.\nFamiliarity with version control tools and SDLC processes.\nExcellent communication skills and ability to work collaboratively in cross-functional teams.\n\nEducation\n\nBachelor's degree in computer science, Information Systems, Electronics & Communication, or a related field.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 765249","performance optimization, SDLC processes, ETL pipelines, Idocs, Sap Hana, Sap Bods, Data Modeling, Version Control Tools, Sap Ecc, Sql Scripting"
"Senior Data Engineer, Global Converse, ITC",Nike,6-8 Years,,India,Login to check your skill match score,"Who You'll Work With\n\nYou will sit on the Converse Enterprise Data & Analytics technology team, reporting to the Enterprise Data & Analytics Director based at Converse HQ. You will provide people leadership for data engineers and scrum leads who are also based at ITC. Furthermore, you will be part of an agile scrum team, working closely with data product managers, lead engineers, and other cross-functional team members to deliver data and analytics capabilities for Converse.\n\nWho We Are Looking For\n\nThe Data Engineering Senior Supervisor possesses a unique blend of technical expertise, leadership acumen, and strategic vision. They oversee the development and maintenance of large-scale data systems, ensuring data quality, integrity, and security. From a leadership perspective, they have experience in managing and mentoring teams of data engineers, providing guidance on technical best practices, and fostering a culture of innovation and collaboration. They are skilled in agile project management methodologies and have a proven track record of delivering complex data engineering projects on time and within budget.\n\nThe Data Engineering Senior Supervisor needs to have a deep understanding of the organization's data strategy and is able to align their team's efforts with business objectives. Their expertise enables them to communicate effectively with stakeholders, support technical decision-making, and drive business growth through data-driven insights.\n\nExperienced in building cloud-scalable, real-time, and high-performance data lake solutions using Databricks, Snowflake, and/or AWS, with proficiency in relational SQL, scripting languages (Shell, Python), and source control tools (GitHub).\nSkilled in developing complex data solutions, with a strong understanding of end-to-end design, data structures, and algorithms. Proficient in workflow scheduling tools (Matillion, Brickflow) and familiar with Terraform, Serverless, and Jenkins.\nProven ability to lead and influence others, with experience in agile team leadership and influencing stakeholders. Effective communicator, both verbally and in writing, with the ability to identify and resolve people and process-related issues.\nStrong problem-solving and analytical mindset, with the ability to take a broad perspective to identify innovative solutions. Willing to learn new skills and technologies, and able to quickly pick up new programming languages, technologies, and frameworks.\nAbility to understand business needs and develop technical solutions to meet them. Strong understanding of solution and technical design.\nMinimum 6 years relevant work experience; Bachelor's degree or equivalent combination of education, experience or training\n\nWhat You'll Work On\n\nAs a Data Engineering Senior Supervisor on the Enterprise Data & Analytics team, you will drive work to completion through leading others along with hands-on development responsibilities. You will deliver quality components of a larger data pipeline to support data analytics products with guidance from experienced peers while working within an agile framework that focuses on delivering quality incremental value. As a Senior Supervisor, you will also be responsible for providing people leadership to data engineers, automation engineers, and scrum leads.\n\nBe responsible for the team's delivery of scalable data solutions to solve a customer need\nGuide the team in troubleshooting data issues and performing root cause analysis to proactively resolve product issues to improve data quality\nManage, mentor, and inspire a team; managing performance, goals and development potential\nBuild reusable components, frameworks and libraries at scale to support analytics products\nClean, prepare and optimize data for ingestion and consumption by implementing automated workflows\nCollaborate on the implementation of new data management projects and re-structure of the current data architecture\nBuild continuous integration, test-driven development and production deployment frameworks\nCollaboratively review design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards","Matillion, Brickflow, snowflake, Serverless, Github, Sql, Jenkins, Shell, Terraform, Databricks, Python, AWS"
IN_Senior Associate_ AWS Data Engineer_Advisory Corporate_Advisory_ Bangalore,PwC India,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nSAP\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.\n\nThose in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nPython,\n\nPy Spark,\n\nSql,\n\nRedshift ,\n\nS3 ,\n\nCloud Watch,\n\nLambda,\n\nAWS Glue\n\nEMR\n\nStep Function\n\nDatabricks\n\nHaving knowledge on visulalization tool will add value\n\n\n\nExperience\n\nShould have worked in technical delivery of above services preferable in similar organizations and having good communication skills\n\nLocation\n\nBangalore\n\nAcademics\n\nShould be engineer (Preferably Comp Science)\n\nCertifications\n\nPreference of AWS Data Engineer Certification\n\nMandatory Skills: AWS Python pyspark Databricks\nPreferred Skills: AZURE\nYears of Experience: 5-7\n\nEducation Qualification-Btech MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Technology\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nAWS Devops\n\nOptional Skills\n\nAcceptance Test Driven Development (ATDD), Acceptance Test Driven Development (ATDD), Accepting Feedback, Active Listening, Analytical Thinking, API Management, Application Development, Application Frameworks, Application Lifecycle Management, Application Software, Business Process Improvement, Business Process Management (BPM), Business Requirements Analysis, C++ Programming Language, Client Management, Code Review, Coding Standards, Communication, Computer Engineering, Computer Science, Continuous Integration/Continuous Delivery (CI/CD), Creativity, Debugging, Embracing Change, Emotional Regulation + 30 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Step Function, S3, AWS Glue, Emr, Cloud Watch, Redshift, Sql, Lambda, Py Spark, Databricks, Python, AWS"
Data Engineer (Talend &Pyspark),NTT Data,4-6 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:321800\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer (Talend &Pyspark) to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nJob Duties: Key Responsibilities:\n. Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n. Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n. Demonstrate proficiency in coding skills, utilizing languages such as PySpark, Talend to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n. Collaborate seamlessly across diverse technical stacks, including AWS.\n. Develop and deliver detailed presentations to effectively communicate complex technical concepts.\n. Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n. Adhere to Agile practices throughout the solution development process.\n. Design, build, and deploy databases and data stores to support organizational requirements.\nMinimum Skills Required: Basic Qualifications:\n. 4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n. Ability to travel at least 25%.\nPreferred Skills:\n. Demonstrate production experience in core data platforms such as AWS.\n. Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in S3 or other NoSQL storage systems.\n. Exhibit a strong understanding of Data integration technologies, encompassing Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services.\n. Showcase professional written and verbal communication skills to effectively convey complex technical concepts.\n. Undergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","NoSQL storage systems, NiFi, AWS Data Migration Services, Streamsets, Kafka, AWS, S3, Talend, Pyspark, Spark"
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324609\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Azure Data Engineer, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, Kudu, AWS Data Migration Services, ADLS, Streamsets, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Sr. Data Engineer,NTT Data,7-9 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:309994\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Sr. Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nPrimary Responsibility as Data Engineer\n1) Build and implement PySpark-based data pipelines in Azure Synapse to transform and load data into ADLS in Delta format\n2) Design and implement dimensional (star/snowflake) and 3NF data models. Optimized for access using Power BI\n3) Perform unit testing of data pipelines and its transformations\n4) Design and build metadata driven data pipelines using Pyspark in Azure Synapse\n5) Analyze and optimize Spark SQL queries\n6) Optimize integration of data lake with Power BI semantic model\n7) Collaborate with cross-functional teams (data architects, engineers, and POs) to ensure data models align with business needs\n8) Perform STM(source to target mapping) from source to multiple layers in the data lake\n9) Maintain version control and CI/CD pipelines in Git and Azure DevOps\n10) Integrate Azure Purview to enable access controls. Additionally implement row level security\n7+ Years of experience in : SQL, PySpark\n. Hands-on experience with Azure Synapse, ADLS, Delta format, and metadata-driven data pipelines\n. Experienced in implementing dimensional (star/snowflake) and 3NF data models\n.Experienced in PySpark and Spark SQL, including query optimization and performance tuning\n. Experienced in writing complex SQL including windowing functions\n. Experienced in performing Source-to-Target Mapping (STM) by analyzing source systems and defining transformation logic\n. Strong problem-solving and analytical skills for debugging and optimizing data pipelines in Azure Synapse\n. Familiarity with CI/CD practices in Git and Azure DevOps\n. Working experience in Azure DevOps-based development environment\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Delta format, 3NF data models, windowing functions, ADLS, Azure Purview, Sql, Spark SQL, Azure Synapse, Git, Pyspark, Azure DevOps"
Data Engineer,NTT Data,4-6 Years,,"Pune, India",IT/Computers - Software,"Req ID:324612\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Kudu, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,NTT Data,4-6 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324631\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Kudu, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
"Financial Data Engineer, Investments, Associate",BlackRock,3-6 Years,,"Mumbai, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, we are looking for a Data Engineer who enjoys building and supporting high impact data pipelines to solve complex challenges while working closely with your colleagues throughout the business.\n\nWe recognize that strength comes from diversity, and will embrace your outstanding skills, curiosity, drive, and passion while giving you the opportunity to grow technically while learning from hands-on leaders in technology and finance.\n\nWith over USD $11 trillion of assets we have an outstanding responsibility: our technology empowers millions of investors to save for retirement, pay for college, buy a home and improve their financial wellbeing.\n\nBeing a financial technologist at BlackRock means you get the best of both worlds: working for one of the most successful financial companies and also working in a software development team responsible for next generation technology and solutions.\n\nWe are seeking a high-reaching individual to help implement financial data engineering projects, initially focusing on our Index Fixed Income Group for the BGM DnA (Data and Analytics) team in India. We are a community of highly qualified Data Engineers, Content & DevOps Specialists who have a passion for working on data solutions that help drive the agenda for our business partners\n\nOur team is based in San Francisco, London & Hungary, and we will complete the global circle with a new engineering team in Mumbai.\n\nAbout BlackRock Global Markets\n\nBlackRock Global Markets (BGM) functions are at the core of BlackRock's markets and investments platform, including ETF and Index Investments (Engine), Global Trading, Securities Lending, Fixed Income, Liquidity and Financing. BGM is passionate about advancing the investment processes and platform architecture in these areas and on ensuring we engage with other market participants in a collaborative, strategic way.\n\nYou should be\n\nSomeone who is passionate about solving sophisticated business problems through data!\nCapable of the design, implementation, and optimization of data pipelines, ETL processes, and data storage solutions\nAble to work closely with multi-functional teams (e.g., Data Science, Product, Analytics, and Citizen Developer teams) to ensure the data infrastructure meets business needs.\nEnthusiastic about establishing and maintaining standard methodologies for data engineering, focusing on data quality, security, and scalability.\n\nKey Requirements\n\n3-6 years Data Engineering experience preferably in the financial sector\nFamiliarity with any aspect of Fixed Income Index and Market Data including ICE, Bloomberg, JP Morgan, FTSE/Russell, and IBOXX. Liquidity, Venue, and Direct Broker Dealer Market Maker Axe Data. Pricing Data from sources like S&P Global Live Bond Pricing or Bloombergs IBVAL.\nUnderstand Portfolio Management Fundamentals: Asset Management and FI Trading.\nA passion for Financial and Capital Markets.\nProven experience working in an agile development team.\nStrong problem solving skills.\nStrong SQL and Python skills with a proven track record optimizing SQL queries.\nCuriosity of financial markets.\n\nGood To Have\n\nBachelor's degree in Computer Science, Engineering, Finance, Economics, or a related field. A Master's degree or equivalent experience is a plus.\nKnowledge of Linux and scripting languages such as Bash\nExperience with MySQL, PostgreSQL, Greenplum, Snowflake or similar databases.\nStrong experience with ETL/ELT tools like DBT, Pentaho, Informatica or similar technologies.\nExperience with DevOps and tools like Azure DevOps\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","dbt, snowflake, Pentaho, PostgreSQL, Bash, Greenplum, Informatica, Sql, ELT, Linux, MySQL, Python, Azure DevOps, Etl"
Data Engineer,NTT Data,3-5 Years,,"Chennai, India",IT/Computers - Software,"Req ID:303790\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\nJob Duties: As a data engineer you will have a strong background in data engineering, an ability to interpret complex datasets, and the skills to provide actionable insights.\n- Works closely with the GenAI architect to design and implement automated data pipelines\n- Analyzes and understands source data and prepares and manages data sets\n- Supports CI/CD efforts\nExtract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.\n. Perform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.\n. Develop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.\n. Collaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.\n. Continuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.\n\nMinimum Skills Required: 3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nPreferred Skills:\n. Strong proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n. Experience with cloud data platforms such as [Platforms - e.g., AWS Redshift, Google BigQuery, Azure Synapse].\n. Familiarity with ETL (Extract, Transform, Load) processes and tools.\n. Knowledge of machine learning techniques and tools.\n. Experience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.\n. Understanding of data governance and data privacy regulations.\n. Ability to query and manipulate databases and data warehouses.\n. Excellent analytical and problem-solving skills.\n. Strong communication skills with the ability to explain complex data insights to non-technical stakeholders.\n. Detail-oriented with a commitment to accuracy.\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","R, Google BigQuery, Excel, Sql, AWS, Etl, Python, Azure Synapse, Redshift"
"Data Engineer - ADF, Data Bricks",Zensar Technologies,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Description\n\nDescription of role and key responsibilities\n\nThe candidate will be required to deliver to all stages of the data engineering process data ingestion, transformation, data modelling and data warehousing, and build self-service data products. The role is a mix of Azure cloud delivery and on-prem (SQL) development. Ultimately all on-prem will be migrated to cloud and decommissioned but we are only part way along that journey.\n\nThere will be a dual reporting line between the main business technology area (Asset Lending) providing the day-to-day direction and management of work items, and the Head of Data for Corporate Banking Technology who will provide guidance on overall Data strategy and alignment with wider bank.\n\nThe role itself will work closely with our Architect, Engineering lead, Analytics team, DevOps, DBAs, and upstream Application teams in Asset Finance, Working Capital and ABL.\n\nSpecifically, The Person Will\n\nWork closely with end-users and Data Analysts to understand the business and their data requirements\nCarry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks\nBuilding dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks\nBuild and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)\nBuild and maintain business focused data products and data marts\nBuild and maintain Azure Analysis Services databases and cubes\nShare support and operational duties within the wider engineering and data teams\nWork with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.\nHelp define test criteria to establish clear conditions for success and ensure alignment with business objectives.\nManage their user stories and acceptance criteria through to production into day-to-day support\nAssist in the testing and validation of new requirements and processes to ensure they meet business needs\n\nStay up-to-date with industry trends and best practices in data engineering\n\nCore skills and knowledge\n\nExcellent data analysis and exploration using T-SQL\nStrong SQL programming (stored procedures, functions)\nExtensive experience with SQL Server and SSIS\nKnowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)\nExperience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2\nExperience in building robust and performant ETL processes\nBuild and maintain Analysis Services databases and cubes (both multidimensional and tabular)\nExperience in using source control & ADO\nUnderstanding and experience of deployment pipelines\nExcellent analytical and problem-solving skills, with the ability to think critically and strategically.\nStrong communication and interpersonal skills, with the ability to engage and influence stakeholders at all levels.\nTo always act with integrity and embrace the philosophy of treating our customers fairly\nAnalytical, ability to arrive at solutions that fit current / future business processes\nEffective writing and verbal communication\nOrganisational skills: Ability to effectively manage and co-ordinate themselves.\nOwnership and self-motivation\nDelivery focus\nAssertive, resilient and persistent\nTeam oriented\nDeal well with pressure and highly effective at multi-tasking and juggling priorities\n\nAny other attributes that would be helpful, but not essential for the role.\n\nDeeper programming ability (C#, .Net Core)\nBuild infrastructure-as-code deployment pipelines\nAsset Finance knowledge\nVehicle Finance knowledge\nABL and Working Capital knowledge\n\nAny financial services and banking experience","Analysis Services, Synapse Analytics, Data Vault 2.0, Kimball dimensional modelling, Azure Data Factory, T-sql, SQL Server, Databricks, ADO, SSIS"
Senior Data Engineer,HCLTech,6-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role : Senior Data Engineer.\nExperience : 6 to 10 years.\nLocation : Bengaluru/Chennai/Hyderabad/Pune/Delhi NCR.\nNotice Period : Immediate to 45 Days.\nBusiness domain knowledge : SaaS, SFDC, NetSuite.\nAreas we support : Product, Finance (ARR reporting), GTM, Marketing, Sales.\nTech Stack : Fivetran, Snowflake, DBT, Tableau, GitHub.\nWhat we are looking for:\n1. SQL and data modeling at intermediate level - write complex SQL queries, build data model and experience with data transformation.\n2. Problem solver: Person who can weed through ambiguity of the ask.\n3. Bias for Action: Asks questions, reaches out to stakeholders, comes up with solutions.\n4. Communication: Effectively communicates with stakeholder and team members.\n5.Documentation: Can create BRD.\n6. Someone well versed in Finance (ARR reporting) and/or GTM (sales and marketing) would be an added advantage.\n7. Experience in SAAS, NetSuite and Salesforce will be a plus.\n8. Independent, self-starter, motivated and experience with working in an onsite/offshore environment.","dbt, snowflake, Fivetran, Github, Data Transformation, Tableau, Data Modeling, Sql"
"Data Engineer - ADF, Data Bricks",Zensar Technologies,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Description\n\nDescription of role and key responsibilities\n\nThe candidate will be required to deliver to all stages of the data engineering process data ingestion, transformation, data modelling and data warehousing, and build self-service data products. The role is a mix of Azure cloud delivery and on-prem (SQL) development. Ultimately all on-prem will be migrated to cloud and decommissioned but we are only part way along that journey.\n\nThere will be a dual reporting line between the main business technology area (Asset Lending) providing the day-to-day direction and management of work items, and the Head of Data for Corporate Banking Technology who will provide guidance on overall Data strategy and alignment with wider bank.\n\nThe role itself will work closely with our Architect, Engineering lead, Analytics team, DevOps, DBAs, and upstream Application teams in Asset Finance, Working Capital and ABL.\n\nSpecifically, The Person Will\n\nWork closely with end-users and Data Analysts to understand the business and their data requirements\nCarry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks\nBuilding dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks\nBuild and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)\nBuild and maintain business focused data products and data marts\nBuild and maintain Azure Analysis Services databases and cubes\nShare support and operational duties within the wider engineering and data teams\nWork with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.\nHelp define test criteria to establish clear conditions for success and ensure alignment with business objectives.\nManage their user stories and acceptance criteria through to production into day-to-day support\nAssist in the testing and validation of new requirements and processes to ensure they meet business needs\n\nStay up-to-date with industry trends and best practices in data engineering\n\nCore skills and knowledge\n\nExcellent data analysis and exploration using T-SQL\nStrong SQL programming (stored procedures, functions)\nExtensive experience with SQL Server and SSIS\nKnowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)\nExperience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2\nExperience in building robust and performant ETL processes\nBuild and maintain Analysis Services databases and cubes (both multidimensional and tabular)\nExperience in using source control & ADO\nUnderstanding and experience of deployment pipelines\nExcellent analytical and problem-solving skills, with the ability to think critically and strategically.\nStrong communication and interpersonal skills, with the ability to engage and influence stakeholders at all levels.\nTo always act with integrity and embrace the philosophy of treating our customers fairly\nAnalytical, ability to arrive at solutions that fit current / future business processes\nEffective writing and verbal communication\nOrganisational skills: Ability to effectively manage and co-ordinate themselves.\nOwnership and self-motivation\nDelivery focus\nAssertive, resilient and persistent\nTeam oriented\nDeal well with pressure and highly effective at multi-tasking and juggling priorities\n\nAny other attributes that would be helpful, but not essential for the role.\n\nDeeper programming ability (C#, .Net Core)\nBuild infrastructure-as-code deployment pipelines\nAsset Finance knowledge\nVehicle Finance knowledge\nABL and Working Capital knowledge\n\nAny financial services and banking experience","Analysis Services, Synapse Analytics, Data Vault 2.0, Kimball dimensional modelling, Azure Data Factory, T-sql, SQL Server, Databricks, ADO, SSIS"
"Financial Data Engineer, Investments, Associate",BlackRock,3-6 Years,,"Mumbai, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, we are looking for a Data Engineer who enjoys building and supporting high impact data pipelines to solve complex challenges while working closely with your colleagues throughout the business.\n\nWe recognize that strength comes from diversity, and will embrace your outstanding skills, curiosity, drive, and passion while giving you the opportunity to grow technically while learning from hands-on leaders in technology and finance.\n\nWith over USD $11 trillion of assets we have an outstanding responsibility: our technology empowers millions of investors to save for retirement, pay for college, buy a home and improve their financial wellbeing.\n\nBeing a financial technologist at BlackRock means you get the best of both worlds: working for one of the most successful financial companies and also working in a software development team responsible for next generation technology and solutions.\n\nWe are seeking a high-reaching individual to help implement financial data engineering projects, initially focusing on our Index Fixed Income Group for the BGM DnA (Data and Analytics) team in India. We are a community of highly qualified Data Engineers, Content & DevOps Specialists who have a passion for working on data solutions that help drive the agenda for our business partners\n\nOur team is based in San Francisco, London & Hungary, and we will complete the global circle with a new engineering team in Mumbai.\n\nAbout BlackRock Global Markets\n\nBlackRock Global Markets (BGM) functions are at the core of BlackRock's markets and investments platform, including ETF and Index Investments (Engine), Global Trading, Securities Lending, Fixed Income, Liquidity and Financing. BGM is passionate about advancing the investment processes and platform architecture in these areas and on ensuring we engage with other market participants in a collaborative, strategic way.\n\nYou should be\n\nSomeone who is passionate about solving sophisticated business problems through data!\nCapable of the design, implementation, and optimization of data pipelines, ETL processes, and data storage solutions\nAble to work closely with multi-functional teams (e.g., Data Science, Product, Analytics, and Citizen Developer teams) to ensure the data infrastructure meets business needs.\nEnthusiastic about establishing and maintaining standard methodologies for data engineering, focusing on data quality, security, and scalability.\n\nKey Requirements\n\n3-6 years Data Engineering experience preferably in the financial sector\nFamiliarity with any aspect of Fixed Income Index and Market Data including ICE, Bloomberg, JP Morgan, FTSE/Russell, and IBOXX. Liquidity, Venue, and Direct Broker Dealer Market Maker Axe Data. Pricing Data from sources like S&P Global Live Bond Pricing or Bloombergs IBVAL.\nUnderstand Portfolio Management Fundamentals: Asset Management and FI Trading.\nA passion for Financial and Capital Markets.\nProven experience working in an agile development team.\nStrong problem solving skills.\nStrong SQL and Python skills with a proven track record optimizing SQL queries.\nCuriosity of financial markets.\n\nGood To Have\n\nBachelor's degree in Computer Science, Engineering, Finance, Economics, or a related field. A Master's degree or equivalent experience is a plus.\nKnowledge of Linux and scripting languages such as Bash\nExperience with MySQL, PostgreSQL, Greenplum, Snowflake or similar databases.\nStrong experience with ETL/ELT tools like DBT, Pentaho, Informatica or similar technologies.\nExperience with DevOps and tools like Azure DevOps\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","dbt, snowflake, Pentaho, PostgreSQL, Bash, Greenplum, Informatica, Sql, ELT, Linux, MySQL, Python, Azure DevOps, Etl"
Data Engineer,NTT Data,3-5 Years,,"Chennai, India",IT/Computers - Software,"Req ID:303790\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\nJob Duties: As a data engineer you will have a strong background in data engineering, an ability to interpret complex datasets, and the skills to provide actionable insights.\n- Works closely with the GenAI architect to design and implement automated data pipelines\n- Analyzes and understands source data and prepares and manages data sets\n- Supports CI/CD efforts\nExtract, transform, and load (ETL) data from various sources, ensuring data quality, integrity, and accuracy.\n. Perform data cleansing, validation, and preprocessing to prepare structured and unstructured data for analysis.\n. Develop and execute queries, scripts, and data manipulation tasks using SQL, Python, or other relevant tools.\n. Collaborate with Data Scientists and other analysts to support predictive modeling, machine learning, and statistical analysis.\n. Continuously monitor data quality and proactively identify anomalies or discrepancies, recommending corrective actions.\n\nMinimum Skills Required: 3 + years of proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n3+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nPreferred Skills:\n. Strong proficiency in data analysis tools such as [Tools - e.g., Excel, SQL, R, Python].\n. Experience with cloud data platforms such as [Platforms - e.g., AWS Redshift, Google BigQuery, Azure Synapse].\n. Familiarity with ETL (Extract, Transform, Load) processes and tools.\n. Knowledge of machine learning techniques and tools.\n. Experience in a specific industry (e.g., financial services, healthcare, manufacturing) can be a plus.\n. Understanding of data governance and data privacy regulations.\n. Ability to query and manipulate databases and data warehouses.\n. Excellent analytical and problem-solving skills.\n. Strong communication skills with the ability to explain complex data insights to non-technical stakeholders.\n. Detail-oriented with a commitment to accuracy.\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","R, Google BigQuery, Excel, Sql, AWS, Etl, Python, Azure Synapse, Redshift"
Senior Data Engineer,HCLTech,6-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role : Senior Data Engineer.\nExperience : 6 to 10 years.\nLocation : Bengaluru/Chennai/Hyderabad/Pune/Delhi NCR.\nNotice Period : Immediate to 45 Days.\nBusiness domain knowledge : SaaS, SFDC, NetSuite.\nAreas we support : Product, Finance (ARR reporting), GTM, Marketing, Sales.\nTech Stack : Fivetran, Snowflake, DBT, Tableau, GitHub.\nWhat we are looking for:\n1. SQL and data modeling at intermediate level - write complex SQL queries, build data model and experience with data transformation.\n2. Problem solver: Person who can weed through ambiguity of the ask.\n3. Bias for Action: Asks questions, reaches out to stakeholders, comes up with solutions.\n4. Communication: Effectively communicates with stakeholder and team members.\n5.Documentation: Can create BRD.\n6. Someone well versed in Finance (ARR reporting) and/or GTM (sales and marketing) would be an added advantage.\n7. Experience in SAAS, NetSuite and Salesforce will be a plus.\n8. Independent, self-starter, motivated and experience with working in an onsite/offshore environment.","dbt, snowflake, Fivetran, Github, Data Transformation, Tableau, Data Modeling, Sql"
Data Engineer (Talend &Pyspark),NTT Data,4-6 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:321800\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer (Talend &Pyspark) to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nJob Duties: Key Responsibilities:\n. Design and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n. Provide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n. Demonstrate proficiency in coding skills, utilizing languages such as PySpark, Talend to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n. Collaborate seamlessly across diverse technical stacks, including AWS.\n. Develop and deliver detailed presentations to effectively communicate complex technical concepts.\n. Generate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n. Adhere to Agile practices throughout the solution development process.\n. Design, build, and deploy databases and data stores to support organizational requirements.\nMinimum Skills Required: Basic Qualifications:\n. 4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n. 2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n. Ability to travel at least 25%.\nPreferred Skills:\n. Demonstrate production experience in core data platforms such as AWS.\n. Possess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in S3 or other NoSQL storage systems.\n. Exhibit a strong understanding of Data integration technologies, encompassing Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services.\n. Showcase professional written and verbal communication skills to effectively convey complex technical concepts.\n. Undergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","NoSQL storage systems, NiFi, AWS Data Migration Services, Streamsets, Kafka, AWS, S3, Talend, Pyspark, Spark"
"Associate, Senior Data Engineer",BlackRock,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.\n\nThis role sits within Preqin, a part of BlackRock. Preqin plays a key role in how we are revolutionizing private markets data and technology for clients globally, complementing our existing Aladdin technology platform to deliver investment solutions for the whole portfolio.\n\nJoin the multi-disciplinary Orion team, where innovation meets real business impact. We're revolutionizing data management by harnessing the power of automation, machine learning, and AI to transform how we collect and process data. As part of the Orion team, you'll be at the forefront of innovation, developing and maintaining our cutting-edge workflow orchestration platform. This role offers the opportunity to work with advanced technologies to streamline large-scale data handling from diverse sources, ensuring that data is always accessible, accurate, and relevant to our clients needs.\n\nAbout The Role\n\nAs an Associate Data Engineer, you will be part of a high-performing engineering pod. Your work ensures our data flows are robust, scalable, and aligned with business needs, forming the backbone of Orion's data-driven products and insights. This role requires an individual with excellent technical skills to evolve our back-end infrastructure. Our Data Engineers have an opportunity to significantly accelerate these changes and help shape our organization for the future. You will utilize your expertise to ingest, model, and maintain data that enhances our internal data processing capabilities. As such, we seek an individual who demonstrates efficient and productive ways of processing data as this project increases in scale. We seek an individual who will be instrumental in bridging the gap between technical and non-technical team members, ensuring clear and effective communication of technical concepts and data-driven strategies.\n\nYour Responsibilities Will Include\n\nDesign and implement robust data models (dimensional, Data Vault, etc.) that support business intelligence and analytics requirements\nBuild scalable, reliable ETL/ELT pipelines using Python that process data from multiple sources\nCreate and maintain analytics-ready datasets with efficient query performance for reporting and business insights\nDevelop database schemas and optimize database performance for both transactional and analytical workloads\nEstablish and enforce data governance practices, including data quality standards and metadata management\nPrioritise work based on data-driven insights and outcome-based goals in collaboration with stakeholders.\nWork closely with engineering teams across the business, ensuring the best technical solutions are adopted, and elevate development standards through knowledge sharing and best practices.\nCollaborate across engineering, product, and data scientist teams to translate business requirements into technical solutions and ensuring our data assets are organized and accessible.\nActively participate in technical discussions about new product directions, data modelling, and architectural decisions, ensuring our technology platform remains extensible.\nAccelerate data collection as scale from millions of sources and across various databases.\nImprove and maintain observability and alerting across our data systems, ensuring visibility into pipeline health and data quality.\nPropose smart, pragmatic, and diverse approaches to address a variety of business problems.\nExplore new technologies, approaches, and ideas that help to drive our business goals in unexpected ways.\n\nWhat We Are Looking For\n\nAt least 4 years of experience in data engineering with strong database expertise\nExperience building ETL/ELT pipelines using Python and implementing efficient data processing workflows\nStrong SQL skills with ability to write complex queries for reporting and analytics needs\nExperience with data modelling methodologies (dimensional/Kimball, Data Vault) and database schema design\nProficiency working with both transactional and analytical database systems (Postgres, Snowflake, etc.)\nExperience of working within cloud provider services Azure or AWS (preferred) and utilisation of infrastructure as code\nA data-driven mindset to make development decisions based on robust analyses\nAbility to collaborate effectively with designers, engineering and data scientist teams to build our technical solutions\nYou have driven technical solution design, taking the balance of engineering quality, testing, scalability and security into consideration\nA let's do it and challenge accepted attitude when faced with less known or challenging tasks, with a willingness to learn new technologies and ways of working\nExcellent verbal and written communication and interpersonal skills, with the ability to influence at all organizational levels and bridge technical perspectives\nProficiency in English required; additional languages and prior work experience at a global firm are desirable\nExperience processing structured and unstructured data\nAbility to perform well in a fast-paced environment, developing iterative sustainable solutions with best practices (security, code quality, documentation) and long-term vision\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","kimball, snowflake, data vault, Postgres, Azure, Sql, Python, ELT, AWS, Etl"
Data Engineer,NTT Data,4-6 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324638\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\nKey Responsibilities:\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\nAdhere to Agile practices throughout the solution development process.\nDesign, build, and deploy databases and data stores to support organizational requirements.\nBasic Qualifications:\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\nExperience with Informatica, Python, Databricks, Azure Data Engineer\nAbility to travel at least 25%.\nPreferred Skills:\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\nUndergraduate or Graduate degree preferred\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","Kudu, Google DataProc, snowflake, HDFS, Azure DataFactory, GCS, AWS Data Migration Services, ADLS, Streamsets, Azure Data Engineer, NiFi, Databricks, Java, Solr, Hadoop, Kafka, Cassandra, Informatica, Gcp, Scala, S3, AWS, Python, Elasticsearch, Spark"
Data Engineer,Havells India Ltd,3-5 Years,,"Noida, India",Login to check your skill match score,"Job Title: Data Engineer\nLocation: Noida\nExperience: 3+ years\nJob Description: We are seeking a skilled and experienced Data Engineer to join our dynamic team. The ideal candidate will have a strong background in data engineering, with a focus on PySpark, Python, and SQL. Experience with Azure Databricks is a plus.\nKey Responsibilities:\nDesign, develop, and maintain scalable data pipelines and systems.\nWork closely with data scientists and analysts to ensure data quality and availability.\nImplement data integration and transformation processes using PySpark and Python.\nOptimize and maintain SQL databases and queries.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions.\nMonitor and troubleshoot data pipeline issues to ensure data integrity and performance.\nRequired Skills and Qualifications:\nBachelor's degree in Computer Science, Information Technology, or a related field.\n3+ years of experience in data engineering.\nProficiency in PySpark, Python, and SQL.\nExperience with Azure Databricks is a plus.\nStrong problem-solving skills and attention to detail.\nExcellent communication and teamwork abilities.\nPreferred Qualifications:\nExperience with cloud platforms such as Azure, AWS, or Google Cloud.\nKnowledge of data warehousing concepts and technologies.\nFamiliarity with ETL tools and processes.\nHow to Apply: Apart from Easy apply on Linkedin also Click on this link https://forms.office.com/r/N0nYycJ36P\n#DataEngineer #Hiring #JobOpening #PySpark #Python #SQL #AzureDatabricks #TechJobs #DataEngineering #CareerOpportunity","Pyspark, Azure Databricks, Python, Sql"
Senior Data Engineer,Ericsson,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Join our Team\n\nAbout this opportunity:\n\nEricsson's Automation Chapter is seeking a highly motivated and self-driven Data Engineer and Senior Data Engineer with strong expertise in SAP HANA and SAP BODS. The ideal candidates will be focused on SAP-centric development and integration, ensuring that enterprise data flows are robust, scalable, and optimized for analytics consumption. You will collaborate with a high-performing team that builds and supports end-to-end data solutions aligned with our SAP ecosystem. You are adaptable and a flexible problem-solver with deep hands-on experience in HANA modeling and ETL workflows, capable of switching contexts across a range of projects with varying scale and complexity.\n\nWhat you bring:\n\nDesign, develop, and optimize SAP HANA objects such as Calculation Views, SQL Procedures, and Custom Functions.\nDevelop robust and reusable ETL pipelines using SAP BODS for both SAP and third-party system integration.\nEnable seamless data flow between SAP ECC and external platforms, ensuring accuracy and performance.\nCollaborate with business analysts, architects, and integration specialists to translate requirements into technical deliverables.\nTune and troubleshoot HANA and BODS jobs for performance, scalability, and maintainability.\nEnsure compliance with enterprise data governance, lineage, and documentation standards.\nSupport ongoing enhancements, production issues, and business-critical data deliveries.\n\nExperience\n\n8+ years of experience in SAP data engineering roles.\nStrong hands-on experience in SAP HANA (native development, modeling, SQL scripting).\nProficient in SAP BODS, including job development, data flows, and integration techniques.\nExperience working with SAP ECC data structures, IDOCs, and remote function calls.\nKnowledge of data warehouse concepts, data modeling, and performance optimization techniques.\nStrong debugging and analytical skills, with the ability to independently drive technical solutions.\nFamiliarity with version control tools and SDLC processes.\nExcellent communication skills and ability to work collaboratively in cross-functional teams.\n\nEducation\n\nBachelor's degree in computer science, Information Systems, Electronics & Communication, or a related field.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 763290","performance optimization, SDLC processes, ETL pipelines, Idocs, Sap Hana, Sap Bods, Data Modeling, Version Control Tools, Sql Scripting, Sap Ecc"
Data Modeler - Principal Data Engineer - R01548372,Brillio,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Principal Data Engineer\n\nPrimary Skills\n\nData Modeling (ER Studio/ER Win)\n\nSpecializationJob requirements\n\nAbout Brillio:\nBrillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.\nBrillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.\n10+ years of overall IT experience with more than 2+ years of experience in Snowflake with hands on experience in modelling\nIn-depth understanding of Data Lake/ODS, ETL concept and modeling structure principles.\nAbility to partner closely with the clients to deliver and drive Physical and logical model solutions\nExperience in Data warehousing - Dimensions, Facts, and Data modeling.\nExcellent communication and ability to lead teams.\nGood to have exposure to AWS eco systems.\nExcellent communication and ability to lead teams. Technical Skills\nData Modeling concepts (Advanced Level)\nModeling experience across large volume-based environments\nExperience with ER Studio\nOverall understanding of Database space (Databases, Data Warehouses, Reporting, Analytics)\nWell versed with Data Modeling platforms like SQLDBM etc\nStrong skills in Entity Relationship Modeling\nGood knowledge about Database Designing Administration, Advanced Data Modeling for Star Schema design\nGood understanding of Normalized Denormalized Star Snowflake design concepts SQL query development for investigation and analysis","Entity Relationship Modeling, SQL query development, snowflake, SQLDBM, Advanced Data Modeling, Star Schema Design, Data Modeling, Data Warehousing, Database Designing, Data Lake, Etl, Er Studio"
IN_Senior Associate_ AWS Data Engineer_Advisory Corporate_Advisory_ Bangalore,PwC India,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nSAP\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.\n\nThose in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nPython,\n\nPy Spark,\n\nSql,\n\nRedshift ,\n\nS3 ,\n\nCloud Watch,\n\nLambda,\n\nAWS Glue\n\nEMR\n\nStep Function\n\nDatabricks\n\nHaving knowledge on visulalization tool will add value\n\n\n\nExperience\n\nShould have worked in technical delivery of above services preferable in similar organizations and having good communication skills\n\nLocation\n\nBangalore\n\nAcademics\n\nShould be engineer (Preferably Comp Science)\n\nCertifications\n\nPreference of AWS Data Engineer Certification\n\nMandatory Skills: AWS Python pyspark Databricks\nPreferred Skills: AZURE\nYears of Experience: 5-7\n\nEducation Qualification-Btech MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Technology\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nAWS Devops\n\nOptional Skills\n\nAcceptance Test Driven Development (ATDD), Acceptance Test Driven Development (ATDD), Accepting Feedback, Active Listening, Analytical Thinking, API Management, Application Development, Application Frameworks, Application Lifecycle Management, Application Software, Business Process Improvement, Business Process Management (BPM), Business Requirements Analysis, C++ Programming Language, Client Management, Code Review, Coding Standards, Communication, Computer Engineering, Computer Science, Continuous Integration/Continuous Delivery (CI/CD), Creativity, Debugging, Embracing Change, Emotional Regulation + 30 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Step Function, S3, AWS Glue, Emr, Cloud Watch, Redshift, Sql, Lambda, Py Spark, Databricks, Python, AWS"
"Senior Data Engineer, Global Converse, ITC",Nike,6-8 Years,,India,Login to check your skill match score,"Who You'll Work With\n\nYou will sit on the Converse Enterprise Data & Analytics technology team, reporting to the Enterprise Data & Analytics Director based at Converse HQ. You will provide people leadership for data engineers and scrum leads who are also based at ITC. Furthermore, you will be part of an agile scrum team, working closely with data product managers, lead engineers, and other cross-functional team members to deliver data and analytics capabilities for Converse.\n\nWho We Are Looking For\n\nThe Data Engineering Senior Supervisor possesses a unique blend of technical expertise, leadership acumen, and strategic vision. They oversee the development and maintenance of large-scale data systems, ensuring data quality, integrity, and security. From a leadership perspective, they have experience in managing and mentoring teams of data engineers, providing guidance on technical best practices, and fostering a culture of innovation and collaboration. They are skilled in agile project management methodologies and have a proven track record of delivering complex data engineering projects on time and within budget.\n\nThe Data Engineering Senior Supervisor needs to have a deep understanding of the organization's data strategy and is able to align their team's efforts with business objectives. Their expertise enables them to communicate effectively with stakeholders, support technical decision-making, and drive business growth through data-driven insights.\n\nExperienced in building cloud-scalable, real-time, and high-performance data lake solutions using Databricks, Snowflake, and/or AWS, with proficiency in relational SQL, scripting languages (Shell, Python), and source control tools (GitHub).\nSkilled in developing complex data solutions, with a strong understanding of end-to-end design, data structures, and algorithms. Proficient in workflow scheduling tools (Matillion, Brickflow) and familiar with Terraform, Serverless, and Jenkins.\nProven ability to lead and influence others, with experience in agile team leadership and influencing stakeholders. Effective communicator, both verbally and in writing, with the ability to identify and resolve people and process-related issues.\nStrong problem-solving and analytical mindset, with the ability to take a broad perspective to identify innovative solutions. Willing to learn new skills and technologies, and able to quickly pick up new programming languages, technologies, and frameworks.\nAbility to understand business needs and develop technical solutions to meet them. Strong understanding of solution and technical design.\nMinimum 6 years relevant work experience; Bachelor's degree or equivalent combination of education, experience or training\n\nWhat You'll Work On\n\nAs a Data Engineering Senior Supervisor on the Enterprise Data & Analytics team, you will drive work to completion through leading others along with hands-on development responsibilities. You will deliver quality components of a larger data pipeline to support data analytics products with guidance from experienced peers while working within an agile framework that focuses on delivering quality incremental value. As a Senior Supervisor, you will also be responsible for providing people leadership to data engineers, automation engineers, and scrum leads.\n\nBe responsible for the team's delivery of scalable data solutions to solve a customer need\nGuide the team in troubleshooting data issues and performing root cause analysis to proactively resolve product issues to improve data quality\nManage, mentor, and inspire a team; managing performance, goals and development potential\nBuild reusable components, frameworks and libraries at scale to support analytics products\nClean, prepare and optimize data for ingestion and consumption by implementing automated workflows\nCollaborate on the implementation of new data management projects and re-structure of the current data architecture\nBuild continuous integration, test-driven development and production deployment frameworks\nCollaboratively review design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards","Matillion, Brickflow, snowflake, Serverless, Github, Sql, Jenkins, Shell, Terraform, Databricks, Python, AWS"
Data Engineer - SAP HANA and Snow Flake,Ericsson,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Join our Team\n\nAbout this opportunity:\n\nEricsson's Automation Chapter is seeking a highly motivated and self-driven Data Engineer and Senior Data Engineer with strong expertise in SAP HANA and SAP BODS. The ideal candidates will be focused on SAP-centric development and integration, ensuring that enterprise data flows are robust, scalable, and optimized for analytics consumption. You will collaborate with a high-performing team that builds and supports end-to-end data solutions aligned with our SAP ecosystem. You are adaptable and a flexible problem-solver with deep hands-on experience in HANA modeling and ETL workflows, capable of switching contexts across a range of projects with varying scale and complexity.\n\nWhat you bring:\n\nDesign, develop, and optimize SAP HANA objects such as Calculation Views, SQL Procedures, and Custom Functions.\nDevelop robust and reusable ETL pipelines using SAP BODS for both SAP and third-party system integration.\nEnable seamless data flow between SAP ECC and external platforms, ensuring accuracy and performance.\nCollaborate with business analysts, architects, and integration specialists to translate requirements into technical deliverables.\nTune and troubleshoot HANA and BODS jobs for performance, scalability, and maintainability.\nEnsure compliance with enterprise data governance, lineage, and documentation standards.\nSupport ongoing enhancements, production issues, and business-critical data deliveries.\n\nExperience\n\n8+ years of experience in SAP data engineering roles.\nStrong hands-on experience in SAP HANA (native development, modeling, SQL scripting).\nProficient in SAP BODS, including job development, data flows, and integration techniques.\nExperience working with SAP ECC data structures, IDOCs, and remote function calls.\nKnowledge of data warehouse concepts, data modeling, and performance optimization techniques.\nStrong debugging and analytical skills, with the ability to independently drive technical solutions.\nFamiliarity with version control tools and SDLC processes.\nExcellent communication skills and ability to work collaboratively in cross-functional teams.\n\nEducation\n\nBachelor's degree in computer science, Information Systems, Electronics & Communication, or a related field.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 765249","performance optimization, SDLC processes, ETL pipelines, Idocs, Sap Hana, Sap Bods, Data Modeling, Version Control Tools, Sap Ecc, Sql Scripting"
IN_Associate_GCP Data engineer_D&A_Advisory_Gurgaon,PwC India,2-5 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nFS X-Sector\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nAssociate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary Senior Associate - GCP Data Engineer\n\nResponsibilities\n\nRole : Sr. Associate\n\nExp : 2-5 years\n\nPwC India is seeking a talented GCP Data Engineer to join our team in Mumbai or Gurgaon. The ideal candidate will have 2-5 years of experience with a strong focus on GCP services, data engineering, and analytics. This role offers an exciting opportunity to work on cutting-edge projects for global clients while leveraging your expertise in cloud technologies and data management.\n\nKey Responsibilities:\n\nGCP Service Implementation:\n\nDesign, develop, and maintain data solutions using GCP services, with a particular focus on Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler\nImplement and optimize data lakes and data warehouses on GCP platforms.\n\nData Pipeline Development:\n\nCreate and maintain efficient ETL processes using PySpark and other relevant tools.\nDevelop scalable and performant data pipelines to process large volumes of data.\nImplement data quality checks and monitoring systems to ensure data integrity.\n\nDatabase Management:\n\nProficient with SQL and NoSQL databases, optimizing queries and database structures for performance.\nDesign and implement database schemas that align with business requirements and data models.\n\nPerformance Optimization:\n\nContinuously monitor and optimize the performance of data processing jobs and queries.\nImplement best practices for cost optimization in AWS environments.\nTroubleshoot and resolve performance bottlenecks in data pipelines and analytics processes.\n\nCollaboration and Documentation:\n\nWork closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nDevelop and maintain comprehensive documentation for data architectures, processes, and best practices.\nParticipate in code reviews and contribute to the team's knowledge base.\n\nRequired Qualifications:\n\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n2-5 years of experience in data engineering, with a focus on GCP technologies.\nStrong hands-on experience with GCP services, particularly Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler\nProficiency in Python and PySpark for data processing and analysis.\nExtremely good with SQL/PL SQL\nDemonstrated ability to optimize data pipelines and queries for performance.\nStrong problem-solving skills and attention to detail.\n\nPreferred Skills:\n\nGCP certifications\nFamiliarity with data visualization tools (e.g., Tableau, Power BI).\nExperience with data modeling and data warehouse concepts.\nInnovation thinking and creativity in solution delivery\n\nMandatory Skill Sets\n\n\nGCP Cloud Services/Python/PySpark\n\nPreferred Skill Sets\n\n\nGCP Cloud Services/Python/PySpark\n\nYears Of Experience Required\n\n\n2-5 years\n\nEducation Qualification\n\nBE/BTech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nGood Clinical Practice (GCP)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis, Intellectual Curiosity, Java (Programming Language), Market Development + 7 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Big Query, Data Proc, GCP Cloud Services, Cloud Scheduler, Cloud Composer, Cloud Run, Data Flow, Pyspark, Pl Sql, Sql, Workflow, Python"
IN_Associate_GCP Data engineer_D&A_Advisory_Gurgaon,PwC India,2-5 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nFS X-Sector\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nAssociate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary Senior Associate - GCP Data Engineer\n\nResponsibilities\n\nRole : Sr. Associate\n\nExp : 2-5 years\n\nPwC India is seeking a talented GCP Data Engineer to join our team in Mumbai or Gurgaon. The ideal candidate will have 2-5 years of experience with a strong focus on GCP services, data engineering, and analytics. This role offers an exciting opportunity to work on cutting-edge projects for global clients while leveraging your expertise in cloud technologies and data management.\n\nKey Responsibilities:\n\nGCP Service Implementation:\n\nDesign, develop, and maintain data solutions using GCP services, with a particular focus on Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler\nImplement and optimize data lakes and data warehouses on GCP platforms.\n\nData Pipeline Development:\n\nCreate and maintain efficient ETL processes using PySpark and other relevant tools.\nDevelop scalable and performant data pipelines to process large volumes of data.\nImplement data quality checks and monitoring systems to ensure data integrity.\n\nDatabase Management:\n\nProficient with SQL and NoSQL databases, optimizing queries and database structures for performance.\nDesign and implement database schemas that align with business requirements and data models.\n\nPerformance Optimization:\n\nContinuously monitor and optimize the performance of data processing jobs and queries.\nImplement best practices for cost optimization in AWS environments.\nTroubleshoot and resolve performance bottlenecks in data pipelines and analytics processes.\n\nCollaboration and Documentation:\n\nWork closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nDevelop and maintain comprehensive documentation for data architectures, processes, and best practices.\nParticipate in code reviews and contribute to the team's knowledge base.\n\nRequired Qualifications:\n\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n2-5 years of experience in data engineering, with a focus on GCP technologies.\nStrong hands-on experience with GCP services, particularly Big Query, Data Proc, Workflow, Cloud Run, Cloud Composer, Data Flow, Cloud Scheduler\nProficiency in Python and PySpark for data processing and analysis.\nExtremely good with SQL/PL SQL\nDemonstrated ability to optimize data pipelines and queries for performance.\nStrong problem-solving skills and attention to detail.\n\nPreferred Skills:\n\nGCP certifications\nFamiliarity with data visualization tools (e.g., Tableau, Power BI).\nExperience with data modeling and data warehouse concepts.\nInnovation thinking and creativity in solution delivery\n\nMandatory Skill Sets\n\n\nGCP Cloud Services/Python/PySpark\n\nPreferred Skill Sets\n\n\nGCP Cloud Services/Python/PySpark\n\nYears Of Experience Required\n\n\n2-5 years\n\nEducation Qualification\n\nBE/BTech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nGood Clinical Practice (GCP)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis, Intellectual Curiosity, Java (Programming Language), Market Development + 7 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Big Query, Data Proc, GCP Cloud Services, Cloud Scheduler, Cloud Composer, Cloud Run, Data Flow, Pyspark, Pl Sql, Sql, Workflow, Python"
Senior Data Engineer,HCLTech,6-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role : Senior Data Engineer.\nExperience : 6 to 10 years.\nLocation : Bengaluru/Chennai/Hyderabad/Pune/Delhi NCR.\nNotice Period : Immediate to 45 Days.\nBusiness domain knowledge : SaaS, SFDC, NetSuite.\nAreas we support : Product, Finance (ARR reporting), GTM, Marketing, Sales.\nTech Stack : Fivetran, Snowflake, DBT, Tableau, GitHub.\nWhat we are looking for:\n1. SQL and data modeling at intermediate level - write complex SQL queries, build data model and experience with data transformation.\n2. Problem solver: Person who can weed through ambiguity of the ask.\n3. Bias for Action: Asks questions, reaches out to stakeholders, comes up with solutions.\n4. Communication: Effectively communicates with stakeholder and team members.\n5.Documentation: Can create BRD.\n6. Someone well versed in Finance (ARR reporting) and/or GTM (sales and marketing) would be an added advantage.\n7. Experience in SAAS, NetSuite and Salesforce will be a plus.\n8. Independent, self-starter, motivated and experience with working in an onsite/offshore environment.","dbt, snowflake, Fivetran, Github, Data Transformation, Tableau, Data Modeling, Sql"
Data Engineer,Bristlecone,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Summary\n\nJOB DESCRIPTION\n\nWe are looking for a skilled and motivated GCP Data Engineer to join our team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and systems on the Google Cloud Platform. You will work closely with analytics, data science, and business teams to deliver reliable and efficient data solutions that power decision-making and innovation.\n\nResponsibilities\n\nKey Responsibilities:\n\nDesign, implement, and optimize scalable datapipelines on GoogleCloudPlatform ( GCP ) .\nBuild and maintain ETLworkflows using GCP services such as CloudDataflow , CloudDataproc , and BigQuery .\nDevelop data models and schemas that support analytics and reporting needs.\nCollaborate with stakeholders to gather business requirements and translate them into technical solutions.\nEnsure data quality, security, and compliance with organizational policies.\nMonitor and troubleshoot data pipeline performance and reliability.\nWork with APIs and external data sources to integrate data into the cloud environment.\nStay updated with advancements in GCP services and data engineering best practices.\n\nQualifications\n\nRequired Skills and Qualifications:\n\nStrong proficiency in GoogleCloudPlatform services, including BigQuery , CloudStorage , Pub / Sub , and CloudComposer .\nExperience with programming languages like Python , Java , or SQL for data processing and transformation.\nKnowledge of datamodeling , datawarehousing , and databasedesign .\nFamiliarity with CI / CDpipelines and workflow orchestration tools (e.g., ApacheAirflow ).\nUnderstanding of IAMpolicies and data security best practices in GCP.\nStrong analytical and problem-solving skills.\nEffective communication and teamwork abilities.\n\nPreferred Qualifications\n\nGCP certifications such as ProfessionalDataEngineer or AssociateCloudEngineer .\nExperience with Terraform or CloudDeploymentManager for infrastructure automation.\nExposure to machinelearning workflows and tools like AIPlatform .\nFamiliarity with DevOpsconcepts and containerization tools like Docker and Kubernetes .\nExperience working with large-scale, distributed systems and big data frameworks like ApacheSpark .\n\nAbout Us\n\nABOUT US\n\nBristlecone is the leading provider of AI-powered application transformation services for the connected supply chain. We empower our customers with speed, visibility, automation, and resiliency to thrive on change.\n\nOur transformative solutions in Digital Logistics, Cognitive Manufacturing, Autonomous Planning, Smart Procurement and Digitalization are positioned around key industry pillars and delivered through a comprehensive portfolio of services spanning digital strategy, design and build, and implementation across a range of technology platforms.\n\nBristlecone is ranked among the top ten leaders in supply chain services by Gartner. We are headquartered in San Jose, California, with locations across North America, Europe and Asia, and over 2,500 consultants. Bristlecone is part of the $19.4 billion Mahindra Group.\n\nEqual Opportunity Employer\n\nBristlecone is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status .\n\nInformation Security Responsibilities\n\nUnderstand and adhere to Information Security policies, guidelines and procedure, practice them for protection of organizational data and Information System.\nTake part in information security training and act while handling information.\nReport all suspected security and policy breach to InfoSec team or appropriate authority (CISO).\nUnderstand and adhere to the additional information security responsibilities as part of the assigned job role.","Java, BigQuery, Docker, Terraform, Kubernetes, Python, Sql"
(IND) Software Engineer III CRM Data Engineer,Walmart Global Tech India,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Position Summary...\n\nDemonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales. Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities. Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.\n\nWhat you'll do...\n\nTeam and Position Summary: This role is on our Marketplace Seller Acquisition and Onboarding team where we own the Marketplace Engineering team is at the forefront of building core platforms and services to enable Walmart to deliver vast selection at competitive prices and with best-in-class seller onboarding experience by enabling third-party sellers to list, sell and manage their products to our customers on walmart.com. We do this by managing the entire seller lifecycle, monitoring customer experience, and delivering high-value insights to our sellers to help them plan their assortment, price, inventory. The team also actively collaborates with partner platform teams to ensure we continue to deliver the best experience to our sellers and our customers. This role will be focused on the Marketplace. Position Responsibilities: You want a challenge Come join a team that is merging digital and physical, building real-time systems at scale, responding quicker to changes. Youll sweep us off our feet if you\n\nAct as the Senior Software Engineer for the team, taking ownership of technical projects and solutions.\n\nLead by example, setting technical standards and driving overall technical architecture and design.\n\nMentor junior developers, enhancing their skills and understanding of Android development.\n\nDesire to keep up with technology trends\n\nEncouraging others to grow and be curious. Provide technical leadership in every stage of the development process, from design to deployment, ensuring adherence to best practices.\n\nHave the desire to learn\n\nMaintain and improve application performance, compatibility, and responsiveness across various devices and Android versions.\n\nDrive for engineering and operational excellence, delivering high-quality solutions and processes.\n\nYoull make an impact byAs a Senior Software Engineer for Walmart, youll have the opportunity to Apply and/or develop CRM solutions to develop efficient and scalable models at Walmart scale. Through this role you have an opportunity to develop intuitive software that meets and exceeds the needs of the customer and the company. You also get to collaborate with team members to develop best practices and requirements for the software. In this role it would be important for you to professionally maintain all codes and create updates regularly to address the customers and companys concerns. You will show your skills in analyzing and testing programs/products before formal launch to ensure flawless performance. Troubleshooting coding problems quickly and efficiently will offer you a chance to grow your skills in a high-pace, high-impact environment. Software security is of prime importance and by developing programs that monitor sharing of private information, you will be able to add tremendous credibility to your work. You will also be required to seek ways to improve the software and its effectiveness. Adhere to Company policies, procedures, mission, values, and standards of ethics and integrityWhat youll do\n\nLead a team of senior Individual Contributors\n\nLeads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirements, wire frames); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with project team and cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery and hand-offs; interacting with project manager to provinput on project plan; and providing leadership to the project team.\n\nSupports business objectives by collaborating with business partners to identify opportunities; addressing high-priority initiatives (for example, business strategy, technical feasibility, implementation alternatives); identifying short- and long-term solutions; and leading cross-functional partnership.\n\nUtilizes industry research to improve Wal-Marts technology environment by analyzing industry best practices; bench marking industry against internal processes and solutions; researching or influencing future industry solutions for fit with internal needs; and defining software development guidelines, standards and processes.\n\nPosition Requirements:Preferred skills: Agentforce, Salesforce-managed models Gen AI, LLMsMinimum qualifications:\n\n3-5 years of Software Engineer experience with Salesforce.com platfrom knowledge and good to have knowledge on Salesforce-managed models Gen AI, LLMs\n\nProven working experience as a CRM Data Engineer with a minimum of 3 years in the field.\nStrong programming skills in Scala and experience with Spark for data processing and analytics\nFamiliarity with Google Cloud Platform (GCP) services such as Big Query, GCS, Dataproc, Pub/Sub, etc.\n\nExperience with data modeling, data integration, and ETL processes\n\nExcellent working experience in Salesforce with Apex, Visual Force, Lightning, andForce.com\n\nStrong Knowledge of Sales cloud, Service cloud, experience cloud (community cloud)\n\nExperience in Application customization and development, including Lightning pages, Lightning Web Components, Aura Components, Apex (classes and triggers).\n\nExperience in integration of salesforce with other systems using Salesforce APIs, SOAP, Rest API etc.\n\nProficient with Microsoft Visual Studio, Salesforce Lightning Design System, and the Salesforce development lifecycle.\n\nKnowledge of Tools such as Data-Loader, ANT, Workbench, GIT, Bit-Bucket Version Control.\n\nknowledge of deployment activities (CI/CD) between the Salesforce environments.\n\nKnowledge of high-quality professional software engineering practices for agile software development cycle, including coding standards, code reviews, source control management, build processes, testing, and deployment.\n\nFundamental knowledge of design patterns\n\nExperience in communicating effectively with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.\n\nMentoring the team members to meet the clients needs and holding them accountable for high standards of delivery.\n\nBeing able to understand and relate technology integration scenarios and be able to apply these learnings in complex troubleshooting scenarios.\n\nRESPONSIBILITIES:\n\nWriting and reviewing great quality code\n\nUnderstanding functional requirements thoroughly and analysing the clients needs in the context of the project\n\nEnvisioning the overall solution for defined functional and non-functional requirements, and being able to define technologies, patterns, and frameworks to realize it.\n\nDetermining and implementing design methodologies and tool sets\n\nEnabling application development by coordinating requirements, schedules, and activities.\n\nBeing able to lead/support UAT and production roll outs.\n\nCreating, understanding, and validating WBS and estimated effort for given module/task, and being able to justify it.\n\nAddressing issues promptly, responding positively to setbacks and challenges with a mindset of continuous improvement\n\nGiving constructive feedback to the team members and setting clear expectations.\n\nHelping the team in troubleshooting and resolving of complex bugs\n\nComing up with solutions to any issue that is raised during code/design review and being able to justify the decision taken.\n\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.\n\nAbout Walmart Global Tech Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionals within the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. Flexible, hybrid work We use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives. Benefits Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include a host of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more. Equal Opportunity Employer: Walmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being respectful of all people.\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nMinimum Qualifications:Option 1: Bachelor's degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nCertification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master's degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area\n\nPrimary Location...\n\n4,5,6, 7 Floor, Building 10, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India R-2156629","Big Query, Salesforce APIs, Aura Components, Data-Loader, Pub Sub, Lightning Web Components, Lightning pages, LLMs, Bit-Bucket, force.com, Salesforce Lightning Design System, Gen AI, workbench, Salesforce-managed models, GCS, Ant, Microsoft Visual Studio, Visual Force, APEX, Scala, Salesforce.com, Soap, Rest Api, Dataproc, Git, Service Cloud, Lightning, Spark, Sales Cloud, Community Cloud"
Data Engineer,S&P Global Market Intelligence,4-6 Years,,India,Banking/Accounting/Financial Services,"About the Role:\n09\nS&P Global Commodity Insights\nThe Team: ThePlatform Solutions groupsupports all development requirements for two of S&P's specialist business teams within the wider Commodity Insights group: Fuel, Chemicals and Resource Solutions, and Gas, Power and Climate Solutions. Our work includes all aspects of creation of, and ongoing support for, our business lines data flows, databases, analyst modelling solutions and workflows, new apps, new client-facing products, and many other work areas besides. Our highly experienced team members are based in locations worldwide to support these S&P teams across the globe.\nThe Impact: As part of the CI division, the FCRS and GPCS business lines have a wide customer base and are experiencing continuing strong growth. We are expanding our team to ensure we can continue to offer these business areas maximum support with their wide-ranging development requirements.\nWhat's in it for you: The new team member will work in a fast-paced environment, working with colleagues worldwide on a varied range of interesting and high-impact projects. Our teams use the latest software and development processes, including a number of proprietary AI tools.\nResponsibilities\nDesign, develop, and maintain scalable ETL/ELT pipelines.\nOptimize and automate data ingestion, transformation, and storage processes.\nWork with structured and unstructured data sources, ensuring data quality and consistency.\nDevelop and maintain data models, warehouses, and databases.\nCollaborate with cross-functional teams to support data-driven decision-making.\nEnsure data security, privacy, and compliance with industry standards.\nTroubleshoot and resolve data-related issues in a timely manner.\nMonitor and improve system performance, reliability, and scalability.\nStay up-to-date with emerging data technologies and recommend improvements to our data architecture and engineering practices.\nWhat We're Looking For:\n4 - 6 years Experience working with Unified analytics platforms like Databricks\nProficiency in database management systems (e.g., SQL, NoSQL, Oracle, MySQL, PostgreSQL).\nStrong programming skills in languages such as Python, Java, or Scala.\nExperience with big data technologies (e.g., Hadoop, Big data processing engines, Message broker systems).\nFamiliarity with cloud data platforms (e.g., AWS, Azure, Google Cloud).\nStrong concepts/experience of designing and developing ETL architectures.\nStrong RDBMS concepts and SQL development skills\nStrong knowledge of data modeling and mapping\nExperience with Data Integration from multiple data sources\nAbout Company Statement:\nS&P Global delivers essential intelligence that powers decision making. We provide the world's leading organizations with the right data, connected technologies and expertise they need to move ahead. As part of our team, you'll help solve complex challenges that equip businesses, governments and individuals with the knowledge to adapt to a changing economic landscape.\nS&P Global Commodity Insights enables organizations to create long-term, sustainable value with data and insights for a complete view on the global energy and commodities markets.\nWe're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit .\nWhat's In It For You\nOur Purpose:\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technology-the right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People:\nWe're more than 35,000 strong worldwide-so we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\nOur Values:\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits:\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything you-and your career-need to thrive at S&P Global.\n\nOur benefits include:\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awards-small perks can make a big difference.\nFor more information on benefits by country visit:\nGlobal Hiring and Opportunity at S&P Global:\nAt S&P Global, we are committed to fostering a connected and engaged workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and merit, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n-----------------------------------------------------------\nEqual Opportunity Employer\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\nIf you need an accommodation during the application process due to a disability, please send an email to: and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster describes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision -\n-----------------------------------------------------------\n20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority - Ratings - (Strategic Workforce Planning)","Big data processing engines, Message broker systems, Oracle, Databricks, Sql, ELT, Data Modeling, Java, Hadoop, Google Cloud, Etl, AWS, Data Integration, MySQL, Nosql, Python, Azure, Scala, PostgreSQL"
AWS Data Engineer,KPMG India,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"KPMG in India, a professional services firm, is the Indian member firm affiliated with KPMG International and was established in September 1993. Our professionals leverage the global network of firms, providing detailed knowledge of local laws, regulations, markets, and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, and Vadodara.\nKPMG in India offers services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed, and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\nKPMG Advisory professionals provide advice and assistance to enable companies, intermediaries, and public sector bodies to mitigate risk, improve performance, and create value. KPMG firms provide a wide range of Risk Advisory and Financial Advisory Services that can help clients respond to immediate needs as well as put in place the strategies for the longer term.\nJob Title: Consultant, Senior Consultant\nSkills: AWS Data Engineer\nLocation: Bengaluru & Mumbai\nRole & Responsibility\nEvaluating, developing, maintaining and testing data engineering solutions for Data Lake and advanced analytics projects.\nImplement processes and logic to extract, transform, and distribute data across one or more data stores from a wide variety of sources\nDistil business requirements and translate into technical solutions for data systems including data warehouses, cubes, marts, lakes, ETL integrations, BI tools or other components.\nCreation and support of data pipelines built on AWS technologies including Glue, Redshift, EMR, Kinesis and Athena\nParticipate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.\nOptimize data integration platform to provide optimal performance under increasing data volumes\nSupport the data architecture and data governance function to continually expand their capabilities\nExperience in development of Solution Architecture for Enterprise Data Lakes (applicable for AM/Manager level candidates)\nShould have exposure to client facing roles\nStrong communication, inter-personal and team management skills\nEducation Qualification and Experience\nProficient in any object-oriented/ functional scripting languages: Pyspark, Python etc.\nExperience in using AWS SDKs for creating data pipelines ingestion, processing and orchestration.\nHands on experience in working with big data on AWS environment including cleaning/transforming/cataloguing/mapping etc.\nGood understanding of AWS components, storage (S3) & compute services (EC2)\nHands on experience in AWS managed services (Redshift, Lambda, Athena) and ETL (Glue).\nExperience in migrating data from on-premise sources (e.g. Oracle, API-based, data extracts) into AWS storage (S3)\nExperience in setup of data warehouse using Amazon Redshift, creating Redshift clusters and perform data analysis queries\nExperience in ETL and data modelling on AWS ecosystem components - AWS Glue, Redshift, DynamoDB\nExperience in setting up AWS Glue to prepare data for analysis through automated ETL processes.\nFamiliarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline\nHands on experience with AWS CLI, Linux tools and shell scripts\nCertifications on AWS will be an added plus.\nBE/BTech/MCA\n4+ years of strong experience in 3-4 of the above-mentioned skills.\nEqual employment opportunity information\nKPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their colour, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability, or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavour for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","AWS DMS, ETL Glue, AWS SDKs, Pyspark, data engineering, Linux Tools, Dynamodb, AWS Glue, Big Data, AWS Data Pipeline, Shell Scripts, amazon emr, Aws Cli, Python, AWS"
AWS Data Engineer - Consultant,KPMG India,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nAbout KPMG in India\n\nKPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Jaipur, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.\n\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\n\nAWS Data Engineering\n\n\n\nEqual employment opportunity information\n\nKPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.\n\nQualifications\n\nB.E","data engineering, AWS"
Celonis Data Engineer - Analyst,KPMG India,1-3 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nAbout KPMG in India\n\nKPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.\n\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\n\nThe person will work on a variety of projects in a highly collaborative, fast-paced environment. The person will be responsible for software development activities of KPMG, India. Part of the development team, he/she will work on the full life cycle of the process, develop code and unit testing. He/she will work closely with Technical Architect, Business Analyst, user interaction designers, and other software engineers to develop new product offerings and improve existing ones. Additionally, the person will ensure that all development practices are in compliance with KPMG's best practices policies and procedures. This role requires quick ramp up on new technologies whenever required.\n\nResponsibilities\n\nRole Celonis Data E ngineer\n\nLocation Bangalore\n\nExperience 1 to 2 Years\n\nKey Responsibilities :-\n\nAt least 1+ years of experience in databases, data integration, and data modeling.\nMinimum 1 years of experience in Celonis process mining.\nStrong skills in SQL and PQL.\nExperience with cloud-based data technologies.\nAbility to lead teams and drive project completion as per business requirements.\nExperience in managing the Celonis platform and optimizing APC consumption.\n\nQualifications\n\n\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n\nEqual Opportunity Employer\n\nKPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","Celonis process mining, cloud-based data technologies, PQL, Databases, Data Modeling, Data Integration, Sql"
AWS Data Engineer - Consultant,KPMG India,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nAbout KPMG in India\n\nKPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Jaipur, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.\n\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\n\nAWS Data Engineering\n\n\n\nEqual employment opportunity information\n\nKPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.\n\nQualifications\n\nB.E","data engineering, AWS"
Data Engineer,SPN Globe,5-10 Years,,"Nagpur, Pune",Login to check your skill match score,"SPN Globe is a specialized IT recruitment and staffing firm. We are the sourcing partner of various software companies on Permanent and Contract to Hire Demands. Also working with few top notch (CMMI-5 level) clients, PAN India clients and major city clients such as Hyderabad, Bengaluru, Pune, Mumbai, Nagpur and NCR.\nPlease find the position details below:\nCompany: IT Company\nRole: Data Engineer\nType: Permanent (No third-party payroll)\nLocation: Pune, Nagpur\nJoining: 0 to 30 days / Immediate Joiners\nExperience: 5 to 10 years\nMandatory Skills: Data Engineering, Python, SQL, Snowflake, Gitlab\nApply immediately to grab it. Email your resume at [HIDDEN TEXT]\nAlso, immediately refer this opportunity to your friends.","snowflake, Gitlab, Data Engineer, Python, Sql"
Data Engineer - Bangalore,Dayworks Private Limited,9-13 Years,INR 35 - 60 LPA,Bengaluru,Login to check your skill match score,"This role is for one of Weekday's clients\nSalary range: Rs 3500000 - Rs 6000000 (ie INR 35-60 LPA)\nMin Experience: 9 years\nLocation: Bengaluru\nJobType: full-time\nAbout the Role:\nWe are seeking a highly experienced and innovative Senior Data Engineer to join our growing data team. As a Data Engineer, you will be responsible for designing, developing, and maintaining robust data pipelines and scalable architectures that support our business intelligence, analytics, and data-driven decision-making initiatives. You will work closely with data scientists, analysts, product managers, and other stakeholders to ensure that our data infrastructure is efficient, reliable, and aligned with business goals.\nThis is a strategic role ideal for someone who thrives in a fast-paced environment, has deep experience in data engineering best practices, and is passionate about leveraging data to drive impact at scale.\nKey Responsibilities:\nData Architecture & Pipeline Development:\nDesign and build highly scalable, efficient, and secure data pipelines for batch and real-time data processing.\nDevelop and maintain ETL/ELT processes to extract, transform, and load data from multiple sources into data warehouses and data lakes.\nData Modeling & Warehousing:\nCreate and maintain optimized data models that support advanced analytics and reporting.\nDesign and implement data warehousing solutions using modern data storage technologies.\nData Quality & Governance:\nEnsure high levels of data availability, quality, and integrity through the implementation of robust data validation, monitoring, and governance practices.\nPartner with compliance and data governance teams to enforce data security and privacy policies.\nCollaboration & Cross-functional Partnership:\nWork closely with data scientists, analysts, and business teams to understand data requirements and provide reliable data solutions.\nCollaborate with DevOps and infrastructure teams to automate deployment and manage cloud-based data environments.\nTooling & Performance Optimization:\nImplement monitoring tools and optimize performance of data pipelines and database systems.\nStay updated with the latest trends in data engineering, and evaluate new tools and technologies for adoption.\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Information Systems, Engineering, or a related field.\n9+ years of hands-on experience in Data Engineering, with a deep understanding of scalable data pipeline architecture.\nProficient in at least one programming language such as Python, Java, or Scala.\nStrong experience with ETL/ELT frameworks, data orchestration tools (e.g., Apache Airflow, DBT), and workflow management.\nSolid experience working with cloud data platforms (e.g., AWS, Azure, GCP) and data storage solutions (e.g., Snowflake, Redshift, BigQuery).\nExpertise in SQL and data modeling for both OLAP and OLTP systems.\nFamiliarity with distributed systems, streaming technologies (Kafka, Spark), and containerization (Docker, Kubernetes) is a plus.\nPreferred Skills:\nExperience in a fast-paced startup or enterprise data team.\nExposure to big data technologies and real-time data processing.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and project management abilities.",data engineering
Data Engineer - Python/SQL,Xpetize Technology Solutions Private Limited,3-5 Years,,Itanagar,"Information Technology, Information Services","Skills : Data engineer\nLocation : Remote\nExperience : 4+ years\nNotice : Immediate only\nKey Skills\nData Engineering Expertise : Bring 3+ years of experience in building data pipelines and managing a secure, modern data stack. This includes CDC streaming ingestion using tools like Debezium into a Hudi data lake that supports AI/ML workloads and a curated Redshift data warehouse.\nAWS Cloud Proficiency : At least 3 years of experience working with AWS cloud infrastructure, including Kafka (MSK), Spark / AWS Glue, and infrastructure as code (IaC) using Terraform.\nStrong Coding Skills : Write and review high-quality, maintainable code that enhances the reliability and scalability of our data platform. We use Python, SQL, and dbt extensively, and you should be comfortable leveraging third-party frameworks to accelerate development.\nData Lake Development : Prior experience building data lakes on S3 using Apache Hudi with Parquet, Avro, JSON, and CSV file formats.\nWorkflow Automation : Build and manage multi-stage workflows using serverless Lambdas and AWS Step Functions to automate and orchestrate data processing pipelines.\nData Governance Knowledge : Familiarity with data governance practices, including data quality, lineage, and privacy, as well as experience using cataloging tools to enhance discoverability and compliance.\nCI/CD Best Practices : Experience developing and deploying data pipeline solutions using CI/CD best practices to ensure reliability and scalability.\nData Integration Tools : Working knowledge of tools such as Stitch and Segment CDP for integrating diverse data sources into a cohesive ecosystem.\nAnalytical and ML Tools Expertise : Knowledge and practical experience with Athena, Redshift, or Sagemaker Feature Store to support analytical and machine learning workflows is a definite bonus!","cdc, S3, Machine Learning, Artificial Intelligence, Aws Cloud, Csv, AWS Glue, Kafka, Json, Avro, Redshift, Sql, Data Pipeline, Terraform, Spark, Python, Coding Skills"
"Data Engineer, AVP",NatWest Markets,16-18 Years,,Bengaluru,Banking,"Join us as a Data Engineer\nWe're looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you'll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWe're offering this role at associate vice president level\nWhat you'll do Your daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll needTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience inDevOps practice andcloud technologies specifically AWS\nGood knowledge of snowflake and capable of creating practical and scalable data solutions using the snowflake platform to meet business needs\nExperience in business intelligence and data engineering, including data warehousing, delivery, and operations.\nHours\n45\nJob Posting Closing Date:\n23/05/2025","Analytical Skills, Forecasting, Data Analysis, Banking, Business Strategy, Business Management, Cost Control"
Data Engineer,R Systems International,7-10 Years,,"Noida, Mumbai, Pune",Information Technology,"Candidate should have strong technical capabilities, particularly in the following areas:\nSQL: Expertise in writing stored procedures, complex queries, and optimizing performance.\nPower BI development :Experience of developing complex power bi reports\nDax :Complex Dax queries to meet business needs\nPaginated Reports (Power BI): Experience in designing and building paginated reports using Power BI Report Builder.\nHealthcare domain experience - Added advantahe\nExcellent Communication skill\nRole\nCandidate should have strong technical capabilities, particularly in the following areas:\nSQL: Expertise in writing stored procedures, complex queries, and optimizing performance.\nPower BI development :Experience of developing complex power bi reports\nDax :Complex Dax queries to meet business needs\nPaginated Reports (Power BI): Experience in designing and building paginated reports using Power BI Report Builder.\nHealthcare domain experience - Added advantahe\nExcellent Communication skill","Power Bi, Dax, Sql, Ssrs, Tsql"
Data Engineer,R Systems International,7-12 Years,,Delhi NCR,Information Technology,"Roles and Responsibilities\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Azure Databricks.\nDevelop complex ETL processes using Python scripts and SQL queries to process large datasets stored in Azure Data Lake Storage.\nCollaborate with cross-functional teams to gather requirements for new data warehousing solutions and implement scalable architectures on Azure Data Warehouse.\nTroubleshoot issues related to ADF pipeline failures, Azure Synapse connections, and SQL queries.","Adf, Power Bi, Azure, Sql, Etl"
Senior Data Engineer - Immediate joiners,R Systems International,7-12 Years,,"Noida, Pune, Chennai",Information Technology,"Hands-on experience ofAzure data services, including Azure Data Factory, Azure Data Lake, Databricks and Spark.\nExperience in ETL Development.\nExperience of developing and servicing data via API endpoints, and Azure API Management.\nPython 3.x, with 6+ years programming experience that includes:\nTool usage, such as Jupyter Notebooks.\nVirtual environments.\nAutomated testing through feature files.\nCI/CD implementations.\nExperience in git and branching strategies.\nGood Understanding of ADF/ADB github integration.\nExperience of batch scheduling.\nExperience in bash scripting.\nExperience using SOAP UI / Postman / Fiddler for API testing.\nExperience of incident and change management processes.","Azure, Adf, Api, Python, Adb, Sql"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Noida,Information Technology,"Key Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)\nFamiliarity with DataHub, DataMesh, and security best practices is a plus\nStrong problem-solving and debugging mindset\nAbility to work under pressure in a fast-paced environment.\nExcellent communication and collaboration skills.\nOwnership, customer orientation, and a bias for actionKey Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Bengaluru,Information Technology,"Key Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)\nFamiliarity with DataHub, DataMesh, and security best practices is a plus\nStrong problem-solving and debugging mindset\nAbility to work under pressure in a fast-paced environment.\nExcellent communication and collaboration skills.\nOwnership, customer orientation, and a bias for actionKey Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer,Mondelez,2-9 Years,,Mumbai,Food and Beverage,"How you will contribute\nYou will:\nExecute the business analytics agenda in conjunction with analytics team leaders\nWork with best-in-class external partners who leverage analytics tools and processes\nUse models/algorithms to uncover signals/patterns and trends to drive long-term business performance\nExecute the business analytics agenda using a methodical approach that conveys to stakeholders what business analytics will deliver\nWhat you will bring\nA desire to drive your future and accelerate your career and the following experience and knowledge:\nUsing data analysis to make recommendations to analytic leaders\nUnderstanding in best-in-class analytics practices\nKnowledge of Indicators (KPIs) and scorecards\nKnowledge of BI tools like Tableau, Excel, Alteryx, R, Python, etc. is a plus\nAre You Ready to Make It Happen at Mondel z International\nJoin our Mission to Lead the Future of Snacking. Make It with Pride\nIn This Role\nAsa DaaSData Engineer, you will have the opportunity to design and build scalable, secure, and cost-effective cloud-based data solutions. You will develop andmaintaindata pipelines to extract, transform, and load data into data warehouses or data lakes, ensuring data quality and validation processes tomaintaindata accuracy and integrity. You will ensure efficient data storage and retrieval foroptimalperformance, and collaborate closely with data teams, product owners, and other stakeholders to stay updated with the latest cloud technologies and best practices.\nRole & Responsibilities:\nDesign and Build:Develop and implement scalable, secure, and cost-effective cloud-based data solutions.\nManage Data Pipelines:Develop andmaintaindata pipelines to extract, transform, and load data into data warehouses or data lakes.\nEnsure Data Quality:Implement data quality and validation processes to ensure data accuracy and integrity.\nOptimize Data Storage:Ensure efficient data storage and retrieval foroptimalperformance.\nCollaborate and Innovate:Work closely with data teams, product owners, and stay updated with the latest cloud technologies and best practices.\nTechnical Requirements:\nProgramming:Python\nDatabase:SQL, PL/SQL, Postgres SQL,Bigquery, Stored Procedure / Routines.\nETL & Integration:AecorSoft, Talend,DBT, Databricks(Optional),Fivetran.\nData Warehousing:SCD, Schema Types, Data Mart.\nVisualization:PowerBI(Optional), Tableau (Optional), Looker.\nGCP Cloud Services:Big Query, GCS.\nSupply Chain:IMS + Shipment functional knowledge good to have.\nSupporting Technologies:Erwin, Collibra,Data Governance,Airflow.\nSoft Skills:\nProblem-Solving:The ability toidentifyand solve complex data-related challenges.\nCommunication:Effective communication skills to collaborate with Product Owners, analysts, and stakeholders.\nAnalytical Thinking:Thecapacityto analyze data and draw meaningful insights.\nAttention to Detail:Meticulousness in data preparation and pipeline development.\nAdaptability:The ability to stay updated with emerging technologies and trends in the dataengineering field.","business analytics, DaaS Data Engineer, cloud-based data solutions, Manage Data Pipelines, Bi Tools, Sql"
"Data Engineer, AVP",NatWest Markets,16-18 Years,,Gurugram,Banking,"Join us as a Data Engineer\nWe're looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you'll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWe're offering this role at associate vice president level\nWhat you'll do Your daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll needTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience inDevOps practice andcloud technologies specifically AWS\nGood knowledge of snowflake and capable of creating practical and scalable data solutions using the snowflake platform to meet business needs\nExperience in business intelligence and data engineering, including data warehousing, delivery, and operations.\nHours\n45\nJob Posting Closing Date:\n23/05/2025","Data Analysis, Analytical Skills, Forecasting, Banking, Business Strategy, Business Management, Cost Control"
Consultant - Data Engineer,Aliqan Services,5-8 Years,,Bengaluru,Information Technology,"As a Consultant - Data Engineer, you will be responsible for:\nWorking with large datasets, including terabyte-scale and growing data\nUtilizing various technologies and tools associated with databases and big data\nDesigning and modeling data to meet business requirements\nDeveloping complex ETL processes from concept to implementation\nOptimizing system performance and tuning for scalability\nCollaborating with business and technical teams to effectively communicate and solve problems\nCandidate Qualifications:\nTo be considered for this position, you should have:\n5+ years of data engineering or general software development experience\nExperience with DWH, Snowflake, SQL, Python, and any cloud platform\nFamiliarity with AWS Data Ecosystem, including AWS S3 and AWS Lambda\nProficiency in writing complex SQL queries across large datasets\nStrong software engineering principles and functional programming skills in Python or equivalent\nExcellent problem-solving abilities and the ability to work in a fast-paced environment\nRequired Skills:\nBigdata\nSnowflake\nETL (any tool)\nDWH\nSpark\nPython\nSQL","snowflake, snowflake, Dwh, Spark, Bigdata, Python, Etl, Dwh, Spark, Bigdata, Python, Etl"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Gurugram,Information Technology,"Key Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)\nFamiliarity with DataHub, DataMesh, and security best practices is a plus\nStrong problem-solving and debugging mindset\nAbility to work under pressure in a fast-paced environment.\nExcellent communication and collaboration skills.\nOwnership, customer orientation, and a bias for actionKey Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer(DevOps),Kezan Consulting,4-9 Years,,Pune,Information Technology,"Key Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)\nFamiliarity with DataHub, DataMesh, and security best practices is a plus\nStrong problem-solving and debugging mindset\nAbility to work under pressure in a fast-paced environment.\nExcellent communication and collaboration skills.\nOwnership, customer orientation, and a bias for actionKey Responsibilities\nEnsure platform uptime and application health as per SLOs/KPIs\nMonitor infrastructure and applications using ELK, Prometheus, Zabbix, etc.\nDebug and resolve complex production issues, performing root cause analysis\nAutomate routine tasks and implement self-healing systems\nDesign and maintain dashboards, alerts, and operational playbooks\nParticipate in incident management, problem resolution, and RCA documentation\nOwn and update SOPs for repeatable processes\nCollaborate with L3 and Product teams for deeper issue resolution\nSupport and guide L1 operations team\nConduct periodic system maintenance and performance tuning\nRespond to user data requests and ensure timely resolution\nAddress and mitigate security vulnerabilities and compliance issues Technical Skillset\nHands-on with Spark, Hive, Cloudera Hadoop, Kafka, Ranger\nStrong Linux fundamentals and scripting (Python, Shell)\nExperience with Apache NiFi, Airflow, Yarn, and Zookeeper\nProficient in monitoring and observability tools: ELK Stack, Prometheus, Loki\nWorking knowledge of Kubernetes, Docker, Jenkins CI/CD pipelines\nStrong SQL skills (Oracle/Exadata preferred)","Airflow, Hive, Hadoop, Pyspark, Shell Scripting, Python"
Data Engineer - Supply Chain,Mondelez,2-5 Years,,Mumbai,Food and Beverage,"Are You Ready to Make It Happen at Mondel z International\nJoin our Mission to Lead the Future of Snacking. Make It With Pride.\nTogether with analytics team leaders you will support our business with excellent data models to uncover trends that can drive long-term business results.\nHow you will contribute\nYou will:\nExecute the business analytics agenda in conjunction with analytics team leaders\nWork with best-in-class external partners who leverage analytics tools and processes\nUse models/algorithms to uncover signals/patterns and trends to drive long-term business performance\nExecute the business analytics agenda using a methodical approach that conveys to stakeholders what business analytics will deliver\nWhat you will bring\nA desire to drive your future and accelerate your career and the following experience and knowledge:\nUsing data analysis to make recommendations to analytic leaders\nUnderstanding in best-in-class analytics practices\nKnowledge of Indicators (KPIs) and scorecards\nKnowledge of BI tools like Tableau, Excel, Alteryx, R, Python, etc. is a plus\nAre You Ready to Make It Happen at Mondel z International\nJoin our Mission to Lead the Future of Snacking. Make It with Pride\nIn This Role\nAs a DaaS Data Engineer, you will have the opportunity to design and build scalable, secure, and cost-effective cloud-based data solutions. You will develop and maintain data pipelines to extract, transform, and load data into data warehouses or data lakes, ensuring data quality and validation processes to maintain data accuracy and integrity. You will ensure efficient data storage and retrieval for optimal performance, and collaborate closely with data teams, product owners, and other stakeholders to stay updated with the latest cloud technologies and best practices.\nRole & Responsibilities:\nDesign and Build:Develop and implement scalable, secure, and cost-effective cloud-based data solutions.\nManage Data Pipelines:Develop and maintain data pipelines to extract, transform, and load data into data warehouses or data lakes.\nEnsure Data Quality:Implement data quality and validation processes to ensure data accuracy and integrity.\nOptimize Data Storage:Ensure efficient data storage and retrieval for optimal performance.\nCollaborate and Innovate:Work closely with data teams, product owners, and stay updated with the latest cloud technologies and best practices to remain current in the field.\nTechnical Requirements:\nProgramming:Python, PySpark, Go/Java\nDatabase:SQL, PL/SQL\nETL & Integration:DBT, Databricks + DLT, AecorSoft, Talend,Informatica/Pentaho/Ab-Initio,Fivetran.\nData Warehousing:SCD, Schema Types, Data Mart.\nVisualization:Databricks Notebook, PowerBI, Tableau, Looker.\nGCP Cloud Services:Big Query, GCS, Cloud Function, PubSub, Dataflow, DataProc, Dataplex.\nAWS Cloud Services:S3, Redshift, Lambda, Glue, CloudWatch, EMR, SNS, Kinesis.\nSupporting Technologies:Graph Database/Neo4j, Erwin, Collibra, Ataccama DQ, Kafka, Airflow.\nExperience with RGM.ai product would have an added advantage.\nSoft Skills:\nProblem-Solving:The ability to identify and solve complex data-related challenges.\nCommunication:Effective communication skills to collaborate with Product Owners, analysts, and stakeholders.\nAnalytical Thinking:The capacity to analyse data and draw meaningful insights.\nAttention to Detail:Meticulousness in data preparation and pipeline development.\nAdaptability:The ability to stay updated with emerging technologies and trends in the data engineering field.","SCD, Schema Types, dbt, Databricks + DLT, DaaS Data Engineer, Data Mart, Python, Sql"
Senior Data Engineer,Tech SMC Square,4-9 Years,,Bengaluru,Outsourcing,"4-5 years of experience in Data Engineering projects (Med to Small Data Lake / Data Warehousing preferred).\n3+ years of experience in Python (Must Have) / Kafka programming (Preferred) / PySpark (Nice to Have) /\n2+ years of working experience with AWS Services like S3 (Must Have), Lambda (Must Have), Redshift (Preferred), Glue (Nice to Have), Data Brew (Nice to Have)\nMust Have:\n3+ years of experience in performing data analysis, data ingestion and data integration. o 3+ years of experience in ETL (Extraction, Transformation & Loading) or ELT and architecting data systems.\n3+ years of experience with schema design, data modelling and SQL queries. o Strong Database experience (Oracle/PostgreSQL/SQL Server/Redshift). o Design and Develop scalable Data warehouse solutions, ETL/ELT pipelines in AWS cloud environment.\nExperience in data workflow management.\nKnowledge on CI/CD (Must Have) and Terraform technologies (Nice to Have).\nPreferred / Nice To Have:\nKnowledge of advanced statistics and experience with statistical data analysis systems (scikit-learn, Pandas, R) is added advantage.\nEducation background:\nBE / B Tech / MS in Computer Science, Information Technology, or equivalent degree in related discipline","SQL/Data Modeling, ETL/ELT, CI/CD, AWS (S3/Lambda), Data Warehousing, Python"
Lead Data Engineer,Tech SMC Square,10-20 Years,,Bengaluru,Outsourcing,"8+ years of experience in data engineering, machine learning, AI, data warehousing, and data management Projects Med to Small.\n3+ years in a leadership role, responsible for the direct oversight of a team of data engineers (offshore) where you oversaw day-to-day activities, attained delivery metrics, and provided technical coaching to staff (Must Have)\nData processing programming using SQL, Python, and similar tools (Must Have).\nSolid experience with cloud-based data tools and platforms - AWS (Must Have), Azure or, Google (Nice To Have)\nDeep hands-on experience using databases and related technologies in a fast-paced business environment with large-scale and complex datasets including design and optimize queries, create data structures, and develop data models (Must Have)\nBe comfortable with building effective cross-team collaborative working relationships with people at all levels of the organization. (Must Have)\nExperience in enterprise level data platforms involving implementation of end-to-end data pipelines. (Must Have)\nExperience with column-oriented database technologies, Redshift (Must Have) Big Query or Vertica (Nice to Have), and traditional database systems SQL Server and Oracle (Must Have), MySQL (Nice to Have).\nExperience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Matillion (Must Havebut can be learned), Lambda (Preferred), Spark (Preferred), AWS Glue ETL (Nice to Have), Spark Streaming (Nice to Have), AWS Glue DataBrew (Nice to Have), Kinesis Data Firehose (Nice to Have)\nLogical programming in Python (Must Have), Spark (Preferred), PySpark (Nice to Have), Java(Nice to Have), Javascript(Nice to Have), and/or Scala (Nice to Have).\nCloud-native data platform design with a focus on streaming and event-driven architectures (Preferred)","Oracle), SQL & Python, Data Warehousing (Redshift, Data Pipeline Architecture, data engineering, SQL Server, Aws Cloud"
Snowflake Data Engineer,Sysintelli Inc,5-10 Years,INR 20 - 30 LPA,Bengaluru,"Consulting, Staffing Agency","Position: SnapLogic Developer\nExperience: 5+ Years\nLocation: Remote\nBudget: 30-35 LPA\nAvailability to Join: On or before 30th January\nKey Responsibilities:\nDesign, develop, and manage robust data integration workflows using SnapLogic.\nWork extensively with Snowflake to manage and optimize data pipelines and warehouse operations.\nWrite efficient SQL queries to process, transform, and analyze large datasets.\nLeverage Python for scripting, data processing, and automation tasks as required.\nCollaborate with cross-functional teams to gather requirements and deliver solutions.\nTroubleshoot and optimize existing workflows for better performance and scalability.\nEnsure data security, integrity, and compliance with best practices.\nRequired Skills and Qualifications:\nSnapLogic Expertise: Hands-on experience in developing and managing SnapLogic pipelines.\nSnowflake Knowledge: Strong proficiency in working with Snowflake databases.\nSQL Proficiency: Advanced SQL skills for complex queries and transformations.\nPython Skills: Experience in scripting and automation using Python.\n5+ years of experience in data integration, ETL, and related fields.\nStrong problem-solving skills and the ability to work independently in a remote environment.\nPreferred Skills:\nKnowledge of data modeling and database optimization techniques.\nExperience with cloud platforms like AWS, Azure, or Google Cloud.\nFamiliarity with DevOps practices and CI/CD pipelines.","snowflake, Snaplogic, Python, Sql"
Data Engineer ( Hiring For one of our client),Vibrantminds Technologies,Fresher,INR 4.5 - 5 LPA,Pune,"Database, Data Mining, Data Visualization, Big Data, Data Integration","Job description\nPosition:Data Engineer\nExperience:Fresher\nJob Location:Pune\nInternship Duration:06 Months\nApprox. Stipend During Internship:Upto Rs. 11,000/- Per Month.\nApprox. Package Post Internship:Upto Rs. 4,50,000/- Per Annum\n(Based on performance during the internship, with a potential CTC of 4.5 LPA)\nJob Location:Pune\nEducational Criteria:\nBE / BTech (All Branches)\nPass out Year 2023&2024 Batch Only\n70% in Graduation\nJob Description:\nWe are looking for data engineers who have the right attitude, aptitude, skills, empathy, compassion, and hunger for learning. Build products in the data analytics space. A passion for shipping high-quality data products, interest in the data products space; curiosity about the bigger picture of building a company, product development and its people. Bachelors degree in Computer Science, Engineering, Statistics, or a related field.\nRoles and Responsibilities\nDevelop and maintain ETL pipelines using Pyspark.\nUnderstand Pyspark concepts, performance optimization techniques and governance tools\nUnderstanding data from various systems to the Enterprise Data\nWarehouse/Data Lake/Data Mesh.\nCollaborate cross-functionally to design effective data solutions\nMonitor, troubleshoot, and optimize pipeline performance and data quality\nMaintain high coding standards and produce thorough documentation.\nSkills Required:\nExperience with big data technologies (Python, PySpark, etc.).\nExposure to building robust and resilient data pipelines which are scalable,\nFault tolerant and reliable in terms of data movement.\nStrong understanding in Python, Pyspark.\nKnowledge of any one cloud platform (AWS, GCP, Azure).\nStrong understanding of SQL and NoSQL technologies.\nBasic knowledge of data warehousing and ETL/ELT processes.\nExcellent written and verbal communication skills.Role & responsibilities\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:B.Tech/B.E. in Electronics/Telecommunication, Information Technology, Mechanical, Computers, Electrical\nKey Skills\nSkills highlighted with are preferred keyskills","Scripting, Hadoop, Linux, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Data Engineer,Calsoft,8-13 Years,,"Bengaluru, Kolkata, Pune",Software,"Role & responsibilities\n8+ years experienceson relevant field below (Internship, prototype, and personal projects won't be counted)\nCoding is required. (Ideally Python or Java)\nOwn end to end lifecycle (From development to deployment to production environment)\nExperience in building or deploying solution in the cloud.\nEitherCloud Native (Serverless): S3, Lambda, AWS Batch, ECS\nOrCloud Agnostic:Kubernetes, Helm Chart, ArgoCD, Prometeus, Grafana.\nCICD experience:Github action or Jenkin.\nInfrastructure as code: e.g., Terraform\nAnd experience inat least one of this focus area:\nBig Data:Building Big data pipeline or Platform to process petabytes of data: (PySpark, Hudi, Data Lineage, AWS Glue, AWS EMR, Kafka, Schema Registry)\nOrGraphDB: Ingesting and consuming data in Graph Database such as Neo4J, AWS Neptune, JanusGraph or DGraph\nPreferred candidate profile\nSpecifically highlight Kafka expertise - include details like:\nExperience with Kafka cluster management and configuration\nStream processing with Kafka Streams or KSQL\nSchema Registry implementation and management\nKafka Connect for data integration\nPut significant focus on PySpark skills:\nExperience building and optimizing PySpark jobs for batch processing\nStream processing with Spark Structured Streaming\nFamiliarity with Delta Lake, Hudi, or Iceberg for lakehouse implementation\nHighlight data engineering skills that complement these technologies:\nData pipeline design and implementation\nExperience with data quality, validation, and lineage tracking\nPerformance optimization for large-scale data processing","Sql, Python, Data Warehousing, Etl, AWS"
Data Engineer,Ford,5-7 Years,,Chennai,Automotive,"Were seeking a highly skilled and experienced Full Stack Data Engineer to play a pivotal role in the development and maintenance of our Enterprise Data Platform.\nIn this role, you'll be responsible for designing, building, and optimizing scalable data pipelines within our Google Cloud Platform (GCP) environment.\nYou'll work with GCP Native technologies like BigQuery, Dataflow, and Pub/Sub, ensuring data governance, security, and optimal performance.\nThis is a fantastic opportunity to leverage your full-stack expertise, collaborate with talented teams, and establish best practices for data engineering at Ford.\nBachelors degree in Computer Science, Information Technology, Information Systems, Data Analytics, or a related field (or equivalent combination of education and experience).\n5-7 years of experience in Data Engineering or Software Engineering, with at least 2 years of hands-on experience building and deploying cloud-based data platforms (GCP preferred).\nStrong proficiency in SQL, Java, and Python, with practical experience in designing and deploying cloud-based data pipelines using GCP services like BigQuery, Dataflow, and DataProc.\nSolid understanding of Service-Oriented Architecture (SOA) and microservices, and their application within a cloud data platform.\nExperience with relational databases (e.g., PostgreSQL, MySQL), NoSQL databases, and columnar databases (e.g., BigQuery).\nKnowledge of data governance frameworks, data encryption, and data masking techniques in cloud environments.\nFamiliarity with CI/CD pipelines, Infrastructure as Code (IaC) tools like Terraform and Tekton, and other automation frameworks.\nExcellent analytical and problem-solving skills, with the ability to troubleshoot complex data platform and microservices issues.\nExperience in monitoring and optimizing cost and compute resources for processes in GCP technologies (e.g., BigQuery, Dataflow, Cloud Run, DataProc).\nA passion for data, innovation, and continuous learning.\nData Pipeline Architect Builder:Spearhead the design, development, and maintenance of scalable data ingestion and curation pipelines from diverse sources. Ensure data is standardized, high-quality, and optimized for analytical use. Leverage cutting-edge tools and technologies, including Python, SQL, and DBT/Dataform, to build robust and efficient data pipelines.\nEnd-to-End Integration Expert:Utilize your full-stack skills to contribute to seamless end-to-end development, ensuring smooth and reliable data flow from source to insight.\nGCP Data Solutions Leader: Leverage your deep expertise in GCP services (BigQuery, Dataflow, Pub/Sub, Cloud Functions, etc.) to build and manage data platforms that not only meet but exceed business needs and expectations.\nData Governance Security Champion: Implement and manage robust data governance policies, access controls, and security best practices, fully utilizing GCPs native security features to protect sensitive data.\nData Workflow Orchestrator: Employ Astronomer and Terraform for efficient data workflow management and cloud infrastructure provisioning, championing best practices in Infrastructure as Code (IaC).\nPerformance Optimization Driver: Continuously monitor and improve the performance, scalability, and efficiency of data pipelines and storage solutions, ensuring optimal resource utilization and cost-effectiveness.\nCollaborative Innovator: Collaborate effectively with data architects, application architects, service owners, and cross-functional teams to define and promote best practices, design patterns, and frameworks for cloud data engineering.\nAutomation Reliability Advocate: Proactively automate data platform processes to enhance reliability, improve data quality, minimize manual intervention, and drive operational efficiency.\nEffective Communicator: Clearly and transparently communicate complex technical decisions to both technical and non-technical stakeholders, fostering understanding and alignment.\nContinuous Learner: Stay ahead of the curve by continuously learning about industry trends and emerging technologies, proactively identifying opportunities to improve our data platform and enhance our capabilities.\nBusiness Impact Translator: Translate complex business requirements into optimized data asset designs and efficient code, ensuring that our data solutions directly contribute to business goals.\nDocumentation Knowledge Sharer: Develop comprehensive documentation for data engineering processes, promoting knowledge sharing, facilitating collaboration, and ensuring long-term system maintainability.","Pub/Sub, BigQuery, data engineering, Terraform, DataFlow, Python, Sql"
Data Engineer,E Solutions,5-10 Years,,Ghaziabad,"Information Technology, Information Services","Proficiency in Python, Java, or SQL for data processing and automation.\nHands-on experience with cloud platforms (especially Azure, Snowflake, and Databricks).\nExpertise in big data technologies (such as Apache Spark, Hadoop, or Kafka).\nDeep understanding of data modeling, performance tuning, and optimization.\nFamiliarity with transformer-based architecture, Generative AI, and LLM data workflows\nLeadership & Collaboration:\nProven ability to lead complex data engineering projects.\nStrong mentorship skills and experience guiding junior engineers.\nExcellent communication and teamwork skills to collaborate with cross-functional teams.","Data Processing, Transformers, Llm, Java, Big Data Technologies, Azure, Python, Sql"
Data Engineer,Alight,8-13 Years,,Gurugram,Information Technology,"Our story\nAt Alight, we believe a company s success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to Be Alight.\nOur Values:\nChampion People- be empathetic and help create a place where everyone belongs.\nGrow with purpose -Be inspired by our higher calling of improving lives.\nBe Alight -act with integrity, be real and empower others.\nIt s why we re so driven to connect passion with purpose. Alight helps clients gain a benefits advantage while building a healthy and financially secure workforce by unifying the benefits ecosystem across health, wealth, wellbeing, absence management and navigation.\nWith a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.\nJoin our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.\nLearn more atcareers.alight.com.\nAlight is seeking a skilled and passionate Data Engineer to join our team. As the Data Engineer, you will be a member of a team responsible for various stages of data pipelines development, including understanding business requirements, coding, testing, documentation, deployment, and production support. As part of the Data Analytics team, you will focus on delivering high-quality enterprise caliber systems on Alight s Data Lake. Your primary role will involve participating in full life-cycle data ingestion and data architecting development projects.\nQualifications:\nKnowledge & Experience:\n5+ years of data integration, data warehousing or data conversion experience.\n3+ years of data modeling experience.\n3+ years of Informatica IICS experience.\n3+ years working with AWS Redshift.\n2+ years AWS step functions, Lambda, Glue, and other Cloud Data Lake Services\n1+ SSIS / SSRS experience\nExcellent analytical and critical thinking skills\nStrong interpersonal skills with the ability to work effectively with diverse and remote teams\nExperience in agile processes and development task estimation\nStrong sense of responsibility for deliverables\nAbility to work in a small team with moderate supervision\nResponsibility Areas:\nDesign data ingestion solutions for small to medium complexity requirements independently, adhering to existing standards\nDevelop high-priority and highly complex solutions for systems based on functional specifications, detailed design, maintainability, and coding and efficiency standards, working independently.\nEstimate and evaluate risks, and prioritize technical tasks based on requirements\nCollaborate actively with Data Engineering Lead, Product Owners, Quality Assurance, and stakeholders to ensure high-quality project delivery\nConduct formal code reviews to ensure compliance with standards\nUtilize appropriately system design, development, and process standards\nCreate, maintain, and publish system-level documentation, including system diagrams, with minimal guidance\nEnsure clarity, conciseness, and completeness of requirements before starting development, collaborating with Business Analysts and stakeholders to evaluate feasibility. Take primary accountability for meeting non-functional requirements.\nAlight requires all virtual interviews to be conducted on video.\nFlexible Working\nSo that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and Top 100 Company for Remote Jobs 5 years in a row.\nBenefits\nWe offer programs and plans for a healthy mind, body, wallet and life because it s important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as several voluntary benefit options.\nBy applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight s employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.\nOur commitment to Inclusion\nWe celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.\nAt Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.\nAuthorization to work in the Employing Country\nApplicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.\nNote, this job description does not restrict managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\nWe offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.\nDISCLAIMER:\nNothing in this job description restricts managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\n.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Manager Quality Assurance, Analytical, Data Conversion, Production Support, Coding, Data Modeling, Ssrs, Informatica, Project Delivery, SSIS"
Data Engineer,Kanini Software Solutions,5-10 Years,,"Bengaluru, Chennai, Pune",Software,"Job DescriptionWant to join KANINI\nWe are looking for aData Engineer who can build a robust database and its architecture. In thisrole, you will assess a wide range of requirements and apply relevant databasetechniques to create a sustainable data architecture before you begin theimplementation process and develop the database from scratch.\nYou areall set to:\nDevelop, maintain,evaluate, and test big data solutions. You will be involved in data engineeringactivities like creating pipelines/workflows for Source-to-Target Data Mapping amongothers.\nYou are someone who can:\nYou will be involved in the design of datasolutions using Hadoop based technologies along with Hadoop, Azure, HDInsightfor Cloudera based Data Late using Scala Programming.\nLiaise and be part of our extensive GCP community,contributing in the knowledge exchange learning programme of the platform.\nBe required to showcase your GCP Data engineeringexperience when communicating with business team on their requirements, turningthese into technical data solutions.\nBe required to build and deliver Data solutionsusing GCP products and offerings.\nHands on and deep experience working with GoogleData Products (e.g. Big Query, Dataflow, Dataproc, AI Building Blocks, Looker,Cloud Data Fusion, Dataprep, etc.).\nExperience in Spark /Scala / Python/Java / Kafka.\nResponsible to Ingest data from files, streams anddatabases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT\nDevelop programs in Scala and Python as part ofdata cleaning and processing\nResponsible to design and develop distributed, highvolume, high velocity multi-threaded event processing systems\nDevelop efficient software code for multiple usecases leveraging Python and Big Data technologies for various use cases builton the platform\nProvide high operational excellence guaranteeinghigh availability and platform stability\nImplement scalable solutions to meet theever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka,any Cloud computing etc\nYou bring in:\nMinimum 4+ years of experience in Big Datatechnologies\nGood to have experience in Cloud, GCP, AWS,Azure Data Engineering with background in Spark/Python/Scala / Java.\nProficient in any of the programming languages -Python, Scala or Java\nMandatory experience in Mid to Expert Levelprogramming capabilities in a large-scale enterprise\nIn-depth experience in modern data platformcomponents such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc\nExperience with Distributed Versioning Controlenvironments such as GIT\nFamiliarity with development tools - experience oneither IntelliJ / Eclipse / VSCode IDE, Build Tool Maven\nDemonstrated experience in modern API platformdesign including how modern UI s are built consuming services / APIs.\nExperience on Azure cloud including Data Factory,Databricks, Data Lake Storage is highly preferred.\nSolid experience in all phases of Software DevelopmentLifecycle - plan, design, develop, test, release, maintain and support, decommission.\nYour qualificationis:\nB.E/B.Tech/M.C.A/MSc(preferably in Computer Science/IT)\nRole:Data Engineer\nIndustry Type:Software Product\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development","Cloud Computing, Gcp, Cloud, Scala, Spark, Eclipse, Software Development Life Cycle, Data Architecture, Big Data, Python"
Data Engineer,Qentelli Solutions Private Limited,4-9 Years,,Hyderabad,"Information Technology, Information Services","Role & responsibilities\nMust Haves Skills\nExtensive experience as Data Engineer with Python Language and Cloud Technologies (AWS preferably).\nExperience in Automating ETL process/Pipelines and AWS Data & Infrastructure with Python.\nExtensive experience with AWS components like S3, Athena, EMR, Glue, Redshift, Kinesis and SageMaker.\nExtensive Experience with SQL/Unix/Linux scripting.\nDeveloping/testing Experience on Cloud/On Prem ETL Technologies (Ab Initio, AWS Glue, Informatica, Alteryx).\nExperience in Data migration from Onprem to Cloud is Plus.\nExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data Science.\nExtensive experience in DevOps/Data Ops space.\nHaving experience in Data Science platforms like SageMaker/Machine Learning Studio/ H2O is plus.\nWork Description SDET Python, AWS, Unix and ETL.\nWork with business stakeholders, Business Systems Analysts and Developers to ensure delivery of Data Applications.\nBuilding Automation Frameworks using Python.\nDesigning and managing the data workflows using Python during development and deployment of data products\nDesign, development of Reports and dashboards.\nAnalyzing and evaluating data sources, data volume, and business rules.\nShould be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.\nInterpret and analyses data from various source systems to support data integration and data reporting needs.\nExperience in testing Database Application to validate source to destination data movement and transformation.\nWork with team leads to prioritize business and information needs.\nDevelop and summarize Data Quality analysis and dashboards.\nKnowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.\nExecute testing of data analytic and data integration on time and within budget.\nTroubleshoot & determine best resolution for data issues and anomalies\nHas deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platforms","Numpy, Amazon Web Services, Pandas, Python, AWS"
Data Engineer,Anblicks Solutions,8-13 Years,,Ahmedabad,Cloud Data Services,"Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise in Azure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake) along with big data processing, real-time analytics, and cloud data integration.\nKey Responsibilities:\n1. Data Pipeline Development ETL/ELT\nDesign and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.\nImplement ETL/ELT workflows for structured and unstructured data processing.\nOptimize data ingestion, transformation, and storage strategies.\n2. Cloud Data Architecture Integration\nDevelop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).\nWork with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.\n3. Database Management Optimization\nDesign and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).\nOptimize SQL queries and indexing strategies for performance.\nImplement data partitioning, compression, and caching for efficiency.\n4. Data Governance, Security Compliance\nEnsure data quality, lineage, and governance with tools like Purview.\nImplement role-based access control (RBAC), encryption, and security policies.\nEnsure compliance with GDPR, HIPAA, and ISO 27001 regulations.\n5. Monitoring Performance Tuning\nUse Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.\nAutomate data pipeline testing and validation.\n6. Collaboration Documentation\nDocument data models, pipeline architectures, and data workflows.\nTechnical Skills:\nCloud Data Services: Azure Data Factory, Azure Synapse, Databricks, SSIS, BigQuery.\nETL Data Pipelines: Apache Spark, Python, SQL.\nBig Data Processing: Delta Lake, Parquet, Hadoop, Event Hubs.\nDatabase Management: SQL Server, Cosmos DB.\nSecurity Compliance: RBAC, Data Masking, Encryption, Purview.\nScripting Automation: Python, PowerShell, Terraform for IaC.","Cloud Architecture, Azure Data Factory, Data Governance, Performance Tuning"
Data Engineer,Genzeon Corporation,2-5 Years,,Pune,Information Technology,"Genzeon is looking for an experienced Full Stack Data Engineer to join our team in Pune. In this role, you will be instrumental in shaping our data architecture, developing full-stack data solutions, and driving innovation in data processing and analytics. This position offers an exciting opportunity to work with a team of passionate professionals dedicated to leveraging data for impactful decisions and products.\nResponsibilities\nData Architecture:Design and build robust, scalable data architectures that support both operational and analytical use cases.\nETL Development:Develop and maintain ETL processes to gather data from various sources, ensuring data quality and consistency.\nData Modeling:Create and optimize data models to support efficient data storage, retrieval, and analysis.\nAPI Development:Design and implement APIs for data ingestion, processing, and retrieval.\nData Visualization:Develop dashboards and reports to visualize complex datasets in a user-friendly manner.\nCloud Solutions:Work with cloud technologies to deploy and maintain data solutions.\nCollaboration:Work closely with data scientists, analysts, and other engineers to integrate data solutions into company products and services.\nInnovation:Stay updated with the latest trends and technologies in data engineering and propose innovative solutions to improve existing systems.\nRequirements\nEducation:Bachelor's or Master's degree in Computer Science, Engineering, or a related field.\nExperience:Proven experience as a Full Stack Data Engineer or in a similar role.\nTechnical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.\nData Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).\nFront-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.\nCloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.\nAnalytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.\nCommunication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.\nBenefits\nCompetitive salary and benefits package.\nDynamic and innovative work environment.\nOpportunities for professional growth and development.\nFlexible working hours and supportive team culture.\nMust Have Skills:\nTechnical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.\nData Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).\nFront-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.\nCloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.\nAnalytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.\nCommunication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.","Java, python, Spark, hadoop, Sql, aws"
Data Engineer,TechnoGen,6-9 Years,,Hyderabad,"Consulting, Information Services","Python Proficiency: Strong understanding of Python, with practical coding experience\nAWS:Comprehensive knowledge of AWS services and their applications\nAirflow: creating and managing Airflow DAG scheduling.\nUnix & SQL: Solid command of Unix commands, shell scripting, and writing efficient SQL scripts\nAnalytical & Troubleshooting Skills: Exceptional ability to analyze data and resolve complex issues.\nDevelopment Tasks: Proven capability to execute a variety of development activities with efficiency\nInsurance Domain Knowledge:Familiarity with the Insurance sector is highly advantageous.\nProduction Data Management: Significant experience in managing and processing production data\nWork Schedule Flexibility:Open to working in any shift, including 24/7 support, as require","Airflow, Trouble Shooting, Core Python, Sql, AWS, data engineering, Data Management"
Data Engineer,E Solutions,5-10 Years,,Noida,"Information Technology, Information Services","Proficiency in Python, Java, or SQL for data processing and automation.\nHands-on experience with cloud platforms (especially Azure, Snowflake, and Databricks).\nExpertise in big data technologies (such as Apache Spark, Hadoop, or Kafka).\nDeep understanding of data modeling, performance tuning, and optimization.\nFamiliarity with transformer-based architecture, Generative AI, and LLM data workflows\nLeadership & Collaboration:\nProven ability to lead complex data engineering projects.\nStrong mentorship skills and experience guiding junior engineers.\nExcellent communication and teamwork skills to collaborate with cross-functional teams.","Data Processing, Transformers, Llm, Java, Big Data Technologies, Python, Sql, Azure"
Data Engineer,IDESLABS,5-7 Years,,Pune,"Recruiting, Staffing Agency","Job description\n5+ years of relevant experience in the field of Data Engineering\nAdvance skills in big data technologies like Hadoop, Python, Spark, SQL.\nMust have experience building data APIs.\nBachelor s in Computer Science or related disciplines\nKnowledge of Data Structures and Algorithm.\nStrong Python programming skills with ability to implement OOPs and functional programming. Knowledge of Scala/Java would be plus.\nStrong knowledge on RDBMS and NoSQL databases with the ability to implement them from scratch. Knowledge of Graph databases will be a plus.\nStrong expertise in building & optimizing data pipelines, architectures, and data sets.\nExperience working with different file formats like Parquet, ORC, Avro, RC, etc.\nExperience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc.\nKnowledge and experience of using orchestration frameworks like Airflow, Oozie, Luigi, etc.\nExperience using Spark, and building jobs using Python/Scala/Java.\nExperience or Knowledge building stream processing platforms using Spark Streaming, Storm, etc. Knowledge of Kafka/Flink+Beam would be plus.\nKnowledge of building REST API end points for data consumption.\nExperience in building scalable data pipelines for both real time and batch using best practices in data modeling, ETL/ELT processes utilizing varioud technologies such as Spark, Kafka, Presto, SAP HANA, Airflow, informatica.\nPerform Data analysis using Python, complex SQLs, and other tools.\nPerform root cause analysis of issues from platform standpoint on Kubernetes, Containers, Hadoop, Spark, Hive, Presto\nExcellent oral and written communication is a must.\nPreferred\nMasters in Computer Science or related disciplines\nExperience building self-service tools for analytics would be plus.\nKnowledge of ELK stack would be a plus.\nKnowledge of implementing CI/CD on the pipelines is a plus.\nKnowledge of Containerization (Docker/Kubernetes) will be plus.\nExperience working with one of the popular Public Cloud based platforms is preferred","Airflow, Java, Python, Docker, Presto, Sap Hana, Spark, Kafka"
Data Engineer,Kanini Software Solutions,1-4 Years,,"Coimbatore, Bengaluru",Software,"Job DescriptionWant to join KANINI\nWe are looking for aData Engineer who can build a robust database and its architecture. In thisrole, you will assess a wide range of requirements and apply relevant databasetechniques to create a sustainable data architecture before you begin theimplementation process and develop the database from scratch.\nYou areall set to:\nDevelop, maintain,evaluate, and test big data solutions. You will be involved in data engineeringactivities like creating pipelines/workflows for Source-to-Target Data Mapping amongothers.\nYou are someone who can:\nYou will be involved in the design of datasolutions using Hadoop based technologies along with Hadoop, Azure, HDInsightfor Cloudera based Data Late using Scala Programming.\nLiaise and be part of our extensive GCP community,contributing in the knowledge exchange learning programme of the platform.\nBe required to showcase your GCP Data engineeringexperience when communicating with business team on their requirements, turningthese into technical data solutions.\nBe required to build and deliver Data solutionsusing GCP products and offerings.\nHands on and deep experience working with GoogleData Products (e.g. Big Query, Dataflow, Dataproc, AI Building Blocks, Looker,Cloud Data Fusion, Dataprep, etc.).\nExperience in Spark /Scala / Python/Java / Kafka.\nResponsible to Ingest data from files, streams anddatabases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT\nDevelop programs in Scala and Python as part ofdata cleaning and processing\nResponsible to design and develop distributed, highvolume, high velocity multi-threaded event processing systems\nDevelop efficient software code for multiple usecases leveraging Python and Big Data technologies for various use cases builton the platform\nProvide high operational excellence guaranteeinghigh availability and platform stability\nImplement scalable solutions to meet theever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka,any Cloud computing etc\nYou bring in:\nMinimum 4+ years of experience in Big Datatechnologies\nGood to have experience in Cloud, GCP, AWS,Azure Data Engineering with background in Spark/Python/Scala / Java.\nProficient in any of the programming languages -Python, Scala or Java\nMandatory experience in Mid to Expert Levelprogramming capabilities in a large-scale enterprise\nIn-depth experience in modern data platformcomponents such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc\nExperience with Distributed Versioning Controlenvironments such as GIT\nFamiliarity with development tools - experience oneither IntelliJ / Eclipse / VSCode IDE, Build Tool Maven\nDemonstrated experience in modern API platformdesign including how modern UI s are built consuming services / APIs.\nExperience on Azure cloud including Data Factory,Databricks, Data Lake Storage is highly preferred.\nSolid experience in all phases of Software DevelopmentLifecycle - plan, design, develop, test, release, maintain and support, decommission.\nYour qualificationis:\nB.E/B.Tech/M.C.A/MSc(preferably in Computer Science/IT)\nRole:Data Engineer\nIndustry Type:Software Product\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development","Cloud Computing, Gcp, Cloud, Scala, Spark, Eclipse, Software Development Life Cycle, Data Architecture, Big Data, Python"
Data Engineer - Senior,Cummins India Limited,8-10 Years,,Pune,Manufacturing,"Although the role category specified in the GPP is Remote, the requirement is for Hybrid.\nKey Responsibilities:\nDesign and Automation: Deploy distributed systems for ingesting and transforming data from various sources (relational, event-based, unstructured).\nData Quality and Integrity: Implement frameworks to monitor and troubleshoot data quality and integrity issues.\nData Governance: Establish processes for managing metadata, access, and retention for internal and external users.\nData Pipelines: Build reliable, efficient, scalable, and quality data pipelines with monitoring and alert mechanisms using ETL/ELT tools or scripting languages.\nDatabase Structure: Design and implement physical data models to optimize database performance through efficient indexing and table relationships.\nOptimization and Troubleshooting: Optimize, test, and troubleshoot data pipelines.\nLarge Scale Solutions: Develop and operate large-scale data storage and processing solutions using distributed and cloud-based platforms (e.g., Data Lakes, Hadoop, Hbase, Cassandra, MongoDB, Accumulo, DynamoDB).\nAutomation: Use modern tools and techniques to automate common, repeatable, and tedious data preparation and integration tasks.\nInfrastructure Renovation: Renovate data management infrastructure to drive automation in data integration and management.\nAgile Development: Ensure the success of critical analytics initiatives using agile development technologies such as DevOps, Scrum, Kanban.\nTeam Development: Coach and develop less experienced team members.\nExternal Qualifications and Competencies\nQualifications:\nCollege, university, or equivalent degree in a relevant technical discipline, or equivalent experience required. Licensing may be required for compliance with export controls or sanctions regulations.\nCompetencies:\nSystem Requirements Engineering: Translate stakeholder needs into verifiable requirements; establish acceptance criteria; track requirements status; assess impact of changes.\nCollaboration: Build partnerships and work collaboratively to meet shared objectives.\nCommunication: Develop and deliver communications that convey a clear understanding of the unique needs of different audiences.\nCustomer Focus: Build strong customer relationships and deliver customer-centric solutions.\nDecision Quality: Make good and timely decisions to keep the organization moving forward.\nData Extraction: Perform ETL activities from various sources using appropriate tools and technologies.\nProgramming: Create, write, and test computer code, test scripts, and build scripts to meet business, technical, security, governance, and compliance requirements.\nQuality Assurance Metrics: Apply measurement science to assess solution outcomes using ITOM, SDLC standards, tools, metrics, and KPIs.\nSolution Documentation: Document information and solutions to enable improved productivity and effective knowledge transfer.\nSolution Validation Testing: Validate configuration item changes or solutions using SDLC standards, tools, and metrics.\nData Quality: Identify, understand, and correct data flaws to support effective information governance.\nProblem Solving: Solve problems using systematic analysis processes; implement robust, data-based solutions; prevent problem recurrence.\nValues Differences: Recognize the value of different perspectives and cultures.\nAdditional Responsibilities Unique to this Position\nSkills:\nETL/Data Engineering Solution Design and Architecture: Expert level.\nSQL and Data Modeling: Expert level (ER Modeling and Dimensional Modeling).\nTeam Leadership: Ability to lead a team of data engineers.\nMSBI (SSIS, SSAS): Experience required.\nDatabricks (Pyspark) and Python: Experience required.\nAdditional Skills: Snowflake, Power BI, Neo4j (good to have).\nCommunication: Good communication skills.\nPreferred Experience:\n8+ years of overall experience.\n5+ years of relevant experience in data engineering.\nKnowledge of the latest technologies and trends in data engineering.\nTechnologies: Familiarity with analyzing complex business systems, industry requirements, and data regulations.\nBig Data Platform: Design and development using open source and third-party tools.\nTools: SPARK, Scala/Java, Map-Reduce, Hive, Hbase, Kafka.\nSQL: Proficiency in SQL query language.\nCloud-Based Implementation: Experience with clustered compute cloud-based implementations.\nLarge File Movement: Experience developing applications requiring large file movement for cloud environments.\nAnalytical Solutions: Experience in building analytical solutions.\nIoT Technology: Intermediate experience preferred.\nAgile Software Development: Intermediate experience preferred.\nRole:Data Engineer\nIndustry Type:Industrial Equipment / Machinery\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate","snowflake, Pyspark, Databricks, Python, Sql"
Big Data Engineer (Java & Spark),Synechron Technologies Private Limited,5-10 Years,,Pune,Information Technology,"As a Big Data Engineer, you will be responsible for translating application storyboards and use cases into functional applications, designing and building efficient and reliable Java code. You will ensure optimal performance, quality, and responsiveness of applications, while also identifying bottlenecks and devising solutions. Your role will involve developing high-performance, low-latency components to run Spark clusters and collaborating with global teams to propose best practices and standards.\nTechnical Skills:\nProgramming Languages: Strong Java experience (8+ years) with Java 1.8 or higher; solid understanding of object-oriented programming and design patterns.\nBig Data Technologies: Experience with HDFS, Hive, HBase, Apache Spark, and Kafka.\nData Processing: Proficient in processing data using Hive, Impala, and HBase; capable of performing analysis on large data sets.\nAPIs and Architecture: Experience in building self-service platform-agnostic data access APIs; knowledge of service-oriented architecture and data standards like JSON, Avro, and Parquet.\nAnalytical Skills: Experience in building advanced analytical models based on business context; strong analytical and problem-solving skills.\nDevelopment Practices: Familiarity with Agile/Scrum methodologies, SCMs like Git, and tools like JIRA; strong understanding of unit testing and SDLC activities.\nScripting and Databases: Experience with Linux shell scripting and RDBMS/NoSQL databases; good knowledge of database principles, practices, and SQL development (preferably with Oracle).\nPerformance Tuning: Experience in application performance tuning and troubleshooting in the Big Data domain.\nOther Technologies: Familiarity with cloud and container technologies, build tools such as Maven, and continuous integration tools like Jenkins or Team City is a plus.\nExperience:\n5-10 years of experience in software development, with strong Java and Big Data technology expertise.\nDemonstrated ability to design solutions and mentor other developers within the team.\nProven experience in working with large data volumes and logical data structures.\nDay-to-Day Activities:\nTranslate application storyboards and use cases into functional applications.\nDesign, build, and maintain reliable and efficient Java code.\nEnsure optimal performance, quality, and responsiveness of applications.\nIdentify and resolve bottlenecks and bugs.\nDevelop high-performance components for Spark clusters.\nCollaborate with global teams to propose best practices and standards.\nTest software prototypes and facilitate handover to the operations team.\nProcess data using Hive, Impala, and HBase, and conduct analysis on large datasets.\nMentor and guide team members in technical skills and best practices.\nQualifications:\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.\nOptional: Familiarity with Arcadia Tool for Analytics.\nSoft Skills:\nExcellent analytical and problem-solving abilities.\nStrong communication and collaboration skills to work effectively with cross-functional teams.\nAbility to mentor and guide junior developers.\nStrong attention to detail and ability to work under pressure.\nCreative thinking and initiative in proposing improvements and innovations.","Troubleshooting, Performance Tuning, RDBMS, Linux, Shell scripting, Data structures, Json, Oracle, Sdlc"
Data Engineer - Senior,Cummins India Limited,6-8 Years,,Pune,Manufacturing,"Job Summary:\nLeads projects for the design, development, and maintenance of a data and analytics platform. Effectively and efficiently processes, stores, and makes data available to analysts and other consumers. Works with key business stakeholders, IT experts, and subject-matter experts to plan, design, and deliver optimal analytics and data science solutions. Works on one or many product teams at a time. Though the role category is generally listed as Remote, this specific position is designated as Hybrid.\nKey Responsibilities:\nBusiness Alignment & CollaborationPartner with the Product Owner to align data solutions with strategic goals and business requirements.\nData Pipeline Development & Management Design, develop, test, and deploy scalable data pipelines for efficient data transport into Cummins Digital Core (Azure DataLake, Snowflake) from various sources (ERP, CRM, relational, event-based, unstructured).\nArchitecture & Standardization Ensure compliance with AAI Digital Core and AAI Solutions Architecture standards for data pipeline design and implementation.\nAutomation & Optimization Design and automate distributed data ingestion and transformation systems, integrating ETL/ELT tools and scripting languages to ensure scalability, efficiency, and quality.\nData Quality & Governance Implement data governance processes, including metadata management, access control, and retention policies, while continuously monitoring and troubleshooting data integrity issues.\nPerformance & Storage Optimization Develop and implement physical data models, optimize database performance (indexing, table relationships), and operate large-scale distributed/cloud-based storage solutions (Data Lakes, Hadoop, HBase, Cassandra, MongoDB, Accumulo, DynamoDB).\nInnovation & Tool Evaluation Conduct proof-of-concept (POC) initiatives, evaluate new data tools, and provide recommendations for improvements in data management and integration.\nDocumentation & Best Practices Maintain standard operating procedures (SOPs) and data engineering documentation to support consistency and efficiency.\nAgile Development & Automation Use Agile methodologies (DevOps, Scrum, Kanban) to drive automation in data integration, preparation, and infrastructure management, reducing manual effort and errors.\nCoaching & Team Development Provide guidance and mentorship to junior team members, fostering skill development and knowledge sharing.\nExternal Qualifications and Competencies\nCompetencies:\nSystem Requirements Engineering:Translates stakeholder needs into verifiable requirements, tracks status, and assesses impact changes.\nCollaborates:Builds partnerships and works collaboratively with others to meet shared objectives.\nCommunicates Effectively:Delivers multi-mode communications tailored to different audiences.\nCustomer Focus:Builds strong customer relationships and provides customer-centric solutions.\nDecision Quality:Makes good and timely decisions that drive the organization forward.\nData Extraction:Performs ETL activities from various sources using appropriate tools and technologies.\nProgramming:Develops, tests, and maintains code using industry standards, version control, and automation tools.\nQuality Assurance Metrics:Measures and assesses solution effectiveness using IT Operating Model (ITOM) standards.\nSolution Documentation:Documents knowledge gained and communicates solutions for improved productivity.\nSolution Validation Testing:Validates configurations and solutions to meet customer requirements using SDLC best practices.\nData Quality:Identifies, corrects, and manages data flaws to support effective governance and decision-making.\nProblem Solving:Uses systematic analysis to determine root causes and implement robust solutions.\nValues Differences:Recognizes and leverages the value of diverse perspectives and cultures.\nEducation, Licenses, Certifications:\nBachelor's degree in a relevant technical discipline, or equivalent experience required.\nThis position may require licensing for compliance with export controls or sanctions regulations.\n\nAdditional Responsibilities Unique to this Position\nPreferred Experience:\nTechnical Expertise Intermediate experience in data engineering with hands-on knowledge of SPARK, Scala/Java, MapReduce, Hive, HBase, Kafka, and SQL.\nBig Data & Cloud Solutions Proven ability to design and develop Big Data platforms, manage large datasets, and implement clustered compute solutions in cloud environments.\nData Processing & Movement Experience developing applications requiring large-scale file movement and utilizing various data extraction tools in cloud-based environments.\nBusiness & Industry Knowledge Familiarity with analyzing complex business systems, industry requirements, and data regulations to ensure compliance and efficiency.\nAnalytical & IoT Solutions Experience building analytical solutions with exposure to IoT technology and its integration into data engineering processes.\nAgile Development Strong understanding of Agile methodologies, including Scrum and Kanban, for iterative development and deployment.\nTechnology Trends Awareness of emerging technologies and trends in data engineering, with a proactive approach to innovation and continuous learning.\nTechnical Skills:\nProgramming Languages:Proficiency in Python, Java, and/or Scala.\nDatabase Management:Expertise in SQL and NoSQL databases.\nBig Data Technologies:Hands-on experience with Hadoop, Spark, Kafka, and similar frameworks.\nCloud Services:Experience with Azure, Databricks, and AWS platforms.\nETL Processes:Strong understanding of Extract, Transform, Load (ETL) processes.\nData Replication:Working knowledge of replication technologies like Qlik Replicate is a plus.\nAPI Integration:Experience working with APIs to consume data from ERP and CRM systems.\nRole:Data Engineer\nIndustry Type:Industrial Equipment / Machinery\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development","snowflake, Pyspark, Databricks, Python, Sql, Azure, Hadoop"
"Sr. Big Data Engineer (Java, Apache Spark and Data Architecture)",Synechron Technologies Private Limited,7-9 Years,,Pune,Software,"Job Summary:\nWe are looking for a highly skilled Sr. Big Data Engineer professional to join our team. This role will focus on designing, developing, and maintaining large-scale data processing systems. The ideal candidate will have extensive experience in Big Data technologies, Java programming, and a strong understanding of data architecture principles. You will be responsible for translating functional requirements into robust data solutions, collaborating with global teams, and mentoring junior developers.\nOverall Responsibilities:\nTranslate application storyboards and use cases into functional applications.\nDesign, build, and maintain efficient, reusable, and reliable Java code.\nEnsure optimal performance, quality, and responsiveness of applications.\nIdentify bottlenecks and bugs, and devise appropriate solutions.\nDevelop high-performance and low-latency components to run Spark clusters.\nInterpret functional requirements into design approaches suitable for the Big Data platform.\nCollaborate with global teams across various locations to deliver high-quality solutions.\nPropose best practices and standards, and ensure smooth handover to the operations team.\nConduct testing of software prototypes and facilitate the transfer to operational teams.\nProcess data using Hive, Impala, and HBase.\nAnalyze large data sets to derive actionable insights.\nTechnical Skills:\nCore Skills:\nSolid understanding of object-oriented programming and design patterns.\nStrong experience with Java (8+ years) using Java 1.8 or higher.\nProficiency in working with large data volumes and logical data structures.\nBig Data Technologies:\nExperience with HDFS, Hive, HBase, Apache Spark, and Kafka.\nFamiliarity with building self-service platform-agnostic data access APIs.\nUnderstanding of service-oriented architecture and data standards (JSON, Avro, Parquet).\nAnalytical Skills:\nExperience in building advanced analytical models based on business context.\nStrong systems analysis, design, and architecture fundamentals.\nDevelopment Tools:\nFamiliarity with Agile/Scrum methodologies.\nExperience with source control management tools like GIT and project management tools like JIRA.\nBasic proficiency in Linux shell scripting.\nUnderstanding of RDMS and NoSQL databases.\nAdditional Skills:\nApplication performance tuning and troubleshooting in the Big Data domain.\nAbility to write reliable, manageable, and high-performance code.\nKnowledge of database principles, SQL development (preferably with Oracle).\nFamiliarity with concurrency patterns and multithreading in Java.\nUnderstanding of domain design concepts, JDBC, and RESTful services.\nOptional:\nFamiliarity with Arcadia Tool for Analytics.\nUnderstanding of cloud and container technologies.\nExperience with build tools such as Maven and continuous integration tools like Jenkins/TeamCity.\nExperience:\nMinimum of 8 years of experience in software development, with a focus on Big Data technologies.\nProven experience in mentoring and guiding other developers within a team.\nDemonstrated analytical and problem-solving skills.\nDay-to-Day Activities:\nParticipate in daily stand-up meetings and sprint planning sessions.\nCollaborate with cross-functional teams to understand business requirements.\nWrite, test, and deploy scalable software solutions.\nConduct code reviews and provide feedback to team members.\nStay updated with the latest trends and advancements in Big Data technologies.\nProvide technical support and mentorship to team members.\nQualifications:\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.\nSoft Skills:\nExcellent written and verbal communication skills.\nStrong collaboration and teamwork abilities.\nExceptional problem-solving and analytical skills.\nAbility to adapt to new technologies and changing requirements.\nStrong time management and prioritization skills.","Big Data Engineer, Java, Spark"
Data Engineer,Amazon Development Centre (India) Private Limited,5-10 Years,,Hyderabad,Software,"Skill Data Engineer\nRole T3, T2\nKey responsibility\nData Engineer Must have 5+ years of experience in below mentioned skills. Must Have Big Data Concepts , Python(Core Python- Able to write code), SQL, Shell Scripting, AWS S3 Good to Have Event-driven/AWA SQS, Microservices, API Development, Kafka, Kubernetes, Argo, Amazon Redshift, Amazon Aurora","Data Engineer, Sql, AWS"
Aws Data Engineer,Amazon Development Centre (India) Private Limited,3-7 Years,,Bengaluru,Software,"Cloud and AWS Expertise:\nIn-depth knowledge of AWS services related to data engineering: EC2, S3, RDS, DynamoDB, Redshift, Glue, Lambda, Step Functions, Kinesis, Iceberg, EMR, and Athena.\nStrong understanding of cloud architecture and best practices for high availability and fault tolerance.\nData Engineering Concepts:\nExpertise in ETL/ELT processes, data modeling, and data warehousing.\nKnowledge of data lakes, data warehouses, and big data processing frameworks like Apache Hadoop and Spark.\nProficiency in handling structured and unstructured data.\nProgramming and Scripting:\nProficiency in Python, Pyspark and SQLfor data manipulation and pipeline development.\nExpertise in working with data warehousing solutions like Redshift.","Aws Data Engineer, Hadoop, Redshift"
Data Engineer,Cummins India Limited,4-8 Years,,Pune,Manufacturing,"Job Summary:\nSupports, develops and maintains a data and analytics platform. Effectively and efficiently process, store and make data available to analysts and other consumers. Works with the Business and IT teams to understand the requirements to best leverage the technologies to enable agile data delivery at scale.\nKey Responsibilities:\nImplements and automates deployment of our distributed system for ingesting and transforming data from various types of sources (relational, event-based, unstructured). Implements methods to continuously monitor and troubleshoot data quality and data integrity issues. Implements data governance processes and methods for managing metadata, access, retention to data for internal and external users.\nDevelops reliable, efficient, scalable and quality data pipelines with monitoring and alert mechanisms that combine a variety of sources using ETL/ELT tools or scripting languages. Develops physical data models and implements data storage architectures as per design guidelines.\nAnalyzes complex data elements and systems, data flow, dependencies, and relationships in order to contribute to conceptual physical and logical data models. Participates in testing and troubleshooting of data pipelines.\nDevelops and operates large scale data storage and processing solutions using different distributed and cloud based platforms for storing data (e.g. Data Lakes, Hadoop, Hbase, Cassandra, MongoDB, Accumulo, DynamoDB, others). Uses agile development technologies, such as DevOps, Scrum, Kanban and continuous improvement cycle, for data driven application.\nGood Understanding of ETL framework and technologies\nTechnologies - Databricks, Pyspark, Python, Snowflake, SQL\nGood to have skills - Power BI, Neo4j, Matillion\nGood communication skills\nRole:Data Engineer\nIndustry Type:Industrial Equipment / Machinery\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development","snowflake, Pyspark, Databricks, Python, Sql"
Data Engineer 2,Bread Financial,2-5 Years,,Bengaluru,Financial Services,"The Data Engineer II works on different projects of data engineering to support the use cases, data ingestion pipeline and identify potential process or data quality issues. The team also supports marketing analytic teams with analytical tools that enable our analytics and business communities to do their job easier, faster and smarter. The team brings together data from different internal external partners and builds a curated Marketing analytics focused data tools ecosystem. The Data Engineer plays a crucial role in building this ecosystem depending on the Marketing analytics communities need.\nEssential Job Functions\nCollaboration - Collaborates with internal/external stakeholders to manage data logistics - including data specifications, transfers, structures, and rules. Collaborates with business users, business analysts and technical architects in transforming business requirements into analytical workbenches, tools and dashboards reflecting usability best practices and current design trends. Demonstrates analytical, interpersonal and professional communication skills. Learns quickly and works effectively individually and as part of a team.\nProcess Improvement - Accesses, extracts, and transforms Credit and Retail data from a variety of sources of all sizes (including client marketing databases, 2nd and 3rd party data) using Hadoop, Spark, SQL, Big data technologies etc. Provide automation help to analytical teams around data centric needs using orchestration tools, SQL and possibly other big data/cloud solutions for efficiency improvement.\nProject Support - Supports Sr. Specialist and Specialist in new analytical proof of concepts and tool exploration projects. Effectively manages time and computing resources in order to deliver on time/correctly on concurrent projects. Involved in creating POCs to ingest and process streaming data using Spark and HDFS.\nData and Analytics - Answers and trouble shoots questions about data sets and analytical tools; Develops, maintains and enhances new and existing analytics tools/Frameworks to support internal customers/consumers. Ingests data from different sources, processes it according to the requirement document in order to store data to Hive or NoSQL database or different warehousing solutions. Manages data coming from different sources, involved in HDFS maintenance, and loading of structured and unstructured data. Applies knowledge in Agile Scrum methodology that leverages the Client BigData platform and used version control tool Git. Imports and exports data using Sqoop from HDFS to RDBMS and vice-versa. Demonstrates an understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management. Creates POCs to ingest and process streaming data using Spark and HDFS. Work on back-end using Scala, Python and Spark to perform several aggregation logics.\nTechnical Skills - Expert in writing complicated SQL Queries and database analysis for good performance. Experience working with python or Scala, Spark, Hadoop, Hive, Oozie, Sqoop, HDFS, Impala, Shell Scripts, Microsoft Azure Services like ADLS/Blob Storage solutions, Azure DataFactory, Azure Functions and Databricks. Utilize basic knowledge of Rest API for designing networked applications.\nReports to: Lead or above\nWorking Conditions/Physical Requirements: Normal office environment\nDirect Reports: 0\nMinimum Requirements:\nDegree Required: Bachelor's Degree\nArea of Study: Computer Science, Engineering\nYears of Work Experience Required: Two to five years or more\nType / focus of work experience required: Data Analytics","Python, Data Analytics, Sql, Azure Cloud, Big Data, Rest Api"
Job | Immediately Hiring for AWS Data Engineer @ IT Company,SPN Globe,5-10 Years,INR 25 - 30.5 LPA,Pune,Login to check your skill match score,"Job | Immediately Hiring for AWS Data Engineer @ IT Company\nSPN Globe is a specialized IT recruitment and staffing firm. We are the sourcing partner of various software companies on Permanent and Contract to Hire Demands. Also working with few top notch (CMMI-5 level) clients, PAN India clients and major city clients such as Hyderabad, Bengaluru, Pune, Mumbai, Nagpur and NCR.\nPlease find the position details below:\nCompany: IT Company\nRole: AWS Data Engineer\nType: Permanent (No third-party payroll)\nLocation: Pune\nJoining: 0 to 30 days / Immediate Joiners\nExperience: 5 to 10 years\nTECHNICAL SKILLS: Python,Spark, Scala, AWS, ETL,AWS Services\nApply immediately to grab it. Email your resume at [HIDDEN TEXT]\nAlso, immediately refer this opportunity to your friends.","Aws Services, Scala, Spark, Python, AWS, Etl"
Data Engineer ( CDP),MResult Technologies Private Limited,3-7 Years,,Remote,Login to check your skill match score,"Job Role: CDP Data Engineer\nWhy MResult\nFounded in 2004, MResult is a global digital solutions partner trusted by leading Fortune 500 companies in industries such as pharma & healthcare, retail, and BFSI. MResult's expertise in data and analytics, data engineering, machine learning, AI, and automation help companies streamline operations and unlock business value. As part of our team, you will collaborate with top minds in the industry to deliver cutting-edge solutions that solve real-world challenges.\nWhat We Offer:\nAt MResult, you can leave your mark on projects at the world's most recognized brands, access opportunities to grow and upskill, and do your best work with the flexibility of hybrid work models. Great work is rewarded, and leaders are nurtured from within.\nOur values Agility, Collaboration, Client Focus, Innovation, and Integrity are woven into our culture, guiding every decision.\nWebsite:https://mresult.com/\nLinkedIn: https://www.linkedin.com/company/mresult/\nWhat This Role Requires\nIn the role of CDP Data Engineer you will be a key contributor to MResult's mission of empowering our clients with data-driven insights and innovative digital solutions. Each day brings exciting challenges and growth opportunities. Here is what you will do:\nDesign, develop, and implement solutions using Customer Data Platform (CDP) to manage and analyze customer data.\nCollaborate with cross-functional teams to understand business requirements and translate\nthem into technical solutions.\nIntegrate CDP with various data sources and ensure seamless data flow and accuracy.\nDevelop and maintain data pipelines, ensuring data is collected, processed, and stored efficiently.\nCreate and manage customer profiles, segments, and audiences within the CDP.\nImplement data governance and security best practices to protect customer data.\nMonitor and optimize the performance of the CDP infrastructure.\nProvide technical support and troubleshooting for CDP-related issues.\nStay updated with the latest trends and advancements in CDP technology and best practices.\nKey Skills to Succeed in This Role:\nOverall experience 3-6 yrs\nExperience in Customer Insights Data.\nExperience in customer insights journey.\nExperience in ADLS, ADF & Synapse is a must.\nExperience in data verse, Power platform and Snowflake.\nManage, Master, and Maximize with MResult\nMResult is an equal-opportunity employer committed to building an inclusive environment free of discrimination and harassment.\nTake the next step in your career with MResult where your ideas help shape the future.","Customer Data Platform, Cdp, ADLS, Adf, Azure, Azure Synapse"
Data engineer Manager,Yotta Techports Private Limited,10-15 Years,INR 25 - 40 LPA,Hyderabad,Software,"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\nQualifications:\nBachelor's degree in computer science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, and expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.","Team Development, snowflake, Teambuilding, Stakeholder Management, amazon emr, Amazon Redshift, Aws S3, AWS Glue, Databricks, Aws Ec2"
Big Data Engineer,Robotics Technologies,10-15 Years,INR 11 - 16 LPA,"Gurugram, Hyderabad, Kolkata","Meeting Software, Private Cloud, Presentation Software, Computer Vision, Consumer Software, Operating Systems, Video Conferencing","We are seeking a highly skilled Big Data Engineer with 10-15 years of experience to join our team in India. The ideal candidate will be responsible for designing and implementing robust big data solutions, ensuring data is available and accessible for analytics and decision-making.\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and architectures for big data processing.\nCollaborate with data scientists and analysts to understand data needs and provide timely data access.\nImplement data integration processes and ETL workflows to ensure data quality and integrity.\nMonitor and optimize performance of big data solutions and infrastructure.\nWork with various big data technologies such as Hadoop, Spark, and Kafka.\nEnsure data security and compliance with relevant regulations.\nStay updated with emerging technologies and propose enhancements to the existing systems.\nSkills and Qualifications\n10-15 years of experience in Big Data technologies and frameworks.\nProficiency in programming languages such as Java, Scala, or Python.\nStrong experience with big data tools like Hadoop, Spark, Hive, and Kafka.\nKnowledge of data modeling, ETL processes, and data warehousing concepts.\nExperience with cloud platforms (AWS, Azure, Google Cloud) and their big data services.\nStrong analytical and problem-solving skills with attention to detail.\nAbility to work independently and collaboratively in a fast-paced environment.","Nosql, Hadoop, Data Modeling, Cloud Services, Spark, Kafka, Data Warehousing, Sql, Python, Etl"
Big Data Engineer,Robotics Technologies,3-7 Years,INR 12 - 44 LPA,"Ahmedabad, Bengaluru, Chennai","Cyber Security, Unified Communications, Data Center Automation, CMS, Messaging","Description\nWe are seeking a skilled Big Data Engineer to join our team in India. The ideal candidate will be responsible for designing, building, and maintaining scalable data processing systems to handle large volumes of data. You will work closely with data scientists and analysts to ensure that our data infrastructure meets the needs of our organization.\nResponsibilities\nDesign and implement scalable data pipelines and architecture for big data solutions.\nAnalyze and process large datasets to extract valuable insights and support decision-making.\nCollaborate with data scientists and analysts to optimize data processing and storage solutions.\nEnsure data quality and integrity through monitoring and troubleshooting data pipelines.\nStay updated with emerging big data technologies and trends to enhance existing systems.\nSkills and Qualifications\n3-7 years of experience in big data technologies such as Hadoop, Spark, and Kafka.\nProficient in programming languages such as Java, Scala, or Python.\nStrong understanding of database systems including SQL and NoSQL databases (e.g., MongoDB, Cassandra).\nExperience with data modeling and ETL processes.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud.\nKnowledge of data warehousing solutions and tools like Snowflake or Redshift.\nAbility to work with large datasets and perform data processing using tools like Hive or Pig.","ETL Processes, Nosql, Hadoop, Data Modeling, Spark, Kafka, Data Warehousing, Sql, Python, AWS"
Data Engineer,Commonwealth Bank,5-7 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title:Data Engineer-Big Data\nLocation:Bengaluru\nBusiness & Team:RM & FS Data Engineering\nImpact & contribution:\nAs a Senior Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you're ahead of trends and work at the forefront of Big Data and Data warehouse technologies.\nWhich is why we're the perfect fit for you. Here, you'll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers most complex data-centric problems.\nTo us, data is everything. It is what powers our cutting-edge features and it's the reason we can provide seamless experiences for millions of customers from app to branch.\nWe're responsible for CommBank's key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We work across the Cloudera Hadoop Big Data, Teradata Group Data Warehouse and Ab Initio platforms.\nRoles & Responsibilities:\nPassionate about building next generation data platforms and data pipeline solution across the bank.\nEnthusiastic, be able to contribute and learn from wider engineering talent in the team.\nReady to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.\nCapable to create both technology blueprints and engineering roadmaps, for a multi- year data transformational journey.\nCan lead and drive a culture where quality, excellence and openness are championed.\nConstantly thinking outside the box and breaking boundaries to solve complex data problems.\nHave experience in providing data driven solutions that source data from various enterprise data platform into Cloudera Hadoop Big Data environment, using technologies like Spark, MapReduce, Hive, Sqoop, Kafka transform and process the source data to produce data assets and transform and egression to other data platforms like Teradata or RDBMS system.\nAre experienced in building effective and efficient Big Data and Data Warehouse frameworks, capabilities, and features, using common programming language (Scala, Java, or Python), with proper data quality assurance and security controls.\nAre experienced in providing data driven solutions in the Cloud to build various enterprise data platform into AWS platform using technologies like S3, EMR, Glue, Iceberg, Kinesis or MSK/Kafka transform and process the data to produce data assets for Redshift and DocumentDB.\nAre confident in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.\nCan collaborate, co-create and contribute to existing Data Engineering practices in the team.\nHave experience and responsible for data security and data management.\nHave a natural drive to educate, communicate and coordinate with different internal stakeholders.\nEssential Skills:\nPreferably with at least 5+ years of hands-on experience in a Data Engineering role.\nExperience in designing, building, and delivering enterprise-wide data ingestion, data integration and data pipeline solutions using common programming language (Scala, Java, or Python) in a Big Data and Data Warehouse platform.\nExperience in building data solution in Hadoop platform, using Spark, Hive,MapReduce, Sqoop, Kafka and various ETL frameworks for distributed data storage and processing. Preferably with at least 5+ years of hands-on experience.\nExperience in building data solution using AWS Cloud technology (EMR, Glue, Iceberg, Kinesis, MSK/Kafka, Redshift, DocumentDB, S3, etc.). Preferably with 2+ years of hands-on experience and certified AWS Data Engineer.\nStrong Unix/Linux Shell scripting and programming skills in Scala, Java, or Python.\nProficient in SQL scripting, writing complex SQLs for building data pipelines.\nExperience in working in Agile teams, including working closely with internal business stakeholders.\nFamiliarity with data warehousing and/or data mart build experience in Teradata, Oracle or RDBMS system is a plus.\nCertification on Cloudera CDP, Hadoop, Spark, Teradata, AWS, Ab Initio is a plus.\nExperience in Ab Initio software products (GDE, Co Operating System, Express It, etc.) is a plus.\nEducational Qualifications: B.Tech and above\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 05/06/2025","MSK, DocumentDB, Glue, Teradata, Iceberg, Emr, Sql, Java, Hadoop, Kafka, Linux, Sqoop, Hive, Mapreduce, Big Data, Scala, S3, Unix, Cloudera, Kinesis, Ab Initio, AWS, Python, Redshift, Spark"
Data Engineer,InfoDriven Solutions Private Limited,2-4 Years,INR 12 - 20 LPA,Thiruvananthapuram / Trivandrum,Login to check your skill match score,"Job Type: Perm\nExp:2-4yrs\nWork Location: Trivandrum\nWork From Office\nRequired Qualifications\nBachelor's degree in Computer Science, Engineering, or a related field.\n2+ years of experience in data engineering or related roles.\nProficiency in SQL and programming languages such as Python, Java, or Scala.\nExperience with big data tools (e.g., Hadoop, Spark, Kafka). Familiarity with cloud platforms (AWS, GCP, Azure) and data warehousing solutions (e.g., Redshift, BigQuery, Snowflake).\nUnderstanding of data modeling, ETL pipelines, and workflow orchestration tools (e.g., Airflow, Luigi).\nStrong problem-solving skills and attention to detail","snowflake, Etl Design, BigQuery, Hadoop, Spark, Kafka, Data Engineer, Redshift, Etl, Java, Java, python"
Data Engineer,Robotics Technologies,10-15 Years,INR 11 - 16 LPA,"Gurugram, Hyderabad, Bengaluru","Meeting Software, Enterprise Applications, Presentation Software, Data Visualization, Presentations, Operating Systems","We are seeking a highly skilled Data Engineer with 10-15 years of experience to join our dynamic team in India. The ideal candidate will be responsible for designing and maintaining scalable data architecture, building data pipelines, and ensuring the availability and integrity of data for analysis and reporting.\nResponsibilities\nDesign, construct, install, and maintain large-scale processing systems and data pipelines.\nDevelop data set processes for data modeling, mining, and production.\nAnalyze and organize data to provide insights and support business decisions.\nCollaborate with data scientists and other stakeholders to understand data needs and deliver solutions.\nEnsure data quality and integrity by implementing data validation checks and troubleshooting issues.\nOptimize and improve data delivery architecture and processes for efficiency.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\nStrong proficiency in programming languages such as Python, Java, or Scala.\nExperience with big data technologies such as Hadoop, Spark, or Kafka.\nIn-depth knowledge of SQL and NoSQL databases like MySQL, PostgreSQL, MongoDB, or Cassandra.\nFamiliarity with data warehousing solutions such as Amazon Redshift, Google BigQuery, or Snowflake.\nUnderstanding of ETL (Extract, Transform, Load) processes and tools.\nAbility to work with cloud platforms like AWS, Azure, or Google Cloud Platform.","Nosql, Hadoop, Data Modeling, Spark, Kafka, Data Warehousing, Python, Sql, Etl, AWS"
Data Engineer,Robotics Technologies,3-7 Years,INR 18 - 54.5 LPA,"Noida, Delhi NCR, Pune","Information Technology, Sales Automation, Information and Communications Technology (ICT), Reputation","Description\nWe are seeking a skilled Data Engineer to join our team in India. The ideal candidate will be responsible for designing and maintaining data systems that support our organization's data needs. You will work with various stakeholders to ensure data is collected, stored, and processed efficiently.\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and ETL processes.\nCollaborate with data scientists and analysts to understand data requirements and ensure data quality.\nImplement data models and maintain data architecture to support analytics and reporting needs.\nMonitor and optimize data performance and data storage solutions.\nEnsure data security and compliance with relevant regulations.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n3-7 years of experience in data engineering or a related field.\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nExperience with big data technologies (e.g., Hadoop, Spark) and data warehousing solutions (e.g., Redshift, Snowflake).\nStrong programming skills in Python, Java, or Scala.\nExperience with data pipeline orchestration tools (e.g., Apache Airflow, Luigi).\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services.","Nosql, Hadoop, Data Modeling, Cloud Services, Spark, Apache Kafka, Data Warehousing, Python, Sql, Etl"
Data Engineer,G Jayaprabu ( Proprietor of JP Professional Recruiter & Services),3-6 Years,INR 8 - 9 LPA,Chennai,IT Management,"We are hiring Data Engineer\nIndustry Type- US Based IT Company\nLocation - Guindy, Chennai,\nExperience - 36 years\nSalary - 9LPA\nResponsibilities:\n* Build and maintain scalable data pipelines and ETL processes for BI needs.\n* Optimize data models and ensure clean, reliable, and well-structured data for Power BI.\n* Integrate data from various internal systems (web apps, databases, accounting platforms).\n* Collaborate with BI analysts and managers to meet reporting requirements.\n* Strong SQL development and performance tuning skills.\n* Hands-on experience with Power BI dataset structuring and integration.\n* Familiarity with Python for ETL, automation, or data cleaning tasks.\n* Knowledge of APIs and scripting for data ingestion\nSuitable candidates can contact\nWhatsApp 9042537746","Power Bi, Data Engineer, Sql, Etl"
Senior Data Engineer,Anblicks Solutions,5-9 Years,,Ahmedabad,Cloud Data Services,"Key Responsibilities\nDesign and develophigh-performance, scalable data pipelinesusingPySpark, SQL, and cloud platforms.\nOptimizedistributed data processingfor improved efficiency and reliability.\nImplementbest practices in data modeling, architecture, and governance.\nEnsuredata integrity, security, and compliancewithin cloud environments.\nAutomatedata workflows, monitor pipeline performance, and troubleshoot issues proactively.\nProvidetechnical leadership and mentorshipto junior engineers, conduct code reviews, and establish best practices.\nRequired Skills & Experience\n4+ years of hands-on experienceinData Engineeringwith expertise inPython, PySpark,Spark, SQL, and Cloud platforms.\nStrong proficiency inSnowflake, Databricks, or Microsoft Fabric.\nSolid experience indata architecture, modeling, and pipeline optimization.\nHands-on experience withworkflow orchestration tools(e.g.,Airflow, DBT).\nProven ability tomentor and leadjunior engineers while driving best practices in data engineering.","Modeling, python, data engineering, databricks, Pyspark, Data Architecture"
Data Engineer II,Tekion,3-5 Years,,Chennai,Software,"Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.\nShould be able to design (high level / low level) software solutions for the new requirements.\nCoding independently and with other team members with proper software industry standard best practices.\nCollaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.\nQualifications:\nB.E/MTech in computer science\n3 - 5 yearsof relevant work experience.\nExperience in building scalable products with preferably big data.\nExcellentPythoncoding skills (Mandatory)\nExperience inApache spark, Data Lakeand other Big data technologies.\nExperience in either Data Warehouses or Relational Database is mandatory.\nExperience inAWScloud\nMandatory Skills: Python , Spark","Spark, Data Lake, Apache Spark, Data Engineer, Python, Aws Cloud"
GCP Data Engineer,IDESLABS,4-8 Years,,Pune,"Recruiting, Staffing Agency","Job description\nTo ensure successful initiation, planning, execution, control and completion of the project by guiding team members on technical aspects, conducting reviews of technical documents and artefacts\nLead project development, production support and maintenance activities\nFill and ensure timesheets are completed, as is the invoicing process, on or before the deadline\nLead the customer interface for the project on an everyday basis, proactively addressing any issues before they are escalated\nCreate functional and technical specification documents\nTrack open tickets/ incidents in queue and allocate tickets to resources and ensure that the tickets are closed within the deadlines\nEnsure analysts adhere to SLA s/KPI s/OLA s\nEnsure that all in the delivery team, including self, are constantly thinking of ways to do things faster, better or in a more economic manner\nLead and ensure project is in compliance with Software Quality Processes and within timelines\nReview functional and technical specification documents\nServe as the single point of contact for the team to the project stakeholders\nPromote team work, motivate, mentor and develop subordinates\nProvide application production support as per process/RACI (Responsible, Accountable, Consulted and Informed) Matrix","Project Managment, Crm, Qa"
Data Engineer,FCS Software Solutions,6-11 Years,,Delhi,Information Technology,"Description:\nSeeking someone who has experience/knowledge in manufacturing. SQL, Power BI and DAX calculations are a must for skillsets for this position. Candidates do not require to have a degree, as long as they have the required skillset/experiences from the job description. We are looking for a Data Engineer with MFG experience for a staff augmentation role.\nThese are the requirements for the role:\nExpert in Azure Data Factory\nProven experience in Data Modelling for Manufacturing data sources.\nProficient SQL design\n+5 years of experience in Data engineering roles\nProve experience in PBI: Dashboarding, DAX calculations, Star scheme development and semantic model building\nManufacturing knowledge\nExperience with GE PPA as data source is desirable\nAPI dev Knowledge\nPython skills","DAX Calculations, Azure Data Factory, Power Bi, Sql, Python"
Data Engineer,Alight,8-13 Years,,Gurugram,Information Technology,"Our story\nAt Alight, we believe a company s success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to Be Alight.\nOur Values:\nChampion People- be empathetic and help create a place where everyone belongs.\nGrow with purpose -Be inspired by our higher calling of improving lives.\nBe Alight -act with integrity, be real and empower others.\nIt s why we re so driven to connect passion with purpose. Alight helps clients gain a benefits advantage while building a healthy and financially secure workforce by unifying the benefits ecosystem across health, wealth, wellbeing, absence management and navigation.\nWith a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.\nJoin our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.\nLearn more atcareers.alight.com.\nAlight is seeking a skilled and passionate Data Engineer to join our team. As the Data Engineer, you will be a member of a team responsible for various stages of data pipelines development, including understanding business requirements, coding, testing, documentation, deployment, and production support. As part of the Data Analytics team, you will focus on delivering high-quality enterprise caliber systems on Alight s Data Lake. Your primary role will involve participating in full life-cycle data ingestion and data architecting development projects.\nQualifications:\nKnowledge & Experience:\n5+ years of data integration, data warehousing or data conversion experience.\n3+ years of data modeling experience.\n3+ years of Informatica IICS experience.\n3+ years working with AWS Redshift.\n2+ years AWS step functions, Lambda, Glue, and other Cloud Data Lake Services\n1+ SSIS / SSRS experience\nExcellent analytical and critical thinking skills\nStrong interpersonal skills with the ability to work effectively with diverse and remote teams\nExperience in agile processes and development task estimation\nStrong sense of responsibility for deliverables\nAbility to work in a small team with moderate supervision\nResponsibility Areas:\nDesign data ingestion solutions for small to medium complexity requirements independently, adhering to existing standards\nDevelop high-priority and highly complex solutions for systems based on functional specifications, detailed design, maintainability, and coding and efficiency standards, working independently.\nEstimate and evaluate risks, and prioritize technical tasks based on requirements\nCollaborate actively with Data Engineering Lead, Product Owners, Quality Assurance, and stakeholders to ensure high-quality project delivery\nConduct formal code reviews to ensure compliance with standards\nUtilize appropriately system design, development, and process standards\nCreate, maintain, and publish system-level documentation, including system diagrams, with minimal guidance\nEnsure clarity, conciseness, and completeness of requirements before starting development, collaborating with Business Analysts and stakeholders to evaluate feasibility. Take primary accountability for meeting non-functional requirements.\nAlight requires all virtual interviews to be conducted on video.\nFlexible Working\nSo that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and Top 100 Company for Remote Jobs 5 years in a row.\nBenefits\nWe offer programs and plans for a healthy mind, body, wallet and life because it s important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as several voluntary benefit options.\nBy applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight s employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.\nOur commitment to Inclusion\nWe celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.\nAt Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.\nAuthorization to work in the Employing Country\nApplicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.\nNote, this job description does not restrict managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\nWe offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.\nDISCLAIMER:\nNothing in this job description restricts managements right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\n.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Manager Quality Assurance, Analytical, Data Conversion, Production Support, Coding, Data Modeling, Ssrs, Informatica, Project Delivery, SSIS"
Gcp Data Engineer,Tredence Analytics Solutions Private Limited,6-11 Years,,Bengaluru,Software,"Role:- GCP Data Engineer/GCP Architect\nExperience:-6+ Years\nLocation :- Pan India\nRole & responsibilities\nCloud Skills:\nStrong experience with Google Cloud Platform (GCP), particularly BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Functions, and Cloud Composer.\nUnderstanding of cloud architecture and scalability best practices.\nProgramming & Scripting:\nProficiency in Python, SQL, Java, or Scala for data manipulation and pipeline development.\nKnowledge of shell scripting and automation tools.\nData Engineering:\nExperience in designing and implementing ETL/ELT workflows and data pipelines.\nFamiliarity with data modeling, schema design, and optimization techniques.\nBig Data & Analytics:\nExperience with distributed computing and big data tools (Apache Hadoop, Spark, or similar frameworks).\nProficient in querying and managing large datasets in BigQuery.\nCollaboration Tools:\nExperience with tools like Git, JIRA, or similar for version control and project management.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication skills to interact with technical and non-technical teams.\nPreferred Qualifications:\nExperience in machine learning or data science applications.\nCertification in Google Cloud Professional Data Engineer or equivalent.\nKnowledge of data visualization tools like Google Data Studio, Tableau, or Power BI.","Gcp Data Engineer, Git, JIRA, ELT"
Data Engineer - MDM/PIM/Syndigo/Attacama/Informatica,Reflections Info Systems,6-11 Years,,"Chennai, Pune",IT Management,"Responsibilities include:\nWork as part of a team to develop Data and Analytics solutions.\nParticipate in the development of cloud data warehouses, data as a service, business intelligence solutions\nAbility to provide solutions that are forward-thinking in data integration.\nDeliver a quality product.\nDeveloping Modern Data Warehouse solutions using Azure or AWS Stack\nCertifications :\nBachelor s degree in computer science & engineering or equivalent demonstrable experience\nDesirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.\nPrimary Skills :\n6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions\nProgramming experience in Scala or Python, SQL\nMin 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks\nWorking knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse\nDemonstrated analytical and problem-solving skills\nExcellent written and verbal skills (English)\nSecondary Skills :\nFamiliar with Agile Practices\nFamiliar with Version control platforms GIT, CodeCommit etc.\nProblem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.\nOwnership\nProactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
GCP Data Engineer,Nihilent,5-7 Years,,Pune,Information Technology,"Unnesting of JSON files in Google BigQuery\nGoogle Cloud Run (using container services, python)\nHands on experience in GCP services like composer, cloud function, cloud run\nExperience in developing CI/CD pipelines.\nExperience in DBT scripts, docker files and knowledge on datavault is a plus.\nStrong technical knowledge and hands on experience of python or java\nRole: Data Engineer\nIndustry Type: IT Services & Consulting\nDepartment: Data Science & Analytics\nEmployment Type: Full Time, Permanent\nRole Category: Data Science & Machine Learning\nEducation\nUG: Any Graduate\nPG: Any Postgraduate","Gcp, Cloud, Json, Python"
Data Engineer Architect,TechnoGen,10-19 Years,,Hyderabad,"Consulting, Information Services","Basic Qualifications\nBachelors degree in computer science, engineering or a related field\nData: 8+ years of experience with data analytics and warehousing inInvestment& Finance Domain\nSQL: Deep knowledge of SQL and query optimization\nELT: Good understanding of ELT methodologies and tools\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers\n3+ years of coding and scripting (Python, Java, Scala) and design experience.","Java, Scala, Data Analytics, Sql, Python, data engineering, Data Architecture"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Delhi, Bengaluru, Chennai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS.\nThis will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives.\nCollaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Data Processing, Cloud Computing, data engineering, Machine Learning, SSIS, Python, AWS"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,Pune,Software,"Key Responsibilities:\n- Design, develop, and maintain data pipelines using Python, SQL, and Kedro\n- Implement serverless solutions using AWS Lambda and Step Functions\n- Develop and manage data workflows in Azure and AWS cloud environments\n- Create integrations between data systems and Power Platform (Power Apps, Power Automate)\n- Design, develop, and maintain APIs for data exchange and integration\n- Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\n- Optimize data storage and retrieval processes for improved performance\n- Collaborate with cross-functional teams to understand data requirements and provide solutions\n- API Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","data engineering, Database Design, Cloud Services, Data Extraction, Azure, Sql, Python"
Data Engineer,Testingxperts Inc,0-3 Years,,Chandigarh,Information Technology,"Job Details\nData Engineer with 0-1 years of experience in Power BI, sql, Python.\nIt is 6 Months internship post to that fulltime, All 2years bond .\nCertification in relevant field best to have\nJob Type: Full-time\nFixed shift: Monday to Friday\nMorning shift\nWork Location: In person,","Powerbi, Power Bi, Python, Sql"
Associate Data Engineer- GET,XenonStack,0-1 Years,,Mohali,Information Technology,"Job Responsibilities -\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with coworkers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analyzed quickly by BI AI Teams .\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nTechnical Requirements-\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch , Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores .\nProfessional Attributes-\nExcellent communication skills\nAttention to detail\nAnalytical mind and Problem Solving Aptitude\nStrong Organizational skills","Consulting, Analytical, Architectural Design, Machine Learning, Big Data Analytics"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS.\nThis will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives.\nCollaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Data Processing, Cloud Computing, data engineering, Machine Learning, SSIS, Python, AWS"
Data Engineer II,Tekion,3-5 Years,,Chennai,Software,"Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.\nShould be able to design (high level / low level) software solutions for the new requirements.\nCoding independently and with other team members with proper software industry standard best practices.\nCollaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.\nQualifications:\nB.E/MTech in computer science\n3 - 5 yearsof relevant work experience.\nExperience in building scalable products with preferably big data.\nExcellentPythoncoding skills (Mandatory)\nExperience inApache spark, Data Lakeand other Big data technologies.\nExperience in either Data Warehouses or Relational Database is mandatory.\nExperience inAWScloud\nMandatory Skills: Python , Spark","Spark, Data Lake, Apache Spark, Data Engineer, Python, Aws Cloud"
Aws Data Engineer,Coditas Technologies,3-7 Years,,Pune,Software,"We are looking for data engineers who have the right attitude, aptitude, skills, empathy,\ncompassion, and hunger for learning. Build products in the data analytics space. A passion\nfor shipping high-quality data products, interest in the data products space; curiosity about\nthe bigger picture of building a company, product development and its people.\nRoles and Responsibilities\nDevelop and manage robust ETL pipelines using Apache Spark (Scala)\nUnderstand park concepts, performance optimization techniques and governance\ntools\nDevelop a highly scalable, reliable, and high-performance data processing pipeline to\nextract, transform and load data from various systems to the Enterprise Data\nWarehouse/Data Lake/Data Mesh\nCollaborate cross-functionally to design effective data solutions\nImplement data workflows utilizing AWS Step Functions for efficient orchestration.\nLeverage AWS Glue and Crawler for seamless data cataloging and automation\nMonitor, troubleshoot, and optimize pipeline performance and data quality\nMaintain high coding standards and produce thorough documentation. Contribute to\nhigh-level (HLD) and low-level (LLD) design discussions\nTechnical Skills\nMinimum 3 years of progressive experience building solutions in Big Data\nenvironments.\nHave a strong ability to build robust and resilient data pipelines which are scalable,\nfault tolerant and reliable in terms of data movement.\n3+ years of hands-on expertise in Python, Spark and Kafka.\nStrong command of AWS services like EMR, Redshift, Step Functions, AWS Glue, and\nAWS Crawler.\nStrong hands on capabilities on SQL and NoSQL technologies.\nSound understanding of data warehousing, modeling, and ETL concepts\nFamiliarity with High-Level Design (HLD) and Low-Level Design (LLD) principles\nExcellent written and verbal communication skills.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate","Time Management, Backend, Analytical, Business Strategy, Data Processing, Actuarial, Big Data, Data Analytics, Apache, Sql"
Gcp Data Engineer,Tredence Analytics Solutions Private Limited,6-11 Years,,Bengaluru,Software,"Role:- GCP Data Engineer/GCP Architect\nExperience:-6+ Years\nLocation :- Pan India\nRole & responsibilities\nCloud Skills:\nStrong experience with Google Cloud Platform (GCP), particularly BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Functions, and Cloud Composer.\nUnderstanding of cloud architecture and scalability best practices.\nProgramming & Scripting:\nProficiency in Python, SQL, Java, or Scala for data manipulation and pipeline development.\nKnowledge of shell scripting and automation tools.\nData Engineering:\nExperience in designing and implementing ETL/ELT workflows and data pipelines.\nFamiliarity with data modeling, schema design, and optimization techniques.\nBig Data & Analytics:\nExperience with distributed computing and big data tools (Apache Hadoop, Spark, or similar frameworks).\nProficient in querying and managing large datasets in BigQuery.\nCollaboration Tools:\nExperience with tools like Git, JIRA, or similar for version control and project management.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication skills to interact with technical and non-technical teams.\nPreferred Qualifications:\nExperience in machine learning or data science applications.\nCertification in Google Cloud Professional Data Engineer or equivalent.\nKnowledge of data visualization tools like Google Data Studio, Tableau, or Power BI.","Gcp Data Engineer, Git, JIRA, ELT"
Data Engineer,FCS Software Solutions,6-11 Years,,Delhi,Information Technology,"Description:\nSeeking someone who has experience/knowledge in manufacturing. SQL, Power BI and DAX calculations are a must for skillsets for this position. Candidates do not require to have a degree, as long as they have the required skillset/experiences from the job description. We are looking for a Data Engineer with MFG experience for a staff augmentation role.\nThese are the requirements for the role:\nExpert in Azure Data Factory\nProven experience in Data Modelling for Manufacturing data sources.\nProficient SQL design\n+5 years of experience in Data engineering roles\nProve experience in PBI: Dashboarding, DAX calculations, Star scheme development and semantic model building\nManufacturing knowledge\nExperience with GE PPA as data source is desirable\nAPI dev Knowledge\nPython skills","DAX Calculations, Azure Data Factory, Power Bi, Sql, Python"
Data Engineer - MDM/PIM/Syndigo/Attacama/Informatica,Reflections Info Systems,6-11 Years,,"Chennai, Pune",IT Management,"Responsibilities include:\nWork as part of a team to develop Data and Analytics solutions.\nParticipate in the development of cloud data warehouses, data as a service, business intelligence solutions\nAbility to provide solutions that are forward-thinking in data integration.\nDeliver a quality product.\nDeveloping Modern Data Warehouse solutions using Azure or AWS Stack\nCertifications :\nBachelor s degree in computer science & engineering or equivalent demonstrable experience\nDesirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.\nPrimary Skills :\n6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions\nProgramming experience in Scala or Python, SQL\nMin 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks\nWorking knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse\nDemonstrated analytical and problem-solving skills\nExcellent written and verbal skills (English)\nSecondary Skills :\nFamiliar with Agile Practices\nFamiliar with Version control platforms GIT, CodeCommit etc.\nProblem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.\nOwnership\nProactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
Sr. Data Engineer,Anblicks Solutions,8-13 Years,,Ahmedabad,Cloud Data Services,"Designing Data Models: Utilize Kimball data modeling techniques and Data Vault 2.0 methodologies to create and maintain robust data models.\nData Integration: Use Fivetran for seamless data extraction and loading, and Azure Data Factory (ADF) for orchestrating data workflows.\nData Warehousing: Manage and optimize data storage in Snowflake, ensuring efficient data retrieval and processing.\nData Transformation: Develop and maintain transformation scripts using DBT (Data Build Tool) to convert raw data into actionable insights.\nMaintaining Data Documentation: Ensure all data processes and models are well-documented and easily understandable.\nCommunicating Results: Present data insights to stakeholders through visual representations and reports.\nCollaborating: Work closely with data analysts, data engineers, and business executives to align data strategies with business goals.\nSkills Required\nTechnical Skills: Proficiency in SQL, Python, and data visualization tools like Tableau or Power BI.\nAnalytical Skills: Ability to interpret complex data and provide actionable insights.\nCommunication Skills: Strong ability to explain technical concepts to non-technical stakeholders.\nProblem-Solving: Aptitude for identifying issues within data and developing solutions.","Problem Solving, Stakeholder Management, Data Visualisation, Sql, Python, Scripting"
Azure Data Engineer,SRS Infoway,6-11 Years,,"Ahmedabad, Bengaluru, Chennai",Information Technology,"Looking for a Data Engineer with 6+ years of experience in Azure, including ADF, Azure Databricks, Python, and PySpark. Strong expertise in designing, developing, and implementing data pipelines in a cloud-based environment.","Azure Data Factory, Pyspark, Azure Databricks, Python, Sql"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Bengaluru, Chennai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Cloud Computing, Artificial Intelligence, Pytorch, Big Data Technologies, Aws, Tensorflow"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Key Responsibilities:\n- Design, develop, and maintain data pipelines using Python, SQL, and Kedro\n- Implement serverless solutions using AWS Lambda and Step Functions\n- Develop and manage data workflows in Azure and AWS cloud environments\n- Create integrations between data systems and Power Platform (Power Apps, Power Automate)\n- Design, develop, and maintain APIs for data exchange and integration\n- Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\n- Optimize data storage and retrieval processes for improved performance\n- Collaborate with cross-functional teams to understand data requirements and provide solutions\n- API Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Database Design, data engineering, Cloud Services, Data Extraction, Azure, Sql, Python"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Delhi, Hyderabad",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Cloud Computing, Artificial Intelligence, Pytorch, Big Data Technologies, Aws, Tensorflow"
Azure Data Engineer,Future Focus Infotech,5-10 Years,,Hyderabad,Information Technology,"Greetings from Future Focus Infotech!!!\nWe have multiple opportunities Azure Data Engineer (F2F interview on 17th May (Saturday)\nExp: 5+yrs\nLocation : Hyderabad\nJob Type- This is a Permanent position with Future Focus Infotech Pvt Ltd & you will be deputed with our client.\nA small glimpse about Future Focus Infotech Pvt Ltd. (Company URL: www.focusinfotech.com)\nIf you are interested in above opportunity, send updated CV and below information to [HIDDEN TEXT]","Azure Data Factory, Databricks, Azure"
Data Engineer-Microsoft Azure Databricks,Dynpro,12-14 Years,,"Hyderabad, Bengaluru, Chennai",Information Technology,"Job Description\nAs a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data projects and collaborating with cross-functional teams to drive data initiatives.\nRoles & Responsibilities:\nServe as a Subject Matter Expert (SME) and manage the team to deliver results.\nTake responsibility for team-level decisions and performance.\nCollaborate with multiple teams and contribute to strategic technical decisions.\nProvide scalable and effective solutions for both team-specific and cross-team challenges.\nLead the design and development of data solutions and infrastructure.\nImplement and optimize data pipelines for efficient and scalable data processing.\nEnsure data quality, consistency, and integrity throughout the data lifecycle.\nPartner with stakeholders to gather requirements and deliver data-driven solutions.\nProfessional & Technical Skills:\nMust-Have Skills:\nProficiency inMicrosoft Azure Databricks\nStrong understanding ofcloud-based data engineeringconcepts\nExperience withbig data technologiessuch asHadoopandSpark\nSolid knowledge ofdata modelinganddatabase designprinciples\nHands-on experience withSQLandNoSQLdatabases.\nExperience Required:\nMinimum of5 yearsof experience working withMicrosoft Azure Data bricks\nEducational Qualifications:\n15 years of full-time education\nBachelors or Masters degree inComputer Science, Engineering, Data Science, or a related field\nAdditional Information:\nThis role isbased in our Mumbai office\nCandidates should be comfortable working in acollaborative, fast-paced environment","Sme, Nosql, Azure, Sql"
Sr. Data Engineer,MITS Solution Private Limited,6-10 Years,INR 20 - 22.5 LPA,"Noida, Bengaluru, Pune",Information Technology,"Job description\nJob Position: Sr Data Engineer\nExperience: 6+ years\nImmediate joiners\nMode: Hybrid (3 days a week),\nLocation: Bangalore, Pune, New Mumbai, Noida, Chennai, Hyderabad, Chennai, Bhopal\nJob description:\nData engineering development & testing with experience in Spark, Scala, and SQL.\nExperienced big data developer with hands on Spark & Scala development skills.Ideal candidate would be a big data engineer passionate about data, data modelling, data enrichment for deriving deep insights accessible via Visualization dashboards or third party tools.\nMust have skills\nMust be a Big Data engineer and passionate about data.\nHands on Hive, Scala, Druid experience plus optionally Python\nHands on Experience in big data lake house like Databricks\nWorking knowledge and hands-on experience in known Big Data platforms and Hadoop tools & technologies\nExperience with extraction, enrichment, and export large volumes data.\nExperience in map-reduce and well-known big data algorithms.\nDatabase development experience with a solid understanding of core database concepts, relational database design ODS and DWH\nPeer reviews, Unit testing, deployment to production working with several teams to deliver big data solutions.\nExperience in Finance big data projects will be a plus\nRole:DBA / Data warehousing - Other\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:DBA / Data warehousing\nEducation\nUG:B.Tech/B.E. in Computers","Hive, Hadoop, Scala, Big Data"
Data Engineer,IDESLABS,5-7 Years,,Pune,"Recruiting, Staffing Agency","Job description\n5+ years of relevant experience in the field of Data Engineering\nAdvance skills in big data technologies like Hadoop, Python, Spark, SQL.\nMust have experience building data APIs.\nBachelor s in Computer Science or related disciplines\nKnowledge of Data Structures and Algorithm.\nStrong Python programming skills with ability to implement OOPs and functional programming. Knowledge of Scala/Java would be plus.\nStrong knowledge on RDBMS and NoSQL databases with the ability to implement them from scratch. Knowledge of Graph databases will be a plus.\nStrong expertise in building & optimizing data pipelines, architectures, and data sets.\nExperience working with different file formats like Parquet, ORC, Avro, RC, etc.\nExperience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc.\nKnowledge and experience of using orchestration frameworks like Airflow, Oozie, Luigi, etc.\nExperience using Spark, and building jobs using Python/Scala/Java.\nExperience or Knowledge building stream processing platforms using Spark Streaming, Storm, etc. Knowledge of Kafka/Flink+Beam would be plus.\nKnowledge of building REST API end points for data consumption.\nExperience in building scalable data pipelines for both real time and batch using best practices in data modeling, ETL/ELT processes utilizing varioud technologies such as Spark, Kafka, Presto, SAP HANA, Airflow, informatica.\nPerform Data analysis using Python, complex SQLs, and other tools.\nPerform root cause analysis of issues from platform standpoint on Kubernetes, Containers, Hadoop, Spark, Hive, Presto\nExcellent oral and written communication is a must.\nPreferred\nMasters in Computer Science or related disciplines\nExperience building self-service tools for analytics would be plus.\nKnowledge of ELK stack would be a plus.\nKnowledge of implementing CI/CD on the pipelines is a plus.\nKnowledge of Containerization (Docker/Kubernetes) will be plus.\nExperience working with one of the popular Public Cloud based platforms is preferred","Airflow, Java, Python, Docker, Presto, Sap Hana, Spark, Kafka"
Azure Data Engineer,SRS Infoway,6-11 Years,,"Delhi NCR, Pune",Information Technology,"Looking for a Data Engineer with 6+ years of experience in Azure, including ADF, Azure Databricks, Python, and PySpark. Strong expertise in designing, developing, and implementing data pipelines in a cloud-based environment.","Azure Data Factory, Pyspark, Azure Databricks, Python, Sql"
Azure Data Engineer,SRS Infoway,6-11 Years,,"Ahmedabad, Bengaluru, Chennai",Information Technology,"Looking for a Data Engineer with 6+ years of experience in Azure, including ADF, Azure Databricks, Python, and PySpark. Strong expertise in designing, developing, and implementing data pipelines in a cloud-based environment.","Azure Data Factory, Pyspark, Azure Databricks, Python, Sql"
Lead Data Engineer,NXP Semiconductors,7-12 Years,,Bengaluru,Semiconductor,"Proven experience as a Data Engineer\nHands on experience in ETL design and development concepts (7+ years)\nExperience with AWS and Azure cloud platforms and their data service offerings\nProficiency in SQL, PySpark, Python\nExperience with GitHub, GitLab, CI/CD\nKnowledge of advanced analytic concepts including AI/ML\nStrong problem-solving skills and ability to work in a fast-paced and collaborative environment\nExcellent oral and written communication skills\nPreferred Skills Qualifications:\nExperience with Agile / DevOps\nProficiency in SQL (Databricks, Teradata)\nExperience with DBT\nExperience with Dataiku platform, including administration","data engineering, Azure, Sql, Python, Etl, AWS"
Lead Data Engineer-Databricks,Anblicks Solutions,10-14 Years,,"Ahmedabad, Hyderabad",Cloud Data Services,"Role & responsibilities\nLead Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise inAzure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake)along with big data processing, real-time analytics, and cloud data integration and Team Leading Experience.\nKey Responsibilities:\n1. Data Pipeline Development & ETL/ELT\nDesign and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.\nImplement ETL/ELT workflows for structured and unstructured data processing.\nOptimize data ingestion, transformation, and storage strategies.\n2. Cloud Data Architecture & Integration\nDevelop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).\nWork with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.\n3. Database Management & Optimization\nDesign and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).\nOptimize SQL queries and indexing strategies for performance.\nImplement data partitioning, compression, and caching for efficiency.\n4. Data Governance, Security & Compliance\nEnsure data quality, lineage, and governance with tools like Purview.\nImplement role-based access control (RBAC), encryption, and security policies.\nEnsure compliance with GDPR, HIPAA, and ISO 27001 regulations.\n5. Monitoring & Performance Tuning\nUse Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.\nAutomate data pipeline testing and validation.\n6. Collaboration & Documentation\nDocument data models, pipeline architectures, and data workflows.\nImmediate joiners are preferred.","data models, log analytics, Data Processing, Azure, Database Management, elt"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Bengaluru,Software,"Position Summary:\nWe are looking for candidates with hands on experience in Big Data or Cloud Technologies.\nMust have technical Skills\n7 to 10 Years of experience\nExpertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have\nGood and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of PySpark (SparkSQL) Must Have\nGood knowledge of Shell script & Python Good to Have\nGood knowledge of SQL Good to Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nGood Communication Skills Must Have\n*Data Ingestion, Processing and Orchestration knowledge\nRoles & Responsibilities\nLead technical implementation of Data Warehouse modernization projects for Impetus\nDesign and development of applications on Cloud technologies\nLead technical discussions with internal & external stakeholders\nResolve technical issues for team\nEnsure that team completes all tasks & activities as planned\nCode Development\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Aviation, Information Technology, Computers\nPG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Pune,Software,"Position Summary:\nWe are looking for candidates with hands on experience in Big Data or Cloud Technologies.\nMust have technical Skills\n7 to 10 Years of experience\nExpertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have\nGood and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of PySpark (SparkSQL) Must Have\nGood knowledge of Shell script & Python Good to Have\nGood knowledge of SQL Good to Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nGood Communication Skills Must Have\n*Data Ingestion, Processing and Orchestration knowledge\nRoles & Responsibilities\nLead technical implementation of Data Warehouse modernization projects for Impetus\nDesign and development of applications on Cloud technologies\nLead technical discussions with internal & external stakeholders\nResolve technical issues for team\nEnsure that team completes all tasks & activities as planned\nCode Development\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Aviation, Information Technology, Computers\nPG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer - Azure Fabric,Kanini Software Solutions,8-10 Years,,"Bengaluru, Chennai, Pune",Software,"We are looking for a Lead Data Engineer with expertise inAzure Fabric, Data Architecture, and ETL Pipelines. The ideal candidate will design and implement scalable data solutions, ensuring efficient data processing, governance, and analytics on the Azure cloud platform.\nKey Responsibilities:\nDesign & develop data pipelines using Azure Fabric, Data Factory, and Synapse Analytics.\nImplement ETL/ELT workflows for structured & unstructured data processing.\nOptimize SQL queries, data modeling, and performance tuning.\nEnsure data security, governance, and compliance using best practices.\nCollaborate with cross-functional teams for data-driven insights and analytics.\nRequired Skills:\nAzure Fabric, Azure Synapse, Azure Data Factory\nETL, SQL, Data Pipeline Design, Data Architecture\nBig Data & Analytics, Data Governance, Performance Optimization\nCI/CD, Azure DevOps, Git\nRole:Data Science & Analytics - Other\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Analytics - Other\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate","Architecture, Fabric, data engineering, Data Pipeline, Azure, Sql, Etl"
Lead Data Engineer,Coditas Technologies,7-11 Years,,Pune,Software,"We are looking for a Tech Lead expertise in Advanced Big Data Technology Stack. The person should be a hands-on technical expert in building and deploying applications using Big Data technologies. The person should have progressive experience in building highly scalable distributed systems. The person should have the ability to build a high-performance strong technical team that adheres to the strong quality standard of application development.\nRoles and responsibilities\nWith over 7 yrs. of hands-on experience with Data Technologies\nRequirement analysis and assess the technical feasibility of proposed solutions.\nAct as the technical specialist in designing and recommending architecture for application development/feature development\nWorking on designing the application architecture and estimating effort in delivering features for new requirements\nImplement data ingestion and transform pipeline for analytics and Dashboard reporting for business\nWork in designing large-scale distributed computing applications using tools like Spark, Kafka, Hive, etc.\nPerforming validations, reconciliation, and consolidations for the imported data, Data migration, and data generation.\nEffectively communicate with the Engineering Managers and Stakeholders to set the right expectations.\nTechnical Skills\nMinimum 5 years of progressive experience building solutions in Big Data environments\nHave a strong ability to build robust and resilient data pipelines that are scalable, fault-tolerant, and reliable in terms of data movement\nHands-on experience of Apache Spark with Python for batch and stream processing\nShould know experience in batch and stream data processing\nExposure to working on projects across multiple domains\nHands-on experience in Apache Kafka\nStrong hands-on capabilities in SQL and NoSQL technologies\nHands-on experience with AWS services like S3, DMS, Redshift, Glue, Lambda, Kinesis, MSK, etc. is must have or similar services of either Azure/GCP\nStrong analytical/quantitative skills and comfortable working with very large sets of data.\nExcellent written and verbal communication skills\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate","Airflow, Data Ingestion, Data Pipeline, Data Modeling, Pyspark, Data Architecture, Data Warehousing, Data Governance, Python, Sql, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Gurugram,Software,"Position Summary:\nWe are looking for candidates with hands on experience in Big Data or Cloud Technologies.\nMust have technical Skills\n7 to 10 Years of experience\nExpertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have\nGood and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of PySpark (SparkSQL) Must Have\nGood knowledge of Shell script & Python Good to Have\nGood knowledge of SQL Good to Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nGood Communication Skills Must Have\n*Data Ingestion, Processing and Orchestration knowledge\nRoles & Responsibilities\nLead technical implementation of Data Warehouse modernization projects for Impetus\nDesign and development of applications on Cloud technologies\nLead technical discussions with internal & external stakeholders\nResolve technical issues for team\nEnsure that team completes all tasks & activities as planned\nCode Development\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Aviation, Information Technology, Computers\nPG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Data Engineer - Azure,MITS Solution Private Limited,5-9 Years,INR 15 - 18 LPA,"Chennai, Bengaluru, Pune",Information Technology,"Job description\nJD:\nIf you are an extraordinary developer and who loves to push the boundaries to solve complex business problems using creative solutions, then we wish to talk with you. As a Data Engineer -Azure, you will work in the Technology team that helps deliver our Data Engineering offerings at large scale to our Fortune clients worldwide. The role is responsible for innovating, building and maintaining technology services.\nRESPONSIBILITIES:\nBe an integral part of large scale client business development and delivery engagements\nDevelop the software and systems needed for end-to-end execution on large projects\nWork across all phases of SDLC, and use Software Engineering principles to build scaled solutions\nBuild the knowledge base required to deliver increasingly complex technology projects\nTeam handling, problem solving, project management and communication skills & creative thinking.\nWork Mode:Hybrid work mode.\nskills-CICD, python, Pyspark, Data Engineer Azure\nQUALIFICATIONS:\nA bachelors degree in Computer Science or related field with 6-10 years of technology experience\nStrong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space\nSoftware development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks\nDatabase programming using any flavours of SQL\nExpertise in relational and dimensional modelling, including big data technologies\nExposure across all the SDLC process, including testing and deployment\nExpertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.\nGood knowledge of Python and Spark are required\nGood understanding of how to enable analytics using cloud technology and ML Ops\nExperience in Azure Infrastructure and Azure Dev Ops will be a strong plus\nProven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)\nCharacteristics of a forward thinker and self-starter\nAbility to work with a global team of consulting professionals across multiple projects\nKnack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems\nPassion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to succeed.\nEDUCATION:\nB.E/B.Tech/M.Tech in Computer Science or related technical degree OR Equivalent.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Computers\nPG:M.Tech in Computers","Pyspark, Data Engineer"
Senior Engineer (Data Engineer),Velotio Technologies,4-6 Years,,Pune,Software,"Job description\nAbout Velotio:\nVelotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.\nDesign and build scalable data infrastructure with efficiency, reliability, and consistency to meet rapidly growing data needs\nBuild the applications required for optimal extraction, cleaning, transformation, and loading data from dis","B2B SaaS, IoT Machine Learning space, data engineering"
Data Engineer Sr Associate,TechnoGen,5-8 Years,,Hyderabad,"Consulting, Information Services","Design, build and maintain complex ELT/ETL jobs that deliver business value.\nExtract, transform and load data from various sources including databases, APIs, and flat files using IICS or Python/SQL.\nTranslate high-level business requirements into technical specs\nConduct unit testing, integration testing, and system testing of data integration solutions to ensure accuracy and quality\nIngest data from disparate sources into the data lake and data warehouse\nCleanse and enrich data and apply adequate data quality controls\nProvide technical expertise and guidance to team members on Informatica IICS/IDMC and data engineering best practices to guide the future development of company Data Platform\nDevelop re-usable tools to help streamline the delivery of new projects\nCollaborate closely with other developers and provide mentorship\nEvaluate and recommend tools, technologies, processes and reference\nArchitectures\nWork in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements\nParticipate in code reviews and ensure all solutions are lined to architectural and requirement specifications and provide feedback on code quality, design, and performance","ELT, Etl, data engineering, Data Integration, Api, Python, Sql, data quality control"
Data Engineer Sr Associate,TechnoGen,5-8 Years,,Hyderabad,"Consulting, Information Services","Python Proficiency: Strong understanding of Python, with practical coding experience\nAWS:Comprehensive knowledge of AWS services and their applications\nAirflow: creating and managing Airflow DAG scheduling.\nUnix & SQL: Solid command of Unix commands, shell scripting, and writing efficient SQL scripts\nAnalytical & Troubleshooting Skills: Exceptional ability to analyze data and resolve complex issues.\nDevelopment Tasks: Proven capability to execute a variety of development activities with efficiency\nInsurance Domain Knowledge:Familiarity with the Insurance sector is highly advantageous.\nProduction Data Management: Significant experience in managing and processing production data\nWork Schedule Flexibility:Open to working in any shift, including 24/7 support, as requireRole & responsibilities","Airflow, Core Python, Sql, Aws, data engineering, Unix"
Data Engineer - OPS,Tredence Analytics Solutions Private Limited,9-14 Years,,Bengaluru,Software,"Job Description: Manager/Tech Lead (Data Engineering)\nLocation:Bangalore\nExperience:9 to 14 years\nRole Overview:\nWe are looking for a highly skilled Manager/Tech Lead, to lead and manage a team of 15 Data Engineers. This position requires a strong technical background, exceptional team management skills, and expertise in data engineering tools and techniques. The role involves end-to-end ownership of data pipelines, integration, and delivery, with a focus on cost and project optimization.\nKey Responsibilities:\nTeam Management:\nManage and mentor a team of 15 Data Engineers, ensuring professional growth and skill enhancement.\nAllocate tasks effectively, monitor performance, and address any issues proactively.\nTechnical Leadership:\nDrive the development and optimization of data pipelines using Databricks, Data Lakes, Azure Data Factory, Delta Live Tables, Python, SQL, and PySpark.\nImplement robust data integration, data quality, and data observability frameworks.\nProject & Delivery Management:\nCollaborate daily with clients, production teams, and delivery teams to align on goals and resolve challenges.\nEnsure timely and efficient project delivery, maintaining high standards of quality and performance.\nCost & Process Optimization:\nIdentify opportunities for cost optimization in existing projects.\nStreamline workflows to improve efficiency and reduce redundancies.\nStakeholder Management:\nAct as a liaison between technical teams and stakeholders, ensuring clear communication of requirements and deliverables.\nManage expectations and provide updates on project progress and risks.\nTech Migration & Innovation:\nLead technology migration initiatives to ensure the adoption of modern tools and platforms.\nStay updated with industry trends and recommend best practices for data engineering.\nSkills & Qualifications:\nMandatory Technical Skills:\nDatabricks, Data Lakes, Azure Data Factory, Delta Live Tables, Python, SQL, PySpark.\nExpertise in building and managing scalable data pipelines and integrations.\nStrong understanding of data quality and observability frameworks.\nManagement Skills:\nProven experience in leading and managing large teams (15+ members).\nStrong project management and delivery skills.\nExcellent stakeholder management and communication abilities.\nOther Requirements:\nExperience in driving delivery for tech teams and managing daily client interactions.\nKnowledge of cost optimization and tech migration best practices.","Pyspark, Data Engineer, Sql, Python"
Senior Data Engineer,Egon Zehnder,8-13 Years,,Gurugram,Information Technology,"Job Summary\nAs a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.\nExecuting end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nThe Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.\nThe Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.\nThe Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.\nIdentifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.\nCollaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.\nExperience & Key Competencies\nEngineering Degree or equivalent.\n3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nProven experience in building ETL/ELT pipelines, preferably from SQL to Azure services\nArchitecture experience of making ETL operations such as Medallion Architecture etc.\nFamiliarity with CI/CD practices for data pipelines and version control using Git\nExperience in integrating new/replacing vendor products in existing ecosystem.\nExperience in migrating data structured, unstructured data to Data lake\nExperience or understanding of performance engineering both at system level and database level.\nExperience or understanding of Data Governance, Data Quality, Data Issue Management.\nWork closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications\nEnsure compliance with all regulations, policies, and procedures.\nEscalate issues/risks pro-actively to appropriate stakeholders.\nRegularly communicate status and challenges to team members and management.\nSelf-driven with keenness to master, suggest and work with different technologies & toolsets.\nExcellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers\nExcellent and resourceful problem-solving skills, adaptable and willingness to learn.\nGood analysis skills - to be able to join the dots across multiple applications and interfaces between them.\nSkill Set\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nExperience in Data factory and Data Lake technologies.\nRich Experience in data modelling techniques and creating various data models.\nExperience in Azure cloud services and architecture patterns.\nUnderstanding of RESTful APIs for data distribution.\nUnderstanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments\nExcellent oral and written communication with an ability to articulate complex systems to multiple teams.\nSelf-motivation and the ability to work under minimal supervision\nBenefits Highlights:\n5 Days working in a Fast-paced work environment\nWork directly with the senior management team\nReward and Recognition\nEmployee friendly policies\nPersonal development and training\nHealth Benefits, Accident Insurance\nPotential Growth for you\nWe will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.","Data Management, Data Quality, Version Control, postgresql, mysql, Sql"
Data Engineer Sr Associate,TechnoGen,6-10 Years,,Hyderabad,"Consulting, Information Services","Role & responsibilities\nBachelors degree in computer science, engineering, or a related field. Master's degree preferred.\nData: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.\nSQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.\nELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC\nMigration Experience: Experience Informatica on prem to IICS/IDMC migration\nCloud: 5+ years experience working in AWS cloud environment\nPython: 5+ years of hands-on experience of development with Python\nWorkflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem-solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers.\nReporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)\nExperience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms.","Airflow, data engineering, Aws Cloud, Tableau, Data Analytics, Apache, Sql, Python"
Data Engineer,Qentelli Solutions Private Limited,4-9 Years,,Hyderabad,"Information Technology, Information Services","Role & responsibilities\nMust Haves Skills\nExtensive experience as Data Engineer with Python Language and Cloud Technologies (AWS preferably).\nExperience in Automating ETL process/Pipelines and AWS Data & Infrastructure with Python.\nExtensive experience with AWS components like S3, Athena, EMR, Glue, Redshift, Kinesis and SageMaker.\nExtensive Experience with SQL/Unix/Linux scripting.\nDeveloping/testing Experience on Cloud/On Prem ETL Technologies (Ab Initio, AWS Glue, Informatica, Alteryx).\nExperience in Data migration from Onprem to Cloud is Plus.\nExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data Science.\nExtensive experience in DevOps/Data Ops space.\nHaving experience in Data Science platforms like SageMaker/Machine Learning Studio/ H2O is plus.\nWork Description SDET Python, AWS, Unix and ETL.\nWork with business stakeholders, Business Systems Analysts and Developers to ensure delivery of Data Applications.\nBuilding Automation Frameworks using Python.\nDesigning and managing the data workflows using Python during development and deployment of data products\nDesign, development of Reports and dashboards.\nAnalyzing and evaluating data sources, data volume, and business rules.\nShould be well versed with the Data flow and Test Strategy for Cloud/ On Prem ETL Testing.\nInterpret and analyses data from various source systems to support data integration and data reporting needs.\nExperience in testing Database Application to validate source to destination data movement and transformation.\nWork with team leads to prioritize business and information needs.\nDevelop and summarize Data Quality analysis and dashboards.\nKnowledge of Data modeling and Data warehousing concepts with emphasis on Cloud/ On Prem ETL.\nExecute testing of data analytic and data integration on time and within budget.\nTroubleshoot & determine best resolution for data issues and anomalies\nHas deep understanding of data architecture & data modeling best practices and guidelines for different data and analytic platforms","Numpy, Amazon Web Services, Pandas, Python, AWS"
Senior Azure Data Engineer,Mindera,7-12 Years,,Bengaluru,Software,Job description\nExperience in distributed computing (spark) and software development.\nExperience in spark-scala.\nExperience in Data Engineering.\nExperience in Python\nExperience of working on Cloud environments (Azure).,"Azure, Python, Sql"
Google Cloud Platform Data Engineer,Ford,8-13 Years,,Chennai,Automotive,"We are seeking a skilled and experienced GCP Data Engineer to be part of Material Logistics IT team.\nThis individual will design, develop, and maintain our data pipelines and data warehousing solutions on Google Cloud Platform for Ford's ambitious command center and logistics service layer Initiatives.\nYou will be responsible for ensuring the efficient and reliable processing and storage of large datasets and architecting, building, and maintaining our cloud infrastructure and ensuring its high availability and scalability.\nQualifications:\nBachelor s degree in computer science or a related field.\n8+ years of experience in software development, with at least 5 years on Google cloud technologies.\nExpertise in Data ingestion and storage, Data Transformation and processing, Data analysis and visualization, Performance optimization and cost management, and a deep understanding, designing, data modeling, data integration, data warehousing and managing large-scale data storage system with below services: Big Query, Cloud Run, Cloud Function, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage and Data Studio, Dataflow, Dataproc, Cloud Build etc.,\nStrong experience with infrastructure-as-code (IaC) tools like Terraform or Cloud Deployment Manager and Expertise in Tekton pipeline.\nDeep understanding of GCP services, including Compute Engine, Kubernetes Engine (GKE), Cloud Storage, Cloud SQL, and Cloud Networking.\nExtensive experience with DevOps practices and tools.\nExpertise in Data encryption, Identity and Access Management and Cloud Security.\nKey Responsibilities:\nWork effectively with fellow software engineers, product owners and other technical experts to deliver curated data products.\nDemonstrate technical knowledge and communication skills with the ability to advocate for well-designed solution.\nDesign, develop, and deploy data pipelines using Dataflow, Dataproc, and other relevant GCP services.\nBuild and maintain data warehouses using BigQuery.\nImplement data quality and validation processes.\nDevelop and maintain data schemas and models.\nCollaborate with software engineers and other stakeholders to meet business requirements.\nManage and monitor GCP infrastructure using tools like Cloud Monitoring and Cloud Logging.\nImplement security best practices for cloud-based applications.\nMentor and guide junior developers.","Data Analysis, Logistics, data engineering, Cloud security, Gcp, Data Modeling, Sql"
Senior Data Engineer,Launch IT Consulting,8-13 Years,,Hyderabad,Advertising,"Be a part of our success story. Launch offers talented and motivated people the opportunity to do the best work of their lives in a dynamic and growing company. Through competitive salaries, outstanding benefits, internal advancement opportunities, and recognized community involvement, you will have the chance to create a career you can be proud of. Your new trajectory starts here at Launch.\nWhat we are looking for: designing and building ETL pipelines, with a focus on Azure Data Factory and Microsoft Fabric services.\nMandatory Skills:\n8+ years of hands-on experience designing and building ETL pipelines, with a focus on Azure Data Factory and Microsoft Fabric services.\nExpertise in implementing CI/CD pipelines and automating deployment processes using Azure DevOps.\nHands-on experience migrating data from on-premises databases to Azure Cloud environments, including Managed Instances.\nProven track record of implementing version control systems and managing data pipeline architecture.\nStrong SQL skills for developing and optimizing queries, stored procedures, and database performance.\nFamiliarity with Delta Lake, Synapse Analytics, or other MS Fabric-specific technologies.\nExperience working in large, agile teams to deliver data solutions in an iterative, collaborative environment.\nPreferred Skills:\nKnowledge of Microsoft Fabric components such as Dataflows, Data Pipelines, and integration with Power BI for seamless analytics delivery.\nUnderstanding of data security practices, including data encryption and role-based access control in Azure.\nExperience with event-driven architectures using Azure Event Hubs or similar tools.\nFamiliarity with Data Ops principles to streamline pipeline monitoring and management.\nExcellent problem-solving skills and ability to quickly adapt to evolving project requirements.","Azure, Sql, Etl, Power Bi, Ci"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,Overview:\nWe are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.\nKey Responsibilities:\nProvide support and expertise in the domain of Data Engineer.\nCollaborate with cross-functional teams to achieve business goals.\nEnsure timely delivery of services and maintain high-quality standards.\nRequired Qualifications:\nProven experience in a relevant field or position.\nStrong understanding of the responsibilities and tools associated with the role.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nCertifications or training relevant to Data Engineer.\nExperience working in a fast-paced environment or large organizations.,"data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,Overview:\nWe are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.\nKey Responsibilities:\nProvide support and expertise in the domain of Data Engineer.\nCollaborate with cross-functional teams to achieve business goals.\nEnsure timely delivery of services and maintain high-quality standards.\nRequired Qualifications:\nProven experience in a relevant field or position.\nStrong understanding of the responsibilities and tools associated with the role.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nCertifications or training relevant to Data Engineer.\nExperience working in a fast-paced environment or large organizations.,"data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Data Engineer,Veeva Systems,4-9 Years,,Mumbai,Cloud Data Services,"Veeva is looking for a hard-working, collaborative expert in data pipelines, ETL tools, and warehousing. You will be creating high-quality solutions to complex problems around data acquisition and integration into existing systems and data sets. If you enjoy working in a fast-paced environment with a variety of exciting challenges to solve, this is the role for you.\nWhat You'll Do\nDesign and build reusable and configurable data pipelines using a mix of standard cloud-based and proprietary data platforms.\nWork with a data scientist to develop AI/ML proofs of concept into full implementations.\nCreate documentation and provide informal training for data analysts on the configuration and use of tools and pipelines.\nWork with other engineering and product teams to understand proprietary platforms and provide input and feedback.\nProvide guidance to more junior data engineers on best practices.\nWork with security teams to ensure that all servers, platforms, and other resources meet security requirements.\nRequirements\nBS degree in Computer Science, Engineering, or a related subject.\n4+ years of experience in Data Engineering roles.\nExperience developing sophisticated data pipelines in cloud-based environments (e.g., AWS) using scalable data processing tools (e.g., Apache Spark).\nData modeling experience.\nDemonstrated ability to work with others, particularly providing guidance to other data engineers.\nAbility to communicate around complex ideas and topics in English with both technical and non-technical individuals.\nNice to Have\nFamiliarity with Agile methodologies.\nDevOps skills, especially CI/CD experience.\nConfiguring and maintaining cloud-based cluster computing resources and orchestration systems (e.g., EC2 instances, Kubernetes clusters, Elastic Beanstalk).\nPerks & Benefits\nFlexible work from anywhere policy.","Data Processing, San, Data Modeling, Cloud, Spark, Agile"
AWS Data Engineer Manager,Wipzo Systech Private Limited,10-15 Years,INR 30 - 45 LPA,"Gurugram, Hyderabad, Pune","Information Technology, Information Services","Role: AWS Data Engineering Manager\nExperience: 10+ Years\nLocation: Pune, Gurgaon, Hyderabad, Bangalore (Hybrid)\nOur Values: Passion, Continuous Learning, Adaptability, Teamwork, Customer Centricity, Reliability\nJob Summary:\nWe are seeking a highly motivated and experienced AWS Engineer to join our MarTech team in the NFL. This position requires an individual with AWS cloud experience and ambition to continually keep up with best practices when it comes to cloud development. The successful candidate must be able to seek out requirements and create best-in-class cloud-native solutions. The engineer must always create solutions that are repeatable, scalable and well-governed. They will deploy and rigorously test solutions to ensure they are robust and secure. The engineer will create and maintain diagrams associated with solutions deployed into production.\nMust have:\n4-6 years of experience with AWS tech stack(S3, Glue, Redshift, Athena, Lambda,CloudWatch, SQS, IAM roles, CloudTrail).\n3-5 years ofSQL, Python & Pyspark programming experience.\nExperience working withETL Tools.\nExperience with CDC mechanisms for database sources.\nExperience building distributed architecture-based systems, especially handling large data volumes and real-time distribution.\nInitiative and problem-solving skills when working independently.\nExpertise in building high-performance, highly scalable, cloud-based applications.\nExperience with SQL and No-SQL databases.\nGood collaboration and communication skills, highly self-driven, and take ownership.\nExperience in Writing well-documented, Clean, and Effective codes is a must.\nGood to have:\nAWS Cloud Certifications.\nKnowledge and experience in designing and developing RESTful services.\n1-3 years of experience inDBTwith Data Modeling, Airflow,MWAA,SQL,Jinja templating, and packages/macros to build robust, performant, and reliable data transformation and feature extraction pipelines.\n1-2 years of experience in Airbyte building ingestion modules for streaming, batch.\nGood experience building Real-Time streaming data pipelines with Kafka, Kinesis etc.\nFamiliarity with Big Data Design Patterns, modeling, and architecture.\nWorking knowledge of DevOps methodologies, including designing CI/CD pipelines.\nGood understanding of Data warehousing & Data Lake solutions concepts.\nResponsibilities:\nCreate and maintain scalable, robust AWS architecture.\nDevelop API-based, CDC, batch, and real-time data pipelines for structured and unstructured datasets.\nEnable integration with third-party systems as needed.\nEnsure solutions are repeatable and scalable across the organization.\nWork with client teams to gather requirements, develop solutions, and deploy them.\nProvide robust solution documentation for a wide audience.\nCollaborate with data professionals to bring applications to life, meeting business needs.\nPrioritize data protection and cloud security in all deliverables.\nEducation:\nBE/B.Tech/MS/M.Tech/ME from reputed institute.\nEvery individual comes with a different set of skills and qualities so even if you dont tick all the boxes for the role today, we urge you to apply as there might be a suitable/unique role for you tomorrow!\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:Any Graduate\nPG:Any Postgraduate\nDoctorate:Any Doctorate","Data Bricks, Glue, Athena, CloudTrail, Pyspark, S3, Kafka, Cloudwatch, Redshift, Sql, Lambda, Kinesis, Docker, Sqs, Iam, Data Engineer, Python, Kubernetes, Etl, Aws"
Azure Data Engineer,Future Focus Infotech,4-8 Years,,Pune,Information Technology,Overview:\nWe are seeking a highly motivated and detail-oriented individual to join our team as a Azure Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.\nKey Responsibilities:\nProvide support and expertise in the domain of Azure Data Engineer.\nCollaborate with cross-functional teams to achieve business goals.\nEnsure timely delivery of services and maintain high-quality standards.\nRequired Qualifications:\nProven experience in a relevant field or position.\nStrong understanding of the responsibilities and tools associated with the role.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nCertifications or training relevant to Azure Data Engineer.\nExperience working in a fast-paced environment or large organizations.,"Data Pipelines, Azure Data Factory, Databricks, Azure, Sql, Etl"
Senior Data Engineer,Launch IT Consulting,8-13 Years,,Hyderabad,Advertising,"Developing and maintaining data models that define the structure, relationships, and constraints of data within enterprise data model\nCreating and implementing data pipelines and ETL processes to move and transform data from various sources into data storage solutions like data warehouses or data lakes\nImplementing measures to ensure the accuracy, integrity, and consistency of data throughout the ETL process\nEnsuring ETL processes are efficient, scalable, and optimized for performance\nMandatory Skills:\n8+ years for Data Modeling and ETL Development Experience\nExperience with dimensional data modeling and star schema\nStrong SQL skills experience\nExperience with ETL Development using Python and Amazon Glue\nExperience loading data into the Amazon Redshift and Amazon S3\nApache Iceberg\nPreferred Skills:\nSpark\nAmazon EMR\nApache Kafka or Apache Flink","Sql, Python, Etl, S3, Emr"
Data Engineer II,Trimble,3-6 Years,,Hyderabad,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.\nKey Responsibilities:\nDesign and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.\nDevelop, optimize, and maintain data pipelines using AWS services.\nImplement robust ETL/ELT processes and event-driven data ingestion.\nEstablish and enforce data governance policies, ensuring data quality, security, and compliance.\nOptimize cloud resources for performance, availability, and cost-efficiency.\nPartner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.\nIdentify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.\nOur current tech-stack:\nAWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.\nDevOps: Cloudformation, Terraform, Git, CodeBuild\nDatabase: Redshift, PostgreSQL, DynamoDB, Athena\nLanguage: Bash, Python, SQL\nQualifications:\nBachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.\nExpertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.\nExtensive experience indata and cloud engineeringroles.\nExpertise inAWSplatforms, including data services.\nStrong competence inETL processes,data warehousing, and big data technologies.\nAdvanced skills inscripting,Python,SQL, and infrastructure automation tools.\nFamiliarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).\nExperience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
Data Engineer,Anblicks Solutions,8-13 Years,,Ahmedabad,Cloud Data Services,"Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise in Azure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake) along with big data processing, real-time analytics, and cloud data integration.\nKey Responsibilities:\n1. Data Pipeline Development ETL/ELT\nDesign and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.\nImplement ETL/ELT workflows for structured and unstructured data processing.\nOptimize data ingestion, transformation, and storage strategies.\n2. Cloud Data Architecture Integration\nDevelop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).\nWork with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.\n3. Database Management Optimization\nDesign and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).\nOptimize SQL queries and indexing strategies for performance.\nImplement data partitioning, compression, and caching for efficiency.\n4. Data Governance, Security Compliance\nEnsure data quality, lineage, and governance with tools like Purview.\nImplement role-based access control (RBAC), encryption, and security policies.\nEnsure compliance with GDPR, HIPAA, and ISO 27001 regulations.\n5. Monitoring Performance Tuning\nUse Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.\nAutomate data pipeline testing and validation.\n6. Collaboration Documentation\nDocument data models, pipeline architectures, and data workflows.\nTechnical Skills:\nCloud Data Services: Azure Data Factory, Azure Synapse, Databricks, SSIS, BigQuery.\nETL Data Pipelines: Apache Spark, Python, SQL.\nBig Data Processing: Delta Lake, Parquet, Hadoop, Event Hubs.\nDatabase Management: SQL Server, Cosmos DB.\nSecurity Compliance: RBAC, Data Masking, Encryption, Purview.\nScripting Automation: Python, PowerShell, Terraform for IaC.","Cloud Architecture, Azure Data Factory, Data Governance, Performance Tuning"
Data Engineer II,Trimble,3-6 Years,,Chennai,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.\nKey Responsibilities:\nDesign and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.\nDevelop, optimize, and maintain data pipelines using AWS services.\nImplement robust ETL/ELT processes and event-driven data ingestion.\nEstablish and enforce data governance policies, ensuring data quality, security, and compliance.\nOptimize cloud resources for performance, availability, and cost-efficiency.\nPartner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.\nIdentify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.\nOur current tech-stack:\nAWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.\nDevOps: Cloudformation, Terraform, Git, CodeBuild\nDatabase: Redshift, PostgreSQL, DynamoDB, Athena\nLanguage: Bash, Python, SQL\nQualifications:\nBachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.\nExpertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.\nExtensive experience indata and cloud engineeringroles.\nExpertise inAWSplatforms, including data services.\nStrong competence inETL processes,data warehousing, and big data technologies.\nAdvanced skills inscripting,Python,SQL, and infrastructure automation tools.\nFamiliarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).\nExperience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
ML/Data Engineer,Ca One Tech,2-7 Years,,"Hyderabad, Bengaluru",Information Technology,"Preferred Qualifications\nVertex AI\nJupyter notebooks\nML model Development\nKubeflow\nData proc\nData Pipelines (Data flow, Pub/sub)\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development","model development, Proc"
Data Engineer IV,Varite India Private Limited,2-5 Years,,Gurugram,"Information Technology, Information Services","VARITE is a global staffing and IT consulting company providing technical consulting and team augmentation services to Fortune 500 companies in the USA, UK, Canada, and India.\nClient Overview: Our client is a leading American technological research and consulting firm based in Stamford, Connecticut. They conduct cutting-edge research on technology and provide insights through private consulting, executive programs, and conferences. With clients including large corporations, government agencies, technology companies, and investment firms, the client serves over 12,000 organizations across more than 100 countries. The company has a workforce of 15,000 employees.\nRole Overview: We are seeking a talented Data Engineer IV to join the People Analytics Data Engineering team for our client. This role will involve the development, automation, testing, and integration of critical HR data assets to prepare the business for the future of work. As a Data Engineer, you will be responsible for building and supporting data platforms, working with data warehousing, dimensional modeling, and ETL/ELT pipelines.\nLocation: Gurgaon, Haryana\nContract Duration: 3 months\nStart Date: 1st January 2025\nExperience Required: 2+ years\nKey Responsibilities:\nDevelop and maintain data pipelines and automation processes using Azure Data Factory.\nImplement ETL/ELT processes focused on data warehousing, dimensional modeling, and incremental loads.\nDevelop cloud-based data solutions within Azure infrastructure.\nWrite complex ANSI SQL queries, including SELECT, DML, DDL, and optimization techniques like indexing and partitioning.\nTrack project and development progress using industry-standard DevOps tools (e.g., Jira).\nEnsure seamless integration and automation for HR data assets, preparing the business for future workforce analytics needs.\nQualifications:\nBachelor's or Master's degree in Computer Science, Computer Engineering, Engineering, Data Science, or a related technical field.\n2+ years of experience in data engineering and data warehousing.\nProficient in ANSI SQL, including advanced techniques like partitioning, indexing, and query optimization.\nHands-on experience with Azure Data Factory for data pipelines, orchestration, and mapping.\nExperience with ETL/ELT processes specific to dimensional modeling concepts and incremental data loads.\nFamiliarity with project tracking in DevOps tools (e.g., Jira).\nPreferred Skills:\nExperience with Data Bricks development.\nKnowledge of the HR/Workforce/People domain (e.g., headcount, hires, terms).\nFamiliarity with Workday source system and its integrations.\nExperience with Business Intelligence tools like Power BI or Tableau.\nExperience with Visier integrations.\nClick apply for this exciting opportunity with a leading research and consulting firm.","Azure Data Factory, Azure Cloud, Ansi Sql"
IT - Data Engineer _ AWS,Systechcorp Inc,2-6 Years,,"Kolkata, Mumbai",Software,Work with practice SMEs to understand the client s challenges and how Genpact solves for their challenges\nUnderstand our right to play and value articulation\nLiaison with practice team to drive product/offering messaging by translating technical nuances to strong client messages\nProductize the offering with the right value articulation to make it client ready\nEnsure every offering has all the GTM readiness collaterals for client readiness\nEstablish strong relationship with practice leaders to be their trusted advisor for offerings\nAbility to quickly research industry and competitor s offerings and ability to incorporate it in our value messaging\nWork with Knowledge Management team to drive strategic placement of the offerings in our infrastructure,"IT, data engineering, Cloud, Aws"
Expert Data Engineer,Ciklum,2-6 Years,,Chennai,Information Technology,"About the Role\nAs an Expert Data Engineer, you will be part of a cross-functional development team dedicated to building innovative, scalable data solutions that drive tomorrow's experiences. You will design and maintain large-scale data infrastructure, work closely with analysts and data scientists, and contribute to internal technical communities.\nResponsibilities\nBuild, deploy, and maintain mission-critical analytics solutions for processing large-scale data.\nDevelop components for data ingestion, real-time streaming, batch processing, and ETL across various storage platforms.\nOwn parts of the engineering infrastructure; improve platform quality, performance, and maintainability.\nCollaborate with other engineers to share knowledge and continuously learn new technologies.\nEnsure solutions meet requirements for functionality, scalability, reliability, and availability.\nSupport end-to-end solutions, performing development, QA, and DevOps roles as needed.\nPartner with business analysts and data scientists to understand and support use cases.\nParticipate in unit activities, conferences, and promote best practices.\nContribute to sales efforts, customer meetings, and digital services engagement.\nRequirement\nTechnical Experience\n5+ years of experience coding in SQL, Java, Python, or Scala, with solid computer science fundamentals.\n3+ years of experience leading production deployments of large-scale backend data systems.\n2+ years of hands-on experience with tools like Hadoop, MapReduce, Pig, Hive, Impala, Spark, Kafka, Storm, and databases like HBase, Cassandra.\n3+ years working with cloud data platforms: AWS, Azure, or GCP.\nData Engineering Expertise\nStrong knowledge of Data Governance, including data quality, security, and compliance standards.\nDeep understanding of Data Warehousing design, optimization, and implementation.\nExperience with SQL and MPP databases (e.g., Vertica, Netezza, Greenplum).\nFamiliarity with data quality testing, automation, and visualization tools.\nExperience in BI report and dashboard design (e.g., Power BI, Tableau).\nEngineering Practices\nExperience with Agile methodologies (e.g., SCRUM).\nStrong grasp of the software development lifecycle, including coding standards, version control, code reviews, CI/CD, and operations.\nProven ability to lead development teams: managing backlogs, peer review processes, and maintaining development standards.\nEducation\nBachelor's degree in Computer Science or Engineering from a top-tier university; Master's preferred.\nDesirable Skills\nExperience in data science or machine learning projects.\nProficiency in backend development and deployment.\nKnowledge of CI/CD pipelines and related tools.\nUnderstanding of enterprise-level data analytics.\nExperience with platforms like Databricks, Snowflake.\nFamiliarity with Kubernetes and container orchestration.","GCP., Java, Azure, Sql, Python, AWS, Machine Learning, Data Science"
Data Engineer,Testingxperts Inc,0-3 Years,,Chandigarh,Information Technology,"Job Details\nData Engineer with 0-1 years of experience in Power BI, sql, Python.\nIt is 6 Months internship post to that fulltime, All 2years bond .\nCertification in relevant field best to have\nJob Type: Full-time\nFixed shift: Monday to Friday\nMorning shift\nWork Location: In person,","Powerbi, Power Bi, Python, Sql"
Associate Data Engineer- GET,XenonStack,0-1 Years,,Mohali,Information Technology,"Job Responsibilities -\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with coworkers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analyzed quickly by BI AI Teams .\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nTechnical Requirements-\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch , Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores .\nProfessional Attributes-\nExcellent communication skills\nAttention to detail\nAnalytical mind and Problem Solving Aptitude\nStrong Organizational skills","Consulting, Analytical, Architectural Design, Machine Learning, Big Data Analytics"
Senior Engineer (Data Engineer),Velotio Technologies,2-7 Years,,Bengaluru,Software,"Job description\nAbout Velotio:\nVelotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.\nDesign and build scalable data infrastructure with efficiency, reliability, and consistency to meet rapidly growing data needs\nBuild the applications required for optimal extraction, cleaning, transformation, and loading data from dis","B2B SaaS, IoT Machine Learning space, data engineering"
GCP Data Engineer,Kognivera It Solutions Private Limited,2-5 Years,,Bengaluru,Software,"About the Role: We are seeking a Lead GCP Data Engineer who can demonstrate a broad range of technical skills across Google Cloud Platform (GCP) and third-party technologies. As a pivotal member of our team, you will be responsible for designing robust data architectures, developing efficient data pipelines, and optimizing data processing workflows on GCP. Your expertise will be instrumental in driving our data engineering initiatives forward, ensuring scalability, reliability, and performance.\nKey Responsibilities\nData Architecture Design: Design scalable and efficient data architectures on GCP that meet the organization's data processing and analysis requirements. Collaborate closely with data scientists, business analysts, and stakeholders to define effective data models and structures.\nData Pipeline Development: Develop and implement data pipelines using GCP services such as Google Cloud Storage, Big Query, Dataflow, and Pub/Sub. Ensure data quality, reliability, and governance throughout the data lifecycle.\nData Transformation and Integration: Utilize technologies like Apache Beam, Apache Spark, and Cloud Dataproc to transform and integrate data from diverse sources. Perform data cleansing, aggregation, enrichment, and normalization to support downstream applications and analytics.\nPerformance Optimization: Optimize data processing workflows to enhance performance and efficiency. Monitor pipelines, identify bottlenecks, and implement optimizations such as improved data partitioning, sharding, and leveraging GCP's autoscaling capabilities.\nContinuous Improvement: Stay abreast of advancements in data engineering and cloud technologies. Explore and implement new GCP features and services to enhance data processing capabilities and drive innovation within the team.\nResearch and Innovation: Conduct research on emerging data engineering technologies, tools, and best practices. Evaluate new methodologies to improve data engineering processes and bring innovative solutions to the organization.\nTask Automation: Automate data engineering tasks using scripting, workflows, or tools like Cloud Composer and Cloud Functions. Streamline data ingestion, transformation, monitoring, and other operational processes to improve efficiency and reduce manual effort.\n\nAttributes & Competencies\nEducation: BE/BTech, MTech, or MCA.\nExperience: Minimum 5+ years in development/migration projects, with at least 3 years focused on GCP. Experience in GCP-based Big Data deployments (batch/real-time) using Big Query, Big Table, Google Cloud Storage, Pub/Sub, Data Fusion, Dataflow, Dataproc, and Airflow.\nTechnical Skills\nGoogle Certified Professional Cloud Architect with experience automating and orchestrating workloads on GCP or other public clouds.\nProficiency in at least one configuration management system (Chef, Puppet, Ansible, Salt, etc.).\nStrong programming skills in Python, Go, with expertise in Git and Git workflows.\nDemonstrated proficiency in CI/CD tools such as Jenkins, TeamCity, or Spinnaker.","Data Modeling, Sql, Python, Etl"
Data Engineer,Systechcorp Inc,2-5 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Senior Data Engineer\nSkills:\nAzure Cloud technologies\nMandatory\nSynapse (DataFlow, Pipeline (including web call), Notebook (Pyspark))\nSQL expertise\nKey vault\nBlob Storage\nPreferable\nLogic Apps\nFunction App (C#)\nAPI Management\nGIT","Synapse, Key Vault, Blob Storage, data engineering, Pyspark, Azure, Sql"
Data Engineer,Systechcorp Inc,2-5 Years,,"Delhi, Bengaluru, Chennai",Software,"Senior Data Engineer\nSkills:\nAzure Cloud technologies\nMandatory\nSynapse (DataFlow, Pipeline (including web call), Notebook (Pyspark))\nSQL expertise\nKey vault\nBlob Storage\nPreferable\nLogic Apps\nFunction App (C#)\nAPI Management\nGIT","Synapse, Key Vault, Blob Storage, data engineering, Pyspark, Azure, Sql"
Data Engineer,Velotio Technologies,2-7 Years,,Bengaluru,Software,"Job description\nDesign and build scalable data infrastructure with efficiency, reliability, and consistency to meet rapidly growing data needs\nBuild the applications required for optimal extraction, cleaning, transformation, and loading data from disparate data sources and formats using the latest big data technologies\nBuilding ETL/ELT pipelines and work with other data infrastructure components, like Data Lakes, Data Warehouses and BI/reporting/analytics tools\nWork with various cloud services like AWS, GCP, Azure to implement highly available, horizontally scalable data processing and storage systems and automate manual processes and workflows\nImplement processes and systems to monitor data quality, to ensure data is always accurate, reliable, and available for the stakeholders and other business processes that depend on it\nWork closely with different business units and engineering teams to develop a long-term data platform architecture strategy and thus foster data-driven decision-making practices across the organization\nHelp establish and maintain a high level of operational excellence in data engineering\nEvaluate, integrate, and build tools to accelerate Data Engineering, Data Science, Business Intelligence, Reporting, and Analytics as needed\nFocus on building test-driven development by writing unit/integration tests\nContribute to design documents and engineering wiki\nYou will enjoy this role if you...\nLike building elegant well-architected software products with enterprise customers\nWant to learn to leverage public cloud services & cutting-edge big data technologies, like Spark, Airflow, Hadoop, Snowflake, and Redshift\nWork collaboratively as part of a close-knit team of geeks, architects, and leads\nDesired Skills & Experience:\n2+ years of data engineering or equivalent knowledge and ability\n2+ years software engineering or equivalent knowledge and ability\nStrong proficiency in at least one of the following programming languages: Python, Scala, or Java\nExperience designing and maintaining at least one type of database (Object Store, Columnar, In-memory, Relational, Tabular, Key-Value Store, Triple-store, Tuple-store, Graph, and other related database types)\nGood understanding of star/snowflake schema designs\nExtensive experience working with big data technologies like Spark, Hadoop, Hive\nExperience building ETL/ELT pipelines and working on other data infrastructure components like BI/reporting/analytics tools\nExperience working with workflow orchestration tools like Apache Airflow, Oozie, Azkaban, NiFi, Airbyte, etc.\nExperience building production-grade data backup/restore strategies and disaster recovery solutions\nHands-on experience with implementing batch and stream data processing applications using technologies like AWS DMS, Apache Flink, Apache Spark, AWS Kinesis, Kafka, etc.\nKnowledge of best practices in developing and deploying applications that are highly available and scalable\nExperience with or knowledge of Agile Software Development methodologies\nExcellent problem-solving and troubleshooting skills\nProcess-oriented with excellent documentation skills","Business Intelligence, Java, Sql, Python, Aws"
Data Engineer 2,Bread Financial,2-5 Years,,Bengaluru,Financial Services,"The Data Engineer II works on different projects of data engineering to support the use cases, data ingestion pipeline and identify potential process or data quality issues. The team also supports marketing analytic teams with analytical tools that enable our analytics and business communities to do their job easier, faster and smarter. The team brings together data from different internal external partners and builds a curated Marketing analytics focused data tools ecosystem. The Data Engineer plays a crucial role in building this ecosystem depending on the Marketing analytics communities need.\nEssential Job Functions\nCollaboration - Collaborates with internal/external stakeholders to manage data logistics - including data specifications, transfers, structures, and rules. Collaborates with business users, business analysts and technical architects in transforming business requirements into analytical workbenches, tools and dashboards reflecting usability best practices and current design trends. Demonstrates analytical, interpersonal and professional communication skills. Learns quickly and works effectively individually and as part of a team.\nProcess Improvement - Accesses, extracts, and transforms Credit and Retail data from a variety of sources of all sizes (including client marketing databases, 2nd and 3rd party data) using Hadoop, Spark, SQL, Big data technologies etc. Provide automation help to analytical teams around data centric needs using orchestration tools, SQL and possibly other big data/cloud solutions for efficiency improvement.\nProject Support - Supports Sr. Specialist and Specialist in new analytical proof of concepts and tool exploration projects. Effectively manages time and computing resources in order to deliver on time/correctly on concurrent projects. Involved in creating POCs to ingest and process streaming data using Spark and HDFS.\nData and Analytics - Answers and trouble shoots questions about data sets and analytical tools; Develops, maintains and enhances new and existing analytics tools/Frameworks to support internal customers/consumers. Ingests data from different sources, processes it according to the requirement document in order to store data to Hive or NoSQL database or different warehousing solutions. Manages data coming from different sources, involved in HDFS maintenance, and loading of structured and unstructured data. Applies knowledge in Agile Scrum methodology that leverages the Client BigData platform and used version control tool Git. Imports and exports data using Sqoop from HDFS to RDBMS and vice-versa. Demonstrates an understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management. Creates POCs to ingest and process streaming data using Spark and HDFS. Work on back-end using Scala, Python and Spark to perform several aggregation logics.\nTechnical Skills - Expert in writing complicated SQL Queries and database analysis for good performance. Experience working with python or Scala, Spark, Hadoop, Hive, Oozie, Sqoop, HDFS, Impala, Shell Scripts, Microsoft Azure Services like ADLS/Blob Storage solutions, Azure DataFactory, Azure Functions and Databricks. Utilize basic knowledge of Rest API for designing networked applications.\nReports to: Lead or above\nWorking Conditions/Physical Requirements: Normal office environment\nDirect Reports: 0\nMinimum Requirements:\nDegree Required: Bachelor's Degree\nArea of Study: Computer Science, Engineering\nYears of Work Experience Required: Two to five years or more\nType / focus of work experience required: Data Analytics","Python, Data Analytics, Sql, Azure Cloud, Big Data, Rest Api"
"Sr. Associate Technical Consultant, Data Engineer",AHEAD,2-5 Years,,Gurugram,Information Technology,"Data Engineer\n(Internally known as a Sr. Associate Technical Consultant)\nAHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.\nThe Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.\nResponsibilities\nA Data Engineer should be able to build, operationalize and monitor data processing systems\nCreate robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset\nImplement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases\nEngineers and supports data structures including but not limited to SQL and NoSQL databases\nEngineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)\nLeverages the right tools for the right job to deliver testable, maintainable, and modern data solutions\nRespond to customer/team inquiries and assist in troubleshooting and resolving challenges\nWorks with other scrum team members to estimate and deliver work inside of a sprint\nResearch data questions, identifies root causes, and interacts closely with business users and technical resources\nQualifications\n3+ years of professional technical experience\n3+ years of hands-on Data Warehousing\n3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake\n2+ years of programming languages such as Python\n3+ years of experience working in cloud environments (Azure)\n2 years of experience in Redshift\nStrong client-facing communication and facilitation skills\nKey Skills\nPython, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP\nWhy AHEAD\nThrough our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.\nWe fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.\nUSA Employment Benefits include\nMedical, Dental, and Vision Insurance\n401(k)\nPaid company holidays\nPaid time off\nPaid parental and caregiver leave\nPlus more! See benefits https://www.aheadbenefits.com/ for additional details.\nThe compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Azure, python, Spark, Redshift, hadoop"
Lead AWS Data Engineer (Python & Scala),iLink Digital,8-13 Years,,Bengaluru,Financial Services,"Develop & Optimize Data Pipelines Architect, build, and enhance scalable data pipelines for high-performance processing.\nTroubleshoot & Sustain Identify, diagnose, and resolve data pipeline issues to ensure operational efficiency.\nData Architecture & Storage Design efficient data storage and retrieval strategies using Postgres, Redshift, and other databases.\nCI/CD Pipeline Management Implement and maintain continuous integration and deployment strategies for smooth workflow automation.\nScalability & Performance Tuning Ensure the robustness of data solutions while optimizing performance at scale.\nCollaboration & Leadership Work closely with cross-functional teams to ensure seamless data flow and lead engineering best practices.\nSecurity & Reliability Establish governance protocols and ensure data integrity across all pipelines.\nTechnical Skills Required:\nProgramming: Expert in Python and Scala\nBig Data Technologies: Proficient in Spark, Kafka\nDevOps & Cloud Infrastructure: Strong understanding of Kubernetes\nSQL & Database Management: Skilled in SQL administration, Postgres, Redshift\nCI/CD Implementation: Experience in automating deployment processes for efficient workflow","cd, python, Scala, Ci, Big Data Technologies, Database Manager, Sql"
Data Engineer,Genzeon Corporation,2-5 Years,,Pune,Information Technology,"Genzeon is looking for an experienced Full Stack Data Engineer to join our team in Pune. In this role, you will be instrumental in shaping our data architecture, developing full-stack data solutions, and driving innovation in data processing and analytics. This position offers an exciting opportunity to work with a team of passionate professionals dedicated to leveraging data for impactful decisions and products.\nResponsibilities\nData Architecture:Design and build robust, scalable data architectures that support both operational and analytical use cases.\nETL Development:Develop and maintain ETL processes to gather data from various sources, ensuring data quality and consistency.\nData Modeling:Create and optimize data models to support efficient data storage, retrieval, and analysis.\nAPI Development:Design and implement APIs for data ingestion, processing, and retrieval.\nData Visualization:Develop dashboards and reports to visualize complex datasets in a user-friendly manner.\nCloud Solutions:Work with cloud technologies to deploy and maintain data solutions.\nCollaboration:Work closely with data scientists, analysts, and other engineers to integrate data solutions into company products and services.\nInnovation:Stay updated with the latest trends and technologies in data engineering and propose innovative solutions to improve existing systems.\nRequirements\nEducation:Bachelor's or Master's degree in Computer Science, Engineering, or a related field.\nExperience:Proven experience as a Full Stack Data Engineer or in a similar role.\nTechnical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.\nData Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).\nFront-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.\nCloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.\nAnalytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.\nCommunication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.\nBenefits\nCompetitive salary and benefits package.\nDynamic and innovative work environment.\nOpportunities for professional growth and development.\nFlexible working hours and supportive team culture.\nMust Have Skills:\nTechnical Expertise:Proficiency in programming languages like Python or Java, and experience with SQL and NoSQL databases.\nData Processing:Experience with big data processing frameworks (e.g., Hadoop, Spark).\nFront-End Development:Knowledge of front-end technologies (e.g., HTML, CSS, JavaScript) for dashboard and report development.\nCloud Platforms:Familiarity with cloud services (AWS, Azure, GCP) and their data-related offerings.\nAnalytical Mindset:Strong problem-solving skills and the ability to work on complex data challenges.\nCommunication Skills:Excellent communication skills to effectively collaborate with team members and stakeholders.","Java, python, Spark, hadoop, Sql, aws"
Senior Data Engineer,NXP Semiconductors,3-8 Years,,Bengaluru,Semiconductor,"You will work with Product Owners, Architects, Data Scientists, and other stakeholders to design, build and maintain ETL/ ELT pipelines, data pipelines and jobs, combining data from multiple source systems into one or multiple target systems.\nSolutions delivered must adhere to EBI and IT architectural principles pertaining to capacity planning, performance management, data security, data privacy, lifecycle management and regulatory compliance.\nAssisting the Operational Support team with analysis and investigation of issues is also expected, as needed.\nRequired Skills and Qualifications\nProven experience as a Data Engineer\nHands on experience in ETL design and development concepts (3+ years)\nExperience with AWS and Azure cloud platforms and their data service offerings\nProficiency in SQL, PySpark, Python\nExperience with GitHub, GitLab, CI/CD\nKnowledge of advanced analytic concepts including AI/ML\nStrong problem-solving skills and ability to work in a fast-paced and collaborative environment\nExcellent oral and written communication skills\nPreferred Skills Qualifications:\nExperience with Agile / DevOps\nProficiency in SQL (Databricks, Teradata)\nExperience with DBT","ETL/ELT, data engineering, Azure, Python, Sql, AWS"
Data Engineer / Senior Data Engineer,Infec Services Private Limited,3-12 Years,,Hyderabad,Software,"Job Descriptions:\nJob Title: Data Engineer / Senior Data Engineer\nJob Type: Contract To Hire(3 Months)\nYrs of Experience:3 to 5 yrs / 6 to 12 yrs\nNotice Period: Immediate - 15 Days\nPrimary Skill Set: Data Engineer, GCP,DBT, Data flow, Composer, Python, SQL, Microservices\nWhat are we required to do\nDesign, build, and maintain scalable and robust data pipelines for processing large volumes of data.\nDevelop ETL (Extract, Transform, Load) processes to ingest, clean, and transform data from various sources.\nImplement and optimize data storage solutions, including data lakes and warehouses, using GCP services like BigQuery, Cloud Storage, and Dataflow.\nIntegrate data from multiple data sources, ensuring data consistency and quality.\nDevelop and maintain automated data processing workflows, monitoring, and alerting systems.\nManage and maintain metadata, data catalogs, and data lineage documentation.\nWork closely with data scientists, analysts, and other stakeholders to understand data requirements and provide appropriate solutions.\nCollaborate with cross-functional teams to ensure data availability and reliability for various use cases.\nProvide technical support and guidance to other team members on data engineering best practices.\nOptimize data processing pipelines for performance, scalability, and cost-efficiency.\nMonitor data pipeline performance, troubleshoot issues, and implement necessary improvements.\nEnsure data security and compliance with relevant regulations and best practices.\nWhat you'll need\nexperience in data engineering or related roles.\nProven experience with Python for data processing, scripting, and automation.\nHands-on experience with Google Cloud Platform (GCP) services, including BigQuery, Cloud Storage, Dataflow, and Pub/Sub.\nStrong understanding of data modeling, ETL processes, and data warehousing concepts.\nStrong problem-solving skills and attention to detail.\nExcellent communication skills, with the ability to explain complex technical concepts to non-technical stakeholders.\nAbility to work independently and as part of a team in a fast-paced environment.","Data Engineer, Sql, Python, Microservices"
Senior Data Engineer,Ciklum,3-7 Years,,Chennai,Information Technology,"About the Role\nAs a Senior Data Engineer, you will be part of a cross-functional development team engineering experiences of tomorrow. This is an ideal role for professionals passionate about data, innovation, and working in a dynamic, international environment.\nResponsibilities\nBuild and prototype scalable data pipelines and develop new API integrations to handle increasing data volume and complexity.\nWrite reusable, testable, and efficient Python code for data ingestion and integration.\nUse SQL and Python to analyze data and derive insights in collaboration with business stakeholders.\nWork on multiple projects simultaneously, adapting to shifting priorities in a fast-paced environment.\nApply creative problem-solving approaches to complex data challenges, continuously improving data strategies and execution.\nContribute to design, code, configurations, and documentation for components managing data ingestion, real-time streaming, batch processing, ETL, and data storage.\nHelp maintain and improve engineering infrastructure, identifying and addressing gaps to boost platform quality, robustness, maintainability, and performance.\nCollaborate with engineering teams to ensure solutions meet functional, performance, availability, scalability, and reliability standards.\nPerform development, QA, and DevOps tasks as needed to ensure end-to-end ownership of solutions.\nWork closely with business analysts and data scientists to support their use cases.\nParticipate in team and unit activities, contribute to community building, and represent the company in conferences and best practice initiatives.\nSupport sales activities, customer engagements, and digital services.\nUnderstand and contribute to cloud infrastructure design and implementation.\nRequirements\nProven experience as a data engineer or in a similar role focused on data integration and management.\nStrong programming skills in Python, including working with APIs and automation.\nSolid foundation in SQL and relational database design.\nExcellent problem-solving skills with an algorithmic mindset.\nStrong communication skills and ability to collaborate in a team-oriented environment.\nExperience building and maintaining robust, scalable data pipelines.\nHands-on experience with batch and streaming data processing systems.\nFamiliarity with best practices in software development, including documentation, testing, and performance optimization.\nDemonstrated ability to work independently and take initiative in high-impact projects.\nUnderstanding of cloud infrastructure and deployment.\nDesirable\nExperience with cloud platforms (e.g., AWS, Google Cloud Platform, Azure).\nKnowledge of data warehousing concepts and ETL methodologies.\nFamiliarity with version control tools such as Git.","SQL and Python, cloud platforms, ETL methodologies, Google Cloud Platform, Data Engineer, Azure, AWS"
Senior Data Engineer - Visa,Foray Software,4-6 Years,,Bengaluru,Consulting,"Bachelor s Degree in Computer Science, Computer Engineering or related technical field required. Master s Degree or other advanced degree preferred.\n4-6+ years of total experience of which 2+ years of relevant experience in Big Data platforms.\nStrong analytical, problem solving and communication/articulation skills.\n3+ years of experience with big data and the Hadoop ecosystem (Spark, HDFS, Hive, Sqoop, Hudi, Parquet, Apache Nifi and Kafka).\nHands On in Scala/Spark\nPython is a plus\nHands on knowledge of Oracle, MS-SQL databases.\nExperience with job schedulers such as CA or AutoSys.\nExperience with source code control systems (e.g. Git, Jenkins, Artifactory).\nExperience with platforms such as Tableau and At Scale is a plus.","Analytical, Git, MS SQL, Spark, Tableau, Big Data, Oracle, Apache, Python"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Mumbai, Pune",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS. This will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives. Collaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Cloud Computing, Artificial Intelligence, Pytorch, Big Data Technologies, Aws, Tensorflow"
GCP Data Engineer,Nihilent,5-7 Years,,Pune,Information Technology,"Unnesting of JSON files in Google BigQuery\nGoogle Cloud Run (using container services, python)\nHands on experience in GCP services like composer, cloud function, cloud run\nExperience in developing CI/CD pipelines.\nExperience in DBT scripts, docker files and knowledge on datavault is a plus.\nStrong technical knowledge and hands on experience of python or java\nRole: Data Engineer\nIndustry Type: IT Services & Consulting\nDepartment: Data Science & Analytics\nEmployment Type: Full Time, Permanent\nRole Category: Data Science & Machine Learning\nEducation\nUG: Any Graduate\nPG: Any Postgraduate","Gcp, Cloud, Json, Python"
Data Engineer AWS/ Power platform Engineer,Systechcorp Inc,3-7 Years,,"Hyderabad, Kolkata, Mumbai",Software,"Design and develop data pipelines for Generative AI projects by leveraging a combination of technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, LangChain, AWS Functions, Redshift, and SSIS.\nThis will involve the logical and efficient integration of these tools to create seamless, high-performance data flows that efficiently support the data requirements of our cutting-edge AI initiatives.\nCollaborate with data scientists, AI researchers, and other stakeholders to understand data requirements and translate them into effective data engineering solutions.\nUser will be managing movement, organization and quality assessments of large set of data to facilitate the creation of Knowledge base for RAG systems and model training\nDemonstrate familiarity with data integration services such as AWS Glue and Azure Data Factory, showcasing the ability to effectively utilize these platforms for seamless data ingestion, transformation, and orchestration across various sources and destinations.\nPossess proficiency in constructing data warehouses and data lakes, demonstrating a strong foundation in organizing and consolidating large volumes of structured and unstructured data for efficient storage, retrieval, and analysis.\nOptimize and maintain data pipelines to ensure high-performance, reliable, and scalable data processing.\nDevelop and implement data validation and quality assurance procedures to ensure the accuracy and consistency of the data used in Generative AI projects.\nMonitor and troubleshoot data pipeline performance, identify bottlenecks, and implement improvements as necessary.\nStay current with emerging trends and technologies in the fields of data engineering, Generative AI, and related areas to ensure the continued success of our projects.\nCollaborate with team members on documentation, knowledge sharing, and best practices for data engineering within a Generative AI context.\nEnsure data privacy and security compliance in accordance with industry standards and regulations.\nQualifications we seek in you:\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\nStrong experience with data engineering technologies, including Vector DB, Graph DB, Airflow, Spark, PySpark, Python, langchain, AWS Functions, Redshift, and SSIS.\nStrong understanding of data warehousing concepts, ETL processes, and data modeling.\nStrong understanding of S3 and code-based scripting to move large volumes of data across application storage layers\nFamiliarity with Generative AI concepts and technologies, such as GPT-4, Transformers, and other natural language processing techniques.\nExcellent problem-solving, analytical, and critical thinking skills.\nStrong communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nPreferred Qualifications/ skills\nKnowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud Platform, is a plus.\nExperience with big data technologies, such as Hadoop, Hive, or Presto, is a plus.\nFamiliarity with machine learning frameworks, such as TensorFlow or PyTorch, is a plus.\nA continuous learning mindset and a passion for staying up-to-date with the latest advancements in data engineering and Generative AI.","Data Processing, Cloud Computing, data engineering, Machine Learning, SSIS, Python, AWS"
"Data Engineer (Pandas, SQL, Snowflake)",Franklin Templeton,4-6 Years,,Hyderabad,Financial Services,"This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. We re looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale.\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Data Engineer responsible forServe as a hands - on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office:\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successfulBachelor s or master s degree in computer science, data science, engineering, or a related field:\n4+ years of experience in data engineering.\nStrong Pandas & SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.","snowflake, data engineering, Pandas, Sql, Python"
Data Engineer,Anblicks Solutions,5-10 Years,,Hyderabad,Cloud Data Services,"Must have 5+ years of experience in Python programming.\nExtensive experience in handling complex projects.\nGood leadership skills and ability to lead projects independently.\nGood communication and teamwork skills.\nGood analytical skills and proficiency in Web based development technologies.\nExpertise in Agile development methodology.\nStrong problem-solving skills.\nStrong knowledge in Design patterns, Security, Performance tuning and App monitoring.\nExperience in API Design, security patterns, Microservices architecture.\nMust have experience in building scalable, low latency based web application using Flask, FastAPI or similar.\nExperience in building python-based SDKs.\nExperience in python based ORM (Object Relational Mapper) frameworks.\nKnowledge in distributed in-memory systems such as Redis, MemCache, GemFire or similar.\nKnowledge in SQL database and no-SQL database.\nUnderstanding on building Production grade application using WSGI/ASGI frameworks such as Gunicorn, Uvicorn etc.\nExperience of Jenkins, CICD pipeline, Logging framework, Splunk, ELK stack is good to have.\nKnowledge and experience with virtualization and cloud platforms (Kubernetes, AWS, OpenShift, Docker Containers)\nFamiliarity with version control system such as Git.\nKnowledge on developing test suites, application debugging skills.\nGood to have:\nUnderstanding on front-end technologies including JavaScript, HTML5, Angular.\nUnderstanding on Event-driven programming using Python.\nKnowledge of Generative AI technologies, LLMs, Embeddings, building RAG based solutions.\nUnderstanding on HDFS, Hive, Impala, PySpark and other components of BigData systems.","web based development, app monitoring, python, Sql, Python Programming, Performance Tuning"
Data Engineer,Systechcorp Inc,2-5 Years,,Pune,Software,"Senior Data Engineer\nSkills:\nAzure Cloud technologies\nMandatory\nSynapse (DataFlow, Pipeline (including web call), Notebook (Pyspark))\nSQL expertise\nKey vault\nBlob Storage\nPreferable\nLogic Apps\nFunction App (C#)\nAPI Management\nGIT","Synapse, Key Vault, Blob Storage, data engineering, Pyspark, Azure, Sql"
Data Engineer,Impetus Technologies,3-5 Years,,"Bengaluru, Noida, Pune",Software,"Must have technical Skills\n3-5 Years of experience\nExpertise and hands-on experience on Python Must Have\nExpertise knowledge on SparkQL/Spark Dataframe Must Have\nGood knowledge of SQL Good to Have\nGood knowledge of Shell script Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nGood knowledge of Cloud- Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nRoles & Responsibilities\nSelected candidate will work on Data Warehouse modernization projects and will responsible for the following activities.\nDevelop programs/scripts in Python/Java + SparkSQL/Spark Dataframe or Python/Java + Cloud native SQL like RedshiftSQL/SnowSQL etc.\nValidation of scripts\nPerformance tuning\nData ingestion from source to target platform\nJob orchestration\nRole:Big Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Any Specialization, Any Graduate","data engineering, Data Pipeline, Pyspark, Big Data, Data Warehousing, Python, Sql, Data Transformation"
Data Engineer,Coditas Technologies,3-6 Years,,Pune,Software,"We are looking for data engineers who have the right attitude, aptitude, skills, empathy,\ncompassion, and hunger for learning. Build products in the data analytics space. A passion\nfor shipping high-quality data products, interest in the data products space; curiosity about\nthe bigger picture of building a company, product development and its people.\nRoles and Responsibilities\nDevelop and manage robust ETL pipelines using Apache Spark (Scala)\nUnderstand park concepts, performance optimization techniques and governance tools\nDevelop a highly scalable, reliable, and high-performance data processing pipeline to extract, transform and load data from various systems to the Enterprise Data Warehouse/Data Lake/Data Mesh\nCollaborate cross-functionally to design effective data solutions\nImplement data workflows utilizing AWS Step Functions for efficient orchestration.\nLeverage AWS Glue and Crawler for seamless data cataloging and automation\nMonitor, troubleshoot, and optimize pipeline performance and data quality\nMaintain high coding standards and produce thorough documentation. Contribute to high-level (HLD) and low-level (LLD) design discussions\nTechnical Skills\nMinimum 3 years of progressive experience building solutions in Big Data environments.\nHave a strong ability to build robust and resilient data pipelines which are scalable, fault tolerant and reliable in terms of data movement.\n3+ years of hands-on expertise in Python, Spark and Kafka.\nStrong command of AWS services like EMR, Redshift, Step Functions, AWS Glue, and AWS Crawler.\nStrong hands on capabilities on SQL and NoSQL technologies.\nSound understanding of data warehousing, modeling, and ETL concepts\nFamiliarity with High-Level Design (HLD) and Low-Level Design (LLD) principles\nExcellent written and verbal communication skills.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate","Airflow, Athena, Python, Sql, S3, Aws Lambda, Amazon Ec2, Amazon Redshift, Pyspark, AWS Glue, Azure"
Data Engineer Sr Associate,TechnoGen,6-10 Years,,Hyderabad,"Consulting, Information Services","Role & responsibilities\nBachelors degree in computer science, engineering, or a related field. Master's degree preferred.\nData: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.\nSQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.\nELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC\nMigration Experience: Experience Informatica on prem to IICS/IDMC migration\nCloud: 5+ years experience working in AWS cloud environment\nPython: 5+ years of hands-on experience of development with Python\nWorkflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem-solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers.\nReporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)\nExperience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms.","Airflow, data engineering, Aws Cloud, Tableau, Data Analytics, Apache, Sql, Python"
Senior Data Engineer,Egon Zehnder,8-13 Years,,Gurugram,Information Technology,"Job Summary\nAs a Data Engineer, you will be responsible for establishing and optimizing the flow of data throughout the organization while ensuring its security.\nExecuting end-to-end data pipeline, from designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nThe Data Engineer is expected to help in driving the database architecture and design for Egon Zehnder large-scale, Intranet and Internet-based applications.\nThe Data Engineer should research new tools and technologies and come up with recommendations on how they can be used in Egon Zehnder applications.\nThe Data Engineer will also be expected to actively participate in design and implementation of projects bearing a high degree of technical complexity and/or scalability and performance significance.\nIdentifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy.\nCollaborate cross-functionally with roles such as IT infrastructure, Digital/application development, and Legal to identify and highlight gaps and risks around Cybersecurity and Data Protection such as GDPR.\nExperience & Key Competencies\nEngineering Degree or equivalent.\n3+ years of experience of SQL writing and data modelling skills, with a solid understanding of data technologies including RDBMS, No-SQL databases.\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nProven experience in building ETL/ELT pipelines, preferably from SQL to Azure services\nArchitecture experience of making ETL operations such as Medallion Architecture etc.\nFamiliarity with CI/CD practices for data pipelines and version control using Git\nExperience in integrating new/replacing vendor products in existing ecosystem.\nExperience in migrating data structured, unstructured data to Data lake\nExperience or understanding of performance engineering both at system level and database level.\nExperience or understanding of Data Governance, Data Quality, Data Issue Management.\nWork closely with the implementation teams of the various product lines to ensure that data architectural standards and best practices are being followed consistently across all Egon Zehnder applications\nEnsure compliance with all regulations, policies, and procedures.\nEscalate issues/risks pro-actively to appropriate stakeholders.\nRegularly communicate status and challenges to team members and management.\nSelf-driven with keenness to master, suggest and work with different technologies & toolsets.\nExcellent communication skills and interpersonal skills suitable for a diverse audience with ability to communicate in a positive friendly and effective manner with technical or non-technical users/customers\nExcellent and resourceful problem-solving skills, adaptable and willingness to learn.\nGood analysis skills - to be able to join the dots across multiple applications and interfaces between them.\nSkill Set\nExperience with one or more of the following databases: SQL Server, MySQL, PostgreSQL, Oracle, Couch-base, Redis, Elastic Search or other NoSQL technologies.\nExperience in Data factory and Data Lake technologies.\nRich Experience in data modelling techniques and creating various data models.\nExperience in Azure cloud services and architecture patterns.\nUnderstanding of RESTful APIs for data distribution.\nUnderstanding of deployment architecture and infrastructure in both on-prem and cloud hosting environments\nExcellent oral and written communication with an ability to articulate complex systems to multiple teams.\nSelf-motivation and the ability to work under minimal supervision\nBenefits Highlights:\n5 Days working in a Fast-paced work environment\nWork directly with the senior management team\nReward and Recognition\nEmployee friendly policies\nPersonal development and training\nHealth Benefits, Accident Insurance\nPotential Growth for you\nWe will nurture your talent in an inclusive culture that values diversity. You will be doing regular catchups with your Manager who will act as your career coach and guide you in your career goals and aspirations.","Data Management, Data Quality, Version Control, postgresql, mysql, Sql"
Data Engineer - MDM/PIM/Syndigo/Attacama/Informatica,Reflections Info Systems,6-11 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru",IT Management,"Responsibilities include:\nWork as part of a team to develop Data and Analytics solutions.\nParticipate in the development of cloud data warehouses, data as a service, business intelligence solutions\nAbility to provide solutions that are forward-thinking in data integration.\nDeliver a quality product.\nDeveloping Modern Data Warehouse solutions using Azure or AWS Stack\nCertifications :\nBachelor s degree in computer science & engineering or equivalent demonstrable experience\nDesirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.\nPrimary Skills :\n6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions\nProgramming experience in Scala or Python, SQL\nMin 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks\nWorking knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse\nDemonstrated analytical and problem-solving skills\nExcellent written and verbal skills (English)\nSecondary Skills :\nFamiliar with Agile Practices\nFamiliar with Version control platforms GIT, CodeCommit etc.\nProblem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.\nOwnership\nProactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
Aws Data Engineer,SRS Infoway,8-10 Years,,"Ahmedabad, Bengaluru, Chennai",Information Technology,"Looking for AWS & MuleSoft developers with expertise in ETL, APIs, AWS Glue, Lambda, SNS/SQS, and RDS for cloud and event-driven architecture. Must be available during North America hours and capable of managing vendor interactions. Strong integration skills required.","Manufacturing Industry, Aws Lambda, Sqs, Sns, AWS Glue, Amazon Rds"
Senior Azure Data Engineer,Mindera,7-12 Years,,Bengaluru,Software,Job description\nExperience in distributed computing (spark) and software development.\nExperience in spark-scala.\nExperience in Data Engineering.\nExperience in Python\nExperience of working on Cloud environments (Azure).,"Azure, Python, Sql"
Data Engineer II,Tekion,3-5 Years,,Chennai,Software,"Be part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.\nShould be able to design (high level / low level) software solutions for the new requirements.\nCoding independently and with other team members with proper software industry standard best practices.\nCollaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.\nQualifications:\nB.E/MTech in computer science\n3 - 5 yearsof relevant work experience.\nExperience in building scalable products with preferably big data.\nExcellentPythoncoding skills (Mandatory)\nExperience inApache spark, Data Lakeand other Big data technologies.\nExperience in either Data Warehouses or Relational Database is mandatory.\nExperience inAWScloud\nMandatory Skills: Python , Spark","Spark, Data Lake, Apache Spark, Data Engineer, Python, Aws Cloud"
Data Engineer Supervisor/Manager,Ford,12-20 Years,,Chennai,Automotive,"We are seeking an experienced Manager to lead a team responsible for the development and maintenance of our Connected Vehicle enterprise data and data products. The ideal candidate will have a strong technical background in data and/or software engineering, along with proven leadership and management skills. This role requires the ability to design and code streaming solutions, prioritize team tasks, make timely decisions, run results-focused meetings, and guide the team to deliver high-quality results. The leader must be knowledgeable in data governance, customer consent, and security standards.\nResponsibilities:\nLead and mentor a high-performing team of local and remote engineers.\nPrioritize team workload, allocate tasks effectively, and ensure team members have the resources to succeed.\nProvide technical expertise and guidance to the team.\nEvaluate and mentor adherence to coding standards, best practices, and architectural guidelines.\nOversee the design, development, maintenance, scalability, reliability, and performance of the connected vehicle data platform pipelines and architecture. Contribute to the long-term strategic direction of the Connected Vehicle Data Platform with a focus on enterprise use.\nCommunicate decisions effectively and transparently to internal and external customers.\nAccurately and routinely track all prioritized work in JIRA to support both financial delivery tracking\nEnforce and ensure data quality, data governance, and security standards.\nLead implementation and delivery of various business customers requests and logic into the data assets with optimized design and code development.\nIdentify and consolidate common tasks across teams to improve efficiency and reduce redundancy.\nStay updated on industry trends and emerging technologies to inform technical decisions.\nQualifications Required:\nBachelor's degree in computer science, Information Technology, Information Systems, or Data Analytics.\n5 years of progressive responsibilities in a complex streaming data environment.\n5+ years experience leading a software/data engineering team.\n5+ years of experience in Big Data Environments or expertise with Big Data tools.\nExpertise in Google Cloud Platform and Services for End-to-End Data Engineering\nExpert knowledge and hands on experience in DevOps and SDLC.\nMonitor and optimize cost and compute for processes in GCP technologies (e.g., BigQuery, Dataflow, Cloud Run, DataProc).\nManage and scale serverless applications and clusters, optimizing resource utilization, and implementing monitoring and logging strategies.\nExpertise in streaming technologies (Kafka, Pub/Sub) and OpenShift, managing high-throughput topics, message ordering, and ensuring data consistency and durability.\nEven better you may have:\nExpertise in other public cloud environments: Amazon Web Services, Microsoft Azure, etc.\nHands on experience on AI Engineering","Devops, data engineering, Gcp, Kafka, DataFlow, AWS"
Senior Data Engineer,Ciklum,8-13 Years,,Chennai,Information Technology,"About the Role\nAs a Senior Data Engineer, you will become part of a cross-functional development team focused on engineering the data experiences of tomorrow. This role is ideal for someone passionate about scalable data systems, eager to solve complex problems, and ready to work in a dynamic environment.\nResponsibilities\nBuild and prototype scalable data pipelines to handle increasing data volume and complexity.\nDevelop and maintain API integrations for efficient data ingestion and transformation.\nWrite reusable, testable, and efficient Python code for data integration tasks.\nUse SQL and Python to extract insights from data in collaboration with business stakeholders.\nManage multiple concurrent projects, adapting to shifting priorities in a fast-paced setting.\nSolve complex challenges creatively to continuously improve data strategies and systems.\nContribute to the design, codebase, configurations, and documentation for components handling data ingestion, real-time streaming, batch processing, ETL, and storage.\nContinuously enhance engineering infrastructure by identifying gaps and improving platform robustness, maintainability, and speed.\nCollaborate with engineering teams to ensure that solutions meet customer expectations in functionality, performance, scalability, and reliability.\nTake end-to-end responsibility, contributing across development, QA, and DevOps roles.\nPartner with business analysts and data scientists to understand their use cases and support them effectively.\nParticipate in unit activities and community-building initiatives, contribute to conferences, and promote best practices.\nSupport sales efforts by participating in customer meetings and providing technical insights on digital services.\nDemonstrate understanding and hands-on application of cloud infrastructure design and implementation.\nRequirements\nProven experience as a Data Engineer or in a similar role with a focus on data integration and management.\nStrong programming skills in Python, particularly in API interactions and automation.\nSolid foundation in SQL and relational database design.\nStrong algorithmic thinking and problem-solving skills.\nEffective communication and collaboration abilities in a team environment.\nHands-on experience with designing and maintaining data ingestion, transformation, and loading pipelines across various storage systems.\nExposure to data streaming tools and batch processing frameworks.\nAbility to troubleshoot and improve engineering infrastructure and deployment environments.\nEnd-to-end ownership mindset, including development, testing, and DevOps.\nExperience working with stakeholders across business and technical functions.\nActive contribution to team knowledge sharing, technical community engagement, and continuous learning.\nUnderstanding of cloud infrastructure and its components (e.g., storage, compute, networking).\nDesirable\nExperience with cloud platforms such as AWS, Google Cloud Platform, or Azure.\nFamiliarity with data warehousing and ETL techniques.\nProficiency with version control systems like Git.","SQL and Python, cloud platforms, ETL methodologies, Google Cloud Platform, Data Engineer, Azure, AWS"
Senior AWS Data Engineer,iLink Digital,5-10 Years,,Bengaluru,Financial Services,"We are seeking a highly skilled Data Engineer with5-7 years of experience to develop, sustain, troubleshoot, and monitor data pipelines.The ideal candidate should be proficient in Python and SQL, with expertise in databases including Redshift, Aurora, and MySQL.This role requires a strong analytical mindset, problem-solving skills, and the ability to work collaboratively in a dynamic environment.\nKey Responsibilities:\nDevelop, optimize, and maintain data pipelines to ensure smooth data flow.\nTroubleshoot and resolve issues related to data pipelines and database performance.\nMonitor data systems to ensure reliability, accuracy, and efficiency.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nTechnical Skills Required:\nProgramming: Proficient in Python and SQL\nDatabases: Strong knowledge of Redshift, Aurora and MySQL\nData Engineering: Experience in building and maintaining scalable data solutions","aurora, python, Redshift, mysql, Sql, data engineering"
Data Engineer Supervisor,Ford,9-13 Years,,Chennai,Automotive,"We are seeking an experienced Manager to lead a team responsible for the development and maintenance of our Connected Vehicle enterprise data and data products. The ideal candidate will possess a strong technical background in data and/or software engineering, coupled with proven leadership and management skills.\nThis role requires the ability to design and code streaming solutions, prioritize team tasks, make timely decisions, run results-focused meetings, and guide the team to deliver high-quality results. The successful leader must be knowledgeable in data governance, customer consent, and security standards.\nQualifications Required:\nBachelor's degree in Computer Science, Information Technology, Information Systems, or Data Analytics.\n5 years of progressive responsibilities in a complex streaming data environment.\n5+ years experience leading a software/data engineering team.\n5+ years of experience in Big Data Environments or expertise with Big Data tools.\nExpertise in Google Cloud Platform and Services for End-to-End Data Engineering.\nExpert knowledge and hands-on experience in DevOps and SDLC.\nAbility to monitor and optimize cost and compute for processes in GCP technologies (e.g., BigQuery, Dataflow, Cloud Run, DataProc).\nExperience managing and scaling serverless applications and clusters, optimizing resource utilization, and implementing monitoring and logging strategies.\nExpertise in streaming technologies (Kafka, Pub/Sub) and OpenShift, managing high-throughput topics, message ordering, and ensuring data consistency and durability.\nEven better you may have:\nExpertise in other public cloud environments: Amazon Web Services, Microsoft Azure, etc.\nHands-on experience in AI Engineering.\nResponsibilities:\nLead and mentor a high-performing team of local and remote engineers.\nPrioritize team workload, effectively allocate tasks, and ensure team members have the necessary resources to succeed.\nProvide technical expertise and guidance to the team; evaluate and mentor adherence to coding standards, best practices, and architectural guidelines.\nOversee the design, development, maintenance, scalability, reliability, and performance of the connected vehicle data platform pipelines and architecture.\nContribute to the long-term strategic direction of the Connected Vehicle Data Platform with a focus on enterprise use.\nCommunicate decisions effectively and transparently to internal and external customers.\nAccurately and routinely track all prioritized work in JIRA to support financial delivery tracking.\nEnforce and ensure data quality, data governance, and security standards.\nLead implementation and delivery of various business customer requests and logic into the data assets with optimized design and code development.\nIdentify and consolidate common tasks across teams to improve efficiency and reduce redundancy.\nStay updated on industry trends and emerging technologies to inform technical decisions.","Architecture, Gcp, Cloud, Data Governance, JIRA, Sdlc, data engineering"
Senior Data Engineer,ThoughtFocus,5-8 Years,,Hyderabad,Information Technology,"Key Responsibilities:\nDatabase Administration Maintenance\nInstall, configure, and maintain database management systems (DBMS) such as MySQL, PostgreSQL, SQL Server, Oracle, or MongoDB.\nEnsure database security, backup, and disaster recovery strategies are in place.\nMonitor database performance and optimize queries, indexing, and storage.\nApply patches, updates, and upgrades to ensure system stability and security.\nDatabase Design Development\nDesign and implement database schemas, tables, and relationships based on business requirements.\nDevelop and optimize stored procedures, functions, and triggers.\nImplement data partitioning, replication, and sharding strategies for scalability.\nPerformance Tuning Optimization\nAnalyze slow queries and optimize database performance using indexing, caching, and tuning techniques.\nConduct database capacity planning and resource allocation.\nMonitor and troubleshoot database-related issues, ensuring minimal downtime.\nSecurity Compliance\nImplement role-based access control (RBAC) and manage user permissions.\nEnsure databases comply with security policies, including encryption, auditing, and GDPR/HIPAA regulations.\nConduct regular security assessments and vulnerability scans.\nCollaboration Automation\nWork closely with developers, system administrators, and DevOps teams to integrate databases with applications.\nAutomate database management tasks using scripts and tools.\nDocument database configurations, processes, and best practices.\nRequired Skills Qualifications:\nExperience:4+ years of experience in database administration, engineering, or related fields.\nEducation:Bachelor s or Master s degree in Computer Science, Information Technology, or related disciplines.\nTechnical Skills:\nStrong knowledge of SQL and database optimization techniques.\nHands-on experience with at least one major RDBMS (MySQL, PostgreSQL, SQL Server, Oracle).\nExperience with NoSQL databases (MongoDB, Cassandra, DynamoDB) is a plus.\nProficiency in database backup, recovery, and high availability solutions (Replication, Clustering, Mirroring).\nFamiliarity with scripting languages (Python, Bash, PowerShell) for automation.\nExperience with cloud-based database solutions (AWS RDS, Azure SQL, Google Cloud Spanner).\nPreferred Qualifications:\nExperience with database migration and cloud transformation projects.\nKnowledge of CI/CD pipelines and DevOps methodologies for database management.\nFamiliarity with big data technologies like Hadoop, Spark, or Elasticsearch.","Stored procedures, Database Design, Performance Tuning, RDBMS, MySQL, Automation"
Sr. Data Engineer,E Zest,3-7 Years,,Bengaluru,Information Technology,"Roles & responsibilities\nData Architecture Design: Designing and implementing data architectures that support data integration, transformation, and storage, ensuring scalability and high performance\nData Pipeline Development: Building and maintaining data pipelines to extract, transform, and load (ETL) data from various sources into data warehouses or data lakes\nBig Data Technologies: Working with big data technologies like Hadoop, Spark, and Kafka to process and manage large volumes of data efficiently\nData Modeling: Designing data models and schemas to support business intelligence, analytics, and reporting requirements\nData Quality and Governance: Implementing data quality checks and data governance processes to ensure data accuracy, consistency, and compliance with data policies\nPerformance Optimization: Identifying performance bottlenecks in data processing workflows and implementing optimizations to improve data pipeline efficiency\nData Security: Implementing data security measures to protect sensitive data and ensure compliance with data privacy regulations","Performance Optimization, Data Architecture Design, Data Pipeline Development, Data Modeling, Big Data Technologies, Data Security"
Data Engineer II,Trimble,3-6 Years,,Chennai,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.\nKey Responsibilities:\nDesign and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.\nDevelop, optimize, and maintain data pipelines using AWS services.\nImplement robust ETL/ELT processes and event-driven data ingestion.\nEstablish and enforce data governance policies, ensuring data quality, security, and compliance.\nOptimize cloud resources for performance, availability, and cost-efficiency.\nPartner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.\nIdentify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.\nOur current tech-stack:\nAWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.\nDevOps: Cloudformation, Terraform, Git, CodeBuild\nDatabase: Redshift, PostgreSQL, DynamoDB, Athena\nLanguage: Bash, Python, SQL\nQualifications:\nBachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.\nExpertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.\nExtensive experience indata and cloud engineeringroles.\nExpertise inAWSplatforms, including data services.\nStrong competence inETL processes,data warehousing, and big data technologies.\nAdvanced skills inscripting,Python,SQL, and infrastructure automation tools.\nFamiliarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).\nExperience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,Overview:\nWe are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.\nKey Responsibilities:\nProvide support and expertise in the domain of Data Engineer.\nCollaborate with cross-functional teams to achieve business goals.\nEnsure timely delivery of services and maintain high-quality standards.\nRequired Qualifications:\nProven experience in a relevant field or position.\nStrong understanding of the responsibilities and tools associated with the role.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nCertifications or training relevant to Data Engineer.\nExperience working in a fast-paced environment or large organizations.,"data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Data Engineer,Future Focus Infotech,4-9 Years,INR 4 - 9 LPA,"Navi Mumbai, Mumbai City, Mumbai",Information Technology,Overview:\nWe are seeking a highly motivated and detail-oriented individual to join our team as a Data Engineer. This role requires a dynamic professional who can adapt to evolving business needs and drive value through their expertise.\nKey Responsibilities:\nProvide support and expertise in the domain of Data Engineer.\nCollaborate with cross-functional teams to achieve business goals.\nEnsure timely delivery of services and maintain high-quality standards.\nRequired Qualifications:\nProven experience in a relevant field or position.\nStrong understanding of the responsibilities and tools associated with the role.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nCertifications or training relevant to Data Engineer.\nExperience working in a fast-paced environment or large organizations.,"data engineering, Data Pipeline, Big Data, Data Engineer, Sql, Etl"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Delhi, Bengaluru, Chennai",Software,"Key Responsibilities:\n- Design, develop, and maintain data pipelines using Python, SQL, and Kedro\n- Implement serverless solutions using AWS Lambda and Step Functions\n- Develop and manage data workflows in Azure and AWS cloud environments\n- Create integrations between data systems and Power Platform (Power Apps, Power Automate)\n- Design, develop, and maintain APIs for data exchange and integration\n- Implement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\n- Optimize data storage and retrieval processes for improved performance\n- Collaborate with cross-functional teams to understand data requirements and provide solutions\n- API Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Database Design, data engineering, Cloud Services, Data Extraction, Azure, Sql, Python"
AWS Data Engineer,IDESLABS,4-7 Years,,Pune,"Recruiting, Staffing Agency","Job description\nThe Data Engineer will expand and optimize the data and data pipeline architecture, as well as optimize data flow and collection for cross functional teams\nThe Data Engineer will perform data architecture analysis, design, development and testing to deliver data applications, services, interfaces, ETL processes, reporting and other workflow and management initiatives\nThe role also will follow modern SDLC principles, test driven development and source code reviews and change control standards in order to maintain compliance with policies\nThis role requires a highly motivated individual with strong technical ability, data capability, excellent communication and collaboration skills including the ability to develop and troubleshoot a diverse range of problems\nDesign and develop enterprise data / data architecture solutions using Hadoop and other data technologies like Spark, Scala","Data Management, Sdlc, Scala, Data Architecture, Spark"
Data Engineer II,Trimble,3-6 Years,,Hyderabad,Information Technology,"TheData Engineerwill lead the design and architecture of AWS-based data solutions, develop end-to-end data pipelines, manage data lakes, and optimize data platforms for performance and scalability. Responsibilities include writing and testing Python, SQL or other code, conducting code reviews, implementing end to end ETL/ELT processes, enforcing data governance policies, and driving innovation in data engineering practices. This role also involves collaboration with cross-functional teams, aligning with stakeholders on technical requirements, providing technical leadership, and mentoring team members to enhance overall team efficiency.\nKey Responsibilities:\nDesign and implement scalable data solutions on AWS, including data lakes, warehouses, and streaming systems.\nDevelop, optimize, and maintain data pipelines using AWS services.\nImplement robust ETL/ELT processes and event-driven data ingestion.\nEstablish and enforce data governance policies, ensuring data quality, security, and compliance.\nOptimize cloud resources for performance, availability, and cost-efficiency.\nPartner with cross-functional teams to gather requirements and deliver comprehensive cloud-based solutions.\nIdentify opportunities to enhance systems, processes, and technologies while troubleshooting complex technical challenges.\nOur current tech-stack:\nAWS: Glue, Lambda, Step Function, Batch, ECS, Quicksight, Machine Learning, Sagemaker, etc.\nDevOps: Cloudformation, Terraform, Git, CodeBuild\nDatabase: Redshift, PostgreSQL, DynamoDB, Athena\nLanguage: Bash, Python, SQL\nQualifications:\nBachelors degree in Computer Science, Engineering, or related field. Masters degree preferred.\nExpertise inAWSplatforms, including data services. Basic knowledge ofAzureis preferred.\nExtensive experience indata and cloud engineeringroles.\nExpertise inAWSplatforms, including data services.\nStrong competence inETL processes,data warehousing, and big data technologies.\nAdvanced skills inscripting,Python,SQL, and infrastructure automation tools.\nFamiliarity withcontainerization(e.g., Docker) and orchestration (e.g., Kubernetes).\nExperience withdata visualization tools(e.g., QuickSight) is a plus.","Git, python, Terraform, Sql, aws, Etl"
Lead Data Engineer,NXP Semiconductors,7-12 Years,,Bengaluru,Semiconductor,"Proven experience as a Data Engineer\nHands on experience in ETL design and development concepts (7+ years)\nExperience with AWS and Azure cloud platforms and their data service offerings\nProficiency in SQL, PySpark, Python\nExperience with GitHub, GitLab, CI/CD\nKnowledge of advanced analytic concepts including AI/ML\nStrong problem-solving skills and ability to work in a fast-paced and collaborative environment\nExcellent oral and written communication skills\nPreferred Skills Qualifications:\nExperience with Agile / DevOps\nProficiency in SQL (Databricks, Teradata)\nExperience with DBT\nExperience with Dataiku platform, including administration","data engineering, Azure, Sql, Python, Etl, AWS"
Lead Data Engineer-Databricks,Anblicks Solutions,10-14 Years,,"Ahmedabad, Hyderabad",Cloud Data Services,"Role & responsibilities\nLead Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise inAzure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake)along with big data processing, real-time analytics, and cloud data integration and Team Leading Experience.\nKey Responsibilities:\n1. Data Pipeline Development & ETL/ELT\nDesign and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.\nImplement ETL/ELT workflows for structured and unstructured data processing.\nOptimize data ingestion, transformation, and storage strategies.\n2. Cloud Data Architecture & Integration\nDevelop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).\nWork with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.\n3. Database Management & Optimization\nDesign and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).\nOptimize SQL queries and indexing strategies for performance.\nImplement data partitioning, compression, and caching for efficiency.\n4. Data Governance, Security & Compliance\nEnsure data quality, lineage, and governance with tools like Purview.\nImplement role-based access control (RBAC), encryption, and security policies.\nEnsure compliance with GDPR, HIPAA, and ISO 27001 regulations.\n5. Monitoring & Performance Tuning\nUse Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.\nAutomate data pipeline testing and validation.\n6. Collaboration & Documentation\nDocument data models, pipeline architectures, and data workflows.\nImmediate joiners are preferred.","data models, log analytics, Data Processing, Azure, Database Management, elt"
Lead Data Engineer - Azure Fabric,Kanini Software Solutions,8-10 Years,,"Bengaluru, Chennai, Pune",Software,"We are looking for a Lead Data Engineer with expertise inAzure Fabric, Data Architecture, and ETL Pipelines. The ideal candidate will design and implement scalable data solutions, ensuring efficient data processing, governance, and analytics on the Azure cloud platform.\nKey Responsibilities:\nDesign & develop data pipelines using Azure Fabric, Data Factory, and Synapse Analytics.\nImplement ETL/ELT workflows for structured & unstructured data processing.\nOptimize SQL queries, data modeling, and performance tuning.\nEnsure data security, governance, and compliance using best practices.\nCollaborate with cross-functional teams for data-driven insights and analytics.\nRequired Skills:\nAzure Fabric, Azure Synapse, Azure Data Factory\nETL, SQL, Data Pipeline Design, Data Architecture\nBig Data & Analytics, Data Governance, Performance Optimization\nCI/CD, Azure DevOps, Git\nRole:Data Science & Analytics - Other\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Analytics - Other\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:Any Postgraduate","Architecture, Fabric, data engineering, Data Pipeline, Azure, Sql, Etl"
Lead Data Engineer,Coditas Technologies,7-11 Years,,Pune,Software,"We are looking for a Tech Lead expertise in Advanced Big Data Technology Stack. The person should be a hands-on technical expert in building and deploying applications using Big Data technologies. The person should have progressive experience in building highly scalable distributed systems. The person should have the ability to build a high-performance strong technical team that adheres to the strong quality standard of application development.\nRoles and responsibilities\nWith over 7 yrs. of hands-on experience with Data Technologies\nRequirement analysis and assess the technical feasibility of proposed solutions.\nAct as the technical specialist in designing and recommending architecture for application development/feature development\nWorking on designing the application architecture and estimating effort in delivering features for new requirements\nImplement data ingestion and transform pipeline for analytics and Dashboard reporting for business\nWork in designing large-scale distributed computing applications using tools like Spark, Kafka, Hive, etc.\nPerforming validations, reconciliation, and consolidations for the imported data, Data migration, and data generation.\nEffectively communicate with the Engineering Managers and Stakeholders to set the right expectations.\nTechnical Skills\nMinimum 5 years of progressive experience building solutions in Big Data environments\nHave a strong ability to build robust and resilient data pipelines that are scalable, fault-tolerant, and reliable in terms of data movement\nHands-on experience of Apache Spark with Python for batch and stream processing\nShould know experience in batch and stream data processing\nExposure to working on projects across multiple domains\nHands-on experience in Apache Kafka\nStrong hands-on capabilities in SQL and NoSQL technologies\nHands-on experience with AWS services like S3, DMS, Redshift, Glue, Lambda, Kinesis, MSK, etc. is must have or similar services of either Azure/GCP\nStrong analytical/quantitative skills and comfortable working with very large sets of data.\nExcellent written and verbal communication skills\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate","Airflow, Data Ingestion, Data Pipeline, Data Modeling, Pyspark, Data Architecture, Data Warehousing, Data Governance, Python, Sql, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Gurugram,Software,"Position Summary:\nWe are looking for candidates with hands on experience in Big Data or Cloud Technologies.\nMust have technical Skills\n7 to 10 Years of experience\nExpertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have\nGood and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of PySpark (SparkSQL) Must Have\nGood knowledge of Shell script & Python Good to Have\nGood knowledge of SQL Good to Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nGood Communication Skills Must Have\n*Data Ingestion, Processing and Orchestration knowledge\nRoles & Responsibilities\nLead technical implementation of Data Warehouse modernization projects for Impetus\nDesign and development of applications on Cloud technologies\nLead technical discussions with internal & external stakeholders\nResolve technical issues for team\nEnsure that team completes all tasks & activities as planned\nCode Development\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Aviation, Information Technology, Computers\nPG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Bengaluru,Software,"Position Summary:\nWe are looking for candidates with hands on experience in Big Data or Cloud Technologies.\nMust have technical Skills\n7 to 10 Years of experience\nExpertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have\nGood and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of PySpark (SparkSQL) Must Have\nGood knowledge of Shell script & Python Good to Have\nGood knowledge of SQL Good to Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nGood Communication Skills Must Have\n*Data Ingestion, Processing and Orchestration knowledge\nRoles & Responsibilities\nLead technical implementation of Data Warehouse modernization projects for Impetus\nDesign and development of applications on Cloud technologies\nLead technical discussions with internal & external stakeholders\nResolve technical issues for team\nEnsure that team completes all tasks & activities as planned\nCode Development\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Aviation, Information Technology, Computers\nPG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Lead Data Engineer,Impetus Technologies,8-11 Years,,Pune,Software,"Position Summary:\nWe are looking for candidates with hands on experience in Big Data or Cloud Technologies.\nMust have technical Skills\n7 to 10 Years of experience\nExpertize and hands-on experience on Spark Data Frame, and Hadoop echo system components Must Have\nGood and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of PySpark (SparkSQL) Must Have\nGood knowledge of Shell script & Python Good to Have\nGood knowledge of SQL Good to Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engine like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Good to Have\nAutomation approach - Good to Have\nGood Communication Skills Must Have\n*Data Ingestion, Processing and Orchestration knowledge\nRoles & Responsibilities\nLead technical implementation of Data Warehouse modernization projects for Impetus\nDesign and development of applications on Cloud technologies\nLead technical discussions with internal & external stakeholders\nResolve technical issues for team\nEnsure that team completes all tasks & activities as planned\nCode Development\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Aviation, Information Technology, Computers\nPG:M.Tech in Any Specialization","HDFS, Hive, Hadoop, Pyspark, Spark, Big Data, Data Warehousing, Python, Sql, Etl, AWS"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Delhi, Hyderabad",Software,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer with BI and DWH - Ikrux,Ikrux Solutions (Opc) Private Limited,15-16 Years,,"Bengaluru, Pune",Information Technology,"Job description\nData Analytics and AIQuality AssuranceArtificial IntelligenceApplication Development and Software EngineeringCloud EngineeringCapability BuildingDevopsCRMSynapse-360\nNeed different solutions\nIkrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.\nindustries Overview\nBanking, financial services and insurance (BFSI)FintechMediaGlobal Capability CentersTechnologyHealthcareE-CommerceOil and Gas\nNeed different solutions\nIkrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.\nJob Category:AWS GlueAzureBusiness IntelligenceData EngineerData IngestionData ModellingData WarehousingPower BISSISTableau\nJob Type:Full Time\nJob Location:BangalorePune\nWe are seeking a highly experienced BI Architect and Developer with 6-15+ years of expertise in business intelligence, data visualization, and data engineering. The ideal candidate should hold a Bachelor s degree in Computer Science, Information Systems, Data Science, or a related field. Proficiency in Power BI and Tableau is essential, along with a strong background in ETL processes and tools such as SSIS. Advanced knowledge of SQL Server for query writing and database management is required, complemented by skills in exploratory data analysis using Python and familiarity with the CRISP-DM model.\nThe candidate should have experience working with diverse data models and databases, including Snowflake, Postgres, Redshift, and MongoDB. Expertise in visualization tools like Power BI, QuickSight, Plotly, and Dash is necessary. A strong programming foundation in Python is essential, particularly in data manipulation and analysis using Pandas, NumPy, and PySpark, as well as data serialization with JSON, CSV, and Parquet. Experience with cloud services like AWS S3, AWS Lambda, and Azure SDK is a plus. Proficiency in workflow orchestration tools such as Airflow and automation of ETL pipelines is required. Additional skills in interacting with REST APIs, web scraping, and version control for collaborative projects will be an advantage.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Workflow, Application Development, Json, Automation, Quality Assurance"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Mumbai, Pune",Software,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Bengaluru, Chennai",Software,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer,Mindera,4-9 Years,,Bengaluru,Software,"Job description\nWe are looking for an experienced Data Engineer to join our team.\nHere at Mindera, we are continuously developing a fantastic team and would love for you to join us.\nAs a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.\nThis is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.\nYou re great at\nPython\nCloud - AWS/GCP\nSQL\nAirflow\nData testing\nSnowflake","Gcp, Python, Sql, Aws"
Data Engineer,Mindera,4-9 Years,,Bengaluru,Software,"Job description\nWe are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you ll create algorithms and conduct statistical analysis. Overall, you ll strive for efficiency by aligning data systems with business goals.\nTo succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.\nIf you are detail-oriented, with excellent organizational skills and experience in this field, we d like to hear from you.\nImplement/support new data solutions in datalake/datawarehouse built on snowflake.\nDevelop and design data pipelines using python, spark and snowflake.\nImplement infrastructure as code using cloudformation and terraform.","Gcp, Python, Sql, Aws"
Data Engineer,Mindera,4-9 Years,,Bengaluru,Software,"Job description\nRequirements\nImplement/support new data solutions in datalake/datawarehouse built on snowflake.\nDevelop and design data pipelines using python, spark and snowflake.\nImplement infrastructure as code using cloudformation and terraform.\nDesign and Implement Continuous Integration/Continuous Deployments pipelines.\nPerform Data Modelling using downstream requirements.\nDevelop transformation scripts using advanced SQL and DBT.\nCreate reports/dashboard using business intelligence tools such as looker and tableau.\nWrite test cases/scenarios to ensure incident free production release.\nCollaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.\nIdentify areas of improvements in data pipelines, snowflake, infrastructure etc Conduct peer reviews of the code.\nDebug production and development issues and provide support to colleagues where necessary.\nPerform data quality checks to ensure quality of the data exposed to the end users.\nPerform production deployments and perform a post-production support and validation (We follow a You build, you run it philosophy)\nBuild strong relationships with team, peers and stakeholders.\nContributes to overall data platform implementation.\nProficient in SQL/Python/Spark\nExposure to DBT would be preferable\nExperience in AWS services such as Glue, Lambda, S3, DynamoDB, RDS, Kinesis, ECS/Fargate.\nExperience working with modern data platforms such as redshift or snowflake\nExperience working with BI tools such as looker and tableau\nExperience working with docker\nProficient in data modelling.","Gcp, Python, Sql, Aws"
Data Engineer- MDM/PIM/Atacama/Informatica,Reflections Info Systems,6-10 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Chennai",IT Management,"Responsibilities include:\nWork as part of a team to develop Data and Analytics solutions.\nParticipate in the development of cloud data warehouses, data as a service, business intelligence solutions\nAbility to provide solutions that are forward-thinking in data integration.\nDeliver a quality product.\nDeveloping Modern Data Warehouse solutions using Azure or AWS Stack\nCertifications :\nBachelor s degree in computer science & engineering or equivalent demonstrable experience\nDesirable to have Cloud Certifications in Data, Analytics, or Ops/Architect space.\nPrimary Skills :\n6+ Yrs experience as a Data Engineer, who played a key/lead role in implementing one or two large data solutions\nProgramming experience in Scala or Python, SQL\nMin 1+ years experience in MDM/PIM Solution Implementation with tools like Ataccama, Syndigo, Informatica\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Snowflake\nMin 2+ years experience in Data Engineering Pipelines, Solutions implementation in Databricks\nWorking knowledge of with some of these AWS and Azure Services like S3, ADLS Gen2, AWS Redshift, AWS Glue, Azure Data Factory, Azure Synapse\nDemonstrated analytical and problem-solving skills\nExcellent written and verbal skills (English)\nSecondary Skills :\nFamiliar with Agile Practices\nFamiliar with Version control platforms GIT, CodeCommit etc.\nProblem Solving - Think of various options to solve the problem. Focus on completion, and not on duration spent.\nOwnership\nProactive rather than reactive","Business intelligence, Version Control, Git, Scala, Informatica, Sql, Python"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Delhi, Hyderabad",Software,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer with BI and DWH - Ikrux,Ikrux Solutions (Opc) Private Limited,15-16 Years,,"Bengaluru, Pune",Information Technology,"Job description\nData Analytics and AIQuality AssuranceArtificial IntelligenceApplication Development and Software EngineeringCloud EngineeringCapability BuildingDevopsCRMSynapse-360\nNeed different solutions\nIkrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.\nindustries Overview\nBanking, financial services and insurance (BFSI)FintechMediaGlobal Capability CentersTechnologyHealthcareE-CommerceOil and Gas\nNeed different solutions\nIkrux s scalable solutions adapt to your needs, ensuring robust protection without compromise.\nJob Category:AWS GlueAzureBusiness IntelligenceData EngineerData IngestionData ModellingData WarehousingPower BISSISTableau\nJob Type:Full Time\nJob Location:BangalorePune\nWe are seeking a highly experienced BI Architect and Developer with 6-15+ years of expertise in business intelligence, data visualization, and data engineering. The ideal candidate should hold a Bachelor s degree in Computer Science, Information Systems, Data Science, or a related field. Proficiency in Power BI and Tableau is essential, along with a strong background in ETL processes and tools such as SSIS. Advanced knowledge of SQL Server for query writing and database management is required, complemented by skills in exploratory data analysis using Python and familiarity with the CRISP-DM model.\nThe candidate should have experience working with diverse data models and databases, including Snowflake, Postgres, Redshift, and MongoDB. Expertise in visualization tools like Power BI, QuickSight, Plotly, and Dash is necessary. A strong programming foundation in Python is essential, particularly in data manipulation and analysis using Pandas, NumPy, and PySpark, as well as data serialization with JSON, CSV, and Parquet. Experience with cloud services like AWS S3, AWS Lambda, and Azure SDK is a plus. Proficiency in workflow orchestration tools such as Airflow and automation of ETL pipelines is required. Additional skills in interacting with REST APIs, web scraping, and version control for collaborative projects will be an advantage.\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Workflow, Application Development, Json, Automation, Quality Assurance"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Mumbai, Pune",Software,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Azure Data Engineer (Power Platform)- Priniciple,Systechcorp Inc,8-13 Years,,"Bengaluru, Chennai",Software,"Key Responsibilities:\nDesign, develop, and maintain data pipelines using Python, SQL, and Kedro\nImplement serverless solutions using AWS Lambda and Step Functions\nDevelop and manage data workflows in Azure and AWS cloud environments\nCreate integrations between data systems and Power Platform (Power Apps, Power Automate)\nDesign, develop, and maintain APIs for data exchange and integration\nImplement solutions to extract data from APIs and store in databases (Dataverse & PostgreSQL).\nOptimize data storage and retrieval processes for improved performance\nCollaborate with cross-functional teams to understand data requirements and provide solutions\nAPI Integration and data extraction from Sharepoint\nRequired Skills and Experience:\nExpert in Azure Data Engineer role, able to build solutions in Power Platform environment, AWS cloud and Azure cloud services.\nGood Knowledge on integration like Integration using API, XML with ADF, Logic App,\nAzure Functions, AWS Step functions, AWS Lambda Functions, etc\nStrong proficiency in Python, Pyspark (especially Kedro Framework) and SQL.\nHaving knowledge and experience in Scala, Java and R would be a good to have skill.\nExperience with Kedro framework for data engineering pipelines\nExpertise in AWS services, particularly Lambda and Step Functions\nProven experience with Power Platform (Power Apps, Power Automate) and Dataverse.\nStrong understanding of API development and integration\nExperience in database design and management\nExcellent problem-solving and communication skills","Data Pipeline, Azure Cloud, Aws Lambda, Scala, API development and integration, Python, Sql"
Data Engineer,Saxon Global INC,4-9 Years,,Bengaluru,"Information Technology, Software, Information Services","Job description\nContributes to the design of information infrastructure, and data management processes\nDevelop good understanding of how data will flow & stored through an organization across multiple applications such as CRM, Broker & Sales tools, Finance, HR etc\nRequired Candidate profile\nMin 3 yrs using Azure(preferred), Python, Kafka, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc.\nshould have L2 Support, L3 Support.","Data Validation, Azure, python, Data Management, Kafka, Data Engineer"
Lead Engineer (Data Engineer),Velotio Technologies,7-10 Years,,Bengaluru,Software,"ob description\nDesign, develop, and maintain robust and scalable data pipelines that ingest, transform, and load data from various sources into data warehouse.\nCollaborate with business stakeholders to understand data requirements and translate them into technical solutions.\nImplement data quality checks and monitoring to ensure data accuracy and integrity.\nOptimize data pipelines for performance and efficiency.\nTroubleshoot and resolve data pipeline issues.\nStay up-to-date with emerging technologies and trends in data engineering.\nQualifications\nBachelor s or Master s degree in Computer Science, Engineering, or a related field.\n7+ years of experience in data engineering or a similar role.\nStrong proficiency in SQL and at least one programming language (e.g., Python, Java).\nExperience with data pipeline tools and frameworks\nExperience with cloud-based data warehousing solutions (Snowflake).\nExperience with AWS Kinesis, SNS, SQS\nExcellent problem-solving and analytical skills.\nStrong communication and interpersonal skills.\nDesired Skills & Experience:\nData pipeline architecture\nData warehousing\nETL (Extract, Transform, Load)\nData modeling\nSQL\nPython or Java or Go\nCloud computing\nBusiness intelligence","Business Intelligence, Java, Cloud Computing, Sql, Python"
Lead/Staff Data Engineer,Tableau Software,8-13 Years,,Hyderabad,Information Technology,"About the Role:\nData Engineering at the Heart of AI Marketing\nJoin theMarketing AI/ML Algorithms and Applicationsteam a powerhouse team within Salesforce s marketing organization, directly influencing how wemarket our vast product portfolio to a global customer base (including 90% of the Fortune 500).\nIn this role, you'll:\nDrive state-of-the-art ML solutionsthat supercharge our internal marketing platforms\nHelp shapecustomer engagement strategiesat massive scale\nWork on AI/ML-powered personalization, propensity modeling, recommender systems & more\nBring yourdata engineering expertiseto fuel real business impact at global scale\nWhat You'll Do\nArchitect and implement scalable data platforms & pipelinesfor ML model development and production\nBuild and manage Feature Storesto enable predictive modeling and analytics\nDesignreal-time and batch ETL systems, integrating complex customer data from multiple sources\nCollaborate withAnalytics, Data Science, and Marketing teamsto deliver actionable insights\nCreate robust data infrastructure formodel deployment, monitoring & tuning\nEnsuredata quality, governance, and compliance, aligned with Salesforces AI Trust framework\nLead and mentor other engineers, promotinginnovation and continuous learning\nWhat You'll Need\n8+ yearsof hands-on experience building enterprise-scale data platforms in the cloud\nExpert-level skills inSQL, Python, PySpark/Spark, Hive, Presto, Kafka\nExperience withcloud platforms (AWS, GCP, Azure)and tools likeDBT, Airflow, Snowflake, Redshift\nStrong grasp ofdata modeling, pipelines, and large-scale data architecture\nFamiliarity withAI/ML model pipelines, CDPs, and Customer 360 initiatives\nPassion for enablingdata-driven decision-makingin aB2B marketing context\nBonus: Familiarity withSalesforce productsor CRM data\nWhy This Role Stands Out\nDriveAI-powered marketingat the #1 CRM company in the world\nCollaborate with some of the bestData Scientists and Marketing experts\nInfluence how Salesforceconnects with millions of global customers\nBe part of a team that s pioneering the future ofAgentic AI + Marketing\nOwn your impactnot just as an engineer, but as a leader and innovator\nAccommodations\nIf you require assistance due to a disability applying for open positions please submit a request via thisAccommodations Request Form .","Salesforce, Business Strategy, CRM, Analytics, Monitoring, Predictive Modeling, Data Modeling, Gcp, Sql, Python"
Module Lead Data Engineer/GCP Module Lead Database Engineer,Impetus Technologies,3-6 Years,,"Noida, Indore",Software,"We are looking for GCP Data Engineer and SQL Programmer with good working experience on PostgreSQL, PL/SQL programming experience and following technical skills\n\nPL/SQL and PostgreSQL programming - Ability to write complex SQL Queries, Stored Procedures.\n\nMigration - Working experience in migrating Database structure and data from Oracle to Postgres SQL preferably on GCP Alloy DB or Cloud SQL\n\nWorking experience on Cloud SQL/Alloy DB\n\nWorking experience to tune autovacuum in postgresql.\n\nWorking experience on tuning Alloy DB / PostgreSQL for better performance.\n\nWorking experience on Big Query, Fire Store, Memory Store, Spanner and bare metal setup for PostgreSQL\n\nAbility to tune the Alloy DB / Cloud SQL database for better performance\n\nExperience on GCP Data migration service\n\nWorking experience on MongoDB\n\nWorking experience on Cloud Dataflow\n\nWorking experience on Database Disaster Recovery\n\nWorking experience on Database Job scheduling\n\nWorking experience on Database logging techniques\n\nKnowledge of OLTP And OLAP\n\nDesirable: GCP Database Engineer Certification\n\nOther Skills:-\n\nOut of the Box Thinking\n\nProblem Solving Skills\n\nAbility to make tech choices (build v/s buy)\n\nPerformance management (profiling, benchmarking, testing, fixing)\n\nEnterprise Architecture\n\nProject management/Delivery Capabilty/ Quality Mindset\n\nScope management\n\nPlan (phasing, critical path, risk identification)\n\nSchedule management / Estimations\n\nLeadership skills\n\nOther Soft Skills\n\nLearning ability\n\nInnovative / Initiative\n\n\nDevelop, construct, test, and maintain data architectures\n\nMigrate Enterprise Oracle database from On Prem to GCP cloud autovacuum in postgresql\n\nAbility to tune autovacuum in postgresql.\n\nWorking on tuning Alloy DB / PostgreSQL for better performance.\n\nPerformance Tuning of PostgreSQL stored procedure code and queries\n\nConverting Oracle stored procedure queries to PostgreSQL stored procedures Queries\n\nCreating Hybrid data store with Datawarehouse and No SQL GCP solutions along with PostgreSQL.\n\nMigrate Oracle Table data from Oracle to Alloy DB\n\nLeading the database team\n\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Project Management, Stored Procedures, Disaster Recovery, Scheduling, Data Warehousing, Sql, Data Migration, OLAP, Oracle, Performance Tuning"
"Sr. Associate Technical Consultant, Data Engineer",AHEAD,3-5 Years,,Gurugram,Information Technology,"Data Engineer\n(Internally known as a Sr. Associate Technical Consultant)\nAHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.\nThe Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.\nResponsibilities\nA Data Engineer should be able to build, operationalize and monitor data processing systems\nCreate robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset\nImplement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases\nEngineers and supports data structures including but not limited to SQL and NoSQL databases\nEngineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)\nLeverages the right tools for the right job to deliver testable, maintainable, and modern data solutions\nRespond to customer/team inquiries and assist in troubleshooting and resolving challenges\nWorks with other scrum team members to estimate and deliver work inside of a sprint\nResearch data questions, identifies root causes, and interacts closely with business users and technical resources\nQualifications\n3+ years of professional technical experience\n3+ years of hands-on Data Warehousing\n3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake\n2+ years of programming languages such as Python\n3+ years of experience working in cloud environments (Azure)\n2 years of experience in Redshift\nStrong client-facing communication and facilitation skills\nKey Skills\nPython, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP\nWhy AHEAD\nThrough our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.\nWe fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.\nUSA Employment Benefits include\nMedical, Dental, and Vision Insurance\n401(k)\nPaid company holidays\nPaid time off\nPaid parental and caregiver leave\nPlus more! See benefits https://www.aheadbenefits.com/ for additional details.\nThe compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Azure, python, Spark, Redshift, hadoop"
"Technical Consultant, Data Engineer",AHEAD,3-5 Years,,Gurugram,Information Technology,"Data Engineer (Internally known as a Technical Consultant)\nAHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.\nThe Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.\nResponsibilities:\nA Data Engineer should be able to build, operationalize and monitor data processing systems\nCreate robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset\nImplement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases\nEngineers and supports data structures including but not limited to SQL and NoSQL databases\nEngineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)\nLeverages the right tools for the right job to deliver testable, maintainable, and modern data solutions\nRespond to customer/team inquiries and assist in troubleshooting and resolving challenges\nWorks with other scrum team members to estimate and deliver work inside of a sprint\nResearch data questions, identifies root causes, and interacts closely with business users and technical resources\nQualifications:\n3+ years of professional technical experience\n3+ years of hands-on Data Warehousing\n3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake\n2+ years of programming languages such as Python\n3+ years of experience working in cloud environments (Azure)\n2 years of experience in Redshift\nStrong client-facing communication and facilitation skills\nKey Skills:\nPython, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP\nWhy AHEAD:\nThrough our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.\nWe fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.\nUSA Employment Benefits include\nMedical, Dental, and Vision Insurance\n401(k)\nPaid company holidays\nPaid time off\nPaid parental and caregiver leave\nPlus more! See benefits https://www.aheadbenefits.com/ for additional details.\nThe compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Nosql, Hadoop, Spark, Data Warehouse, Python, Sql"
Big Data Engineer - Xebia,Foray Software,3-6 Years,,Bengaluru,Consulting,"Implementation experience on building large scale data applications from scratch (initial stages)\nProgramming experience in Python or Java\nGood experience of deploying applications on AWS and usage of its services\nMust have experience with Hadoop Distributed File System (HDFS), Amazon Simple Storage Service (S3)\nMust have good experience on SQL\nData organization in Data Lake (experience in Delta Lake or Databricks is added advantage)\nDetailed understanding of Data pipeline creation\nDetailed experience of Data ingestion\nTechno functional experience working with technical team (data engineering / data science) and the business (functional) teams","Delta, Deployment, HDFS, Techno-functional, Hadoop, Data Science, Big Data, Sql, Python, AWS"
Data Engineer OpenData Commercial,Veeva Systems,3-7 Years,,Mumbai,Cloud Data Services,"We are seeking a skilled Data Engineer to design and build reusable and configurable data pipelines using a mix of standard cloud-based and proprietary data platforms. This role involves collaborating with data scientists to develop AI/ML proofs of concept into full implementations, creating documentation, and providing informal training to data analysts. You will also work closely with other engineering and product teams and provide guidance to more junior data engineers.\nWhat You'll Do:\nDesign and build reusable and configurable data pipelines using a mix of standard cloud-based and proprietary data platforms.\nWork with a data scientist to develop AI/ML proofs of concept into full implementations.\nCreate documentation and provide informal training for data analysts on the configuration and use of tools and pipelines.\nWork with other engineering and product teams to understand proprietary platforms and provide input and feedback.\nProvide guidance to more junior data engineers on best practices.\nWork with security teams to ensure that all servers, platforms, and other resources meet security requirements.\nRequirements:\nBS degree in Computer Science, Engineering, or a related subject.\n4+ years of experience in Data Engineering roles.\nExperience developing sophisticated data pipelines in cloud-based environments (e.g., AWS) using scalable data processing tools (e.g., Apache Spark).\nData modeling experience.\nDemonstrated ability to work with others, particularly providing guidance to other data engineers.\nAbility to communicate around complex ideas and topics in English with both technical and non-technical individuals.\nNice to Have:\nFamiliarity with Agile methodologies.\nDevOps skills, especially CI/CD experience.\nConfiguring and maintaining cloud-based cluster computing resources and orchestration systems (e.g., EC2 instances, Kubernetes clusters, Elastic Beanstalk).","Data Pipelines, CI/CD, data engineering, Data Modeling, Apache Spark, AWS"
"Technical Consultant, Data Engineer",AHEAD,3-5 Years,,Gurugram,Information Technology,"Data Engineer (Internally known as a Technical Consultant)\nAHEAD is looking for a Technical Consultant Data Engineer to work closely with our dynamic project teams (both on-site and remotely). This Data Engineer will be responsible for hands-on engineering of Data platforms that support our clients advanced analytics, data science, and other data engineering initiatives. This consultant will build, and support modern data environments that reside in the public cloud or multi-cloud enterprise architectures.\nThe Data Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines using modern Data Engineering tools/architectures as well as design and integration of existing transactional processing systems. As a Data Engineer, you will implement data pipelines to enable analytics and machine learning on rich datasets.\nResponsibilities:\nA Data Engineer should be able to build, operationalize and monitor data processing systems\nCreate robust and automated pipelines to ingest and process structured and unstructured data from various source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset\nImplement custom applications using tools such as Kinesis, Lambda and other cloud native tools as required to address streaming use cases\nEngineers and supports data structures including but not limited to SQL and NoSQL databases\nEngineers and maintain ELT processes for loading data lake (Snowflake, Cloud Storage, Hadoop)\nLeverages the right tools for the right job to deliver testable, maintainable, and modern data solutions\nRespond to customer/team inquiries and assist in troubleshooting and resolving challenges\nWorks with other scrum team members to estimate and deliver work inside of a sprint\nResearch data questions, identifies root causes, and interacts closely with business users and technical resources\nQualifications:\n3+ years of professional technical experience\n3+ years of hands-on Data Warehousing\n3+ years of experience building highly scalable data solutions using Hadoop, Spark, Databricks, Snowflake\n2+ years of programming languages such as Python\n3+ years of experience working in cloud environments (Azure)\n2 years of experience in Redshift\nStrong client-facing communication and facilitation skills\nKey Skills:\nPython, Azure Cloud, Redshift, NoSQL, Git, ETL/ELT, Spark, Hadoop, Data Warehouse, Data Lake, Data Engineering, Snowflake, SQL/RDBMS, OLAP\nWhy AHEAD:\nThrough our daily work and internal groups like Moving Women AHEAD and RISE AHEAD, we value and benefit from diversity of people, ideas, experience, and everything in between.\nWe fuel growth by stacking our office with top-notch technologies in a multi-million-dollar lab, by encouraging cross department training and development, sponsoring certifications and credentials for continued learning.\nUSA Employment Benefits include\nMedical, Dental, and Vision Insurance\n401(k)\nPaid company holidays\nPaid time off\nPaid parental and caregiver leave\nPlus more! See benefits https://www.aheadbenefits.com/ for additional details.\nThe compensation range indicated in this posting reflects the On-Target Earnings (OTE) for this role, which includes a base salary and any applicable target bonus amount. This OTE range may vary based on the candidates relevant experience, qualifications, and geographic location.","snowflake, Nosql, Hadoop, Spark, Data Warehouse, Python, Sql"
Module Lead Data Engineer/GCP Module Lead Database Engineer,Impetus Technologies,3-6 Years,,"Noida, Indore",Software,"We are looking for GCP Data Engineer and SQL Programmer with good working experience on PostgreSQL, PL/SQL programming experience and following technical skills\n\nPL/SQL and PostgreSQL programming - Ability to write complex SQL Queries, Stored Procedures.\n\nMigration - Working experience in migrating Database structure and data from Oracle to Postgres SQL preferably on GCP Alloy DB or Cloud SQL\n\nWorking experience on Cloud SQL/Alloy DB\n\nWorking experience to tune autovacuum in postgresql.\n\nWorking experience on tuning Alloy DB / PostgreSQL for better performance.\n\nWorking experience on Big Query, Fire Store, Memory Store, Spanner and bare metal setup for PostgreSQL\n\nAbility to tune the Alloy DB / Cloud SQL database for better performance\n\nExperience on GCP Data migration service\n\nWorking experience on MongoDB\n\nWorking experience on Cloud Dataflow\n\nWorking experience on Database Disaster Recovery\n\nWorking experience on Database Job scheduling\n\nWorking experience on Database logging techniques\n\nKnowledge of OLTP And OLAP\n\nDesirable: GCP Database Engineer Certification\n\nOther Skills:-\n\nOut of the Box Thinking\n\nProblem Solving Skills\n\nAbility to make tech choices (build v/s buy)\n\nPerformance management (profiling, benchmarking, testing, fixing)\n\nEnterprise Architecture\n\nProject management/Delivery Capabilty/ Quality Mindset\n\nScope management\n\nPlan (phasing, critical path, risk identification)\n\nSchedule management / Estimations\n\nLeadership skills\n\nOther Soft Skills\n\nLearning ability\n\nInnovative / Initiative\n\n\nDevelop, construct, test, and maintain data architectures\n\nMigrate Enterprise Oracle database from On Prem to GCP cloud autovacuum in postgresql\n\nAbility to tune autovacuum in postgresql.\n\nWorking on tuning Alloy DB / PostgreSQL for better performance.\n\nPerformance Tuning of PostgreSQL stored procedure code and queries\n\nConverting Oracle stored procedure queries to PostgreSQL stored procedures Queries\n\nCreating Hybrid data store with Datawarehouse and No SQL GCP solutions along with PostgreSQL.\n\nMigrate Oracle Table data from Oracle to Alloy DB\n\nLeading the database team\n\nRole:Data Engineer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Project Management, Stored Procedures, Disaster Recovery, Scheduling, Data Warehousing, Sql, Data Migration, OLAP, Oracle, Performance Tuning"
Big Data Engineer - Xebia,Foray Software,3-6 Years,,Gurugram,Consulting,"Implementation experience on building large scale data applications from scratch (initial stages)\nProgramming experience in Python or Java\nGood experience of deploying applications on AWS and usage of its services\nMust have experience with Hadoop Distributed File System (HDFS), Amazon Simple Storage Service (S3)\nMust have good experience on SQL\nData organization in Data Lake (experience in Delta Lake or Databricks is added advantage)\nDetailed understanding of Data pipeline creation\nDetailed experience of Data ingestion\nTechno functional experience working with technical team (data engineering / data science) and the business (functional) teams","Delta, Deployment, HDFS, Techno-functional, Hadoop, Data Science, Big Data, Sql, Python, AWS"
"DATA ENGINEER - AWS , PYTHON , TERRAFORM",Augusta Infotech,6-11 Years,,Gurugram,"Consulting, Information Services","The Technical Specialist Data Services is responsible for working on data pipelines and engineering project delivery and / or production support activities from a technical perspective. This role would typically include day to day activities creating and supporting data pipelines, deployments including solving production and pre-production issues, working on change, incidents, problems and service improvement or engineering initiatives.\nThis role demands strong technical skills with a focus to ensure maximum uptime of the production infrastructure. Working as a member of a global database support team the successful candidate will play a pro-active role in the support of Emerging Services estate. The role requires Expert level skills in AWS/Azure Cloud devops which includes working on Terraform, Lambda (Python and Java) , Building micro services and CICD pipelines. Were looking for a candidate who can support Technologies like NiFi , Spark and Kafka. Working knowledge on Database, Oracle Golden Gate will be an advantage.\nThe role will involve regular BAU (Business As Usual) support and will involve 24 x 7 On-Call support and weekend shift working.\nKey Responsibilities\nThe key responsibilities of this role are:\nDesign and develop secure data pipelines.\n24x7 support for Production Emerging Data Services estate. This includes all Production, Disaster Recovery, Emergency Break Fix, User Acceptance Testing and Development environments.\nStrategize data migration from Data Centre to Cloud includes Batch and CDC migration.\nAutomation and process improvement.\nRapid response to address high severity (severity 1, 2 and 3H) incidents.\nSupport and delivery of assigned project work.\nPro-active issue detection and resolution as well as service monitoring.\nOff business hours support on rotation basis.\nCollaborate with other Business and Technology teams.\nExperience and Qualifications Required\nExcellent Cloud Devops Engineer who can work and support CICD Pipeline using Terraform, cloud formation.\nWorking experience on Lambda, microservices, API.\nData migration using NiFi, Spark, Kafka and Oracle Golden-Gate from on-prem to Cloud.\nWorking experience with Snowflake will be an addon.\nSupport of engineering pipelines, monitoring and housekeeping, upgrade and patching, troubleshooting, root cause analysis and performance tuning.\nProficient in ITIL processes.\nInterpersonal Skills\nA good team player.\nGood written and verbal communication skills, together with the ability to communicate with management and other business and technical groups.\nGood analytical and, problem solving skills.\nAbility to adapt to changing business needs (flexible) and learning new skills quickly.\nCustomer focused strong service ethic.\nAbility to work on own initiative with minimal direction.\nTechnical skills\nAbove 6 Years of progressive working knowledge as Devops data engineer with technologies including but not restricted to:\nNiFi\nKafka\nBig Data - Spark\nHands-on experience on Cloud (AWS/Azure)\nAutomation/Scripting expertise in Terraform, Lambda (Python/Java), Cloud formation\nPerformance tuning with Data Pipelines\nArchitecting Data migration Solution (Nice to Have)\nBasic knowledge of Snowflake, Oracle, SQL Server (Nice to have)","Python, Sql, Terraform, Data Warehousing, AWS"
Big Data Engineer - Xebia,Foray Software,3-6 Years,,Bengaluru,Consulting,"Implementation experience on building large scale data applications from scratch (initial stages)\nProgramming experience in Python or Java\nGood experience of deploying applications on AWS and usage of its services\nMust have experience with Hadoop Distributed File System (HDFS), Amazon Simple Storage Service (S3)\nMust have good experience on SQL\nData organization in Data Lake (experience in Delta Lake or Databricks is added advantage)\nDetailed understanding of Data pipeline creation\nDetailed experience of Data ingestion\nTechno functional experience working with technical team (data engineering / data science) and the business (functional) teams","Delta, Deployment, HDFS, Techno-functional, Hadoop, Data Science, Big Data, Sql, Python, AWS"
Associate Data Engineer- GET,XenonStack,0-1 Years,,Chennai,Information Technology,"Job Responsibilities -\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with coworkers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analyzed quickly by BI AI Teams .\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nTechnical Requirements-\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch , Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores .\nProfessional Attributes-\nExcellent communication skills\nAttention to detail\nAnalytical mind and Problem Solving Aptitude\nStrong Organizational skills","Consulting, Analytical, Architectural Design, Machine Learning, Big Data Analytics"
GCP Data Engineer,SP Staffing Services Private Limited,6-12 Years,,"Hyderabad, Bengaluru, Pune",Software,"We are seeking an experienced GCP Data Engineer proficient in data processing frameworks, programming languages, and GCP services. The ideal candidate will have hands-on experience with cloud-based solutions, particularly GCP services such as BigQuery, Dataflow, and Spanner. Strong problem-solving abilities, a solid understanding of data security, and expertise in ETL processes are essential.\nKey Responsibilities:\nDevelop and implement data processing solutions using frameworks such as Apache Beam (Data Flow) and Kafka.\nWork with GCP services like BigQuery, Dataflow, and Spanner to optimize data pipelines and workflows.\nDesign and model data structures for efficient data storage and retrieval.\nOversee ETL processes to ensure seamless data extraction, transformation, and loading.\nCollaborate with cross-functional teams to address data engineering challenges and improve data systems.\nEnsure scalability and security of data infrastructure while maintaining high performance.\nLeverage Apache Airflow or similar tools to manage workflows and pipelines.\nKey Requirements:\nProficiency in programming languages like Python, Java, or Scala.\nExpertise in Apache Beam, Kafka, and GCP services such as BigQuery, Dataflow, and Spanner.\nExperience in data modeling, database design, and ETL processes.\nStrong problem-solving abilities and the ability to manage complex data engineering tasks.\nFamiliarity with cloud storage solutions and data security best practices.\nUnderstanding of scalability principles in cloud-based data engineering environments.","Data Engineer, Etl, Gcp"
GCP Data Engineer!,TechnoGen,5-10 Years,,Hyderabad,"Consulting, Information Services","Description\nWe are seeking a skilled GCP Data Engineer to join our dynamic team in India. The ideal candidate will have extensive experience in designing, building, and maintaining data pipelines and data processing systems on Google Cloud Platform. The role involves collaborating with cross-functional teams to ensure that our data infrastructure meets the needs of the organization.\nResponsibilities\nDesign and implement data processing systems on Google Cloud Platform (GCP).\nDevelop and maintain scalable data pipelines using tools such as Dataflow, Dataproc, and BigQuery.\nCollaborate with data scientists and analysts to understand data requirements and ensure data availability.\nMonitor and optimize data pipelines for performance and cost efficiency.\nEnsure data quality and compliance with data governance standards.\nImplement security best practices for data storage and access.\nSkills and Qualifications\n5-10 years of experience in data engineering or related field.\nStrong proficiency in Google Cloud Platform services, particularly BigQuery, Dataflow, and Cloud Storage.\nExperience with SQL and NoSQL databases, including data modeling and ETL processes.\nProficiency in programming languages such as Python, Java, or Go.\nFamiliarity with data warehousing concepts and data architecture principles.\nKnowledge of data visualization tools and techniques.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","ETL Processes, Cloud Storage, BigQuery, Data Modeling, Sql, Python, data engineering"
GCP Data Engineer,Nihilent,5-7 Years,,Chennai,Information Technology,"Unnesting of JSON files in Google BigQuery\nGoogle Cloud Run (using container services, python)\nHands on experience in GCP services like composer, cloud function, cloud run\nExperience in developing CI/CD pipelines.\nExperience in DBT scripts, docker files and knowledge on datavault is a plus.\nStrong technical knowledge and hands on experience of python or java\nRole: Data Engineer\nIndustry Type: IT Services & Consulting\nDepartment: Data Science & Analytics\nEmployment Type: Full Time, Permanent\nRole Category: Data Science & Machine Learning\nEducation\nUG: Any Graduate\nPG: Any Postgraduate","Gcp, Cloud, Json, Python"
GCP Data Engineer,Impetus Technologies,4-7 Years,,Bengaluru,Software,"We need GCP engineers for capacity building;\n- The candidate should have extensive production experience (1-2 Years ) in GCP, Other cloud experience would be a strong bonus.\n- Strong background in Data engineering 2-3 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.\n- Exposure to enterprise application development is a must\nRoles and Responsibilities\n4-7 years of IT experience range is preferred.\nAble to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Big Query, GCS- At least 4 of these Services.\nGood to have knowledge on Cloud Composer, Cloud SQL, Big Table, Cloud Function.\nStrong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOPs.\nGood hands on expertise on either Python or Java programming.\nGood Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.\nGood to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.\nAbility to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.\nExperience with technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.\nExtensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.\nAct as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.\nTechnical ability to become certified in required GCP technical certifications.\nRole:Data Engineer\nIndustry Type:Software Product\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:M.Com in Any Specialization, MCA in Any Specialization, M.Tech in Any Specialization","CloudGen, Engine, Java, Devops, Hive, Gcp, Pyspark, Iam, Spark, Big Data Technologies, Sql"
IT - Data Engineer _ AWS,Systechcorp Inc,2-6 Years,,"Delhi, Hyderabad",Software,Work with practice SMEs to understand the client s challenges and how Genpact solves for their challenges\nUnderstand our right to play and value articulation\nLiaison with practice team to drive product/offering messaging by translating technical nuances to strong client messages\nProductize the offering with the right value articulation to make it client ready\nEnsure every offering has all the GTM readiness collaterals for client readiness\nEstablish strong relationship with practice leaders to be their trusted advisor for offerings\nAbility to quickly research industry and competitor s offerings and ability to incorporate it in our value messaging\nWork with Knowledge Management team to drive strategic placement of the offerings in our infrastructure,"IT, data engineering, Cloud, Aws"
Azure Data Engineer,Trigent Software Private Limited,5-10 Years,,"Delhi, Bengaluru, Chennai",Software,"Detailed JD *(Roles and Responsibilities)\nExperience in Databricks, PL/SQL, PySpark, Python, and Azure Data Factory (ADF).\nExperience in designing, developing, and maintaining data pipelines and data streams.\nExperience in moving/transforming data across layers (Bronze, Silver, Gold) using ADF, Python, and PySpark.\nExperience in working with stakeholders to understand their data needs and provide solutions.\nExperience in collaborating with other teams to ensure data quality and consistency.\nExperience in developing and maintaining data models and data dictionaries.\nOptimize ETL processes for performance and scalability.\nExperience in developing and maintaining data integrity and accuracy\nExperience in developing and maintaining data governance policies and procedures.\nExperience in developing and maintaining data security policies and procedures.\nGood understanding of deployment processes\nAbility to manage customer handling\nFlexible for support and maintenance type of project\nMandatory skills*\nDatabricks, PySpark, Python, Azure Data Factory (ADF)","Azure Data Factory, Pyspark, Databricks, Python, Etl"
Big Data Engineer,Coditas Technologies,2-5 Years,,Pune,Software,"We are looking for people who have the right attitude, aptitude, skills, empathy, compassion, and hunger for learning.Built products in the data analytics space, either frontend/backend/cloud. A passion for shipping high-quality products, interest in the data products space, curiosity about the bigger picture of building a company, product development, people, and product\nRoles and Responsibilities\nWe are looking for a savvy Data Engineering professional to join the newly formed Data Engineering team\nWe are looking for Big Data specialists who have proven skills on working large scale data systems\nThe hire will be responsible for building and optimizing data pipeline architectures, as well as optimizing data flow and collection for multiple source systems\nThe ideal candidate should be experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up\nHave got strong ability to build robust and resilient data pipelines which are fault tolerant and reliable in terms of data movement\nShould have knowledge of experience on batch and stream data processing\nCreate end to end data products and productionize them in cloud/in-house servers\nTechnical Skills\nMinimum 2-4 years of progressive experience building solutions in Big Data environments\nShould have solid hands on experience on Big Data technologies like Hadoop, HBase, Hive, Pig, Oozie, MapReduce, Yarn, HDFS, Zookeeper\nHands on knowledge of Apache Spark with Java/Scala for batch and stream processing will be highly preferred\nKnowledge of Apache Kafka will be added advantage but not mandatory\nStrong hands on capabilities on SQL and NoSQL technologies\nShould be able to build performant, fault tolerant, scalable solutions\nExcellent written and verbal communication skills\nWhat is in it for you\nA clear career path with a hybrid of technical track and management track to grow in the company\nIn the technical track, the candidate will be developing sophisticated actuarial and analytics skill\nCommunicating with clients and manage the track, the candidate will develop skills sets in business strategy and build strong tech products\nYou should have\nExcellent problem solving and analytical skills\nFluency in written and communication skills in English\nGood time-management skills\nShould be a quick learner with ability to learn new tools/technology quickly\nGood analytical and problem-solving skills\nIntellectual and analytical curiosity\nExpertise in data analytics and providing data driven insights that help in taking business decisions\nThe traits of a self-motivated, independent and detail-oriented team player\nRole:Back End Developer\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Time Management, Backend, Analytical, Business Strategy, Data Processing, Actuarial, Big Data, Data Analytics, Apache, Sql"
Data Engineer - Insights & Analytics,Philips,10-12 Years,,"Bengaluru, India",Hospitals/Healthcare/Diagnostics,"Job Description\nJob title:\nData Engineer - Insights & Analytics\n\nYour role:\nEngage with the Enterprise Informatics leadership team to understand their challenges and translate them into actionable analytics solutions.\nPerform advanced analytics, data mapping, and reporting to support initiatives and commercial operations projects.\nEnsure timely delivery of high-quality analysis and reports.\nStay updated on the latest insights and analytics developments implement machine learning and AI solutions in key use cases.\nAnalyze multiple business flows and data models, ensuring the application of relevant analytics methodologies and standards.\nDevelop and support Insights & Analytics solutions, including maintaining existing EI CommOps business-owned reports.\n\n\nYou're the right fit if:\nYour have proven technical skills in PowerBI, Azure DataBricks, Advanced Excel + Macros experience with Salesforce and SAP is preferred.\nYou should have successfully integrated and deployed AI technologies, including machine learning models and Large Language Models (LLMs), into production environments, demonstrating significant business impact.\nYou are proficient in one or two key comercial data domains (e.g., OIT, Sales, Services, SSOP, Pricing,...)\nYou should possess a solid foundation in Python, which is crucial for developing and optimizing analytics solutions in our projects.\nYou have at least 10 years of relevant experience in roles such as Business Analytics Specialist, Data Analyst, or Business Process Expert, with a strong understanding of commercial processes.\nYou have expertise in designing, building, and maintaining scalable ETL pipelines to support data transformation and structuring for business intelligence.\nYou should exhibit a problem-solving mindset with experience in root cause analysis and lean principles.\nYou are eager to work with global teams.\nYou demonstrate excellent analytical thinking and the ability to interpret large volumes of information, synthesise insights, and effectively communicate analyses and proposals to various audiences, including senior management.\nYou hold a Bacheloru2019s degree in Computer Science, Information Management, Data Science, Statistics, or a related field a Masteru2019s degree is preferred.\n\nHow we work together\nWe believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.\nOnsite roles require full-time presence in the companyu2019s facilities.\nField roles are most effectively done outside of the companyu2019s main facilities, generally at the customersu2019 or suppliersu2019 locations.\n\nIndicate if this role is an office/field/onsite role.\n\nAbout Philips\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.\nu2022 Learn more about .\nu2022 Discover .\nu2022 Learn more about .\n\nIf youu2019re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion .\n#LI-EU #LI-Hybrid #LI-PHILIN","Salesforce, AI technologies, ETL pipelines, machine learning models, SAP, Macros, Powerbi, Azure Databricks, Advanced Excel, Python"
GCP Data Engineer,Impetus Technologies,4-7 Years,,Bengaluru,Software,"We need GCP engineers for capacity building;\n- The candidate should have extensive production experience (1-2 Years ) in GCP, Other cloud experience would be a strong bonus.\n- Strong background in Data engineering 2-3 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.\n- Exposure to enterprise application development is a must\nRoles and Responsibilities\n4-7 years of IT experience range is preferred.\nAble to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Big Query, GCS- At least 4 of these Services.\nGood to have knowledge on Cloud Composer, Cloud SQL, Big Table, Cloud Function.\nStrong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOPs.\nGood hands on expertise on either Python or Java programming.\nGood Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.\nGood to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.\nAbility to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.\nExperience with technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.\nExtensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.\nAct as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.\nTechnical ability to become certified in required GCP technical certifications.\nRole:Data Engineer\nIndustry Type:Software Product\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:Software Development\nEducation\nUG:B.Tech/B.E. in Any Specialization\nPG:M.Com in Any Specialization, MCA in Any Specialization, M.Tech in Any Specialization","CloudGen, Engine, Java, Devops, Hive, Gcp, Pyspark, Iam, Spark, Big Data Technologies, Sql"
GCP Data Engineer!,TechnoGen,5-10 Years,,Hyderabad,"Consulting, Information Services","Description\nWe are seeking a skilled GCP Data Engineer to join our dynamic team in India. The ideal candidate will have extensive experience in designing, building, and maintaining data pipelines and data processing systems on Google Cloud Platform. The role involves collaborating with cross-functional teams to ensure that our data infrastructure meets the needs of the organization.\nResponsibilities\nDesign and implement data processing systems on Google Cloud Platform (GCP).\nDevelop and maintain scalable data pipelines using tools such as Dataflow, Dataproc, and BigQuery.\nCollaborate with data scientists and analysts to understand data requirements and ensure data availability.\nMonitor and optimize data pipelines for performance and cost efficiency.\nEnsure data quality and compliance with data governance standards.\nImplement security best practices for data storage and access.\nSkills and Qualifications\n5-10 years of experience in data engineering or related field.\nStrong proficiency in Google Cloud Platform services, particularly BigQuery, Dataflow, and Cloud Storage.\nExperience with SQL and NoSQL databases, including data modeling and ETL processes.\nProficiency in programming languages such as Python, Java, or Go.\nFamiliarity with data warehousing concepts and data architecture principles.\nKnowledge of data visualization tools and techniques.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","ETL Processes, Cloud Storage, BigQuery, Data Modeling, Sql, Python, data engineering"
Data Engineer,Ford Motor Company,4-6 Years,,"Chennai, India",Automotive/Automobile/Ancillaries,"JOB DESCRIPTION\nThis role requires a combination of software engineer, data enginner and an ML developer where you're expected to build solutions integrating data pipelines & ML models. It also requires you to have basic knowledge and (preferably) hands-on experience on building back-end services(APIs) which integrate data & applications.\nRESPONSIBILITIES\nKey Responsibilities:\nData Pipeline Development:Design, develop, and maintain scalable and reliable batch data pipelines using Python and Java, leveraging industry standard frameworks like Data proc and Dataflow.\nELT Implementation:Implement efficient data extraction, transformation, and loading processes to move data from various sources into data warehouses, data lakes, or other storage systems.\nBackend Development:Develop and maintain backend services (potentially in Python or Java) that interact with data stores, trigger pipelines, and/or serve data via APIs.\nData Storage Interaction:Work with various data storage technologies, including relational databases (SQL), data lakes (GCS, Big Query), and data warehouses (BigQuery).\nPerformance Optimization:Analyze and optimize the performance of data pipelines and related services to ensure efficiency and cost-effectiveness when dealing with large datasets.\nMonitoring and Reliability:Implement monitoring, logging, and alerting for data pipelines and services to ensure their health, reliability, and data quality. Troubleshoot and resolve production issues.\nCollaboration:Collaborate effectively with Product Owners, Data Scientists, Data Engineers, MLOps Engineers, and other engineering teams to understand requirements and deliver integrated solutions.\nCode Quality & Testing:Write clean, well-tested, and maintainable code. Participate in code reviews.\nTechnical Contribution:Contribute to architectural discussions and help define technical approaches for data and AI-related projects.\nAI/ML Integration (Additional responsibility):Work closely with Data Scientists to operationalize machine learning models. This includes building the infrastructure and code (incl. chatbots) to integrate models into data pipelines or backend services for training data preparation, inference, or prediction serving.\nOperational support: Handle tickets (incidents/requests) for data pipelines/chatbot applications & work with product owners/business customers to track the tickets to closure within pre-defined SLAs.\nQUALIFICATIONS\nRequired Skills and Qualifications:\n4+ years of professional experience in software development.\nStrong proficiency and hands-on experience in both Python(Must-have) and Java(Nice to have).\nExperience building and maintaining data pipelines (batch or streaming) preferably on Cloud platforms(especially GCP).\nExperience with at least one major distributed data processing framework (e.g., DBT, DataForm, Apache Spark, Apache Flink, or similar).\nExperience with workflow orchestration tools (e.g., Apache Airflow, Qlik replicate etc).\nExperience working with relational databases (SQL) and understanding of data modeling principles.\nExperience with cloud platforms (Preferably GCP. AWS or Azure will also do) and relevant data services (e.g., BigQuery, GCS, Data Factory, Dataproc, Dataflow, S3, EMR, Glue etc.).\nExperience with data warehousing concepts and platforms (BigQuery, Snowflake, Redshift etc.).\nUnderstanding of concepts related to integrating or deploying machine learning models into production systems.\nExperience working in an Agile development environment & hands-on in any Agile work management tool(Rally, JIRA etc.).\nExperience with version control systems, particularly Git.\nSolid problem-solving, debugging, and analytical skills.\nExcellent communication and collaboration skills.\nExperience working in a production support team (L2/L3) for operational support.\nPreferred Skills and Qualifications (Nice to Have):\nFamiliarity with data quality and data governance concepts.\nExperience building and consuming APIs (REST, gRPC) related to data or model serving.\nBachelor's or Master's degree in Computer Science, Engineering, Data Science, or a related field.","Data Storage Interaction, snowflake, AI ML Integration, DataForm, Data Pipeline Development, Glue, ELT Implementation, GCS, Monitoring and Reliability, Performance Optimization, dbt, Sql, Data Factory, Emr, Java, DataFlow, Backend Development, BigQuery, Dataproc, Git, S3, Apache Airflow, Apache Flink, Apache Spark, Python, Redshift"
Associate Data Engineer,Commonwealth Bank,3-5 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title:Associate Data Engineer-Big Data\nLocation:Bengaluru\nBusiness & Team:RM & FS Data Engineering\nImpact & contribution:\nAs a Associate Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you're ahead of trends and work at the forefront of Big Data and Data warehouse technologies.\nWhich is why we're the perfect fit for you. Here, you'll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers most complex data-centric problems.\nTo us, data is everything. It is what powers our cutting-edge features and it's the reason we can provide seamless experiences for millions of customers from app to branch.\nWe're responsible for CommBank's key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We work across the Cloudera Hadoop Big Data, Teradata Group Data Warehouse and Ab Initio platforms.\nRoles & Responsibilities:\nPassionate about building next generation data platforms and data pipeline solution across the bank.\nEnthusiastic, be able to contribute and learn from wider engineering talent in the team.\nReady to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.\nCapable to create both technology blueprints and engineering roadmaps, for a multi- year data transformational journey.\nCan lead and drive a culture where quality, excellence and openness are championed.\nConstantly thinking outside the box and breaking boundaries to solve complex data problems.\nAre experienced in providing data driven solutions that source data from various enterprise data platform into Cloudera Hadoop Big Data environment, using technologies like Spark, MapReduce, Hive, Sqoop, Kafka transform and process the source data to produce data assets and transform and egression to other data platforms like Teradata or RDBMS system.\nAre confident in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.\nCan collaborate, co-create and contribute to existing Data Engineering practices in the team.\nHave experience and responsible for data security and data management.\nHave a natural drive to educate, communicate and coordinate with different internal stakeholders.\nEssential Skills:\nPreferably with at least 3+ years of hands-on experience in a Data Engineering role.\nExperience in designing, building, and delivering enterprise-wide data ingestion, data integration and data pipeline solutions using common programming language (Scala, Java, or Python) in a Big Data and Data Warehouse platform.\nExperience in building data solution in Hadoop platform, using Spark, MapReduce, Sqoop, Kafka and various ETL frameworks for distributed data storage and processing. Preferably with at least 3+ years of hands-on experience.\nExperience in building data solution using AWS Cloud technology (EMR, Glue, Iceberg, Kinesis, MSK/Kafka, Redshift, DocumentDB, S3, etc.). Preferably with 1+ years of hands-on experience and certified AWS Data Engineer.\nStrong Unix/Linux Shell scripting and programming skills in Scala, Java, or Python.\nProficient in SQL scripting, writing complex SQLs for building data pipelines.\nExperience in working in Agile teams, including working closely with internal business stakeholders.\nFamiliarity with data warehousing and/or data mart build experience in Teradata, Oracle or RDBMS system is a plus.\nCertification on Cloudera CDP, Hadoop, Spark, Teradata, AWS, Ab Initio is a plus.\nExperience in Ab Initio software products (GDE, Co Operating System, Express It, etc.) is a plus.\nEducational Qualifications: B.Tech and above\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 05/06/2025","DocumentDB, MSK, Teradata, Iceberg, Glue, Java, S3, Hadoop, Scala, Kafka, Big Data, Emr, Redshift, Sql, Mapreduce, Hive, Kinesis, Sqoop, Spark, Cloudera, Ab Initio, Python, AWS"
Senior Data Engineer (AI/ML),Global Payments,5-7 Years,,"Pune, India",Banking/Accounting/Financial Services,"Every day, Global Payments makes it possible for millions of people to move money between buyers and sellers using our payments solutions for credit, debit, prepaid and merchant services. Our worldwide team helps over 3 million companies, more than 1,300 financial institutions and over 600 million cardholders grow with confidence and achieve amazing results. We are driven by our passion for success and we are proud to deliver best-in-class payment technology and software solutions. Join our dynamic team and make your mark on the payments technology landscape of tomorrow.\nRESPONSIBILITIES\nDesign, develop, implement, test, and maintain scalable and efficient data pipelines for large scale structured and unstructured datasets, including document, image, and event data used in GenAI and ML use cases..\nCollaborate closely with data scientists, AI/ML engineers, MLOps and Product Owners to understand data requirements and ensure data availability and quality.\nBuild and optimize data architectures for both batch and real-time processing.\nDevelop and maintain data warehouses and data lakes to store and manage large volumes of structured and unstructured data.\nImplement data validation and monitoring processes to ensure data integrity.\nImplement and manage vector databases (eg. pgVector, Pinecone, FAISS, etc) and embedding pipelines to support retrieval-augmented architectures.\nSupport data sourcing and ingestion strategies, including API, data lakes, and message queues\nEnforce data quality, lineage, observability, and governance standards for AI workloads\nWork with cross-functional IT and business teams in an Agile environment to deliver successful data solutions.\nHelp foster a data-driven culture via information sharing, design for scalability, and operational efficiency.\nStay updated with the latest trends and best practices in data engineering and big data technologies.\nMust Haves:\nBachelor's degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data engineering or a related field.\nExperience with the following:\nStrong programming skills in Python (and optionally Scala or Java) with Spark, Airflow, or similar orchestration tools.\nDeep experience with cloud data platforms (eg Databricks, GCP BigQuery, Snowflake, AWS Glue)\nStrong familiarity with LLM workflows, RAG solutions, embeddings, reranking, and vector search concepts.\nProficiency in SQL and experience with data modeling for AI/ML use cases\nExperience with NoSQL databases (MongoDB, Cassandra, or similar).\nKnowledge of containerization technologies like Docker and orchestration systems like Kubernetes.\nCloud platform experience GCP, AWS or Azure are acceptable.\nUnderstanding of responsible AI principles as applied to data sourcing and processing\nExcellent problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nBonus Attributes:\nExperience with real-time data processing frameworks (Kafka, Flink, etc.).\nExperience working with data scientists on machine learning projects.\nExperience supporting generative AI model training or inference in production environments\nKnowledge of LLM and integration with foundation AI platforms (eg AWS Bedrock, Google VertexAI, Snowflake Cortex, Azure OpenAI)\nHands-on exposure and understanding of LangChain, LangGraph, CrewAI, or similar orchestration frameworks\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch, scikit-learn).\nExperience with CI/CD tools and practices.\nKnowledge of data governance and data security best practices.\nCertifications in data engineering or cloud technologies.\nAbilities:\nAbility to work with a high level of initiative, accuracy, and attention to detail.\nAbility to prioritize multiple assignments effectively. Ability to meet established deadlines.\nAbility to successfully, efficiently, and professionally interact with staff and customers.\nExcellent organization skills.\nCritical thinking ability ranging from moderately to highly complex.\nFlexibility in meeting the business needs of the customer and the company.\nAbility to work creatively and independently with latitude and minimal supervision.\nAbility to utilize experience and judgment in accomplishing assigned goals.\nExperience in navigating organizational structure.\nGlobal Payments Inc. is an equal opportunity employer. Global Payments provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including pregnancy), national origin, ancestry, age, marital status, sexual orientation, gender identity or expression, disability, veteran status, genetic information or any other basis protected by law. If you wish to request reasonable accommodations related to applying for employment or provide feedback about the accessibility of this website, please contact .","snowflake, Airflow, scikit-learn, Flink, reranking, embeddings, LLM workflows, vector search, RAG solutions, Spark, Databricks, Sql, Java, Tensorflow, Kafka, BigQuery, Pytorch, Cassandra, Gcp, Scala, AWS Glue, AWS, Kubernetes, Python, Azure, Docker, MongoDB"
Senior Data Engineer (AI/ML),Global Payments,5-7 Years,,"Pune, India",Banking/Accounting/Financial Services,"Every day, Global Payments makes it possible for millions of people to move money between buyers and sellers using our payments solutions for credit, debit, prepaid and merchant services. Our worldwide team helps over 3 million companies, more than 1,300 financial institutions and over 600 million cardholders grow with confidence and achieve amazing results. We are driven by our passion for success and we are proud to deliver best-in-class payment technology and software solutions. Join our dynamic team and make your mark on the payments technology landscape of tomorrow.\nRESPONSIBILITIES\nDesign, develop, implement, test, and maintain scalable and efficient data pipelines for large scale structured and unstructured datasets, including document, image, and event data used in GenAI and ML use cases..\nCollaborate closely with data scientists, AI/ML engineers, MLOps and Product Owners to understand data requirements and ensure data availability and quality.\nBuild and optimize data architectures for both batch and real-time processing.\nDevelop and maintain data warehouses and data lakes to store and manage large volumes of structured and unstructured data.\nImplement data validation and monitoring processes to ensure data integrity.\nImplement and manage vector databases (eg. pgVector, Pinecone, FAISS, etc) and embedding pipelines to support retrieval-augmented architectures.\nSupport data sourcing and ingestion strategies, including API, data lakes, and message queues\nEnforce data quality, lineage, observability, and governance standards for AI workloads\nWork with cross-functional IT and business teams in an Agile environment to deliver successful data solutions.\nHelp foster a data-driven culture via information sharing, design for scalability, and operational efficiency.\nStay updated with the latest trends and best practices in data engineering and big data technologies.\nMust Haves:\nBachelor's degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data engineering or a related field.\nExperience with the following:\nStrong programming skills in Python (and optionally Scala or Java) with Spark, Airflow, or similar orchestration tools.\nDeep experience with cloud data platforms (eg Databricks, GCP BigQuery, Snowflake, AWS Glue)\nStrong familiarity with LLM workflows, RAG solutions, embeddings, reranking, and vector search concepts.\nProficiency in SQL and experience with data modeling for AI/ML use cases\nExperience with NoSQL databases (MongoDB, Cassandra, or similar).\nKnowledge of containerization technologies like Docker and orchestration systems like Kubernetes.\nCloud platform experience GCP, AWS or Azure are acceptable.\nUnderstanding of responsible AI principles as applied to data sourcing and processing\nExcellent problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nBonus Attributes:\nExperience with real-time data processing frameworks (Kafka, Flink, etc.).\nExperience working with data scientists on machine learning projects.\nExperience supporting generative AI model training or inference in production environments\nKnowledge of LLM and integration with foundation AI platforms (eg AWS Bedrock, Google VertexAI, Snowflake Cortex, Azure OpenAI)\nHands-on exposure and understanding of LangChain, LangGraph, CrewAI, or similar orchestration frameworks\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch, scikit-learn).\nExperience with CI/CD tools and practices.\nKnowledge of data governance and data security best practices.\nCertifications in data engineering or cloud technologies.\nAbilities:\nAbility to work with a high level of initiative, accuracy, and attention to detail.\nAbility to prioritize multiple assignments effectively. Ability to meet established deadlines.\nAbility to successfully, efficiently, and professionally interact with staff and customers.\nExcellent organization skills.\nCritical thinking ability ranging from moderately to highly complex.\nFlexibility in meeting the business needs of the customer and the company.\nAbility to work creatively and independently with latitude and minimal supervision.\nAbility to utilize experience and judgment in accomplishing assigned goals.\nExperience in navigating organizational structure.\nGlobal Payments Inc. is an equal opportunity employer. Global Payments provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including pregnancy), national origin, ancestry, age, marital status, sexual orientation, gender identity or expression, disability, veteran status, genetic information or any other basis protected by law. If you wish to request reasonable accommodations related to applying for employment or provide feedback about the accessibility of this website, please contact .","snowflake, Airflow, scikit-learn, Flink, reranking, embeddings, LLM workflows, vector search, RAG solutions, NoSQL databases, Spark, Databricks, Sql, Java, Tensorflow, Kafka, BigQuery, Pytorch, Cassandra, Gcp, Scala, AWS Glue, AWS, Kubernetes, Python, Azure, Docker, MongoDB"
Senior Data Engineer,Commonwealth Bank,7-10 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Organization\nAt CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\nJob Title: SeniorData Engineer\nLocation: Bangalore\nBusiness & Team:\nThe Business Banking Technology Domain works in an Agile methodology with our business banking business to plan, prioritise and deliver on high value technology objectives with key results that meet our regulatory obligations and protect the community.\nYou will work within the VRM Crew that is working on initiatives such as Gen AI based cash flow coach to provide relevant data to our regulators\nImpact & Contribution:\nAs a Senior Data Engineer and database engineer you will be creating and managing the cloud databases and data pipelines that underpin our decoupled cloud architecture and API first approach. You have proven expertise in database design, data ingestion, transformation, data writing, scheduling and query management within a cloud environment.\nYou will have proven experience and expertise in working with AWS Cloud Infrastructure Engineers, Software/API Developers to design, develop, deploy and operate data services and solutions that underpin a cloud ecosystem. You will take ownership and accountability of functional and non-functional design and work within a team of Engineers to create innovative solutions that unlock value and modernise technology designs.\nYou will role model continuous improvement mindset in the team, and in your project interactions, by taking technical ownership of key assets, including roadmaps and technical direction of data services running on our AWS environments.\nRoles & Responsibilities:\nCan design and implement databases for data integration in the enterprise\nCan performance tune applications from a database code and design perspective\nCan automate data ingestion and transformation processes using scheduling tools.\nMonitor and troubleshoot data pipelines to ensure reliability and performance.\nCan design application logical database requirements and implement physical solutions\nCan collaborate with business and technical teams in order to design and build critical databases and data pipelines\nEssential Skills:\nMinimum 7 to 10 years of experience\nAWS Data products such as AWS Glue and AWS EMR\nOracle and AWS Aurora RDS such as PostgreSQL\nAWS S3 ingestion, transformation and writing to databases\nProficiency in programming languages like Python, Scala or Java for developing data ingestion and transformation scripts.\nStrong knowledge of SQL for writing, optimizing, and debugging queries.\nFamiliarity with database design, indexing, and normalization principles.\nUnderstanding of data formats (JSON, CSV, XML) and techniques for converting between them. Ability to handle data validation, cleaning, and transformation.\nProficiency in automation tools and scripting (e.g., bash scripting, cron jobs) for scheduling and monitoring data processes.\nExperience with version control systems (e.g., Git) for managing code and collaboration.\nEducation Qualifications:\nBachelor's degree in engineering in Computer Science/Information Technology.\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through to submit a valid application. We're keen to support you with the next step in your career.\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\nAdvertising End Date: 29/06/2025","AWS Data products such as AWS Glue and AWS EMR, Experience with version control systems e.g. Git, Oracle and AWS Aurora RDS such as PostgreSQL, Strong knowledge of SQL, AWS S3 ingestion transformation and writing to databases"
Data Engineer,PayPal,2-4 Years,,"Chennai, India",Banking/Accounting/Financial Services,"The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nMeet Your Team\n\nAt the heart of our fintech innovation lies the Data Engineering & Analytics Team - a tight-knit group of engineers, data scientists, and analysts building the real-time intelligence layer that powers everything from fraud detection to personalized financial recommendations.\n\nYou'll work alongside team members who bring experience from top-tier tech, finance, and environments, and who value clean architecture, reproducible data science, and high-velocity experimentation.\n\nWe don't just crunch numbers - we build the data backbone that makes real-time decisioning possible across millions of user interactions.\n\nWhether it's building low-latency pipelines, optimizing streaming systems, or unlocking insights from noisy data, we're driven by impact - and we're looking for someone who's excited to build with us.\n\nWhat do you need to know about the role\n\nAs a Data Engineer in our team, you'll play a critical role in building and maintaining real-time and near real-time data pipelines that transform raw data into meaningful, trustworthy datasets for our stakeholders across the organization. This role involves backend development primarily in Java, with a focus on building scalable, low-latency systems that support timely and accurate decision-making.\n\nYou'll solve both technical and domain challenges - from optimizing data ingestion and transformation, to aligning with evolving compliance and regulatory needs. You'll collaborate closely with engineering peers, compliance analysts, and product managers to deliver solutions that are audit-ready, scalable, and aligned with our broader data strategy.\n\nThis is a high-impact role that offers deep exposure to the company's products, platforms, and data governance practices - an excellent opportunity to shape how compliance data is consumed in real time and help ensure the organization remains both agile and compliant.\nJob Description:\nYour way to impact\nAt PayPal, Backend Software Engineers are the architects of our global payment platform.You'll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers experiences and our company's success.\nYour day-to-day\nAs a Software Engineer - Backendyou'll contribute to building robust backend systems. You'll collaborate closely with experienced engineers to learn and grow your skills.\nDevelop and maintain backend components.\nWrite clean, efficient code adhering to coding standards.\nParticipate in code reviews and provide feedback.\nWhat do you need to Bring\nBachelor's degree in Computer Science or related field.\n2+ years of backend development experience.Strong foundation in programming concepts and data structures.\nProficiency in at least one backend language (Java, Python, Ruby on Rails)\nProficiency in back-end development utilizing Java EE technologies (Java, application servers, servlet containers, JMS, JPA, Spring MVC, Hibernate)\nStrong understanding of web services and Service-Oriented Architecture (SOA) standards, including REST, OAuth, and JSON, with experience in Java environments.\nExperience with ORM (Object-Relational Mapper) tools, working within Java-based solutions like Hibernate.\nExperience with databases (SQL, NoSQL)\nPreferred Qualifications\nExperience with large-scale, high-performance systems.\nKnowledge of the payment processing industry and relevant regulations.\nExperience with cloud platforms (AWS, GCP, Azure).\nContributions to open-source projects.\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we're committed to building an equitable and inclusive global economy. And we can't do this without our most important asset-you. That's why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit.\nWho We Are:\nto learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at.\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don't hesitate to apply.","Java application servers, Java EE technologies, servlet containers, AWS, Sql, Java, Ruby On Rails, Oauth, Json, Hibernate, Spring MVC, Jpa, Nosql, Python, Azure, Gcp, REST, Jms"
Senior Data Engineer,NielsenIQ,7-9 Years,,India,IT/Computers - Software,"Job Description\nSenior Data Engineer\nMission of the Role\nYou are a passionate and skilled data engineer who enjoys building scalable, production-grade data pipelines that enable machine learning and AI at scale. You thrive on solving complex data challenges and understand how critical a robust data pipeline is to the success of any predictive model.\nAs a Senior Data Engineer on the Forecasting team, you will be responsible for designing, implementing, and maintaining the data pipelines that power deep neural network models used for global sales and revenue forecasting. You will work closely with ML engineers, data scientists, and product stakeholders to build automated, secure, and cost-efficient pipelines using GCP-native services.\nYou take pride in writing clean, testable, and production-ready code, and are comfortable operating in cloud environments. You know how to make data pipelines observable, recoverable, and performant. You are also comfortable taking architectural ownership and contributing to long-term platform improvements.\nYou will:\nDesign and build robust, scalable, and maintainable data pipelines for training and inference of deep learning models used in forecasting and other Machine Learning based algorithms.\nDevelop and orchestrate end-to-end workflows using GCP-native tools such as Cloud Workflows, Cloud Run, BigQuery, Cloud Functions, and Pub/Sub.\nAutomate and maintain the pipeline lifecycle, including data extraction, transformation, loading (ETL/ELT), and triggering model runs.\nMonitor and Optimise performance, cost-efficiency, and data freshness across all stages of the pipeline.\nCollaborate with data scientists to productionise AI/ML models and integrate them seamlessly into business workflows.\nOwn the data ingestion, validation, and transformation logic powering the forecasting models, ensuring high-quality, consistent input data.\nHandle deployment, versioning, and configuration of pipeline components using Docker, Git, and CI/CD practices.\nImplement logging, monitoring, and alerting to ensure reliability and traceability in a high-scale environment.\nReview code, mentor junior engineers, and help define best practices in our evolving data engineering stack.\nQualifications\nYou have:\n7+ years of experience in data engineering or backend engineering roles.\nStrong expertise in Python and SQL, with experience building production-grade data pipelines.\nSolid understanding of Docker, Git, and shell scripting in Linux environments.\nHands-on experience with GCP services\nExperience in building, deploying, and maintaining data workflows that feed AI/ML models.\nFamiliarity with model lifecycle management and infrastructure challenges in ML pipelines.\nProficiency in setting up CI/CD pipelines for data and ML systems using GitLab CI, Cloud Build, or similar tools.\nExposure to Java for backend services or pipeline components (even if not primary language).\nA proactive, collaborative mindset and strong communication skills across engineering and data science teams.\nNice to have:\nExposure to forecasting or time series modelling pipelines.\nExperience with event-driven architectures.\nFamiliarity with infrastructure-as-code tools like Terraform\nUnderstanding of data quality frameworks and observability tools\nKnowledge of model versioning tools and experiment tracking systems\nAdditional Information\nWhy Join us\nYou'll be working on a high-impact platform that forecasts consumer demand and drives business-critical decisions globally\nYou'll be part of a tight-knit, collaborative, and innovative Forecasting & AI team within a leading global data science organisation\nYou'll have access to top-tier infrastructure, GCP services, and challenging real-world problems in predictive modelling\nFlexible working hours, remote-friendly culture, and strong focus on personal and professional growth\nCompetitive compensation and performance-based bonuses\nOur Benefits\nFlexible working environment\nVolunteer time off\nLinkedIn Learning\nEmployee-Assistance-Program (EAP)\nAbout NIQ\nNIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights-delivered with advanced analytics through state-of-the-art platforms-NIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.\nFor more information, visit NIQ.com\nWant to keep up with our latest updates\nFollow us on: | | |\nOur commitment to Diversity, Equity, and Inclusion\nNIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center:","Cloud Run, Cloud Workflows, GCP-native tools, Cloud Functions, Git, BigQuery, Sql, Python, Docker"
Associate Analyst - Data Engineer,PepsiCo,4-6 Years,,"Hyderabad, India",Food Processing & Packaged Food,"Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo's global business scale to enable business insights, advanced analytics, and new product development. PepsiCo's Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nWhat PepsiCo Data Management and Operations does:\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineer, you will be the key technical expert building PepsiCo's data productsto drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help developingvery large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\n\nAct as a subject matter expert across different digital projects.\nOversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance, and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\nDevelop and optimize procedures to productionalize data science models.\nDefine and manage SLA's for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\n\nQualifications\n\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong organizational and interpersonal skills comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.","Data Analytics tools, snowflake, Deequ, SQL optimization, Great Expectations, MPP database technology, Data Lake Infrastructure, Synapse, Apache Griffin, ELT, Azure Data Factory, Data Warehousing, Pyspark, Azure Machine Learning, Etl, Azure Databricks, Azure Log Analytics, Python, Azure, Scala, Redshift"
Azure Databricks Lead (Sr. Data Engineer),Tiger Analytics,5-7 Years,,"Chennai, India",Internet/E-commerce,"Job Description\nWho we are\n\nTiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer full-stack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow.\n\nOur team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence.\n\nWe are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.\nCurious about the role What your typical day would look like\n\nRole Overview:\n\nWe are seeking street-smart and technically strong Senior Data Engineers / Leads who can take ownership of designing and developing cutting-edge data and AI platforms using Azure-native technologies and Databricks. You will play a critical role in building scalable data pipelines, modern data architectures, and intelligent analytics solutions.\n\nKey Responsibilities:\nDesign and implement scalable, metadata-driven frameworks for data ingestion, quality, and transformation across both batch and streaming datasets.\nDevelop and optimize end-to-end data pipelines to process structured and unstructured data, enabling the creation of analytical data products.\nBuild robust exception handling, logging, and monitoring mechanisms for better observability and operational support.\nTake ownership of complex modules and lead the development of critical data workflows and components.\nProvide guidance to data engineers and peers on best practices.\nCollaborate with cross-functional teams-including business consultants, data architects & scientists, and application developers-to deliver impactful analytics solutions.\nJob Requirement\n\n5+ years of overall technical experience, with a minimum of 2 years of hands-on experience with Microsoft Azure and Databricks.\nProven experience delivering at least one end-to-end Data Lakehouse solution on Azure Databricks using the Medallion Architecture.\nStrong working knowledge of the Databricks ecosystem, including: PySpark, Notebooks, Structured Streaming, Unity Catalog, Delta Live Tables, Workflows, and SQL Warehouse.\nAdvanced programming, unit testing, and debugging skills in Python and SQL.\nHands-on experience with Azure-native services such as: Azure Data Factory, ADLS Gen2, Azure SQL Database, and Event Hub.\nSolid understanding of data modeling techniques, including both Dimensional and Third Normal Form (3NF) models.\nExposure to developing LLM/Generative AI-powered applications.\nMust have excellent understanding of CI/CD workflows using Azure DevOps.\nBonus: Knowledge of Azure infrastructure, including provisioning, networking, security, and governance.\nEducational Background:\nBachelor's degree (B.E/B.Tech) in Computer Science, Information Technology, or a related field from a reputed institute (preferred).\nYou are important to us, let's stay connected!\nEvery individual comes with a different set of skills and qualities so even if you don't tick all the boxes for the role today we urge you to apply as there might be a suitable/unique role for you tomorrow. We are an equal- opportunity employer. Ourdiverse and inclusive culture and values guide us to listen, trust, respect, and encourage people to grow the way they desire, packages are among the best in industry.","Azure SQL Database, CI CD workflows, ADLS Gen2, Delta Live Tables, Unity Catalog, Event Hub, Structured Streaming, Databricks, Sql, Azure Data Factory, Workflows, Pyspark, Microsoft Azure"
Enterprise Data Operations Analyst - Data Engineer,PepsiCo,7-9 Years,,"Hyderabad, India",Food Processing & Packaged Food,"Overview\n\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.\n\nResponsibilities\n\nAct as a subject matter expert across different digital projects.\nOversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance, and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\nDevelop and optimize procedures to productionalize data science models.\nDefine and manage SLA's for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\n\nQualifications\n\n7+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like Github and deployment & CI tools.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nFamiliarity with business intelligence tools (such as PowerBI).\nB Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.","Azure Machine Learning tools, Deequ, SQL optimization, Great Expectations, Data Lake Infrastructure, Apache Griffin, Statistical ML techniques, CI tools, Data Analytics tools, Azure Log Analytics, ELT, Data Warehousing, Azure Data Factory, Github, Pyspark, Data Modelling, Etl, Azure Databricks, Powerbi, Python, Kubernetes, Azure, Scala"
Lead Data Engineer,Komhar Infotech Private Limited,8-10 Years,,Hyderabad,Software,"We are looking for aLeadData Engineerto join our data engineering team. The ideal candidate will have strong experience in IBM Infosphere DataStage, ETL development, and leading end-to-end data integration projects. The role requires hands-on development, team leadership, and close collaboration with business and technical stakeholders.\nKey Responsibilities:\nLead the design, development, and implementation of ETL processes using IBM DataStage.\nCollaborate with business analysts and data architects to understand data requirements.\nOptimize, maintain, and troubleshoot existing DataStage jobs and data pipelines.\nEnsure high-quality deliverables through code reviews, unit testing, and documentation.\nMentor junior team members and provide technical leadership.\nWork closely with QA and DevOps teams for deployment and release management.\nParticipate in project planning, estimation, and status reporting activities.\nRequired Skills:\nStrong hands-on experience with IBM Infosphere DataStage.\nSolid understanding of ETL concepts, data warehousing, and data modeling.\nProficient in SQL and working with large relational databases (Oracle, DB2, etc.).\nExperience with performance tuning and troubleshooting of ETL jobs.\nFamiliarity with data quality, data governance, and master data management practices.\nExcellent communication and problem-solving skills.\nPreferred Skills:\nExperience with cloud-based ETL tools or hybrid cloud environments (e.g., AWS, Azure).\nKnowledge of scheduling tools like Control-M, Autosys.\nExposure to Agile/Scrum methodologies.\nKnowledge in shell scripting.\nDataStage, PySpark, Hive, Impala & Snowflake\nWork Location: Hyderabad\nWork Mode: WFO",snowflake
Senior Data engineer,Pago Analytics India Private Limited,5-6 Years,INR 13 - 15 LPA,Hyderabad,Login to check your skill match score,"Data Engineer - Remote\nOverview:\nA highly skilled Data Engineer with 5+ years of experience specializing in cloud data architecture and integration. Expertise in leveraging AWS cloud services to build scalable data pipelines, ensuring seamless integration with cloud-based ERP systems like Microsoft Dynamics, Salesforce, and Oracle Fusion. Adept at handling large-scale data processing, transformation, and storage solutions.\n*Mandatory Skillsets:*\nAWS Cloud Services (S3, Redshift, Lambda, Glue, RDS, Athena)\nPython & PySpark (Data Transformation, Big Data Processing, Automation)\nETL Process Development & Optimization\n*ERP System Integration (Microsoft Dynamics 365, Salesforce, Oracle Fusion)*\nData Pipeline Development & Workflow Automation\nSQL & NoSQL Query Optimization\nData Modelling & Data Transformation\nData Validation, Cleansing & Quality Assurance\nPerformance Monitoring & Troubleshooting\nProject Management & Documentation","Salesforce, Oracle Fusion)*, *ERP System Integration (Microsoft Dynamics 365"
Enterprise Data Operations Analyst - Data Engineer,PepsiCo,7-9 Years,,"Hyderabad, India",Food Processing & Packaged Food,"Overview\n\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.\n\nResponsibilities\n\nAct as a subject matter expert across different digital projects.\nOversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance, and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\nDevelop and optimize procedures to productionalize data science models.\nDefine and manage SLA's for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\n\nQualifications\n\n7+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like Github and deployment & CI tools.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nFamiliarity with business intelligence tools (such as PowerBI).\nB Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.","Azure Machine Learning tools, Deequ, SQL optimization, Great Expectations, Data Lake Infrastructure, Apache Griffin, Statistical ML techniques, CI tools, Data Analytics tools, Azure Log Analytics, ELT, Data Warehousing, Azure Data Factory, Github, Pyspark, Data Modelling, Etl, Azure Databricks, Powerbi, Python, Kubernetes, Azure, Scala"
Lead Data Engineer,Komhar Infotech Private Limited,8-10 Years,,Hyderabad,Software,"We are looking for aLeadData Engineerto join our data engineering team. The ideal candidate will have strong experience in IBM Infosphere DataStage, ETL development, and leading end-to-end data integration projects. The role requires hands-on development, team leadership, and close collaboration with business and technical stakeholders.\nKey Responsibilities:\nLead the design, development, and implementation of ETL processes using IBM DataStage.\nCollaborate with business analysts and data architects to understand data requirements.\nOptimize, maintain, and troubleshoot existing DataStage jobs and data pipelines.\nEnsure high-quality deliverables through code reviews, unit testing, and documentation.\nMentor junior team members and provide technical leadership.\nWork closely with QA and DevOps teams for deployment and release management.\nParticipate in project planning, estimation, and status reporting activities.\nRequired Skills:\nStrong hands-on experience with IBM Infosphere DataStage.\nSolid understanding of ETL concepts, data warehousing, and data modeling.\nProficient in SQL and working with large relational databases (Oracle, DB2, etc.).\nExperience with performance tuning and troubleshooting of ETL jobs.\nFamiliarity with data quality, data governance, and master data management practices.\nExcellent communication and problem-solving skills.\nPreferred Skills:\nExperience with cloud-based ETL tools or hybrid cloud environments (e.g., AWS, Azure).\nKnowledge of scheduling tools like Control-M, Autosys.\nExposure to Agile/Scrum methodologies.\nKnowledge in shell scripting.\nDataStage, PySpark, Hive, Impala & Snowflake\nWork Location: Hyderabad\nWork Mode: WFO",snowflake
Senior Data Engineer,TalentBasket,5-8 Years,INR 20.5 - 25 LPA,"Cochin / Kochi / Ernakulam, Thiruvananthapuram / Trivandrum",Login to check your skill match score,"Job Description\nJob Title: Senior Data Engineer Data Quality, Ingestion & API Development\nJob Overview\nWe are seeking an experienced Senior Data Engineer to lead the development of a scalable data\ningestion framework while ensuring high data quality and validation. The successful candidate\nwill also be responsible for designing and implementing robust APIs for seamless data\nintegration. This role is ideal for someone with deep expertise in building and managing big data\npipelines using modern AWS-based technologies, and who is passionate about driving quality\nand efficiency in data processing systems.\nKey Responsibilities\nData Ingestion Framework:\no Design & Development: Architect, develop, and maintain an end-to-end data\ningestion framework that efficiently extracts, transforms, and loads data from\ndiverse sources.\no Framework Optimization: Use AWS services such as AWS Glue, Lambda,\nEMR, ECS , EC2 and Step Functions to build highly scalable, resilient, and\nautomated data pipelines.\nData Quality & Validation:\no Validation Processes: Develop and implement automated data quality checks,\nvalidation routines, and error-handling mechanisms to ensure the accuracy and\nintegrity of incoming data.\no Monitoring & Reporting: Establish comprehensive monitoring, logging, and\nalerting systems to proactively identify and resolve data quality issues.\nAPI Development:\no Design & Implementation: Architect and develop secure, high-performance\nAPIs to enable seamless integration of data services with external applications\nand internal systems.\no Documentation & Best Practices: Create thorough API documentation and\nestablish standards for API security, versioning, and performance optimization.\nCollaboration & Agile Practices:\no Cross-Functional Communication: Work closely with business stakeholders,\ndata scientists, and operations teams to understand requirements and translate\nthem into technical solutions.\no Agile Development: Participate in sprint planning, code reviews, and agile\nceremonies, while contributing to continuous improvement initiatives and CI/CD\npipeline development (using tools like GitLab).\nRequired Qualifications\nExperience & Technical Skills:\no Professional Background: At least 5 years of relevant experience in data\nengineering with a strong emphasis on analytical platform development.\no Programming Skills: Proficiency in Python and/or PySpark, SQL for\ndeveloping ETL processes and handling large-scale data manipulation.\no AWS Expertise: Extensive experience using AWS services including AWS Glue,\nLambda, Step Functions, and S3 to build and manage data ingestion frameworks.\no Data Platforms: Familiarity with big data systems (e.g., AWS EMR, Apache\nSpark, Apache Iceberg) and databases like DynamoDB, Aurora, Postgres, or\nRedshift.\no API Development: Proven experience in designing and implementing RESTful\nAPIs and integrating them with external and internal systems.\no CI/CD & Agile: Hands-on experience with CI/CD pipelines (preferably with\nGitLab) and Agile development methodologies.\nSoft Skills:\no Strong problem-solving abilities and attention to detail.\no Excellent communication and interpersonal skills with the ability to work\nindependently and collaboratively.\no Capacity to quickly learn and adapt to new technologies and evolving business\nrequirements.\nPreferred Qualifications\nBachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\nExperience with additional AWS services such as Kinesis, Firehose, and SQS.\nFamiliarity with data lakehouse architectures and modern data quality frameworks.\nPrior experience in a role that required proactive data quality management and API-\ndriven integrations in complex, multi-cluster environments.create a catchy linkedin post for this JD also mention the email to contact section to this post","Analytical Thinking, Python and/or PySpark, AWS EMR, communication, Interpersonal, AWS Step Functions, ECS / EC2, Problem-solving, Aws Lambda, Sql, AWS Glue"
Opening for Lead/Principal Data engineer- Bangalore,Hireatease Consulting Private Limited,7-14 Years,INR 40 - 55.5 LPA,Bengaluru,Information Technology,"Accountabilities\nBridge business needs with technical solutions by leading IT application design and implementation.\nCollaborate with team members to define and deliver requirements, translating them into detailed specifications.\nOptimize performance and mitigate risks through improvements in design and processes.\nAdvise on industry trends and standard methodologies to enhance performance and business outcomes.\nProvide strategic direction and guidance to IT teams and business units.\nChip in to Data & Software Engineering standards and best practices.\nResearch new technologies to boost system performance and scalability.\nLead and mentor technical teams (Data Engineering, Cloud Engineering, Software Engineering etc.) and work with AI/GenAI leads to foster collaboration and innovation.\nLead technical teams in design, architecture, and innovation, resolving complex issues.\nEnsure platform stability, scalability, and simplicity.\nFoster continuous improvement and innovation.\nBe responsible for technical leadership throughout the software development lifecycle and provide hands-on support in development, technical delivery, review, and testing while collaborating with global teams.\nEssential Skills/Experience\n9+ years in software and data engineering.\nStrong problem-solving with a focus on high-quality solutions.\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python","Data Brick, Governance, Modeling, Py Spark, Python"
Azure data Engineer,Xpetize Technology Solutions Private Limited,8-13 Years,INR 15 - 22 LPA,Remote,Login to check your skill match score,"Design and develop warehouse solutions using Azure Synapse Analytics, ADLS, ADF, Databricks, Power BI, Azure Analysis Services\nShould be proficient in SSIS, SQL and Query optimization.\nShould have worked in onshore offshore model managing challenging scenarios.\nExpertise in working with large amounts of data (structured and unstructured), building data pipelines for ETL workloads and generate insights utilizing Data Science, Analytics.\nExpertise in Azure, AWS cloud services, and DevOps/CI/CD frameworks.\nAbility to work with ambiguity and vague requirements and transform them into deliverables.\nGood combination of technical and interpersonal skills with strong written and verbal communication; detail-oriented with the ability to work independently.\nDrive automation efforts across the data analytics team utilizing Infrastructure as Code (IaC) using Terraform, Configuration Management, and Continuous Integration (CI) / Continuous Delivery (CD) tools such as Jenkins.\nHelp build define architecture frameworks, best practices & processes. Collaborate on Data warehouse architecture and technical design discussions.\nExpertise in Azure Data factory and should be familiar with building pipelines for ETL projects.\nExpertise in SQL knowledge and experience working with relational databases.\nExpertise in Python and ETL projects\nExperience in data bricks will be of added advantage.\nShould have expertise in data life cycle, data ingestion, transformation, data loading, validation, and performance tuning.","Azure Data Factory, Azure Data Lake, Ssis"
Data Engineer,Freelancer Riddhi Govind Nighojkar,5-10 Years,INR 5 - 16 LPA,Remote,Login to check your skill match score,"Snowflake with DBTExperience: 5-10 YearsMandatory skills: Candidate should be on Data Engineer background,Snowflake, SQL, Looker(All mandatory)Good to have: PythonSkills:1. Expertise in building data platforms & warehouses with Snowflake covering all aspects of data engineering practices.2. Expertise in using ETL tools like DBT etc.3. Expert at writing and optimizing SQL queries.4. Knowledge of AWS/Azure and various services related to data engineering is a must.5. Should have worked on developing and optimizing data pipelines (spark code).6. Should have experience with NFRs for data platforms like data quality, governance, security and compliance aspects.","snowflake, looker, Data Warehousing, Sql, Python Language"
GCP Data Engineer,Growel Softech Private Limited,8-10 Years,INR 25 - 30 LPA,"Mumbai City, Bengaluru, Chennai",Login to check your skill match score,"Description\nWe are seeking a skilled GCP Data Engineer with 8-10 years of experience to join our dynamic team in India. The ideal candidate will be responsible for designing and implementing robust data processing systems on Google Cloud Platform, ensuring seamless data flow and high-quality data management.\nResponsibilities\nDesign and implement data processing systems on Google Cloud Platform (GCP) using tools like BigQuery, Dataflow, and Pub/Sub.\nDevelop and maintain ETL processes to ensure efficient data ingestion and transformation.\nCollaborate with data scientists and analysts to understand data requirements and provide solutions.\nMonitor and optimize the performance of data pipelines and storage solutions.\nEnsure data quality and integrity by implementing validation and testing strategies.\nParticipate in architecture discussions and contribute to the design of data solutions.\nSkills and Qualifications\n8-10 years of experience in data engineering or a related field.\nProficiency in Google Cloud Platform services such as BigQuery, Dataflow, and Pub/Sub.\nStrong programming skills in languages such as Python, Java, or Scala.\nExperience with SQL and data modeling techniques.\nFamiliarity with data warehousing concepts and methodologies.\nKnowledge of data pipeline orchestration tools like Apache Airflow or similar.\nUnderstanding of data governance and security best practices.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.","ETL Processes, Pub/Sub, Cloud Storage, BigQuery, Data Modeling, Terraform, Apache Beam, DataFlow, Sql, Python"
Sr. DATA Engineer,Number 11,6-15 Years,INR 22 - 28.5 LPA,Pune,Login to check your skill match score,"We are hiring for our client NCS (Singapore based organization).\nSkills required -SQL , Python, ETL,Pyspak\nKey Responsibilities:\nDesign, build, and optimize scalable#SQL-based data pipelines\nDevelop and manage#ETL workflowsacross varied data sources\nCollaborate with cross-functional teams (data analysts, scientists, engineers)\nWrite clean#Pythonscripts for automation and data manipulation\nUtilize#PySparkfor big data processing and distributed workflows\nMonitor and troubleshoot pipeline performance and ensure data reliability\nMaintain data governance, security, and integrity standards\nRequired Skills & Qualifications:\n68 yearsof experience in data engineering or related fields\nExpert-level proficiency in #SQL complex queries, tuning, stored procedures\nStrong understanding of#ETL processesanddata warehousing\nProficiency in#Pythonscripting\nHands-on experience with#PySparkor other distributed frameworks\nFamiliarity with#PostgreSQL, #MySQL, #SQLServer, or other relational DBs\nExcellent analytical and problem-solving skills","Py Spark, Sql, Python, Etl"
Big Data Engineer,Robotics Technologies,10-15 Years,INR 10 - 15.5 LPA,"Hyderabad, Bengaluru, Kolkata","Sales Automation, Embedded Software, Artificial Intelligence, Embedded Systems, Presentation Software, Web Development, File Sharing","We are seeking a seasoned Big Data Engineer with 10-15 years of experience to join our dynamic team in India. The ideal candidate will have a strong background in designing and implementing data processing systems and will be responsible for managing large-scale data pipelines.\nResponsibilities\nDesign and implement scalable data pipelines to process large datasets.\nDevelop and maintain ETL processes to extract, transform, and load data from various sources.\nOptimize and tune data processing systems for maximum performance and efficiency.\nCollaborate with data scientists and analysts to understand data requirements and deliver solutions.\nEnsure data quality and integrity across all data systems.\nImplement data security and compliance measures in line with industry standards.\nMonitor and troubleshoot data pipeline performance and issues.\nSkills and Qualifications\n10-15 years of experience in Big Data technologies such as Hadoop, Spark, and Kafka.\nProficiency in programming languages such as Java, Scala, or Python.\nExperience with data warehousing solutions like Amazon Redshift, Google BigQuery, or Snowflake.\nStrong knowledge of SQL and NoSQL databases.\nFamiliarity with cloud platforms like AWS, Azure, or Google Cloud Platform.\nExperience with containerization and orchestration tools such as Docker and Kubernetes.\nUnderstanding of data modeling and data architecture best practices.","Cloud Platforms, Nosql, Hadoop, Etl Tools, Data Modeling, Spark, Kafka, Data Warehousing, Sql, Python"
Senior Data engineer,Pago Analytics India Private Limited,5-6 Years,INR 13 - 15 LPA,Hyderabad,Login to check your skill match score,"Data Engineer - Remote\nOverview:\nA highly skilled Data Engineer with 5+ years of experience specializing in cloud data architecture and integration. Expertise in leveraging AWS cloud services to build scalable data pipelines, ensuring seamless integration with cloud-based ERP systems like Microsoft Dynamics, Salesforce, and Oracle Fusion. Adept at handling large-scale data processing, transformation, and storage solutions.\n*Mandatory Skillsets:*\nAWS Cloud Services (S3, Redshift, Lambda, Glue, RDS, Athena)\nPython & PySpark (Data Transformation, Big Data Processing, Automation)\nETL Process Development & Optimization\n*ERP System Integration (Microsoft Dynamics 365, Salesforce, Oracle Fusion)*\nData Pipeline Development & Workflow Automation\nSQL & NoSQL Query Optimization\nData Modelling & Data Transformation\nData Validation, Cleansing & Quality Assurance\nPerformance Monitoring & Troubleshooting\nProject Management & Documentation","Salesforce, Oracle Fusion)*, *ERP System Integration (Microsoft Dynamics 365"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements\n\nJob Requirements\n\nRole/ Job Title: Data Engineer - Gen AI\n\nFunction/ Department: Data & Analytics\n\nPlace of Work: Mumbai\n\nJob Purpose\n\nThe data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.\n\nJob & Responsibilities\n\nBuild data engineering pipeline focused on unstructured data pipelines\nConduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.\nDesign, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.\nWork on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability\nDrive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets\nA suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.\nCreating Technical Design Documentation of the projects/pipelines\nGood skills in technical debugging of the code in case of issues. Also, working with git for code versioning\n\nEducation Qualification\n\nGraduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)\n\nPost-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA\n\nExperience Range : 5-10 years of relevant experience","big data infrastructure, HDFS, Hive, Dynamodb, Api, MongoDB, HBase, Yarn, Mapreduce, Etl"
Data Engineer,MResult,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Why MResult\nFounded in 2004, MResult is a global digital solutions partner trusted by leading Fortune 500 companies in industries such as pharma & healthcare, retail, and BFSI. MResult's expertise in data and analytics, data engineering, machine learning, AI, and automation help companies streamline operations and unlock business value. As part of our team, you will collaborate with top minds in the industry to deliver cutting-edge solutions that solve real-world challenges.\nWhat We Offer:\nAt MResult, you can leave your mark on projects at the world's most recognized brands, access opportunities to grow and upskill, and do your best work with the flexibility of hybrid work models. Great work is rewarded, and leaders are nurtured from within.\nOur values Agility, Collaboration, Client Focus, Innovation, and Integrity are woven into our culture, guiding every decision.\nWhat This Role Requires:\nIn the role of Data Engineer, you will be a key contributor to MResult's mission of empowering our clients with data-driven insights and innovative digital solutions. Each day brings exciting challenges and growth opportunities. Here is what you will do:\nRoles & Responsibilities:\nProject solutioning, including scoping, and estimation.\nData sourcing, investigation, and profiling.\nPrototyping and design thinking.\nDeveloping data pipelines & complex data workflows.\nActively contribute to project documentation and playbook, including but not limited to physical models, conceptual models, data dictionaries and data cataloguing.\nKey Skills to Succeed in This Role:\nOverall 6+ years of experience, 3+ years of hands-on experience in working with Python in building data pipelines and processes.\nProficiency in SQL programming, including the ability to create and debug stored procedures, functions, and views.\n3+ years of hands-on experience delivering data lake/data warehousing projects. Innovation Insights.\nExperience in working with cloud native SQL and NoSQL database platforms. Snowflake experience is desirable.\nExperience in AWS services EC2, EMR, RDS, Spark is preferred.\nSolid understanding of Scrum/Agile is preferred and working knowledge of CI/CD, GitHub MLflow.\nManage, Master, and Maximize with MResult\nMResult is an equal-opportunity employer committed to building an inclusive environment free of discrimination and harassment.\nTake the next step in your career with MResult where your ideas help shape the future.","Cloud Native SQL, snowflake, MLflow, Nosql, Github, Data Lake, Data Warehousing, Python, Sql"
Data Engineer,Cardinal Health,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Department Overview:\naugmented Intelligence (augIntel) builds automation, analytics and artificial intelligence solutions that drive success for Cardinal Health by creating material savings, efficiencies and revenue growth opportunities. The team drives business innovation by leveraging emerging technologies and turning them into differentiating business capabilities.\nJob Overview:\nDesigning, building and operationalizing large-scale enterprise data solutions and applications using one or more of Google Cloud Platform data and analytics services in combination with technologies like Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, Cloud BigQuery, Cloud PubSub, Cloud Functions, Airflow.\nResponsibilities:\nDesigning and implementing data transformation, ingestion and curation functions on GCP cloud using GCP native or custom programming\nDesigning and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Python. Optimizing data pipelines for performance and cost for large scale data lakes.\nDesired Qualifications:\nBachelor's degree preferred or equivalent work experience.\n5+ years of engineering experience in , Data Analytics and Data Integration related fields.\n3+ years of experience writing complex SQL queries, stored procedures, etc.\n1+ years of hands-on GCP experience in Data Engineering and Cloud Analytics solutions.\nExperience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery.\nAgile development skills and experience.\nExperience with CI/CD pipelines such as Concourse, Jenkins.\nGoogle Cloud Platform certification is a plus.\nPerks and benefits\nVariable pay, WiFi reimbursement, Cab facilities, shift allowance (1PM-10PM)","Airflow, Cloud Functions, Concourse, Cloud Dataflow, Cloud DataProc, CI CD pipelines, Cloud PubSub, Cloud BigQuery, Google Cloud Platform, Sql, Jenkins, Spark, Apache Beam, Python"
Data Engineer,NTT DATA North America,Fresher,,"Bengaluru, India",Login to check your skill match score,"Req ID: 321499\n\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties:\n\nWork closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\nWork closely with Data modeller to ensure data models support the solution design\nDevelop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\nAnalysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\nDevelop documentation and artefacts to support projects\n\nMinimum Skills Required:\n\nADF\nFivetran (orchestration & integration)\nSQL\nSnowflake DWH\n\nAbout NTT DATA\n\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com\n\nNTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Snowflake DWH, Fivetran, Adf, Sql"
Data Engineer,Knight Frank India,2-5 Years,,"Mumbai, India",Real Estate,"Responsibilities\nTo work with the multiple teams to extend the team's data integration, modelling and reporting capabilities\nDevelop and Maintain Scalable Data Pipelines: Design and implement robust data pipelines using Azure Data Bricks (ADB) and Microsoft Fabric to efficiently process and transform large datasets, ensuring data quality and timely delivery across various business units.\nProvide solution and integration development requirements, functional specs, design, custom development, integration, testing, and deployment\nLeverage Azure DevOps and Git to establish and maintain Cl/CD pipelines for automating the deployment\nand monitoring of data integration workflows, ensuring smooth updates and version control across environments.\nTo be able to work with a degree of independence on projects\nTo ensure easy access to data for the wider team, collaborating with stakeholders and Knight Frank teams on projects, analysing and cleansing complex datasets, and advising on best practices for data integration and modelling.\nDevelop and maintain scalable RESTful APIs for seamless data exchange, leveraging JSON for efficient data ingestion, transformation, and integration across systems, ensuring high performance and reliability.\nExperience\nDemonstrated 2-3 years of strong experience using T-SQL.\nMore than 3 years of working experience in developing Data warehouses.\nMore than 3 years of experience in building ETL/ELT pipelines, serving both on-prem and cloud-based data warehouses.\nWorking as a data engineer in Azure based projects for not less than 2 years, having an in-depth understanding of the following:\nETL Pipelines developed with Azure Data Bricks & Azure Synapse pipelines\nStrong proficiency in Python and PySpark for developing efficient ETL workflows, data processing, and automation of data pipelines in distributed environments.\nHands-on experience with Azure DevOps, Git, and Cl/CD pipeline automation for continuous integration\nand delivery of data workflows and infrastructure deployments.\nDemonstrated working experience that requires understanding of underlying data structures and different integration methods and uses them to develop quality solutions within agreed timelines.\nDemonstrated working with multiple stakeholders across multiple disciplines to understand their data\nrequirements and enhancing their current architecture.\nHaving strong troubleshooting and error-handling skillset, along with a keen inclination towards process automation.\nProactive to remain up to date with latest technologies and techniques.\nDemonstrated experience in working effectively as part of a team, managing/mentoring individuals - all along showing a strong collaborative mindset and resourcefulness.\nSystems\nStrong experience with the following\nT-SQL & Data Modelling\nAzure Data Bricks\nSQL Server\nData Security & Governance\nCl/CD & DevOps\nREST API & JSON\nExperience of the following desirable\nPower BI\nSSIS\nMicrosoft Fabric\nAzure Synapse\nParticular Aptitudes/Skills Required\nHighly organized, systematic and adaptable with an excellent attention to detail, with the ability to recognize the relative importance of software issues and to prioritize work effectively\nAbility to communicate clearly and deal with others at all levels in a polite, professional, friendly and helpful manner, both face to face, by email and on the telephone and always maintain a good working relationship\nDemonstrated an ability to work on multiple projects simultaneously\nAbility to work with all members of the team, in a professional and yet dynamic and creative environment, fostering an open, friendly and constructive working relationship with all members of the team\nAbility to work within a high-pressure environment, balance priorities and remain calm under pressure\nThe successful candidate will be flexible, self-motivated, organized and pro-active with excellent computing and administration skills and the ability to adapt to a wide range of tasks. They will also have a hands on attitude and possess the necessary skills, manner and experience to provide an effective support service to the department/office\nDesire to learn new technologies and continuously develop new skills and expertise\nAbility to work out of hours to deliver application upgrades in agreed maintenance windows","Data Security Governance, Rest Api, Azure Synapse, T-sql, Azure Data Bricks, Pyspark, SQL Server, Json, Python, Azure DevOps"
Data Engineer,Cardinal Health,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Department Overview:\naugmented Intelligence (augIntel) builds automation, analytics and artificial intelligence solutions that drive success for Cardinal Health by creating material savings, efficiencies and revenue growth opportunities. The team drives business innovation by leveraging emerging technologies and turning them into differentiating business capabilities.\nJob Overview:\nDesigning, building and operationalizing large-scale enterprise data solutions and applications using one or more of Google Cloud Platform data and analytics services in combination with technologies like Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, Cloud BigQuery, Cloud PubSub, Cloud Functions, Airflow.\nResponsibilities:\nDesigning and implementing data transformation, ingestion and curation functions on GCP cloud using GCP native or custom programming\nDesigning and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Python. Optimizing data pipelines for performance and cost for large scale data lakes.\nDesired Qualifications:\nBachelor's degree preferred or equivalent work experience.\n5+ years of engineering experience in , Data Analytics and Data Integration related fields.\n3+ years of experience writing complex SQL queries, stored procedures, etc.\n1+ years of hands-on GCP experience in Data Engineering and Cloud Analytics solutions.\nExperience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery.\nAgile development skills and experience.\nExperience with CI/CD pipelines such as Concourse, Jenkins.\nGoogle Cloud Platform certification is a plus.\nPerks and benefits\nVariable pay, WiFi reimbursement, Cab facilities, shift allowance (1PM-10PM)","Airflow, Cloud Functions, Concourse, Cloud Dataflow, Cloud DataProc, CI CD pipelines, Cloud PubSub, Cloud BigQuery, Google Cloud Platform, Sql, Jenkins, Spark, Apache Beam, Python"
Data Engineer,NTT DATA North America,Fresher,,"Bengaluru, India",Login to check your skill match score,"Req ID: 321499\n\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties:\n\nWork closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\nWork closely with Data modeller to ensure data models support the solution design\nDevelop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\nAnalysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\nDevelop documentation and artefacts to support projects\n\nMinimum Skills Required:\n\nADF\nFivetran (orchestration & integration)\nSQL\nSnowflake DWH\n\nAbout NTT DATA\n\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com\n\nNTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Snowflake DWH, Fivetran, Adf, Sql"
Data engineer,Qloron Pvt Ltd,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Senior Data Engineer\n\nExperience: Minimum 5+ Years\n\nEmployment Type: Full-Time\n\nJob Summary\n\nWe are seeking a skilled and experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in Power BI development, data modeling, and SQL, with the ability to translate business requirements into actionable insights. You will be responsible for end-to-end dashboard development, managing junior developers, and optimizing performance across reports and dashboards.\n\nKey Responsibilities\n\nAnalyze business requirements and translate them into data models and reporting solutions.\nPerform GAP analysis between existing data models and business needs.\nDesign and model efficient Power BI schemas and architecture.\nTransform and prepare data using Power BI, SQL, and ETL tools.\nDevelop complex DAX formulas, measures, and calculated columns for analytics.\nCreate visually appealing and functional Power BI reports and dashboards.\nWrite SQL queries and stored procedures to retrieve and manage data effectively.\nDesign robust Power BI solutions aligned with business objectives.\nLead and guide a team of Power BI developers, ensuring high-quality deliverables.\nIntegrate data from multiple sources into Power BI for holistic analysis.\nOptimize the performance of Power BI dashboards and reports.\nCollaborate with business stakeholders to align deliverables with strategic goals.\n\nRequired Skills\n\nMinimum 5+ years of hands-on experience with Power BI development.\nStrong proficiency in DAX, Power Query, and Power BI Service.\nExcellent command of SQL, including stored procedures.\nProven experience in data modeling, data transformation, and ETL processes.\nStrong understanding of data warehousing concepts (mandatory).\nExperience working with multiple data sources and integrating them within Power BI.\nLeadership capabilities to manage and mentor junior developers.\nSolid communication and stakeholder management skills.\n\nPreferred Qualifications\n\nBachelor's Degree in Computer Science, Information Technology, or equivalent.\nKnowledge of Data Engineering concepts is a plus.\nExperience with cloud platforms such as Azure or AWS is advantageous.","Data Modeling, Power Bi, Data Warehousing, Power Query, Dax, Sql, Etl"
Data Engineer,Knight Frank India,2-5 Years,,"Mumbai, India",Real Estate,"Responsibilities\nTo work with the multiple teams to extend the team's data integration, modelling and reporting capabilities\nDevelop and Maintain Scalable Data Pipelines: Design and implement robust data pipelines using Azure Data Bricks (ADB) and Microsoft Fabric to efficiently process and transform large datasets, ensuring data quality and timely delivery across various business units.\nProvide solution and integration development requirements, functional specs, design, custom development, integration, testing, and deployment\nLeverage Azure DevOps and Git to establish and maintain Cl/CD pipelines for automating the deployment\nand monitoring of data integration workflows, ensuring smooth updates and version control across environments.\nTo be able to work with a degree of independence on projects\nTo ensure easy access to data for the wider team, collaborating with stakeholders and Knight Frank teams on projects, analysing and cleansing complex datasets, and advising on best practices for data integration and modelling.\nDevelop and maintain scalable RESTful APIs for seamless data exchange, leveraging JSON for efficient data ingestion, transformation, and integration across systems, ensuring high performance and reliability.\nExperience\nDemonstrated 2-3 years of strong experience using T-SQL.\nMore than 3 years of working experience in developing Data warehouses.\nMore than 3 years of experience in building ETL/ELT pipelines, serving both on-prem and cloud-based data warehouses.\nWorking as a data engineer in Azure based projects for not less than 2 years, having an in-depth understanding of the following:\nETL Pipelines developed with Azure Data Bricks & Azure Synapse pipelines\nStrong proficiency in Python and PySpark for developing efficient ETL workflows, data processing, and automation of data pipelines in distributed environments.\nHands-on experience with Azure DevOps, Git, and Cl/CD pipeline automation for continuous integration\nand delivery of data workflows and infrastructure deployments.\nDemonstrated working experience that requires understanding of underlying data structures and different integration methods and uses them to develop quality solutions within agreed timelines.\nDemonstrated working with multiple stakeholders across multiple disciplines to understand their data\nrequirements and enhancing their current architecture.\nHaving strong troubleshooting and error-handling skillset, along with a keen inclination towards process automation.\nProactive to remain up to date with latest technologies and techniques.\nDemonstrated experience in working effectively as part of a team, managing/mentoring individuals - all along showing a strong collaborative mindset and resourcefulness.\nSystems\nStrong experience with the following\nT-SQL & Data Modelling\nAzure Data Bricks\nSQL Server\nData Security & Governance\nCl/CD & DevOps\nREST API & JSON\nExperience of the following desirable\nPower BI\nSSIS\nMicrosoft Fabric\nAzure Synapse\nParticular Aptitudes/Skills Required\nHighly organized, systematic and adaptable with an excellent attention to detail, with the ability to recognize the relative importance of software issues and to prioritize work effectively\nAbility to communicate clearly and deal with others at all levels in a polite, professional, friendly and helpful manner, both face to face, by email and on the telephone and always maintain a good working relationship\nDemonstrated an ability to work on multiple projects simultaneously\nAbility to work with all members of the team, in a professional and yet dynamic and creative environment, fostering an open, friendly and constructive working relationship with all members of the team\nAbility to work within a high-pressure environment, balance priorities and remain calm under pressure\nThe successful candidate will be flexible, self-motivated, organized and pro-active with excellent computing and administration skills and the ability to adapt to a wide range of tasks. They will also have a hands on attitude and possess the necessary skills, manner and experience to provide an effective support service to the department/office\nDesire to learn new technologies and continuously develop new skills and expertise\nAbility to work out of hours to deliver application upgrades in agreed maintenance windows","Data Security Governance, Rest Api, Azure Synapse, T-sql, Azure Data Bricks, Pyspark, SQL Server, Json, Python, Azure DevOps"
Data Engineer,PwC Acceleration Centers,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"ole: Associate\nTower: Data, Analytics & Specialist Managed Service\nExperience: : 3 -5.5 years\nKey Skills: AWS , Snowflake, DBT\nEducational Qualification: BE / B Tech / ME / M Tech / MBA\nWork Location: Bangalore\nJob Description\nAs a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:\nUse feedback and reflection to develop self-awareness, personal strengths, and address development areas.\nFlexible to work in stretch opportunities/assignments.\nDemonstrate critical thinking and the ability to bring order to unstructured problems.\nTicket Quality and deliverables review, Status Reporting for the project.\nAdherence to SLAs, experience in incident management, change management and problem management.\nSeek and embrace opportunities which give exposure to different situations, environments, and perspectives.\nUse straightforward communication, in a structured way, when influencing and connecting with others.\nAble to read situations and modify behavior to build quality relationships.\nUphold the firm's code of ethics and business conduct.\nDemonstrate leadership capabilities by working, with clients directly and leading the engagement.\nWork in a team environment that includes client interactions, workstream management, and cross-team collaboration.\nGood team player, take up cross competency work and contribute to COE activities.\nEscalation/Risk management.\nPosition Requirements:\nRequired Skills:\nAWS Cloud Engineer:\nJob description:\nCandidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas:\nShould have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms.\nShould have minimum 1-3 years of Operate/Managed Services/Production Support Experience\nShould have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc.\nDesigning and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes.\nShould have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage.\nImplementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases\nShould have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools\nScaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments.\nShould have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc.\nShould have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc.\nShould have Strong communication, problem solving, quantitative and analytical abilities.\nNice to have:\nAWS certification\nManaged Services- Data, Analytics & Insights Managed Service\nAt PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients better.\nWithin our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client's enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC's Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today's dynamic business environment.\nWithin our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective.\nAs a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.\nKindly share your resume to [HIDDEN TEXT]\nRegards,\nMirunalini MJ","AWS DMS, Prefect, snowflake, dbt, Aws Lambda, Hadoop, Collibra, Pyspark, AWS Glue, Informatica, Sql, Spark, Python, AWS"
Data Engineer,Tredence Inc.,1-3 Years,,"Bengaluru, India",Login to check your skill match score,"Data Engineer\nExperience Level: 12 months to 16 months only (Excluding internship)\nJob location- Bengaluru, Chennai, Gurgaon, Kolkata, Pune, Hyderabad\nB.Tech/BE/ME/M.Tech- 2023 & 2024 Grads only. Rest can avoid please\nMUST HAVE\nMinimum of 12 months of Data Engineering experience.\nStrong technical knowledge of tools like Azure Data Factory/Databricks/GCP/Snowflake, SQL, Python\n. Experience in collaborating with business stakeholders to identify and meet data requirements\nExperience in using Azure services and tools to ingest, egress, and transform data from multiple sources\nDelivered ETL/ELT solutions including data extraction, transformation, cleansing, data integration and data management\nImplemented batch & near real time data ingestion pipelines\nExperience in working on Event-driven cloud platform for cloud services and apps, Data integration for building and managing pipeline, Data warehouse running on serverless infrastructure, Workflow orchestration using Azure Cloud Data Engineering components Databricks, Synapse, etc.\nExcellent written and oral communication skills\nGOOD TO HAVE Proven ability to work with large cross functional teams with good communication skills.\nExperience on Cloud migration methodologies and processes\nAzure Certified Data Engineer\nExposure to Azure Dev ops and Github\nAbility to drive customer calls independently\nAbility to take ownership and work with team","snowflake, Workflow orchestration, Azure services, Event-driven cloud platform, Synapse, Azure Cloud components, Databricks, Data Integration, Sql, ELT, Azure Data Factory, Data Management, Etl, Python, Gcp"
Data Engineer,Catalyst Clinical Research,3-5 Years,,India,Login to check your skill match score,"Catalyst Clinical Research provides customizable solutions to the biopharmaceutical and biotechnology industries through Catalyst Oncology, a full-service oncology CRO,andmulti-therapeutic global functional and CRO services through Catalyst Flex. The company's customer-centric flexible service model, innovative technology, expert team members, and global presence advance clinical studies. Visit CatalystCR.com\n\nThe Data Engineer is a key member of the Data Engineering Team responsible for performing tasks related to all aspects of DataOps lifecycle. The products and services you build will enable analytical applications, AI products, and integrations enterprise systems. You--along with your teammates-- will work with the internal and external stakeholders to turn requirements into solutions that will drive better decision making through the organization. You will develop marts, warehouses, models, and logic that contain and distill the intricate innerworkings of Catalyst through data; you will work with the Associate Director, Enterprise Data Architecture to ensure this fit within the overall analytics framework.\n\nDesign, build, and maintain scalable data pipelines using Databricks Delta, and structured streaming with Delta Live Tables.\nManage Unity Catalog for efficient data governance across multiple domains, regions, and end users.\nDevelop and manage transformations in dbt and Databricks using Medallion/Multi-Hop Architecture, ensuring best practices in code quality and data modeling.\nManage DAG workflows using systems like Airflow or Databricks Workflows to optimize data processing tasks.\nBuild upon our CI/CD strategies on GitLab/GitActions for automated testing and deployment of data artifacts.\nCollaborate with cross-functional teams across different regions to ensure highly-availability and quality data integration.\n\nEducation: B.S. or M.S. Computer Science, Engineering, Economics, Mathematics, related field, or relevant experience\n\nExperience\n\n3+ years of Data Engineering experience, including Webhooks, API, ELT/ETL, Data Lakehouse Architecture, and Event-Driven Architectures.\n3+ years of Data Architecture experience, including data modeling for semantic layers, normalization forms and OBT.\n3+ years of experience with cloud computing technologies (Azure, AWS, GCP)\n3+ years of experience with the Databricks Data Intelligence platform\nWorking knowledge of UX design methods as it relates to analytical and AI platforms.\nPrior experience with project management tools such as JIRA.\n\nRequired Certifications: N/A\n\nRequired Skills;\n\nProficient in Python or PySpark\nProficient in SQL/ Spark SQL\nSolid understanding of cloud computing environments like (Azure, AWS, GCP)\nKnowledge of Big Data technologies (Spark)\nSolid exposure to Delta Live Tables and Databricks Workflows.\nSolid understanding of Data Lakehouse design\nSolid understanding of code modularization strategies for Jupyter notebook -style coding\nAdept with data structures such as delta, parquet, YAML, XML, JSON, and HTML\nProficient in the administration of the Databricks platform\nStrong organizational, problem-solving, and analytical skills\nAbility to manage priorities and workflow.\nProven ability to handle multiple projects and meet deadlines.\nStrong interpersonal skills.\nAbility to deal effectively with a diversity of individuals at all organizational levels.\nCommitment to excellence and high standards.\nCreative, flexible, and innovative team player.\nAbility to work independently and as a member of various teams and committees.\n\nNice To Have\n\nDemonstrated experience working with MLFlow, create feature stores, develop using vector databases, or graph databases.\nFamiliarity with SDTM, FHIR, HL7 & SMART.\n\nWorking Hours\n\nEvery day: 1:30 PM - 9:00 PM IST\n\nOR\n\nMonday, Wednesday, Friday: 2:30 PM - 10:30 PM IST\nTuesday, Thursday: 9:00 AM - 5:00 PM IST\n\nNote: Working hours may vary based on individual seniority, business demand, and ability to work independently. This will be evaluated on a case-by-case basis.\n\nPI265776752","Airflow, GitLab GitActions, SQL Spark SQL, dbt, Big Data technologies Spark, Delta Live Tables, Python or PySpark, Data Lakehouse design, Databricks"
Data Engineer,ThoughtSpot,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"We are seeking an experienced Data Engineer to join our AI search team. As a Senior Data Engineer at ThoughtSpot, you will be responsible for designing, building, and maintaining the data infrastructure that powers our analytics platform and underlying Natural Language Search engine . You will work closely with data scientists, software engineers, and product managers to ensure our data systems are robust, scalable, and efficient. We have a rapidly expanding list of happy customers who love our product and we're growing to serve even more\n\nWhat You'll Do\n\nDesign, develop, and maintain scalable data pipelines to process large volumes of data from various sources.\nWorking closely with our product teams and ML engineers to build features that directly impact the robustness and accuracy of our search infrastructure and NLS engine\nEnsure data quality and consistency through rigorous testing and validation processes.\nMonitor and troubleshoot data pipeline performance and resolve any issues\n\nWhat You Bring\n\n6+ years of experience in python, building data infra and pipelines for product companies\nProficiency in programming languages such as Python and data processing libraries such as Pandas and Numpy. Experience with Java is good to have.\nExperience building and maintaining large data pipelines, data infrastructure, (Kubernetes preferred) and Machine learning infrastructure\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases/warehouses\nWorking knowledge of message queuing, stream processing, and highly scalable data stores\nExperience working with data tools: Spark, Kafka, etc.\nExperience with data pipeline and workflow management tools like Dagster or Airflow with code and no-code ETL implementations\nExperience with cloud services such as AWS, GCP, Azure etc\nKnowledge of data warehousing concepts and experience with databases like Snowflake, Redshift with experience with data model design, and optimization for data warehousing\nUnderstanding of data governance, data quality, data anonymization and security best practices.\nAdvanced knowledge of development good practices such as testing, code reviews and git.\nExperience with monitoring, tracing tools and alerting tools.\nStrong problem-solving skills and the ability to work independently and as part of a team.\nYou love building and leading exceptional teams in a fast-paced, entrepreneurial environment. You have a strong bias for action and being resourceful\nYou are obsessed with the customer experience and uncompromising engineering standards\nBring amazing problem-solving skills and an ability to identify, quantify, debug, and remove bottlenecks and functional issues\nGreat communication skills, both verbal and written, and an interest in working with a diverse set of peers and customers\nAlignment with ThoughtSpot Values\n\nWhat makes ThoughtSpot a great place to work\n\nThoughtSpot is the experience layer of the modern data stack, leading the industry with our AI-powered analytics and natural language search. We hire people with unique identities, backgrounds, and perspectivesthis balance-for-the-better philosophy is key to our success. When paired with our culture of Selfless Excellence and our drive for continuous improvement (2% done), ThoughtSpot cultivates a respectful culture that pushes norms to create world-class products. If you're excited by the opportunity to work with some of the brightest minds in the business and make your mark on a truly innovative company, we invite you to read more about our mission, and apply to the role that's right for you.\n\nThoughtSpot for All\n\nBuilding a diverse and inclusive team isn't just the right thing to do for our people, it's the right thing to do for our business. We know we can't solve complex data problems with a single perspective. It takes many voices, experiences, and areas of expertise to deliver the innovative solutions our customers need. At ThoughtSpot, we continually celebrate the diverse communities that individuals cultivate to empower every Spotter to bring their whole authentic self to work. We're committed to being real and continuously learning when it comes to equality, equity, and creating space for underrepresented groups to thrive. Research shows that in order to apply for a job, women feel they need to meet 100% of the criteria while men usually apply after meeting 60%. Regardless of how you identify, if you believe you can do the job and are a good match, we encourage you to apply.","Airflow, Machine Learning Infrastructure, Data Anonymization, Alerting Tools, Tracing Tools, snowflake, Dagster, Security Best Practices, Kafka, Redshift, Sql, Data Quality, Numpy, Pandas, Gcp, Spark, Data Warehousing, Data Governance, Azure, Python, Kubernetes, AWS"
Data Engineer,Aryng,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Welcome! You made it to the job description page!\n\nAryng is looking for a Data Engineer with experience in developing enterprise-class distributed data engineering solutions on the cloud. We are seeking an entrepreneurial and technology-proficient Data Engineer who is an expert in the implementation of a large-scale, highly efficient data platform, batch, and real-time pipelines and tools for Aryng clients. This role is based out of India. You will work closely with a team of highly qualified data scientists, business analysts, and\n\nengineers to ensure we build effective solutions for our clients. Your biggest strength is creative and effective problem-solving.\n\nKey Responsibilities:\n\nShould have implement asynchronous data ingestion, high volume stream data processing, and real-time data analytics using various Data Engineering Techniques\nImplement application components using Cloud technologies and infrastructure\nAssist in defining the data pipelines and able to identify bottlenecks to enable the adoption of data management methodologies\nImplementing cutting edge cloud platform solutions using the latest tools and platforms offered by GCP, AWS, and Azure. (AWS is preferred)\n\nFunctional capabilities: Requirement gathering, Client Mgt, team handling, Program delivery, Project Management (Project Estimation, Scope of Project, Agile methodology).\n\nRequirements\n\n3-5 years of data engineering experience is a must\n3+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers. AWS is preferred.\nShould be comfortable working and interacting with clients\n2+ years experience in Python\nMust be strong in SQL and its concepts\nExperience in Big Query, Snowflake, Redshift, DBT\nStrong understanding of data warehousing, data lake, and cloud concepts\nExcellent communication and presentation skills\nExcellent problem-solving skills, highly proactive and self-driven\nConsulting background is a big plus\nMust have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or a related area of study\nWorking knowledge of Airflow is preferred\n\nGood to have:\n\n\nExperience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark\nExperience in Tableau, Looker, or other BI tools is preferred\n\nAvailability:\n\n\nAvailable to join immediately\n\nThis role requires mandatory overlap hours with clients in the US from 8 am to 1 pm PST.\n\n\nBenefits\n\nDirect Client Access\nFlexible work hours\nRapidly Growing Company\nAwesome work culture\nLearn From Experts\nWork-life Balance\nCompetitive Salary\nExecutive Presence\nEnd to End Problem Solving\n50%+ Tax Benefit\n100% Remote company\nFlat Hierarchy\nOpportunity to become a thought leader\n\nWhy Join Aryng: Click on the Youtube link","Big Query, Airflow, dbt, snowflake, Data Lake, Data Warehousing, Redshift, Sql, Python"
Data Engineer,Rakuten Symphony,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"About Us\n\nAll people need connectivity.\n\nThe Rakuten Group is reinventing telecom by greatly reducing cost, rewarding big users not penalizing them, empowering more people and leading the human centric AI future.\n\nThe mission is to connect everybody and enable all to be.\n\nRakuten. Telecom Invented.\n\nJob Description\n\nJob Title - Data Engineer\n\nLocation - Bangalore (Onsite)\n\nWhy should you choose us\n\nRakuten Symphony is a Rakuten Group company, that provides global B2B services for the mobile telco industry and enables next-generation, cloud-based, international mobile services. Building on the technology Rakuten used to launch Japan's newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Let's build the future of mobile telecommunications together!\n\nWhat will you do\n\nOur Data Platform team is building a world-class autonomous data service platform to cater services such as data lake as a service, database as a service, data transformation as a service and AI services.\n\nWe are looking for a Data Engineer to help us build functional systems for our data platform services.\n\nFor our autonomous data platform services as a Data Engineer you will be responsible for the end-to-end research and development of relevant features for the platform following the product lifecycle of the offerings.\n\nIf you have in-depth knowledge in Spark , NiFi and other distributed systems, we'd like to meet you.\n\nUltimately, you will develop various self-managed and scalable services of the data platform which will be offered as cloud services to the end users.\n\nRoles And Responsibilities\n\nWork experience as a Data Engineer or similar software engineering role (3-8 years)\nGood knowledge of NiFi, Spark and distributed eco systems, knowledge of Kubernetes application development, how to make K8 centric applications is a plus\nExpertise in development using Java/Scala Not python\nMust be able to quickly design and implement tolerant and highly available pipelines using distributed eco systems and should have hands-on experience with any NoSQL DB, Presto/Trino, NiFi, and Airflow Casandra\nSound knowledge of Spring frame work and spring frame work related solutions for web application development\nProblem-solving attitude, tinkering approach, and creative thinking\n\nOur Commitment To You\n\nRakuten Group's mission is to contribute to society by creating value through innovation and entrepreneurship. By providing high-quality services that help our users and partners grow,\nWe aim to advance and enrich society.\nTo fulfill our role as a Global Innovation Company, we are committed to maximizing both corporate and shareholder value.\n\nJob Requirement\n\nResponsibilities\n\nResearch and comparative analysis\nSystem architecture design\nImplement integrations\nDeploy updates and fixes\nPerform root cause analysis for production errors\nInvestigate and resolve technical issues\nArchitecture and support documentation\n\nRequirements\n\nWork experience as a Data Engineer or similar software engineering role (3-8 years)\nGood knowledge of NiFi, Spark and distributed eco systems, knowledge of Kubernetes application development, how to make K8 centric applications is a plus\nExpertise in development using Java/Scala Not python\nMust be able to quickly design and implement tolerant and highly available pipelines using distributed eco systems and should have hands-on experience with any NoSQL DB, Presto/Trino, NiFi, and Airflow Casandra\nSound knowledge of Spring frame work and spring frame work related solutions for web application development\nProblem-solving attitude, tinkering approach, and creative thinking","Airflow, NoSQL DB, Trino, NiFi, Java, Presto, Scala, Spark, Kubernetes"
Data Engineer,Ideas2IT Technologies,Fresher,,"Chennai, India",Login to check your skill match score,"Job Role\n\nAs a Data Engineer, you'll build and maintain data pipelines and architectures.Responsibilities include optimizing databases and ETL processes, using Python or SQL,and collaborating with data teams for informed decision-making.\n\nWhy Choose Ideas2IT\n\nIdeas2IT has all the good attributes of a product startup and a services company. Since we launch our products, you will have ample opportunities to learn and contribute. However, single-product companies stagnate in the technologies they use. In our multiple product initiatives and customer-facing projects, you will have the opportunity to work on various technologies.\n\nAGI is going to change the world. Big companies like Microsoft are betting heavily on this (see here and here). We are following suit. As a Data Engineer, exclusively focus on engineering data pipelines for complex products\n\nWhat's in it for you\n\nA robust distributed platform to manage a self-healing swarm of bots onunreliable network / compute\nLarge-scale Cloud-Native applications\nDocument Comprehension Engine leveraging RNN and other latest OCR techniques\nCompletely data-driven low-code platform\nYou will leverage cutting-edge technologies like Blockchain, IoT, and Data Science as you work on projects for leading Silicon Valley startups.\nYour role does not start or end with just Java development; you will enjoy the freedom to share your suggestions on the choice of tech stacks across the length of the project\nIf there is a certain technology you would like to explore, you can do your Technical PoCs\nWork in a culture that values capability over experience and continuous learning as a core tenet\n\nHere's what you'll bring\n\nProficiency in SQL and experience with database technologies (e.g., MySQL, PostgreSQL, SQL Server).Experience in any one of the cloud environments AWS, Azure\nExperience with data modeling, data warehousing, and building ETL pipelines.\nExperience building large-scale data pipelines and data-centric applications using any distributed storage platform\nExperience in data processing tools like Pandas, pyspark.\nExperience in cloud services like S3, Lambda, SQS, Redshift, Azure Data Factory, ADLS, Function Apps, etc.\nExpertise in one or more high-level languages (Python/Scala)\nAbility to handle large-scale structured and unstructured data from internal and third-party sources\nAbility to collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision-making across the organization\nExperience with data visualization tools like PowerBI, Tableau\nExperience in containerization technologies like Docker , Kubernetes","Function Apps, ADLS, S3, Scala, PostgreSQL, SQL Server, Pyspark, Tableau, Redshift, Sql, Lambda, Azure Data Factory, Pandas, Docker, Powerbi, MySQL, Sqs, Azure, Kubernetes, Python, AWS"
Data Engineer,DePronto Infotech,2-4 Years,,"Mumbai, India",Login to check your skill match score,"Company Description\nDePronto Infotech is a Software Development Company operating in India and UK, specializing in high-performance front-end technologies, API Development, Data Engineering services, and Customized application development focused on Analytics and Automation with respect to Risk & Compliance domain. With over 15+ years of experience in the BFSI domain, DePronto Infotech partners with reputed clients for governance, risk, and compliance solutions. The company emphasizes employee development to keep pace with new trends and technologies, aiming to offer economical business solutions through innovation. The firm's vision is to become the preferred provider of technology solutions for its clients.\nRole Description\nThis is a full-time remote role for a Data Engineer. The Data Engineer will be responsible for designing, developing, and managing robust data solutions with a solid foundation in SQL. Day-to-day tasks will include data modeling, Extract Transform Load (ETL) processes, data warehousing, and data analytics. The Data Engineer will work closely with stakeholders to understand data requirements and ensure data integrity and quality.\nKey Responsibilities:\nDesign, create, and maintain:\nSQL queries, scripts, views, stored procedures, functions, and triggers to support analytics and operational workflows.\nData models for OLTP/OLAP systems using best practices (e.g., normalization, indexing, partitioning).\nETL/ELT pipelines using SQL and orchestration tools (e.g., Airflow, dbt).\nOptimize SQL queries and database performance through indexing, partitioning, and query refactoring.\nEnsure data integrity, accuracy, and consistency across systems.\nCollaborate with data analysts, engineers, and business users to understand requirements and translate them into technical solutions.\nParticipate in schema evolution, version control of SQL code, and automated deployments (CI/CD).\nMonitor data pipelines and implement automated alerts and logging.\nMandatory Skills:\nAdvanced SQL (window functions, CTEs, joins, subqueries, dynamic SQL)\nStrong experience with:\nStored Procedures, Functions, Triggers\nViews and Materialized Views\nWriting and optimizing complex queries\nRelational databases: PostgreSQL, MySQL, or SQL Server\nData modeling for transactional and analytical systems\nETL/ELT processes and orchestration using tools like Airflow, dbt, or Apache NiFi\nVersion control with Git\nExperience working with large datasets and performance tuning\nPreferred Skills / Modern Tech Stack Exposure:\nCloud Platforms: AWS (Redshift, RDS, Athena, S3), GCP (BigQuery), Azure SQL\nData Lakes and Delta Lake technologies\nNoSQL exposure: MongoDB, DynamoDB (bonus)\nPython or Scala for scripting data workflows\nFamiliarity with event-driven data architectures (Kafka, Kinesis)\nExperience with DataOps, CI/CD pipelines, and Infrastructure as Code (e.g., Terraform)\nExposure to BI tools: Looker, Power BI, Tableau\nExperience with data catalogs, data quality tools, and observability frameworks\nQualifications:\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\n2+ years of experience in a Data Engineering or similar role with strong SQL focus.\nStrong problem-solving and communication skills.\nAbility to work independently and in a cross-functional team.\nNice to Have:\nCertification in cloud data engineering (e.g., AWS Data Analytics Specialty, Google Professional Data Engineer)\nKnowledge of data privacy and compliance frameworks (GDPR, HIPAA)","Airflow, Looker, dbt, PostgreSQL, Kafka, Data Modeling, Tableau, Data Analytics, ELT, Kinesis, Terraform, MySQL, Python, AWS, Power Bi, Scala, Dynamodb, SQL Server, Data Warehousing, Sql, Git, Gcp, MongoDB, Azure, Etl"
Data Engineer,Chiselon Technologies Pvt Ltd,Fresher,,"Chennai, India",Login to check your skill match score,"Qualifications and Skills\nProficiency in Python, SQL, and Snowflake (Mandatory skill) for developing robust data solutions.\nExpertise in working with big data frameworks and technologies for managing and processing large datasets.\nExperience with Microsoft Azure for cloud-based data storage and processing solutions.\nStrong understanding of data modeling principles to design efficient and scalable data architectures.\nHands-on experience with ETL and ELT processes to ensure seamless data integration and transformation.\nProficiency in PySpark for distributed data processing and analysis across large datasets.\nSolid problem-solving skills with the ability to troubleshoot complex data-related issues effectively.\nExcellent communication skills to collaborate with cross-functional teams and stakeholders efficiently.\nRoles and Responsibilities\nDesign, construct, and maintain scalable data pipelines to support various business needs and analytics.\nCollaborate with data scientists and analysts to understand data requirements and deliver suitable solutions.\nOptimize and monitor data workflows to ensure high performance and reliability of data systems.\nWork with engineering teams to integrate data solutions seamlessly with existing systems and applications.\nImplement best practices in data management, including data quality, integrity, and security measures.\nStay updated with industry trends and advancements to continually improve data engineering processes.\nDeliver data-driven insights and recommendations to drive strategic business decisions.\nTroubleshoot and resolve data-related issues promptly to maintain system integrity and performance.","Big Data frameworks, snowflake, Data modeling principles, Pyspark, Microsoft Azure, Python, Sql, ELT, Etl"
Data Engineer,Alight Solutions,5-7 Years,,"Gurugram, India",Login to check your skill match score,"Our story\n\nAt Alight, we believe a company's success starts with its people. At our core, we Champion People, help our colleagues Grow with Purpose and true to our name we encourage colleagues to Be Alight.\n\nOur Values:\n\nChampion People be empathetic and help create a place where everyone belongs.\n\nGrow with purpose Be inspired by our higher calling of improving lives.\n\nBe Alight act with integrity, be real and empower others.\n\nIt's why we're so driven to connect passion with purpose. Alight helps clients gain a benefits advantage while building a healthy and financially secure workforce by unifying the benefits ecosystem across health, wealth, wellbeing, absence management and navigation.\n\nWith a comprehensive total rewards package, continuing education and training, and tremendous potential with a growing global organization, Alight is the perfect place to put your passion to work.\n\nJoin our team if you Champion People, want to Grow with Purpose through acting with integrity and if you embody the meaning of Be Alight.\n\nLearn more at careers.alight.com .\n\nAlight is seeking a skilled and passionate Data Engineer to join our team. As the Data Engineer, you will be a member of a team responsible for various stages of data pipelines development, including understanding business requirements, coding, testing, documentation, deployment, and production support. As part of the Data Analytics team, you will focus on delivering high-quality enterprise caliber systems on Alight's Data Lake. Your primary role will involve participating in full life-cycle data ingestion and data architecting development projects.\n\nQualifications:\n\nKnowledge & Experience:\n5+ years of data integration, data warehousing or data conversion experience.\n3+ years of data modeling experience.\n3+ years of Informatica IICS experience.\n3+ years working with AWS Redshift.\n2+ years AWS step functions, Lambda, Glue, and other Cloud Data Lake Services\n1+ SSIS / SSRS experience\nExcellent analytical and critical thinking skills\nStrong interpersonal skills with the ability to work effectively with diverse and remote teams\nExperience in agile processes and development task estimation\nStrong sense of responsibility for deliverables\nAbility to work in a small team with moderate supervision\n\nResponsibility Areas:\n\nDesign data ingestion solutions for small to medium complexity requirements independently, adhering to existing standards\nDevelop high-priority and highly complex solutions for systems based on functional specifications, detailed design, maintainability, and coding and efficiency standards, working independently.\nEstimate and evaluate risks, and prioritize technical tasks based on requirements\nCollaborate actively with Data Engineering Lead, Product Owners, Quality Assurance, and stakeholders to ensure high-quality project delivery\nConduct formal code reviews to ensure compliance with standards\nUtilize appropriately system design, development, and process standards\nCreate, maintain, and publish system-level documentation, including system diagrams, with minimal guidance\nEnsure clarity, conciseness, and completeness of requirements before starting development, collaborating with Business Analysts and stakeholders to evaluate feasibility. Take primary accountability for meeting non-functional requirements.\n\nAlight requires all virtual interviews to be conducted on video.\n\nFlexible Working\n\nSo that you can be your best at work and home, we consider flexible working arrangements wherever possible. Alight has been a leader in the flexible workspace and Top 100 Company for Remote Jobs 5 years in a row.\n\nBenefits\n\nWe offer programs and plans for a healthy mind, body, wallet and life because it's important our benefits care for the whole person. Options include a variety of health coverage options, wellbeing and support programs, retirement, vacation and sick leave, maternity, paternity & adoption leave, continuing education and training as well as several voluntary benefit options.\n\nBy applying for a position with Alight, you understand that, should you be made an offer, it will be contingent on your undergoing and successfully completing a background check consistent with Alight's employment policies. Background checks may include some or all the following based on the nature of the position: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, credit check, and/or drug test. You will be notified during the hiring process which checks are required by the position.\n\nOur commitment to Inclusion\n\nWe celebrate differences and believe in fostering an environment where everyone feels valued, respected, and supported. We know that diverse teams are stronger, more innovative, and more successful.\n\nAt Alight, we welcome and embrace all individuals, regardless of their background, and are dedicated to creating a culture that enables every employee to thrive. Join us in building a brighter, more inclusive future.\n\nAuthorization to work in the Employing Country\n\nApplicants for employment in the country in which they are applying (Employing Country) must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the Employing Country and with Alight.\n\nNote, this job description does not restrict management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.\n\nWe offer you a competitive total rewards package, continuing education & training, and tremendous potential with a growing worldwide organization.\n\nDISCLAIMER:\n\nNothing in this job description restricts management's right to assign or reassign duties and responsibilities of this job to other entities; including but not limited to subsidiaries, partners, or purchasers of Alight business units.","Data Conversion, AWS step functions, Cloud Data Lake Services, Informatica IICS, Aws Redshift, Ssrs, Data Warehousing, Data Modeling, Data Integration, SSIS"
Data Engineer,Azoon Tech Consulting LLC,Fresher,,"Hyderabad, India",Login to check your skill match score,"Join a World-Class AI Startup at the Forefront of Agent Technology\nWe're a stealth-mode AI startup, founded by globally recognized executive leaders with a proven history of building game-changing technologies. We're now building a world-class engineering team to shape the future of AI agentsand this is your chance to get in at the ground floor.\nWe offer a rare opportunity to work on cutting-edge AI infrastructure, learn fast, and grow alongside some of the best minds in the industry. This isn't just a jobit's a chance to build something meaningful and be rewarded for your impact.\nWhat's in it for you:\nWork directly on the next generation of AI agent technology\nCollaborate in a high-impact, fast-paced environment\nFlexible location options with a hybrid/remote-friendly culture\nWe're looking for engineers with hands-on expertise in:\nAWS Cloud, Apache Spark, Airflow, Python, and FastAPI\nFramework and system design\nApache Spark performance tuning and optimization\nIf you're excited about building from scratch, solving deep technical problems, and making a real impactyou'll thrive here.\nQualifications\nBachelor's in Computer Science","Airflow, Apache Spark performance tuning and optimization, Framework and system design, Apache Spark, Aws Cloud, FastAPI, Python"
Data Engineer,Aditi Consulting,5-8 Years,,"Bengaluru, India",Login to check your skill match score,"We are hiring Data Engineer for Bangalore Location\nLocation: Bangalore\nExperience : 5 8 Yrs\nWorking Model : Hybrid\nEXPERIENCE:\n5 - 8 years preferred experience in a data engineering role.\nMinimum of 4 years of preferred experience in Azure data services (Data Factory, Databricks, ADLS, SQL DB, etc.)\nEDUCATION:\nMinimum Bachelor's Degree in Computer Science, Computer Engineering or in STEM Majors (Science, Technology, Engineering, and\nMath)\nSKILLS/REQUIREMENTS:\nStrong working knowledge of Databricks, ADF.\nExpertise working with databases and SQL.\nStrong working knowledge of code management and continuous integrations systems (Azure DevOps or Github)\npreferred\nFamiliarity with Agile delivery methodologies\nFamiliarity with NoSQL databases (such as MongoDB) preferred.\nAny experience on IoT Data Standards like Project Haystack, Brick Schema, Real Estate Core is an added advantage\nAbility to multi-task and reprioritize in a dynamic environment.\nOutstanding written and verbal communication skills","Sql, Databricks, Github, Azure DevOps"
Data Engineer,Klodev,6-10 Years,,"Coimbatore, India",Login to check your skill match score,"Design, develop, and maintain data pipelines and ETL processes using PySpark or Scala.\nWork extensively with Python to build scalable data solutions.\nShould be strong in SQL\nDevelop and manage data workflows in Azure Data Factory (ADF).\nImplement and optimize data solutions on cloud platforms such as Azure or AWS.\nEnsure data integrity, performance, and security across data pipelines.\nCollaborate with data scientists, analysts, and other stakeholders to support business needs.\nLead and mentor a team of junior data engineers (if applicable).\n\nRequirements\n\n6 to 10 years of experience in data engineering or related roles.\nStrong hands-on experience with PySpark or Scala.\nProficiency in Python for data processing and automation.\nExperience working with Azure Data Factory (ADF).\nHands-on experience with cloud platforms (Azure or AWS).\nStrong understanding of ETL processes, data warehousing, and big data technologies.\nExperience in leading a team is a plus.","ETL processes, Scala, Pyspark, Big Data Technologies, Data Warehousing, Azure, Python, Sql, AWS"
Data Engineer,TELUS Digital,4-6 Years,,"Noida, India",Login to check your skill match score,"Requirements\n\nDescription and Requirements\n\nExploring bleeding-edge technologies, tools and frameworks to experiment with and build better products for existing customers.\nEvaluating areas of improvement with technical products built and implementing ideas which will make us better than yesterday.\nCollaborating with developers to work on technical designs and develop code, configurations, and scripts to enhance the development lifecycle and integrate systems.\nCollaborate proactively and respectfully with our team and customers\nDevelop tools and integrations to support other developers in building products.\nTake solutions from concept to production by writing code, configurations, and scripts.\nImprove existing platforms or implement new features for any of our products.\nCreate comprehensive documentation for implemented solutions, including implementation details and usage instructions.\nPromote our culture of focus, flow, and joy to gain developers support for our solution\n\nQualifications\n\n\nBuild Data pipelines required for optimal extraction, anonymization, and transformation of data from a wide variety of data sources using SQL, NoSQL and AWS big data technologies. Streaming Batch\nWork with stakeholders including the Product Owners, Developers and Data scientists to assist with data-related technical issues and support their data infrastructure needs.\nEnsure that data is secure and separated following corporate compliance and data governance policies.\nTake ownership of existing ETL scripts, maintain and rewrite them in modern data transformation tools whenever needed.\nBeing an automation advocate for data transformation, cleaning and reporting tools.\nYou are proficient in developing software from idea to production\nYou can write automated test suites for your preferred language\nYou have frontend development experience with frameworks such as React.js/Angular\nYou have backend development experience building and integrating with REST APIs and Databases using languages such as Java Spring, JavaScript on Node.js, Flask on Python.\nYou have experience with cloud-native technologies, such as Cloud Composer, Dataflow, Dataproc, BigQuery, GKE, Cloud run, Docker, Kubernetes, and Terraform.\nYou have used cloud platforms such as Google Cloud/AWS for application hosting.\nYou have used and understand CI/CD best practices with tools such as GitHub Actions, GCP Cloud Build.\nYou have experience with YAML and JSON for configuration.\nYou are up-to-date on the latest trends in AI Technology\n\nAdditional Job Description\n\n\nGreat-to-haves\n\n4+ years of experience as a data or software engineer\n4+ years of experience in SQL and Python\n2+ years of experience with ELT/ETL platforms (Airflow, DBT, Apache Beam, PySpark, Airbyte)\n2+ years of experience with BI reporting tools (Looker, Metabase, Quicksight, PowerBI, Tableau)\nExtensive knowledge of the Google Cloud Platform, specifically the Google Kubernetes Engine\nExperience with GCP cloud data related services ( Dataflow, GCS, Datastream, Data Fusion, Data Application, BigQuery, Data Flow, Data Proc, Dataplex, PubSub, CloudSQL, BigTable)\nExperience in health industry an asset\nExpertise in Python, Java\nInterest in PaLM, LLM usage and LLMOps\nFamiliarity with LangFuse or Backstage plugins or GitHub Actions\nStrong experience with GitHub beyond source control\nFamiliarity with monitoring, alerts, and logging solutions\n\nJoin us on this exciting journey to make Generative AI accessible to all and create a positive impact with technology.\n\nEEO Statement\n\nAt TELUS Digital, we enable customer experience innovation through spirited teamwork, agile thinking, and a caring culture that puts customers first. TELUS Digital is the global arm of TELUS Corporation, one of the largest telecommunications service providers in Canada. We deliver contact center and business process outsourcing (BPO) solutions to some of the world's largest corporations in the consumer electronics, finance, telecommunications and utilities sectors. With global call center delivery capabilities, our multi-shore, multi-language programs offer safe, secure infrastructure, value-based pricing, skills-based resources and exceptional customer service - all backed by TELUS, our multi-billion dollar telecommunications parent.\n\nEqual Opportunity Employer\n\nAt TELUS Digital, we are proud to be an equal opportunity employer and are committed to creating a diverse and inclusive workplace. All aspects of employment, including the decision to hire and promote, are based on applicants qualifications, merits, competence and performance without regard to any characteristic related to diversity.","Airflow, GCP Cloud Build, Cloud Run, GitHub Actions, Cloud Composer, dbt, GKE, Airbyte, Looker, Metabase, Yaml, Pyspark, Tableau, Json, Angular, Nosql, Javascript, Terraform, Docker, Flask, react.js, Python, AWS, Java, BigQuery, Node.js, Dataproc, Sql, Spring, Quicksight, Powerbi, Apache Beam, DataFlow, Kubernetes"
Data Engineer I,FedEx ACC,2-5 Years,,"Hyderabad, India",Login to check your skill match score,"Responsible for designing and implementing data pipelines, ensuring data quality and accessibility, creating data models, managing ETL processes, optimizing databases, collaborating with stakeholders, utilizing big data tools, implementing real-time data processing, ensuring data governance and security, documenting data architecture, and delivering data-driven insights.\n\nDesign and implement data pipelines to collect, process, and store data efficiently.\nEnsure data quality, integrity, and accessibility.\nCreate data models to support business analytics and reporting.\nWork with data scientists, analysts, and other stakeholders to understand data needs, and design schemas.\nBuild and manage ETL (Extract, Transform, Load) processes for data integration and automate data ingestion and transformation tasks.\nAdminister and optimize databases (both SQL and NoSQL). Monitor database performance and troubleshoot issues.\nCollaborate with software engineers to integrate data systems with applications.\nUtilize big data tools for processing large datasets, such as Hadoop or Spark.\nImplement distributed systems for real-time data processing.\nEnsure data governance and security through compliance with data governance policies and data security standards.\nImplement data access controls and encryption techniques.\nDocument data architecture, pipelines, and processes.\nProvide reports and insights based on data analysis.\n\nEducation: Bachelors degree or equivalent in Computer Science, MIS, or similar discipline.\n\nAccreditation: Specific business accreditation for Business Intelligence.\n\nExperience: Relevant work experience in data engineering based on the following number of years:\n\nAssociate: Prior experience not required\n\nStandard I: Two (2) years\n\nStandard II: Three (3) years\n\nSenior I: Four (4) years\n\nSenior II: Five (5) years\n\nKnowledge, Skills And Abilities\n\nFluency in English\nAnalytical Skills\nAccuracy & Attention to Detail\nNumerical Skills\nPlanning & Organizing Skills\nPresentation Skills\n\nPreferred Qualifications\n\nPay Transparency:\n\nPay\n\nAdditional Details:\n\nFedEx was built on a philosophy that puts people first, one we take seriously. We are an equal opportunity/affirmative action employer and we are committed to a diverse, equitable, and inclusive workforce in which we enforce fair treatment, and provide growth opportunities for everyone.\n\nAll qualified applicants will receive consideration for employment regardless of age, race, color, national origin, genetics, religion, gender, marital status, pregnancy (including childbirth or a related medical condition), physical or mental disability, or any other characteristic protected by applicable laws, regulations, and ordinances.\n\nOur Company\n\nFedEx is one of the world's largest express transportation companies and has consistently been selected as one of the top 10 World's Most Admired Companies by Fortune magazine. Every day FedEx delivers for its customers with transportation and business solutions, serving more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx team members, who are tasked with making every FedEx experience outstanding.\n\nOur Philosophy\n\nThe People-Service-Profit philosophy (P-S-P) describes the principles that govern every FedEx decision, policy, or activity. FedEx takes care of our people; they, in turn, deliver the impeccable service demanded by our customers, who reward us with the profitability necessary to secure our future. The essential element in making the People-Service-Profit philosophy such a positive force for the company is where we close the circle, and return these profits back into the business, and invest back in our people. Our success in the industry is attributed to our people. Through our P-S-P philosophy, we have a work environment that encourages team members to be innovative in delivering the highest possible quality of service to our customers. We care for their well-being, and value their contributions to the company.\n\nOur Culture\n\nOur culture is important for many reasons, and we intentionally bring it to life through our behaviors, actions, and activities in every part of the world. The FedEx culture and values have been a cornerstone of our success and growth since we began in the early 1970's. While other companies can copy our systems, infrastructure, and processes, our culture makes us unique and is often a differentiating factor as we compete and grow in today's global marketplace.","Real-time data processing, Data pipelines, data models, Nosql, Data Security, Data Quality, Data Governance, Sql"
Data Engineer - I,slice,1-3 Years,,"Bengaluru, India",Login to check your skill match score,"About Us\nslice, India's leading consumer payments and credit platform, has now merged with North East Small Finance Bank (NESFB), marking a significant step into the banking sector. Trusted by over 18 million Indians, we are on a mission to build the most loved bank in Indiaoffering simple, transparent, and customer-first banking.\nAbout the role:\nAs a Data Engineer - I, you will play a pivotal role in our mission to drive innovation and excellence in our software solutions. You will be part of a talented team of engineers and work on challenging projects that will expand your horizons and elevate your skills to new heights. Reporting directly to the Engineering Manager, you will be entrusted with crucial responsibilities and given the autonomy to shape the future of our products.\nNote: We are inviting Backend Developers working in Data teams of any scalable product companies with similar skillset to apply. This is ahardcore coding role that requires strong technical skills and deep problem-solving abilities.\nWhat You'll Work On:\nTrino query governance and abuse prevention\nAI-based anomaly detection using Apache Flink\nDesigning and maintaining self-healing, SLA-driven pipelines\nBuilding an LLMOps platform using Ray and MLflow\nCreating zero-SQL tooling for pipeline automation\nOptimizing performance and cost across Spark jobs and EKS clusters\nBuilding a central governance layer and organizational data maps\nYou Should Have:\n1-3 years of strong programming experience in Python (PySpark) and/or Golang\nHands-on experience with Kubernetes (EKS) and REST API development using Django or FastAPI\nProficiency in distributed systems and data processing frameworks like Spark and Flink\nExperience with Trino, Redpanda, Ray, and MLflow\nWorking knowledge of Delta Lake on S3 and AWS big data services like EMR and Glue\nFamiliarity with Superset, Apache Ranger, and low-latency data stores\nStrong problem-solving skills and a deep understanding of scalable cloud-native infrastructure\nLife at slice:\nLife so good, you'd think we're kidding\nCompetitive salaries. Period.\nAn extensive medical insurance that looks out for our employees & their dependants. We'll love you and take care of you, our promise.\nFlexible working hours. Just don't call us at 3AM, we like our sleep schedule.\nTailored vacation & leave policies so that you enjoy every important moment in your life.\nA reward system that celebrates hard work and milestones throughout the year. Expect a gift coming your way anytime you kill it here.\nLearning and upskilling opportunities. Seriously, not kidding.\nGood food, games, and a cool office to make you feel like home.\nAn environment so good, you'll forget the term colleagues can't be your friends.","Apache Ranger, MLflow, Ray, Flink, EKS, AWS big data services, Superset, Glue, Trino, Redpanda, Delta Lake, Golang, Pyspark, Emr, Django, Rest API Development, Spark, FastAPI, Kubernetes, Python"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.\n\nWe are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.\n\nEngineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.\n\nKey Responsibilities\n\nDesign, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.\nDevelop data transformation using DBT (Data Build Tool) with SQL or Python.\nDesign and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.\nDevelop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.\nEnsure data quality and integrity through automated testing and validation using tools like Great Expectations.\nImplement all observability requirements in the data pipeline.\nOptimize data workflows for performance and scalability.\nPerforms code and design reviews for tasks done by other team members.\nWorks with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.\nWorks on the development of technical standards for the product and platform.\nCollaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.\nProvide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.\nDocument data engineering processes and best practices whenever required.\n\nRequired Skills And Qualifications\n\nMust have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.\nExposure on leading complex software projects.\nStrong programming skills in Python.\nExperience with Apache Airflow or any other orchestration framework for data orchestration.\nProficiency in DBT for data transformation and modeling.\nExperience with data quality validation tools like Great Expectations or any other similar tools.\nStrong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.\nExperience with cloud-based data warehouse platform like Snowflake.\nExperience working on NoSQL databases like Elasticsearch and MongoDB.\nExperience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.\nExperience on Cloud platforms like AWS and/or Azure.\nExperience working with backend microservices and APIs using Java or C#.\nExposure on message-oriented middleware technologies like Kafka is a plus.\nAbility to work collaboratively in a team environment.\nNeed to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.\nExperience with Financial Services application is a plus.\nEffective communication skills, both written and verbal.\nBachelor's or master's degree in computer science, Engineering, or a related field.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
Lead Azure Data Engineer,Aspire Systems,7-9 Years,,"Chennai, India",Login to check your skill match score,"Job Title: Lead Azure Data Engineer\nExperience: 7+ Years\nLocation: Chennai / Bangalore /Hyderabad /Kochi\nAbout Aspire Systems\nAspire Systems is a global technology services firm offering end-to-end digital transformation and product engineering solutions. We are currently seeking an experienced Lead Azure Data Engineer to join our Data & Analytics team and help build scalable, secure, and intelligent data platforms.\nRole Overview\nAs an Azure Data Engineer, you will be responsible for creating, managing, and optimizing data lakes and pipelines using Azure technologies. You will work with internal teams to build high-performance data platforms that support enterprise-grade analytics and reporting.\nKey Responsibilities\nDesign and build data lakes and lakehouses from scratch, and configure existing systems.\nDevelop and manage Azure Data Factory (ADF) and Synapse pipelines for data ingestion and transformation.\nWork on data security implementation across storage and data movement layers.\nUse Python, PySpark, and Spark SQL for data transformation and orchestration.\nCreate and maintain CI/CD pipelines for data workflows.\nUnderstand and work with various file formats: JSON, Parquet, CSV, Excel, and both structured and unstructured datasets.\nCollaborate with internal teams to support application integration and performance optimization.\nProvide user support and documentation where required.\nKey Skills Required\nExpertise in Azure Data Lake, Lakehouse Architecture, Synapse Analytics, Databricks, and T-SQL.\nStrong experience in ETL/ELT pipelines, working with large volumes of data.\nProficient with SQL Server, Synapse DB, and data warehousing concepts.\nUnderstanding of metadata management, data catalogs, DWH, MPP, OLTP, and OLAP systems.\nFamiliar with Source Control tools like Git, TFS, or SVN.\nKnowledge of Azure Data Fabric, Microsoft Purview, and MDM tools is a plus.\nExperience working with non-functional requirements and performance tuning.\nStrong communication skills and the ability to work effectively in a collaborative team environment.\nWhy Aspire\nWork on cutting-edge Azure Data & AI solutions.\nCollaborate with a global team on enterprise-scale transformation projects.\nGrowth opportunities into Data Architect or Tech Lead roles.\nContinuous learning and certification support.\nInterested candidates can share their resume with [HIDDEN TEXT]","MDM tools, OLAP systems, Synapse DB, Microsoft Purview, Azure Data Fabric, ETL ELT pipelines, Synapse Analytics, Azure Data Lake Lakehouse Architecture, data catalogs, Spark SQL, T-sql, Metadata Management, Pyspark, SQL Server, Mpp, Dwh, Databricks, Data Warehousing Concepts, Python, Oltp"
IN_Senior Associate_Azure Data Engineer_Data & Analytics_Advisory_PAN India,PwC India,8-13 Years,,"Ahmedabad, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nAnalyses current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics Services.\nProvide technical leadership and thought leadership as a senior member of the Analytics Practice in areas such as data access & ingestion, data processing, data integration, data modeling, database design & implementation, data visualization, and advanced analytics.\nEngage and collaborate with customers to understand business requirements/use cases and translate them into detailed technical specifications.\nDevelop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL.\nMaintain best practice standards for the development or cloud-based data warehouse solutioning including naming standards.\nDesigning and implementing highly performant data pipelines from multiple sources using Apache Spark and/or Azure Databricks\nIntegrating the end-to-end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is always maintained\nWorking with other members of the project team to support delivery of additional project components (API interfaces)\nEvaluating the performance and applicability of multiple tools against customer requirements\nWorking within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.\nIntegrate Databricks with other technologies (Ingestion tools, Visualization tools).\nProven experience working as a data engineer\nHighly proficient in using the spark framework (python and/or Scala)\nExtensive knowledge of Data Warehousing concepts, strategies, methodologies.\nDirect experience of building data pipelines using Azure Data Factory and Apache Spark (preferably in Databricks).\nHands on experience designing and delivering solutions using Azure including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics\nExperience in designing and hands-on development in cloud-based analytics solutions.\nExpert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required.\nDesigning and building of data pipelines using API ingestion and Streaming ingestion methods.\nKnowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential.\nThorough understanding of Azure Cloud Infrastructure offerings.\nStrong experience in common data warehouse modeling principles including Kimball.\nWorking knowledge of Python is desirable\nExperience developing security models.\nDatabricks & Azure Big Data Architecture Certification would be plus\n\nMandatory Skill Sets\n\n\nADE, ADB, ADF\n\nPreferred Skill Sets\n\nADE, ADB, ADF\n\nYears Of Experience Required\n\n8-13 Years\n\nEducation Qualification\n\nBE, B.Tech, MCA, M.Tech\n\na\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nAndroid Debug Bridge (ADB), Microsoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure SQL Data Warehouse, Dev-Ops processes, Microsoft Azure Data Analytics Services, Infrastructure as code, Azure Stream Analytics, API ingestion, Azure App Service, Streaming ingestion, Scala, Apache Spark, Azure Databricks, Azure Synapse, Azure Cosmos DB, Azure Data Factory, Data Pipeline, Azure Data Lake, Data Warehousing, Python"
"IN_Senior Associate_Data Engineer(PySpark,Python)_Data & Analytics_Advisory_Bangalore",PwC India,4-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nA career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nCreating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities:\n\nRoles & Responsibilities\n\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.\nDesign end-to-end job flows that span across systems, including quality checks and controls.\nCreate technology delivery plans to implement system changes.\nPerform data analysis, data profiling, and data sourcing in relational and Big Data environments.\nConvert functional requirements into logical and physical data models.\nAssist in ETL development, testing, and troubleshooting ETL issues.\nTroubleshoot data issues and work with data providers for resolution; provide L3 support when needed.\nDesign and develop ETL workflows using modern coding and testing standards.\nParticipate in agile ceremonies and actively drive towards team goals.\nCollaborate with a global team of technologists.\nLead with ideas and innovation.\nManage communication and partner with end users to design solutions.\n\nRequired Skills:\n\n\nMust have: Total experience required 4-10 years (relevant experience minimum 5 years)\n\n5 years of project experience in Python/Shell scripting in Data Engineering (experience in building and optimizing data pipelines, architectures, and data sets with large data volumes).\n3+ years of experience in PySpark scripting, including the architecture framework of Spark.\n3-5 years of strong experience in database development (Snowflake/ SQL Server/Oracle/Sybase/DB2) in designing schema, complex procedures, complex data scripts, query authoring (SQL), and performance optimization.\nStrong understanding of Unix environment and batch scripting languages (Shell/Python).\nStrong knowledge of Big Data/Hadoop platform.\nStrong engineering skills with the ability to understand existing system designs and enhance or migrate them.\nStrong logical data modeling skills within the Financial Services domain.\nExperience in data integration and data conversions.\nStrong collaboration and communication skills.\nStrong organizational and planning skills.\nStrong analytical, profiling, and troubleshooting skills.\n\nGood To Have:\n\n\nExperience with ETL tools (e.g Informatica, Azure Data Factory) and pipelines across disparate sources is a plus.\nExperience working with Databricks is a plus.\nFamiliarity with standard Agile & DevOps methodology & tools (Jenkins, Sonar, Jira).\nGood understanding of developing ETL processes using Informatica or other ETL tools.\nExperience working with Source Code Management solutions (e.g., Git).\nKnowledge of Investment Management Business.\nExperience with job scheduling tools (e.g., Autosys).\nExperience with data visualization software (e.g., Tableau).\nExperience with data modeling tools (e.g., Power Designer).\nBasic familiarity with using metadata stores to maintain a repository of Critical Data Elements. (e.g. Collibra)\nFamiliarity with XML or other markup languages.\n\nMandatory Skill Sets:\n\n\nETL,Python/Shell scripting , building pipelines,pyspark, database, sql\n\nPreferred Skill Sets:\n\ninformatica, hadoop, databricks, collibra\n\nYears Of Experience Required:\n\n4 to 10 years\n\nEducation Qualification:\n\nGraduate Engineer or Management Graduate\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration\n\nDegrees/Field Of Study Preferred:\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nExtract Transform Load (ETL), Python (Programming Language), Structured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment, Performance Management Software + 16 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","power designer, snowflake, Sybase, Sonar, Hadoop, Collibra, Pyspark, SQL Server, Tableau, Jira, Informatica, Sql, Jenkins, Git, Azure Data Factory, DB2, Shell scripting, Databricks, Oracle, Python, Etl"
IN_Manager_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nManager\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:\n\nDesign, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.\nImplement data ingestion and transformation processes to facilitate efficient data warehousing.\nUtilize cloud services to enhance data processing capabilities:\nAWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.\nAzure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.\nGCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.\nOptimize Spark job performance to ensure high efficiency and reliability.\nStay proactive in learning and implementing new technologies to improve data processing frameworks.\nCollaborate with cross-functional teams to deliver robust data solutions.\nWork on Spark Streaming for real-time data processing as necessary.\n\nQualifications\n\n\n7-10 years of experience in data engineering with a strong focus on cloud environments.\nProficiency in PySpark or Spark is mandatory.\nProven experience with data ingestion, transformation, and data warehousing.\nIn-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP):\nDemonstrated ability in performance optimization of Spark jobs.\nStrong problem-solving skills and the ability to work independently as well as in a team.\nCloud Certification (AWS, Azure, or GCP) is a plus.\nFamiliarity with Spark Streaming is a bonus\n\nPreferred Skills:\n\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nMandatory Skill Sets:\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nYears Of Experience Required:\n\n7-10 years\n\nEducation Qualification:\n\nBE/BTECH, ME/MTECH, MBA, MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering, Master of Engineering, Bachelor of Technology\n\nDegrees/Field Of Study Preferred:\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft SQL Server, PySpark, Python (Programming Language)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation + 24 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","GCP Dataflow, Pyspark, AWS Glue, Sql, Azure Data Factory, Spark Streaming, Gcp, Spark, Databricks, Azure, Python, AWS"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline + 38 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql, Etl"
Associate - Data Engineer,BlackRock,5-8 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.\n\nWe are seeking a highly skilled and motivated Senior level Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.\n\nEngineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.\n\nKey Responsibilities\n\nDesign, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.\nDevelop data transformation using DBT (Data Build Tool) with SQL or Python.\nDevelop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.\nEnsure data quality and integrity through automated testing and validation using tools like Great Expectations.\nImplement all observability requirements in the data pipeline.\nOptimize data workflows for performance and scalability.\nMonitor and troubleshoot data pipeline issues, ensuring timely resolution.\nDocument data engineering processes and best practices whenever required.\nDevelop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.\n\nRequired Skills And Qualifications\n\nMust have 5 to 8 years of experience in data engineering, with a focus on building data pipelines.\nStrong programming skills in Python.\nExperience with Apache Airflow or any other orchestration framework for data orchestration.\nProficiency in DBT for data transformation and modeling.\nExperience with data quality validation tools like Great Expectations or any other similar tools.\nStrong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.\nExperience with cloud-based data warehouse platform like Snowflake.\nExperience working on NoSQL databases like Elasticsearch and MongoDB.\nExperience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.\nExperience on Cloud platforms like AWS and/or Azure.\nExperience working with backend microservices and APIs using Java or C#.\nAbility to work collaboratively in a team environment.\nNeed to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.\nExperience with Financial Services application is a plus.\nEffective communication skills, both written and verbal.\nBachelor's or master's degree in computer science, Engineering, or a related field.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Python, Kubernetes, AWS"
IN_Senior Associate_ Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nDesign, develop, and maintain scalable data pipelines using Azure data services such as Azure Data Factory, Azure Databricks, and Apache Spark.\nImplement efficient Extract, Transform, Load (ETL) processes to move and transform data across various sources.\nKnowledge about data warehousing concepts\nExtensive working experience in Azure Databricks\nUtilize Azure SQL Database, Azure Blob Storage, Azure Data Lake Storage, and other Azure data services to store and retrieve data.\nPerformance optimization and troubleshooting capabilities\nExperience in working in pharma and/or healthcare clients will be a plus Technology: SQL, ADF, Azure Databricks, ADLS, Synapse, PySpark Desired Candidate Profile:\nB.E./B.Tech. (preferably in Computer Science) or MCA or Statistics/Applied Mathematics\nExperience in designing solutions with IaaS, PaaS, and SaaS.\nProficient in data modeling and ETL pipeline implementation.\nStrong knowledge of Azure services.\nProven track record in business development and client relationship management.\nExcellent communication and interpersonal skills.\n\nMandatory Skill Sets\n\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Technology, Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nGCP Dataflow, Good Clinical Practice (GCP)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Blob Storage, Azure SQL Database, Azure Data Lake Storage, Paas, Saas, Apache Spark, Iaas, Data Modeling, Azure Databricks, Sql, Azure Data Factory, Etl"
IN_Senior Associate_Azure Data Engineer_Data & Analytics_Advisory_PAN India,PwC India,8-13 Years,,"Ahmedabad, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nAnalyses current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics Services.\nProvide technical leadership and thought leadership as a senior member of the Analytics Practice in areas such as data access & ingestion, data processing, data integration, data modeling, database design & implementation, data visualization, and advanced analytics.\nEngage and collaborate with customers to understand business requirements/use cases and translate them into detailed technical specifications.\nDevelop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL.\nMaintain best practice standards for the development or cloud-based data warehouse solutioning including naming standards.\nDesigning and implementing highly performant data pipelines from multiple sources using Apache Spark and/or Azure Databricks\nIntegrating the end-to-end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is always maintained\nWorking with other members of the project team to support delivery of additional project components (API interfaces)\nEvaluating the performance and applicability of multiple tools against customer requirements\nWorking within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.\nIntegrate Databricks with other technologies (Ingestion tools, Visualization tools).\nProven experience working as a data engineer\nHighly proficient in using the spark framework (python and/or Scala)\nExtensive knowledge of Data Warehousing concepts, strategies, methodologies.\nDirect experience of building data pipelines using Azure Data Factory and Apache Spark (preferably in Databricks).\nHands on experience designing and delivering solutions using Azure including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics\nExperience in designing and hands-on development in cloud-based analytics solutions.\nExpert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required.\nDesigning and building of data pipelines using API ingestion and Streaming ingestion methods.\nKnowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential.\nThorough understanding of Azure Cloud Infrastructure offerings.\nStrong experience in common data warehouse modeling principles including Kimball.\nWorking knowledge of Python is desirable\nExperience developing security models.\nDatabricks & Azure Big Data Architecture Certification would be plus\n\nMandatory Skill Sets\n\n\nADE, ADB, ADF\n\nPreferred Skill Sets\n\nADE, ADB, ADF\n\nYears Of Experience Required\n\n8-13 Years\n\nEducation Qualification\n\nBE, B.Tech, MCA, M.Tech\n\na\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nAndroid Debug Bridge (ADB), Microsoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure SQL Data Warehouse, Dev-Ops processes, Microsoft Azure Data Analytics Services, Infrastructure as code, Azure Stream Analytics, API ingestion, Azure App Service, Streaming ingestion, Scala, Apache Spark, Azure Databricks, Azure Synapse, Azure Cosmos DB, Azure Data Factory, Data Pipeline, Azure Data Lake, Data Warehousing, Python"
Azure data engineer,HCLTech,7-9 Years,,"Chennai, India",Login to check your skill match score,"We are seeking an experienced Azure Data Engineer to design, implement, and manage data solutions on the Microsoft Azure platform. The ideal candidate will have extensive experience with Azure services, including Azure Functions, Azure Synapse, Azure Data Factory.\n\nLocations - Chennai\n\nExperience - 7 to 9 Yea\nrs\nRequired Skills and Qualificatio\nns:\nStrong expertise in Azure services, including Azure Functions, Azure Synapse, Azure Data Factory, and API Managem\nent.Proficiency in designing and implementing APIs for data integration and acc\ness.Hands-on experience with ETL processes, data modeling, and data warehous\ning.Knowledge of programming languages such as Python, SQL, or\nC#.Familiarity with big data technologies like Azure Databricks or Apache Spark is a p\nlus.Excellent problem-solving and analytical ski\nlls.Strong communication and collaboration abilit\nies.\nIf interested, please share profile at shraddha_rathor@hcltech.com with below deta\nils -\nTotal Expe\nrienceRelevant Experience as Azure Data En\ngineerNotice\nPer\niodC\nTCECTCCurrent Lo\ncationPreferred Lo\ncation","ETL processes, Azure Synapse, Azure Data Factory, Azure Functions, Apache Spark, Azure Databricks, Data Modeling, Sql, Python"
IN-Senior Associate_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary: A career within PWC\n\n\n\nResponsibilities:\n\n\n\nJob Title: Cloud Data Engineer (AWS/Azure/Databricks/GCP)\n\nExperience:3-8 years in Data Engineering\n\nJob Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:\n\n- Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.\n\nImplement data ingestion and transformation processes to facilitate efficient data warehousing.\nUtilize cloud services to enhance data processing capabilities:\n\n-AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.\n\n-Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.\n\n-GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.\n\nOptimize Spark job performance to ensure high efficiency and reliability.\nStay proactive in learning and implementing new technologies to improve data processing frameworks.\nCollaborate with cross-functional teams to deliver robust data solutions.\nWork on Spark Streaming for real-time data processing as necessary.\n\nQualifications:\n\n3-8 years of experience in data engineering with a strong focus on cloud environments.\nProficiency in PySpark or Spark is mandatory.\nProven experience with data ingestion, transformation, and data warehousing.\nIn-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP):\nDemonstrated ability in performance optimization of Spark jobs.\nStrong problem-solving skills and the ability to work independently as well as in a team.\nCloud Certification (AWS, Azure, or GCP) is a plus.\nFamiliarity with Spark Streaming is a bonus.\n\nMandatory skill sets:\n\n\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nPreferred skill sets:\n\n\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nYears of experience required:\n\n\n\n3-8 years\n\nEducation qualification:\n\n\n\nBE/BTECH, ME/MTECH, MBA, MCA\n\n\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Technology, Master of Engineering, Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nNode.js\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation + 19 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Data Ingestion, Pyspark, Sql, Spark Streaming, Data Pipeline, Gcp, Spark, Databricks, Data Warehousing, Azure, Data Transformation, AWS"
"IN_Senior Associate_Data Engineer(PySpark,Python)_Data & Analytics_Advisory_Bangalore",PwC India,4-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nA career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nCreating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities:\n\nRoles & Responsibilities\n\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.\nDesign end-to-end job flows that span across systems, including quality checks and controls.\nCreate technology delivery plans to implement system changes.\nPerform data analysis, data profiling, and data sourcing in relational and Big Data environments.\nConvert functional requirements into logical and physical data models.\nAssist in ETL development, testing, and troubleshooting ETL issues.\nTroubleshoot data issues and work with data providers for resolution; provide L3 support when needed.\nDesign and develop ETL workflows using modern coding and testing standards.\nParticipate in agile ceremonies and actively drive towards team goals.\nCollaborate with a global team of technologists.\nLead with ideas and innovation.\nManage communication and partner with end users to design solutions.\n\nRequired Skills:\n\n\nMust have: Total experience required 4-10 years (relevant experience minimum 5 years)\n\n5 years of project experience in Python/Shell scripting in Data Engineering (experience in building and optimizing data pipelines, architectures, and data sets with large data volumes).\n3+ years of experience in PySpark scripting, including the architecture framework of Spark.\n3-5 years of strong experience in database development (Snowflake/ SQL Server/Oracle/Sybase/DB2) in designing schema, complex procedures, complex data scripts, query authoring (SQL), and performance optimization.\nStrong understanding of Unix environment and batch scripting languages (Shell/Python).\nStrong knowledge of Big Data/Hadoop platform.\nStrong engineering skills with the ability to understand existing system designs and enhance or migrate them.\nStrong logical data modeling skills within the Financial Services domain.\nExperience in data integration and data conversions.\nStrong collaboration and communication skills.\nStrong organizational and planning skills.\nStrong analytical, profiling, and troubleshooting skills.\n\nGood To Have:\n\n\nExperience with ETL tools (e.g Informatica, Azure Data Factory) and pipelines across disparate sources is a plus.\nExperience working with Databricks is a plus.\nFamiliarity with standard Agile & DevOps methodology & tools (Jenkins, Sonar, Jira).\nGood understanding of developing ETL processes using Informatica or other ETL tools.\nExperience working with Source Code Management solutions (e.g., Git).\nKnowledge of Investment Management Business.\nExperience with job scheduling tools (e.g., Autosys).\nExperience with data visualization software (e.g., Tableau).\nExperience with data modeling tools (e.g., Power Designer).\nBasic familiarity with using metadata stores to maintain a repository of Critical Data Elements. (e.g. Collibra)\nFamiliarity with XML or other markup languages.\n\nMandatory Skill Sets:\n\n\nETL,Python/Shell scripting , building pipelines,pyspark, database, sql\n\nPreferred Skill Sets:\n\ninformatica, hadoop, databricks, collibra\n\nYears Of Experience Required:\n\n4 to 10 years\n\nEducation Qualification:\n\nGraduate Engineer or Management Graduate\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration\n\nDegrees/Field Of Study Preferred:\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nExtract Transform Load (ETL), Python (Programming Language), Structured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment, Performance Management Software + 16 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","power designer, snowflake, Sybase, Sonar, Hadoop, Collibra, Pyspark, SQL Server, Tableau, Jira, Informatica, Sql, Jenkins, Git, Azure Data Factory, DB2, Shell scripting, Databricks, Oracle, Python, Etl"
IN-Manager_Azure Data Engineer_Data Analytics_Advisory_Bangalore,PwC India,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nOperations\n\nManagement Level\n\nManager\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary: A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nResponsibilities\n\nMust have:\n\nCandidates with a minimum of 5 years of relevant experience for 10-12 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills.\nProven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\n\nGood To Have\n\n\nGood to have certifications: Apache Spark 3.0\nExperience in any one visualization tool like Power BI, tableau etc.\nUnderstanding of Spark Architecture landscape\n\nMandatory Skill Sets\n\n\nSpark, Pyspark, Azure\n\nPreferred Skill Sets\n\nSpark, Pyspark, Azure\n\nYears Of Experience Required\n\n8-12yrs\n\nEducation Qualification\n\nB.Tech / M.Tech / MBA / MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Master of Engineering, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation + 23 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date","Pyspark scripting, Data Bricks, Synapse, Stored Procedures, Azure Analysis Services, Data Factory, Power Bi, Tableau, Python Scripting, Github, Advanced Sql, Azure DevOps"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.\n\nWe are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.\n\nEngineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.\n\nKey Responsibilities\n\nDesign, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.\nDevelop data transformation using DBT (Data Build Tool) with SQL or Python.\nDesign and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.\nDevelop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.\nEnsure data quality and integrity through automated testing and validation using tools like Great Expectations.\nImplement all observability requirements in the data pipeline.\nOptimize data workflows for performance and scalability.\nPerforms code and design reviews for tasks done by other team members.\nWorks with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.\nWorks on the development of technical standards for the product and platform.\nCollaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.\nProvide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.\nDocument data engineering processes and best practices whenever required.\n\nRequired Skills And Qualifications\n\nMust have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.\nExposure on leading complex software projects.\nStrong programming skills in Python.\nExperience with Apache Airflow or any other orchestration framework for data orchestration.\nProficiency in DBT for data transformation and modeling.\nExperience with data quality validation tools like Great Expectations or any other similar tools.\nStrong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.\nExperience with cloud-based data warehouse platform like Snowflake.\nExperience working on NoSQL databases like Elasticsearch and MongoDB.\nExperience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.\nExperience on Cloud platforms like AWS and/or Azure.\nExperience working with backend microservices and APIs using Java or C#.\nExposure on message-oriented middleware technologies like Kafka is a plus.\nAbility to work collaboratively in a team environment.\nNeed to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.\nExperience with Financial Services application is a plus.\nEffective communication skills, both written and verbal.\nBachelor's or master's degree in computer science, Engineering, or a related field.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
Associate Data Engineer,Bristlecone,Fresher,,"Mumbai, India",Login to check your skill match score,"Job Summary\n\nJOB DESCRIPTION\n\nWe are looking for a motivated and detail-oriented Junior Data Engineer to join our team. The ideal candidate will assist in building, maintaining, and optimizing scalable data pipelines and systems for data processing and analytics. This role is perfect for individuals who are eager to learn and grow in the field of data engineering while working on exciting, data-driven projects.\n\nResponsibilities\n\nKey Responsibilities:\n\nAssist in designing and developing data pipelines to ingest, transform, and load data into data warehouses or data lakes.\nWork with ETLprocesses to automate data workflows and ensure data integrity.\nCollaborate with the team to troubleshoot and resolve data-related issues.\nSupport the implementation of data models, schemas, and storage solutions.\nOptimize data processing workflows for performance and scalability.\nMaintain documentation for data systems, pipelines, and processes.\nStay updated with emerging data engineering tools and technologies.\n\nQualifications\n\nRequired Skills and Qualifications:\n\nKnowledge of programming languages such as Python , SQL , or Java for data processing.\nFamiliarity with relational databases (e.g., MySQL , PostgreSQL ) and data warehousing concepts.\nBasic understanding of cloud platforms (e.g., AWS , GCP , Azure ) for data storage and processing.\nFamiliarity with big data tools like Spark , Hadoop , or Kafka is a plus.\nStrong analytical and problem-solving skills.\nAbility to work collaboratively in a team environment.\nEagerness to learn and adapt to new technologies and methodologies.\n\nPreferred Qualifications\n\nExposure to data visualization tools like PowerBI , Tableau , or Looker .\nUnderstanding of APIs and data integration techniques.\nKnowledge of version control systems like Git .\nExperience with workflow orchestration tools like ApacheAirflow .\nRelevant certifications in cloud platforms or data engineering tools are a plus.\n\nAbout Us\n\nABOUT US\n\nBristlecone is the leading provider of AI-powered application transformation services for the connected supply chain. We empower our customers with speed, visibility, automation, and resiliency to thrive on change.\n\nOur transformative solutions in Digital Logistics, Cognitive Manufacturing, Autonomous Planning, Smart Procurement and Digitalization are positioned around key industry pillars and delivered through a comprehensive portfolio of services spanning digital strategy, design and build, and implementation across a range of technology platforms.\n\nBristlecone is ranked among the top ten leaders in supply chain services by Gartner. We are headquartered in San Jose, California, with locations across North America, Europe and Asia, and over 2,500 consultants. Bristlecone is part of the $19.4 billion Mahindra Group.\n\nEqual Opportunity Employer\n\nBristlecone is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status .\n\nInformation Security Responsibilities\n\nUnderstand and adhere to Information Security policies, guidelines and procedure, practice them for protection of organizational data and Information System.\nTake part in information security training and act while handling information.\nReport all suspected security and policy breach to InfoSec team or appropriate authority (CISO).\nUnderstand and adhere to the additional information security responsibilities as part of the assigned job role.","Looker, Java, Hadoop, Apis, PostgreSQL, Kafka, Tableau, Sql, Git, Gcp, Powerbi, MySQL, Spark, Azure, Python, AWS"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.\n\nWe are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.\n\nEngineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.\n\nKey Responsibilities\n\nDesign, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.\nDevelop data transformation using DBT (Data Build Tool) with SQL or Python.\nDesign and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.\nDevelop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.\nEnsure data quality and integrity through automated testing and validation using tools like Great Expectations.\nImplement all observability requirements in the data pipeline.\nOptimize data workflows for performance and scalability.\nPerforms code and design reviews for tasks done by other team members.\nWorks with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.\nWorks on the development of technical standards for the product and platform.\nCollaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.\nProvide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.\nDocument data engineering processes and best practices whenever required.\n\nRequired Skills And Qualifications\n\nMust have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.\nExposure on leading complex software projects.\nStrong programming skills in Python.\nExperience with Apache Airflow or any other orchestration framework for data orchestration.\nProficiency in DBT for data transformation and modeling.\nExperience with data quality validation tools like Great Expectations or any other similar tools.\nStrong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.\nExperience with cloud-based data warehouse platform like Snowflake.\nExperience working on NoSQL databases like Elasticsearch and MongoDB.\nExperience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.\nExperience on Cloud platforms like AWS and/or Azure.\nExperience working with backend microservices and APIs using Java or C#.\nExposure on message-oriented middleware technologies like Kafka is a plus.\nAbility to work collaboratively in a team environment.\nNeed to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.\nExperience with Financial Services application is a plus.\nEffective communication skills, both written and verbal.\nBachelor's or master's degree in computer science, Engineering, or a related field.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline + 38 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql"
Lead Azure Data Engineer,Aspire Systems,7-9 Years,,"Chennai, India",Login to check your skill match score,"Job Title: Lead Azure Data Engineer\nExperience: 7+ Years\nLocation: Chennai / Bangalore /Hyderabad /Kochi\nAbout Aspire Systems\nAspire Systems is a global technology services firm offering end-to-end digital transformation and product engineering solutions. We are currently seeking an experienced Lead Azure Data Engineer to join our Data & Analytics team and help build scalable, secure, and intelligent data platforms.\nRole Overview\nAs an Azure Data Engineer, you will be responsible for creating, managing, and optimizing data lakes and pipelines using Azure technologies. You will work with internal teams to build high-performance data platforms that support enterprise-grade analytics and reporting.\nKey Responsibilities\nDesign and build data lakes and lakehouses from scratch, and configure existing systems.\nDevelop and manage Azure Data Factory (ADF) and Synapse pipelines for data ingestion and transformation.\nWork on data security implementation across storage and data movement layers.\nUse Python, PySpark, and Spark SQL for data transformation and orchestration.\nCreate and maintain CI/CD pipelines for data workflows.\nUnderstand and work with various file formats: JSON, Parquet, CSV, Excel, and both structured and unstructured datasets.\nCollaborate with internal teams to support application integration and performance optimization.\nProvide user support and documentation where required.\nKey Skills Required\nExpertise in Azure Data Lake, Lakehouse Architecture, Synapse Analytics, Databricks, and T-SQL.\nStrong experience in ETL/ELT pipelines, working with large volumes of data.\nProficient with SQL Server, Synapse DB, and data warehousing concepts.\nUnderstanding of metadata management, data catalogs, DWH, MPP, OLTP, and OLAP systems.\nFamiliar with Source Control tools like Git, TFS, or SVN.\nKnowledge of Azure Data Fabric, Microsoft Purview, and MDM tools is a plus.\nExperience working with non-functional requirements and performance tuning.\nStrong communication skills and the ability to work effectively in a collaborative team environment.\nWhy Aspire\nWork on cutting-edge Azure Data & AI solutions.\nCollaborate with a global team on enterprise-scale transformation projects.\nGrowth opportunities into Data Architect or Tech Lead roles.\nContinuous learning and certification support.\nInterested candidates can share their resume with [HIDDEN TEXT]","MDM tools, OLAP systems, Synapse DB, Microsoft Purview, Azure Data Fabric, ETL ELT pipelines, Synapse Analytics, Azure Data Lake Lakehouse Architecture, data catalogs, Spark SQL, T-sql, Metadata Management, Pyspark, SQL Server, Mpp, Dwh, Databricks, Data Warehousing Concepts, Python, Oltp"
IN_Senior Associate_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,3-8 Years,,"Mumbai, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary: A career within PWC\n\nResponsibilities\n\nJob Title: Cloud Data Engineer (AWS/Azure/Databricks/GCP) Experience:3-8 years in Data Engineering Job Description: We are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions. Key Responsibilities: - Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP. - Implement data ingestion and transformation processes to facilitate efficient data warehousing. - Utilize cloud services to enhance data processing capabilities: - AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS. - Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus. - GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion. - Optimize Spark job performance to ensure high efficiency and reliability.\n\nStay proactive in learning and implementing new technologies to improve data processing frameworks. - Collaborate with cross-functional teams to deliver robust data solutions. - Work on Spark Streaming for real-time data processing as necessary. Qualifications: - 3-8 years of experience in data engineering with a strong focus on cloud environments. - Proficiency in PySpark or Spark is mandatory. - Proven experience with data ingestion, transformation, and data warehousing. - In-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP): - Demonstrated ability in performance optimization of Spark jobs. - Strong problem-solving skills and the ability to work independently as well as in a team. - Cloud Certification (AWS, Azure, or GCP) is a plus. - Familiarity with Spark Streaming is a bonus.\n\nMandatory Skill Sets\n\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nPreferred Skill Sets\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nYears Of Experience Required\n\n3-8 years\n\nEducation Qualification\n\nBE/BTECH, ME/MTECH, MBA, MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nNode.js\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation + 19 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","GCP Dataflow, Azure Data Factory, Gcp, Pyspark, AWS Glue, Spark, Azure, Sql, Python, AWS"
AWS Data Engineer,Zensar Technologies,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Hi Applicants,\nWe have an urgent demand for AWS Data Engineer. Please go through the Job Description below and share your resume if you feel interested.\nRequirement for AWS Data Engineer\nExperience-5+ Years\nLocation-Hyderabad\nNotice Period-Immediate\nJob Description\nKey Responsibilities\nDesign, Develop, and Maintain Redshift Data Warehouses:\nArchitect, build, and optimize Redshift clusters for high performance and scalability.\nDesign and implement data models, schemas, and ETL/ELT pipelines.\nDevelop and maintain complex SQL queries and stored procedures.\nEnsure data quality and integrity within the Redshift environment.\nData Loading and Transformation:\nDevelop and implement efficient data loading strategies from various sources (e.g., databases, files, APIs).\nDesign and implement data transformation and cleansing processes.\nOptimize data loading performance and minimize data latency.\nPerformance Tuning and Optimization:\nMonitor and analyze Redshift cluster performance.\nIdentify and troubleshoot performance bottlenecks.\nImplement performance tuning strategies, such as indexing, partitioning, and query optimization.\nAWS Integration:\nLeverage other AWS services, such as S3, Glue, EMR, and Lambda, to enhance data warehousing solutions.\nIntegrate Redshift with other cloud-based applications and services.\nSecurity and Compliance:\nImplement and maintain data security measures, including access control, encryption, and data masking.\nEnsure compliance with relevant data privacy regulations (e.g., GDPR, CCPA).\nCollaboration and Communication:\nCollaborate with data analysts, data scientists, and business stakeholders to understand their data needs.\nCommunicate technical information effectively to both technical and non-technical audiences.\nRequired Skills\nStrong proficiency in SQL\nExperience with AWS Redshift\nKnowledge of data warehousing concepts and best practices\nExperience with ETL/ELT processes\nExperience with data modeling and schema design\nUnderstanding of cloud computing concepts and AWS ecosystem\nExperience with scripting languages (e.g., Python, Shell)\nStrong analytical and problem-solving skills\nDesired Skills\nExperience with AWS services like S3, Glue, EMR, Lambda\nExperience with data visualization tools (e.g., Tableau, Power BI)\nExperience with data quality and validation tools\nExperience with Agile development methodologies\nAWS certifications (e.g., AWS Certified Data Analytics - Specialty)","ETL ELT processes, cloud computing concepts, AWS ecosystem, data modeling and schema design, data warehousing concepts and best practices, Aws Redshift, Sql"
Lead Azure Data Engineer,Aspire Systems,7-9 Years,,"Chennai, India",Login to check your skill match score,"Job Title: Lead Azure Data Engineer\nExperience: 7+ Years\nLocation: Chennai / Bangalore /Hyderabad /Kochi\nAbout Aspire Systems\nAspire Systems is a global technology services firm offering end-to-end digital transformation and product engineering solutions. We are currently seeking an experienced Lead Azure Data Engineer to join our Data & Analytics team and help build scalable, secure, and intelligent data platforms.\nRole Overview\nAs an Azure Data Engineer, you will be responsible for creating, managing, and optimizing data lakes and pipelines using Azure technologies. You will work with internal teams to build high-performance data platforms that support enterprise-grade analytics and reporting.\nKey Responsibilities\nDesign and build data lakes and lakehouses from scratch, and configure existing systems.\nDevelop and manage Azure Data Factory (ADF) and Synapse pipelines for data ingestion and transformation.\nWork on data security implementation across storage and data movement layers.\nUse Python, PySpark, and Spark SQL for data transformation and orchestration.\nCreate and maintain CI/CD pipelines for data workflows.\nUnderstand and work with various file formats: JSON, Parquet, CSV, Excel, and both structured and unstructured datasets.\nCollaborate with internal teams to support application integration and performance optimization.\nProvide user support and documentation where required.\nKey Skills Required\nExpertise in Azure Data Lake, Lakehouse Architecture, Synapse Analytics, Databricks, and T-SQL.\nStrong experience in ETL/ELT pipelines, working with large volumes of data.\nProficient with SQL Server, Synapse DB, and data warehousing concepts.\nUnderstanding of metadata management, data catalogs, DWH, MPP, OLTP, and OLAP systems.\nFamiliar with Source Control tools like Git, TFS, or SVN.\nKnowledge of Azure Data Fabric, Microsoft Purview, and MDM tools is a plus.\nExperience working with non-functional requirements and performance tuning.\nStrong communication skills and the ability to work effectively in a collaborative team environment.\nWhy Aspire\nWork on cutting-edge Azure Data & AI solutions.\nCollaborate with a global team on enterprise-scale transformation projects.\nGrowth opportunities into Data Architect or Tech Lead roles.\nContinuous learning and certification support.\nInterested candidates can share their resume with [HIDDEN TEXT]","MDM tools, OLAP systems, Synapse DB, Microsoft Purview, Azure Data Fabric, ETL ELT pipelines, Synapse Analytics, Azure Data Lake Lakehouse Architecture, data catalogs, Spark SQL, T-sql, Metadata Management, Pyspark, SQL Server, Mpp, Dwh, Databricks, Data Warehousing Concepts, Python, Oltp"
"IN_Senior Associate_Data Engineer(PySpark,Python)_Data & Analytics_Advisory_Bangalore",PwC India,4-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nA career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nCreating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities:\n\nRoles & Responsibilities\n\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.\nDesign end-to-end job flows that span across systems, including quality checks and controls.\nCreate technology delivery plans to implement system changes.\nPerform data analysis, data profiling, and data sourcing in relational and Big Data environments.\nConvert functional requirements into logical and physical data models.\nAssist in ETL development, testing, and troubleshooting ETL issues.\nTroubleshoot data issues and work with data providers for resolution; provide L3 support when needed.\nDesign and develop ETL workflows using modern coding and testing standards.\nParticipate in agile ceremonies and actively drive towards team goals.\nCollaborate with a global team of technologists.\nLead with ideas and innovation.\nManage communication and partner with end users to design solutions.\n\nRequired Skills:\n\n\nMust have: Total experience required 4-10 years (relevant experience minimum 5 years)\n\n5 years of project experience in Python/Shell scripting in Data Engineering (experience in building and optimizing data pipelines, architectures, and data sets with large data volumes).\n3+ years of experience in PySpark scripting, including the architecture framework of Spark.\n3-5 years of strong experience in database development (Snowflake/ SQL Server/Oracle/Sybase/DB2) in designing schema, complex procedures, complex data scripts, query authoring (SQL), and performance optimization.\nStrong understanding of Unix environment and batch scripting languages (Shell/Python).\nStrong knowledge of Big Data/Hadoop platform.\nStrong engineering skills with the ability to understand existing system designs and enhance or migrate them.\nStrong logical data modeling skills within the Financial Services domain.\nExperience in data integration and data conversions.\nStrong collaboration and communication skills.\nStrong organizational and planning skills.\nStrong analytical, profiling, and troubleshooting skills.\n\nGood To Have:\n\n\nExperience with ETL tools (e.g Informatica, Azure Data Factory) and pipelines across disparate sources is a plus.\nExperience working with Databricks is a plus.\nFamiliarity with standard Agile & DevOps methodology & tools (Jenkins, Sonar, Jira).\nGood understanding of developing ETL processes using Informatica or other ETL tools.\nExperience working with Source Code Management solutions (e.g., Git).\nKnowledge of Investment Management Business.\nExperience with job scheduling tools (e.g., Autosys).\nExperience with data visualization software (e.g., Tableau).\nExperience with data modeling tools (e.g., Power Designer).\nBasic familiarity with using metadata stores to maintain a repository of Critical Data Elements. (e.g. Collibra)\nFamiliarity with XML or other markup languages.\n\nMandatory Skill Sets:\n\n\nETL,Python/Shell scripting , building pipelines,pyspark, database, sql\n\nPreferred Skill Sets:\n\ninformatica, hadoop, databricks, collibra\n\nYears Of Experience Required:\n\n4 to 10 years\n\nEducation Qualification:\n\nGraduate Engineer or Management Graduate\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration\n\nDegrees/Field Of Study Preferred:\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nExtract Transform Load (ETL), Python (Programming Language), Structured Query Language (SQL)\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment, Performance Management Software + 16 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","power designer, snowflake, Sybase, Sonar, Hadoop, Collibra, Pyspark, SQL Server, Tableau, Jira, Informatica, Sql, Jenkins, Git, Azure Data Factory, DB2, Shell scripting, Databricks, Oracle, Python, Etl"
IN-Manager_Azure Data Engineer_Data Analytics_Advisory_Bangalore,PwC India,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nOperations\n\nManagement Level\n\nManager\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary: A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nResponsibilities\n\nMust have:\n\nCandidates with a minimum of 5 years of relevant experience for 10-12 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills.\nProven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\n\nGood To Have\n\n\nGood to have certifications: Apache Spark 3.0\nExperience in any one visualization tool like Power BI, tableau etc.\nUnderstanding of Spark Architecture landscape\n\nMandatory Skill Sets\n\n\nSpark, Pyspark, Azure\n\nPreferred Skill Sets\n\nSpark, Pyspark, Azure\n\nYears Of Experience Required\n\n8-12yrs\n\nEducation Qualification\n\nB.Tech / M.Tech / MBA / MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Master of Engineering, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation + 23 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date","Pyspark scripting, Data Bricks, Synapse, Stored Procedures, Azure Analysis Services, Data Factory, Power Bi, Tableau, Python Scripting, Github, Advanced Sql, Azure DevOps"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements\n\nJob Requirements\n\nRole/ Job Title: Data Engineer - Gen AI\n\nFunction/ Department: Data & Analytics\n\nPlace of Work: Mumbai\n\nJob Purpose\n\nThe data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.\n\nJob & Responsibilities\n\nBuild data engineering pipeline focused on unstructured data pipelines\nConduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.\nDesign, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.\nWork on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability\nDrive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets\nA suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.\nCreating Technical Design Documentation of the projects/pipelines\nGood skills in technical debugging of the code in case of issues. Also, working with git for code versioning\n\nEducation Qualification\n\nGraduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)\n\nPost-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA\n\nExperience Range : 5-10 years of relevant experience","Data Ingestion, Data engineering pipeline, Big data infrastructure, Technical debugging, HDFS, Data Consumption, Technical Design Documentation, Unstructured data pipelines, Dynamodb, Data Architecture, HBase, Yarn, Mapreduce, Git, Hive, MongoDB, Data Transformation"
Vice President - Data Engineer,BlackRock,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"About This Role\n\nAt BlackRock, technology has always been at the core of what we do and today, our technologists continue to shape the future of the industry with their innovative work. We are not only curious but also collaborative and eager to embrace experimentation as a means to solve complex challenges. Here you'll find an environment that promotes working across teams, businesses, regions and specialties and a firm committed to supporting your growth as a technologist through curated learning opportunities, tech-specific career paths, and access to experts and leaders around the world.\n\nWe are seeking a highly skilled and motivated Lead Data Engineer to join the Private Market Data Engineering team within Aladdin Data at BlackRock for driving our Private Market Data Engineering vision of making private markets more accessible and transparent for clients. In this role, you will work multi-functionally with Product, Data Research, Engineering, and Program management.\n\nEngineers looking to work in the areas of orchestration, data modeling, data pipelines, APIs, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. The candidate will have extensive experience in leading, designing and developing data pipelines using Python, Apache Airflow orchestration platform, DBT (Data Build Tool), Great Expectations for data validation, Apache Spark, MongoDB, Elasticsearch, Snowflake and PostgreSQL. In this role, you will be responsible for leading, designing, developing, and maintaining robust and scalable data pipelines. You will collaborate with various stakeholders to ensure the data pipelines are efficient, reliable, and meet the needs of the business.\n\nKey Responsibilities\n\nDesign, develop, and maintain data pipelines using Aladdin Data Enterprise Data Platform framework.\nDevelop data transformation using DBT (Data Build Tool) with SQL or Python.\nDesign and develop ETL/ELT data pipelines using Python, SQL and deploy them as containerized apps on a Kubernetes cluster.\nDevelop APIs for data distribution on top of the standard data model of the Enterprise Data Platform.\nEnsure data quality and integrity through automated testing and validation using tools like Great Expectations.\nImplement all observability requirements in the data pipeline.\nOptimize data workflows for performance and scalability.\nPerforms code and design reviews for tasks done by other team members.\nWorks with other senior technical staff in the team in the evaluation of tools and technologies to ensure high quality data platform.\nWorks on the development of technical standards for the product and platform.\nCollaborates with Engineering Managers and the business team to understand the system requirements to develop the best possible technical solution for the product.\nProvide Technical guidance to the team on best practices, programming techniques and provide solutions to any technical issue or a problem.\nDocument data engineering processes and best practices whenever required.\n\nRequired Skills And Qualifications\n\nMust have 8 to 12 years of experience in data engineering, with a focus on designing and building data pipelines.\nExposure on leading complex software projects.\nStrong programming skills in Python.\nExperience with Apache Airflow or any other orchestration framework for data orchestration.\nProficiency in DBT for data transformation and modeling.\nExperience with data quality validation tools like Great Expectations or any other similar tools.\nStrong at writing SQL and experience with relational databases like SQL Server, PostgreSQL.\nExperience with cloud-based data warehouse platform like Snowflake.\nExperience working on NoSQL databases like Elasticsearch and MongoDB.\nExperience working with container orchestration platform like Kubernetes on AWS and/or Azure cloud environments.\nExperience on Cloud platforms like AWS and/or Azure.\nExperience working with backend microservices and APIs using Java or C#.\nExposure on message-oriented middleware technologies like Kafka is a plus.\nAbility to work collaboratively in a team environment.\nNeed to possess critical skills of being detail oriented, passion to learn new technologies and good analytical and problem-solving skills.\nExperience with Financial Services application is a plus.\nEffective communication skills, both written and verbal.\nBachelor's or master's degree in computer science, Engineering, or a related field.\n\nOur Benefits\n\nTo help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and Flexible Time Off (FTO) so you can relax, recharge and be there for the people you care about.\n\nOur hybrid work model\n\nBlackRock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. Employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. Some business groups may require more time in the office due to their roles and responsibilities. We remain focused on increasing the impactful moments that arise when we work together in person aligned with our commitment to performance and innovation. As a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at BlackRock.\n\nAbout BlackRock\n\nAt BlackRock, we are all connected by one mission: to help more and more people experience financial well-being. Our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. Their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress.\n\nThis mission would not be possible without our smartest investment the one we make in our employees. It's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\n\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com/company/blackrock\n\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.","Great Expectations, dbt, snowflake, Java, PostgreSQL, Kafka, Sql, Apache Airflow, Elasticsearch, MongoDB, Azure, Kubernetes, Python, AWS"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive data-driven decision-making. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Algorithm Development, Alteryx (Automation Platform), Analytical Thinking, Analytic Research, Big Data, Business Data Analytics, Communication, Complex Data Analysis, Conducting Research, Creativity, Customer Analysis, Customer Needs Analysis, Dashboard Creation, Data Analysis, Data Analysis Software, Data Collection, Data-Driven Insights, Data Integration, Data Integrity, Data Mining, Data Modeling, Data Pipeline + 38 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql, Etl"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nbn*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Cosmos DB, Azure Data Factory, Databricks, Sql, Etl"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql, Etl"
IN-Senior Associate_Data Engineer_D&A_Advisory_PAN India,PwC India,4-8 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nben*Why PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nRequirment gathering and analysis.\n\nPAN India\n\n10\n\nSenior Associate\n\n3-7\n\nExperience with different databases like Synapse, SQL DB, Snowflake etc.\n\nDesign and implement data pipelines using Azure Data Factory, Databricks, Synapse\n\nCreate and manage Azure SQL Data Warehouses and Azure Cosmos DB databases\n\nExtract, transform, and load (ETL) data from various sources into Azure Data Lake Storage\n\nImplement data security and governance measures\n\nMonitor and optimize data pipelines for performance and efficiency\n\nTroubleshoot and resolve data engineering issues\n\nProvide optimized solution for any problem related to data engineering\n\nAbility to work with verity of sources like Relational DB, API, File System, Realtime streams, CDC etc.\n\nStrong knowledge on Databricks, Delta tables\n\n\n\nTechnology: SQL, ADF, ADLS, Synapse, Pyspark, Databricks\n\nMandatory Skill Sets\n\nAzure DE\n\nPreferred Skill Sets\n\nAzure DE\n\nYears Of Experience Required\n\n4-8\n\nEducation Qualification\n\nBtech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion, Industry Trend Analysis + 12 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Azure Data Lake Storage, Synapse, Azure SQL Data Warehouses, Databricks Delta tables, Pyspark, Azure Data Factory, Azure Cosmos DB, Databricks, Sql, Etl"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements\n\nJob Requirements\n\nRole/ Job Title: Data Engineer - Gen AI\n\nFunction/ Department: Data & Analytics\n\nPlace of Work: Mumbai\n\nJob Purpose\n\nThe data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.\n\nJob & Responsibilities\n\nBuild data engineering pipeline focused on unstructured data pipelines\nConduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.\nDesign, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.\nWork on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability\nDrive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets\nA suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.\nCreating Technical Design Documentation of the projects/pipelines\nGood skills in technical debugging of the code in case of issues. Also, working with git for code versioning\n\nEducation Qualification\n\nGraduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)\n\nPost-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA\n\nExperience Range : 5-10 years of relevant experience","big data infrastructure, HDFS, Hive, Dynamodb, Api, MongoDB, HBase, Yarn, Mapreduce, Etl"
Senior Data Engineer,IDFC FIRST Bank,6-8 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements\n\nJob Requirements\n\nRole/ Job Title: Senior Data Engineer\n\nBusiness: New Age\n\nFunction/ Department: Data & Analytics\n\nPlace of Work: Mumbai/Bangalore\n\nRoles & Responsibilities\n\nMinimum 6 years of Data Engineering experience and 3 years in large scale Data Lake ecosystem\n\nProven expertise in SQL, Spark Python, Scala, Hadoop ecosystem,\n\nHave worked on multiple TBs/PBs of data volume from ingestion to consumption\n\nWork with business stakeholders to identify and document high impact business problems and potential solutions\n\nFirst-hand experience with the complete software development life cycle including requirement analysis, design, development, deployment, and support\n\nAdvanced understanding of Data Lake/Lakehouse architecture and experience/exposure to Hadoop (cloudera,hortonworks) and AWS\n\nWork on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer. Versed with API and its usability\n\nA suitable candidate will also be proficient Spark, Spark Streaming, AWS, and EMR\n\nA suitable candidate will also demonstrate machine learning experience and experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc.\n\nThe candidate will additionally demonstrate substantial experience and a deep knowledge of data mining techniques, relational, and non-relational databases.\n\nAdvanced skills in technical debugging of the architecture in case of issues\n\nCreating Technical Design Documentation (HLD/LLD) of the projects/pipelines\n\nSecondary Responsibilities\n\nAbility to work independently and handle your own development effort.\n\nExcellent oral and written communication skills Learn and use internally available analytic technologies\n\nIdentify key performance indicators and establish strategies on how to deliver on these key points for analysis solutions\n\nUse educational background in data engineering and perform data mining analysis\n\nWork with BI analysts/engineers to create prototypes, implementing traditional classifiers and determiners, predictive and regressive analysis points\n\nEngage in the delivery and presentation of solutions\n\nManagerial & Leadership Responsibilities\n\nLead moderately complex initiatives within Technology and contribute to large scale data processing framework initiatives related to enterprise strategy deliverables\n\nBuild and maintain optimized and highly available data pipelines that facilitate deeper analysis and reporting\n\nReview and analyze moderately complex business, operational or technical challenges that require an in-depth evaluation of variable factors\n\nOversee the data integration work, including integrating a data model with datalake, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis\n\nResolve moderately complex issues and lead teams to meet data engineering deliverables while leveraging solid understanding of data information policies, procedures and compliance requirements\n\nCollaborate and consult with colleagues and managers to resolve data engineering issues and achieve strategic goals\n\nKey Success Metrics\n\nEnsure timely deliverables. Spot Data fixes. Lead technical aspects of the projects. Error free deliverables.","HDFS, Hadoop, Scala, Emr, HBase, Yarn, Sql, Mapreduce, Hive, Spark, Oozie, Python, AWS"
IN_Senior Associate_Cloud Data Engineer-- Data and Analytics_Advisory_Pan India,PwC India,3-8 Years,,"Mumbai, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Associate\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decision-making and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary: A career within PWC\n\nResponsibilities\n\nJob Title: Cloud Data Engineer (AWS/Azure/Databricks/GCP) Experience:3-8 years in Data Engineering Job Description: We are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions. Key Responsibilities: - Design, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP. - Implement data ingestion and transformation processes to facilitate efficient data warehousing. - Utilize cloud services to enhance data processing capabilities: - AWS: Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS. - Azure: Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus. - GCP: Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion. - Optimize Spark job performance to ensure high efficiency and reliability.\n\nStay proactive in learning and implementing new technologies to improve data processing frameworks. - Collaborate with cross-functional teams to deliver robust data solutions. - Work on Spark Streaming for real-time data processing as necessary. Qualifications: - 3-8 years of experience in data engineering with a strong focus on cloud environments. - Proficiency in PySpark or Spark is mandatory. - Proven experience with data ingestion, transformation, and data warehousing. - In-depth knowledge and hands-on experience with cloud services(AWS/Azure/GCP): - Demonstrated ability in performance optimization of Spark jobs. - Strong problem-solving skills and the ability to work independently as well as in a team. - Cloud Certification (AWS, Azure, or GCP) is a plus. - Familiarity with Spark Streaming is a bonus.\n\nMandatory Skill Sets\n\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nPreferred Skill Sets\n\nPython, Pyspark, SQL with (AWS or Azure or GCP)\n\nYears Of Experience Required\n\n3-8 years\n\nEducation Qualification\n\nBE/BTECH, ME/MTECH, MBA, MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nNode.js\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation + 19 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","GCP Dataflow, Azure Data Factory, Gcp, Pyspark, AWS Glue, Spark, Azure, Sql, Python, AWS"
Data Engineer-DIG,TresVista,3-7 Years,,"Bengaluru, India",Login to check your skill match score,"Roles and Responsibilities:\nInteracting with the client (internal or external) to understand their problems and work on solutions that address their needs\nDriving projects and working closely with a team of individuals to ensure proper requirements are identified, useful user stories are created, and work is planned logically and efficiently to deliver solutions that support changing business requirements\nManaging the various activities within the team, strategizing how to approach tasks, creating timelines and goals, distributing information/tasks to the various team members\nConducting meetings, documenting, and communicating findings effectively to clients, management and cross-functional teams\nCreating Ad-hoc reports for multiple internal requests across departments\nAutomating the process using data transformation tools\nPrerequisites\nStrong analytical, problem-solving, interpersonal, and communication skills\nAdvanced knowledge of DBMS, Data Modelling along with advanced querying capabilities using SQL Working experience in cloud technologies (GCP/ AWS/Azure/Snowflake)\nPrior experience in building and deploying ETL/ELT pipelines using CI/CD, and orchestration tools such as Apache Airflow, GCP workflows, etc.\nProficiency in Python for building ETL/ELT processes and data modeling\nProficiency in Reporting and Dashboards creation using Power BI/Tableau\nKnowledge in building ML models and leveraging Gen AI for modern architectures.\nExperience working with version control platforms like GitHub\nFamiliarity with IaC tools like Terraform and Ansible is good to have\nStakeholder Management and client communication experience would be preferred\nExperience in the Financial Services domain will be an added plus\nExperience in Machine Learning tools and techniques will be good to have\nExperience\n3-7 years\nEducation\nBTech/MTech/BE/ME/MBA in Analytics\nCompensation\nThe compensation structure will be as per industry standards","Gen AI, GCP workflows, snowflake, Github, Data Modelling, Machine Learning, Power Bi, Tableau, Sql, ELT, Apache Airflow, Gcp, Terraform, Ansible, Dbms, Azure, Python, Etl, AWS"
Data Engineer 3 (Hyderabad),Hyland,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Overview\n\nOverview\n\nHyland Software is widely known as a great company to work for and a great company to do business with. Being a leader in providing software solution for managing content, processes, and cases for organizations across the globe we enabled more than 20,000 organizations to digitalize their workplaces and transform their operations.\n\nCurrently we are looking for the position of Data Engineer 3\n\nThe Data Engineer 3 is responsible for managing data pipelines and developing analytical solutions. This position requires technical expertise to design, build, and implement data solutions within a collaborative environment.\n\nWhat You Will Be Doing\n\nDesign, build, and maintain scalable data pipelines to clean and transform data so that it is ready for analysis.\nDevelop end-to-end solutions from data extraction to operational deployment.\nCollaborate with data scientists, data analysts, and business stakeholders to understand data requirements and deliver data solutions.\nImplement data quality checks and monitoring to ensure data accuracy, consistency, and reliability.\nCreate and maintain thorough documentation for data pipelines architectures and processes.\nEnsure data storage and processing meet security standards and comply with relevant regulations.\nTroubleshoot and resolve data-related issues in a timely manner.\nOperate as a trusted advisor on issues and trends, advising on best practices in data engineering and big data technologies.\nEvaluate the team's models and suggest enhancements as needed promote best practices for modeling and create scalable operational needs.\nCreate and maintain thorough documentation for data pipelines architectures and processes.\nMentor coach train and provide feedback to other team members; provide feedback to leadership on abilities of team.\nComply with all corporate and departmental privacy and data security policies and practices, including but not limited to, Hyland's Information Systems Security Policy.\n\nWhat Will Make You Successful\n\nBachelor's degree or equivalent experience\nProven experience with data engineering including working with data warehouses ETL\nExperience in Data and ML Pipelines orchestration\nProficiency in SQL Python and at least one data processing framework (e.g. Spark Flink TensorFlow PyTorch)\nKnowledge of machine learning concepts and experience applying them in real-world use cases.\nExperience with Snowflake or Microsoft Fabric\nAt least 5 years experience as a Data Engineer with experience in development and maintenance support\nExcellent collaboration skills applied successfully within team as well as with all levels of employees in other areas\nExcellent critical thinking and problem solving skills\nExcellent ability to use original thinking to translate goals into the implementation of new ideas and design solutions\nSelf-motivated with the ability to manage projects to completion independently\nAble to thrive in a fast paced deadline driven environment\nExcellent attention to detail\nExcellent ability to handle sensitive information with discretion and tact\nExcellent ability to establish rapport and gain the trust of others; effective at gaining consensus\nAbility to work independently and in a team environment\nAbility to coach mentor and provide feedback to team members in a timely manner\nAbility to provide guidance and support to developing team members\nUp to 5% travel time required\n\nHyland's Offering\n\nWe're proud of our culture and take employee engagement seriously. By listening to employees feedback, we're able to provide meaningful benefits and programs to our workforce.\n\nLearning & Development - development budget (used for certifications, conferences etc..), tuition assistance program, 4,000+ self-paced online courses, instructor-led webinars, mentorship programs, structured on-boarding experience full of trainings, dedicated Learning & Development department supporting our employees.\nR&D focus cutting edge technologies, constant modernization efforts, dynamic and innovative environment, dedicated R&D Education Services department to help you grow.\nWork-life balance culture flexible work environment and working hours (we are working in task-based system!), possibility to work from home, we value trust, and we believe efficiency does not depend on your actual location, however we would like to spend time together in the office!\nWell-being - private medical healthcare, life insurance, gym reimbursement, psychologist & dietician consultation, wellness manager care, constant wellbeing programs\nCommunity Engagement Volunteer time off (12h/year), Hylanders for Hylanders relief found, Mission fit giving, Dolars-for-doers matching gift programs.\nDiversity & Inclusion employee resource groups, inclusion benefits and policies\nNiceties & Events snacks and beverages, employee referral program, birthday, baby gifts and employee programs\n\nIf you would like to join the company where honesty, integrity and fairness lie in the bottom of values, where people are truly passionate about technology and dedicated to their work connect with us!\n\nWe are committed to a policy of Equal Employment Opportunity and will not discriminate against an applicant or employee on the basis of race, color, religion, creed, national origin or ancestry, sex, age, physical or mental disability, veteran or military status, genetic information, sexual orientation, marital status, gender identity or any other legally recognized protected basis under federal, state or local laws, regulations or ordinances. The information collected by this application is solely to determine suitability for employment, verify identity and maintain employment statistics on applicants.","snowflake, Data and ML Pipelines orchestration, Microsoft Fabric, Flink, Tensorflow, Pytorch, Spark, Python, Sql, Etl"
Data Engineer,Louisa AI,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"About Louisa AI\n\nLouisa AI, a cutting-edge institutional revenue enablement platform, was originally developed at Goldman Sachs and became an independent entity in 2023. It utilizes AI to maximize revenues by mapping the expertise and relationships within organizations, primarily serving financial institutions. Louisa AI emphasizes connecting people through AI, not replacing them, leveraging relationship graphs and news integration to enhance revenue generation and connections based on expertise, relationships, and relevant information.\n\nResponsibilities\n\nAs a data engineer on the Louisa team, you will have the opportunity to:\n\nData Architecture And Design\n\nDesign and implement scalable and efficient data models for storage and retrieval.\nDevelop and maintain data architecture, ensuring optimal performance and data integrity.\n\nETL Development\n\nCreate and optimize ETL processes to extract, transform, and load data from various sources into the data warehouse.\nImplement data transformation logic to ensure data consistency and quality.\n\nProgramming And Scripting\n\nUtilize programming languages (e.g., Python, Java) and scripting languages for data processing and manipulation.\nDevelop custom code and scripts for data extraction, transformation, and loading tasks.\n\nData Quality And Governance\n\nImplement data quality checks and measures to ensure accuracy and reliability.\nAdhere to data governance and security policies, ensuring compliance with relevant regulations.\n\nMinimum Qualifications\n\nBachelor's degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.\nMinimum 5 years of relevant development experience.\nStrong proficiency in programming languages (e.g., Python, Java).\nExperience with big data technologies like Spark, Hive, etc and orchestration tool like airflow.\nExperience with Data warehousing tools like Snowflake/Big Query/Databricks SQL.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services.\nExcellent problem-solving and analytical skills.\nStrong communication and collaboration skills.\n\nExtra Awesome\n\nTechnical expertise with data models, data mining, and segmentation techniques\nComfortable multi-tasking, managing multiple stakeholders and working in a global team.","Big Query, Airflow, snowflake, Java, Sql, Hive, Gcp, Spark, Databricks, Azure, Python, AWS"
Data Engineer,Synechron Technologies Pvt. Ltd.,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"Greetings,\nWe have immediate opportunity for Data Engineer 5-10 years\nSynechron Mumbai/ Pune/ Bangalore/ Chennai/ Kolkata/ Gurgaon\nJob Role: Data Engineer\nJob Location: Mumbai/ Pune/ Bangalore/ Chennai/ Kolkata/ Gurgaon\nAbout Synechron\nWe began life in 2001 as a small, self-funded team of technology specialists. Since then, we've grown our organization to 14,500+ people, across 58 offices, in 21 countries, in key global markets.\nInnovative tech solutions for business\nWe're now a leading global digital consulting firm, providing innovative technology solutions for business. As a trusted partner, we're always at the forefront of change as we lead digital optimization and modernization journeys for our clients.\nCustomized end-to-end solutions\nOur expertise in AI, Consulting, Data, Digital, Cloud & DevOps and Software Engineering, delivers customized, end-to-end solutions that drive business value and growth.\nJob Description:\nSoftware Requirements:\nStrong proficiency in SQL\nExperience with HIVE\nFamiliarity with SQOOP\nKnowledge of SPARK is a plus\nExperience in performance optimization techniques\nProficiency in on-premises solutions, especially with Azure\nFamiliarity with Cloudera and HortonWorks platforms\nOverall Responsibilities:\nDesign, develop, and maintain data pipelines and ETL processes to support data analytics and reporting needs.\nEnsure data integrity and quality throughout the data lifecycle.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.\nOptimize database performance and troubleshoot issues related to data storage and retrieval.\nStay updated on industry trends and emerging technologies to continuously enhance data architecture and operations.\nTechnical Skills:\nDatabase Technologies:\nSQL (Must-have)\nHIVE (Must-have)\nSQOOP (Must-have)\nSPARK (Preferred)\nPerformance Optimization:\nTechniques for optimizing data processing and query performance.\nCloud Technologies:\nAzure (on-premises focus)\nBig Data Platforms:\nCloudera\nHortonWorks\nExperience:\n4.5+ years of experience in data engineering or related field.\nProven experience with database management and data warehousing.\nFamiliarity with big data technologies and cloud-based solutions is an advantage.\nDay-to-Day Activities:\nDevelop and maintain data pipelines from various sources to databases.\nMonitor data flows and troubleshoot issues in real time.\nCollaborate with stakeholders to gather requirements and deliver actionable insights.\nConduct data quality checks and performance optimizations on existing systems.\nParticipate in team meetings to discuss project progress and share knowledge.\nFor more information on the company, please visit our website or LinkedIn community.\nIf you find this this opportunity interesting kindly share your updated profile on [HIDDEN TEXT]\nWith below details (Mandatory)\nTotal Experience\nExperience as Data Engineer-\nExperience in Spark-\nExperience in Hive-\nExperience in SQL\nCurrent CTC-\nExpected CTC-\nNotice period-\nCurrent Location-\nReady to relocate to Mumbai-\nIf you had gone through any interviews in Synechron before If Yes when\nRegards,\nBansi Hindocha\n[HIDDEN TEXT]","Hive, Sqoop, Hortonworks, Spark, Cloudera, Azure, Sql"
Tech Lead- Data Engineer,Thought Frameworks,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"INBOX : Tech Lead- Data Engineer @ [HIDDEN TEXT]\nPosition : Tech Lead Data Engineer\nExp : 8+ Yrs\nJob Location : Gurugram (3 days work from office)\nMandate skill : Snowflake, Python, SQL, ETL/ELT with Asset Management experience (Finance domain)\nAll the skill will reflect in profile project and tech skill.\nNP : with 15 - 20 days\nWork Type : Full Time Hire\nJob Description :\nKey Responsibilities:\nData Pipeline Development: Design, build, and maintain robust ELT (Extract, Load, Transform) pipelines using Snowflake to support data ingestion, integration, and transformation.\nTechnical Leadership: Lead offshore development teams in implementing best practices for data engineering and ELT development.\nData Integration: Collaborate with stakeholders to understand data sources and integration requirements, ensuring seamless connectivity and data flow between systems.\nPerformance Optimization: Optimize data pipelines for performance, scalability, and reliability, including query tuning and resource management within Snowflake.\nData Quality Assurance: Implement and monitor data validation procedures to ensure data accuracy and consistency across systems.\nCollaboration and Communication: Work closely with project managers, data architects, and business analysts to align project milestones and deliverables with business goals.\nDocumentation: Create and maintain detailed documentation of data pipelines, data flow diagrams, and transformation logic.\nIssue Resolution: Troubleshoot and resolve issues related to data pipelines, including job failures and performance bottlenecks.\nCompliance and Security: Ensure all data management processes comply with data governance policies and regulatory requirements in financial services.\nRequired Qualifications:\nBachelor's degree in Computer Science, Information Technology, or a related field.\n5+ years of experience in data engineering with a strong focus on ELT processes and data pipeline development.\nHands-on experience with Snowflake cloud data platform, including data sharing, secure views, and performance optimization.\nProficiency in SQL and familiarity with data integration and ETL/ELT tools.\nExperience managing and collaborating with offshore development teams.\nStrong problem-solving skills and the ability to work independently to meet deadlines.\nExcellent communication skills for effectively interacting with technical and non-technical stakeholders.\nPreferred Qualifications:\nCertifications in Snowflake or relevant data technologies.\nExperience in the financial services sector with an understanding of data security and compliance requirements.\nFamiliarity with cloud platforms (e.g., AWS, Azure) and data orchestration tools (e.g., Apache Airflow).\nTech Lead- Data Engineer\nExperience with scripting languages such as Python or JavaScript for data transformation.\nKnowledge of data visualization tools (e.g., Tableau, Power BI).","snowflake, Data Pipeline Development, Performance Optimization, Data Integration, Sql, ELT, Etl, Python, data quality assurance"
Data Engineer,Bounteous,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Data Engineer\nLocation: Bangalore (On-Site)\nStart Date: 1st week of June (immediate joiners or candidates serving notice period only)\nExperience: 5 10 years (candidates with less than 5Years)\nRequired Skills:\nPython\nPySpark\nAWS (EMR)\nFalcon Framework\nKey Responsibilities:\nBuild and optimize large-scale data pipelines on AWS EMR\nDevelop and maintain PySpark jobs for data processing and analytics\nImplement data ingestion workflows using the Falcon framework\nCollaborate with stakeholders to deliver robust, scalable data solutions","AWS EMR, Falcon Framework, Pyspark, Python"
Data Engineer,Kavi India,2-4 Years,,India,Login to check your skill match score,"We are seeking a detail-oriented and highly motivated Data Engineer to join our growing Data &\nAnalytics team. In this role, you will be responsible for designing, building, and maintaining robust\ndata pipelines and infrastructure that power insights across the organization. You'll work closely with\ndata scientists, analysts, and engineers to ensure the integrity, accessibility, and scalability of our data\nsystems.\n\nResponsibilities:\n\nDesign, develop, and maintain scalable data pipelines and ETL processes.\nBuild and optimize data architecture to ensure data quality and consistency.\nIntegrate data from diverse internal and external sources.\nCollaborate with cross-functional teams to define data requirements and deliver solutions.\nImplement best practices for data governance, security, and compliance.\nMonitor pipeline performance and perform real-time troubleshooting of data issues.\nParticipate in code reviews and contribute to documentation and standards.\n\nRequired Qualifications & Skills:\n\nBachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical\nexperience).\nMinimum of 2 years of professional experience in data engineering or software development.\nSolid understanding of SQL and proficiency in at least one programming language, such as\nPython, Java, or Scala.\nPractical experience building and maintaining data pipelines using tools like Apache Airflow or\ndbt.\nHands-on experience with cloud platforms (AWS, GCP, Azure) and data warehousing solutions\n(Redshift, BigQuery, Snowflake).\nFamiliarity with big data technologies and frameworks, including Spark, Kafka, and Hadoop.\nDemonstrated ability to solve complex problems with a strong focus on detail.\nExperience implementing CI/CD practices for data workflows.\nWorking knowledge of data modeling principles and schema design.\nExposure to machine learning pipelines or real-time analytics systems is a plus.","snowflake, dbt, Sql, Apache Airflow, Data Modeling, Java, Hadoop, Schema Design, Kafka, BigQuery, AWS, Python, Azure, Gcp, Scala, Spark, Redshift"
Principal Data Engineer,MakeMyTrip,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Principal Data Engineer\nExperience: Must have 8+ years of experience\nAbout Role:\nWe are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.\nAbout us:\nNurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.\nPost-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.\nGO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.\nAbout the team:\nMakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.\nData Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.\nThere are two key responsibilities of Data Engineering team:\nOne to develop the platform for data capture, storage, processing, serving and querying.\nSecond is to develop data products starting from;\no personalization & recommendation platform\no customer segmentation & intelligence\no data insights engine for persuasions and\no the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users\nWe developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.\nTechnology experience:\nExtensive experience working with large data sets with hands-on technology skills to design and build robust data architecture\nExtensive experience in data modeling and database design\nAt least 6+ years of hands-on experience in Spark/BigData Tech stack\nStream processing engines Spark Structured Streaming/Flink\nAnalytical processing on Big Data using Spark\nAt least 6+ years of experience in Scala\nHands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems\nAt least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform\nAt least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc\nAwareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies\nShould have experience in architecting and implementing domain centric big data solutions\nAbility to frame architectural decisions and provide technology leadership & direction\nExcellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Senior Data Engineer,Wavicle Data Solutions,6-8 Years,INR 17.45 - 22.78 LPA,India,Login to check your skill match score,"We are seeking a highly skilled Senior Azure Databricks Data Engineer to design, develop, and optimize data solutions on Azure. The ideal candidate will have expertise in Azure Data Factory (ADF), Databricks, SQL, Python, and experience working with SAP IS-Auto as a data source. This role involves data modeling, systematic layer modeling, and ETL/ELT pipeline development to enable efficient data processing and analytics.\nExperience: 6+ years\nKey Responsibilities:\nDevelop & Optimize ETL Pipelines: Build robust and scalable data pipelines using ADF, Databricks, and Python for data ingestion, transformation, and loading.\nData Modeling & Systematic Layer Modeling: Design logical, physical, and systematic data models for structured and unstructured data.\nIntegrate SAP IS-Auto: Extract, transform, and load data from SAP IS-Auto into Azure-based data platforms.\nDatabase Management: Develop and optimize SQL queries, stored procedures, and indexing strategies to enhance performance.\nBig Data Processing: Work with Azure Databricks for distributed computing, Spark for large-scale processing, and Delta Lake for optimized storage.\nData Quality & Governance: Implement data validation, lineage tracking, and security measures for high-quality, compliant data.\nCollaboration: Work closely with business analysts, data scientists, and DevOps teams to ensure data availability and usability.\nRequired Skills:\nAzure Cloud Expertise: Strong experience in Azure Data Factory (ADF), Databricks, and Azure Synapse.\nProgramming: Proficiency in Python for data processing, automation, and scripting.\nSQL & Database Skills: Advanced knowledge of SQL, T-SQL, or PL/SQL for data manipulation.\nSAP IS-Auto Data Handling: Experience integrating SAP IS-Auto as a data source into data pipelines.\nData Modeling: Hands-on experience in dimensional modeling, systematic layer modeling, and entity-relationship modeling.\nBig Data Frameworks: Strong understanding of Apache Spark, Delta Lake, and distributed computing.\nPerformance Optimization: Expertise in query optimization, indexing, and performance tuning.\nData Governance & Security: Knowledge of RBAC, encryption, and data privacy standards.\nPreferred Qualifications:\nExperience with CI/CD for data pipelines using Azure DevOps.\nKnowledge of Kafka/Event Hub for real-time data processing.\nExperience with Power BI/Tableau for data visualization (not mandatory but a plus).","SAP IS-Auto, rbac, Delta Lake, Data Privacy Standards, Data Modeling, Apache Spark, Encryption, Sql, Databricks, Data Governance, Python, Etl"
Data Engineer Associate - Operate,PwC Acceleration Centers in India,2-5 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services.\n\nDriven by curiosity, you are a reliable, contributing member of a team. In our fast-paced environment, you are expected to adapt to working with a variety of clients and team members, each presenting varying challenges and scope. Every experience is an opportunity to learn and grow. You are expected to take ownership and consistently deliver quality work that drives value for our clients and success as a team. As you navigate through the Firm, you build a brand for yourself, opening doors to more opportunities.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\nApply a learning mindset and take ownership for your own development.\nAppreciate diverse perspectives, needs, and feelings of others.\nAdopt habits to sustain high performance and develop your potential.\nActively listen, ask questions to check understanding, and clearly express ideas.\nSeek, reflect, act on, and give feedback.\nGather information from a range of sources to analyse facts and discern patterns.\nCommit to understanding how the business works and building commercial awareness.\nLearn and apply professional and technical standards (e.g. refer to specific PwC tax and audit guidance), uphold the Firm's code of conduct and independence requirements.\n\nRole: Associate\n\nTower: Data, Analytics & Specialist Managed Service\n\nExperience: 2.0 - 5.5 years\n\nKey Skills: AWS\n\nEducational Qualification: BE / B Tech / ME / M Tech / MBA\n\nWork Location: India.;l\n\nJob Description\n\nAs a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:\n\nUse feedback and reflection to develop self-awareness, personal strengths, and address development areas.\nFlexible to work in stretch opportunities/assignments.\nDemonstrate critical thinking and the ability to bring order to unstructured problems.\nTicket Quality and deliverables review, Status Reporting for the project.\nAdherence to SLAs, experience in incident management, change management and problem management.\nSeek and embrace opportunities which give exposure to different situations, environments, and perspectives.\nUse straightforward communication, in a structured way, when influencing and connecting with others.\nAble to read situations and modify behavior to build quality relationships.\nUphold the firm's code of ethics and business conduct.\nDemonstrate leadership capabilities by working, with clients directly and leading the engagement.\nWork in a team environment that includes client interactions, workstream management, and cross-team collaboration.\nGood team player, take up cross competency work and contribute to COE activities.\nEscalation/Risk management.\n\nPosition Requirements\n\nRequired Skills:\n\nAWS Cloud Engineer\n\nJob description:\n\nCandidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas:\n\nShould have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms.\nShould have minimum 1-3 years of Operate/Managed Services/Production Support Experience\nShould have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc.\nDesigning and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes.\nShould have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage.\nImplementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases\nShould have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools\nScaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments.\nShould have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc.\nShould have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc.\nShould have Strong communication, problem solving, quantitative and analytical abilities.\n\nNice To Have\n\nAWS certification\n\nManaged Services- Data, Analytics & Insights Managed Service\n\nAt PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients better.\n\nWithin our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client's enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC's Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today's dynamic business environment.\n\nWithin our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective.\n\nAs a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.","AWS DMS, Prefect, dbt, snowflake, Aws Lambda, Hadoop, Collibra, Pyspark, AWS Glue, Informatica, Sql, Spark, Python, AWS"
Senior Data Engineer,DataRobot,5-7 Years,,India,Login to check your skill match score,"Job Description:\n\nDataRobot delivers AI that maximizes impact and minimizes business risk. Our platform and applications integrate into core business processes so teams can develop, deliver, and govern AI at scale. DataRobot empowers practitioners to deliver predictive and generative AI, and enables leaders to secure their AI assets. Organizations worldwide rely on DataRobot for AI that makes sense for their business today and in the future.\n\nTitle: Senior Data Engineer (India)\n\nAbout DataRobot\n\nDataRobot delivers the industry-leading agentic AI applications and platform that maximize impact and minimize risk for your business.. DataRobot's enterprise AI platformdemocratizes data science with end-to-end automation for building, deploying, and managing machine learning models. This platform maximizes business value by delivering AI at scale and continuously optimizing performance over time. The company's proven combination of cutting-edge software and world-class AI implementation, training, and support services empowers any organization, regardless of size, industry, or resources, to drive better business outcomes with AI.\n\nYou will be responsible for the following:\n\nPartner with internal customers and business analysts to understand business needs and build strong relationships with key stakeholders.\nDevelop, deploy, and support analytic data products, such as data marts, ETL jobs (extract/transform/load), functions (in Python/SQL/DBT) in a cloud data warehouse environment using Snowflake, Stitch/Fivetran/Airflow, AWS services (e.g., EC2, lambda, kinesis)\nNavigate various data sources and efficiently locate data in a complex data ecosystem.\nWork closely with our data analysts and data scientists to build data models and metrics to support their analytics needs.\nMaintain and support deployed ETL pipelines and ensure data quality.\nDevelop monitoring and alerting systems to provide visibility into the health of data infrastructure, cloud applications, and data pipelines.\nPartner with the IT enterprise applications and engineering teams on integration efforts between systems that impact data & Analytics\n\nRequirements:\n\nBA/BS preferred in a technical or engineering field\n5-7 years of experience in a data engineering or data analyst role.\nStrong understanding of data warehousing concepts, working experience with relational databases (Snowflake, Redshift, Postgres, etc.), and SQL.\nExperience working with cloud providers like AWS, Azure, GCP, etc.\nSolid programming foundations and proficiency in data-related languages like Python, Scala, and R.\nExperience with DevOps workflows and tools like DBT, GitHub, Airflow, etc.\nExperience with an infrastructure-as-code tool such as Terraform or CloudFormation\nExcellent communication skills. Ability to effectively communicate with both technical and non-technical audiences\nKnowledge of real-time stream technologies like AWS Firehose, Spark, etc.\nHighly collaborative in working with teammates and stakeholders\nAWS cloud certification is a plus\n\nThe talent and dedication of our employees are at the core of DataRobot's journey to be an iconic company. We strive to attract and retain the best talent by providing competitive pay and benefits with our employees well-being at the core. Here's what your benefits package may include depending on your location and local legal requirements: Medical, Dental & Vision Insurance, Flexible Time Off Program, Paid Holidays, Paid Parental Leave, Global Employee Assistance Program (EAP) and more!\n\nDataRobot Operating Principles:\n\nWow Our Customers\nSet High Standards\nBe Better Than Yesterday\nBe Rigorous\nAssume Positive Intent\nHave the Tough Conversations\nBe Better Together\nDebate, Decide, Commit\nDeliver Results\nOvercommunicate\n\nResearch shows that many women only apply to jobs when they meet 100% of the qualifications while many men apply to jobs when they meet 60%. At DataRobot we encourage ALL candidates, especially women, people of color, LGBTQ+ identifying people, differently abled, and other people from marginalized groups to apply to our jobs, even if you do not check every box. We'd love to have a conversation with you and see if you might be a great fit.\n\nDataRobot is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. DataRobot is committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities. Please see the United States Department of Labor's EEO poster and EEO poster supplement for additional information.\n\nAll applicant data submitted is handled in accordance with our Applicant Privacy Policy.","Airflow, Stitch, AWS Firehose, R, dbt, snowflake, Fivetran, Github, Cloudformation, Scala, Sql, Terraform, Spark, Python"
Data Engineer,AhinsaAI,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Data Engineer\nLocation: Bengaluru, India (Full-time)\nAt AhinsaAI, we're building a trusted foundation for enterprise AI - governed, ethical, and aligned. Our platform delivers loyal AI solutions designed with consent, transparency, and institutional oversight at their core. We're seeking a Data Engineer to architect scalable, secure, and privacy-preserving data infrastructure that powers our mission across regulated industries.\nKey Responsibilities\nData Architecture: Design and implement modular, high-availability data systems optimized for AI/ML workflows, compliance, and scale.\nPipeline Development: Build and maintain robust ETL/ELT pipelines for structured and unstructured data ensuring integrity, traceability, and minimal latency.\nPrivacy-First Engineering: Embed data anonymization, encryption, and access controls into every layer of the data stack aligned with HIPAA, GDPR, and AI governance frameworks.\nAutomation & Reliability: Drive automation across ingestion, transformation, and validation processes using modern data engineering best practices.\nCross-Functional Collaboration: Partner with AI researchers, compliance officers, and product teams to deliver secure, compliant, and analytics-ready data systems.\nWhat We're Looking For\nEducation: Bachelor's/Master's in Computer Science, Data Engineering, or a related field.\nExperience: 3+ years of experience building data infrastructure in cloud-native, high-scale environments - preferably in fintech, healthtech, or AI startups.\nTech Stack: Expertise in Python and SQL; hands-on experience with Spark, Kafka, Airflow, or dbt; bonus: familiarity with Snowflake, BigQuery, Redshift, or Lakehouse architectures.\nCloud & Infra: Proficient with AWS/GCP/Azure; experience with Terraform, Docker, Kubernetes is a plus.\nMindset: Privacy-aware, systems-driven, and aligned with ethical tech practices. You think in terms of governance, not just performance.\nTeam Fit: Autonomous, reliable, and excited to build future-proof systems that power responsible AI at scale.\nWhy Join AhinsaAI\nBuild Trust-First AI: Design the data foundation for AI solutions rooted in transparency, alignment, and institutional trust.\nGlobal Impact: Your infrastructure will support Fortune 500 clients deploying responsible AI across finance, healthcare, and public sectors.\nFrontier Innovation: Work at the intersection of data engineering, AI alignment, and privacy-enhancing technologies.\nHigh Ownership: Shape core systems with autonomy, purpose, and long-term impact.\nHow to Apply\nIf you're excited to build ethical data infrastructure for the age of intelligent systems - we'd love to hear from you.\nSend your resume, a short note on why you're excited to join AhinsaAI, and any relevant links (GitHub, portfolio, case studies) to [HIDDEN TEXT]\nLearn more: https://Ahinsa.ai","Airflow, Lakehouse architectures, snowflake, dbt, BigQuery, Kafka, Redshift, Sql, Gcp, Docker, Terraform, Spark, Azure, Kubernetes, Python, AWS"
Data Engineer,Knack Consulting Services Pvt Ltd.,7-12 Years,,"Delhi, India",Login to check your skill match score,"Job Title Data Engineer\nMandatory Skills - Azure/ AWS + Data Bricks + Pyspark\nYears Of Experience 7 to 12 Years\nLocation- Hyderabad, Greater Noida, Gurgaon, Pune\nJob Description\nMust to Have Bachelor's degree in Computer Science, Information Technology, or a related field.\n7+ years of experience in data engineering with a focus on Azure cloud services.\nProficiency in Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Azure SQL Database.\nStrong experience with SQL, Python, or other scripting languages.\nFamiliarity with data modelling, ETL design, and big data tools such as Hadoop or Spark.\nExperience with data warehousing concepts, data lakes, and data pipelines.\nUnderstanding of data governance, data quality, and security best practices.\nExcellent problem-solving skills and ability to work in a fast-paced, collaborative environment.\nData factory, data bricks etc The ideal candidate will have extensive experience in data engineering, working with Azure cloud services, and designing and implementing scalable data solutions.\nCandidate will play a crucial role in developing, optimizing, and maintaining data pipelines and architectures, ensuring data quality and availability across various platform.\nDevelop, and maintain data pipelines and ETL processes using Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.\nBuild and optimize data storage solutions using Azure Data Lake, Azure SQL Database, and Azure Cosmos DB.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.\nImplement data quality checks, data governance, and security best practices across data platforms.\nMonitor, troubleshoot, and optimize data workflows for performance and scalability.\nDevelop and maintain data models, data cataloguing, and metadata management.\nAutomate data integration and transformation processes using Azure DevOps and CI/CD pipelines.\nStay up-to-date with emerging Azure technologies and data engineering trends.\nSkill- Good to have Azure certification (e.g., Microsoft Certified: Azure Data Engineer Associate) is a plus. Experience with Azure Logic Apps, Azure Functions, and API Management. Knowledge of Power BI, Tableau, or other data visualization tools","Azure SQL Database, Data Bricks, CI CD pipelines, Pyspark, Hadoop, Power Bi, Azure Databricks, Tableau, Sql, Azure Cosmos DB, Azure Data Factory, Azure Synapse Analytics, Spark, Azure Data Lake, Azure, Python, Azure DevOps, AWS"
Senior Principal Data Engineer,MakeMyTrip,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Senior Principal Data Engineer\nExperience: Must have 10+ years of experience\nAbout Role:\nWe are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.\nAbout us:\nNurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.\nPost-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.\nGO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.\nAbout the team:\nMakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.\nData Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.\nThere are two key responsibilities of Data Engineering team:\nOne to develop the platform for data capture, storage, processing, serving and querying.\nSecond is to develop data products starting from;\no personalization & recommendation platform\no customer segmentation & intelligence\no data insights engine for persuasions and\no the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users\nWe developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.\nTechnology experience:\nExtensive experience working with large data sets with hands-on technology skills to design and build robust data architecture\nExtensive experience in data modeling and database design\nAt least 6+ years of hands-on experience in Spark/BigData Tech stack\nStream processing engines Spark Structured Streaming/Flink\nAnalytical processing on Big Data using Spark\nAt least 6+ years of experience in Scala\nHands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems\nAt least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform\nAt least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc\nAwareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies\nShould have experience in architecting and implementing domain centric big data solutions\nAbility to frame architectural decisions and provide technology leadership & direction\nExcellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Data Engineer,IDFC FIRST Bank,5-10 Years,,"Mumbai, India",Login to check your skill match score,"Job Requirements\n\nJob Requirements\n\nRole/ Job Title: Data Engineer - Gen AI\n\nFunction/ Department: Data & Analytics\n\nPlace of Work: Mumbai\n\nJob Purpose\n\nThe data engineer will be working with our data scientists who are building solutions using generative AI in the domain of text, audio and images and tabular data. They will be responsible for working with large volumes of structured and unstructured data in its storage, retrieval and augmentation with our GenAI solutions which use the said data.\n\nJob & Responsibilities\n\nBuild data engineering pipeline focused on unstructured data pipelines\nConduct requirements gathering and project scoping sessions with subject matter experts, business users, and executive stakeholders to discover and define business data needs in GenAI.\nDesign, build, and optimize the data architecture and extract, transform, and load (ETL) pipelines to make them accessible for Data Scientists and the products built by them.\nWork on end-to-end data lifecycle from Data Ingestion, Data Transformation and Data Consumption layer, versed with API and its usability\nDrive the highest standards in data reliability, data integrity, and data governance, enabling accurate, consistent, and trustworthy data sets\nA suitable candidate will also demonstrate experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, MongoDB, DynamoDB, etc.\nCreating Technical Design Documentation of the projects/pipelines\nGood skills in technical debugging of the code in case of issues. Also, working with git for code versioning\n\nEducation Qualification\n\nGraduation: Bachelor of Science (B.Sc) / Bachelor of Technology (B.Tech) / Bachelor of Computer Applications (BCA)\n\nPost-Graduation: Master of Science (M.Sc) /Master of Technology (M.Tech) / Master of Computer Applications (MCA\n\nExperience Range : 5-10 years of relevant experience","Data Ingestion, Data engineering pipeline, Big data infrastructure, Technical debugging, HDFS, Data Consumption, Technical Design Documentation, Unstructured data pipelines, Dynamodb, Data Architecture, HBase, Yarn, Mapreduce, Git, Hive, MongoDB, Data Transformation"
Senior Azure Data Engineer,Landmark Group,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Designation Azure Data Engineer\n\nLocation Bengaluru, India\n\nTop Skills\n\nCloud & Azure Technologies: Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, and Azure Databricks/Synapse for complex stored procedures\nProgramming & Scripting: Proficiency in Python, SQL (compulsory), and Scala; hands-on experience with PLSQL\nData Expertise: Data modelling, data warehousing, and data integration concepts\nBig Data & Infrastructure: Familiarity with Apache Spark, Hadoop, and cloud infrastructure management\nDevOps & Automation: Experience with Azure DevOps, PowerShell, and automation tools for deployment and monitoring\nData Governance & Security: Robust understanding of data governance practices, data quality management, and security principles\nSoft Skills: Strategic planning, strong communication, collaboration, and problem-solving skills\n\nRoles And Responsibilities\n\nDesign & Implementation: Architect, implement, and optimize end-to-end data solutions on the Azure ecosystem\nETL Pipeline Development: Build and maintain efficient ETL pipelines using Azure Data Factory while ensuring data quality and integration with Azure Data Lake\nStored Procedures & Analytics: Develop and fine-tune complex stored procedures within Azure Databricks or Synapse to enable advanced analytics on large-scale data (exceeding 50TB)\nData Management: Apply your expertise in SQL, PLSQL, and data modelling to manipulate, transform, and analyze data effectively\nCollaboration: Work cross-functionally to support data-driven initiatives; utilize Azure Synapse Analytics to convert raw data into actionable insights\nOptimization & Troubleshooting: Monitor and enhance data storage, processing, and retrieval mechanisms while addressing performance issues\nSecurity & Governance: Enforce and maintain strong data governance, adherence to data security standards, and compliance with privacy regulations\nProcess Automation: Collaborate with DevOps teams to automate deployments, configurations, and monitoring using tools such as Azure DevOps and PowerShell\n\nQualification\n\nEducation: Bachelor's degree in Computer Science, Engineering, or a related field (advanced degree preferred)\nExperience: 68 years of hands-on experience designing and implementing data engineering solutions in the Azure ecosystem\nTechnical Expertise:\nProficiency in developing solutions with Azure Data Factory, Azure Data Lake Storage, Azure Databricks, and Azure Synapse Analytics\nExtensive experience in SQL (compulsory) and PLSQL\nStrong background in data modelling, data warehousing, and data integration concepts\nFamiliarity with big data frameworks such as Apache Spark or Hadoop is required\nCertifications: Relevant Azure certifications (e.g., Azure Data Engineer Associate or Azure Solutions Architect Expert) are preferred\nIndustry Experience: Experience in a retail setup is a plus\n\nEmployee Value Proposition Join the dynamic team at Landmark Digital and play a pivotal role in driving innovation through state-of-the-art data engineering practices. You'll work with the latest Azure technologies to design, deploy, and optimize robust data pipelines that directly influence strategic, data-driven decision-making. This role offers a collaborative environment that fosters continuous learning, career growth, and the opportunity to impact retail transformation on a global scale.\n\nPowered by JazzHR\n\nEBJ9dGj6Jf","Azure Data Lake Storage, Hadoop, PowerShell, Apache Spark, Azure Databricks, Data Warehousing, Sql, Azure Data Factory, Azure Synapse Analytics, Data Integration, Plsql, Data Modelling, Python, Azure DevOps"
DBT Data Engineer,Cognisol,6-8 Years,,"Chennai, India",Login to check your skill match score,"DBT Data Engineer -\n\nKey Skillset-DBT,Python,SQL,AWS,pYSPARK\n\nYears of Exp- 6 to 7 Years\n\nWork Mode-Work From Home(Candidate should be available for 1st week of Onboarding @ Chennai Loc)\n\nShift Time-UK Shift time\n\nNotice: Immediate to 15 days only\n\nPlacement Type: Contractual Position\n\nKey Responsibilities\n\nData Pipeline Development: Design, implement, and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from various sources into cloud platforms like AWS.\nCloud Integration: Utilize AWS services such as Glue, Step Functions, Redshift, and Lambda to process and store data efficiently.\nData Transformation: Develop and maintain accurate data pipelines using Python and SQL, transforming data for aggregations, wrangling, quality control, and calculations.\nWorkflow Automation: Enhance end-to-end workflows with automation tools to accelerate data flow and pipeline management.\nCollaboration: Work closely with business analysts, data scientists, and cross-functional teams to understand data requirements and develop solutions that support data-driven decision-making.\n\nQualifications\n\nEducational Background: Bachelor's or Master's degree in Information Technology, Bioinformatics, Computer Science, or a related field.\nProfessional Experience: Several years of experience in data engineering, with hands-on expertise in:\nDeveloping and managing large-scale ETL data pipelines on AWS.\nProficiency in Python and SQL for data pipeline development.\nUtilizing AWS services such as Glue, Step Functions, Redshift, and Lambda.\nFamiliarity with tools like Docker, Linux Shell Scripting, Pandas, PySpark, and Numpy.\nSoft Skills: Strong problem-solving abilities, excellent communication skills, and the capacity to work collaboratively in a dynamic environment.\nSkills: etl,pipelines,dbt,python,aws,pyspark,pipeline,sql,cloud","Step Functions, dbt, Glue, Pyspark, Shell Scripting, Redshift, Sql, Lambda, Numpy, Pandas, Linux, Docker, Python, Etl, AWS"
Data Engineer MS Fabric_8+years,Zorba AI,8-10 Years,,India,Login to check your skill match score,"Company Overview\n\nZorba Consulting India is a leading consultancy firm focused on delivering innovative solutions and strategies to enhance business performance. With a commitment to excellence, we prioritize collaboration, integrity, and customer-centric values in our operations. Our mission is to empower organizations by transforming data into actionable insights and enabling data-driven decision-making. We are dedicated to fostering a culture of continuous improvement and supporting our team members professional development.\n\nRole Responsibilities\n\nDesign and implement data pipelines using MS Fabric.\nDevelop data models to support business intelligence and analytics.\nManage and optimize ETL processes for data extraction, transformation, and loading.\nCollaborate with cross-functional teams to gather and define data requirements.\nEnsure data quality and integrity in all data processes.\nImplement best practices for data management, storage, and processing.\nConduct performance tuning for data storage and retrieval for enhanced efficiency.\nGenerate and maintain documentation for data architecture and data flow.\nParticipate in troubleshooting data-related issues and implement solutions.\nMonitor and optimize cloud-based solutions for scalability and resource efficiency.\nEvaluate emerging technologies and tools for potential incorporation in projects.\nAssist in designing data governance frameworks and policies.\nProvide technical guidance and support to junior data engineers.\nParticipate in code reviews and ensure adherence to coding standards.\nStay updated with industry trends and best practices in data engineering.\n\nQualifications\n\n8+ years of experience in data engineering roles.\nStrong expertise in MS Fabric and related technologies.\nProficiency in SQL and relational database management systems.\nExperience with data warehousing solutions and data modeling.\nHands-on experience in ETL tools and processes.\nKnowledge of cloud computing platforms (Azure, AWS, GCP).\nFamiliarity with Python or similar programming languages.\nAbility to communicate complex concepts clearly to non-technical stakeholders.\nExperience in implementing data quality measures and data governance.\nStrong problem-solving skills and attention to detail.\nAbility to work independently in a remote environment.\nExperience with data visualization tools is a plus.\nExcellent analytical and organizational skills.\nBachelor's degree in Computer Science, Engineering, or related field.\nExperience in Agile methodologies and project management.\n\nSkills: etl processes,sql,python scripting,data integration,performance tuning,cloud technologies,data quality measures,data quality assurance,data modeling,cloud computing (azure, aws, gcp),python,ms fabric,data governance,databricks,data visualization tools,data warehousing","data visualization tools, MS Fabric, ETL processes, Performance Tuning, Data Warehousing, Data Modeling, Sql, Gcp, Data Governance, Azure, Python, AWS, Cloud Computing"
AWS Data Engineer,"GSPANN Technologies, Inc",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"About GSPANN\nGSPANN is a global IT services and consultancy provider headquartered in Milpitas, California (U.S.A.). With five global delivery centers across the globe, GSPANN provides digital solutions that support the customer buying journeys of B2B and B2C brands worldwide.\nWith a strong focus on innovation and client satisfaction, GSPANN delivers cutting-edge solutions that drive business success and operational excellence. GSPANN helps retail, finance, manufacturing, and high-technology brands deliver competitive customer experiences and increased revenues through our solution delivery, technologies, practices, and operations for each client. For more information, visit www.gspann.com\nJD for your reference:\nGSPANN is looking for AWS Data Engineer. As we march ahead on a tremendous growth trajectory, we seek passionate and talented professionals to join our growing family.\nJob Position-\nAWS Data Engineer\nExperience- 6+ years\nLocation- Bangalore\nSkills- AWS+Redshift+Snowflake, SQL, Bigdata, StepFunction, Python/ PySpark, Airflow\nResponsibilities\nActively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing, roll-out, and support.\nSolve complex business problems by utilizing a disciplined development methodology.\nProduce scalable, flexible, efficient, and supportable solutions using appropriate technologies.\nAnalyse the source and target system data. Map the transformation that meets the requirements.\nInteract with the client and onsite coordinators during different phases of a project.\nDesign and implement product features in collaboration with business and Technology stakeholders.\nAnticipate, identify, and solve issues concerning data management to improve data quality.\nClean, prepare, and optimize data at scale for ingestion and consumption.\nSupport the implementation of new data management projects and re-structure the current data architecture.\nImplement automated workflows and routines using workflow scheduling tools.\nUnderstand and use continuous integration, test-driven development, and production deployment frameworks.\nParticipate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards.\nAnalyze and profile data for the purpose of designing scalable solutions.\nTroubleshoot straightforward data issues and perform root cause analysis to proactively resolve product issues.\nRequired Skills\n6+ years experience developing Data and analytic solutions.\nExperience building data lake solutions leveraging one or more of the following AWS, EMR, S3, Hive & PySpark\nExperience with relational SQL\nExperience with scripting languages such as Python\nExperience with source control tools such as GitHub and related dev process\nExperience with workflow scheduling tools such as Airflow\nIn-depth knowledge of AWS Cloud (S3, EMR, Databricks)\nHas a passion for data solutions.\nHas a strong problem-solving and analytical mindset\nWorking experience in the design, Development, and test of data pipelines.\nExperience working with Agile Teams.\nAble to influence and communicate effectively, both verbally and in writing, with team members and business stakeholders\nAble to quickly pick up new programming languages, technologies, and frameworks.\nBachelor's Degree in computer science\nWhy Choose GSPANN\nAt GSPANN, we don't just serve our clientswe co-create. The GSPANNians are passionate technologists who thrive on solving the toughest business challenges, delivering trailblazing innovations for marquee clients. This collaborative spirit fuels a culture where every individual is encouraged to sharpen their skills, feed their curiosity, and take ownership to learn, experiment, and succeed.\nWe believe in celebrating each other's successesbig or smalland giving back to the communities we call home. If you're ready to push boundaries and be part of a close-knit team that's shaping the future of tech, we invite you to carry forward the baton of innovation with us.\nLet's Co-Create the FutureTogether.\nDiscover Your Inner Technologist\nExplore and expand the boundaries of tech innovation without the fear of failure.\nAccelerate Your Learning\nShape your career while scripting the future of tech. Seize the ample learning opportunities to grow at a rapid pace.\nFeel Included\nAt GSPANN, everyone is welcome. Age, gender, culture, and nationality do not matter here, what matters is YOU.\nInspire and Be Inspired\nWhen you work with the experts, you raise your game. At GSPANN, you're in the company of marquee clients and extremely talented colleagues.\nEnjoy Life\nWe love to celebrate milestones and victories, big or small. Ever so often, we come together as one large GSPANN family.\nGive Back\nTogether, we serve communities. We take steps, small and large so we can do good for the environment, weaving in sustainability and social change in our endeavors.\nWe invite you to carry forward the baton of innovation in technology with us.\nLet's Co-Create\nGSPANN | Consulting Services, Technology Services, and IT Services Provider\nGSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.\nGSPANN | Consulting Services, Technology Services, and IT Services Provider\nGSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.","Airflow, snowflake, StepFunction, Pyspark, Bigdata, Redshift, Python, Sql, AWS"
Sr. Data Engineer - Python Job,YASH Technologies,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"YASH Technologies is a leading technology integrator specializing in helping clients reimagine operating models, enhance competitiveness, optimize costs, foster exceptional stakeholder experiences, and drive business transformation.\n\nAt YASH, we're a cluster of the brightest stars working with cutting-edge technologies. Our purpose is anchored in a single truth bringing real positive changes in an increasingly virtual world and it drives us beyond generational gaps and disruptions of the future.\n\nWe are looking forward to hire AWS Professionals in the following areas :\n\nJob Description\n\nData Engineering (DataEng)\n\nIT would be great if we can find someone with prior data engineering experience in Palantir Foundry. It would be an added advantage if the candidate is familiar with the Workshop component within Palantir Foundry.\n\nThis Position will be Right to Hire\n\nExperience 7+ years\nDegree in computer science, engineering, or similar fields\nSkill Set: AWS, Python, PySpark\n\nPrimary Responsibilities\n\nResponsible for designing, developing, testing and supporting data pipelines and applications\nIndustrialize data feeds\nExperience in working with cloud environments AWS\nCreates data pipelines into existing systems\nExperience with enforcing security controls and best practices to protect sensitive data within AWS data pipelines, including encryption, access controls, and auditing mechanisms.\nImproves data cleansing and facilitates connectivity of data and applied technologies between both external and internal data sources.\nEstablishes a continuous quality improvement process and to systematically optimizes data quality\nTranslates data requirements from data users to ingestion activities\nB.Tech/ B.Sc./M.Sc. in Computer Science or related field and 3+ years of relevant industry experience\nInterest in solving challenging technical problems\nNice to have test driven development and CI/CD workflows\nKnowledge of version control software such as Git and experience in working with major hosting services (e. g. Azure DevOps, Github, Bitbucket, Gitlab)\nNice to have in working with cloud environments such as AWSe especially creating serverless architectures and using infrastructure as code facilities such as CloudFormation/CDK, Terraform, ARM.\nHands-on experience in working with various frontend and backend languages (e.g., Python, R, Java, Scala, C/C++, Rust, Typescript\n\nAt YASH, you are empowered to create a career that will take you to where you want to go while working in an inclusive team environment. We leverage career-oriented skilling models and optimize our collective intelligence aided with technology for continuous learning, unlearning, and relearning at a rapid pace and scale.\n\nOur Hyperlearning workplace is grounded upon four principles\n\nFlexible work arrangements, Free spirit, and emotional positivity\nAgile self-determination, trust, transparency, and open collaboration\nAll Support needed for the realization of business goals,\nStable employment with a great atmosphere and ethical corporate culture","R, Palantir Foundry, Java, Github, Rust, C, Cloudformation, Scala, Pyspark, CDK, Git, Terraform, Bitbucket, Typescript, Gitlab, Arm, Python, Azure DevOps, AWS"
GCP Senior Data Engineer,Xebia,4-6 Years,,India,Login to check your skill match score,"We are looking for a Senior Data Engineer with strong expertise in GCP, Databricks, and Airflow to design and implement a GCP Cloud Native Data Processing Framework. The ideal candidate will work on building scalable data pipelines and help migrate existing workloads to a modern framework.\nShift: 2 PM 11 PM\nWork Mode: Hybrid (3 days a week) across Xebia locations\nNotice Period: Immediate joiners or those with a notice period of up to 30 days\nKey Responsibilities:\nDesign and implement a GCP Native Data Processing Framework leveraging Spark and GCP Cloud Services.\nDevelop and maintain data pipelines using Databricks and Airflow for transforming Raw Silver Gold data layers.\nEnsure data integrity, consistency, and availability across all systems.\nCollaborate with data engineers, analysts, and stakeholders to optimize performance.\nDocument standards and best practices for data engineering workflows.\nRequired Experience:\n4+ years of experience in data engineering, architecture, and pipeline development.\nStrong knowledge of GCP, Databricks, PySpark, and BigQuery.\nExperience with Orchestration tools like Airflow, Dagster, or GCP equivalents.\nUnderstanding of Data Lake table formats (Delta, Iceberg, etc.).\nProficiency in Python for scripting and automation.\nStrong problem-solving skills and collaborative mindset.\nHow to Apply\nIf you're interested, please share your updated CV along with the following details to [HIDDEN TEXT]:\nFull Name:\nTotal Experience:\nCurrent CTC:\nExpected CTC:\nCurrent Location:\nPreferred Location:\nNotice Period / Last Working Day (if serving notice):\nPrimary Skill Set (Choose from above or mention any other relevant expertise):\nPlease apply only if you have not applied recently or are not currently in the interview process for any open roles at Xebia.\nLooking forward to your response!\nBest regards,\nVijay S\nAssistant Manager - TAG\nhttps://www.linkedin.com/in/vijay-selvarajan/","Orchestration tools, Airflow, GCP Cloud Services, BigQuery, Gcp, Pyspark, Spark, Databricks, Python"
Data Engineer II,Tekion Corp,3-5 Years,,"Chennai, India",Login to check your skill match score,"About Tekion:\n\nPositively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.\n\nKey Responsibilities:\n\nBe part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.\nShould be able to design (high level / low level) software solutions for the new requirements.\nCoding independently and with other team members with proper software industry standard best practices.\nCollaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.\n\nQualifications:\n\nB.E/MTech in computer science\n3 - 5 years of relevant work experience.\nExperience in building scalable products with preferably big data.\nExcellent Python coding skills (Mandatory)\nExperience in Apache spark, Data Lake and other Big data technologies.\nExperience in either Data Warehouses or Relational Database is mandatory.\nExperience in AWS cloud\n\nMandatory Skills: Python , Spark\n\nTekion is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, victim of violence or having a family member who is a victim of violence, the intersectionality of two or more protected categories, or other applicable legally protected characteristics.\n\nFor more information on our privacy practices, please refer to our Applicant Privacy Notice here.","data warehouses, Relational Database, Apache Spark, Data Lake, Python, Aws Cloud, Big Data Technologies"
ML Data Engineer,S&P Global,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"About The Role\n\nGrade Level (for internal use):\n\n10\n\nResponsibilities\n\nTo work closely with various stakeholders to collect, clean, model and visualise datasets.\nTo create data driven insights by researching, designing and implementing ML models to deliver insights and implement action-oriented solutions to complex business problems\nTo drive ground-breaking ML technology within the Modelling and Data Science team.\nTo extract hidden value insights and enrich accuracy of the datasets.\nTo leverage technology and automate workflows creating modernized operational processes aligning with the team strategy.\nTo understand, implement, manage, and maintain analytical solutions & techniques independently.\nTo collaborate and coordinate with Data, content and modelling teams and provide analytical assistance of various commodity datasets\nTo drive and maintain high quality processes and delivering projects in collaborative Agile team environments.\n\nRequirements\n\n7+ years of programming experience particularly in Python\n4+ years of experience working with SQL or NoSQL databases.\n1+ years of experience working with Pyspark.\nUniversity degree in Computer Science, Engineering, Mathematics, or related disciplines.\nStrong understanding of big data technologies such as Hadoop, Spark, or Kafka.\nDemonstrated ability to design and implement end-to-end scalable and performant data pipelines.\nExperience with workflow management platforms like Airflow.\nStrong analytical and problem-solving skills.\nAbility to collaborate and communicate effectively with both technical and non-technical stakeholders.\nExperience building solutions and working in the Agile working environment\nExperience working with git or other source control tools\nStrong understanding of Object-Oriented Programming (OOP) principles and design patterns.\nKnowledge of clean code practices and the ability to write well-documented, modular, and reusable code.\nStrong focus on performance optimization and writing efficient, scalable code.\n\nNice To Have\n\nExperience working with Oil, gas and energy markets\nExperience working with BI Visualization applications (e.g. Tableau, Power BI)\nUnderstanding of cloud-based services, preferably AWS\nExperience working with Unified analytics platforms like Databricks\nExperience with deep learning and related toolkits: Tensorflow, PyTorch, Keras, etc.\n\nAbout S&P Global Commodity Insights\n\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nInclusive Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering an inclusive workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and equal opportunity, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_ English_formattedESQA508c.pdf\n\nIFTECH202.1 - Middle Professional Tier I (EEO Job Group)\n\nJob ID: 314321\n\nPosted On: 2025-04-23\n\nLocation: Hyderabad, Telangana, India","Airflow, Object-Oriented Programming, Pyspark, Hadoop, Kafka, Sql, Tensorflow, Nosql, Git, Pytorch, Spark, Keras, Python"
ML Data Engineer,S&P Global,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"About The Role\n\nGrade Level (for internal use):\n\n10\n\nResponsibilities\n\nTo work closely with various stakeholders to collect, clean, model and visualise datasets.\nTo create data driven insights by researching, designing and implementing ML models to deliver insights and implement action-oriented solutions to complex business problems\nTo drive ground-breaking ML technology within the Modelling and Data Science team.\nTo extract hidden value insights and enrich accuracy of the datasets.\nTo leverage technology and automate workflows creating modernized operational processes aligning with the team strategy.\nTo understand, implement, manage, and maintain analytical solutions & techniques independently.\nTo collaborate and coordinate with Data, content and modelling teams and provide analytical assistance of various commodity datasets\nTo drive and maintain high quality processes and delivering projects in collaborative Agile team environments.\n\nRequirements\n\n7+ years of programming experience particularly in Python\n4+ years of experience working with SQL or NoSQL databases.\n1+ years of experience working with Pyspark.\nUniversity degree in Computer Science, Engineering, Mathematics, or related disciplines.\nStrong understanding of big data technologies such as Hadoop, Spark, or Kafka.\nDemonstrated ability to design and implement end-to-end scalable and performant data pipelines.\nExperience with workflow management platforms like Airflow.\nStrong analytical and problem-solving skills.\nAbility to collaborate and communicate effectively with both technical and non-technical stakeholders.\nExperience building solutions and working in the Agile working environment\nExperience working with git or other source control tools\nStrong understanding of Object-Oriented Programming (OOP) principles and design patterns.\nKnowledge of clean code practices and the ability to write well-documented, modular, and reusable code.\nStrong focus on performance optimization and writing efficient, scalable code.\n\nNice To Have\n\nExperience working with Oil, gas and energy markets\nExperience working with BI Visualization applications (e.g. Tableau, Power BI)\nUnderstanding of cloud-based services, preferably AWS\nExperience working with Unified analytics platforms like Databricks\nExperience with deep learning and related toolkits: Tensorflow, PyTorch, Keras, etc.\n\nAbout S&P Global Commodity Insights\n\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nInclusive Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering an inclusive workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and equal opportunity, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_ English_formattedESQA508c.pdf\n\nIFTECH202.1 - Middle Professional Tier I (EEO Job Group)\n\nJob ID: 314321\n\nPosted On: 2025-04-23\n\nLocation: Hyderabad, Telangana, India","Airflow, Pyspark, Hadoop, Kafka, Sql, Tensorflow, Nosql, Git, Pytorch, Spark, Keras, Python"
Principal Data Engineer,Lenity Health,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Role Overview\nLocation: Hebbal, Bengaluru\nWork Mode: Monday to Friday | On-site\nCompensation: 3035 LPA\nExperience Level: Senior-Level (510 years)\nAbout Lenity Health\nThe healthcare industry operates on systems so outdated, that it clogs the flow of innovation and efficient care coordination. These inefficiencies lead to lost opportunities in modernizing care delivery.\nLenity Health is a venture-backed, early stage startup with a deep subject matter expertise in US healthcare, redefining healthcare technology with AI at its core. We aim to coordinate care for all seniors in the United States, using AI agents and data driven care management tools.\nWe take pride in transforming healthcare delivery, improving health outcomes, and streamlining processes to reduce the cost of care. Our solutions are optimized to reduce inefficiencies and ensure patient interventions are made at the right time, for the right reasons.\nIf you are an inquisitive innovator with a passion to learn and make an impact, then Lenity is the place for you.\nPosition Summary\nAs Principal Data Engineer, you will lead the technical vision and implementation of Lenity's data platform for one of its customers. You'll architect and scale the cloud data infrastructure, build robust data pipelines, and ensure high-quality, compliant, and actionable data across all lines of businessclinical, operational, and financial.\nYou'll work closely with the leadership, analysts, clinicians, and product teams to deliver trusted data products that power decisions across the organization. This is a strategic, hands-on role for a senior technologist who thrives in a collaborative, fast-paced environment.\nResponsibilities\nArchitect, build, and optimize Azure-based data platform, integrating data across EHRs, claims systems, and third-party healthcare APIs.\nLead development of ELT workflows using dbt, Airflow, and Azure Data Factory to deliver scalable, testable, and monitored data pipelines.\nDevelop and enforce standards for data modeling, transformation, and governance, with a focus on high performance, maintainability, and transparency.\nPartner with analytics and operations teams to enable rapid insights across population health, provider performance, utilization, and cost-of-care datasets.\nEnsure full compliance with HIPAA, HITECH, and internal security policies, working closely with compliance and security teams.\nGuide integration of structured and unstructured data from EHRs, claims, lab results, FHIR/HL7 feeds, and SDoH data sources.\nImplement and manage tools for data cataloging, lineage, and observability, driving improved trust in enterprise data assets.\nMentor and support a growing team of data engineers, enabling a strong engineering culture rooted in collaboration, accountability, and innovation.\nContribute to enterprise data strategy and roadmap, aligning engineering efforts with business priorities and growth initiatives.\nRequired Qualifications\n510 years of experience in data engineering, with at least 3 years in a lead or principal-level role.\nProven expertise with dbt, Apache Airflow, and Azure Data Factory in production healthcare data environments.\nDeep understanding of SQL, Python, and cloud-native data warehouse platforms such as Snowflake or Azure Synapse.\nDemonstrated experience architecting and scaling data platforms on Azure or similar public cloud infrastructure.\nHands-on experience with healthcare data standards and formats including HL7, FHIR, X12 837/835, and EHR/claims data.\nStrong knowledge of HIPAA compliance, PHI handling, and data governance in regulated healthcare settings.\nExcellent communication and leadership skills with a track record of mentoring data teams and partnering cross-functionally.\nPreferred Qualifications\nExperience working in a Healthcare Organization.\nFamiliarity with Power BI, Looker, or other data visualization tools.\nExposure to real-time or event-driven data architecture (Kafka, Spark Streaming).\nKnowledge of data cataloging tools (e.g., Alation, Microsoft Purview).\nStrong opinions on CI/CD for data pipelines, Git-based workflows, and infrastructure-as-code.\nWhy Choose Lenity Health\nAccelerate Your Career Access certifications, courses, and mentorship to sharpen your skills and grow professionally.\nComprehensive Health Benefits Enjoy full health coverage, including free annual dental and health check-ups, plus unlimited teleconsultations.\nBeyond Insurance We offer not just group health insurance, but also term and accident insurance for added financial security.\nStay Active Get reimbursed for health club memberships to keep your body and mind energized.\nWorkplace Perks Enjoy a complimentary daily meal at the office to keep you fueled throughout the day.\nInnovative Culture Collaborate with some of the brightest minds in health tech, where your ideas truly make an impact.\nReady to Make a Difference\nApply now and be part of a team that's transforming healthcare for millions of seniors across the U.S.\nApply via LinkedIn Easy Apply\nJoin us in reshaping senior healthcare with expertise, compassion, and innovation\n#PrincipalDataEngineer #DataEngineeringJobs #HealthcareTech #AzureDataFactory #dbtJobs #Airflow #HealthcareData #OnsiteJobs #BangaloreTechJobs #AIinHealthcare #JoinLenityHealth #HiringNow #PlatformEngineering #DataPipelineJobs #TechForGood","FHIR, dbt, snowflake, X12, 835, 837, Sql, Apache Airflow, Azure Synapse, Azure Data Factory, Hl7, Python"
Lead AI/Data Engineer,Medtronic,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"At Medtronic you can begin a life-long career of exploration and innovation, while helping champion healthcare access and equity for all. You'll lead with purpose, breaking down barriers to innovation in a more connected, compassionate world.\n\nA Day in the Life\n\nAs a Senior Data Engineer Techno-Manager, where you will lead the design, development, and implementation of scalable data solutions. In this role, you will provide technical leadership (90%) while also mentoring and guiding a team of Data Scientists and Power BI Developers (10%). You will work closely with cross-functional teams to enable data-driven decision-making and support advanced analytics initiatives.\n\nJoin our Global Finance Analytics Center of Excellence (COE) as a Senior Data Engineer Techno-Manager, where you will lead the design, development, and implementation of scalable data solutions. In this role, you will provide technical leadership (90%) while also mentoring and guiding a team of Data Scientists and Power BI Developers (10%). You will work closely with cross-functional teams to enable data-driven decision-making and support advanced analytics initiatives. This role requires an average of 2-3 days per week of overlapping work hours with the USA team to ensure seamless collaboration.\n\nA Day in the Life\n\nAs a Senior Data Engineer Techno-Manager, you will:\n\nLead the end-to-end data engineering efforts, ensuring efficient data pipelines, ETL processes, and data governance practices.\nArchitect and optimize data solutions in Snowflake, Azure, and other cloud platforms for enterprise analytics and AI/ML models.\nCollaborate with Data Scientists to design and implement scalable machine-learning pipelines.\nOversee Power BI development, ensuring efficient data modeling and visualization best practices.\nManage stakeholder expectations while delivering high-quality, reliable, and scalable data solutions.\nMentor and guide junior engineers, fostering best practices in coding, architecture, and data pipeline automation.\nEnsure data integrity, security, and compliance while working with structured and unstructured data sources.\nWork closely with business and IT teams to drive automation, self-service analytics, and cloud-based transformations.\nEngage with USA teams for strategic discussions, project updates, and technical alignment .\n\nAs a People Manager, you will provide leadership, coaching, and career development opportunities to your team members. You will foster a culture of innovation, continuous learning, and collaboration, ensuring that team members have the resources and guidance needed to succeed in their roles. You will also facilitate communication between global teams and ensure that the team is aligned with business objectives.\n\nThis role requires 2-3 hours of overlap with USA teams, typically during early mornings or late evenings, to align with project requirements, attend stakeholder meetings, and ensure smooth collaboration across different time zones.\n\nMust Have: Minimum Requirements\n\nBachelor's or Master's degree in Computer Science, Engineering, Data Science, or related field.\n10+ years of experience in Data Engineering, Big Data, or Cloud Data Technologies.\nStrong expertise in Snowflake, SQL, Python, and ETL processes.\nExperience in Power BI (data modeling, DAX, performance optimization, and visualization best practices).\nCloud experience with Azure, AWS, or GCP, including data lakes, warehousing, and orchestration tools.\nExperience with modern data stack (Databricks, Apache Spark, Airflow, etc.).\nExposure to AI/ML models and working with Data Scientists for productionizing models.\nStrong problem-solving and communication skills with a global mindset.\nAbility to balance technical depth with stakeholder engagement and people management.\n\nNice to Have\n\n\nExperience in Snowflake performance tuning and cost optimization.\nHands-on experience with CI/CD pipelines for data engineering workflows.\nKnowledge of APIs and integration with enterprise applications.\nPrior experience leading small teams or mentoring engineers.\nFamiliarity with SAP, ERP, OneStream or other enterprise data sources.\n\nPhysical Job Requirements\n\n\nThe above statements are intended to describe the general nature and level of work being performed by employees assigned to this position, but they are not an exhaustive list of all the required responsibilities and skills of this position.\n\nBenefits & Compensation\n\nMedtronic offers a competitive Salary and flexible Benefits Package\n\nA commitment to our employees lives at the core of our values. We recognize their contributions. They share in the success they help to create. We offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage.\n\nThis position is eligible for a short-term incentive called the Medtronic Incentive Plan (MIP).\n\nAbout Medtronic\n\nWe lead global healthcare technology and boldly attack the most challenging health problems facing humanity by searching out and finding solutions.\n\nOur Mission to alleviate pain, restore health, and extend life unites a global team of 95,000+ passionate people.\n\nWe are engineers at heart putting ambitious ideas to work to generate real solutions for real people. From the R&D lab, to the factory floor, to the conference room, every one of us experiments, creates, builds, improves and solves. We have the talent, diverse perspectives, and guts to engineer the extraordinary.\n\nLearn more about our business, mission, and our commitment to diversity here","Airflow, snowflake, ETL processes, Power Bi, Apache Spark, Sql, Gcp, Databricks, Dax, Azure, Python, AWS"
Staff Data Engineer,Zinnia,6-8 Years,,"Pune, India",Login to check your skill match score,"Who We Are\n\nZinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.\n\nWho You Are\n\nAs a seasoned Data Engineer specializing in data engineering, you bring extensive expertise in optimizing data workflows using various database tools like Oracle, BigQuery, and SQL Server. You possess a deep understanding of ELT/ETL processes, data integration, and have a strong command of Python for data manipulation and automation tasks. You will possess advanced expertise in working with data platforms like Google Big Query, DBT, Python, and Airflow. Responsible for designing and maintaining scalable ETL pipelines, optimizing complex data systems, and ensuring smooth data flow across different platforms. As a Senior Data Engineer, you will also be required to work collaboratively in a team and contribute to building data infrastructure that drives business insights\n\nWhat You'll Do\n\nDesign, develop, and optimize complex ETL pipelines that integrate large data sets from various sources.\nBuild and maintain high-performance data models using Google BigQuery and DBT for data transformation.\nDevelop Python scripts for data ingestion, transformation, and automation.\nImplement and manage data workflows using Apache Airflow for scheduling and orchestration.\nCollaborate with data scientists, analysts, and other stakeholders to ensure data availability, reliability, and performance.\nTroubleshoot and optimize data systems, identifying issues and resolving them proactively.\nWork on cloud-based platforms, particularly AWS, to leverage scalability and storage options for data pipelines.\nEnsure data integrity, consistency, and security across systems.\nTake ownership of end-to-end data engineering tasks while mentoring junior team members.\nContinuously improve processes and technologies for more efficient data processing and delivery.\nAct as a key contributor to developing and supporting complex data architectures.\n\nWhat You'll Need\n\nBachelor's degree in computer science, Information Technology, or a related field.\n6+ years of hands-on experience in Data Engineering or related fields, with a strong background in building and optimizing data pipelines\nStrong proficiency in Google Big Query, including designing and optimizing queries.\nAdvanced knowledge of DBT for data transformation and model management.\nProficiency in Python for data engineering tasks, including scripting, data manipulation, and automation.\nSolid experience with Apache Airflow for workflow orchestration and task automation.\nExtensive experience in building and maintaining ETL pipelines.\nFamiliarity with cloud platforms, particularly AWS (Amazon Web Services), including tools like S3, Lambda, Redshift, or Glue.\nJava knowledge is a plus.\nExcellent problem-solving and troubleshooting abilities.\nStrong communication and collaboration skills with the ability to work effectively in a team environment.\nSelf-motivated, detail-oriented, and able to work with minimal supervision.\nAbility to manage multiple priorities and deadlines in a fast-paced environment.\nExperience with other cloud platforms (e.g., GCP, Azure) is a plus.\nKnowledge of data warehousing best practices and architecture.\n\nWHAT'S IN IT FOR YOU\n\nAt Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability","dbt, Google BigQuery, Apache Airflow, SQL Server, Oracle, Python, Etl, AWS"
Data Engineer/BI Engineer,canibuild,4-6 Years,,India,Login to check your skill match score,"About Us\n\nCanibuild automates the residential construction industry's design, approval, and sales processes, allowing clients to answer Can I build this on this plot of land instantly. As a fast-growing SaaS platform backed by Australia's largest hedge fund, we serve clients across Australia, New Zealand, Canada, and the US.\n\nRole Overview\n\nWe are seeking a highly skilled Data Engineer / BI Engineer with experience in designing ETL/ELT pipelines, managing databases, and developing dashboards using Power BI. The ideal candidate will be responsible for handling end-to-end data workflows, from data extraction and transformation to visualization and insights generation. This role also provides opportunities to upskill and contribute to AI-driven data initiatives, making it an excellent fit for professionals eager to grow in a modern data ecosystem.\n\nKey Responsibilities\n\nDesign, develop, and maintain ETL/ELT pipelines using Python, Airflow, and SQL in an AWS environment\nManage and optimize data lake and data warehouse solutions on AWS\nDevelop and maintain data-driven dashboards and reports in Power BI, connecting to SQL Server and PostgreSQL Aurora databases\nExtract and integrate data from third-party APIs to populate the data lake\nPerform data profiling and source system analysis to ensure data quality and integrity\nCollaborate with business stakeholders to capture and understand data requirements\nOptimize SQL queries for performance and ensure efficient database operations\nImplement best practices for data engineering, visualization, and database management\nParticipate in architectural decisions and contribute to the continuous improvement of data solutions\nFollow agile and lean development practices for efficient project execution\nIndependently validate and assess the accuracy of data outputs before delivery, ensuring results align with business expectations\nProactively evaluate different technical approaches and suggest alternatives to ensure optimal outcomes within the broader system and business context\nStay updated on the latest AI and data engineering advancements, with opportunities to apply AI-powered solutions\n\nRequirements\n\n\n4+ years of experience in data engineering, ETL/ELT pipeline development, and database management\nStrong expertise in SQL (T-SQL, MS SQL) with a focus on query optimization and database performance tuning\nProficiency in Python (including data-specific libraries such as Pandas, NumPy, etc.) and Airflow for ETL/ELT processes\nExperience extracting and managing data from third-party APIs (REST, JSON)\nProven experience in designing and developing data warehouse solutions on AWS\nStrong expertise in Power BI for data visualization, dashboard creation, and connecting to SQL Server/PostgreSQL Aurora\nFamiliarity with agile methodologies and a continuous improvement mindset\nDemonstrated ability to think critically and evaluate multiple technical solutions in the broader context of system architecture and business goals\nExcellent problem-solving skills and the ability to proactively identify and implement alternative solutions\nStrong communication skills and ability to work collaboratively in a team-oriented environment\nWillingness to upskill in AI-driven data solutions and contribute to AI-powered applications\n\nPreferred Qualifications\n\n\nExposure to AWS cloud data services such as RedShift, Athena, Lambda, Glue, etc\nExperience with additional BI tools like Tableau\nKnowledge of data lake architectures and best practices\nExperience with AI-driven data analytics, or integrating AI models with BI solutions\n\nBenefits\n\n\nWork with a talented team of data and AI professionals on meaningful, industry-transforming projects\nOpportunity to gain hands-on experience with AI technologies and modern data engineering practices\nCompetitive salary, benefits, and opportunities for growth\nA collaborative, fast-paced, and innovative culture that values initiative, ownership, and smart solutions\nFlexible remote work opportunities","Airflow, Aurora, Data Warehouse, T-sql, Power Bi, PostgreSQL, Json, Sql, ELT, MS SQL, REST, Data Lake, Python, Etl"
Senior Microsoft Data Engineer,OneMagnify,Fresher,,"Chennai, India",Login to check your skill match score,"OneMagnify is a global performance marketing organization working at the intersection of brand marketing, technology, and analytics. The Company's core offerings accelerate business, amplify real-time results, and help set their clients apart from their competitors. OneMagnify partners with clients to design, implement and manage marketing and brand strategies using analytical and predictive data models that provide valuable customer insights to drive higher levels of sales conversion.\n\nOneMagnify's commitment to employee growth and development extends far beyond typical approaches. We take great pride in fostering an environment where each of our 700+ colleagues can thrive and achieve their personal best. OneMagnify has been recognized as a Top Workplace, Best Workplace and Cool Workplace in the United States for 10 consecutive years and recently was recognized as a Top Workplace in India.\n\nOneMagnify needs a Senior Snowflake Data Engineer to join our team. We blend brand marketing, technology, and analytics to boost business and client competitiveness.\n\nAt OneMagnify, we are dedicated to encouraging an environment where every individual can thrive and achieve their personal best. We have been consistently recognized as a Top Workplace, Best Workplace, and Cool Workplace in the United States for the past 10 years. Join our team and be a part of our outstanding culture!\n\nLocation: Chennai, India\n\nJoin OneMagnify and be part of a world-class team that competes at the highest level. Apply now and take your career to new heights!\n\nResponsibilities:\n\nDesign and develop scalable and efficient data solutions leveraging both on-premise SQL Server and Azure Data Services.\nCollaborate with cross-functional teams to define, implement, and optimize hybrid data architectures.\nEnsure data performance, security, and integrity across SQL Server, Azure Synapse Analytics, Azure Data Factory, and Azure SQL Database.\nImplement data migration and integration projects between on-premise SQL Server and Azure cloud environments.\nDevelop and maintain ETL pipelines using Azure Data Factory, SQL Server Integration Services (SSIS), Databricks, or Synapse Pipelines.\nProvide technical mentorship and support to junior developers in both on-prem and cloud data environments.\nWork closely with analytics and business intelligence teams to ensure effective data modeling, reporting, and governance across platforms.\n\nRequirements:\n\n\nBachelor's degree in Information Technology, Computer Science, or a related field, or equivalent experience.\nProven experience in a Senior Data Engineer or similar role with expertise in both on-premise SQL Server and Azure cloud technologies.\nStrong proficiency in SQL, database administration, and performance tuning across SQL Server and Azure databases.\nExperience with Azure Synapse Analytics, Azure Data Factory, Azure SQL Database, and SQL Server Integration Services (SSIS).\nFamiliarity with data lakes, structured and unstructured data processing, and hybrid data architectures.\nKnowledge of Azure security standard methodologies and data governance principles.\nStrong problem-solving skills and attention to detail.\nAbility to work in a fast-paced, collaborative environment.\nStrong written and verbal communication skills.\n\nBenefits\n\n\nWe offer a comprehensive benefits package including Medical Insurance, PF, Gratuity, paid holidays, and more.\n\nAbout Us\n\nWhether it's awareness, advocacy, engagement, or efficacy, we move brands forward with work that connects with audiences and delivers results. Through meaningful analytics, engaging communications and innovative technology solutions, we help clients tackle their most ambitious projects and overcome their biggest challenges.\n\nWe are an equal opportunity employer\n\nWe believe that Innovative ideas and solutions start with unique perspectives. That's why we're committed to providing every employee a workplace that's free of discrimination and intolerance. We're proud to be an equal opportunity employer and actively search for like-minded people to join our team","Azure SQL Database, Azure Data Services, ETL pipelines, Azure Data Factory, Azure Synapse Analytics, SQL Server, Data Governance, Databricks, Data Migration, Data Modeling"
Lead Data Engineer,M&G Global Services Private Limited,12-14 Years,,"Mumbai, India",Login to check your skill match score,"We are M&G Global Services Private Limited (formerly known as 10FA India Private Limited, and prior to that Prudential Global Services Private Limited). We are a fully owned subsidiary of the M&G plc group of companies, operating as a Global Capability Centre providing a range of value adding services to the Group since 2003. At M&G our purpose is to give everyone real confidence to put their money to work. As an international savings and investments business with roots stretching back more than 170 years, we offer a range of financial products and services through Asset Management, Life and Wealth. All three operating segments work together to deliver attractive financial outcomes for our clients, and superior shareholder returns.\n\nM&G Global Services has rapidly transformed itself into a powerhouse of capability that is playing an important role in M&G plc's ambition to be the best loved and most successful savings and investments company in the world.\n\nOur diversified service offerings extending from Digital Services (Digital Engineering, AI, Advanced Analytics, RPA, and BI & Insights), Business Transformation, Management Consulting & Strategy, Finance, Actuarial, Quants, Research, Information Technology, Customer Service, Risk & Compliance and Audit provide our people with exciting career growth opportunities. Through our behaviours of telling it like it is, owning it now, and moving it forward together with care and integrity; we are creating an exceptional place to work for exceptional talent.\n\nJob Description\n\nJob Title\n\nLead Data Engineer\n\nGrade\n\n2B\n\nLevel\n\nSenior Manager Data\n\nJob Function\n\nDigital Transformation\n\nJob Sub Function\n\nAzure Data Engineering & DevOps & BI\n\nReports to\n\n3B (VP Data Engineering)\n\nLocation\n\nMumbai\n\nBusiness Area\n\nM&G Global Services\n\nOverall Job Purpose\n\nTo implement data engineering solutions using latest technologies available in Azure Cloud space conforming to the best in class design standard & agreed requirements to achieve business objective\n\nAccountabilities/Responsibilities\n\nLead data engineering projects to build and operationalize data solutions for business using Azure services in combination with custom solutions Azure Data Factory, Azure Data Flows, Azure Databricks, Azure Data Lake Gen 2, Azure SQL etc\nProven experience on leading a team of data engineers providing technical guidance and ensuring alignment with agreed architectural principles\nExperience in migrating on-premise data warehouses to data platforms on AZURE cloud\nDesigning and implementing data engineering, ingestion and transformation functions using ADF, Databricks\nProficient in Py-Spark\nExperience in building Python based APIs on Azure Function Apps\nExperience on Azure Logic apps\nExperience in Lakehouse/Datawarehouse implementation using modern data platform architecture\nCapacity Planning and Performance Tuning on ADF & Databricks pipelines\nSupport data visualization development using Power BI\nExposure across all the SDLC process, including testing and deployment\nExperience in relational and dimensional modelling, including big data technologies\nExperience in Azure DevOps Build CI/CD pipelines for ADF, ADLS, Databricks, Azure SQL DB etc\nExperience of working in secured Azure environments using Azure KeyVaults, Service Principals, and Managed Identities\nGood to have knowledge on Apigee (Googles API Management)\nUnderstanding of data masking, encryption and other practices used in handling sensitive data\nAbility to interact with Business for requirement gathering and query resolutions\nWorking on off shore office based development teams, collaborating within a team environment and participating in typical project lifecycle activities such as requirement analysis, testing and release\nDevelop Azure Data skills within the team through knowledge sharing sessions, articles, etc.\nAdherence to organisations Risk & Controls requirements\nShould have skills for Stakeholder management, process adherence, planning & documentationss\n\nKey Stakeholder Management\n\nInternal\n\nBusiness Teams\n\nProject Manager\n\nArchitects\n\nData Scientists\n\nTeam members\n\nExternal\n\nKnowledge, Skills, Experience & Educational Qualification\n\nKnowledge & Skills:\n\nAzure Data Factory,\nAzure Data Lake Storage V2\nAzure SQL\nAzure DataBricks\nPyspark\nAzure DevOps\nPower BI Report\nTechnical leadership\nConfidence & excellent communication\n\nExperience:\n\nOverall 12+ years of experience\n5 + Experience on Azure data engineering\n5 + experience of managing data deliveries\n\nEducational Qualification:\n\nGraduate/Post-graduate. Preferably with specialisation in Computer Science, Statistics, Mathematics, Data Science, Engineering or related discipline\n\nMicrosoft Azure certification (good to have)\n\nM&G Behaviours relevant to all roles:\n\nInspire Others: support and encourage each other, creating an environment where everyone can contribute and succeed\n\nEmbrace Change: be open to change, willing to be challenged and able to adapt quickly and imaginatively to new ideas\n\nDeliver Results: focus on performance, set high standards and deliver with energy and determination\n\nKeep it simple: cut through complexity, keep the outcome in mind, keeping your approach simple and adapting your message to every audience\n\nWe have a diverse workforce and an inclusive culture at M&G Global Services, regardless of gender, ethnicity, age, sexual orientation, nationality, disability or long term condition, we are looking to attract, promote and retain exceptional people. We also welcome those who take part in military service and those returning from career breaks.","Azure Data Lake Storage V2, Azure Function Apps, Power Bi, Azure Sql, Azure Data Factory, Azure Databricks, Azure Logic Apps, Pyspark, Azure DevOps"
Data Engineer,GeMTech PARAS,2-5 Years,,"Delhi, India",Login to check your skill match score,"We're looking for a hands-on Data Engineer to manage and scale our data scraping pipelines across 60+ websites. The job involves handling OCR-processed PDFs, ensuring data quality, and building robust, self-healing workflows that fuel AI-driven insights.\nYou'll Work On:\nManaging and optimizing Airflow scraping DAGs\nImplementing validation checks, retry logic & error alerts\nCleaning and normalizing OCR text (Tesseract / Textract)\nHandling deduplication, formatting, and missing data\nMaintaining MySQL/PostgreSQL data integrity\nCollaborating with ML engineers on downstream pipelines\nWhat You Bring:\n25 years of hands-on experience in Python data engineering\nExperience with Airflow, Pandas, and OCR tools\nSolid SQL skills and schema design (MySQL/PostgreSQL)\nComfort with CSVs and building ETL pipelines\nBonus: Scrapy or Selenium scraping experience","Airflow, OCR tools, ETL pipelines, Scrapy, Pandas, MySQL, PostgreSQL, Selenium, Python, Sql"
Senior Data Engineer Analyst,IMRIEL Technology Solutions Private Ltd,3-5 Years,,"Pune, India",Login to check your skill match score,"We are seeking a skilled Data Engineer with expertise in maintaining scalable semantic models using AtScale and cloud-based data warehouse platforms.\nWe at IMRIEL (An Allata Company) are looking for experienced and technically strong Analytics Data Engineers to design, build, and maintain scalable semantic models using AtScale and cloud-based data warehouse platforms. This role involves developing logical cubes, defining MDX-based business measures, and enabling governed, self-service BI consumption for enterprise analytics.\nExperience:3 to 5 years.\nLocation:Vadodara & Pune\nWhat you'll be doing:\nDesign and implement robust semantic data models using AtScale that abstract curated datasets into business-consumable layers.\nConduct a comprehensive POC to evaluate three potential semantic layer platforms: AtScale, Microsoft Fabric and Cube.dev. This includes assessing their performance, scalability, and integration with cloud-based platforms like Databricks, Snowflake, etc.\nDevelop and maintain logical cubes with calculated measures, dimension hierarchies, and drill-down paths to support self-service analytics.\nLeverage MDX (Multidimensional Expressions) to define advanced business logic, KPIs, and aggregations aligned with enterprise reporting needs.\nConfigure and manage aggregate tables using AtScale Aggregate Designer, optimizing cube performance and reducing query latency.\nIntegrate semantic models with BI tools such as Power BI, Tableau, and Excel Pivot Tables, ensuring seamless end-user experiences.\nCollaborate with data engineers to align semantic models with curated data sources, transformation views, and data pipelines.\nApply star and snowflake schema design to model fact and dimension tables, ensuring optimal structure for analytical workloads.\nImplement Slowly Changing Dimensions (SCD Types 1 & 2) and maintain historical accuracy in reporting models.\nManage row-level security (RLS) and role-based access control (RBAC) policies within semantic layers for governed data access.\nParticipate in semantic model versioning, CI/CD-based deployments, and technical documentation.\nTroubleshoot semantic layer performance issues using AtScale query logs, plan analysis, and catching strategies.\nWhat you need:\nBasic Skills:\nMinimum 3 years of hands-on experience with AtScale, including building and maintaining semantic models, designing logical cubes, and implementing calculated measures using MDX. Proficiency in AtScale interface, modeling best practices, and performance tuning is essential.\nAdvanced experience in developing and optimizing DAX expressions for complex calculations in Power BI models, with a proven ability to translate these into new semantic layer technologies like AtScale or Cube.dev.\nStrong experience with MDX, including creating calculated members, KPIs, and advanced expressions. Excellent SQL skills with the ability to write complex queries using joins, CTEs, window functions, and performance tuning.\nSolid understanding of dimensional modeling. Ability to design fact/dimension tables using star/snowflake schemas, support SCD logic, and maintain model consistency.\nShould be familiar with the Kimball methodology for dimensional modeling, including concepts like conformed dimensions, fact table granularity, and slowly changing dimensions, to design scalable and analytics-friendly data structures.\nHands-on experience with Snowflake, Redshift, or BigQuery. Familiarity with virtual warehouses, caching, clustering, partitioning, and compute-storage separation.\nExperience implementing RLS and RBAC. Ability to define and enforce granular access controls within semantic models.\nStrong grasp of OLAP concepts like query abstraction, drill-down/roll-up, and cube optimization. Understanding of business logic abstraction from physical data.\nSkilled in using AtScale performance tools such as the Aggregate Designer, log analysis, and query optimization.\nProficient in managing model development lifecycle using Git, automation tools, and collaboration workflows with data/analytics teams.\nStrong verbal and written communication to document models, explain logic, and coordinate with cross-functional teams.\nResponsibilities:\nOwn the design, development, deployment, and maintenance of scalable, governed semantic models.\nImplement complex MDX logic and optimized aggregate strategies to meet performance benchmarks.\nProven ability to design and implement scalable AtScale architectures, including the development of architectural blueprints and data flow diagrams.\nEvaluate and implement the best semantic layer architecture for Power BI by leveraging tools like Microsoft Fabric or other modern BI accelerators to support self-service analytics.\nDefine business measures, hierarchies, and drill-down paths in semantic models aligned with enterprise KPIs.\nAlign semantic layers with upstream data transformations, curated datasets, and data warehouse architecture.\nEnforce governance and security through robust RLS and RBAC implementations.\nContinuously monitor, test, and tune semantic model performance using diagnostic tools and AtScale logging.\nEnsure semantic layer reusability, consistency, and business-aligned metric standardization.\nCollaborate with BI developers and analysts to understand reporting needs and validate model outputs.\nMaintain documentation, data lineage, and business glossaries that support transparency and user adoption.\nContribute to reusable templates, modeling standards, and automation frameworks.\nNice-to-Have to have:\nExperience with AtScale REST APIs for metadata-driven automation and CI/CD pipelines.\nFamiliarity with BI visualization platforms such as Power BI, Tableau, Looker, and Excel OLAP integration.\nScripting experience in Python, Shell, or YAML for configuration management or automation tasks.\nCloud certifications in Snowflake, Databricks, AWS, Azure, or Google Cloud Platform.\nExposure to metadata management, data cataloging, or enterprise data governance tools.\nPersonal Attributes:\nHigh attention to detail with a focus on producing scalable, accurate, and governed semantic solutions.\nStrong interpersonal and communication skills to collaborate effectively with technical and non-technical stakeholders.\nSelf-motivated and accountable, with the ability to take full ownership of deliverables.\nAdaptability to evolving tools, data technologies, and enterprise analytics strategies.","MDX, kimball methodology, snowflake, AtScale, BigQuery, Power Bi, OLAP, Tableau, Redshift, Sql, Git, Excel, Dax"
Lead Snowflake Data Engineer,Ventra Health,7-9 Years,,"Chennai, India",Login to check your skill match score,"Ventra is a leading business solutions provider for facility-based physicians practicing anesthesia, emergency medicine, hospital medicine, pathology, and radiology. Focused on Revenue Cycle Management, Ventra partners with private practices, hospitals, health systems, and ambulatory surgery centers to deliver transparent and data-driven solutions that solve the most complex revenue and reimbursement issues, enabling clinicians to focus on providing outstanding care to their patients and communities.\n\nJob Summary\n\nWe are seeking an experienced Lead Snowflake Data Engineer to join our Data & Analytics team. This role involves designing, implementing, and optimizing Snowflake-based data solutions while providing strategic direction and leadership to a team of junior and mid-level data engineers. The ideal candidate will have deep expertise in Snowflake, cloud data platforms, ETL/ELT processes, and Medallion data architecture best practices. The lead data engineer role has a strong focus on performance optimization, security, scalability, and Snowflake credit control and management. This is a tactical role requiring independent in-depth data analysis and data discovery to understand our existing source systems, fact and dimension data models, and implement an enterprise data warehouse solution in Snowflake.\n\nEssential Functions And Tasks\n\n\nLead the design, development, and maintenance of a scalable Snowflake data solution serving our enterprise data & analytics team.\nArchitect and implement data pipelines, ETL/ELT workflows, and data warehouse solutions using Snowflake and related technologies.\nOptimize Snowflake database performance, storage, and security.\nProvide guidance on Snowflake best practices.\nCollaborate with cross-functional teams of data analysts, business analysts, data scientists, and software engineers, to define and implement data solutions.\nEnsure data quality, integrity, and governance across the organization.\nProvide technical leadership and mentorship to junior and mid-level data engineers.\nTroubleshoot and resolve data-related issues, ensuring high availability and performance of the data platform.\n\nEducation And Experience Requirements\n\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\n7+ years of experience in-depth data engineering, with at least 3+ minimum years of dedicated experience engineering solutions in a Snowflake environment.\nTactical expertise in ANSI SQL, performance tuning, and data modeling techniques.\nStrong experience with cloud platforms (preference to Azure) and their data services.\nProficiency in ETL/ELT development using tools such as Azure Data Factory, dbt, Matillion, Talend, or Fivetran.\nHands-on experience with scripting languages like Python for data processing.\nStrong understanding of data governance, security, and compliance best practices.\nSnowflake SnowPro certification; preference to the engineering course path.\nExperience with CI/CD pipelines, DevOps practices, and Infrastructure as Code (IaC).\nKnowledge of streaming data processing frameworks such as Apache Kafka or Spark Streaming.\nFamiliarity with BI and visualization tools such as PowerBI.\n\nKnowledge, Skills, And Abilities\n\n\nFamiliarity working in an agile scum team, including sprint planning, daily stand-ups, backlog grooming, and retrospectives.\nAbility to self-manage large complex deliverables and document user stories and tasks through Azure Dev Ops.\nPersonal accountability to committed sprint user stories and tasks.\nStrong analytical and problem-solving skills with the ability to handle complex data challenges.\nAbility to read, understand, and apply state/federal laws, regulations, and policies.\nAbility to communicate with diverse personalities in a tactful, mature, and professional manner.\nAbility to remain flexible and work within a collaborative and fast paced environment.\nUnderstand and comply with company policies and procedures.\nStrong oral, written, and interpersonal communication skills.\nStrong time management and organizational skills.\n\nVentra Health\n\n\nEqual Employment Opportunity (Applicable only in the US)\n\nVentra Health is an equal opportunity employer committed to fostering a culturally diverse organization. We strive for inclusiveness and a workplace where mutual respect is paramount. We encourage applications from a diverse pool of candidates, and all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, or veteran status. We will provide reasonable accommodations to qualified individuals with disabilities, as needed, to assist them in performing essential job functions.\n\nRecruitment Agencies\n\nVentra Health does not accept unsolicited agency resumes. Ventra Health is not responsible for any fees related to unsolicited resumes.\n\nSolicitation of Payment\n\nVentra Health does not solicit payment from our applicants and candidates for consideration or placement.\n\nAttention Candidates\n\nPlease be aware that there have been reports of individuals falsely claiming to represent Ventra Health or one of our affiliated entities Ventra Health Private Limited and Ventra Health Global Services. These scammers may attempt to conduct fake interviews, solicit personal information, and, in some cases, have sent fraudulent offer letters.\n\nTo protect yourself, verify any communication you receive by contacting us directly through our official channels. If you have any doubts, please contact us at [HIDDEN TEXT] to confirm the legitimacy of the offer and the person who contacted you. All legitimate roles are posted on https://ventrahealth.com/careers/.\n\nStatement of Accessibility\n\nVentra Health is committed to making our digital experiences accessible to all users, regardless of ability or assistive technology preferences. We continually work to enhance the user experience through ongoing improvements and adherence to accessibility standards. Please review at https://ventrahealth.com/statement-of-accessibility/.","DevOps practices, Matillion, dbt, snowflake, Fivetran, ELT, Spark Streaming, Azure Data Factory, Powerbi, Apache Kafka, Data Governance, Ansi Sql, Talend, Python, Etl"
Data Engineer,Prachodayath Global Services Private Limited,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"Data Engineering is the core team for all decision making systems and responsible for creating bespoke data processing workflows to enable Mistplay AI products and advanced analytics. You will become a part of our core data engineering function focusing on driving operational excellence across all data stacks. You will work closely with our ML and Analytics teams to modernize our data platform and continuous innovation on data features. You will provide the technical ownership to help drive continuous improvement to our current data processing workflows and new lakehouse architecture.\n\nRole :- Senior Data Engineer\nExperience -7+\nWork Mode - Onsite\nFull-time, Permanent Role\nBudget - 11-12 LPA\n\nAs a Senior Data Engineer II you will be working closely with engineering, operations, and product to deploy new applications on our data lakehouse, refactoring legacy elements and design new features/data pipelines. This position requires someone who is passionate about data architecture design, big data technologies, deep technical proficiency in distributed data processing, real-time streaming, a strong problem-solver, a team collaborator and has a growth mindset.\n\nWhat You'll Do\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nCollaborate with development, AI, and data science teams to integrate data solutions into complex enterprise systems, ensuring seamless interoperability with existing platforms.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nAnalyze and interpret trends in complex data sets, utilizing visualization tools (e.g., Tableau, Power BI) to create dashboards and reports that tell compelling data stories.\nCreate and maintain standardized operational tools and reporting mechanisms to communicate data health and business performance to various audiences, including executives.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nAutomate data analyses and authoring pipelines using tools such as Kinesis, Airflow, Lambda, Databricks, DBT, and other relevant technologies.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\nWhat You'll Bring\nBachelor's degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nInterested candidates share your resume at [HIDDEN TEXT]","Airflow, Beam, dbt, Java, Apache Flink, Scala, Redis, Lambda, Git, Kinesis, Terraform, Spark, Databricks, Python, Kubernetes"
DBT Data Engineer - Remote,Cognisol,6-8 Years,,"Chennai, India",Login to check your skill match score,"Key Skillset-DBT,Python,SQL,AWS,pYSPARK\n\nYears of Exp- 6 to 7 Years\n\nWork Mode-Work From Home(Candidate should be available for 1st week of Onboarding @ Chennai Loc)\n\nShift Time-UK Shift time\n\nNotice: Immediate to 15 days only\n\nPlacement Type: Contractual Position\n\nKey Responsibilities\n\nData Pipeline Development: Design, implement, and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from various sources into cloud platforms like AWS.\nCloud Integration: Utilize AWS services such as Glue, Step Functions, Redshift, and Lambda to process and store data efficiently.\nData Transformation: Develop and maintain accurate data pipelines using Python and SQL, transforming data for aggregations, wrangling, quality control, and calculations.\nWorkflow Automation: Enhance end-to-end workflows with automation tools to accelerate data flow and pipeline management.\nCollaboration: Work closely with business analysts, data scientists, and cross-functional teams to understand data requirements and develop solutions that support data-driven decision-making.\n\nQualifications\n\nEducational Background: Bachelor's or Master's degree in Information Technology, Bioinformatics, Computer Science, or a related field.\nProfessional Experience: Several years of experience in data engineering, with hands-on expertise in:\nDeveloping and managing large-scale ETL data pipelines on AWS.\nProficiency in Python and SQL for data pipeline development.\nUtilizing AWS services such as Glue, Step Functions, Redshift, and Lambda.\nFamiliarity with tools like Docker, Linux Shell Scripting, Pandas, PySpark, and Numpy.\nSoft Skills: Strong problem-solving abilities, excellent communication skills, and the capacity to work collaboratively in a dynamic environment.\nSkills: etl,linux shell scripting,pyspark,docker,pipelines,sql,aws,redshift,glue,pipeline,cloud,python,lambda,dbt,step functions,pandas,numpy","dbt, Glue, Pyspark, Shell Scripting, Redshift, Sql, Lambda, Numpy, Pandas, Linux, Docker, Python, AWS"
Data Engineer-Specialized-Associate - Operate,PwC Acceleration Centers in India,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services.\n\nKEY SKILLS - Mainframe, Teradata Datastage\n\nMainframe and Teradata DataStage Associate\n\nSummary:.\n\nMinimum Degree Required: Bachelor's degree in computer science/IT or relevant field\n\nDegree Preferred: Master's degree in computer science/IT or relevant field\n\nMinimum Years of Experience: 6 - 8 year(s)\n\nCertifications Required: NA\n\nRequired Knowledge/Skills: (character count limit 5000) *PLEASE ONLY USE THIS FIELD IF THIS IS A MUST HAVE SKILL FOR APPLICANT*\n\nJob Summary\n\nWe are seeking a skilled and experienced IT professional to join our team as a Mainframe and Teradata DataStage Associate. The successful candidate will be responsible for developing, maintaining, and optimizing ETL processes using IBM DataStage, as well as managing and supporting data operations on Mainframe and Teradata platforms.\n\nKey Responsibilities\n\nDesign, develop, and implement ETL processes using IBM DataStage to support data integration and transformation requirements.\nManage and maintain data on Mainframe and Teradata systems, ensuring data integrity and performance optimization.\nCollaborate with business analysts and stakeholders to understand data requirements and translate them into technical specifications.\nTroubleshoot and resolve issues related to ETL processes and data management on Mainframe and Teradata platforms.\nMonitor and tune the performance of ETL jobs and database queries to ensure optimal performance.\nDevelop and maintain documentation related to ETL processes, data flows, and system configurations.\nParticipate in code reviews and ensure adherence to best practices and coding standards.\nProvide support for data migration and integration projects, ensuring timely and accurate data delivery.\nStay updated with the latest developments in Mainframe, Teradata, and DataStage technologies and recommend improvements.\n\nQualifications\n\nJob Summary -\n\nA career in our Managed Services team will provide you with an opportunity to collaborate with a wide array of teams to help our clients implement and operate new capabilities, achieve operational efficiencies, and harness the power of technology. Our Data, Testing & Analytics as a Service team brings a unique combination of industry expertise, technology, data management and managed services experience to create sustained outcomes for our clients and improve business performance. We empower companies to transform their approach to analytics and insights while building your skills in exciting new directions. Have a voice at our table to help design, build and operate the next generation of software and services that manage interactions across all aspects of the value chain.\n\nMinimum Degree Required (BQ) *:\n\nBachelor's degree\n\nDegree Preferred\n\nRequired Field(s) of Study (BQ):\n\nPreferred Field(s) Of Study\n\nComputer and Information Science, Management Information Systems\n\nMinimum Year(s) of Experience (BQ) *: US\n\nCertification(s) Preferred\n\nMinimum of 1 year of experience\n\nPreferred Skills (PQs)\n\nPosition Requirements:\n\nDatasphere\n\nRequired Skills:\n\nMore than 2 years of hands-on experience in SAP Datasphere / DWC at least 1 full life cycle project implementation.\nWork on development/maintenance of DWC Models, CDS Views, SQL Scripts SAC Stories\nShould have experience in building complex models in SAP Datasphere/ DWC\nDeveloping SAP Datasphere end-to-end Dataflows Design, build data flows, and develop chains to load and monitor Data Loading.\nKnowledge in setting up the connections to Datasphere and from Datasphere.\nKnowledge in handling the delta in Datasphere.\nUnit testing the dataflows and reconciling the data to Source Systems.\nGood exposure in troubleshooting data issues and provide workarounds in cases where there are product limitations.\nGood exposure with Datasphere security setup, currency conversion.\nGood knowledge in writing CDS Analytical Queries and S4HANA Embedded Analytics.\nGood exposure in performance tuning of the models in the datasphere.\nGood knowledge on Datasphere and Data Lake integration.\nGood Knowledge on using the Database explorer and SAP Hana Cockpit through Datasphere.\n\nNice To Have\n\nGood knowledge in either BW Modeling or HANA Modeling.\nBW/4HANA And/or Native HANA (or HANA Cloud) modeling, including SQL Scripting, Graphical View-Modelling, SDA extraction.","DWC, SAP Datasphere, SAC Stories, Teradata, DataStage, Cds Views, Mainframe, Sql"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position\n\nThis role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.\n\nKey Accountabilities\n\nAssist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.\nWork closely with senior engineers and analysts to gather requirements and build working prototypes.\nSupport data integration from internal, third-party, and public sources.\nParticipate in developing and maintaining Data Warehouse schemas.\nContribute to documentation and testing efforts to ensure data reliability.\nLearn and apply data standards and governance practices as guided by the team.\n\nSkills and Experience | Essential\n\nKnowledge of data engineering concepts and data structures.\nExposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.\nUnderstanding of ETL processes and data pipelines.\nAbility to work collaboratively in an Agile/Kanban team environment.\nMicrosoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage\n\nSkills and Experience | Desirable\n\nFamiliarity with Medallion Architecture principles.\nExposure to MS Purview or other data governance tools.\nUnderstanding of data warehousing and reporting concepts.\nInterest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
Senior Data Engineer (SQL and ADF),thinkbridge,5-7 Years,,India,Login to check your skill match score,"We are hiring a Senior Data Engineer with 5+ years of experience in SQL , who has good hands-on in ADF and ETL. Good communication is mandatory.\nAbout thinkbridge\nthinkbridge is how growth-stage companies can finally turn into tech disruptors. They get a new way there with world-class technology strategy, development, maintenance, and data science all in one place. But solving technology problems like these involves a lot more than code. That's why we encourage thinkers to spend 80% of their time thinking through solutions and 20% coding them. With an average client tenure of 4+ years, you won't be hopping from project to project here unless you want to. So, you really can get to know your clients and understand their challenges on a deeper level. At thinkbridge, you can expand your knowledge during work hours specifically reserved for learning. Or even transition to a completely different role in the organization. It's all about challenging yourself while you challenge small thinking.\nthinkbridge is a place where you can:\nThink bigger because you have the time, opportunity, and support it takes to dig deeper and tackle larger issues.\nMove faster because you'll be working with experienced, helpful teams who can guide you through challenges, quickly resolve issues, and show you new ways to get things done.\nGo further because you have the opportunity to grow professionally, add new skills, and take on new responsibilities in an organization that takes a long-term view of every relationship.\nthinkbridge.. there's a new way there.\nWhat is expected of you\nAs part of the job, you will be required to\nRead everything in detail that comes your way.\nElicit, analyze, specify & validate business requirements.\nDefine & Document the engineering requirements including architecture, components, usable interfaces & other characteristics.\nPlace specific emphasis on technical and usability design.\nCode and verify the solution.\nDebug & squash bugs.\nKeep stakeholders informed.\nKeep up with the technology and the domain.\nIf your beliefs resonate with these, you are looking at the right place!\nAccountability Finish what you started.\nCommunication Context-aware, pro-active and clean communication.\nOutcome High throughput.\nQuality High-quality work and consistency.\nOwnership Go beyond.\nRequirements\nMust-Have:\nShould have hands-on experience in writing SQL Queries and Stored Procedures.\nExcellent Communication.\nShould have experience in ADF (Azure Data Factory)\nGood experience in building ETL Solutions for large datasets\nGood to have:\nGood to have SSIS Experience\nOther Details :\nRemote First\nFlexible work hours\nNo loss of pay for pre-approved leaves\nFamily Insurance\nQuarterly in-person Collaboration Week -WWW","Adf, SSIS, Sql, Etl"
Senior Data Engineer,Commonwealth Bank,10-13 Years,,"Bengaluru, India",Login to check your skill match score,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\n\nJob Title: Sr Data Engineering\n\nLocation: Bangalore\n\nBusiness & Team:\n\nTechnology Team is responsible for the world leading application of technology and operations across every aspect of CommBank, from innovative product platforms for our customers to essential tools within our business. We also use technology to drive efficient and timely processing, an essential component of great customer service.\n\nCommBank is recognised as leading the industry in IT and operations with its world-class platforms and processes, agile IT infrastructure, and innovation in everything from payments to internet banking and mobile apps.\n\nThe Group Security (GS) team protects the Bank and our customers from cyber compromise, through proactive management of cyber security, privacy, and operational risk. Our team includes:\n\nCyber Strategy & Performance\nCyber Security Centre\nCyber Protection & Design\nCyber Delivery\nCyber Data Engineering\nCyber Data Security\nIdentity & Access Technology\n\nThe Group Security Senior Data Engineering team provides specialised data services and platforms for the CommBank group & is accountable for developing Group's data strategy, data policy & standards, governance and set requirements for data enablers/tools. The team is also accountable to facilitate a community of practitioners to share best practice and build data talent and capabilities.\n\nImpact & contribution :-\n\nTo ensure the Group achieves a sustainable competitive advantage through data engineering, you will play a key role in supporting and executing the Group's data strategy.\n\nWe are looking for an experienced Data Engineer to join our Group Security Team, which is part of the wider Cyber Security Engineering practice. In this role, you will be responsible for setting up the Group Security Data Platform to ingest data from various organizations security telemetry data, along with additional data assets and data products. This platform will provide security controls and services leveraged across the Group.\n\nRoles & Responsibilities\n\nYou will be expected to perform the following tasks in a manner consistent with CBA's Values and People Capabilities.\n\nCORE RESPONSIBILITIES:\n\nPossesses hands-on technical experience working in AWS. The individual should have knowledge about AWS services like EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, KMS, Step functions, SQS,SNS, Cloud Watch.\nThe individual should possess a robust set of technical and soft skills and be an excellent AWS Data Engineer with a focus on complex Automation and Engineering Framework development.\nBeing well-versed in Python is mandatory, and experience in developing complex frameworks using Python is required.\nPassionate about Cloud/DevSecOps/Automation and possess a keen interest in solving complex problems systematically.\nDrive the development and implementation of scalable data solutions and data pipelines using various AWS services.\nPossess the ability to work independently and collaborate closely with team members and technology leads.\nExhibit a proactive approach, constantly seeking innovative solutions to complex technical challenges.\nCan take responsibility for nominated technical assets related to areas of expertise, including roadmaps and technical direction.\nCan own and develop technical strategy, overseeing medium to complex engineering initiatives.\n\nEssential Skills:-\n\nAbout 10-13 years of experience as a Data Engineering professional in a data-intensive environment.\nThe individual should have strong analytical and reasoning skills in the relevant area.\nProficiency in AWS cloud services, specifically EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, Step functions, SQS,SNS, Cloud Watch.\nExcellent skills in Python-based framework development are mandatory.\nProficiency in SQL for efficient querying, managing databases, handling complex queries, and optimizing query performance.\nExcellent automation skills are expected in areas such as\nAutomating the testing framework using tools such as PyPy, Pytest, and various test cases including unit, integration, functional tests, and mockups.\nAutomating the data pipeline and expediting tasks such as data ingestion and transformation.\nAPI-based automated and integrated calls(REST, cURL, authentication & authorization, tokens, pagination, openApi, Swagger)\nImplementing advanced engineering techniques and handling ad hoc requests to automate processes on demand.\nImplementing automated and secured file transfer protocols like XCOM, FTP, SFTP, and HTTP/S\nExperience with Terraform, Jenkins, Teracity and Artifactory is essential as part of DevOps. Additionally, Docker and Kubernetes are also considered.\nProficiency in building orchestration workflows using Apache Airflow.\nStrong understanding of streaming data processing concepts, including event-driven architectures.\nFamiliarity with CI/CD pipeline development, such as Jenkins.\nExtensive experience and understanding in Data Modelling, SCD Types, Data Warehousing, and ETL processes.\nExcellent experience with GitHub or any preferred version control systems.\nExpertise in data pipeline development using various data formats/types.\nMandatory knowledge and experience in big data processing using PySpark/Spark and performance optimizations of applications\nProficiency in handling various file formats (CSV, JSON, XML, Parquet, Avro, and ORC) and automating processes in the big data environment.\nAbility to use Linux/Unix environments for development and testing.\nShould be aware of security best practices to protect data and infrastructure, including encryption, tokenization, masking, firewalls, and security zones.\nWell-structured documentation skills and the ability to create a well-defined knowledge base.\nCertifications such as AWS Certified Data Analytics/Engineer/Developer Specialty or AWS Certified Solutions Architect.\nShould be able to perform extreme engineering and design a robust, efficient, and cost-effective data engineering pipelines which are highly available and dynamically scalable on demand.\nEnable the systems to effectively respond to high demands and heavy loads maintaining the high throughput and high I/O performance with no data loss\nOwn and lead E2E Data engineering life cycle right from Requirement gathering, design, develop, test, deliver and support as part of DevSecOPS process.\nMust demonstrate skills and mindset to implement encryption methodologies like SSL/TLS and data encryption at rest and in transit and other data security best practices\nHands on work experience with data design tools like Erwin and demonstrate the capabilities of building data models, data warehouse, data lakes, data assets and data products\nMust be able to constructively challenge the status quo and lead to establish data governance, metadata management, ask the right questions, design with right principles\n\nEducation Qualification :-\n\nA Bachelor's or Master's degree in Engineering, specializing in Computer Science, Information Technology or relevant qualifications.\n\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through Sidekick to submit a valid application. We're keen to support you with the next step in your career.\n\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\n\nAdvertising End Date: 23/05/2025","Big Data Processing, Data Pipeline Development, Github, Data Modelling, Pyspark, Automation, Sql, Apache Airflow, Jenkins, Terraform, Docker, Data Warehousing, Kubernetes, Python, Etl, AWS"
Data Engineer - Mastery,Technogen India Pvt. Ltd.,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Note: More details below for DE, Investments Domain role.\n\nDomain Key words : Capital Markets, Investment Banking\n\nTechnical Key words: Informatica, batch/shell scripting, data warehousing , SQL, ETL, Python (basic to intermediate)\n\nObjectives of the role\n\nDesign, build and maintain complex ELT jobs that deliver business value\nTranslate high-level business requirements into technical specs\nIngest data from disparate sources into the data lake and data warehouse\nCleanse and enrich data and apply adequate data quality controls\nProvide insight and direction to guide the future development of organization's data platform\nDevelop re-usable tools to help streamline the delivery of new projects\nCollaborate closely with other developers and provide mentorship\nEvaluate and recommend tools, technologies, processes and reference architectures\nWork in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements\n\nBasic Qualifications\n\nBachelor's degree in computer science, engineering or a related field\nData: 8+ years of experience with data analytics and warehousing in Investment & Finance Domain\nSQL: Deep knowledge of SQL and query optimization\nELT: Good understanding of ELT methodologies and tools\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers\n3+ years of coding and scripting (Python, Java, Scala) and design experience.\n3+ years of experience with Spark framework.\nExperience with Vertica or any Columnar Databases.\nStrong data integrity, analytical and multitasking skills.\n\nPreferred Qualifications\n\nMaster's degree in computer science or engineering or a related field\nCloud: Experience working in a cloud environment (e.g. AWS)\nPython: Hands on experience developing with Python\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nWorkflow: Good knowledge of orchestration and scheduling tools (e.g. Apache Airflow)\nReporting: Experience with data reporting (e.g. Microstrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)","Alation, Looker, Microstrategy, Data Warehousing, Apache Spark, Kafka, Tableau, Informatica, Sql, ELT, Apache Airflow, shell scripting, Spark, Vertica, Python, Etl"
Big Data Engineer (Freelancer),Soul AI,4-6 Years,,India,Login to check your skill match score,"About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact.\nWe are looking for an experienced Big Data Engineer with at least 4 years of experience to design, develop, and manage large-scale data processing systems.\nKey Responsibilities:\nBuild and maintain data pipelines for large datasets.\nDesign systems for real-time data processing.\nCollaborate with data scientists and engineers to optimize data workflows.\nRequired Qualifications:\n4+ years of experience as a Big Data Engineer.\nStrong proficiency with big data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.\nExperience with cloud platforms like AWS, GCP, or Azure.\nWhy Join Us\nCompetitive pay (1200/hour).\nFlexible hours.\nRemote opportunity.\nNOTE: Pay will vary by project and typically is up to Rs. 1200 per hour (if you work an average of 3 hours every day - that could be as high as Rs. 108K per month) once you clear our screening process.\nShape the future of AI with Soul AI!","NoSQL databases, Gcp, Hadoop, Spark, Kafka, Azure, AWS"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description\nOncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.\nJob Type:Full-time or Contract\nStart Date: June 1st 2025\nRole Description\nThis is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.\nJob Description:\nCandidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively\n6+ Years of experience as a Data Engineer\nStrong technical expertise in SQL and Snowflake is a must.\nStrong knowledge of joins and common table expressions (CTEs)\nStrong experience with Python\nStrong expertise in ETL process and with various data model concepts\nKnowledge of star schema and snowflake schema\nGood to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue\nExperience with Big Data Tools and technologies\nKey Skills:\nGood Understanding of data structures and data analysis using SQL\nKnowledge of implementing ETL/ELT for data solutions end-to-end\nUnderstanding requirements, and data solutions (ingest, storage, integration, processing)\nKnowledge of analyzing data using SQL\nConducting End to End verification and validation for the entire application\nResponsibilities:\nUnderstand and translate business needs into data models supporting long-term solutions.\nPerform reverse engineering of physical data models from databases and SQL scripts.\nAnalyze data-related system integration challenges and propose appropriate solutions.\nAssist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
Senior Data Engineer,Ninja Van,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"About the Role:\nWe are looking for a highly skilled Senior Data Engineer to join our growing team. The ideal\ncandidate will have over 6 years of experience in building and managing data systems, with\nexpertise in Python, Spark, and Airflow. This role offers an exciting opportunity to work in a\nproduct-driven environment and make a significant impact on our data architecture and pipeline.\nKey Responsibilities:\nDesign, implement, and maintain scalable data pipelines and systems.\nWork with Python, Spark, and Airflow to build robust and reliable data solutions.\nEnsure the integrity and governance of code, focusing on best practices for deployment\nand testing.\nHandle database management with hands-on experience in MySQL, PostgreSQL,\nDynamoDB, and MongoDB.\nCollaborate with cross-functional teams to improve cloud-based data architectures,\nprimarily focusing on GCP, AWS, Oracle OCI, and Huawei Cloud.\nImplement Change Data Capture (CDC) mechanisms and work on continuous data\nintegration and delivery.\nMentor and guide junior engineers, promoting best practices and knowledge sharing.\nSupport performance tuning, scalability, and system optimization efforts across the data\nstack.\nKey Requirements:\nExperience: 6+ years with significant hands-on experience in Python, Spark, and\nAirflow.\nDatabases: Proficiency in MySQL, PostgreSQL, DynamoDB, and MongoDB.\nCloud: Expertise with at least two of the following: GCP, AWS, Oracle OCI, or Huawei\nCloud.\nCDC & Code Governance: Strong understanding and implementation of Change Data\nCapture (CDC) and robust code governance.\nMentorship: Proven experience mentoring junior engineers, ensuring growth and\ndevelopment within the team.\nTenure: Minimum 3 years in the current/previous company to ensure deep technical\nexpertise.\nPreferred Experience: Candidates from product-based companies with proven impact\nin their current role.","Airflow, Oracle OCI, Huawei Cloud, Gcp, MySQL, PostgreSQL, Spark, Dynamodb, MongoDB, Python, AWS"
Data Engineer,Aditi Consulting,5-8 Years,,"Bengaluru, India",Login to check your skill match score,"We are hiring Data Engineer for Bangalore Location\nLocation: Bangalore\nExperience : 5 8 Yrs\nWorking Model : Hybrid\nEXPERIENCE:\n5 - 8 years preferred experience in a data engineering role.\nMinimum of 4 years of preferred experience in Azure data services (Data Factory, Databricks, ADLS, SQL DB, etc.)\nEDUCATION:\nMinimum Bachelor's Degree in Computer Science, Computer Engineering or in STEM Majors (Science, Technology, Engineering, and\nMath)\nSKILLS/REQUIREMENTS:\nStrong working knowledge of Databricks, ADF.\nExpertise working with databases and SQL.\nStrong working knowledge of code management and continuous integrations systems (Azure DevOps or Github)\npreferred\nFamiliarity with Agile delivery methodologies\nFamiliarity with NoSQL databases (such as MongoDB) preferred.\nAny experience on IoT Data Standards like Project Haystack, Brick Schema, Real Estate Core is an added advantage\nAbility to multi-task and reprioritize in a dynamic environment.\nOutstanding written and verbal communication skills","Sql, Databricks, Github, Azure DevOps"
Sr Data Bricks Data Engineer,Rainier Softech Solutions Pvt Ltd,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"Required Skills\n6+ years as a data engineer with any data modelling tool (ADF ,Snowflake ,Google Big query, AWS Redshift , Data Bricks )\nExperience in python and cloud analytics services.\n2 years of experience in SQL and Data Bricks\nFamiliarity with any cloud DevOps & database CI/CD.\nStong in advanced SQL concepts.\nStrong in ETL and Data Modelling concepts.\nGood communication skills","Cloud analytics services, snowflake, Data Bricks, Advanced SQL concepts, Sql, Etl, Aws Redshift, Python, Adf, Data Modelling, Cloud Devops"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position\n\nThis role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.\n\nKey Accountabilities\n\nAssist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.\nWork closely with senior engineers and analysts to gather requirements and build working prototypes.\nSupport data integration from internal, third-party, and public sources.\nParticipate in developing and maintaining Data Warehouse schemas.\nContribute to documentation and testing efforts to ensure data reliability.\nLearn and apply data standards and governance practices as guided by the team.\n\nSkills and Experience | Essential\n\nKnowledge of data engineering concepts and data structures.\nExposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.\nUnderstanding of ETL processes and data pipelines.\nAbility to work collaboratively in an Agile/Kanban team environment.\nMicrosoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage\n\nSkills and Experience | Desirable\n\nFamiliarity with Medallion Architecture principles.\nExposure to MS Purview or other data governance tools.\nUnderstanding of data warehousing and reporting concepts.\nInterest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
Data Engineer,PwC Acceleration Centers,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"ole: Associate\nTower: Data, Analytics & Specialist Managed Service\nExperience: : 3 -5.5 years\nKey Skills: AWS , Snowflake, DBT\nEducational Qualification: BE / B Tech / ME / M Tech / MBA\nWork Location: Bangalore\nJob Description\nAs a Associate, you will work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:\nUse feedback and reflection to develop self-awareness, personal strengths, and address development areas.\nFlexible to work in stretch opportunities/assignments.\nDemonstrate critical thinking and the ability to bring order to unstructured problems.\nTicket Quality and deliverables review, Status Reporting for the project.\nAdherence to SLAs, experience in incident management, change management and problem management.\nSeek and embrace opportunities which give exposure to different situations, environments, and perspectives.\nUse straightforward communication, in a structured way, when influencing and connecting with others.\nAble to read situations and modify behavior to build quality relationships.\nUphold the firm's code of ethics and business conduct.\nDemonstrate leadership capabilities by working, with clients directly and leading the engagement.\nWork in a team environment that includes client interactions, workstream management, and cross-team collaboration.\nGood team player, take up cross competency work and contribute to COE activities.\nEscalation/Risk management.\nPosition Requirements:\nRequired Skills:\nAWS Cloud Engineer:\nJob description:\nCandidate is expected to demonstrate extensive knowledge and/or a proven record of success in the following areas:\nShould have minimum 2 years hand on experience building advanced Data warehousing solutions on leading cloud platforms.\nShould have minimum 1-3 years of Operate/Managed Services/Production Support Experience\nShould have extensive experience in developing scalable, repeatable, and secure data structures and pipelines to ingest, store, collect, standardize, and integrate data that for downstream consumption like Business Intelligence systems, Analytics modelling, Data scientists etc.\nDesigning and implementing data pipelines to extract, transform, and load (ETL) data from various sources into data storage systems, such as data warehouses or data lakes.\nShould have experience in building efficient, ETL/ELT processes using industry leading tools like AWS, AWS GLUE, AWS Lambda, AWS DMS, PySpark, SQL, Python, DBT, Prefect, Snoflake, etc.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in AWS.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and troubleshooting data pipelines and resolving issues related to data processing, transformation, or storage.\nImplementing and maintaining data security and privacy measures, including access controls and encryption, to protect sensitive data\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases\nShould have experience in Building and maintaining Data Governance solutions (Data Quality, Metadata management, Lineage, Master Data Management and Data security) using industry leading tools\nScaling and optimizing schema and performance tuning SQL and ETL pipelines in data lake and data warehouse environments.\nShould have Hands-on experience with Data analytics tools like Informatica, Collibra, Hadoop, Spark, Snowflake etc.\nShould have Experience of ITIL processes like Incident management, Problem Management, Knowledge management, Release management, Data DevOps etc.\nShould have Strong communication, problem solving, quantitative and analytical abilities.\nNice to have:\nAWS certification\nManaged Services- Data, Analytics & Insights Managed Service\nAt PwC we relentlessly focus on working with our clients to bring the power of technology and humans together and create simple, yet powerful solutions. We imagine a day when our clients can simply focus on their business knowing that they have a trusted partner for their IT needs. Every day we are motivated and passionate about making our clients better.\nWithin our Managed Services platform, PwC delivers integrated services and solutions that are grounded in deep industry experience and powered by the talent that you would expect from the PwC brand. The PwC Managed Services platform delivers scalable solutions that add greater value to our client's enterprise through technology and human-enabled experiences. Our team of highly skilled and trained global professionals, combined with the use of the latest advancements in technology and process, allows us to provide effective and efficient outcomes. With PwC's Managed Services our clients are able to focus on accelerating their priorities, including optimizing operations and accelerating outcomes. PwC brings a consultative first approach to operations, leveraging our deep industry insights combined with world class talent and assets to enable transformational journeys that drive sustained client outcomes. Our clients need flexible access to world class business and technology capabilities that keep pace with today's dynamic business environment.\nWithin our global, Managed Services platform, we provide Data, Analytics & Insights where we focus more so on the evolution of our clients Data and Analytics ecosystem. Our focus is to empower our clients to navigate and capture the value of their Data & Analytics portfolio while cost-effectively operating and protecting their solutions. We do this so that our clients can focus on what matters most to your business: accelerating growth that is dynamic, efficient and cost-effective.\nAs a member of our Data, Analytics & Insights Managed Service team, we are looking for candidates who thrive working in a high-paced work environment capable of working on a mix of critical Data, Analytics & Insights offerings and engagement including help desk support, enhancement, and optimization work, as well as strategic roadmap and advisory level work. It will also be key to lend experience and effort in helping win and support customer engagements from not only a technical perspective, but also a relationship perspective.\nKindly share your resume to [HIDDEN TEXT]\nRegards,\nMirunalini MJ","AWS DMS, Prefect, snowflake, dbt, Aws Lambda, Hadoop, Collibra, Pyspark, AWS Glue, Informatica, Sql, Spark, Python, AWS"
Senior Data Engineer-ADF/Python,DecisionTree Analytics & Services,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Company Description\nDecisionTree Analytics & Services is a global provider of advanced analytics and campaign management solutions based in Gurugram. We specialize in transforming raw data into scalable and smart insights to empower organizations to make data-driven decisions. Our solutions range from data integration and automation to advanced machine learning algorithms for pattern identification and decision acceleration.\nRole Description\nThis is a full-time on-site role for a Senior Data Engineer-ADF/Python at DecisionTree Analytics & Services in Gurugram. The Senior Data Engineer will be responsible for tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.\nQualifications\nData Engineering and Data Modeling skills\nExperience with Azure Data Factory\nData Warehousing and Data Analytics proficiency\nStrong programming skills in languages like Python\nAbility to work in a fast-paced, collaborative environment\nStrong problem-solving and analytical skills","data engineering, Azure Data Factory, Data Modeling, Data Warehousing, Data Analytics, Python"
Senior Data Engineer-ADF/Python,DecisionTree Analytics & Services,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Company Description\nDecisionTree Analytics & Services is a global provider of advanced analytics and campaign management solutions based in Gurugram. We specialize in transforming raw data into scalable and smart insights to empower organizations to make data-driven decisions. Our solutions range from data integration and automation to advanced machine learning algorithms for pattern identification and decision acceleration.\nRole Description\nThis is a full-time on-site role for a Senior Data Engineer-ADF/Python at DecisionTree Analytics & Services in Gurugram. The Senior Data Engineer will be responsible for tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.\nQualifications\nData Engineering and Data Modeling skills\nExperience with Azure Data Factory\nData Warehousing and Data Analytics proficiency\nStrong programming skills in languages like Python\nAbility to work in a fast-paced, collaborative environment\nStrong problem-solving and analytical skills","data engineering, Azure Data Factory, Data Modeling, Data Warehousing, Data Analytics, Python"
Big Data Engineer (Freelancer),Soul AI,4-6 Years,,India,Login to check your skill match score,"About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact.\nWe are looking for an experienced Big Data Engineer with at least 4 years of experience to design, develop, and manage large-scale data processing systems.\nKey Responsibilities:\nBuild and maintain data pipelines for large datasets.\nDesign systems for real-time data processing.\nCollaborate with data scientists and engineers to optimize data workflows.\nRequired Qualifications:\n4+ years of experience as a Big Data Engineer.\nStrong proficiency with big data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.\nExperience with cloud platforms like AWS, GCP, or Azure.\nWhy Join Us\nCompetitive pay (1200/hour).\nFlexible hours.\nRemote opportunity.\nNOTE: Pay will vary by project and typically is up to Rs. 1200 per hour (if you work an average of 3 hours every day - that could be as high as Rs. 108K per month) once you clear our screening process.\nShape the future of AI with Soul AI!","NoSQL databases, Gcp, Hadoop, Spark, Kafka, Azure, AWS"
Senior Data Engineer,United Airlines,2-5 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Achieving our goals starts with supporting yours. Grow your career, access top-tier health and wellness benefits, build lasting connections with your team and our customers, and travel the world using our extensive route network.\n\nCome join us to create what's next. Let's define tomorrow, together.\n\nDescription\n\nUnited's Digital Technology team designs, develops, and maintains massively scaling technology solutions brought to life with innovative architectures, data analytics, and digital solutions.\n\nFind your future at United! We're reinventing what our industry looks like, and what an airline can be from the planes we fly to the people who fly them. When you join us, you're joining a global team of 100,000+ connected by a shared passion with a wide spectrum of experience and skills to lead the way forward.\n\n\n\nAchieving our ambitions starts with supporting yours. Evolve your career and find your next opportunity. Get the care you need with industry-leading health plans and best-in-class programs to support your emotional, physical, and financial wellness. Expand your horizons with travel across the world's biggest route network. Connect outside your team through employee-led Business Resource Groups.\n\n\n\nCreate what's next with us. Let's define tomorrow together.\n\nJob Overview And Responsibilities\n\nExciting opportunity to be a part of a brand new best-in-class data science & analytics team to create the world's first travel media network from the world's best and largest airline. An entrepreneurial and meticulous data engineer who builds underlying supporting data for measurement and reporting across United's Travel Media Network.\n\nDesign and build scalable and reliable data infrastructure and pipelines (ingestion, integration, ETL, real-time connectors) to support data for measurement and reporting\nBuild connections with relevant endpoints for data ingestion\nCollaborate with data scientists, analysts, and other stakeholders to understand data needs and requirements\nEnsure data quality, performance, and security across the entire data lifecycle; developing high-quality and well-documented data sets\nContinuously improve data infrastructure and processes to increase efficiency, scalability, and reliability\nStay up to date with emerging technologies and best practices in data engineering\n\nThis position is offered on local terms and conditions. Expatriate assignments and sponsorship for employment visas, even on a time-limited visa status, will not be awarded.\n\nQualifications\n\nWhat's needed to succeed (Minimum Qualifications):\n\nBachelor's degree in Computer Science, Engineering or a related field\n2-5 years hands-on industry experience in Data Engineering (or equivalent quantitative job title)\nDeep technical knowledge of data engineering; highly skilled in SQL, relational databases, big data\nSkilled in development of data warehousing, data flow design and development, and ETL processes\nProficient with Python or Scala, Azure Data Factory, Synapse\nProficient with cloud-based data technologies such as AWS, Azure, and/or GCP\nFamiliarity with data visualization tools (Power BI, Tableau)\nStrong collaborator with cross-functional teams from tech, design, and business and experience leading teams in an agile setting\nAbility to communicate and explain data and its implications to various stakeholders\nAbility to evaluate different options proactively and to solve problems in an innovative way, developing new solutions or combining existing methods to create new approaches\nMust be legally authorized to work in India for any employer without sponsorship\nMust be fluent in English and Hindi (written and spoken)\nSuccessful completion of interview required to meet job qualification\nReliable, punctual attendance is an essential function of the position\n\nWhat will help you propel from the pack (Preferred Qualifications):\n\nAdvanced computer engineering degree preferred\n\nGGN00001979","data flow design, Relational Databases, ETL processes, Power Bi, Scala, Data Warehousing, Tableau, Sql, Azure Data Factory, Gcp, Big Data, Python, AWS"
Data Engineer - Mastery,Technogen India Pvt. Ltd.,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Note: More details below for DE, Investments Domain role.\n\nDomain Key words : Capital Markets, Investment Banking\n\nTechnical Key words: Informatica, batch/shell scripting, data warehousing , SQL, ETL, Python (basic to intermediate)\n\nObjectives of the role\n\nDesign, build and maintain complex ELT jobs that deliver business value\nTranslate high-level business requirements into technical specs\nIngest data from disparate sources into the data lake and data warehouse\nCleanse and enrich data and apply adequate data quality controls\nProvide insight and direction to guide the future development of organization's data platform\nDevelop re-usable tools to help streamline the delivery of new projects\nCollaborate closely with other developers and provide mentorship\nEvaluate and recommend tools, technologies, processes and reference architectures\nWork in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements\n\nBasic Qualifications\n\nBachelor's degree in computer science, engineering or a related field\nData: 8+ years of experience with data analytics and warehousing in Investment & Finance Domain\nSQL: Deep knowledge of SQL and query optimization\nELT: Good understanding of ELT methodologies and tools\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers\n3+ years of coding and scripting (Python, Java, Scala) and design experience.\n3+ years of experience with Spark framework.\nExperience with Vertica or any Columnar Databases.\nStrong data integrity, analytical and multitasking skills.\n\nPreferred Qualifications\n\nMaster's degree in computer science or engineering or a related field\nCloud: Experience working in a cloud environment (e.g. AWS)\nPython: Hands on experience developing with Python\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nWorkflow: Good knowledge of orchestration and scheduling tools (e.g. Apache Airflow)\nReporting: Experience with data reporting (e.g. Microstrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)","Alation, Looker, Microstrategy, Data Warehousing, Apache Spark, Kafka, Tableau, Informatica, Sql, ELT, Apache Airflow, shell scripting, Spark, Vertica, Python, Etl"
Senior Data Engineer,USEReady,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"Role: Senior Data Engineer\nExperience-3-8 yrs\nLocation: Bangalore, Gurgaon, Mohali, Pune\nAbout the Role:\nWe are seeking a skilled and proactive Data Engineer with 3-8 years of hands-on experience in Snowflake, Python, Streamlit, and SQL, along with expertise in consuming REST APIs and working with modern ETL tools likeMatillion, Fivetran etc. The ideal candidate will have a strong foundation in data modeling, data warehousing, and data profiling, and will play a key role in designing and implementing robust data solutions that drive business insights and innovation.\nKey Responsibilities:\nDesign, develop, and maintain data pipelines and workflows using Snowflake and an ETL tool (e.g., Matillion, dbt, Fivetran, or similar).\nDevelop data applications and dashboards using Python and Streamlit.\nCreate and optimize complex SQL queries for data extraction, transformation, and loading.\nIntegrate REST APIs for data access and process automation.\nPerform data profiling, quality checks, and troubleshooting to ensure data accuracy and integrity.\nDesign and implement scalable and efficient data models aligned with business requirements.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data needs and deliver actionable solutions.\nImplement best practices in data governance, security, and compliance.\nRequired Skills and Qualifications:\n35 years of professional experience in a data engineering or development role.\nStrong expertise in Snowflake, including performance tuning and warehouse optimization.\nProficient in Python, including data manipulation with libraries like Pandas.\nExperience building web-based data tools using Streamlit.\nSolid understanding and experience with RESTful APIs and JSON data structures.\nStrong SQL skills and experience with advanced data transformation logic.\nExperience with an ETL tool commonly used with Snowflake (e.g., dbt, Matillion, Fivetran, Airflow).\nHands-on experience in data modeling (dimensional and normalized), data warehousing concepts, and data profiling techniques.\nFamiliarity with version control (e.g., Git) and CI/CD processes is a plus.\nPreferred Qualifications:\nExperience working in cloud environments (AWS, Azure, or GCP).\nKnowledge of data governance and cataloging tools.\nExperience with agile methodologies and working in cross-functional teams.","Airflow, Matillion, CI CD, dbt, snowflake, Streamlit, Fivetran, Data Warehousing, Data Modeling, Sql, Git, Etl Tools, Rest Apis, Data Profiling, Python"
"Data Engineer with Databricks, Azure and Power BI (DAX) Skills",Sony India Software Centre,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Description\n\nTotal relevant experience in Data Engineering should be 6-8 yrs.\n\nMust have good knowledge on Azure Databricks, Azure Datalake and Power BI including DAX skills.\n\nShould have good Power BI Experience which includes -\n\nStrong Data modelling skills.\nExpert in Tabular model design.\nExpert in writing complex DAX formulas.\nHands on experience in DAX optimization\n\nStrong knowledge on the DWBI with SQL.\n\nExperience of working with On-premise/cloud BI solution implementation.\n\nGood knowledge in Python.\n\nGood to have these skills in Databricks -\n\nUnderstanding of AI capabilities.\nUnity Catalog - Implementation\nAI/BI Genie\nDevOps\n\nPower Automate (Good to have)\n\nDepartment\n\nRegional Apps - APCNJP - AP Platforms - G3A\n\nOpen Positions\n\n1\n\nSkills Required\n\nData Bricks, power bi, Azure Data Lake, SQL Development, Python\n\nRole\n\nDesign, develop, and maintain data pipelines and ETL processes using Azure Databricks\nCollaborate with business analysts to understand and meet business requirements for data and analytics\nDevelop and optimize data models and visualizations using Power BI (DAX) to present meaningful insights to stakeholders\nImplement best practices for data governance, security, and compliance in Azure environments\nMonitor and troubleshoot data pipelines, ensuring data integrity and reliability\nStay abreast of the latest trends and technologies in data engineering and analytics\nProvide technical guidance and mentorship to junior members of the team\n\nLocation\n\nBengaluru\n\nEducation/Qualification\n\nB Tech\n\nDesirable Skills\n\nAzure Databricks, Azure Datalake, Power BI with Dax, SQL, Python\n\nYears Of Exp\n\n6 to 8 years\n\nDesignation\n\nTechnical Specialist","Azure Datalake, Data Bricks, Power Bi, Azure Databricks, Dax, Sql, Python"
ETL Data Engineer,QTek Digital,6-9 Years,,"Hyderabad, India",Login to check your skill match score,"Job description\nCompany Description\nQTek Digital is a data solutions provider specializing in custom solutions in data management, data warehouse, and data science. Our team of data scientists, data analysts, and data engineers work together to solve today's challenges and unlock future possibilities. As an employee-centric company, we prioritize engagement, empowerment, and enrichment of our employees.\nRole Description\nThis is a full-time remote role for a BI ETL Engineer at QTek Digital. As a BI ETL Engineer, you will be responsible for day-to-day tasks such as data modeling, utilizing analytical skills, implementing data warehousing solutions, and executing Extract, Transform, Load (ETL) processes. This role requires strong problem-solving abilities and the ability to work independently.\nQualifications\n6-9 years of experience with ETL and ELT pipeline build out using Pentaho, SSIS, FiveTran, Airbyte, or similar tools.\n6-8 Years of experience in SQL and data manipulation languages\nStrong Data Modeling, Dashboard, and Analytical Skills\nExcellent understanding of data warehousing concepts, esp. Kimball design.\nExperience with Pentaho and Airbyte administration will be a huge plus.\nStrong skills in Data Modeling, Dashboard design, and Analytics\nExperience in Data Warehousing and Extract Transform Load (ETL) processes\nStrong problem-solving and troubleshooting skills\nExcellent communication and collaboration skills\nAbility to work independently and in a team\nBachelor's degree in computer science, Information Systems, or a related field\nThis role is based onsite in our Hyderabad Office.\nThe compensation range for this role is INR 5-19 Lakhs depending on a variety of factors including skills and experience.","Airbyte, FiveTran, Pentaho, Data Modeling, Data Warehousing, SSIS, Sql, ELT, Etl"
Lead Data Engineer - Future Detections,JPMorganChase,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nBe an integral part of an agile team that's constantly pushing the envelope to enhance, build, and deliver top-notch technology products.\n\nAs a Lead Data Engineer at JPMorgan Chase within the Cybersecurity & Tech Controls , you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. Drive significant business impact through your capabilities and contributions, and apply deep technical expertise and problem-solving methodologies to tackle a diverse array of challenges that span multiple technologies and applications.\n\nJob Responsibilities\n\nRegularly provides technical guidance and direction to support the business and its technical teams, contractors, and vendors\nDevelops secure and high-quality production code, and reviews and debugs code written by others\nDrives decisions that influence the product design, application functionality, and technical operations and processes\nServes as a function-wide subject matter expert in one or more areas of focus\nActively contributes to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle\nInfluences peers and project decision-makers to consider the use and application of leading-edge technologies\nAdds to the team culture of diversity, equity, inclusion, and respect\n\nRequired Qualifications, Capabilities, And Skills\n\nFormal training or certification on Data engineering concepts and 5+ years applied experience\nHands-on practical experience Big Data , large volume data transfer analysis.\nExperience in spark/ iceberg / parquets.\nExperience with cloud platforms like AWS and container technologies such as Kubernetes and Docker\nKnowledge of software applications and technical processes with considerable in-depth knowledge in one or more technical disciplines (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)\nExperience to tackle design and functionality problems independently with little to no oversight\n\nPreferred Qualifications, Capabilities, And Skills\n\nPractical cloud native experience\nComputer Science, Computer Engineering, Mathematics, or a related technical field\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Mobile, parquets, Data engineering concepts, iceberg, Machine Learning, Big Data, Artificial Intelligence, cloud, Docker, Spark, Kubernetes, AWS"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description\nOncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.\nJob Type:Full-time or Contract\nStart Date: June 1st 2025\nRole Description\nThis is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.\nJob Description:\nCandidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively\n6+ Years of experience as a Data Engineer\nStrong technical expertise in SQL and Snowflake is a must.\nStrong knowledge of joins and common table expressions (CTEs)\nStrong experience with Python\nStrong expertise in ETL process and with various data model concepts\nKnowledge of star schema and snowflake schema\nGood to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue\nExperience with Big Data Tools and technologies\nKey Skills:\nGood Understanding of data structures and data analysis using SQL\nKnowledge of implementing ETL/ELT for data solutions end-to-end\nUnderstanding requirements, and data solutions (ingest, storage, integration, processing)\nKnowledge of analyzing data using SQL\nConducting End to End verification and validation for the entire application\nResponsibilities:\nUnderstand and translate business needs into data models supporting long-term solutions.\nPerform reverse engineering of physical data models from databases and SQL scripts.\nAnalyze data-related system integration challenges and propose appropriate solutions.\nAssist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
"Senior Data Engineer (ELT/ELT, Python, Snowflake, AWS)",Reap,6-8 Years,,"Mumbai, India",Login to check your skill match score,"Role Overview\n\nAs part of a rapidly growing data team, we are looking to hire Senior Data Engineers to contribute to our dynamic and innovative projects. We are looking for someone who is ready for the challenge, is solutions-oriented, and thinks out of the box. The ideal candidate should be passionate about using data to drive impact across the organization and a collaborative mindset to work effectively within cross-functional teams. If you are enthusiastic about pushing boundaries, solving complex problems, and making a significant impact, we encourage you to apply and be part of our exciting journey in shaping the future of our products.\n\nResponsibilities\n\nDesign and development: Design, develop, and deploy scalable ETL/ELT pipelines and APIs for ingestion and transformation. Implement data modeling best practices for optimal accessibility, flexibility and query performance. Implement data governance practices, including data security, privacy, and compliance, to ensure data integrity and regulatory compliance.\nCollaboration: Work closely with cross-functional teams, including product managers, designers, and other engineers, to ensure seamless product development from concept to deployment. Influence product and cross-functional teams to identify data opportunities to drive impact.\nContinuous learning: Stay updated with the latest industry trends and technologies, ensuring our tech stack remains modern and competitive.\nObservability and Support: Build monitor and alerts for data pipelines monitoring, identify and resolve performance issues, troubleshoot data-related problems in collaboration with other teams, and ensure data platform SLAs are met.\n\nTo Be Successful You Will Need To Have\n\nExperience: 6+ year work experience in data engineering and cloud platforms. Previous experience in a senior or lead engineering role.\nTechnical proficiency: Expertise in ETL, data modeling, and cloud data warehousing. Strong programming skills in Python, SQL, AWS, Snowflake and related tech stack. Hands-on experience with big data processing and API integrations.\nProblem solving: Strong analytical and problem-solving skills, with a keen attention to detail and a passion for troubleshooting and debugging\nExperience in Credit Cards, Payments and AWS certification would be an advantage.\nExposure to AI, machine learning, and predictive analytics is highly desirable.\n\nBenefits\n\nA Global & Dynamic Team\nRemote Work Friendly\n\nAfter submitting your application, please check your inbox for a confirmation email. If you don't see it, kindly check your spam or junk folder and adjust your settings to ensure future communication reaches your inbox. You can follow the steps here.","API Integrations, Big Data Processing, snowflake, Cloud Data Warehousing, Data Modeling, Python, Sql, Etl, AWS"
Senior Data Engineer,WebMD,4-6 Years,,"Navi Mumbai, Mumbai, India",Login to check your skill match score,"Position Requirements:\n4+ years of experience with RDBMS databases such as Oracle, MSSQL or PostgreSQL\n2+ years of experience with Pentaho Data Integration or any ETL tools such as Talend, Informatica, DataStage or HOP.\nWorking knowledge of orchestration tools such Oozie and Airflow\nExperience working in both OLAP and OLTP environments\nExperience working on-prem, not just cloud environments\nExperience working with teams outside of IT (i.e. Application Developers, Business Intelligence, Finance, Marketing, Sales)\nExperience managing or developing in the Hadoop ecosystem is preferred\nProgramming background with either Python, Scala, Java or C/C++ is a plus\nExperience with Spark. PySpark, SparkSQL, Spark Streaming, etc\nStrong in any of the Linux distributions, RHEL, CentOS or Fedora\nExperience using reporting and Data Visualization platforms (Tableau, Pentaho BI) is good to have\nWeb analytics or Business Intelligence a plus\nUnderstanding of Ad stack and data (Ad Servers, DSM, Programmatic, DMP, etc)","Airflow, Pentaho BI, DMP, Ad stack, Programmatic, DSM, Business Intelligence, Java, C, Hadoop Ecosystem, Scala, Pyspark, OLAP, Tableau, Sparksql, Spark Streaming, Web Analytics, Ad Servers, Spark, Oozie, Pentaho Data Integration, Python, Oltp"
Senior Data Engineer,Endowus,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"About us\nEndowus is Asia's leading fee-only wealth platform. Headquartered in Singapore, we are the first digital advisor to span both private wealth and public pension savings (CPF & SRS), helping everyone grow all their money with expert advice, institutional access to financial solutions, low & fair fees, and a delightful personalised digital wealth experience.\nOur clients entrust us with a responsibility that goes far beyond technology or financial markets - they entrust us with their wealth - their livelihoods and ambitions of a better future for themselves and their loved ones.\nOur mission is clear: help people invest better so they can live easier today, and better tomorrow.\nThe team has deep domain knowledge in finance and technology, bringing together decades of experience from various banks and tech companies. We treasure our diversity in background and experience, and we look for people who share our beliefs in our mission.\nAbout the Team\nThe Client Interactions & Insights Platform team builds and operates the scalable data platform that powers data analytics and business intelligence for better decision making at Endowus. Working with colleagues in Data Analytics, Growth, Marketing, and Operations teams, the data engineering team creates data solutions that provide them with performant, near real time access to internal & third party data and insights.\nWe are a full stack team that builds our systems using cloud native patterns and operates them with high standards of engineering & operational excellence.\nAbout this role, responsibilities & ownership\nLead technical design, delivery, reliability & security of our core data platform.\nWork closely with the Product team, other Engineering teams, and stakeholders in Data Analytics, Growth, Marketing, Operations, Compliance & IT Risk to achieve our business goals.\nStrive for high levels of technical quality, reliability, and delivery efficiency.\nMentor and grow a small talented team of junior data engineers.\nRequirements\nBachelors or above in Computer Science, a related field, or equivalent professional experience.\nAt least 6 years experience in designing and implementing highly scalable, distributed data collection, aggregation, and analysis systems built for handling large volumes of data in the cloud.\nSignificant hands-on experience developing data pipelines with Apache Spark with Scala\nAt least 2 years experience as a tech lead facing business users directly and leading technical initiatives\nSignificant hands-on experience building and optimising data pipelines for data collection, transformation, aggregation in Apache Flink or Apache Spark, using dependency and workflow management tools such as Airflow, operating in a public cloud environment like AWS, GCP or Azure.\nAdvanced SQL knowledge and strong experience working with relational and non-relational databases.\nExperience integrating BI tools such as Tableau, Mode, Looker, etc.\nExperience integrating data sources using REST and streaming protocols, especially using Kafka.\nExperience with building systems & processes to handle data quality, data privacy, and data sovereignty requirements.\nExperience with agile processes, testing, CI/CD, and production error/metrics monitoring.\nSelf-driven with a strong sense of ownership.\nComfortable with numbers and motivated by steep learning curves.\nHas a strong product sense and is empathetic to customers experiences of using the product.\nPreferred Skills & Experience\nDomain experience in a B2C context is a strong plus.\nKnowledge of finance, wealth, and trading domain.\nSome exposure to CQRS / Event Sourcing patterns.\nExperience with AWS or GCP, Cassandra, Kafka, Kubernetes, Terraform.\nOur Investors, recognition, licensing\nFounded in 2017, Endowus has raised a total of US$50 million in funding from investors such as UBS, EDBI, Prosus Ventures, Z Venture Capital, Samsung Ventures, Singtel Innov8, and global leading venture capital firms Lightspeed Venture Partners and SoftBank Ventures Asia.\nEndowus leadership and growth have been recognised by the industry and it has attained numerous awards including, Singapore's Best Digital Wealth Management (Asia Asset Management's Best of the Best Awards 2024), Singapore's Best Digital Upgrade for enhancements made on the Endowus app (The Asset Triple A Digital Awards 2024), Singapore's Best Digital Wealth Management Experience (The Asset Triple A Digital Awards 2023), and Best WealthTech Solution 2023 (Asian Private Banker 9th Technology Awards). Endowus is also among the firms named in the World Economic Forum's Technology Pioneers 2023, LinkedIn Top Start-ups 2023 and Forbes 100 to Watch list for 2022.\nThe Endowus Group comprises Endowus licensed companies in Hong Kong and Singapore, as well as Hong Kong-based multi family office Carret Private. Endowus Group serves over a hundred thousand clients with content, advice and access. With group assets of over US$6 billion, it is one of the largest independent wealth managers in Asia. From a combination of 100% trailer fees rebates as direct cashback to clients, savings from the access to institutional share class and exclusive funds, Endowus has created more than US$40 million in savings per year for its clients.","Mode, Airflow, data sovereignty, agile processes, Looker, streaming protocols, Relational Databases, non-relational databases, Apache Flink, Scala, Apache Spark, Kafka, Tableau, Sql, Data Quality, REST, Gcp, Data Privacy, Azure, AWS"
Data Engineer,Xebia,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description:\nWe are looking for a skilled Data science Data Engineer to join our team, working on end-to-end data engineering and data science use cases. The ideal candidate will have strong expertise in Python or Scala, Spark (Databricks), and SQL, building scalable and efficient data pipelines on Azure.\nPrimary Skills:\nData Engineering & Cloud:\nProficiency in Azure Data Platform (Data Factory, Databricks).\nStrong skills in SQL and [Python or Scala] for data manipulation.\nExperience with ETL/ELT pipelines and data transformations.\nFamiliarity with Big Data technologies (Spark, Delta Lake, Parquet).\nML & MLOps Integration:\nExperience supporting ML pipelines with efficient data workflows.\nKnowledge of MLOps practices (CI/CD, model monitoring, versioning).\nData Optimization & Performance:\nExpertise in data pipeline optimization and performance tuning.\nExperience on feature engineering and model deployment.\nAnalytical & Problem-Solving:\nStrong troubleshooting and problem-solving skills.\nExperience with data quality checks and validation.\nNice-to-Have Skills:\nExposure to NLP, time-series forecasting, and anomaly detection.\nFamiliarity with data governance frameworks and compliance practices.\nUnderstanding of retail, or workforce analytics.","Parquet, Azure Data Platform, Delta Lake, Data Factory, Scala, Sql, ELT, MLops, Spark, Databricks, Python, Etl"
Data Engineer,Louis Dreyfus Company,Fresher,,"Bengaluru, India",Login to check your skill match score,"Company Description\n\nLouis Dreyfus Company is a leading merchant and processor of agricultural goods. Our activities span the entire value chain from farm to fork, across a broad range of business lines, we leverage our global reach and extensive asset network to serve our customers and consumers around the world. Structured as a matrix organization of six geographical regions and ten platforms, Louis Dreyfus Company is active in over 100 countries and employs approximately 18,000 people globally.\n\nJob Description\n\nBackground: The Crop Monitor and Weather Desk initiatives are critical components of our data-driven strategy in the Fundamental Trading division. These projects rely heavily on robust data engineering to ensure accurate and timely data processing, directly impacting our trading decisions and overall performance.\n\nCurrent Situation: An external developer who was integral to these projects left our service provider company EPAM. This departure has created a gap in our data engineering capabilities, which needs to be addressed promptly to maintain the continuity and quality of our operations.\n\nProposal: We propose to internalize this role by hiring a dedicated data engineer in our Bangalore office. This position will be unbudgeted for the current year but included in the 2025 budget.\n\nRationale:\n\nCost Efficiency: Internalizing the role in Bangalore will be more cost-effective compared to continuing with external service providers. The cost savings will be realized through reduced contractor fees and better control over project timelines and deliverables. Quality and Continuity: Having an in-house data engineer will ensure better alignment with our project goals and provide continuity in our data engineering efforts. This will lead to improved data quality and more reliable outputs for the Crop Monitor and Weather Desk projects. Strategic Location: Bangalore is a strategic location for our data engineering needs due to its rich talent pool. This will enable us to attract and retain top talent, further enhancing our capabilities.\n\nLong-term Benefits: Internalizing this role will not only address the immediate gap but also strengthen our internal expertise and reduce dependency on external vendors in the long run\n\nAdditional Information\n\nAdditional Information for the job\n\nDiversity & Inclusion\n\nLDC is driven by a set of shared values and high ethical standards, with diversity and inclusion being part of our DNA. LDC is an equal opportunity employer committed to providing a working environment that embraces and values diversity, equity and inclusion.\n\nLDC encourages diversity, supports local communities and environmental initiatives. We encourage people of all backgrounds to apply.\n\nSustainability\n\nSustainable value is at the heart of our purpose as a company.\n\nWe are passionate about creating fair and sustainable value, both for our business and for other value chain stakeholders: our people, our business partners, the communities we touch and the environment around us\n\nWhat We Offer\n\nWe provide a dynamic and stimulating international environment, which will stretch and develop your abilities and channel your skills and expertise with outstanding career development opportunities in one of the largest and most solid private companies in the world.\n\nWe offer\nA workplace culture that embraces diversity and inclusivity\nOpportunities for Professional Growth and Development\nEmployee Recognition Program\nEmployee Wellness Programs - Confidential access to certified counselors for employee and eligible family members, along with monthly wellness awareness sessions.\nCertified Great Place to Work",data engineering
GCP Lead Data Engineer,StatusNeo,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Role Overview:\nThe Data Engineer will focus on developing, maintaining, and optimizing data pipelines. You will work on BigQuery, and Incorta,\nensuring efficient data ingestion, transformation, and governance while integrating AI-driven automation.\nKey Responsibilities:\nBuild and optimize data pipelines for ingestion, transformation, and storage using BigQuery, SQL Managed Services, and Incorta.\nImplement AI-driven automation for data pipeline monitoring, performance tuning, and anomaly detection.\nEnsure data governance, security, and compliance standards are met across all platforms.\nOptimize data workflows for cost efficiency and scalability.\nCollaborate with BI, AI, and application teams for seamless data access and analytics.\nIntegrate data from multiple sources including Salesforce, Oracle AMS, and other enterprise applications.\nRequired Skills:\nPrimary: BigQuery, ETL Development, SQL Managed Services, Incorta\nSecondary: Data Pipeline Optimization, Cost Optimization, Pipeline Maintenance .","SQL Managed Services, Pipeline Maintenance, Cost Optimization, Data Pipeline Optimization, Incorta, BigQuery, Etl Development"
Staff Data Engineer,Revenera,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Revenera helps product executives build better products, accelerate time to value and monetize what matters. Revenera's leading solutions help software and technology companies drive top line revenue with modern software monetization, understand usage and compliance with software usage analytics, empower the use of open source with software composition analysis and deliver an excellent user experiencefor embedded, on premises, cloud, and SaaS products.\n\nStaff Data Engineer, India\n\nRevenera helps product executives build better products, accelerate time to value and monetize what matters. Revenera's leading solutions help software and technology companies drive top line revenue with modern software monetization, understand usage and compliance with software usage analytics, empower the use of open source with software composition analysis and deliver an excellent user experiencefor embedded, on-premises, cloud, and SaaS products.\n\nRevenera's Monetization platform is the global standard for electronic software licensing and entitlement management. It helps you implement your digital business model, define packaging options for your software, manage customers and their use rights and deliver software and updates.\n\nRevenera is looking for an experienced Data Engineering Manager to lead our team of Software and Data Engineers who build our Data Warehouse and Data Analytics platforms for FlexNet Operations.\n\nJob Description\n\nExperienced Senior Data Engineer design, build, and optimize scalable data pipelines and systems. You will be responsible for managing complex datasets, ensuring high data quality, and collaborating with cross-functional teams.\n\nResponsibilities\n\nArchitect, develop, document and maintain robust data pipelines and ETL processes.\nDesign and implement data storage solutions (data warehouses) optimized for performance and cost.\nCollaborate with PO/PM and other stakeholders to understand data needs and deliver solutions. Advising on and contributing to projects & delivery planning for all data engineering elements.\nImplement and manage data quality, data governance, and metadata management frameworks.\nOptimize data systems for scalability, reliability, and performance. Ensure compliance with data privacy and security regulations (GDPR, CCPA, etc.).\nMentor junior data engineers and lead technical initiatives.\nContinually looking for innovative ways to make improvements in building a scalable data platform based on the latest trends and research.\nWorking closely with IT to ensure data security and cloud configuration remains at optimum performance at all times.\n\nRequired Skills And Qualifications\n\n10-12 years of previous experience of working with large complex data engineering projects.\nExperience with designing and developing ETL pipelines. In-depth understanding of data modelling, ETL processes, and data warehousing concepts.\nTechnical knowledge of data engineering techniques and concepts, including data ingestion, processing, and storage.\nExpert-level proficiency in SQL. Working knowledge in Python, reactJS/NodeJS, Javascript, typescript Strong understanding of database management.\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) and containerization technologies (e.g., Docker, Kubernetes) is a plus\nPrior experience with cloud-based analytical tools like Snowflake is highly advantageous. Excellent communication and technical presentation skills. Able to work methodically under pressure and work to tight deadlines.\nAbility to multi-task and work across a range of projects and issues with various timelines and priorities.\nAble to work in a flexible and agile environment.\nExposure to real-time data streaming and event-driven architecture.\nKnowledge of AI, Machine Learning workflows and Machine Learning Operations is a plus.\n\nRevenera is proud to be an equal opportunity employer. Qualified applicants will be considered for open roles regardless of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by local/national laws, policies and/or regulations. Regarding disability, we encourage candidates requiring accommodations to please let us know by emailing [HIDDEN TEXT].","Event-driven architecture, Data Storage, Real-time data streaming, AI Machine Learning workflows, Data ingestion, ETL processes, Machine Learning Operations, Nodejs, Data Warehousing Concepts, Sql, Typescript, Reactjs, Javascript, Python"
Data Engineer - Apache Airflow,Ekfrazo Technologies Private Limited,4-6 Years,,"Mumbai, India",Login to check your skill match score,"Job Title: Data Engineer\nLocation: Mumbai- WFO\nExperience: 4+ Years\nNotice Period: Immediate Joiners Only\nAbout the Role:\nWe are seeking a skilled and motivated Data Engineer to join our team in Mumbai. The ideal candidate will have a strong background in Python (OOPS), workflow orchestration using Airflow, and experience working with Azure cloud services and Snowflake. This role is best suited for someone passionate about building scalable data pipelines and backend applications or SDKs.\nKey Responsibilities:\nDesign, build, and maintain scalable and robust data pipelines using Apache Airflow\nDevelop backend components and reusable SDKs in Python, with a strong emphasis on OOPS principles\nIntegrate and manage large-scale data workflows on Azure Cloud, leveraging services such as Data Factory, Blob Storage, and more\nWork with Snowflake for data warehousing and analytics\nImplement monitoring and data quality checks using tools like Great Expectations\nCollaborate with cross-functional teams to understand business data needs and deliver high-quality solutions\nMust-Have Skills:\n4+ years of experience in Data Engineering or related roles\nHands-on experience with Apache Airflow\nStrong Python programming skills, with emphasis on Object-Oriented Programming (OOPS) and backend application or SDK development\nProven experience working with Azure cloud platform, especially services relevant to data engineering (e.g., Data Factory, Blob Storage, Azure Functions)\nGood understanding of Snowflake architecture, data loading/unloading, and query optimization\nStrong analytical and problem-solving skills\nNice to Have:\nExperience with Great Expectations or similar Data Quality frameworks\nPrior work on building or maintaining Data Quality (DQ) frameworks\nAdvanced knowledge of Snowflake features like Snowpipe, Streams & Tasks, or Materialized Views","snowflake, Blob Storage, Great Expectations, Apache Airflow, Data Factory, Azure Cloud Services, Python"
"Senior Data Engineer ( python , Java and SQL , Data pipelines )",NielsenIQ,7-9 Years,,India,IT/Computers - Software,"Job Description\nSenior Data Engineer\nMission of the Role\nYou are a passionate and skilled data engineer who enjoys building scalable, production-grade data pipelines that enable machine learning and AI at scale. You thrive on solving complex data challenges and understand how critical a robust data pipeline is to the success of any predictive model.\nAs a Senior Data Engineer on the Forecasting team, you will be responsible for designing, implementing, and maintaining the data pipelines that power deep neural network models used for global sales and revenue forecasting. You will work closely with ML engineers, data scientists, and product stakeholders to build automated, secure, and cost-efficient pipelines using GCP-native services.\nYou take pride in writing clean, testable, and production-ready code, and are comfortable operating in cloud environments. You know how to make data pipelines observable, recoverable, and performant. You are also comfortable taking architectural ownership and contributing to long-term platform improvements.\nYou will:\nDesign and build robust, scalable, and maintainable data pipelines for training and inference of deep learning models used in forecasting and other Machine Learning based algorithms.\nDevelop and orchestrate end-to-end workflows using GCP-native tools such as Cloud Workflows, Cloud Run, BigQuery, Cloud Functions, and Pub/Sub.\nAutomate and maintain the pipeline lifecycle, including data extraction, transformation, loading (ETL/ELT), and triggering model runs.\nMonitor and Optimise performance, cost-efficiency, and data freshness across all stages of the pipeline.\nCollaborate with data scientists to productionise AI/ML models and integrate them seamlessly into business workflows.\nOwn the data ingestion, validation, and transformation logic powering the forecasting models, ensuring high-quality, consistent input data.\nHandle deployment, versioning, and configuration of pipeline components using Docker, Git, and CI/CD practices.\nImplement logging, monitoring, and alerting to ensure reliability and traceability in a high-scale environment.\nReview code, mentor junior engineers, and help define best practices in our evolving data engineering stack.\nQualifications\nYou have:\n7+ years of experience in data engineering or backend engineering roles.\nStrong expertise in Python and SQL, with experience building production-grade data pipelines.\nSolid understanding of Docker, Git, and shell scripting in Linux environments.\nHands-on experience with GCP services\nExperience in building, deploying, and maintaining data workflows that feed AI/ML models.\nFamiliarity with model lifecycle management and infrastructure challenges in ML pipelines.\nProficiency in setting up CI/CD pipelines for data and ML systems using GitLab CI, Cloud Build, or similar tools.\nExposure to Java for backend services or pipeline components (even if not primary language).\nA proactive, collaborative mindset and strong communication skills across engineering and data science teams.\nNice to have:\nExposure to forecasting or time series modelling pipelines.\nExperience with event-driven architectures.\nFamiliarity with infrastructure-as-code tools like Terraform\nUnderstanding of data quality frameworks and observability tools\nKnowledge of model versioning tools and experiment tracking systems\nAdditional Information\nWhy Join us\nYou'll be working on a high-impact platform that forecasts consumer demand and drives business-critical decisions globally\nYou'll be part of a tight-knit, collaborative, and innovative Forecasting & AI team within a leading global data science organisation\nYou'll have access to top-tier infrastructure, GCP services, and challenging real-world problems in predictive modelling\nFlexible working hours, remote-friendly culture, and strong focus on personal and professional growth\nCompetitive compensation and performance-based bonuses\nOur Benefits\nFlexible working environment\nVolunteer time off\nLinkedIn Learning\nEmployee-Assistance-Program (EAP)\nAbout NIQ\nNIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights-delivered with advanced analytics through state-of-the-art platforms-NIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.\nFor more information, visit NIQ.com\nWant to keep up with our latest updates\nFollow us on: | | |\nOur commitment to Diversity, Equity, and Inclusion\nNIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center:","Cloud Run, Cloud Workflows, GCP-native tools, Pub Sub, Cloud Functions, Python, Sql, BigQuery, Shell scripting, Java, Docker, Git"
Data Engineer,Recro,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"What you will enjoy doing\nLead Technical Architecture & Development\no Design and develop enterprise-level data architecture and scalable solutions,\nhandling diverse data types (time-series, structured, semi-structured, and\nunstructured) across Linde.\no Drive technical development of unified data platforms to enable data access,\ngovernance, and security at scale.\no Create scalable, reusable, and standardized data interfaces for internal and external\ndata exchange, in full compliance with Linde's cybersecurity and data protection\npolicies.\nData Engineering & Automation\no Build and maintain robust data pipelines for extracting and processing data from\nvaried sources including ERP systems, iHistorian, IoT devices, web platforms, and\nexternal APIs.\no Leverage modern data-engineering frameworks and cloud technologies (Python,\nSQL, Spark, Delta Lake, Databricks, Azure) for large-scale, secure, and efficient data\nprocessing.\no Automate workflows, optimize compute resources, and manage CI/CD pipelines\nthrough DevOps practices to ensure reliable and high-performance data operations.\no Oversee platform operability and ensure 24/7 availability of data repositories to\nsupport business continuity.\nCollaboration & Execution\no Engage with business stakeholders to understand requirements and deliver fit-for-\npurpose data solutions that address strategic and operational needs.\no Collaborate closely with IT and cross-functional digital teams to implement and\ndeliver data solutions in a coordinated manner.\no Interact with multi-disciplinary project teams to support successful end-to-end\nproject execution.\nInnovation & Continuous Improvement\no Stay up to date with the latest trends in data technologies and actively contribute to\ninnovation initiatives.\no Propose and implement enhancements for performance, scalability, and\nmaintainability of existing data systems and platforms.\nYou'll be working in the Global Artificial Intelligence team, Linde's AI global corporate division\nengaged with real business challenges and opportunities in multiple countries. Focus of this role is to support the AI team with extending existing and building new AI products for a vast amount of uses cases across Linde's business and value chain.You'll collaborate across different business and\ncorporate functions in international team composed of Project Managers, Data Scientists, Data and\nSoftware Engineers in the AI team and others in the Linde's Global AI team.\nWhat Makes You Great\nBachelor's degree in Computer Science or related Engineering areas with 3+ years of\nexperience in manufacturing settings to develop data-engineering solutions, tools and\nsoftware applications.\n3+ years of experience evaluating and implementing data-engineering and software\ntechnologies.\nExperience in programming languages and frameworks: SQL, Python, Spark, Databricks\n(Delta Lake)\nExperience in data storages: SQL and NoSQL databases, Azure Data Lake Storage\nExperience in developing data solutions, models, API and software applications using SQL,\nPython and .NET\nWorking knowledge of Azure infrastructure management and resource deployment\nWorking knowledge of system networking and security\nWorking knowledge of data visualization tools: PowerBI, Grafana, and Tableau.\nPerseverance and results driven attitude to achieve goals and objectives on time.\nStrong analytical and problem-solving skills.\nStrong communication and presentation skills.\nPreferred:\nMaster's or PhD degree in Computer Science or related Engineering areas with 5 years of\nexperience in developing data-engineering solutions, tools and software applications.\nStrong programming skills and demonstrated ability to work in complex software\ndevelopments.\nKnowledge of machine learning theory with practical development experience.","external APIs, Azure Data Lake Storage, iHistorian, Delta Lake, ERP systems, web platforms, IoT devices, SQL and NoSQL databases, Databricks, Sql, .NET, Grafana, Tableau, Powerbi, Python, Azure, Spark"
Data Engineer,Avalara India,6-8 Years,,"Pune, India",Login to check your skill match score,"What You'll Do\n\nWe are seeking an experienced Lead Data Engineer with experience in the Data Engineering. We are looking for a background in ETL processes, data warehousing, data modeling, and hands-on expertise in SQL and Python. The ideal candidate will have exposure to cloud technologies and will play a key role in designing and managing scalable, high-performance data systems that support marketing and sales insights.\n\nYou will report to Manager- Data engineering\n\nWhat Your Responsibilities Will Be\n\nYou will Design, develop, and maintain efficient ETL pipelines using DBT,Airflow to move and transform data from multiple sources into a data warehouse.\nYou will Lead the development and optimization of data models (e.g., star, snowflake schemas) and data structures to support reporting.\nYou will Leverage cloud platforms (e.g., AWS, Azure, Google Cloud) to manage and scale data storage, processing, and transformation processes.\nYou will Work with business teams, marketing, and sales departments to understand data requirements and translate them into actionable insights and efficient data structures.\nYou will Use advanced SQL and Python skills to query, manipulate, and transform data for multiple use cases and reporting needs.\nYou will Implement data quality checks and ensure that the data adheres to governance best practices, maintaining consistency and integrity across datasets.\nYou will Experience using Git for version control and collaborating on data engineering projects.\n\nWhat You'll Need To Be Successful\n\nBachelor's degree with 6+ years of experience in Data Engineering.\nETL/ELT Expertise: experience in building, improving ETL/ELT processes.\nData Modeling: experience with designing and implementing data models such as star and snowflake schemas, and working with denormalized tables to optimize reporting performance.\nExperience with cloud-based data platforms (AWS, Azure, Google Cloud)\nSQL and Python Proficiency: Advanced SQL skills for querying large datasets and Python for automation, data processing, and integration tasks.\nDBT Experience: Hands-on experience with DBT (Data Build Tool) for transforming and managing data models.\n\nGood To Have Skills\n\nFamiliarity with AI concepts such as machine learning (ML), (NLP), and generative AI. Work with AI-driven tools and models for data analysis, reporting, and automation.\nOversee and implement DBT models to improve the data transformation process.\nExperience in the marketing and sales domain, with lead management, marketing analytics, and sales data integration.\nFamiliarity with business intelligence reporting tools, Power BI, for building data models and generating insights.\n\nHow We'll Take Care Of You\n\nTotal Rewards\n\nIn addition to a great compensation package, paid time off, and paid parental leave, many Avalara employees are eligible for bonuses.\n\nHealth & Wellness\n\nBenefits vary by location but generally include private medical, life, and disability insurance.\n\nInclusive culture and diversity\n\nAvalara strongly supports diversity, equity, and inclusion, and is committed to integrating them into our business practices and our organizational culture. We also have a total of 8 employee-run resource groups, each with senior leadership and exec sponsorship.\n\nLearn more about our benefits by region here: Avalara North America\n\nWhat You Need To Know About Avalara\n\nWe're Avalara. We're defining the relationship between tax and tech.\n\nWe've already built an industry-leading cloud compliance platform, processing nearly 40 billion customer API calls and over 5 million tax returns a year, and this year we became a billion-dollar business. Our growth is real, and we're not slowing down until we've achieved our mission - to be part of every transaction in the world.\n\nWe're bright, innovative, and disruptive, like the orange we love to wear. It captures our quirky spirit and optimistic mindset. It shows off the culture we've designed, that empowers our people to win. Ownership and achievement go hand in hand here. We instill passion in our people through the trust we place in them.\n\nWe've been different from day one. Join us, and your career will be too.\n\nWe're An Equal Opportunity Employer\n\nSupporting diversity and inclusion is a cornerstone of our company we don't want people to fit into our culture, but to enrich it. All qualified candidates will receive consideration for employment without regard to race, color, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law. If you require any reasonable adjustments during the recruitment process, please let us know.","Airflow, dbt, Git, Data Modeling, Data Warehousing, Azure, Google Cloud, Sql, Python, AWS, Etl"
Data Engineer,Lingaro,8-10 Years,,India,Login to check your skill match score,"Role: Data Engineer Lead Consultant\nLocation: India (Full Time-Remote)\nPreference: Immediate Joiners\nAbout Lingaro:\nLingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.\nSince 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.\nAbout\nData Engineering:Data e\nngineering involves the development of solutions for the collection, transformation, storage and management of data to support data-driven decision making and enable efficient data analysis by end users. It focuses on the technical aspects of data processing, integration, and delivery to ensure that data is accurate, reliable, and accessible in a timely manner. It also focuses on the scalability, cost-effectiveness, security, and supportability of the solution. Data engineering encompasses multiple toolsets and architectural concepts across on-premises and cloud stacks, including but not limited to data warehousing, data lakes, lake house, data mesh, and includes extraction, ingestion, and synchronization of structured and unstructured data across the data ecosystem. It also includes processing organization and orchestration, as well as performance optimization of data processing.Job Re\nsponsibilities:Provid\ne leadership and guidance to the data engineering team, including mentoring, coaching, and fostering a collaborative work environment. Set clear goals, assign tasks, and manage resources to ensure successful project delivery. Work closely with developers to support them and improve data engineering processes. Suppor\nt team members with troubleshooting and resolving complex technical issues and challenges. Provid\ne technical expertise and direction in data engineering, guiding the team in selecting appropriate tools, technologies, and methodologies. Stay updated with the latest advancements in data engineering and ensure the team follows best practices and industry standards. Collab\norate with stakeholders to understand project requirements, define scope, and create project plans. Suppor\nt project managers to ensure that projects are executed effectively, meeting timelines, budgets, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Act as\na trusted advisor for the customer. Overse\ne the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration. Align\ncoding standards, conduct code reviews to ensure proper code quality level. Identi\nfy and introduce quality assurance processes for data pipelines and workflows. Optimi\nze data processing and storage for performance, efficiency and cost savings. Evalua\nte and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Act as\nmain point of contact to other teams/contributors engaged in the project. Mainta\nin technical documentation of the project, control validity and perform regular reviews of it. Ensure\ncompliance with security standards and regulations. Requi\nr\nements:A bach\nelor's or master's degree in Computer Science, Information Systems, or a related field is typically required. Additional certifications in cloud are advantageous. Minimu\nm of 8 years of experience in data engineering or a related field. Strong\ntechnical skills in data engineering, including proficiency in programming languages such as Python, SQL, Pyspark. Famili\narity with Azure cloud platform viz. Azure Databricks, Data Factory, Data Lake etc., and experience in implementing data solutions in a cloud environment. Expert\nise in working with various data tools and technologies, such as ETL frameworks, data pipelines, and data warehousing solutions. In-dep\nth knowledge of data management principles and best practices, including data governance, data quality, and data integration. Excell\nent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues. Knowle\ndge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects. Excell\nent communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams, stakeholders, and senior management. Contin\nuous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies. Consul\nting exposure, with external customer focus mindset is preferred.Why\nj\noin us: Stabl\ne\nemployment. On the market since 2008, 1300+ talents currently on board in 7 global sites.100% remote.Flexibility regarding working hours.Full-time positionComprehensive online onboarding program with a Buddy from day 1.Cooperation with top-tier engineers and experts.Unlimited access to the Udemy learning platform from day 1.Certificate training programs. Lingarians earn 500+ technology certificates yearly.Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.Grow as we grow as a company. 76% of our managers are internal promotions.A diverse, inclusive, and values-driven community.Autonomy to choose the way you work. We trust your ideas.Create our community together. Refer your friends to receive bonuses.Activities to support your well-being and health.Plenty of opportunities to donate to charities and support the environment.","ETL frameworks, data pipelines, data warehousing solutions, Pyspark, Data Factory, Data Lake, Azure Databricks, Python, Sql"
Data Engineer,Anblicks,Fresher,,"Hyderabad, India",Login to check your skill match score,"Data Engineer to design, develop, and maintain data pipelines and ETL workflows for processing large-scale structured and unstructured data. The ideal candidate will have expertise in Azure Data Services (Azure Data Factory, Synapse, Databricks, SQL, SSIS, and Data Lake) along with big data processing, real-time analytics, and cloud data integration.\n\nKey Responsibilities\n\nData Pipeline Development & ETL/ELT\nDesign and build scalable data pipelines using Azure Data Factory, Synapse Pipelines ,Databricks, SSIS and ADF Connectors like Salesforce.\nImplement ETL/ELT workflows for structured and unstructured data processing.\nOptimize data ingestion, transformation, and storage strategies.\nCloud Data Architecture & Integration\nDevelop data integration solutions for ingesting data from multiple sources (APIs, databases, streaming data).\nWork with Azure Data Lake, Azure Blob Storage, and Delta Lake for data storage and processing.\nDatabase Management & Optimization\nDesign and maintain cloud data bases (Azure Synapse, BigQuery, Cosmos DB).\nOptimize SQL queries and indexing strategies for performance.\nImplement data partitioning, compression, and caching for efficiency.\nData Governance, Security & Compliance\nEnsure data quality, lineage, and governance with tools like Purview.\nImplement role-based access control (RBAC), encryption, and security policies.\nEnsure compliance with GDPR, HIPAA, and ISO 27001 regulations.\nMonitoring & Performance Tuning\nUse Azure Monitor, Log Analytics, and OpenTelemetry to track performance and troubleshoot data issues.\nAutomate data pipeline testing and validation.\nCollaboration & Documentation\nDocument data models, pipeline architectures, and data workflows.\n\nTechnical Skills\n\nCloud Data Services: Azure Data Factory, Azure Synapse, Databricks, SSIS, BigQuery.\nETL & Data Pipelines: Apache Spark, Python, SQL.\nBig Data Processing: Delta Lake, Parquet, Hadoop, Event Hubs.\nDatabase Management: SQL Server, Cosmos DB.\nSecurity & Compliance: RBAC, Data Masking, Encryption, Purview.\nScripting & Automation: Python, PowerShell, Terraform for IaC.","Purview, Event Hubs, Parquet, Synapse, rbac, Delta Lake, Azure Data Services, Encryption, ELT, Terraform, Cosmos DB, Python, BigQuery, Hadoop, PowerShell, Apache Spark, SQL Server, SSIS, Sql, Azure Data Factory, Data Lake, Databricks, data masking, Etl"
Data Engineer -Pharma Commerical Domain,Predigle,3-5 Years,,"Chennai, India",Login to check your skill match score,"We are seeking an experiencedData Engineerwith a strong background inPharma Commercial Datato join our growing data team. The ideal candidate will have hands-on experience withAzure Databricks,Snowflake, and a deep understanding of pharmaceutical commercial datasets, including sales, claims, and HCP-level data.\nKey Responsibilities\nDesign, build, and maintain scalable and efficient data pipelines focused onPharma Commercial datasets\nDevelop data integration workflows usingAzure Databricksand manage data warehousing inSnowflake.\nWork closely with business stakeholders, analytics teams, and data scientists to ensure data solutions support strategic commercial initiatives.\nEnsure data quality, consistency, and governance across all pharma commercial datasets.\nAutomate routine data processes and monitor pipeline performance for production stability.\nParticipate in architecture reviews and recommend improvements for performance and scalability.\nRequired Qualifications\nMinimum3 years of hands-on experience with Pharma Commercial Data, including datasets like sales, prescription, claims, payer data, or CRM/HCP information.\nProven expertise inAzure DatabricksandSnowflake.\nStrong proficiency inSQLandPythonfor data manipulation and transformation.\nSolid understanding of data modeling, ETL/ELT frameworks, and cloud-based data engineering best practices.\nExperience with data orchestration tools (e.g., Airflow, Azure Data Factory).\nAbility to work independently and communicate effectively with business and technical teams.\nPreferred Qualifications\nExposure to commercial analytics use cases like HCP segmentation, field force effectiveness, and targeting.\nFamiliarity with data privacy, HIPAA, and compliance regulations in the pharma domain.\nExperience working in Agile teams and DevOps environments.\nWhat We Offer\nOpportunity to work on impactful commercial data initiatives in the pharmaceutical industry.\nCompetitive compensation and benefits.\nRemote-friendly, flexible work culture.\nSupportive team environment and continuous learning opportunities.\nAbout Predigle:\nPredigle, an EsperGroup company, is an American multinational organization focused on building a disruptive technology platform that revolutionizes the way businesses conduct their daily operations.\nPredigle has grown rapidly to offer multiple products and services,As a growing startup, we offer an entrepreneurial work environment where ideas are valued, creativity is encouraged, and learning opportunities are immense.\nhttps://espergroup.com\n/ https://predigle.com/\nhttps://www.linkedin.com/company/predigle/","Airflow, snowflake, data orchestration tools, Azure Data Factory, Azure Databricks, Python, Sql, Etl, ELT"
Data Engineer - Looker,"iitjobs, Inc.",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"We have an great opportunity for the role of Data Engineer- Looker.\nMandatory Skills : Looker Action, Looker Dashboarding, Looker Data Entry, LookML, SQL Queries.\nRelevant Exp : 6+ Yrs\nJob Summary:\nWe are seeking a skilled Data Engineer with deep expertise in Looker, including Looker Actions, Dashboarding, Data Entry, LookML, and SQL Queries. You will play a key role in designing, implementing, and optimizing Looker-based solutions that enable data visualization, accessibility, and actionable insights.\nKey Responsibilities:\nDesign and develop Looker dashboards to provide actionable insights and data visualization for stakeholders.\nImplement Looker Actions for seamless integration with workflows and business processes.\nManage and maintain data entry pipelines within Looker to ensure data accuracy and completeness.\nDevelop and maintain LookML models for efficient data analysis and exploration.\nWrite and optimize SQL queries to extract, transform, and load data into Looker.\nCollaborate with data analysts and business teams to ensure solutions meet user needs.\nMonitor Looker performance and troubleshoot any issues to ensure reliability.\nDocument workflows, LookML models, and dashboarding best practices.\nStay current with Looker updates and industry trends to implement new features effectively.\n-Immediate Joiners to 15 days Preferred\n-Job location- Remote\nThanks and Regards,\niitjobs, Inc.\nRegister for a global opportunity on the world's first & only Global Technology Job Portal: www.iitjobs.com\nDownload our app on the Apple App Store and Google Play Store!\nRefer and earn 50,000!","Looker Data Entry, Looker Dashboarding, Looker Actions, Looker, LookML, Sql Queries"
Senior Data Engineer - Fabric,Anblicks,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Details\nJob Summary:\nWe are seeking an experienced Senior Data Engineer with strong expertise in Microsoft Fabric to support our enterprise data modernization and analytics transformation initiatives. The ideal candidate will have a deep understanding of data pipelines, lakehouse architecture, Power BI, Synapse integration, and experience in modernizing legacy data systems to cloud-native solutions. This role is critical in building scalable, secure, and high-performing data solutions on the Microsoft ecosystem.\nKey Responsibilities:\nDesign and implement data pipelines using Microsoft Fabric s Data Factory, Synapse Data Engineering, and OneLake components.\nBuild and maintain lakehouse architectures leveraging Delta Lake, Parquet, and OneLake within Microsoft Fabric.\nLead initiatives to modernize legacy ETL/ELT processes to cloud-native data pipelines.\nWork closely with Data Architects, BI Developers, and Analysts to deliver scalable data models for analytics and reporting.\nOptimize performance of Power BI datasets and reports through best practices in data modeling and DAX.\nImplement data governance and security controls, including Microsoft Purview, role-based access, and lineage tracking.\nCollaborate with cross-functional teams in cloud migration, especially from on-premises SQL/Oracle/Hadoop platforms to Microsoft Azure & Fabric.\nEvaluate and implement CI/CD practices for data pipelines using Azure DevOps or GitHub Actions.\nRequired Skills & Qualifications:\nBachelor s/Master s degree in Computer Science, Information Systems, or related field.\n8+ years of experience in data engineering\nStrong hands-on experience with Microsoft Fabric components:\nData Factory\nLakehouse / OneLake\nSynapse Data Engineering\nPower BI\nExperience with data modeling (star/snowflake) and performance tuning in Power BI.\nDeep understanding of modern data architecture patterns including lakehouse, medallion architecture, and ELT frameworks.\nExpertise in SQL, PySpark, T-SQL, DAX, and Power Query (M language).\nExperience modernizing platforms from SSIS, Informatica, or Hadoop to cloud-native tools.\nFamiliarity with Azure ecosystem Azure Data Lake, Azure SQL DB, Azure Functions, Azure Synapse, Azure Data Factory.\nStrong experience in CI/CD pipelines, preferably with Azure DevOps.\nFamiliarity with data security, GDPR, HIPAA, and enterprise data governance.\nPreferred Qualifications:\nMicrosoft certifications such as:\nMicrosoft Certified: Fabric Analytics Engineer Associate\nAzure Data Engineer Associate (DP-203)\nExperience with DataOps and Agile delivery methods.\nKnowledge of Machine Learning/AI integration with Fabric is a plus.\nHands-on with Notebooks in Microsoft Fabric using Python or Scala.\nSoft Skills:\nStrong analytical and problem-solving skills.\nExcellent communication and stakeholder management capabilities.\nAbility to lead projects, mentor junior engineers, and collaborate with cross-functional teams.","GitHub Actions, Lakehouse, Microsoft Fabric, OneLake, CI CD, Azure SQL DB, Synapse, Power Query M language, Sql, Data Factory, T-sql, Azure Data Factory, Pyspark, Power Bi, Azure Functions, Dax, Azure Synapse, Azure Data Lake, Azure DevOps"
Data engineer,Qloron Pvt Ltd,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Senior Data Engineer\n\nExperience: Minimum 5+ Years\n\nEmployment Type: Full-Time\n\nJob Summary\n\nWe are seeking a skilled and experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in Power BI development, data modeling, and SQL, with the ability to translate business requirements into actionable insights. You will be responsible for end-to-end dashboard development, managing junior developers, and optimizing performance across reports and dashboards.\n\nKey Responsibilities\n\nAnalyze business requirements and translate them into data models and reporting solutions.\nPerform GAP analysis between existing data models and business needs.\nDesign and model efficient Power BI schemas and architecture.\nTransform and prepare data using Power BI, SQL, and ETL tools.\nDevelop complex DAX formulas, measures, and calculated columns for analytics.\nCreate visually appealing and functional Power BI reports and dashboards.\nWrite SQL queries and stored procedures to retrieve and manage data effectively.\nDesign robust Power BI solutions aligned with business objectives.\nLead and guide a team of Power BI developers, ensuring high-quality deliverables.\nIntegrate data from multiple sources into Power BI for holistic analysis.\nOptimize the performance of Power BI dashboards and reports.\nCollaborate with business stakeholders to align deliverables with strategic goals.\n\nRequired Skills\n\nMinimum 5+ years of hands-on experience with Power BI development.\nStrong proficiency in DAX, Power Query, and Power BI Service.\nExcellent command of SQL, including stored procedures.\nProven experience in data modeling, data transformation, and ETL processes.\nStrong understanding of data warehousing concepts (mandatory).\nExperience working with multiple data sources and integrating them within Power BI.\nLeadership capabilities to manage and mentor junior developers.\nSolid communication and stakeholder management skills.\n\nPreferred Qualifications\n\nBachelor's Degree in Computer Science, Information Technology, or equivalent.\nKnowledge of Data Engineering concepts is a plus.\nExperience with cloud platforms such as Azure or AWS is advantageous.","Data Modeling, Power Bi, Data Warehousing, Power Query, Dax, Sql, Etl"
"Manager 1, Domo Data Engineer",Kenvue,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nManager Domo Data Engineer\n\nWho We Are\n\nAt Kenvue , we realize the extraordinary power of everyday care. Built on over a century of heritage and rooted in science, we're the house of iconic brands - including Neutrogena, Aveeno, Tylenol, Listerine, Johnson's and BAND-AID Brand Adhesive Bandages that you already know and love. Science is our passion; care is our talent. Our global team is made up of 22,000 diverse and brilliant people, passionate about insights, innovation and committed to delivering the best products to our customers. With expertise and empathy, being a Kenvuer means having the power to impact the life of millions of people every day. We put people first, care fiercely, earn trust with science and solve with courage and have brilliant opportunities waiting for you! Join us in shaping our futureand yours. For more information, click here .\n\nWhat You Will Do\n\nAs a Data Engineer in the Global Analytics team, you will play a crucial role in designing, building, and maintaining scalable data pipelines and data models. You will work with cutting-edge technologies such as Snowflake, Python, Databricks and Azure Services to enable data-driven insights and support various analytics initiatives across the organization.\n\nKey Responsibilities\n\nData Pipeline Development- Design, develop, and maintain robust data pipelines to ingest, transform, and store data from diverse sources into Snowflake and DOMO ETL systems. other data storage solutions.\nStrong understanding and experience (5+ years) of DOMO ETL and best practice. Experience of designing and implementing data marts, data lakes or data warehouses using Domo\nData Modeling- Collaborate with data analysts and business stakeholders to create and optimize data models that meet analytical requirements and support reporting needs.\nETL Processes- Implement ETL processes using DOMO and other tools to ensure data integrity, quality, and availability for analytics and reporting.\nCollaboration- Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver relevant datasets for analysis.\nProactive approach to solution designing becoming a trusted advisor within the team.\nPerformance Optimization- Monitor and optimize the performance of data pipelines and data models, ensuring efficient processing and quick turnaround times.\nDocumentation- Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to support knowledge sharing and future development.\nCloud Infrastructure- Utilize Azure services to deploy and manage data solutions, ensuring security, scalability, and reliability.\nTroubleshooting- Identify and resolve data-related issues, ensuring data quality and accuracy across all analytics initiatives.\n\nWhat We Are Looking For\n\nRequired Qualifications-\n\nMust have-\n\nBachelor's degree in computer science, Information Technology, Data Science, or a related field.\nProven experience (8+ years) as a Data Engineer or in a similar role, with a strong understanding of data engineering concepts and best practices with at least 2+ in DOMO data engineering\nProficiency in Python for data manipulation, pipeline development, and automation.\nExperience with Snowflake, including data warehousing concepts and SQL for querying and managing data.\nFamiliarity with Databricks for data processing and analytics workflows.\nKnowledge of Azure cloud services and architecture, particularly in relation to data storage and processing.\nExperience with DBT (Data Build Tool) for transforming data and managing data models.\nStrong analytical and problem-solving skills, with attention to detail and a commitment to data quality.\nExcellent communication skills to collaborate effectively with technical and non-technical stakeholders.\nExperience with data visualization tools (e.g., Tableau, Power BI) for creating reports and dashboards.\nFamiliarity with orchestration tools.\nKnowledge of machine learning concepts and techniques.\n\nPrimary Location\n\nAsia Pacific-India-Karnataka-Bangalore\n\nJob Function\n\nDigital Product Development\n\nJob Qualifications\n\nNice to have-","snowflake, Azure Services, DBT Data Build Tool, Databricks, Sql, Power Bi, Python, Tableau, Domo"
Data Engineer,DELTACLASS TECHNOLOGY SOLUTIONS LIMITED,5-7 Years,,"Chennai, India",Login to check your skill match score,"Currently we are looking for Data Engieer role.\n\nResponsibilities\n\nRole :Data Engineer\n\nExp : 5+Yrs\n\nLocation: Hyderabad,Chennai\n\nNotice : Immediate to 15 Days\n\nJD\n\nData Engineer\n\nHyderabad/Chennai (Hybrid)\n\nFull-time with Info Services\n\nRequirement\n\nBS or higher degree in Computer Science (or equivalent field)\n3+ years of programming experience with Java and Python\nStrong in writing SQL queries and understanding of Kafka, Spark/Flink.\nExposure to AWS Lambda, AWS Cloud Watch, Step Functions, EC2, Cloud Formation, Jenkins\n3+ years of experience with Snowflake or Databricks\n\nBS or higher degree in Computer Science (or equivalent field)\n\n3+ years of programming experience with Java and Python\nStrong in writing SQL queries and understanding of Kafka, Spark/Flink.\nExposure to AWS Lambda, AWS Cloud Watch, Step Functions, EC2, Cloud Formation, Jenkins\n3+ years of experience with Snowflake or Databricks\n\nBS or higher degree in Computer Science (or equivalent field)\n\n3+ years of programming experience with Java and Python\nStrong in writing SQL queries and understanding of Kafka, Spark/Flink.\nExposure to AWS Lambda, AWS Cloud Watch, Step Functions, EC2, Cloud Formation, Jenkins\n3+ years of experience with Snowflake or Databricks\n\nQualifications\n\nAny Graduation Qualification is Fine.\n\nSkills: sql,cloud watch,,sql quries,ec2,snowflake,,spark,aws lambda,data engineer,snowflake,aws cloud watch,java,flink,step functions,jenkins,kafka,databricks,cloud formation,python,aws lamda,","Step Functions, Flink, AWS Cloud Watch, snowflake, Java, Aws Lambda, Cloud Formation, Kafka, Sql, Jenkins, Ec2, Spark, Databricks, Python"
Lead Snowflake Data Engineer,Ventra Health,7-9 Years,,"Chennai, India",Login to check your skill match score,"Ventra is a leading business solutions provider for facility-based physicians practicing anesthesia, emergency medicine, hospital medicine, pathology, and radiology. Focused on Revenue Cycle Management, Ventra partners with private practices, hospitals, health systems, and ambulatory surgery centers to deliver transparent and data-driven solutions that solve the most complex revenue and reimbursement issues, enabling clinicians to focus on providing outstanding care to their patients and communities.\n\nJob Summary\n\nWe are seeking an experienced Lead Snowflake Data Engineer to join our Data & Analytics team. This role involves designing, implementing, and optimizing Snowflake-based data solutions while providing strategic direction and leadership to a team of junior and mid-level data engineers. The ideal candidate will have deep expertise in Snowflake, cloud data platforms, ETL/ELT processes, and Medallion data architecture best practices. The lead data engineer role has a strong focus on performance optimization, security, scalability, and Snowflake credit control and management. This is a tactical role requiring independent in-depth data analysis and data discovery to understand our existing source systems, fact and dimension data models, and implement an enterprise data warehouse solution in Snowflake.\n\nEssential Functions And Tasks\n\n\nLead the design, development, and maintenance of a scalable Snowflake data solution serving our enterprise data & analytics team.\nArchitect and implement data pipelines, ETL/ELT workflows, and data warehouse solutions using Snowflake and related technologies.\nOptimize Snowflake database performance, storage, and security.\nProvide guidance on Snowflake best practices.\nCollaborate with cross-functional teams of data analysts, business analysts, data scientists, and software engineers, to define and implement data solutions.\nEnsure data quality, integrity, and governance across the organization.\nProvide technical leadership and mentorship to junior and mid-level data engineers.\nTroubleshoot and resolve data-related issues, ensuring high availability and performance of the data platform.\n\nEducation And Experience Requirements\n\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\n7+ years of experience in-depth data engineering, with at least 3+ minimum years of dedicated experience engineering solutions in a Snowflake environment.\nTactical expertise in ANSI SQL, performance tuning, and data modeling techniques.\nStrong experience with cloud platforms (preference to Azure) and their data services.\nProficiency in ETL/ELT development using tools such as Azure Data Factory, dbt, Matillion, Talend, or Fivetran.\nHands-on experience with scripting languages like Python for data processing.\nStrong understanding of data governance, security, and compliance best practices.\nSnowflake SnowPro certification; preference to the engineering course path.\nExperience with CI/CD pipelines, DevOps practices, and Infrastructure as Code (IaC).\nKnowledge of streaming data processing frameworks such as Apache Kafka or Spark Streaming.\nFamiliarity with BI and visualization tools such as PowerBI.\n\nKnowledge, Skills, And Abilities\n\n\nFamiliarity working in an agile scum team, including sprint planning, daily stand-ups, backlog grooming, and retrospectives.\nAbility to self-manage large complex deliverables and document user stories and tasks through Azure Dev Ops.\nPersonal accountability to committed sprint user stories and tasks.\nStrong analytical and problem-solving skills with the ability to handle complex data challenges.\nAbility to read, understand, and apply state/federal laws, regulations, and policies.\nAbility to communicate with diverse personalities in a tactful, mature, and professional manner.\nAbility to remain flexible and work within a collaborative and fast paced environment.\nUnderstand and comply with company policies and procedures.\nStrong oral, written, and interpersonal communication skills.\nStrong time management and organizational skills.\n\nVentra Health\n\n\nEqual Employment Opportunity (Applicable only in the US)\n\nVentra Health is an equal opportunity employer committed to fostering a culturally diverse organization. We strive for inclusiveness and a workplace where mutual respect is paramount. We encourage applications from a diverse pool of candidates, and all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, or veteran status. We will provide reasonable accommodations to qualified individuals with disabilities, as needed, to assist them in performing essential job functions.\n\nRecruitment Agencies\n\nVentra Health does not accept unsolicited agency resumes. Ventra Health is not responsible for any fees related to unsolicited resumes.\n\nSolicitation of Payment\n\nVentra Health does not solicit payment from our applicants and candidates for consideration or placement.\n\nAttention Candidates\n\nPlease be aware that there have been reports of individuals falsely claiming to represent Ventra Health or one of our affiliated entities Ventra Health Private Limited and Ventra Health Global Services. These scammers may attempt to conduct fake interviews, solicit personal information, and, in some cases, have sent fraudulent offer letters.\n\nTo protect yourself, verify any communication you receive by contacting us directly through our official channels. If you have any doubts, please contact us at [HIDDEN TEXT] to confirm the legitimacy of the offer and the person who contacted you. All legitimate roles are posted on https://ventrahealth.com/careers/.\n\nStatement of Accessibility\n\nVentra Health is committed to making our digital experiences accessible to all users, regardless of ability or assistive technology preferences. We continually work to enhance the user experience through ongoing improvements and adherence to accessibility standards. Please review at https://ventrahealth.com/statement-of-accessibility/.","DevOps practices, Matillion, dbt, snowflake, Fivetran, ELT, Spark Streaming, Azure Data Factory, Powerbi, Apache Kafka, Data Governance, Ansi Sql, Talend, Python, Etl"
Data Engineer-Specialized-Associate - Operate,PwC Acceleration Centers in India,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in managed services focus on a variety of outsourced solutions and support clients across numerous functions. These individuals help organisations streamline their operations, reduce costs, and improve efficiency by managing key processes and functions on their behalf. They are skilled in project management, technology, and process optimization to deliver high-quality services to clients. Those in managed service management and strategy at PwC will focus on transitioning and running services, along with managing delivery teams, programmes, commercials, performance and delivery risk. Your work will involve the process of continuous improvement and optimising of the managed services process, tools and services.\n\nKEY SKILLS - Mainframe, Teradata Datastage\n\nMainframe and Teradata DataStage Associate\n\nSummary:.\n\nMinimum Degree Required: Bachelor's degree in computer science/IT or relevant field\n\nDegree Preferred: Master's degree in computer science/IT or relevant field\n\nMinimum Years of Experience: 6 - 8 year(s)\n\nCertifications Required: NA\n\nRequired Knowledge/Skills: (character count limit 5000) *PLEASE ONLY USE THIS FIELD IF THIS IS A MUST HAVE SKILL FOR APPLICANT*\n\nJob Summary\n\nWe are seeking a skilled and experienced IT professional to join our team as a Mainframe and Teradata DataStage Associate. The successful candidate will be responsible for developing, maintaining, and optimizing ETL processes using IBM DataStage, as well as managing and supporting data operations on Mainframe and Teradata platforms.\n\nKey Responsibilities\n\nDesign, develop, and implement ETL processes using IBM DataStage to support data integration and transformation requirements.\nManage and maintain data on Mainframe and Teradata systems, ensuring data integrity and performance optimization.\nCollaborate with business analysts and stakeholders to understand data requirements and translate them into technical specifications.\nTroubleshoot and resolve issues related to ETL processes and data management on Mainframe and Teradata platforms.\nMonitor and tune the performance of ETL jobs and database queries to ensure optimal performance.\nDevelop and maintain documentation related to ETL processes, data flows, and system configurations.\nParticipate in code reviews and ensure adherence to best practices and coding standards.\nProvide support for data migration and integration projects, ensuring timely and accurate data delivery.\nStay updated with the latest developments in Mainframe, Teradata, and DataStage technologies and recommend improvements.\n\nQualifications\n\nJob Summary -\n\nA career in our Managed Services team will provide you with an opportunity to collaborate with a wide array of teams to help our clients implement and operate new capabilities, achieve operational efficiencies, and harness the power of technology. Our Data, Testing & Analytics as a Service team brings a unique combination of industry expertise, technology, data management and managed services experience to create sustained outcomes for our clients and improve business performance. We empower companies to transform their approach to analytics and insights while building your skills in exciting new directions. Have a voice at our table to help design, build and operate the next generation of software and services that manage interactions across all aspects of the value chain.\n\nMinimum Degree Required (BQ) *:\n\nBachelor's degree\n\nDegree Preferred\n\nRequired Field(s) of Study (BQ):\n\nPreferred Field(s) Of Study\n\nComputer and Information Science, Management Information Systems\n\nMinimum Year(s) of Experience (BQ) *: US\n\nCertification(s) Preferred\n\nMinimum of 1 year of experience\n\nPreferred Skills (PQs)\n\nPosition Requirements:\n\nDatasphere\n\nRequired Skills:\n\nMore than 2 years of hands-on experience in SAP Datasphere / DWC at least 1 full life cycle project implementation.\nWork on development/maintenance of DWC Models, CDS Views, SQL Scripts SAC Stories\nShould have experience in building complex models in SAP Datasphere/ DWC\nDeveloping SAP Datasphere end-to-end Dataflows Design, build data flows, and develop chains to load and monitor Data Loading.\nKnowledge in setting up the connections to Datasphere and from Datasphere.\nKnowledge in handling the delta in Datasphere.\nUnit testing the dataflows and reconciling the data to Source Systems.\nGood exposure in troubleshooting data issues and provide workarounds in cases where there are product limitations.\nGood exposure with Datasphere security setup, currency conversion.\nGood knowledge in writing CDS Analytical Queries and S4HANA Embedded Analytics.\nGood exposure in performance tuning of the models in the datasphere.\nGood knowledge on Datasphere and Data Lake integration.\nGood Knowledge on using the Database explorer and SAP Hana Cockpit through Datasphere.\n\nNice To Have\n\nGood knowledge in either BW Modeling or HANA Modeling.\nBW/4HANA And/or Native HANA (or HANA Cloud) modeling, including SQL Scripting, Graphical View-Modelling, SDA extraction.","DWC, SAP Datasphere, SAC Stories, Teradata, DataStage, Cds Views, Mainframe, Sql"
Data Engineer,Prachodayath Global Services Private Limited,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"Data Engineering is the core team for all decision making systems and responsible for creating bespoke data processing workflows to enable Mistplay AI products and advanced analytics. You will become a part of our core data engineering function focusing on driving operational excellence across all data stacks. You will work closely with our ML and Analytics teams to modernize our data platform and continuous innovation on data features. You will provide the technical ownership to help drive continuous improvement to our current data processing workflows and new lakehouse architecture.\n\nRole :- Senior Data Engineer\nExperience -7+\nWork Mode - Onsite\nFull-time, Permanent Role\nBudget - 11-12 LPA\n\nAs a Senior Data Engineer II you will be working closely with engineering, operations, and product to deploy new applications on our data lakehouse, refactoring legacy elements and design new features/data pipelines. This position requires someone who is passionate about data architecture design, big data technologies, deep technical proficiency in distributed data processing, real-time streaming, a strong problem-solver, a team collaborator and has a growth mindset.\n\nWhat You'll Do\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nCollaborate with development, AI, and data science teams to integrate data solutions into complex enterprise systems, ensuring seamless interoperability with existing platforms.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nAnalyze and interpret trends in complex data sets, utilizing visualization tools (e.g., Tableau, Power BI) to create dashboards and reports that tell compelling data stories.\nCreate and maintain standardized operational tools and reporting mechanisms to communicate data health and business performance to various audiences, including executives.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nAutomate data analyses and authoring pipelines using tools such as Kinesis, Airflow, Lambda, Databricks, DBT, and other relevant technologies.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\nWhat You'll Bring\nBachelor's degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nInterested candidates share your resume at [HIDDEN TEXT]","Airflow, Beam, dbt, Java, Apache Flink, Scala, Redis, Lambda, Git, Kinesis, Terraform, Spark, Databricks, Python, Kubernetes"
DBT Data Engineer - Remote,Cognisol,6-8 Years,,"Chennai, India",Login to check your skill match score,"Key Skillset-DBT,Python,SQL,AWS,pYSPARK\n\nYears of Exp- 6 to 7 Years\n\nWork Mode-Work From Home(Candidate should be available for 1st week of Onboarding @ Chennai Loc)\n\nShift Time-UK Shift time\n\nNotice: Immediate to 15 days only\n\nPlacement Type: Contractual Position\n\nKey Responsibilities\n\nData Pipeline Development: Design, implement, and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from various sources into cloud platforms like AWS.\nCloud Integration: Utilize AWS services such as Glue, Step Functions, Redshift, and Lambda to process and store data efficiently.\nData Transformation: Develop and maintain accurate data pipelines using Python and SQL, transforming data for aggregations, wrangling, quality control, and calculations.\nWorkflow Automation: Enhance end-to-end workflows with automation tools to accelerate data flow and pipeline management.\nCollaboration: Work closely with business analysts, data scientists, and cross-functional teams to understand data requirements and develop solutions that support data-driven decision-making.\n\nQualifications\n\nEducational Background: Bachelor's or Master's degree in Information Technology, Bioinformatics, Computer Science, or a related field.\nProfessional Experience: Several years of experience in data engineering, with hands-on expertise in:\nDeveloping and managing large-scale ETL data pipelines on AWS.\nProficiency in Python and SQL for data pipeline development.\nUtilizing AWS services such as Glue, Step Functions, Redshift, and Lambda.\nFamiliarity with tools like Docker, Linux Shell Scripting, Pandas, PySpark, and Numpy.\nSoft Skills: Strong problem-solving abilities, excellent communication skills, and the capacity to work collaboratively in a dynamic environment.\nSkills: etl,linux shell scripting,pyspark,docker,pipelines,sql,aws,redshift,glue,pipeline,cloud,python,lambda,dbt,step functions,pandas,numpy","dbt, Glue, Pyspark, Shell Scripting, Redshift, Sql, Lambda, Numpy, Pandas, Linux, Docker, Python, AWS"
AWS Data Engineer (DBT),Artefact,3-9 Years,,"Pune, India",Login to check your skill match score,"Job Title AWS Data Engineer + DBT (Data Build Tool)\n1) Min Exp 3-5 years\n2) Min Exp - 5-9 years\nKey Responsibilities:\nWork with cross-functional teams to build and maintain data pipelines and project infrastructure\nManage end-to-end data engineering tasks, including data ingestion, transformation, and integration with the data lake\nEnsure data quality and perform data transformations within the AWS ecosystem\nSupport CDP-related initiatives, particularly involving AWS cloud services and popular marketing and analytics platforms\nRequired Skills and Experience:\nAWS Data Engineering: Hands-on experience with AWS S3, Glue, Lambda, Redshift and other AWS services.\nDBT: Experience with DBT to make activities accessible to people with data analytical skills.\nSQL: Ability to write and optimize complex SQL queries\nPython: Experience in Python for data processing and automation tasks\nVersion Control: Knowledge of tools like Jenkins and Bitbucket for CI/CD\nAdditional Skills:\nData Modelling: Experience in creating and maintaining data models for large-scale environments\nBig Data Technologies: Familiarity with Databricks and other big data processing frameworks\nWorkflow Orchestration: Familiarity with Apache Airflow or AWS Step Functions for managing complex data workflows\nQualifications:\nProven experience in delivering data engineering projects, ideally within large organizations\nAbility to take ownership of tasks and work independently in an individual contributor role\nNo strict educational background requirements, but strong hands-on experience in delivering projects with diverse teams is essential","AWS Step Functions, dbt, Apache Airflow, Jenkins, Bitbucket, Databricks, Sql, Python, Aws S3"
Senior Data Engineer (SQL and ADF),thinkbridge,5-7 Years,,India,Login to check your skill match score,"We are hiring a Senior Data Engineer with 5+ years of experience in SQL , who has good hands-on in ADF and ETL. Good communication is mandatory.\nAbout thinkbridge\nthinkbridge is how growth-stage companies can finally turn into tech disruptors. They get a new way there with world-class technology strategy, development, maintenance, and data science all in one place. But solving technology problems like these involves a lot more than code. That's why we encourage thinkers to spend 80% of their time thinking through solutions and 20% coding them. With an average client tenure of 4+ years, you won't be hopping from project to project here unless you want to. So, you really can get to know your clients and understand their challenges on a deeper level. At thinkbridge, you can expand your knowledge during work hours specifically reserved for learning. Or even transition to a completely different role in the organization. It's all about challenging yourself while you challenge small thinking.\nthinkbridge is a place where you can:\nThink bigger because you have the time, opportunity, and support it takes to dig deeper and tackle larger issues.\nMove faster because you'll be working with experienced, helpful teams who can guide you through challenges, quickly resolve issues, and show you new ways to get things done.\nGo further because you have the opportunity to grow professionally, add new skills, and take on new responsibilities in an organization that takes a long-term view of every relationship.\nthinkbridge.. there's a new way there.\nWhat is expected of you\nAs part of the job, you will be required to\nRead everything in detail that comes your way.\nElicit, analyze, specify & validate business requirements.\nDefine & Document the engineering requirements including architecture, components, usable interfaces & other characteristics.\nPlace specific emphasis on technical and usability design.\nCode and verify the solution.\nDebug & squash bugs.\nKeep stakeholders informed.\nKeep up with the technology and the domain.\nIf your beliefs resonate with these, you are looking at the right place!\nAccountability Finish what you started.\nCommunication Context-aware, pro-active and clean communication.\nOutcome High throughput.\nQuality High-quality work and consistency.\nOwnership Go beyond.\nRequirements\nMust-Have:\nShould have hands-on experience in writing SQL Queries and Stored Procedures.\nExcellent Communication.\nShould have experience in ADF (Azure Data Factory)\nGood experience in building ETL Solutions for large datasets\nGood to have:\nGood to have SSIS Experience\nOther Details :\nRemote First\nFlexible work hours\nNo loss of pay for pre-approved leaves\nFamily Insurance\nQuarterly in-person Collaboration Week -WWW","Adf, SSIS, Sql, Etl"
ETL Data Engineer,QTek Digital,6-9 Years,,"Hyderabad, India",Login to check your skill match score,"Job description\nCompany Description\nQTek Digital is a data solutions provider specializing in custom solutions in data management, data warehouse, and data science. Our team of data scientists, data analysts, and data engineers work together to solve today's challenges and unlock future possibilities. As an employee-centric company, we prioritize engagement, empowerment, and enrichment of our employees.\nRole Description\nThis is a full-time remote role for a BI ETL Engineer at QTek Digital. As a BI ETL Engineer, you will be responsible for day-to-day tasks such as data modeling, utilizing analytical skills, implementing data warehousing solutions, and executing Extract, Transform, Load (ETL) processes. This role requires strong problem-solving abilities and the ability to work independently.\nQualifications\n6-9 years of experience with ETL and ELT pipeline build out using Pentaho, SSIS, FiveTran, Airbyte, or similar tools.\n6-8 Years of experience in SQL and data manipulation languages\nStrong Data Modeling, Dashboard, and Analytical Skills\nExcellent understanding of data warehousing concepts, esp. Kimball design.\nExperience with Pentaho and Airbyte administration will be a huge plus.\nStrong skills in Data Modeling, Dashboard design, and Analytics\nExperience in Data Warehousing and Extract Transform Load (ETL) processes\nStrong problem-solving and troubleshooting skills\nExcellent communication and collaboration skills\nAbility to work independently and in a team\nBachelor's degree in computer science, Information Systems, or a related field\nThis role is based onsite in our Hyderabad Office.\nThe compensation range for this role is INR 5-19 Lakhs depending on a variety of factors including skills and experience.","Airbyte, FiveTran, Pentaho, Data Modeling, Data Warehousing, SSIS, Sql, ELT, Etl"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description\nOncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.\nJob Type:Full-time or Contract\nStart Date: June 1st 2025\nRole Description\nThis is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.\nJob Description:\nCandidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively\n6+ Years of experience as a Data Engineer\nStrong technical expertise in SQL and Snowflake is a must.\nStrong knowledge of joins and common table expressions (CTEs)\nStrong experience with Python\nStrong expertise in ETL process and with various data model concepts\nKnowledge of star schema and snowflake schema\nGood to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue\nExperience with Big Data Tools and technologies\nKey Skills:\nGood Understanding of data structures and data analysis using SQL\nKnowledge of implementing ETL/ELT for data solutions end-to-end\nUnderstanding requirements, and data solutions (ingest, storage, integration, processing)\nKnowledge of analyzing data using SQL\nConducting End to End verification and validation for the entire application\nResponsibilities:\nUnderstand and translate business needs into data models supporting long-term solutions.\nPerform reverse engineering of physical data models from databases and SQL scripts.\nAnalyze data-related system integration challenges and propose appropriate solutions.\nAssist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
Data Engineer - Senior Associate,PwC Acceleration Centers in India,5-8 Years,,"Hyderabad, India",Login to check your skill match score,"At PwC, our people in data management focus on organising and maintaining data to enable accuracy and accessibility for effective decision-making. These individuals handle data governance, quality control, and data integration to support business operations. In data governance at PwC, you will focus on establishing and maintaining policies and procedures to optimise the quality, integrity, and security of data. You will be responsible for optimising data management processes and mitigate risks associated with data usage.\n\nFocused on relationships, you are building meaningful client connections, and learning how to manage and inspire others. Navigating increasingly complex situations, you are growing your personal brand, deepening technical expertise and awareness of your strengths. You are expected to anticipate the needs of your teams and clients, and to deliver quality. Embracing increased ambiguity, you are comfortable when the path forward isn't clear, you ask questions, and you use these moments as opportunities to grow.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\nRespond effectively to the diverse perspectives, needs, and feelings of others.\nUse a broad range of tools, methodologies and techniques to generate new ideas and solve problems.\nUse critical thinking to break down complex concepts.\nUnderstand the broader objectives of your project or role and how your work fits into the overall strategy.\nDevelop a deeper understanding of the business context and how it is changing.\nUse reflection to develop self awareness, enhance strengths and address development areas.\nInterpret data to inform insights and recommendations.\nUphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.\n\nJob Description/Activities To Be Performed\n\nWorking comprehensively with Azure Data Factory and Azure Cloud technologies with multiple data source systems on regular basis.\nCreate and maintain optimal data pipelines in - On Premise SQL Server Infrastructure and Azure Cloud Environments.\nResponsible for expanding and optimizing data and data pipeline architecture as well as optimizing data flow and collection for cross functional utilization.\nDesign, develop and deploy high volume ETL pipelines to manage complex and near-real time data collection\nExpertise working with SQL Server 2005/2008 R2/2012 tools Such as Management Studio, Query Analyzer, SQL Profiler, SQL Agent, SSIS and SSRS.\nDevelop complex queries using sub queries and multiple joins.\nWork with the data management team and project leads to assist with data related technical issues, data analysis and support infrastructure needs\nMonitor Data Quality Controls/Remediate\nData governance, version management, CI/CD deployments\n\nJob Qualifications\n\n5 to 8 years of core hands on experience as a Data Engineer, developing, maintaining and optimising data pipelines in Azure and SQL Server infrastructure.\nHand on with MS-SQL Server 2012/2008 R2/2008/2005, SQL Server Enterprise Manager, Transact-SQL T-SQL\nCore expertise in Azure Data Factory (ADF), and SQL Server Integration Services SSIS\nHandle common database procedures such as upgrade, backup, recovery, migration, etc.\nAdvanced working knowledge and experience with relational databases and database administration/ management\nAbility to analyse and anticipate client requests, interpret asks and act according to expectations\nB Tech/M Tech M Sc (Math/ Stats) or equivalent from a premier institute\nMust have 4 to 6 years of on-the-job experience of working in Data Engineer profiles","Azure Cloud technologies, Transact-SQL, SQL Agent, Management Studio, T-sql, Ssrs, Sql Server 2005, SSIS, Sql Server 2012, Sql Profiler, Azure Data Factory, Query Analyzer"
Senior Principal Data Engineer,MakeMyTrip,10-12 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Position: Senior Principal Data Engineer\nExperience: Must have 10+ years of experience\nAbout Role:\nWe are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.\nAbout us:\nNurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.\nPost-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.\nGO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.\nAbout the team:\nMakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.\nData Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.\nThere are two key responsibilities of Data Engineering team:\nOne to develop the platform for data capture, storage, processing, serving and querying.\nSecond is to develop data products starting from;\no personalization & recommendation platform\no customer segmentation & intelligence\no data insights engine for persuasions and\no the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users\nWe developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.\nTechnology experience:\nExtensive experience working with large data sets with hands-on technology skills to design and build robust data architecture\nExtensive experience in data modeling and database design\nAt least 6+ years of hands-on experience in Spark/BigData Tech stack\nStream processing engines Spark Structured Streaming/Flink\nAnalytical processing on Big Data using Spark\nAt least 6+ years of experience in Scala\nHands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems\nAt least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform\nAt least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc\nAwareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies\nShould have experience in architecting and implementing domain centric big data solutions\nAbility to frame architectural decisions and provide technology leadership & direction\nExcellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Senior Principal Data Engineer,MakeMyTrip,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Senior Principal Data Engineer\nExperience: Must have 10+ years of experience\nAbout Role:\nWe are looking for experienced Data engineers with excellent problem-solving skills to develop machine-learning powered Data Products design to enhance customer experiences.\nAbout us:\nNurtured from the seed of a single great idea - to empower the traveler - MakeMyTrip went on to pioneer India's online travel industry Founded in the year 2000 by Deep Kalra, MakeMyTrip has since transformed how India travels. One of our most memorable moments has been to ring the bell at NASDAQ in 2010.\nPost-merger with the Ibibo group in 2017, we created a stronger identity and traction for our portfolio of brands, increasing the pace of product and technology innovations. Ranked amongst the LinkedIn Top 25 companies 2018.\nGO-MMT is the corporate entity of three giants in the Online Travel IndustryGoibibo, MakeMyTrip and RedBus. The GO-MMT family celebrates the compounded strengths of their brands. The group company is easily the most sought after corporate in the online travel industry.\nAbout the team:\nMakeMyTrip as India's leading online travel company and provides petabytes of raw data which is helpful for business growth, analytical and machine learning needs.\nData Platform Team is a horizontal function at MakeMyTrip to support various LOBs (Flights, Hotels, Holidays, Ground) and works heavily on streaming datasets which powers personalized experiences for every customer from recommendations to in-location engagement.\nThere are two key responsibilities of Data Engineering team:\nOne to develop the platform for data capture, storage, processing, serving and querying.\nSecond is to develop data products starting from;\no personalization & recommendation platform\no customer segmentation & intelligence\no data insights engine for persuasions and\no the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users\nWe developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.\nTechnology experience:\nExtensive experience working with large data sets with hands-on technology skills to design and build robust data architecture\nExtensive experience in data modeling and database design\nAt least 6+ years of hands-on experience in Spark/BigData Tech stack\nStream processing engines Spark Structured Streaming/Flink\nAnalytical processing on Big Data using Spark\nAt least 6+ years of experience in Scala\nHands-on administration, configuration management, monitoring, performance tuning of Spark workloads, Distributed platforms, and JVM based systems\nAt least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform\nAt least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc\nAwareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies\nShould have experience in architecting and implementing domain centric big data solutions\nAbility to frame architectural decisions and provide technology leadership & direction\nExcellent problem solving, hands-on engineering, and communication skills","Spark Structured Streaming, NoSQL databases, Flink, Google Cloud Platform, Scala, Spark, Azure, AWS"
Big Data Engineer-4+ Years,Cortex Consultants LLC,4-7 Years,,"Chennai, India",Login to check your skill match score,"Total IT / development experience of 3+ years\n\nExperience in Spark (Scala-Spark or PySpark) developing Big Data applications on Hadoop, Hive and/or Kafka, HBase, MongoDB+B4+A3:B5d technology strategies\nExposure to deploying on Cloud platforms\nAt least 2 years of development experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Spark-Scala/PySpark\nAt least 2 years of development experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS\nAt least 2 years of developing applications in Agile with Monitoring, Build Tools, Version Control, Unit Test, Unix Shell Scripting, TDD, CI/CD, Change Management to support DevOps\n\nGOOD-TO-HAVE\n\nBanking domain knowledge\nHands-on experience in SAS toolset / statistical modelling migrating to Machine Learning models\nBanking Risk, Fraud or Digital Marketing Machine Learning models and use cases\nETL / Data Warehousing, SQL and Data Modelling experience prior to Big Data experience Location Chennai / Bangalore / Pune Experience 4-7 Yrs\n\nSkills: pyspark,ci/cd,machine learning,mongodb,data pipelines,unit test,data,unix shell scripting,hadoop,hive,kafka,learning,agile,spark,management,scala-spark,change management,machine learning models,a3,hbase,tdd,scala,big data","data pipelines, scala-spark, Management, ci cd, machine learning models, Hive, Machine Learning, Scala, Pyspark, change management, Unix Shell Scripting, Unit Test, Tdd, Spark, Kafka, hadoop, agile, mongodb, hbase, Big Data"
Data Engineer - Looker,"iitjobs, Inc.",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"We have an great opportunity for the role of Data Engineer- Looker.\nMandatory Skills : Looker Action, Looker Dashboarding, Looker Data Entry, LookML, SQL Queries.\nRelevant Exp : 6+ Yrs\nJob Summary:\nWe are seeking a skilled Data Engineer with deep expertise in Looker, including Looker Actions, Dashboarding, Data Entry, LookML, and SQL Queries. You will play a key role in designing, implementing, and optimizing Looker-based solutions that enable data visualization, accessibility, and actionable insights.\nKey Responsibilities:\nDesign and develop Looker dashboards to provide actionable insights and data visualization for stakeholders.\nImplement Looker Actions for seamless integration with workflows and business processes.\nManage and maintain data entry pipelines within Looker to ensure data accuracy and completeness.\nDevelop and maintain LookML models for efficient data analysis and exploration.\nWrite and optimize SQL queries to extract, transform, and load data into Looker.\nCollaborate with data analysts and business teams to ensure solutions meet user needs.\nMonitor Looker performance and troubleshoot any issues to ensure reliability.\nDocument workflows, LookML models, and dashboarding best practices.\nStay current with Looker updates and industry trends to implement new features effectively.\n-Immediate Joiners to 15 days Preferred\n-Job location- Remote\nThanks and Regards,\niitjobs, Inc.\nRegister for a global opportunity on the world's first & only Global Technology Job Portal: www.iitjobs.com\nDownload our app on the Apple App Store and Google Play Store!\nRefer and earn 50,000!","Looker Data Entry, Looker Dashboarding, Looker Actions, Looker, LookML, Sql Queries"
Senior Data Engineer - Fabric,Anblicks,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Details\nJob Summary:\nWe are seeking an experienced Senior Data Engineer with strong expertise in Microsoft Fabric to support our enterprise data modernization and analytics transformation initiatives. The ideal candidate will have a deep understanding of data pipelines, lakehouse architecture, Power BI, Synapse integration, and experience in modernizing legacy data systems to cloud-native solutions. This role is critical in building scalable, secure, and high-performing data solutions on the Microsoft ecosystem.\nKey Responsibilities:\nDesign and implement data pipelines using Microsoft Fabric s Data Factory, Synapse Data Engineering, and OneLake components.\nBuild and maintain lakehouse architectures leveraging Delta Lake, Parquet, and OneLake within Microsoft Fabric.\nLead initiatives to modernize legacy ETL/ELT processes to cloud-native data pipelines.\nWork closely with Data Architects, BI Developers, and Analysts to deliver scalable data models for analytics and reporting.\nOptimize performance of Power BI datasets and reports through best practices in data modeling and DAX.\nImplement data governance and security controls, including Microsoft Purview, role-based access, and lineage tracking.\nCollaborate with cross-functional teams in cloud migration, especially from on-premises SQL/Oracle/Hadoop platforms to Microsoft Azure & Fabric.\nEvaluate and implement CI/CD practices for data pipelines using Azure DevOps or GitHub Actions.\nRequired Skills & Qualifications:\nBachelor s/Master s degree in Computer Science, Information Systems, or related field.\n8+ years of experience in data engineering\nStrong hands-on experience with Microsoft Fabric components:\nData Factory\nLakehouse / OneLake\nSynapse Data Engineering\nPower BI\nExperience with data modeling (star/snowflake) and performance tuning in Power BI.\nDeep understanding of modern data architecture patterns including lakehouse, medallion architecture, and ELT frameworks.\nExpertise in SQL, PySpark, T-SQL, DAX, and Power Query (M language).\nExperience modernizing platforms from SSIS, Informatica, or Hadoop to cloud-native tools.\nFamiliarity with Azure ecosystem Azure Data Lake, Azure SQL DB, Azure Functions, Azure Synapse, Azure Data Factory.\nStrong experience in CI/CD pipelines, preferably with Azure DevOps.\nFamiliarity with data security, GDPR, HIPAA, and enterprise data governance.\nPreferred Qualifications:\nMicrosoft certifications such as:\nMicrosoft Certified: Fabric Analytics Engineer Associate\nAzure Data Engineer Associate (DP-203)\nExperience with DataOps and Agile delivery methods.\nKnowledge of Machine Learning/AI integration with Fabric is a plus.\nHands-on with Notebooks in Microsoft Fabric using Python or Scala.\nSoft Skills:\nStrong analytical and problem-solving skills.\nExcellent communication and stakeholder management capabilities.\nAbility to lead projects, mentor junior engineers, and collaborate with cross-functional teams.","GitHub Actions, Lakehouse, Microsoft Fabric, OneLake, CI CD, Azure SQL DB, Synapse, Power Query M language, Sql, Data Factory, T-sql, Azure Data Factory, Pyspark, Power Bi, Azure Functions, Dax, Azure Synapse, Azure Data Lake, Azure DevOps"
Lead Data Engineer,M&G Global Services Private Limited,12-14 Years,,"Mumbai, India",Login to check your skill match score,"We are M&G Global Services Private Limited (formerly known as 10FA India Private Limited, and prior to that Prudential Global Services Private Limited). We are a fully owned subsidiary of the M&G plc group of companies, operating as a Global Capability Centre providing a range of value adding services to the Group since 2003. At M&G our purpose is to give everyone real confidence to put their money to work. As an international savings and investments business with roots stretching back more than 170 years, we offer a range of financial products and services through Asset Management, Life and Wealth. All three operating segments work together to deliver attractive financial outcomes for our clients, and superior shareholder returns.\n\nM&G Global Services has rapidly transformed itself into a powerhouse of capability that is playing an important role in M&G plc's ambition to be the best loved and most successful savings and investments company in the world.\n\nOur diversified service offerings extending from Digital Services (Digital Engineering, AI, Advanced Analytics, RPA, and BI & Insights), Business Transformation, Management Consulting & Strategy, Finance, Actuarial, Quants, Research, Information Technology, Customer Service, Risk & Compliance and Audit provide our people with exciting career growth opportunities. Through our behaviours of telling it like it is, owning it now, and moving it forward together with care and integrity; we are creating an exceptional place to work for exceptional talent.\n\nJob Description\n\nJob Title\n\nLead Data Engineer\n\nGrade\n\n2B\n\nLevel\n\nSenior Manager Data\n\nJob Function\n\nDigital Transformation\n\nJob Sub Function\n\nAzure Data Engineering & DevOps & BI\n\nReports to\n\n3B (VP Data Engineering)\n\nLocation\n\nMumbai\n\nBusiness Area\n\nM&G Global Services\n\nOverall Job Purpose\n\nTo implement data engineering solutions using latest technologies available in Azure Cloud space conforming to the best in class design standard & agreed requirements to achieve business objective\n\nAccountabilities/Responsibilities\n\nLead data engineering projects to build and operationalize data solutions for business using Azure services in combination with custom solutions Azure Data Factory, Azure Data Flows, Azure Databricks, Azure Data Lake Gen 2, Azure SQL etc\nProven experience on leading a team of data engineers providing technical guidance and ensuring alignment with agreed architectural principles\nExperience in migrating on-premise data warehouses to data platforms on AZURE cloud\nDesigning and implementing data engineering, ingestion and transformation functions using ADF, Databricks\nProficient in Py-Spark\nExperience in building Python based APIs on Azure Function Apps\nExperience on Azure Logic apps\nExperience in Lakehouse/Datawarehouse implementation using modern data platform architecture\nCapacity Planning and Performance Tuning on ADF & Databricks pipelines\nSupport data visualization development using Power BI\nExposure across all the SDLC process, including testing and deployment\nExperience in relational and dimensional modelling, including big data technologies\nExperience in Azure DevOps Build CI/CD pipelines for ADF, ADLS, Databricks, Azure SQL DB etc\nExperience of working in secured Azure environments using Azure KeyVaults, Service Principals, and Managed Identities\nGood to have knowledge on Apigee (Googles API Management)\nUnderstanding of data masking, encryption and other practices used in handling sensitive data\nAbility to interact with Business for requirement gathering and query resolutions\nWorking on off shore office based development teams, collaborating within a team environment and participating in typical project lifecycle activities such as requirement analysis, testing and release\nDevelop Azure Data skills within the team through knowledge sharing sessions, articles, etc.\nAdherence to organisations Risk & Controls requirements\nShould have skills for Stakeholder management, process adherence, planning & documentationss\n\nKey Stakeholder Management\n\nInternal\n\nBusiness Teams\n\nProject Manager\n\nArchitects\n\nData Scientists\n\nTeam members\n\nExternal\n\nKnowledge, Skills, Experience & Educational Qualification\n\nKnowledge & Skills:\n\nAzure Data Factory,\nAzure Data Lake Storage V2\nAzure SQL\nAzure DataBricks\nPyspark\nAzure DevOps\nPower BI Report\nTechnical leadership\nConfidence & excellent communication\n\nExperience:\n\nOverall 12+ years of experience\n5 + Experience on Azure data engineering\n5 + experience of managing data deliveries\n\nEducational Qualification:\n\nGraduate/Post-graduate. Preferably with specialisation in Computer Science, Statistics, Mathematics, Data Science, Engineering or related discipline\n\nMicrosoft Azure certification (good to have)\n\nM&G Behaviours relevant to all roles:\n\nInspire Others: support and encourage each other, creating an environment where everyone can contribute and succeed\n\nEmbrace Change: be open to change, willing to be challenged and able to adapt quickly and imaginatively to new ideas\n\nDeliver Results: focus on performance, set high standards and deliver with energy and determination\n\nKeep it simple: cut through complexity, keep the outcome in mind, keeping your approach simple and adapting your message to every audience\n\nWe have a diverse workforce and an inclusive culture at M&G Global Services, regardless of gender, ethnicity, age, sexual orientation, nationality, disability or long term condition, we are looking to attract, promote and retain exceptional people. We also welcome those who take part in military service and those returning from career breaks.","Azure Data Lake Storage V2, Azure Function Apps, Power Bi, Azure Sql, Azure Data Factory, Azure Databricks, Azure Logic Apps, Pyspark, Azure DevOps"
Data Engineer(AWS),goML,3-7 Years,,"Delhi, India",Login to check your skill match score,"About the Role\nWe're looking for a skilled Data Engineer (AWS) with a strong foundation in ETL, data warehousing, and integration to join our team at goML. In this remote role, you'll be responsible for delivering robust and scalable data solutions across multiple client projects.\nKey Responsibilities\nLead the design and development of data solutions for various client engagements.\nApply development best practices (e.g., agile methodologies, unit testing, peer reviews) to ensure quality and timely delivery.\nCollaborate across teamsfrom requirement gathering through deploymentto ensure smooth execution and handoff.\nTranslate business requirements into technical specifications and guide development teams for accurate implementation.\nCreate comprehensive project artifacts including solution designs, technical documentation, test plans, and deployment strategies.\nDrive user onboarding and change management efforts during project rollouts.\nRequirements\n37 years of relevant experience, preferably in consulting or large-scale tech solution delivery.\nStrong understanding of ETL processes, data warehousing, and data integration.\nProficient in SQL/PL SQL and database development.\nExperience designing scalable and flexible data solutions based on business needs.\nBachelor's or master's in computer science, Information Systems, or related fields.\nHands-on experience with backend database systems (e.g., Oracle) and/or ETL tools (e.g., Informatica).\nHands on Experience in AWS.","ETL processes, backend database systems, Data Warehousing, Pl Sql, Etl Tools, Informatica, Data Integration, Oracle, Sql, Database Development, AWS"
"Manager 1, Domo Data Engineer",Kenvue,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nManager Domo Data Engineer\n\nWho We Are\n\nAt Kenvue , we realize the extraordinary power of everyday care. Built on over a century of heritage and rooted in science, we're the house of iconic brands - including Neutrogena, Aveeno, Tylenol, Listerine, Johnson's and BAND-AID Brand Adhesive Bandages that you already know and love. Science is our passion; care is our talent. Our global team is made up of 22,000 diverse and brilliant people, passionate about insights, innovation and committed to delivering the best products to our customers. With expertise and empathy, being a Kenvuer means having the power to impact the life of millions of people every day. We put people first, care fiercely, earn trust with science and solve with courage and have brilliant opportunities waiting for you! Join us in shaping our futureand yours. For more information, click here .\n\nWhat You Will Do\n\nAs a Data Engineer in the Global Analytics team, you will play a crucial role in designing, building, and maintaining scalable data pipelines and data models. You will work with cutting-edge technologies such as Snowflake, Python, Databricks and Azure Services to enable data-driven insights and support various analytics initiatives across the organization.\n\nKey Responsibilities\n\nData Pipeline Development- Design, develop, and maintain robust data pipelines to ingest, transform, and store data from diverse sources into Snowflake and DOMO ETL systems. other data storage solutions.\nStrong understanding and experience (5+ years) of DOMO ETL and best practice. Experience of designing and implementing data marts, data lakes or data warehouses using Domo\nData Modeling- Collaborate with data analysts and business stakeholders to create and optimize data models that meet analytical requirements and support reporting needs.\nETL Processes- Implement ETL processes using DOMO and other tools to ensure data integrity, quality, and availability for analytics and reporting.\nCollaboration- Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver relevant datasets for analysis.\nProactive approach to solution designing becoming a trusted advisor within the team.\nPerformance Optimization- Monitor and optimize the performance of data pipelines and data models, ensuring efficient processing and quick turnaround times.\nDocumentation- Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to support knowledge sharing and future development.\nCloud Infrastructure- Utilize Azure services to deploy and manage data solutions, ensuring security, scalability, and reliability.\nTroubleshooting- Identify and resolve data-related issues, ensuring data quality and accuracy across all analytics initiatives.\n\nWhat We Are Looking For\n\nRequired Qualifications-\n\nMust have-\n\nBachelor's degree in computer science, Information Technology, Data Science, or a related field.\nProven experience (8+ years) as a Data Engineer or in a similar role, with a strong understanding of data engineering concepts and best practices with at least 2+ in DOMO data engineering\nProficiency in Python for data manipulation, pipeline development, and automation.\nExperience with Snowflake, including data warehousing concepts and SQL for querying and managing data.\nFamiliarity with Databricks for data processing and analytics workflows.\nKnowledge of Azure cloud services and architecture, particularly in relation to data storage and processing.\nExperience with DBT (Data Build Tool) for transforming data and managing data models.\nStrong analytical and problem-solving skills, with attention to detail and a commitment to data quality.\nExcellent communication skills to collaborate effectively with technical and non-technical stakeholders.\nExperience with data visualization tools (e.g., Tableau, Power BI) for creating reports and dashboards.\nFamiliarity with orchestration tools.\nKnowledge of machine learning concepts and techniques.\n\nPrimary Location\n\nAsia Pacific-India-Karnataka-Bangalore\n\nJob Function\n\nDigital Product Development\n\nJob Qualifications\n\nNice to have-","snowflake, Azure Services, DBT Data Build Tool, Databricks, Sql, Power Bi, Python, Tableau, Domo"
Senior Data Engineer Analyst,IMRIEL Technology Solutions Private Ltd,3-5 Years,,"Pune, India",Login to check your skill match score,"We are seeking a skilled Data Engineer with expertise in maintaining scalable semantic models using AtScale and cloud-based data warehouse platforms.\nWe at IMRIEL (An Allata Company) are looking for experienced and technically strong Analytics Data Engineers to design, build, and maintain scalable semantic models using AtScale and cloud-based data warehouse platforms. This role involves developing logical cubes, defining MDX-based business measures, and enabling governed, self-service BI consumption for enterprise analytics.\nExperience:3 to 5 years.\nLocation:Vadodara & Pune\nWhat you'll be doing:\nDesign and implement robust semantic data models using AtScale that abstract curated datasets into business-consumable layers.\nConduct a comprehensive POC to evaluate three potential semantic layer platforms: AtScale, Microsoft Fabric and Cube.dev. This includes assessing their performance, scalability, and integration with cloud-based platforms like Databricks, Snowflake, etc.\nDevelop and maintain logical cubes with calculated measures, dimension hierarchies, and drill-down paths to support self-service analytics.\nLeverage MDX (Multidimensional Expressions) to define advanced business logic, KPIs, and aggregations aligned with enterprise reporting needs.\nConfigure and manage aggregate tables using AtScale Aggregate Designer, optimizing cube performance and reducing query latency.\nIntegrate semantic models with BI tools such as Power BI, Tableau, and Excel Pivot Tables, ensuring seamless end-user experiences.\nCollaborate with data engineers to align semantic models with curated data sources, transformation views, and data pipelines.\nApply star and snowflake schema design to model fact and dimension tables, ensuring optimal structure for analytical workloads.\nImplement Slowly Changing Dimensions (SCD Types 1 & 2) and maintain historical accuracy in reporting models.\nManage row-level security (RLS) and role-based access control (RBAC) policies within semantic layers for governed data access.\nParticipate in semantic model versioning, CI/CD-based deployments, and technical documentation.\nTroubleshoot semantic layer performance issues using AtScale query logs, plan analysis, and catching strategies.\nWhat you need:\nBasic Skills:\nMinimum 3 years of hands-on experience with AtScale, including building and maintaining semantic models, designing logical cubes, and implementing calculated measures using MDX. Proficiency in AtScale interface, modeling best practices, and performance tuning is essential.\nAdvanced experience in developing and optimizing DAX expressions for complex calculations in Power BI models, with a proven ability to translate these into new semantic layer technologies like AtScale or Cube.dev.\nStrong experience with MDX, including creating calculated members, KPIs, and advanced expressions. Excellent SQL skills with the ability to write complex queries using joins, CTEs, window functions, and performance tuning.\nSolid understanding of dimensional modeling. Ability to design fact/dimension tables using star/snowflake schemas, support SCD logic, and maintain model consistency.\nShould be familiar with the Kimball methodology for dimensional modeling, including concepts like conformed dimensions, fact table granularity, and slowly changing dimensions, to design scalable and analytics-friendly data structures.\nHands-on experience with Snowflake, Redshift, or BigQuery. Familiarity with virtual warehouses, caching, clustering, partitioning, and compute-storage separation.\nExperience implementing RLS and RBAC. Ability to define and enforce granular access controls within semantic models.\nStrong grasp of OLAP concepts like query abstraction, drill-down/roll-up, and cube optimization. Understanding of business logic abstraction from physical data.\nSkilled in using AtScale performance tools such as the Aggregate Designer, log analysis, and query optimization.\nProficient in managing model development lifecycle using Git, automation tools, and collaboration workflows with data/analytics teams.\nStrong verbal and written communication to document models, explain logic, and coordinate with cross-functional teams.\nResponsibilities:\nOwn the design, development, deployment, and maintenance of scalable, governed semantic models.\nImplement complex MDX logic and optimized aggregate strategies to meet performance benchmarks.\nProven ability to design and implement scalable AtScale architectures, including the development of architectural blueprints and data flow diagrams.\nEvaluate and implement the best semantic layer architecture for Power BI by leveraging tools like Microsoft Fabric or other modern BI accelerators to support self-service analytics.\nDefine business measures, hierarchies, and drill-down paths in semantic models aligned with enterprise KPIs.\nAlign semantic layers with upstream data transformations, curated datasets, and data warehouse architecture.\nEnforce governance and security through robust RLS and RBAC implementations.\nContinuously monitor, test, and tune semantic model performance using diagnostic tools and AtScale logging.\nEnsure semantic layer reusability, consistency, and business-aligned metric standardization.\nCollaborate with BI developers and analysts to understand reporting needs and validate model outputs.\nMaintain documentation, data lineage, and business glossaries that support transparency and user adoption.\nContribute to reusable templates, modeling standards, and automation frameworks.\nNice-to-Have to have:\nExperience with AtScale REST APIs for metadata-driven automation and CI/CD pipelines.\nFamiliarity with BI visualization platforms such as Power BI, Tableau, Looker, and Excel OLAP integration.\nScripting experience in Python, Shell, or YAML for configuration management or automation tasks.\nCloud certifications in Snowflake, Databricks, AWS, Azure, or Google Cloud Platform.\nExposure to metadata management, data cataloging, or enterprise data governance tools.\nPersonal Attributes:\nHigh attention to detail with a focus on producing scalable, accurate, and governed semantic solutions.\nStrong interpersonal and communication skills to collaborate effectively with technical and non-technical stakeholders.\nSelf-motivated and accountable, with the ability to take full ownership of deliverables.\nAdaptability to evolving tools, data technologies, and enterprise analytics strategies.","MDX, kimball methodology, snowflake, AtScale, BigQuery, Power Bi, OLAP, Tableau, Redshift, Sql, Git, Excel, Dax"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position\n\nThis role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.\n\nKey Accountabilities\n\nAssist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.\nWork closely with senior engineers and analysts to gather requirements and build working prototypes.\nSupport data integration from internal, third-party, and public sources.\nParticipate in developing and maintaining Data Warehouse schemas.\nContribute to documentation and testing efforts to ensure data reliability.\nLearn and apply data standards and governance practices as guided by the team.\n\nSkills and Experience | Essential\n\nKnowledge of data engineering concepts and data structures.\nExposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.\nUnderstanding of ETL processes and data pipelines.\nAbility to work collaboratively in an Agile/Kanban team environment.\nMicrosoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage\n\nSkills and Experience | Desirable\n\nFamiliarity with Medallion Architecture principles.\nExposure to MS Purview or other data governance tools.\nUnderstanding of data warehousing and reporting concepts.\nInterest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
Senior Data Engineer,Commonwealth Bank,10-13 Years,,"Bengaluru, India",Login to check your skill match score,"Organization: At CommBank, we never lose sight of the role we play in other people's financial wellbeing. Our focus is to help people and businesses move forward to progress. To make the right financial decisions and achieve their dreams, targets, and aspirations. Regardless of where you work within our organisation, your initiative, talent, ideas, and energy all contribute to the impact that we can make with our work. Together we can achieve great things.\n\nJob Title: Sr Data Engineering\n\nLocation: Bangalore\n\nBusiness & Team:\n\nTechnology Team is responsible for the world leading application of technology and operations across every aspect of CommBank, from innovative product platforms for our customers to essential tools within our business. We also use technology to drive efficient and timely processing, an essential component of great customer service.\n\nCommBank is recognised as leading the industry in IT and operations with its world-class platforms and processes, agile IT infrastructure, and innovation in everything from payments to internet banking and mobile apps.\n\nThe Group Security (GS) team protects the Bank and our customers from cyber compromise, through proactive management of cyber security, privacy, and operational risk. Our team includes:\n\nCyber Strategy & Performance\nCyber Security Centre\nCyber Protection & Design\nCyber Delivery\nCyber Data Engineering\nCyber Data Security\nIdentity & Access Technology\n\nThe Group Security Senior Data Engineering team provides specialised data services and platforms for the CommBank group & is accountable for developing Group's data strategy, data policy & standards, governance and set requirements for data enablers/tools. The team is also accountable to facilitate a community of practitioners to share best practice and build data talent and capabilities.\n\nImpact & contribution :-\n\nTo ensure the Group achieves a sustainable competitive advantage through data engineering, you will play a key role in supporting and executing the Group's data strategy.\n\nWe are looking for an experienced Data Engineer to join our Group Security Team, which is part of the wider Cyber Security Engineering practice. In this role, you will be responsible for setting up the Group Security Data Platform to ingest data from various organizations security telemetry data, along with additional data assets and data products. This platform will provide security controls and services leveraged across the Group.\n\nRoles & Responsibilities\n\nYou will be expected to perform the following tasks in a manner consistent with CBA's Values and People Capabilities.\n\nCORE RESPONSIBILITIES:\n\nPossesses hands-on technical experience working in AWS. The individual should have knowledge about AWS services like EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, KMS, Step functions, SQS,SNS, Cloud Watch.\nThe individual should possess a robust set of technical and soft skills and be an excellent AWS Data Engineer with a focus on complex Automation and Engineering Framework development.\nBeing well-versed in Python is mandatory, and experience in developing complex frameworks using Python is required.\nPassionate about Cloud/DevSecOps/Automation and possess a keen interest in solving complex problems systematically.\nDrive the development and implementation of scalable data solutions and data pipelines using various AWS services.\nPossess the ability to work independently and collaborate closely with team members and technology leads.\nExhibit a proactive approach, constantly seeking innovative solutions to complex technical challenges.\nCan take responsibility for nominated technical assets related to areas of expertise, including roadmaps and technical direction.\nCan own and develop technical strategy, overseeing medium to complex engineering initiatives.\n\nEssential Skills:-\n\nAbout 10-13 years of experience as a Data Engineering professional in a data-intensive environment.\nThe individual should have strong analytical and reasoning skills in the relevant area.\nProficiency in AWS cloud services, specifically EC2, S3, Lambda, Athena, Kinesis, Redshift, Glue, EMR, DynamoDB, IAM, SecretManager, Step functions, SQS,SNS, Cloud Watch.\nExcellent skills in Python-based framework development are mandatory.\nProficiency in SQL for efficient querying, managing databases, handling complex queries, and optimizing query performance.\nExcellent automation skills are expected in areas such as\nAutomating the testing framework using tools such as PyPy, Pytest, and various test cases including unit, integration, functional tests, and mockups.\nAutomating the data pipeline and expediting tasks such as data ingestion and transformation.\nAPI-based automated and integrated calls(REST, cURL, authentication & authorization, tokens, pagination, openApi, Swagger)\nImplementing advanced engineering techniques and handling ad hoc requests to automate processes on demand.\nImplementing automated and secured file transfer protocols like XCOM, FTP, SFTP, and HTTP/S\nExperience with Terraform, Jenkins, Teracity and Artifactory is essential as part of DevOps. Additionally, Docker and Kubernetes are also considered.\nProficiency in building orchestration workflows using Apache Airflow.\nStrong understanding of streaming data processing concepts, including event-driven architectures.\nFamiliarity with CI/CD pipeline development, such as Jenkins.\nExtensive experience and understanding in Data Modelling, SCD Types, Data Warehousing, and ETL processes.\nExcellent experience with GitHub or any preferred version control systems.\nExpertise in data pipeline development using various data formats/types.\nMandatory knowledge and experience in big data processing using PySpark/Spark and performance optimizations of applications\nProficiency in handling various file formats (CSV, JSON, XML, Parquet, Avro, and ORC) and automating processes in the big data environment.\nAbility to use Linux/Unix environments for development and testing.\nShould be aware of security best practices to protect data and infrastructure, including encryption, tokenization, masking, firewalls, and security zones.\nWell-structured documentation skills and the ability to create a well-defined knowledge base.\nCertifications such as AWS Certified Data Analytics/Engineer/Developer Specialty or AWS Certified Solutions Architect.\nShould be able to perform extreme engineering and design a robust, efficient, and cost-effective data engineering pipelines which are highly available and dynamically scalable on demand.\nEnable the systems to effectively respond to high demands and heavy loads maintaining the high throughput and high I/O performance with no data loss\nOwn and lead E2E Data engineering life cycle right from Requirement gathering, design, develop, test, deliver and support as part of DevSecOPS process.\nMust demonstrate skills and mindset to implement encryption methodologies like SSL/TLS and data encryption at rest and in transit and other data security best practices\nHands on work experience with data design tools like Erwin and demonstrate the capabilities of building data models, data warehouse, data lakes, data assets and data products\nMust be able to constructively challenge the status quo and lead to establish data governance, metadata management, ask the right questions, design with right principles\n\nEducation Qualification :-\n\nA Bachelor's or Master's degree in Engineering, specializing in Computer Science, Information Technology or relevant qualifications.\n\nIf you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through Sidekick to submit a valid application. We're keen to support you with the next step in your career.\n\nWe're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.\n\nAdvertising End Date: 23/05/2025","Big Data Processing, Data Pipeline Development, Github, Data Modelling, Pyspark, Automation, Sql, Apache Airflow, Jenkins, Terraform, Docker, Data Warehousing, Kubernetes, Python, Etl, AWS"
Senior Data Engineer (SQL and ADF),thinkbridge,5-7 Years,,India,Login to check your skill match score,"We are hiring a Senior Data Engineer with 5+ years of experience in SQL , who has good hands-on in ADF and ETL. Good communication is mandatory.\nAbout thinkbridge\nthinkbridge is how growth-stage companies can finally turn into tech disruptors. They get a new way there with world-class technology strategy, development, maintenance, and data science all in one place. But solving technology problems like these involves a lot more than code. That's why we encourage thinkers to spend 80% of their time thinking through solutions and 20% coding them. With an average client tenure of 4+ years, you won't be hopping from project to project here unless you want to. So, you really can get to know your clients and understand their challenges on a deeper level. At thinkbridge, you can expand your knowledge during work hours specifically reserved for learning. Or even transition to a completely different role in the organization. It's all about challenging yourself while you challenge small thinking.\nthinkbridge is a place where you can:\nThink bigger because you have the time, opportunity, and support it takes to dig deeper and tackle larger issues.\nMove faster because you'll be working with experienced, helpful teams who can guide you through challenges, quickly resolve issues, and show you new ways to get things done.\nGo further because you have the opportunity to grow professionally, add new skills, and take on new responsibilities in an organization that takes a long-term view of every relationship.\nthinkbridge.. there's a new way there.\nWhat is expected of you\nAs part of the job, you will be required to\nRead everything in detail that comes your way.\nElicit, analyze, specify & validate business requirements.\nDefine & Document the engineering requirements including architecture, components, usable interfaces & other characteristics.\nPlace specific emphasis on technical and usability design.\nCode and verify the solution.\nDebug & squash bugs.\nKeep stakeholders informed.\nKeep up with the technology and the domain.\nIf your beliefs resonate with these, you are looking at the right place!\nAccountability Finish what you started.\nCommunication Context-aware, pro-active and clean communication.\nOutcome High throughput.\nQuality High-quality work and consistency.\nOwnership Go beyond.\nRequirements\nMust-Have:\nShould have hands-on experience in writing SQL Queries and Stored Procedures.\nExcellent Communication.\nShould have experience in ADF (Azure Data Factory)\nGood experience in building ETL Solutions for large datasets\nGood to have:\nGood to have SSIS Experience\nOther Details :\nRemote First\nFlexible work hours\nNo loss of pay for pre-approved leaves\nFamily Insurance\nQuarterly in-person Collaboration Week -WWW","Adf, SSIS, Sql, Etl"
ETL Data Engineer,QTek Digital,6-9 Years,,"Hyderabad, India",Login to check your skill match score,"Job description\nCompany Description\nQTek Digital is a data solutions provider specializing in custom solutions in data management, data warehouse, and data science. Our team of data scientists, data analysts, and data engineers work together to solve today's challenges and unlock future possibilities. As an employee-centric company, we prioritize engagement, empowerment, and enrichment of our employees.\nRole Description\nThis is a full-time remote role for a BI ETL Engineer at QTek Digital. As a BI ETL Engineer, you will be responsible for day-to-day tasks such as data modeling, utilizing analytical skills, implementing data warehousing solutions, and executing Extract, Transform, Load (ETL) processes. This role requires strong problem-solving abilities and the ability to work independently.\nQualifications\n6-9 years of experience with ETL and ELT pipeline build out using Pentaho, SSIS, FiveTran, Airbyte, or similar tools.\n6-8 Years of experience in SQL and data manipulation languages\nStrong Data Modeling, Dashboard, and Analytical Skills\nExcellent understanding of data warehousing concepts, esp. Kimball design.\nExperience with Pentaho and Airbyte administration will be a huge plus.\nStrong skills in Data Modeling, Dashboard design, and Analytics\nExperience in Data Warehousing and Extract Transform Load (ETL) processes\nStrong problem-solving and troubleshooting skills\nExcellent communication and collaboration skills\nAbility to work independently and in a team\nBachelor's degree in computer science, Information Systems, or a related field\nThis role is based onsite in our Hyderabad Office.\nThe compensation range for this role is INR 5-19 Lakhs depending on a variety of factors including skills and experience.","Airbyte, FiveTran, Pentaho, Data Modeling, Data Warehousing, SSIS, Sql, ELT, Etl"
Data Engineer Snowflake,Oncorre Inc,6-8 Years,,"Bhubaneswar, India",Login to check your skill match score,"Company Description\nOncorre Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey. Established in 2007, Oncorre provides cutting-edge engineering solutions for Fortune companies and Government Agencies across the USA. Our mission is to help enterprises accelerate adoption of new technologies, untangle complex issues during digital evolution, and orchestrate ongoing innovation. We offer flexible service models, including both onsite and offsite support, to meet our clients diverse needs.\nJob Type:Full-time or Contract\nStart Date: June 1st 2025\nRole Description\nThis is a full-time hybrid role for a Data Engineer - SQL and Snowflake at Oncorre Inc. The role will involve tasks such as data engineering, data modeling, ETL processes, data warehousing, and data analytics.\nJob Description:\nCandidate should Provide technical expertise in needs identification, data modeling, data movement, and translating business needs into technical solutions with adherence to established data guidelines and approaches from a business unit or project perspective. Good knowledge of conceptual, logical, and physical data models, the implementation of RDBMS, operational data store (ODS), data marts, and data lakes on target platforms (SQL/NoSQL). Oversee and govern the expansion of existing data architecture and the optimization of data query performance via best practices. The candidate must be able to work independently and collaboratively\n6+ Years of experience as a Data Engineer\nStrong technical expertise in SQL and Snowflake is a must.\nStrong knowledge of joins and common table expressions (CTEs)\nStrong experience with Python\nStrong expertise in ETL process and with various data model concepts\nKnowledge of star schema and snowflake schema\nGood to know about AWS services such as S3, Athena, Glue, EMR/Spark with a major emphasis on S3 and Glue\nExperience with Big Data Tools and technologies\nKey Skills:\nGood Understanding of data structures and data analysis using SQL\nKnowledge of implementing ETL/ELT for data solutions end-to-end\nUnderstanding requirements, and data solutions (ingest, storage, integration, processing)\nKnowledge of analyzing data using SQL\nConducting End to End verification and validation for the entire application\nResponsibilities:\nUnderstand and translate business needs into data models supporting long-term solutions.\nPerform reverse engineering of physical data models from databases and SQL scripts.\nAnalyze data-related system integration challenges and propose appropriate solutions.\nAssist with and support setting the data architecture direction (including data movement approach, architecture/technology strategy, and any other data-related considerations to ensure business value)","Snowflake schema, snowflake, ETL processes, Big Data Tools, Data Modeling, Data Analytics, Data Warehousing, Sql, Nosql, RDBMS, Star Schema, Python"
Big Data Engineer (Freelancer),Soul AI,4-6 Years,,India,Login to check your skill match score,"About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data, AI-first scaled operations services, and more. Based in SF and Hyderabad, we are a young, fast-moving team on a mission to build AI for Good, driving innovation and positive societal impact.\nWe are looking for an experienced Big Data Engineer with at least 4 years of experience to design, develop, and manage large-scale data processing systems.\nKey Responsibilities:\nBuild and maintain data pipelines for large datasets.\nDesign systems for real-time data processing.\nCollaborate with data scientists and engineers to optimize data workflows.\nRequired Qualifications:\n4+ years of experience as a Big Data Engineer.\nStrong proficiency with big data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.\nExperience with cloud platforms like AWS, GCP, or Azure.\nWhy Join Us\nCompetitive pay (1200/hour).\nFlexible hours.\nRemote opportunity.\nNOTE: Pay will vary by project and typically is up to Rs. 1200 per hour (if you work an average of 3 hours every day - that could be as high as Rs. 108K per month) once you clear our screening process.\nShape the future of AI with Soul AI!","NoSQL databases, Gcp, Hadoop, Spark, Kafka, Azure, AWS"
Lead Data Engineer - Future Detections,JPMorganChase,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nBe an integral part of an agile team that's constantly pushing the envelope to enhance, build, and deliver top-notch technology products.\n\nAs a Lead Data Engineer at JPMorgan Chase within the Cybersecurity & Tech Controls , you are an integral part of an agile team that works to enhance, build, and deliver trusted market-leading technology products in a secure, stable, and scalable way. Drive significant business impact through your capabilities and contributions, and apply deep technical expertise and problem-solving methodologies to tackle a diverse array of challenges that span multiple technologies and applications.\n\nJob Responsibilities\n\nRegularly provides technical guidance and direction to support the business and its technical teams, contractors, and vendors\nDevelops secure and high-quality production code, and reviews and debugs code written by others\nDrives decisions that influence the product design, application functionality, and technical operations and processes\nServes as a function-wide subject matter expert in one or more areas of focus\nActively contributes to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle\nInfluences peers and project decision-makers to consider the use and application of leading-edge technologies\nAdds to the team culture of diversity, equity, inclusion, and respect\n\nRequired Qualifications, Capabilities, And Skills\n\nFormal training or certification on Data engineering concepts and 5+ years applied experience\nHands-on practical experience Big Data , large volume data transfer analysis.\nExperience in spark/ iceberg / parquets.\nExperience with cloud platforms like AWS and container technologies such as Kubernetes and Docker\nKnowledge of software applications and technical processes with considerable in-depth knowledge in one or more technical disciplines (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)\nExperience to tackle design and functionality problems independently with little to no oversight\n\nPreferred Qualifications, Capabilities, And Skills\n\nPractical cloud native experience\nComputer Science, Computer Engineering, Mathematics, or a related technical field\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Mobile, parquets, Data engineering concepts, iceberg, Machine Learning, Big Data, Artificial Intelligence, cloud, Docker, Spark, Kubernetes, AWS"
"Data Engineer with Databricks, Azure and Power BI (DAX) Skills",Sony India Software Centre,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Description\n\nTotal relevant experience in Data Engineering should be 6-8 yrs.\n\nMust have good knowledge on Azure Databricks, Azure Datalake and Power BI including DAX skills.\n\nShould have good Power BI Experience which includes -\n\nStrong Data modelling skills.\nExpert in Tabular model design.\nExpert in writing complex DAX formulas.\nHands on experience in DAX optimization\n\nStrong knowledge on the DWBI with SQL.\n\nExperience of working with On-premise/cloud BI solution implementation.\n\nGood knowledge in Python.\n\nGood to have these skills in Databricks -\n\nUnderstanding of AI capabilities.\nUnity Catalog - Implementation\nAI/BI Genie\nDevOps\n\nPower Automate (Good to have)\n\nDepartment\n\nRegional Apps - APCNJP - AP Platforms - G3A\n\nOpen Positions\n\n1\n\nSkills Required\n\nData Bricks, power bi, Azure Data Lake, SQL Development, Python\n\nRole\n\nDesign, develop, and maintain data pipelines and ETL processes using Azure Databricks\nCollaborate with business analysts to understand and meet business requirements for data and analytics\nDevelop and optimize data models and visualizations using Power BI (DAX) to present meaningful insights to stakeholders\nImplement best practices for data governance, security, and compliance in Azure environments\nMonitor and troubleshoot data pipelines, ensuring data integrity and reliability\nStay abreast of the latest trends and technologies in data engineering and analytics\nProvide technical guidance and mentorship to junior members of the team\n\nLocation\n\nBengaluru\n\nEducation/Qualification\n\nB Tech\n\nDesirable Skills\n\nAzure Databricks, Azure Datalake, Power BI with Dax, SQL, Python\n\nYears Of Exp\n\n6 to 8 years\n\nDesignation\n\nTechnical Specialist","Azure Datalake, Data Bricks, Power Bi, Azure Databricks, Dax, Sql, Python"
Senior Data Engineer,USEReady,3-8 Years,,"Bengaluru, India",Login to check your skill match score,"Role: Senior Data Engineer\nExperience-3-8 yrs\nLocation: Bangalore, Gurgaon, Mohali, Pune\nAbout the Role:\nWe are seeking a skilled and proactive Data Engineer with 3-8 years of hands-on experience in Snowflake, Python, Streamlit, and SQL, along with expertise in consuming REST APIs and working with modern ETL tools likeMatillion, Fivetran etc. The ideal candidate will have a strong foundation in data modeling, data warehousing, and data profiling, and will play a key role in designing and implementing robust data solutions that drive business insights and innovation.\nKey Responsibilities:\nDesign, develop, and maintain data pipelines and workflows using Snowflake and an ETL tool (e.g., Matillion, dbt, Fivetran, or similar).\nDevelop data applications and dashboards using Python and Streamlit.\nCreate and optimize complex SQL queries for data extraction, transformation, and loading.\nIntegrate REST APIs for data access and process automation.\nPerform data profiling, quality checks, and troubleshooting to ensure data accuracy and integrity.\nDesign and implement scalable and efficient data models aligned with business requirements.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data needs and deliver actionable solutions.\nImplement best practices in data governance, security, and compliance.\nRequired Skills and Qualifications:\n35 years of professional experience in a data engineering or development role.\nStrong expertise in Snowflake, including performance tuning and warehouse optimization.\nProficient in Python, including data manipulation with libraries like Pandas.\nExperience building web-based data tools using Streamlit.\nSolid understanding and experience with RESTful APIs and JSON data structures.\nStrong SQL skills and experience with advanced data transformation logic.\nExperience with an ETL tool commonly used with Snowflake (e.g., dbt, Matillion, Fivetran, Airflow).\nHands-on experience in data modeling (dimensional and normalized), data warehousing concepts, and data profiling techniques.\nFamiliarity with version control (e.g., Git) and CI/CD processes is a plus.\nPreferred Qualifications:\nExperience working in cloud environments (AWS, Azure, or GCP).\nKnowledge of data governance and cataloging tools.\nExperience with agile methodologies and working in cross-functional teams.","Airflow, Matillion, CI CD, dbt, snowflake, Streamlit, Fivetran, Data Warehousing, Data Modeling, Sql, Git, Etl Tools, Rest Apis, Data Profiling, Python"
"Senior Data Engineer (ELT/ELT, Python, Snowflake, AWS)",Reap,6-8 Years,,"Mumbai, India",Login to check your skill match score,"Role Overview\n\nAs part of a rapidly growing data team, we are looking to hire Senior Data Engineers to contribute to our dynamic and innovative projects. We are looking for someone who is ready for the challenge, is solutions-oriented, and thinks out of the box. The ideal candidate should be passionate about using data to drive impact across the organization and a collaborative mindset to work effectively within cross-functional teams. If you are enthusiastic about pushing boundaries, solving complex problems, and making a significant impact, we encourage you to apply and be part of our exciting journey in shaping the future of our products.\n\nResponsibilities\n\nDesign and development: Design, develop, and deploy scalable ETL/ELT pipelines and APIs for ingestion and transformation. Implement data modeling best practices for optimal accessibility, flexibility and query performance. Implement data governance practices, including data security, privacy, and compliance, to ensure data integrity and regulatory compliance.\nCollaboration: Work closely with cross-functional teams, including product managers, designers, and other engineers, to ensure seamless product development from concept to deployment. Influence product and cross-functional teams to identify data opportunities to drive impact.\nContinuous learning: Stay updated with the latest industry trends and technologies, ensuring our tech stack remains modern and competitive.\nObservability and Support: Build monitor and alerts for data pipelines monitoring, identify and resolve performance issues, troubleshoot data-related problems in collaboration with other teams, and ensure data platform SLAs are met.\n\nTo Be Successful You Will Need To Have\n\nExperience: 6+ year work experience in data engineering and cloud platforms. Previous experience in a senior or lead engineering role.\nTechnical proficiency: Expertise in ETL, data modeling, and cloud data warehousing. Strong programming skills in Python, SQL, AWS, Snowflake and related tech stack. Hands-on experience with big data processing and API integrations.\nProblem solving: Strong analytical and problem-solving skills, with a keen attention to detail and a passion for troubleshooting and debugging\nExperience in Credit Cards, Payments and AWS certification would be an advantage.\nExposure to AI, machine learning, and predictive analytics is highly desirable.\n\nBenefits\n\nA Global & Dynamic Team\nRemote Work Friendly\n\nAfter submitting your application, please check your inbox for a confirmation email. If you don't see it, kindly check your spam or junk folder and adjust your settings to ensure future communication reaches your inbox. You can follow the steps here.","API Integrations, Big Data Processing, snowflake, Cloud Data Warehousing, Data Modeling, Python, Sql, Etl, AWS"
Senior Data Engineer,WebMD,4-6 Years,,"Navi Mumbai, Mumbai, India",Login to check your skill match score,"Position Requirements:\n4+ years of experience with RDBMS databases such as Oracle, MSSQL or PostgreSQL\n2+ years of experience with Pentaho Data Integration or any ETL tools such as Talend, Informatica, DataStage or HOP.\nWorking knowledge of orchestration tools such Oozie and Airflow\nExperience working in both OLAP and OLTP environments\nExperience working on-prem, not just cloud environments\nExperience working with teams outside of IT (i.e. Application Developers, Business Intelligence, Finance, Marketing, Sales)\nExperience managing or developing in the Hadoop ecosystem is preferred\nProgramming background with either Python, Scala, Java or C/C++ is a plus\nExperience with Spark. PySpark, SparkSQL, Spark Streaming, etc\nStrong in any of the Linux distributions, RHEL, CentOS or Fedora\nExperience using reporting and Data Visualization platforms (Tableau, Pentaho BI) is good to have\nWeb analytics or Business Intelligence a plus\nUnderstanding of Ad stack and data (Ad Servers, DSM, Programmatic, DMP, etc)","Airflow, Pentaho BI, DMP, Ad stack, Programmatic, DSM, Business Intelligence, Java, C, Hadoop Ecosystem, Scala, Pyspark, OLAP, Tableau, Sparksql, Spark Streaming, Web Analytics, Ad Servers, Spark, Oozie, Pentaho Data Integration, Python, Oltp"
Collibra Data Engineer (8 yrs Noida),Orbion Infotech,Fresher,,"Noida, India",Login to check your skill match score,"Tips: Provide a summary of the role, what success in the position looks like, and how this role fits into the organization overall.\n\nResponsibilities\n\n[Be specific when describing each of the responsibilities. Use gender-neutral, inclusive language.]\n\nExample: Determine and develop user requirements for systems in production, to ensure maximum usability\n\nQualifications\n\n[Some qualifications you may want to include are Skills, Education, Experience, or Certifications.]\n\nExample: Excellent verbal and written communication skills\n\nSkills: etl processes,sql,data modeling,python,business intelligence,data quality,dgc,communication skills,data governance,collibra","dgc, etl processes, business intelligence, Data Quality, Collibra, python, Data Governance, Data Modeling, Sql"
Azure Data Engineer,Dexian,6-8 Years,,"Chennai, India",Login to check your skill match score,"Job Description:\nExcellent understanding on data architecture system (source, target, transformations, processing, etc.,) and migration b/w DB platforms\nHands-On Experience of 6+ years on Azure data analytics and Datawarehouse in Azure.\nMust have hands-on experience in the Azure services like Azure Data Explorer, Azure Databricks, Azure Data factory, Azure Synapse Analytics and Azure Fabric etc\nMust have strong hands-on experience with Python\nStrong hands-onexperience in creating data pipeline monitors in the Azure environment.\nTools & Technology Experience preferred:\nObject-oriented /object function scripting languages: Python\nData migration from on premise systems - RDBMS to Cloud Datawarehouse\nRelational SQL and NoSQL databases, including Snowflake and PostgreSQL\nData pipeline using Azure stack\nAzure cloud services: Datafactory, Databricks, SQL Datawarehouse\nShift: 2PM-11PM\nWork Mode: WFO","Relational SQL, snowflake, NoSQL databases, Azure Data Explorer, Azure Fabric, Azure Data Factory, Azure Synapse Analytics, PostgreSQL, Azure Databricks, Python"
Senior Data Engineer,"NTT DATA, Inc.",5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Make an impact with NTT DATA\n\nJoin a company that is pushing the boundaries of what is possible. We are renowned for our technical excellence and leading innovations, and for making a difference to our clients and society. Our workplace embraces diversity and inclusion it's a place where you can grow, belong and thrive.\n\nYour day at NTT DATA\n\nThe Senior Data Engineer is an advanced subject matter expert, accountable for the transformation of data into a structured format that can be easily analyzed in a query or report.\n\nThis role is responsible for developing structured data sets that can be reused or compliment by other data sets and reports.\n\nThis role analyzes the data sources and data structure and will design and develop data models to support the analytics requirements of the business which includes management / operational / predictive / data science capabilities.\n\nWhat You'll Be Doing\n\nKey Responsibilities:\n\nDesigns data models in a structured data format to enable analysis thereof.\nDesigns and develops scalable extract, transformation and loading (ETL) packages from the business source systems and the development of ETL routines to populate data from sources,\nParticipates in the transformation of object and data models into appropriate database schemas within design constraints.\nInterprets installation standards to meet project needs and produces database components as required.\nDirects test scenarios and is responsible for participating in thorough testing and validation to support the accuracy of data transformations.\nAccountable for running data migrations across different databases and applications, for example MS Dynamics, Oracle, SAP and other ERP systems.\nWorks across multiple IT and business teams to define and implement data table structures and data models based on requirements.\nAccountable for analysis, and development of ETL and migration documentation.\nWorks with various stakeholders to evaluate potential data requirements.\nAccountable for the definition and management of scoping, requirements, definition, and prioritization activities for small-scale changes and assist with more complex change initiatives.\nNetworks with various stakeholders, contributing to the recommendation of improvements in automated and non-automated components of the data tables, data queries and data models.\n\nKnowledge and Attributes:\n\nAdvanced knowledge of the definition and management of scoping requirements, definition and prioritization activities.\nAdvanced understanding of database concepts, object and data modelling techniques and design principles and conceptual knowledge of building and maintaining physical and logical data models.\nAdvanced expertise in Microsoft Azure Data Factory, SQL Analysis Server, SAP Data Services, SAP BTP.\nAdvanced understanding of data architecture landscape between physical and logical data models\nAnalytical mindset with excellent business acumen skills.\nProblem-solving aptitude with the ability to communicate effectively, both written and verbal.\nAbility to think strategically and build effective relationships at all levels within the organization.\nAdvanced expert in programing languages (Perl, bash, Shell Scripting, Python, etc.).\n\nAcademic Qualifications and Certifications:\n\nBachelor's degree or equivalent in computer science, software engineering, information technology, or a related field.\nRelevant certifications preferred such as SAP, Microsoft Azure etc.\nCertified Data Engineer, Certified Professional certification preferred.\n\nRequired Experience:\n\nAdvanced demonstrated experience in data engineering, data mining within a fast-paced environment.\nProficient in building modern data analytics solutions that delivers insights from large and complex data sets with multi-terabyte scale.\nAdvanced demonstrated experience with architecture and design of secure, highly available and scalable systems.\nAdvanced proficiency in automation, scripting and proven examples of successful implementation.\nAdvanced proficiency in scripting languages (Perl, bash, Shell Scripting, Python, etc.).\nAdvanced demonstrated experience with big data tools like Hadoop, Cassandra, Storm etc.\nAdvanced demonstrated experience in any applicable language, preferably .NET.\nAdvanced proficiency in SAP, SQL, MySQL databases and Microsoft SQL.\nAdvanced demonstrated experience working with data sets and ordering data through MS Excel functions, e.g. macros, pivots.\n\nWorkplace type:\n\nRemote Working\n\nAbout NTT DATA\n\nNTT DATA is a $30+ billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long-term success. We invest over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure, and connectivity. We are also one of the leading providers of digital and AI infrastructure in the world. NTT DATA is part of NTT Group and headquartered in Tokyo.\n\nEqual Opportunity Employer\n\nNTT DATA is proud to be an Equal Opportunity Employer with a global culture that embraces diversity. We are committed to providing an environment free of unfair discrimination and harassment. We do not discriminate based on age, race, colour, gender, sexual orientation, religion, nationality, disability, pregnancy, marital status, veteran status, or any other protected category. Join our growing global team and accelerate your career with us. Apply today.","SAP SQL, Microsoft Azure Data Factory, Sap Data Services, Ms Excel, Storm, Hadoop, SAP BTP, Cassandra, Bash, Shell Scripting, Microsoft Sql, Sql, Perl, MySQL, Python"
Senior Data Engineer,Verint,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"At Verint, we believe customer engagement is the core of every global brand. Our mission is to help organizations discover opportunities previously only scarcely imagined by connecting work, data, and experiences enterprise wide. We hire innovators with the passion, creativity, and drive to answer constantly shifting market challenges and deliver impactful results for our customers. Our commitment to attracting and retaining a talented, diverse, and engaged team creates a collaborative environment that openly celebrates all cultures and affords personal and professional growth opportunities. Learn more at www.verint.com.\nWe are looking for Senior Data Engineer who love to solve complex problems across a full spectrum of latest technologies and take lead on POC, Technical design, being ahead of team. Java, Spring Boot, with DevOps/CloudOps Engineer with expertise in AWS services, Terraform, and Kubernetes. The ideal candidate will have to have a strong background in building and managing services on cloud infrastructure using best practices in DevOps and cloud technologies.\nEssential Requirements:\nJava +Spark\nTotal Experience 5+ yrs.\nBachelor's degree in computer science / software engineering (or similar)\nJava with Spring boot\nDepth and technical understanding of: data structures, I/O, multi-threading, Restful Web Services\nSpark Experience\nAWS Cloud Knowledge, Deployment, troubleshooting\nDatabase knowledge (RDBS/NoSQL)\nStrong debugging skills of performance/memory leaks/crash/Multi-threaded/Algorithms.\nExperience with source control tools (GIT or similar)\nExperience with compiling and continuous integration tools like: Eclipse/IntelliJ, Gradle/Maven, Jenkins, Artifactory\nExcellent teamwork and communication skills.\nHigh self-learning and self-managed abilities.\nIntegration/build/static code analysis tools\nSystem wide and end to product understanding.\nLeadership skills\nStrong interpersonal skills, high communication skills.\nResponsibilities:\nAnalyze and influence technical, system, and other user requirements.\nDesign SW solutions, considering large scale, high availability, security, robustness, performance, cloud, multi tenancy.\nDevelop highest standard code, complex solutions, in enterprise level application and architecture Feature leading - leading engineers working on a mutual feature through integrations, reviews, status reports, problem solving.\nWork with cross functional stakeholders such as product management, additional SW engineers, architects and team leads.\nDeep understanding of robustness implementations, performance and sizing. Deep understanding and advanced implementation of continuous Integration and delivery.\nStrong understanding and oversee of all phases of the development life cycle, such as Automation, CICD, TDD, Integrations, Builds, Deployment.\nPreferred Requirements:\nWorking Knowledge or Exposure to NLP, AI/ML\nData Warehousing Concepts\nWorking Knowledge or Exposure to Python\nWorking Knowledge or Exposure AWS SDK\nData Modelling\nLarge scale systems design exposure\nIf Interested, please send in your resumes to [HIDDEN TEXT]\nThanks\nNeena","Java, Maven, Eclipse, Spring Boot, Artifactory, Intellij, Nosql, Jenkins, Git, Gradle, Terraform, Spark, Kubernetes, AWS"
Senior Data Engineer,MDA Edge,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"Job Summary:\nWe are seeking a results-driven Senior Data Engineer to join our engineering team. The ideal candidate will have hands-on expertise in data pipeline development, cloud infrastructure, and business intelligence (BI) support, with a strong command of the modern data stack.\nIn this role, you will be responsible for building scalable ETL/ELT workflows, managing data lakes and marts, and ensuring seamless data delivery to analytics and BI teams. The position requires deep technical proficiency in PostgreSQL, Python scripting, Apache Airflow, AWS (or other cloud environments), and modern data and BI tools.\nKey Responsibilities:\nPostgreSQL & Data Modeling:\nDesign and optimize complex SQL queries, stored procedures, and indexes.\nPerform performance tuning and query plan analysis.\nContribute to schema design and data normalization.\nData Migration & Transformation:\nMigrate data from multiple sources to cloud or operational data store (ODS) platforms.\nDesign schema mapping and implement transformation logic.\nEnsure consistency, integrity, and accuracy in migrated data.\nPython Scripting for Data Engineering:\nDevelop automation scripts for data ingestion, cleansing, and transformation.\nHandle various file formats (JSON, CSV, XML), REST APIs, and cloud SDKs (e.g., Boto3).\nMaintain reusable script modules for operational pipelines.\nData Orchestration with Apache Airflow:\nDevelop and manage Directed Acyclic Graphs (DAGs) for batch and stream workflows.\nImplement retries, task dependencies, notifications, and failure handling.\nIntegrate Airflow with cloud services, data lakes, and data warehouses.\nCloud Platforms (AWS / Azure / GCP):\nManage data storage solutions (S3, GCS, Blob), compute services, and data pipelines.\nConfigure permissions, IAM roles, encryption, and logging for security.\nMonitor and optimize cost and performance of cloud-based data operations.\nData Marts & Analytics Layer:\nDesign and manage data marts using dimensional models.\nBuild star and snowflake schemas to support BI and self-service analytics.\nImplement incremental load strategies and partitioning for efficiency.\nModern Data Stack Integration:\nWork with tools such as DBT, Fivetran, Redshift, Snowflake, BigQuery, and Kafka.\nSupport modular pipeline design and metadata-driven frameworks.\nEnsure high availability and scalability of the data stack.\nBI & Reporting Tools (Power BI / Superset / Supertech):\nCollaborate with BI teams to design datasets and optimize queries.\nSupport dashboard development and reporting layers.\nManage access, data refresh schedules, and performance optimization for BI tools.\nRequired Skills & Qualifications:\n4-6 years of hands-on experience in data engineering roles.\nStrong SQL skills in PostgreSQL (query tuning, complex joins, stored procedures).\nAdvanced Python scripting skills for automation and ETL processes.\nProven experience with Apache Airflow (custom DAGs, error handling).\nSolid understanding of cloud architecture, particularly AWS.\nExperience with data marts and dimensional data modeling.\nExposure to modern data stack tools (DBT, Kafka, Snowflake, etc.).\nFamiliarity with BI tools such as Power BI, Apache Superset, or Supertech BI.\nKnowledge of version control (Git) and CI/CD pipelines is a plus.\nExcellent problem-solving and communication skills.","Supertech BI, dbt, snowflake, Fivetran, Apache Superset, BigQuery, Power Bi, PostgreSQL, Kafka, Redshift, Apache Airflow, Git, Python, AWS"
Data Engineer - Apache Airflow,Ekfrazo Technologies Private Limited,4-6 Years,,"Mumbai, India",Login to check your skill match score,"Job Title: Data Engineer\nLocation: Mumbai- WFO\nExperience: 4+ Years\nNotice Period: Immediate Joiners Only\nAbout the Role:\nWe are seeking a skilled and motivated Data Engineer to join our team in Mumbai. The ideal candidate will have a strong background in Python (OOPS), workflow orchestration using Airflow, and experience working with Azure cloud services and Snowflake. This role is best suited for someone passionate about building scalable data pipelines and backend applications or SDKs.\nKey Responsibilities:\nDesign, build, and maintain scalable and robust data pipelines using Apache Airflow\nDevelop backend components and reusable SDKs in Python, with a strong emphasis on OOPS principles\nIntegrate and manage large-scale data workflows on Azure Cloud, leveraging services such as Data Factory, Blob Storage, and more\nWork with Snowflake for data warehousing and analytics\nImplement monitoring and data quality checks using tools like Great Expectations\nCollaborate with cross-functional teams to understand business data needs and deliver high-quality solutions\nMust-Have Skills:\n4+ years of experience in Data Engineering or related roles\nHands-on experience with Apache Airflow\nStrong Python programming skills, with emphasis on Object-Oriented Programming (OOPS) and backend application or SDK development\nProven experience working with Azure cloud platform, especially services relevant to data engineering (e.g., Data Factory, Blob Storage, Azure Functions)\nGood understanding of Snowflake architecture, data loading/unloading, and query optimization\nStrong analytical and problem-solving skills\nNice to Have:\nExperience with Great Expectations or similar Data Quality frameworks\nPrior work on building or maintaining Data Quality (DQ) frameworks\nAdvanced knowledge of Snowflake features like Snowpipe, Streams & Tasks, or Materialized Views","snowflake, Blob Storage, Great Expectations, Apache Airflow, Data Factory, Azure Cloud Services, Python"
Data Engineer,ICICIDirect,6-12 Years,,"Navi Mumbai, Mumbai, India",Login to check your skill match score,"Job Responsibilities:\nTo work on Data Analytics to solve various business use cases.\nDesign and implement data pipelines to extract data from various sources (databases, flat files, APIs, streaming data, etc.)\nDevelop efficient ETL processes using tools like Apache Spark, Apache Sqoop, Python/SQL, Pyspark,\nGood to have expertise/ knowledge in Apache Iceberg, Apace Ni-fi/ Apache Flink and Apache Kafka.\nBasic Understanding of Data Pipeline design using PL/SQL jobs.\nProvide seamless data Availability for data analysts, data scientists, and other stakeholders.\nDevelop various Business Requirements leveraging SQL and Python.\nStay updated with the latest industry trends and advancements in data analytics and AI, Data Lake, Delta Lake concepts.\nRequired Experience:\nOver all 6 -12 years of experience in Data Engineering & Data Analytics.\nHands-on experience in coding with Python and SQL.\nExperience in building ETL, ELT Pipelines using Python.\nStrong expertise in Hadoop Ecosystem (Hive, Impala, Cloudera), Spark, Sqoop, Delta Lake, Data warehouse Concepts, RDBMS System, No SQL Databases, Apache Tools like Ni-fi, Kafka etc.\nExperience in Cloudera data platform is preferred.\nRequired Qualification:\nBachelor's or Master's degree in Computer Science, IT or any Engineering field.","Apache Iceberg, No SQL Databases, ETL processes, Delta Lake, Apache Sqoop, Apache Flink, Apache Spark, Pl Sql, Impala, Sql, Data Warehouse Concepts, Hive, Hadoop Ecosystem, RDBMS, Apache Kafka, Cloudera, Python"
Senior Data Engineer,MDA Edge,4-6 Years,,"Bengaluru, India",Login to check your skill match score,"Job Summary:\nWe are seeking a results-driven Senior Data Engineer to join our engineering team. The ideal candidate will have hands-on expertise in data pipeline development, cloud infrastructure, and business intelligence (BI) support, with a strong command of the modern data stack.\nIn this role, you will be responsible for building scalable ETL/ELT workflows, managing data lakes and marts, and ensuring seamless data delivery to analytics and BI teams. The position requires deep technical proficiency in PostgreSQL, Python scripting, Apache Airflow, AWS (or other cloud environments), and modern data and BI tools.\nKey Responsibilities:\nPostgreSQL & Data Modeling:\nDesign and optimize complex SQL queries, stored procedures, and indexes.\nPerform performance tuning and query plan analysis.\nContribute to schema design and data normalization.\nData Migration & Transformation:\nMigrate data from multiple sources to cloud or operational data store (ODS) platforms.\nDesign schema mapping and implement transformation logic.\nEnsure consistency, integrity, and accuracy in migrated data.\nPython Scripting for Data Engineering:\nDevelop automation scripts for data ingestion, cleansing, and transformation.\nHandle various file formats (JSON, CSV, XML), REST APIs, and cloud SDKs (e.g., Boto3).\nMaintain reusable script modules for operational pipelines.\nData Orchestration with Apache Airflow:\nDevelop and manage Directed Acyclic Graphs (DAGs) for batch and stream workflows.\nImplement retries, task dependencies, notifications, and failure handling.\nIntegrate Airflow with cloud services, data lakes, and data warehouses.\nCloud Platforms (AWS / Azure / GCP):\nManage data storage solutions (S3, GCS, Blob), compute services, and data pipelines.\nConfigure permissions, IAM roles, encryption, and logging for security.\nMonitor and optimize cost and performance of cloud-based data operations.\nData Marts & Analytics Layer:\nDesign and manage data marts using dimensional models.\nBuild star and snowflake schemas to support BI and self-service analytics.\nImplement incremental load strategies and partitioning for efficiency.\nModern Data Stack Integration:\nWork with tools such as DBT, Fivetran, Redshift, Snowflake, BigQuery, and Kafka.\nSupport modular pipeline design and metadata-driven frameworks.\nEnsure high availability and scalability of the data stack.\nBI & Reporting Tools (Power BI / Superset / Supertech):\nCollaborate with BI teams to design datasets and optimize queries.\nSupport dashboard development and reporting layers.\nManage access, data refresh schedules, and performance optimization for BI tools.\nRequired Skills & Qualifications:\n4-6 years of hands-on experience in data engineering roles.\nStrong SQL skills in PostgreSQL (query tuning, complex joins, stored procedures).\nAdvanced Python scripting skills for automation and ETL processes.\nProven experience with Apache Airflow (custom DAGs, error handling).\nSolid understanding of cloud architecture, particularly AWS.\nExperience with data marts and dimensional data modeling.\nExposure to modern data stack tools (DBT, Kafka, Snowflake, etc.).\nFamiliarity with BI tools such as Power BI, Apache Superset, or Supertech BI.\nKnowledge of version control (Git) and CI/CD pipelines is a plus.\nExcellent problem-solving and communication skills.","Supertech BI, dbt, snowflake, Fivetran, Apache Superset, BigQuery, Power Bi, PostgreSQL, Kafka, Redshift, Apache Airflow, Git, Python, AWS"
Senior Data Engineer,Verint,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"At Verint, we believe customer engagement is the core of every global brand. Our mission is to help organizations discover opportunities previously only scarcely imagined by connecting work, data, and experiences enterprise wide. We hire innovators with the passion, creativity, and drive to answer constantly shifting market challenges and deliver impactful results for our customers. Our commitment to attracting and retaining a talented, diverse, and engaged team creates a collaborative environment that openly celebrates all cultures and affords personal and professional growth opportunities. Learn more at www.verint.com.\nWe are looking for Senior Data Engineer who love to solve complex problems across a full spectrum of latest technologies and take lead on POC, Technical design, being ahead of team. Java, Spring Boot, with DevOps/CloudOps Engineer with expertise in AWS services, Terraform, and Kubernetes. The ideal candidate will have to have a strong background in building and managing services on cloud infrastructure using best practices in DevOps and cloud technologies.\nEssential Requirements:\nJava +Spark\nTotal Experience 5+ yrs.\nBachelor's degree in computer science / software engineering (or similar)\nJava with Spring boot\nDepth and technical understanding of: data structures, I/O, multi-threading, Restful Web Services\nSpark Experience\nAWS Cloud Knowledge, Deployment, troubleshooting\nDatabase knowledge (RDBS/NoSQL)\nStrong debugging skills of performance/memory leaks/crash/Multi-threaded/Algorithms.\nExperience with source control tools (GIT or similar)\nExperience with compiling and continuous integration tools like: Eclipse/IntelliJ, Gradle/Maven, Jenkins, Artifactory\nExcellent teamwork and communication skills.\nHigh self-learning and self-managed abilities.\nIntegration/build/static code analysis tools\nSystem wide and end to product understanding.\nLeadership skills\nStrong interpersonal skills, high communication skills.\nResponsibilities:\nAnalyze and influence technical, system, and other user requirements.\nDesign SW solutions, considering large scale, high availability, security, robustness, performance, cloud, multi tenancy.\nDevelop highest standard code, complex solutions, in enterprise level application and architecture Feature leading - leading engineers working on a mutual feature through integrations, reviews, status reports, problem solving.\nWork with cross functional stakeholders such as product management, additional SW engineers, architects and team leads.\nDeep understanding of robustness implementations, performance and sizing. Deep understanding and advanced implementation of continuous Integration and delivery.\nStrong understanding and oversee of all phases of the development life cycle, such as Automation, CICD, TDD, Integrations, Builds, Deployment.\nPreferred Requirements:\nWorking Knowledge or Exposure to NLP, AI/ML\nData Warehousing Concepts\nWorking Knowledge or Exposure to Python\nWorking Knowledge or Exposure AWS SDK\nData Modelling\nLarge scale systems design exposure\nIf Interested, please send in your resumes to [HIDDEN TEXT]\nThanks\nNeena","Java, Maven, Eclipse, Spring Boot, Artifactory, Intellij, Nosql, Jenkins, Git, Gradle, Terraform, Spark, Kubernetes, AWS"
Data Engineer,NAZZTEC,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Senior Databricks Data Engineer (AWS Platform)\nLocation: Hyderabad\nShift: B Shift (12 PM 10 PM IST)\nExperience Required: 5+ Years\nJob Summary:\nWe are seeking a Senior Databricks Data Engineer with strong experience in building and optimizing data pipelines and architectures on AWS using Databricks and PySpark. The ideal candidate will also have hands-on experience in Big Data technologies, real-time streaming, and CI/CD pipelines. This role demands client-facing experience and the ability to operate in a fast-paced environment while delivering high-quality data engineering solutions.\nKey Responsibilities:\nDesign and develop scalable data engineering solutions using Databricks on AWS with PySpark and Databricks SQL.\nBuild and maintain data pipelines using Delta Lake for batch and streaming data.\nDesign and implement real-time data streaming applications using Kafka or Kinesis.\nDevelop and maintain ETL workflows and data warehouse architectures aligned with business goals.\nCollaborate with data scientists, analysts, and stakeholders to ensure data quality and integrity.\nUse Airflow to orchestrate complex data workflows.\nDevelop and maintain CI/CD pipelines using GIT, Jenkins, Docker, Kubernetes, and Terraform.\nWrite efficient and reusable code in Python, Java, or Scala.\nEngage with clients regularly to gather requirements, provide updates, and build trusted relationships.\nRequired Skills & Experience:\n5+ years of experience in Databricks engineering on AWS using PySpark, Databricks SQL, and Delta Lake.\n5+ years of experience in ETL, Big Data/Hadoop, and data warehouse design/delivery.\n2+ years of hands-on experience with Kafka or Kinesis for real-time data streaming.\n4+ years of experience in programming languages such as Python, Java, or Scala.\nExperience using Apache Airflow in at least one project for data pipeline orchestration.\nAt least 1 year of experience developing CI/CD pipelines with GIT, Jenkins, Docker, Kubernetes, Shell Scripting, and Terraform.\nProfessional Attributes:\nWillingness to work in B Shift (12 PM 10 PM IST).\nStrong client-facing skills with the ability to build trusted relationships.\nExcellent problem-solving and critical-thinking abilities.\nStrong communication and collaboration skills in cross-functional teams.\nPreferred Qualifications:\nAWS certifications or relevant cloud training.\nFamiliarity with DataOps or MLOps practices.\nExperience in Agile or Scrum development methodologies.","Databricks SQL, data warehouse design, Delta Lake, Java, Hadoop, Scala, Pyspark, Kafka, Big Data, Apache Airflow, Jenkins, Git, Kinesis, Terraform, Docker, Databricks, Python, Kubernetes, Etl, AWS"
Python Data Engineer,Planful,4-6 Years,,"Hyderabad, India",Login to check your skill match score,"About Us\n\nPlanful is the pioneer of financial performance management cloud software. The Planful platform, which helps businesses drive peak financial performance, is used around the globe to streamline business-wide planning, budgeting, consolidations, reporting, and analytics. Planful empowers finance, accounting, and business users to plan confidently, close faster, and report accurately. More than 1,500 customers, including Bose, Boston Red Sox, Five Guys, Grafton Plc, Gousto, Specialized and Zappos rely on Planful to accelerate cycle times, increase productivity, and improve accuracy. Planful is a private company backed by Vector Capital, a leading global private equity firm. Learn more at planful.com.\n\nAbout The Role\n\nWe are looking for self-driven, self-motivated, and passionate technical experts who would love to join us in solving the hardest problems in the EPM space. If you are capable of diving deep into our tech stack to glean through memory allocations, floating point calculations, and data indexing (in addition to many others), come join us.\n\nPosition Responsibilities\n\nShape financial time-series data: outlier detection/handling, missing-value imputation, techniques for small/limited datasets.\nDevelop & refine forecasting models (ARIMA, Prophet, LSTM/GRU).\nImplement anomaly-detection solutions (Isolation Forest, One-Class SVM, statistical methods).\nProfile & optimize Python code (vectorization, multiprocessing, cProfile).\nMonitor model performance and iterate to improve accuracy.\nCollaborate with data scientists and stakeholders to integrate solutions.\n\nRequired Skills And Experience\n\n4+ years in a mid-level Data Engineer role, preferably in analytics or fintech.\nExpert in Python (pandas, NumPy, SciPy, scikit-learn) with hands-on performance tuning.\nExperience with Flask and Django for building and deploying Python services.\nFamiliarity with AI-assisted development tools and IDEs (Cursor, Windsurf) and modern editor integrations (VS Code + Cline).\nProficiency in C# for .NET integration tasks.\nDemonstrated time-series forecasting expertise (ARIMA, Prophet, LSTM/GRU).\nStrong data preprocessing skills: outlier treatment, imputation, handling sparse datasets.\nProficient in SQL for complex queries on large datasets.\nExcellent analytical thinking, problem-solving, and communication skills.\n\nWhy Planful\n\nPlanful Exists To Enrich The World By Helping Our Customers And Our People Achieve Peak Performance. To Foster The Best In Class Work We're So Proud Of, We've Created a Best In Class Culture, Including\n\n2 Volunteer days, Birthday PTO, and quarterly company Wellness Days\n3 months supply of diapers and meal deliveries for the first month of your Maternity/Paternity leave\nAnnual Planful Palooza, our in-person, company-wide culture kickoff\n\nCompany-wide Mentorship program with Executive sponsorship of CFO and Managed solutions that drive our products and services forward. You will lead a team of talented engineers, collaborating closely with cross-functional teams across our India and North America leadership hubs to deliver high-quality software solutions that meet business objectives. This role offers an exciting opportunity to leverage your expertise in end to end product development to drive innovation, mentor junior team members, and contribute to the overall technical vision of the organization.","AI-assisted development tools, prophet, GRU, scikit-learn, LSTM, Isolation Forest, VS Code, One-Class SVM, Numpy, Sql, Django, Pandas, Python, Arima, Scipy, Flask"
Data Engineer,VARITE INC,5-7 Years,,India,Login to check your skill match score,"VARITE is looking for a qualified Data Engineerfor one of its clients.\nWHAT THE CLIENT DOES\nThe company started out as a hardware/software vendor, but over time added more subscription-based services.\nWHAT WE DO\nEstablished in the Year 2000, VARITE is an award-winning minority business enterprise providing global consulting & staffing services to Fortune 1000 companies and government agencies. With 850+ global consultants, VARITE is committed to delivering excellence to its customers by leveraging its global experience and expertise in providing comprehensive scientific, engineering, technical, and non-technical staff augmentation and talent acquisition services.\nHERE'S WHAT YOU'LL DO:\nJob Details:\nTitle: Data Engineer\nLocation: India/Remote\nDuration: 6 Months\nJob Description: We are looking for a Data Engineer with MFG experience for a staff augmentation role.\nThese are the requirements for the role:\nExpert in Azure Data Factory\nProven experience in Data Modelling for Manufacturing data sources\nProficient SQL design\n+5 years of experience in Data engineering roles\nProve experience in PBI: Dashboarding, DAX calculations, Star scheme development and semantic model building\nManufacturing knowledge\nExperience with GE PPA as data source is desirable\nAPI dev Knowledge\nPython skills\nLocation: Offshore with 3 up to 5 hours overlap with CST time zone\nIf this opportunity interests you, please respond by clicking on EasyApply.\nKnow someone who would be perfect for this role Refer them to us and if they are hired, you could be eligible for our employee referral bonus! Help us grow our team with top talent from your network.\nVARITE is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status","PBI Dashboarding, Star scheme development, GE PPA, DAX calculations, Manufacturing knowledge, semantic model building, Data Modelling, Azure Data Factory, Python, Sql"
Senior Data Engineer,Azoon Tech Consulting LLC,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"About Us\nWe are building a next-generation data engineering platform designed to simplify complex data workflows, automate orchestration, and deliver high-performance pipelines at scale. Our stack includes Python, Apache Airflow, PySpark, and FastAPI, and we're looking for passionate engineers who want to shape the future of data engineering.\nRole Overview\nAs a Principal Data Engineer, you will play a critical role in designing and building core components of our data platform. This is a hands-on leadership role where you'll drive architecture decisions, mentor engineers, and own complex data problems end-to-end. You'll collaborate closely with product and platform teams to define scalable, reliable, and high-performance data systems.\nKey Responsibilities\nDesign and build scalable, reusable data engineering components and frameworks using Python, Airflow, and PySpark.\nLead the architecture of end-to-end data pipelines from ingestion and transformation to orchestration and monitoring.\nBuild and maintain FastAPI-based services to expose metadata, control plane, and developer APIs.\nDrive best practices in software engineering and data architecture (code quality, testing, CI/CD, performance).\nMentor and guide a team of data and backend engineers.\nCollaborate with product managers, designers, and other engineers to deliver features on time and at high quality.\nEvaluate and introduce tools, frameworks, and technologies that improve the developer experience and platform scalability.\nRequirements\nTechnical Skills:\n10+ years of experience in software or data engineering, including experience building large-scale data platforms or products.\nExpert-level Python skills with a strong software engineering background.\nDeep expertise in Apache Airflow (or similar orchestration tools) and PySpark (or distributed data processing frameworks).\nStrong understanding of API design and development using FastAPI or similar frameworks.\nExperience with data modeling, schema design, and working with large-scale data (TB/PB scale).\nHands-on experience with cloud-native data platforms (AWS, GCP, or Azure).\nFamiliarity with containerization (Docker), CI/CD pipelines, and infrastructure-as-code is a plus.\nLeadership & Communication:\nProven track record in leading technical design discussions and making architecture decisions.\nStrong mentorship skills and the ability to drive a high-performing engineering culture.\nExcellent communication skills, both written and verbal, with the ability to convey complex ideas to both technical and non-technical stakeholders.\nNice to Have\nExperience building developer tools or internal platforms.\nExposure to modern data tools like dbt, Snowflake, Delta Lake, etc.\nOpen-source contributions in data or backend engineering.\nWhat We Offer\nOpportunity to lead and shape a cutting-edge data platform from the ground up.\nCollaborative and product-minded engineering culture.\nCompetitive compensation, equity, and benefits.\nFlexible remote/onsite working model.","API design and development, Infrastructure-as-code, Data Modeling, Schema Design, Apache Airflow, FastAPI, Python, Pyspark"
Sr. Data Engineer,Madison Logic,5-7 Years,,"Pune, India",Login to check your skill match score,"About Madison Logic:\nOur team is reshaping B2B marketing and having fun in the process! When joining Madison Logic, you are committing to giving 100% and always striving for more. As a truly global company, we take pride in a diverse culture free from gender, racial, and other forms of bias.\nOur Vision: We empower B2B organizations globally to convert their best accounts faster\nOur Values:\nURGENCY Lead with Action. Prioritize Follow-up.\nACCOUNTABILITY Don't Point Fingers. Take Responsibility.\nINNOVATION Think Big. Innovate.\nRESPECT Respect Customers. Respect Each Other.\nINTEGRITY Act Ethically. Lead by Example.\nAt ML you will work with & learn from an incredible group of people who care about your success as much as they care about their own. Our team is at the heart of what we do and our success starts with you!\nAbout the Role:\nWe are currently seeking a Senior Data Engineer to play a crucial role in shaping the future of our data and analytics capabilities. As a Senior Data Engineer at Madison Logic, your responsibilities will include designing, constructing, and deploying scalable data pipelines. Additionally, you will be responsible for developing APIs that will empower our machine learning products and features. Your expertise will be invaluable in refining data models across various components of our data infrastructure to accommodate the growing demands of data processing and analytics at Madison Logic.\nIn this highly collaborative position, you will closely collaborate with product, engineering, and data teams to achieve our business objectives.\nResponsibilities:\nDevelop and maintain the core data pipelines, involving the creation of production-level SQL and Python code to fuel our platforms.\nAdapt and enhance data models and data schemas to align with both business and engineering requirements.\nConduct data analysis to contribute to the enhancement of overall business performance.\nIdentify and select optimal data sources for specific analytical tasks.\nEstablish procedures for data mining, data modeling, and data production.\nCollaborate with internal and external partners to address challenges and ensure successful outcomes.\nBasic Qualifications:\nOn-site working at the ML physical office, 5-days per week required\nAbility to work UK Shift Timing (11:00am 8:00pm Local Time) Required\nFluent in English language (verbal and written) and possessing a clear and concise communication style.\nEducational Background: Possess a Bachelor's degree in computer science, statistics, or mathematics.\nProgramming Expertise: 5+ years of experience with Python, with the ability to write production-level code.\nSQL Proficiency: 5+ years of experience in SQL, with excellent skills in navigating multiple data tables and comprehending data models.\nCloud Computing: 3+ years of hands-on experience with cloud computing services, particularly AWS (Amazon Web Services).\nData Architecture: Proven experience in designing data architectures, including Kafka, Data Warehouses and Operational Data Stores (ODS).\nCloud-Based Analytics: Possess a strong understanding of cloud-based analytics platforms, such as Snowflake and AWS SageMaker.\nData Workflow Management: Ideally, possess at least 1 year of experience with data workflow management tools, with Airflow experience being a plus.\nData Cleaning: Be skilled in data cleaning and standardization processes.\nSQL Engine: Exhibit an excellent understanding of SQL engines and the capability to perform advanced performance tuning.\nDesired Characteristics:\nSelf-Sufficient and proactive nature, able & comfortable figuring things out, resorting to escalation only when after exhausting all other options\nStrong sense of urgency required\nExceptional communication skills, both verbal and written, with a knack for explaining complex concepts in a clear & concise manner across all levels and functions\nTeam members are encouraged to work collaboratively with an emphasis on results, not on hierarchy or titles.\nIndia-Specific Benefits\n5 LPA Medical Coverage\nLife Insurance\nProvident Fund Contributions\nLearning & Development Stipend (Over-And-Above CTC)\nWellness Stipend (Over-And-Above CTC)\nTransportation available for female team-members with shifts starting or ending between the hours of 9:30pm and 7:00am\nWelcoming in-office environment (located within AWFIS co-working space, Amanora Mall)\nTeam members are encouraged to work collaboratively with an emphasis on results, not on hierarchy or titles.\nExpected Compensation: (Dependent upon Experience)\nFixed CTC: 23,00,000 - 27,00,000 a year\nWork Environment:\nWe offer a mix of in-office and hybrid working. Hybrid remote work arrangements are not available for all positions. Please refer to the job posting detail to determine what in-office requirements apply. Where applicable, hybrid WFH days work must be conducted from your home office located in a jurisdiction in which Madison Logic has the legal right to operate. WFH requires availability and responsiveness on a full-time basis from a distraction free environment with access to high-speed internet. Please inquire for more details.\nPay Transparency/Equity:\nWe are committed to paying our team equitably for their work, commensurate with their individual skills and experience. Salary Range and additional compensation, including discretionary bonuses and incentive pay, are determined by a rigorous review process taking into account the experience, education, certifications and skills required for the specific role, equity with similarly situated team members, as well as employer-verified region-specific market data provided by an independent 3rd party partner.\nWe will provide more information about our perks & benefits upon request.\nOur Commitment to Diversity & Inclusion:\nMadison Logic is proud to be an equal opportunity employer. We are committed to equal employment opportunity regardless of sex, race, color, religion, national origin, sexual orientation, age, marital status, disability, gender identity or Veteran status.\nPrivacy Disclosure:\nAll of the information collected in this form and/or by your application by submission of your online profile is necessary and relevant to the performance of the job applied for. We will process the information provided by you in this form, your CV (including physical and online resume profiles), by the referees you have noted, and by the educational institutions with whom we may undertake to verify your qualifications with, in accordance with our privacy policy and for recruitment purposes only.\nFor more information on how we process the information you have provided including relevant lawful bases (where relevant) please see our privacy policy which is available on our website ( https://www.madisonlogic.com/privacy/ ).","SQL engines, Airflow, AWS SageMaker, snowflake, Data Cleaning, data warehouses, Kafka, Python, Sql"
Sr Data Engineer,ADM,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nYour Responsibilities\n\nDesign, develop, and maintain comprehensive BI models using SAP Datasphere, ensuring alignment with business requirements and objectives.\nCollaborate with business stakeholders to understand data needs and translate them into effective and scalable BI solutions.\nWork closely with SAP BTP to leverage its capabilities for seamless integration, data orchestration, and advanced analytics.\nImplement and enforce data governance best practices to ensure data accuracy, consistency, and compliance with regulatory standards.\nDevelop and maintain data quality measures to proactively identify and address issues in the BI models.\nOptimize BI models for performance and responsiveness, considering scalability and efficiency in handling large datasets.\nCollaborate with IT infrastructure teams to ensure the underlying systems support optimal BI performance.\nProvide training and support to end-users on utilizing BI models effectively for reporting and analysis.\nCollaborate with other teams to address and resolve user-reported issues promptly\nCreate and maintain comprehensive documentation for BI models, data integration processes, and best practices.\nKeep documentation up to date with any changes or enhancements made to the BI environment.\n\nYour Profile\n\nMinimum of 8 years plus of experience in SAP BTP DataSphere and SAC with, SAP BW/4HANA\nIn-depth knowledge of SAP BTP DataSphere architecture, components, and capabilities\nStrong proficiency in SAP BW/4HANA data modeling, data provisioning, and data transformation techniques.\nExpertise in S/4HANA Analytics, including CDS Views Development, SAP HANA Cloud and SAP BW/4HANA.\nExperience in the integration of SAP and non-SAP data sources\nExcellent analytical, problem-solving, and communication skills.\n\n#IncludingYou\n\nDiversity, equity, inclusion and belonging are cornerstones of ADM's efforts to continue innovating, driving growth, and delivering outstanding performance. We are committed to attracting and retaining a diverse workforce and create welcoming, truly inclusive work environments environments that enable every ADM colleague to feel comfortable on the job, make meaningful contributions to our success, and grow their career. We respect and value the unique backgrounds and experiences that each person can bring to ADM because we know that diversity of perspectives makes us better, together.\n\nFor more information regarding our efforts to advance Diversity, Equity, Inclusion & Belonging, please visit our website here: Diversity, Equity and Inclusion | ADM.\n\nAbout ADM\n\nAt ADM, we unlock the power of nature to provide access to nutrition worldwide. With industry-advancing innovations, a complete portfolio of ingredients and solutions to meet any taste, and a commitment to sustainability, we give customers an edge in solving the nutritional challenges of today and tomorrow. We're a global leader in human and animal nutrition and the world's premier agricultural origination and processing company. Our breadth, depth, insights, facilities and logistical expertise give us unparalleled capabilities to meet needs for food, beverages, health and wellness, and more. From the seed of the idea to the outcome of the solution, we enrich the quality of life the world over. Learn more at www.adm.com.\n\nReq/Job ID\n\n97488BR\n\nRef ID","SAP Datasphere, SAP BW 4HANA, Data provisioning, Integration of SAP and non-SAP data sources, CDS Views Development, SAP HANA Cloud, SAP BTP, Data Transformation, Data Modeling"
Data Engineer,Lingaro,8-10 Years,,India,Login to check your skill match score,"Role: Data Engineer Lead Consultant\nLocation: India (Full Time-Remote)\nPreference: Immediate Joiners\nAbout Lingaro:\nLingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.\nSince 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.\nAbout\nData Engineering:Data e\nngineering involves the development of solutions for the collection, transformation, storage and management of data to support data-driven decision making and enable efficient data analysis by end users. It focuses on the technical aspects of data processing, integration, and delivery to ensure that data is accurate, reliable, and accessible in a timely manner. It also focuses on the scalability, cost-effectiveness, security, and supportability of the solution. Data engineering encompasses multiple toolsets and architectural concepts across on-premises and cloud stacks, including but not limited to data warehousing, data lakes, lake house, data mesh, and includes extraction, ingestion, and synchronization of structured and unstructured data across the data ecosystem. It also includes processing organization and orchestration, as well as performance optimization of data processing.Job Re\nsponsibilities:Provid\ne leadership and guidance to the data engineering team, including mentoring, coaching, and fostering a collaborative work environment. Set clear goals, assign tasks, and manage resources to ensure successful project delivery. Work closely with developers to support them and improve data engineering processes. Suppor\nt team members with troubleshooting and resolving complex technical issues and challenges. Provid\ne technical expertise and direction in data engineering, guiding the team in selecting appropriate tools, technologies, and methodologies. Stay updated with the latest advancements in data engineering and ensure the team follows best practices and industry standards. Collab\norate with stakeholders to understand project requirements, define scope, and create project plans. Suppor\nt project managers to ensure that projects are executed effectively, meeting timelines, budgets, and quality standards. Monitor progress, identify risks, and implement mitigation strategies. Act as\na trusted advisor for the customer. Overse\ne the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration. Align\ncoding standards, conduct code reviews to ensure proper code quality level. Identi\nfy and introduce quality assurance processes for data pipelines and workflows. Optimi\nze data processing and storage for performance, efficiency and cost savings. Evalua\nte and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards). Act as\nmain point of contact to other teams/contributors engaged in the project. Mainta\nin technical documentation of the project, control validity and perform regular reviews of it. Ensure\ncompliance with security standards and regulations. Requi\nr\nements:A bach\nelor's or master's degree in Computer Science, Information Systems, or a related field is typically required. Additional certifications in cloud are advantageous. Minimu\nm of 8 years of experience in data engineering or a related field. Strong\ntechnical skills in data engineering, including proficiency in programming languages such as Python, SQL, Pyspark. Famili\narity with Azure cloud platform viz. Azure Databricks, Data Factory, Data Lake etc., and experience in implementing data solutions in a cloud environment. Expert\nise in working with various data tools and technologies, such as ETL frameworks, data pipelines, and data warehousing solutions. In-dep\nth knowledge of data management principles and best practices, including data governance, data quality, and data integration. Excell\nent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues. Knowle\ndge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects. Excell\nent communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams, stakeholders, and senior management. Contin\nuous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies. Consul\nting exposure, with external customer focus mindset is preferred.Why\nj\noin us: Stabl\ne\nemployment. On the market since 2008, 1300+ talents currently on board in 7 global sites.100% remote.Flexibility regarding working hours.Full-time positionComprehensive online onboarding program with a Buddy from day 1.Cooperation with top-tier engineers and experts.Unlimited access to the Udemy learning platform from day 1.Certificate training programs. Lingarians earn 500+ technology certificates yearly.Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.Grow as we grow as a company. 76% of our managers are internal promotions.A diverse, inclusive, and values-driven community.Autonomy to choose the way you work. We trust your ideas.Create our community together. Refer your friends to receive bonuses.Activities to support your well-being and health.Plenty of opportunities to donate to charities and support the environment.","ETL frameworks, data pipelines, data warehousing solutions, Pyspark, Data Factory, Data Lake, Azure Databricks, Python, Sql"
Snowflake Data Engineer,COVET IT INC,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"Hi,\nPlease go through the below requirements and let me know your interest and forward your resume along with your contact information to [HIDDEN TEXT]\nLocation :Bengaluru, KA\nExperience : 7 to 10 years\nJob Description:\nSnowflake + DBT + data stage + strong in sql and basic knowledge in python and Kafka.\nAgile methodology","dbt, snowflake, Agile Methodology, Kafka, Python, Sql, Data Stage"
Senior Data Engineer,MDA Edge,4-6 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Job Summary:\nWe are seeking a results-driven Senior Data Engineer to join our engineering team. The ideal candidate will have hands-on expertise in data pipeline development, cloud infrastructure, and business intelligence (BI) support, with a strong command of the modern data stack.\nIn this role, you will be responsible for building scalable ETL/ELT workflows, managing data lakes and marts, and ensuring seamless data delivery to analytics and BI teams. The position requires deep technical proficiency in PostgreSQL, Python scripting, Apache Airflow, AWS (or other cloud environments), and modern data and BI tools.\nKey Responsibilities:\nPostgreSQL & Data Modeling:\nDesign and optimize complex SQL queries, stored procedures, and indexes.\nPerform performance tuning and query plan analysis.\nContribute to schema design and data normalization.\nData Migration & Transformation:\nMigrate data from multiple sources to cloud or operational data store (ODS) platforms.\nDesign schema mapping and implement transformation logic.\nEnsure consistency, integrity, and accuracy in migrated data.\nPython Scripting for Data Engineering:\nDevelop automation scripts for data ingestion, cleansing, and transformation.\nHandle various file formats (JSON, CSV, XML), REST APIs, and cloud SDKs (e.g., Boto3).\nMaintain reusable script modules for operational pipelines.\nData Orchestration with Apache Airflow:\nDevelop and manage Directed Acyclic Graphs (DAGs) for batch and stream workflows.\nImplement retries, task dependencies, notifications, and failure handling.\nIntegrate Airflow with cloud services, data lakes, and data warehouses.\nCloud Platforms (AWS / Azure / GCP):\nManage data storage solutions (S3, GCS, Blob), compute services, and data pipelines.\nConfigure permissions, IAM roles, encryption, and logging for security.\nMonitor and optimize cost and performance of cloud-based data operations.\nData Marts & Analytics Layer:\nDesign and manage data marts using dimensional models.\nBuild star and snowflake schemas to support BI and self-service analytics.\nImplement incremental load strategies and partitioning for efficiency.\nModern Data Stack Integration:\nWork with tools such as DBT, Fivetran, Redshift, Snowflake, BigQuery, and Kafka.\nSupport modular pipeline design and metadata-driven frameworks.\nEnsure high availability and scalability of the data stack.\nBI & Reporting Tools (Power BI / Superset / Supertech):\nCollaborate with BI teams to design datasets and optimize queries.\nSupport dashboard development and reporting layers.\nManage access, data refresh schedules, and performance optimization for BI tools.\nRequired Skills & Qualifications:\n4-6 years of hands-on experience in data engineering roles.\nStrong SQL skills in PostgreSQL (query tuning, complex joins, stored procedures).\nAdvanced Python scripting skills for automation and ETL processes.\nProven experience with Apache Airflow (custom DAGs, error handling).\nSolid understanding of cloud architecture, particularly AWS.\nExperience with data marts and dimensional data modeling.\nExposure to modern data stack tools (DBT, Kafka, Snowflake, etc.).\nFamiliarity with BI tools such as Power BI, Apache Superset, or Supertech BI.\nKnowledge of version control (Git) and CI/CD pipelines is a plus.\nExcellent problem-solving and communication skills.","dbt, snowflake, Supertech, Fivetran, Apache Superset, BigQuery, Power Bi, PostgreSQL, Kafka, Redshift, Apache Airflow, Git, Python, AWS"
Snowflake Data Engineer,Ventra Health,4-6 Years,,"Chennai, India",Login to check your skill match score,"Ventra is a leading business solutions provider for facility-based physicians practicing anesthesia, emergency medicine, hospital medicine, pathology, and radiology. Focused on Revenue Cycle Management, Ventra partners with private practices, hospitals, health systems, and ambulatory surgery centers to deliver transparent and data-driven solutions that solve the most complex revenue and reimbursement issues, enabling clinicians to focus on providing outstanding care to their patients and communities.\n\nJob Summary\n\nWe are seeking a Snowflake Data Engineer to join our Data & Analytics team. This role involves designing, implementing, and optimizing Snowflake-based data solutions. The ideal candidate will have proven, hands-on data engineering expertise in Snowflake, cloud data platforms, ETL/ELT processes, and Medallion data architecture best practices. The data engineer role has a day-to-day focus on implementation, performance optimization and scalability. This is a tactical role requiring independent data analysis and data discovery to understand our existing source systems, fact and dimension data models, and implement an enterprise data warehouse solution in Snowflake. This role will take direction from the Lead Snowflake Data Engineer and Director of Data Engineering for their work while bringing their own domain expertise and experience.\n\nEssential Functions And Tasks\n\n\nParticipate in the design, development, and maintenance of a scalable Snowflake data solution serving our enterprise data & analytics team.\nImplement data pipelines, ETL/ELT workflows, and data warehouse solutions using Snowflake and related technologies.\nOptimize Snowflake database performance\nCollaborate with cross-functional teams of data analysts, business analysts, data scientists, and software engineers, to define and implement data solutions.\nEnsure data quality, integrity, and governance.\nTroubleshoot and resolve data-related issues, ensuring high availability and performance of the data platform.\n\nEducation And Experience Requirements\n\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\n4+ years of experience in-depth data engineering, with at least 1+ minimum year(s) of dedicated experience engineering solutions in an enterprise scale Snowflake environment.\nTactical expertise in ANSI SQL, performance tuning, and data modeling techniques.\nStrong experience with cloud platforms (preference to Azure) and their data services.\nExperience in ETL/ELT development using tools such as Azure Data Factory, dbt, Matillion, Talend, or Fivetran.\nHands-on experience with scripting languages like Python for data processing.\nSnowflake SnowPro certification; preference to the engineering course path.\nExperience with CI/CD pipelines, DevOps practices, and Infrastructure as Code (IaC).\nKnowledge of streaming data processing frameworks such as Apache Kafka or Spark Streaming.\nFamiliarity with BI and visualization tools such as PowerBI.\n\nKnowledge, Skills, And Abilities\n\n\nFamiliarity working in an agile scum team, including sprint planning, daily stand-ups, backlog grooming, and retrospectives.\nAbility to self-manage medium complexity deliverables and document user stories and tasks through Azure Dev Ops.\nPersonal accountability to committed sprint user stories and tasks\nStrong analytical and problem-solving skills with the ability to handle complex data challenges\nAbility to read, understand, and apply state/federal laws, regulations, and policies.\nAbility to communicate with diverse personalities in a tactful, mature, and professional manner.\nAbility to remain flexible and work within a collaborative and fast paced environment.\nUnderstand and comply with company policies and procedures.\nStrong oral, written, and interpersonal communication skills.\nStrong time management and organizational skills.\n\nVentra Health\n\n\nEqual Employment Opportunity (Applicable only in the US)\n\nVentra Health is an equal opportunity employer committed to fostering a culturally diverse organization. We strive for inclusiveness and a workplace where mutual respect is paramount. We encourage applications from a diverse pool of candidates, and all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, or veteran status. We will provide reasonable accommodations to qualified individuals with disabilities, as needed, to assist them in performing essential job functions.\n\nRecruitment Agencies\n\nVentra Health does not accept unsolicited agency resumes. Ventra Health is not responsible for any fees related to unsolicited resumes.\n\nSolicitation of Payment\n\nVentra Health does not solicit payment from our applicants and candidates for consideration or placement.\n\nAttention Candidates\n\nPlease be aware that there have been reports of individuals falsely claiming to represent Ventra Health or one of our affiliated entities Ventra Health Private Limited and Ventra Health Global Services. These scammers may attempt to conduct fake interviews, solicit personal information, and, in some cases, have sent fraudulent offer letters.\n\nTo protect yourself, verify any communication you receive by contacting us directly through our official channels. If you have any doubts, please contact us at [HIDDEN TEXT] to confirm the legitimacy of the offer and the person who contacted you. All legitimate roles are posted on https://ventrahealth.com/careers/.\n\nStatement of Accessibility\n\nVentra Health is committed to making our digital experiences accessible to all users, regardless of ability or assistive technology preferences. We continually work to enhance the user experience through ongoing improvements and adherence to accessibility standards. Please review at https://ventrahealth.com/statement-of-accessibility/.","Matillion, snowflake, dbt, Fivetran, ELT, Devops, Azure Data Factory, Spark Streaming, Powerbi, Apache Kafka, Ansi Sql, Talend, Azure, Python, Etl"
Senior Data Engineer,DataHavn,5-10 Years,,"Delhi, India",Login to check your skill match score,"Position: Senior Data Engineer\nExperience Required: 5 10 years\nLocation: Noida\nEmployment Type: Full-time\nResponsibilities:\n* Data Pipeline Development:\nBuild and maintain scalable data pipelines to extract, transform, and load (ETL) data from various sources (e.g., databases, APIs, files) into data warehouses or data lakes.\n* Data Infrastructure:\nDesign, implement, and manage data infrastructure components including data warehouses, data lakes, and data marts.\n* Data Quality:\nEnsure high data quality through data validation, cleansing, and standardization processes.\n* Performance Optimization:\nOptimize data pipelines and infrastructure for performance, scalability, and efficiency.\n* Collaboration:\nWork closely with data analysts, data scientists, and business stakeholders to understand data needs and translate them into technical requirements.\n* Tool and Technology Selection:\nEvaluate and select appropriate tools and technologies for data engineering (e.g., SQL, Python, Spark, Hadoop, cloud platforms).\n* Documentation:\nCreate and maintain clear, comprehensive documentation for data pipelines, infrastructure, and processes.\nRequired Skills:\n* Strong proficiency in SQL and at least one programming language (e.g., Python, Java).\n* Hands-on experience with data warehousing and data lake technologies (e.g., Snowflake, AWS Redshift, Databricks).\n* Solid understanding of cloud platforms (e.g., AWS, GCP, Azure) and their data services.\n* Knowledge of data modeling and data architecture principles.\n* Familiarity with ETL/ELT tools and frameworks.\n* Excellent problem-solving and analytical skills.\n* Ability to work independently as well as collaboratively in a team environment.\nPreferred Qualifications:\n* Experience with real-time data processing and streaming technologies (e.g., Kafka, Flink).\n* Understanding of machine learning and AI concepts.\n* Experience with data visualization tools (e.g., Tableau, Power BI).\n* Certifications in cloud platforms or data engineering.","ETL ELT tools, snowflake, AWS GCP Azure, Java, Databricks, Redshift, Python, Sql, AWS"
Data Engineer-Dataiku/Python,InfoCepts,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Position: Data Engineer (Python/Dataiku/Spark/SAS DI)\nLocation: Bangalore/Chennai\nPurpose of the Position:\nAs a Data Engineer, this position requires candidate who are enthusiastic about specialized skills in any DE tools like Python/Dataiku/Spark/SAS DI and features. As a member of the team, you will help our clients, by building data pipelines that supports to progress on their Data and Analytics journey.\nKey Result Areas and Activities:\nDriving Data Engineering Initiatives:\nWork with clients to understand and define the data initiatives based on business needs.\nReview all project architectures/codes and contribute towards the development of the project delivery.\nData Integration and ETL Development:\nDesign, develop, and implement data integration solutions using Data Engineering tools like Python/Dataiku/Spark/SAS data integration.\nDevelop and maintain automated data pipelines for real-time and scheduled data ingestions.\nPerformance Tuning and Optimization:\nConduct performance tuning and optimization of queries and data models.\nOptimize data workflows for efficiency and scalability.\nDatabase Design and Implementation:\nDesign and implement database objects such as tables, views, schemas, and stored procedures.\nEnsure data quality and consistency across the data warehouse.\nCollaboration and Communication:\nWork closely with cross-functional teams to gather requirements and translate them into technical specifications.\nCollaborate seamlessly with clients across multiple geographies.\nProblem Solving and Troubleshooting:\nTroubleshoot and resolve data-related issues.\nApply strong analytical and problem-solving abilities to ensure data quality.\nDrive the Project & Technical Team:\nWork with client to gather the data requirements.\nProvide required technical support to development team while delivering the code/project.\nManage end to project deliverables as technical lead/spoc for client and Infocepts.\nEssential Skills:\nExperience on data engineering tools like Python/Dataiku/Spark/SAS data integration.\nCollate data from various data sources and implement business logic while loading into data mart layer.\nMust have strong SQL experience and extract data from various databases and perform required analysis on data.\nShould have BI knowledge to understand the analytics needs of business users.\nCommunication with business users to understand the requirements. Able to convert the business requirements into technical requirements.\nUnderstanding of data modeling concepts, including relational and dimensional modeling.\nDesirable Skills:\nGood analytical skill to perform data analysis for different banking products data\nExperience on any BI tools like Microstrategy, Power BI\nQualifications:\n3+ years of experience to Design and develop efficient and scalable data integration/engineering solutions to support our organization's data analytics and reporting needs.\nTechnical certifications that attribute to continuous learning aspirations.\nQualities:\nInfluences and implements change with confidence and sound decisions.\nTackles problems head-on with a logical, systematic, and practical approach; follows up with developers.\nConsults, writes, and presents persuasively.\nWorks effectively in self-organized, cross-functional teams.\nIterates based on new information, peer reviews, and feedback.\nCollaborates seamlessly with clients across geographies.\nProficient in English (read/write/speak) and email communication.","Dataiku, Data Modeling, Bi Tools, SAS, Spark, Data Integration, Python, Sql, Etl Development, Database Design"
Azure Fabric Data Engineer,Adastra,6-8 Years,,India,Login to check your skill match score,"Job Summary:\nWe are seeking a highly skilledAzureFabricDataEngineerto lead agile, lab-style development efforts in collaboration directly with business stakeholders. This is a senior-level role requiring deep expertise inMicrosoftFabric,dataengineering, and a solid understanding ofbusiness intelligencepractices. The ideal candidate is a self-starter who can independently developdatasolutions, rapidly prototype, and iterate based on business feedback.\nJob Description:\nDesign, develop, and deploydatapipelines and solutions withinMicrosoftFabric.\nCollaborate closely with business users to rapidly develop and iterate ondata-driven prototypes.\nTranslate business requirements into technical specifications and scalabledatamodels.\nBuild and manageDataflows, Lakehouses, Pipelines, and NotebooksinFabric.\nEnsuredataquality, reliability, and performance across solutions.\nSupportdatavisualization needs and basic BI reporting (Power BI or equivalent).\nMaintain thorough documentation of processes, architectures, and models.\nRequired Skills & Experience:\n6+ years of experience inDataEngineeringwith a focus onAzureservices.\nHands-on experience withMicrosoftFabric(including Lakehouse,DataFactory (Pipelines), Notebooks, etc.).\nProficient inSQL,Python, andPower Query/M.\nStrong understanding ofDataWarehousing,ETL/ELT, andLakehouse architecture.\nExperience working inagile, business-facing development environments.\nFamiliarity withPower BIand building foundational reports/dashboards.\nImmediate joiner would be an advantage\nTo apply for a job, please send your CV to [HIDDEN TEXT]\nFRAUD ALERT: Be cautious of fake job postings and individuals posing as Adastra employees.\nHOW TO VERIFY IT'S US:\nOur employees will only use email addresses ending in @adastragrp.com. Any other domains, even if similar, are not legitimate.\nWe will never request any form of payment, including but not limited to fees, certification costs, or deposits.\nPlease reach out to [HIDDEN TEXT] only if you have any questions.","Lakehouse architecture, Power Query M, Power Bi, Python, Sql, ELT, Etl"
Sigma Computing - Data Engineer / Business Intelligence Engineer,Oncorre Inc,Fresher,,"Pune, India",Login to check your skill match score,"Company Description\nOncorre, Inc. is a boutique digital transformation consultancy headquartered in Bridgewater, New Jersey, USA. Established in 2007, the company provides cutting-edge engineering solutions for Fortune companies and Government Agencies. Oncorre's mission is to help enterprises accelerate the adoption of new technologies, resolve complex issues that arise during digital evolution, and promote ongoing innovation. We serve clients across the US with flexible onsite and offsite models tailored to their needs.\n*** Experience in Sigma computing is must\nOverview:\nOncorre is seeking a talented [Data Engineer/BI Engineer] to develop and optimize data solutions that empower business insights using Sigma Computing. You will work with modern cloud data platforms to build scalable pipelines, enable data exploration, and support analytics initiatives.\nKey Responsibilities:\nDesign and develop data models, reports, and visualizations to support business insights.\nBuild and maintain robust data pipelines and workflows using cloud data platforms like Snowflake.\nCollaborate with product, data, and engineering teams to gather requirements and implement solutions.\nOptimize queries and data processes for performance and accuracy.\nAssist in establishing data governance, security, and compliance standards.\nDocument data workflows, architecture, and best practices.\nStay informed about new data technologies and incorporate them into workflows.\nSkills & Qualifications:\nExperience with cloud data warehouses like Snowflake, Redshift, or BigQuery.\nProficiency in SQL, Python, or other relevant scripting languages.\nFamiliarity with ETL/ELT tools and data pipeline orchestration (e.g., Airflow).\nStrong understanding of data modeling, analytics, and data visualization.\nExcellent problem-solving, communication, and collaborative skills.\nExperience working in a fast-paced, data-driven environment.","Airflow, snowflake, Sigma Computing, BigQuery, Redshift, Sql, Python, ELT, Etl"
Data Engineer/BI Engineer,canibuild,4-6 Years,,India,Login to check your skill match score,"About Us\n\nCanibuild automates the residential construction industry's design, approval, and sales processes, allowing clients to answer Can I build this on this plot of land instantly. As a fast-growing SaaS platform backed by Australia's largest hedge fund, we serve clients across Australia, New Zealand, Canada, and the US.\n\nRole Overview\n\nWe are seeking a highly skilled Data Engineer / BI Engineer with experience in designing ETL/ELT pipelines, managing databases, and developing dashboards using Power BI. The ideal candidate will be responsible for handling end-to-end data workflows, from data extraction and transformation to visualization and insights generation. This role also provides opportunities to upskill and contribute to AI-driven data initiatives, making it an excellent fit for professionals eager to grow in a modern data ecosystem.\n\nKey Responsibilities\n\nDesign, develop, and maintain ETL/ELT pipelines using Python, Airflow, and SQL in an AWS environment\nManage and optimize data lake and data warehouse solutions on AWS\nDevelop and maintain data-driven dashboards and reports in Power BI, connecting to SQL Server and PostgreSQL Aurora databases\nExtract and integrate data from third-party APIs to populate the data lake\nPerform data profiling and source system analysis to ensure data quality and integrity\nCollaborate with business stakeholders to capture and understand data requirements\nOptimize SQL queries for performance and ensure efficient database operations\nImplement best practices for data engineering, visualization, and database management\nParticipate in architectural decisions and contribute to the continuous improvement of data solutions\nFollow agile and lean development practices for efficient project execution\nIndependently validate and assess the accuracy of data outputs before delivery, ensuring results align with business expectations\nProactively evaluate different technical approaches and suggest alternatives to ensure optimal outcomes within the broader system and business context\nStay updated on the latest AI and data engineering advancements, with opportunities to apply AI-powered solutions\n\nRequirements\n\n\n4+ years of experience in data engineering, ETL/ELT pipeline development, and database management\nStrong expertise in SQL (T-SQL, MS SQL) with a focus on query optimization and database performance tuning\nProficiency in Python (including data-specific libraries such as Pandas, NumPy, etc.) and Airflow for ETL/ELT processes\nExperience extracting and managing data from third-party APIs (REST, JSON)\nProven experience in designing and developing data warehouse solutions on AWS\nStrong expertise in Power BI for data visualization, dashboard creation, and connecting to SQL Server/PostgreSQL Aurora\nFamiliarity with agile methodologies and a continuous improvement mindset\nDemonstrated ability to think critically and evaluate multiple technical solutions in the broader context of system architecture and business goals\nExcellent problem-solving skills and the ability to proactively identify and implement alternative solutions\nStrong communication skills and ability to work collaboratively in a team-oriented environment\nWillingness to upskill in AI-driven data solutions and contribute to AI-powered applications\n\nPreferred Qualifications\n\n\nExposure to AWS cloud data services such as RedShift, Athena, Lambda, Glue, etc\nExperience with additional BI tools like Tableau\nKnowledge of data lake architectures and best practices\nExperience with AI-driven data analytics, or integrating AI models with BI solutions\n\nBenefits\n\n\nWork with a talented team of data and AI professionals on meaningful, industry-transforming projects\nOpportunity to gain hands-on experience with AI technologies and modern data engineering practices\nCompetitive salary, benefits, and opportunities for growth\nA collaborative, fast-paced, and innovative culture that values initiative, ownership, and smart solutions\nFlexible remote work opportunities","Airflow, Aurora, Data Warehouse, T-sql, Power Bi, PostgreSQL, Json, Sql, ELT, MS SQL, REST, Data Lake, Python, Etl"
Staff Data Engineer,Zinnia,6-8 Years,,"Pune, India",Login to check your skill match score,"Who We Are\n\nZinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.\n\nWho You Are\n\nAs a seasoned Data Engineer specializing in data engineering, you bring extensive expertise in optimizing data workflows using various database tools like Oracle, BigQuery, and SQL Server. You possess a deep understanding of ELT/ETL processes, data integration, and have a strong command of Python for data manipulation and automation tasks. You will possess advanced expertise in working with data platforms like Google Big Query, DBT, Python, and Airflow. Responsible for designing and maintaining scalable ETL pipelines, optimizing complex data systems, and ensuring smooth data flow across different platforms. As a Senior Data Engineer, you will also be required to work collaboratively in a team and contribute to building data infrastructure that drives business insights\n\nWhat You'll Do\n\nDesign, develop, and optimize complex ETL pipelines that integrate large data sets from various sources.\nBuild and maintain high-performance data models using Google BigQuery and DBT for data transformation.\nDevelop Python scripts for data ingestion, transformation, and automation.\nImplement and manage data workflows using Apache Airflow for scheduling and orchestration.\nCollaborate with data scientists, analysts, and other stakeholders to ensure data availability, reliability, and performance.\nTroubleshoot and optimize data systems, identifying issues and resolving them proactively.\nWork on cloud-based platforms, particularly AWS, to leverage scalability and storage options for data pipelines.\nEnsure data integrity, consistency, and security across systems.\nTake ownership of end-to-end data engineering tasks while mentoring junior team members.\nContinuously improve processes and technologies for more efficient data processing and delivery.\nAct as a key contributor to developing and supporting complex data architectures.\n\nWhat You'll Need\n\nBachelor's degree in computer science, Information Technology, or a related field.\n6+ years of hands-on experience in Data Engineering or related fields, with a strong background in building and optimizing data pipelines\nStrong proficiency in Google Big Query, including designing and optimizing queries.\nAdvanced knowledge of DBT for data transformation and model management.\nProficiency in Python for data engineering tasks, including scripting, data manipulation, and automation.\nSolid experience with Apache Airflow for workflow orchestration and task automation.\nExtensive experience in building and maintaining ETL pipelines.\nFamiliarity with cloud platforms, particularly AWS (Amazon Web Services), including tools like S3, Lambda, Redshift, or Glue.\nJava knowledge is a plus.\nExcellent problem-solving and troubleshooting abilities.\nStrong communication and collaboration skills with the ability to work effectively in a team environment.\nSelf-motivated, detail-oriented, and able to work with minimal supervision.\nAbility to manage multiple priorities and deadlines in a fast-paced environment.\nExperience with other cloud platforms (e.g., GCP, Azure) is a plus.\nKnowledge of data warehousing best practices and architecture.\n\nWHAT'S IN IT FOR YOU\n\nAt Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability","dbt, Google BigQuery, Apache Airflow, SQL Server, Oracle, Python, Etl, AWS"
Data Engineer II,Tekion Corp,3-5 Years,,"Chennai, India",Login to check your skill match score,"About Tekion:\n\nPositively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.\n\nKey Responsibilities:\n\nBe part of developing scalable, reliable and highly available production level data platforms, data pipelines to meet various business needs.\nShould be able to design (high level / low level) software solutions for the new requirements.\nCoding independently and with other team members with proper software industry standard best practices.\nCollaborate with data analysts/ product managers in understanding the data. - Work closely with Data Analysts and QA (Quality Assurance) teams and deliver the product end to end.\n\nQualifications:\n\nB.E/MTech in computer science\n3 - 5 years of relevant work experience.\nExperience in building scalable products with preferably big data.\nExcellent Python coding skills (Mandatory)\nExperience in Apache spark, Data Lake and other Big data technologies.\nExperience in either Data Warehouses or Relational Database is mandatory.\nExperience in AWS cloud\n\nMandatory Skills: Python , Spark\n\nTekion is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, victim of violence or having a family member who is a victim of violence, the intersectionality of two or more protected categories, or other applicable legally protected characteristics.\n\nFor more information on our privacy practices, please refer to our Applicant Privacy Notice here.","data warehouses, Relational Database, Apache Spark, Data Lake, Python, Aws Cloud, Big Data Technologies"
ML Data Engineer,S&P Global,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"About The Role\n\nGrade Level (for internal use):\n\n10\n\nResponsibilities\n\nTo work closely with various stakeholders to collect, clean, model and visualise datasets.\nTo create data driven insights by researching, designing and implementing ML models to deliver insights and implement action-oriented solutions to complex business problems\nTo drive ground-breaking ML technology within the Modelling and Data Science team.\nTo extract hidden value insights and enrich accuracy of the datasets.\nTo leverage technology and automate workflows creating modernized operational processes aligning with the team strategy.\nTo understand, implement, manage, and maintain analytical solutions & techniques independently.\nTo collaborate and coordinate with Data, content and modelling teams and provide analytical assistance of various commodity datasets\nTo drive and maintain high quality processes and delivering projects in collaborative Agile team environments.\n\nRequirements\n\n7+ years of programming experience particularly in Python\n4+ years of experience working with SQL or NoSQL databases.\n1+ years of experience working with Pyspark.\nUniversity degree in Computer Science, Engineering, Mathematics, or related disciplines.\nStrong understanding of big data technologies such as Hadoop, Spark, or Kafka.\nDemonstrated ability to design and implement end-to-end scalable and performant data pipelines.\nExperience with workflow management platforms like Airflow.\nStrong analytical and problem-solving skills.\nAbility to collaborate and communicate effectively with both technical and non-technical stakeholders.\nExperience building solutions and working in the Agile working environment\nExperience working with git or other source control tools\nStrong understanding of Object-Oriented Programming (OOP) principles and design patterns.\nKnowledge of clean code practices and the ability to write well-documented, modular, and reusable code.\nStrong focus on performance optimization and writing efficient, scalable code.\n\nNice To Have\n\nExperience working with Oil, gas and energy markets\nExperience working with BI Visualization applications (e.g. Tableau, Power BI)\nUnderstanding of cloud-based services, preferably AWS\nExperience working with Unified analytics platforms like Databricks\nExperience with deep learning and related toolkits: Tensorflow, PyTorch, Keras, etc.\n\nAbout S&P Global Commodity Insights\n\nAt S&P Global Commodity Insights, our complete view of global energy and commodities markets enables our customers to make decisions with conviction and create long-term, sustainable value.\n\nWe're a trusted connector that brings together thought leaders, market participants, governments, and regulators to co-create solutions that lead to progress. Vital to navigating Energy Transition, S&P Global Commodity Insights coverage includes oil and gas, power, chemicals, metals, agriculture and shipping.\n\nS&P Global Commodity Insights is a division of S&P Global (NYSE: SPGI). S&P Global is the world's foremost provider of credit ratings, benchmarks, analytics and workflow solutions in the global capital, commodity and automotive markets. With every one of our offerings, we help many of the world's leading organizations navigate the economic landscape so they can plan for tomorrow, today.\n\nFor more information, visit http://www.spglobal.com/commodity-insights.\n\nWhat's In It For You\n\nOur Purpose\n\nProgress is not a self-starter. It requires a catalyst to be set in motion. Information, imagination, people, technologythe right combination can unlock possibility and change the world.\n\nOur world is in transition and getting more complex by the day. We push past expected observations and seek out new levels of understanding so that we can help companies, governments and individuals make an impact on tomorrow. At S&P Global we transform data into Essential Intelligence, pinpointing risks and opening possibilities. We Accelerate Progress.\n\nOur People\n\nWe're more than 35,000 strong worldwideso we're able to understand nuances while having a broad perspective. Our team is driven by curiosity and a shared belief that Essential Intelligence can help build a more prosperous future for us all.\n\nFrom finding new ways to measure sustainability to analyzing energy transition across the supply chain to building workflow solutions that make it easy to tap into insight and apply it. We are changing the way people see things and empowering them to make an impact on the world we live in. We're committed to a more equitable future and to helping our customers find new, sustainable ways of doing business. We're constantly seeking new solutions that have progress in mind. Join us and help create the critical insights that truly make a difference.\n\nOur Values\n\nIntegrity, Discovery, Partnership\n\nAt S&P Global, we focus on Powering Global Markets. Throughout our history, the world's leading organizations have relied on us for the Essential Intelligence they need to make confident decisions about the road ahead. We start with a foundation of integrity in all we do, bring a spirit of discovery to our work, and collaborate in close partnership with each other and our customers to achieve shared goals.\n\nBenefits\n\nWe take care of you, so you can take care of business. We care about our people. That's why we provide everything youand your careerneed to thrive at S&P Global.\n\nOur Benefits Include\n\nHealth & Wellness: Health care coverage designed for the mind and body.\nFlexible Downtime: Generous time off helps keep you energized for your time on.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: It's not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.\n\nFor more information on benefits by country visit: https://spgbenefits.com/benefit-summaries\n\nInclusive Hiring And Opportunity At S&P Global\n\nAt S&P Global, we are committed to fostering an inclusive workplace where all individuals have access to opportunities based on their skills, experience, and contributions. Our hiring practices emphasize fairness, transparency, and equal opportunity, ensuring that we attract and retain top talent. By valuing different perspectives and promoting a culture of respect and collaboration, we drive innovation and power global markets.\n\nEqual Opportunity Employer\n\nS&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.\n\nIf you need an accommodation during the application process due to a disability, please send an email to:[HIDDEN TEXT]and your request will be forwarded to the appropriate person.\n\nUS Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law. Pay Transparency Nondiscrimination Provision - https://www.dol.gov/sites/dolgov/files/ofccp/pdf/pay-transp_ English_formattedESQA508c.pdf\n\nIFTECH202.1 - Middle Professional Tier I (EEO Job Group)\n\nJob ID: 314321\n\nPosted On: 2025-04-23\n\nLocation: Hyderabad, Telangana, India","Airflow, Object-Oriented Programming, Pyspark, Hadoop, Kafka, Sql, Tensorflow, Nosql, Git, Pytorch, Spark, Keras, Python"
Data Engineer MS Fabric_8+years,Zorba AI,8-10 Years,,India,Login to check your skill match score,"Company Overview\n\nZorba Consulting India is a leading consultancy firm focused on delivering innovative solutions and strategies to enhance business performance. With a commitment to excellence, we prioritize collaboration, integrity, and customer-centric values in our operations. Our mission is to empower organizations by transforming data into actionable insights and enabling data-driven decision-making. We are dedicated to fostering a culture of continuous improvement and supporting our team members professional development.\n\nRole Responsibilities\n\nDesign and implement data pipelines using MS Fabric.\nDevelop data models to support business intelligence and analytics.\nManage and optimize ETL processes for data extraction, transformation, and loading.\nCollaborate with cross-functional teams to gather and define data requirements.\nEnsure data quality and integrity in all data processes.\nImplement best practices for data management, storage, and processing.\nConduct performance tuning for data storage and retrieval for enhanced efficiency.\nGenerate and maintain documentation for data architecture and data flow.\nParticipate in troubleshooting data-related issues and implement solutions.\nMonitor and optimize cloud-based solutions for scalability and resource efficiency.\nEvaluate emerging technologies and tools for potential incorporation in projects.\nAssist in designing data governance frameworks and policies.\nProvide technical guidance and support to junior data engineers.\nParticipate in code reviews and ensure adherence to coding standards.\nStay updated with industry trends and best practices in data engineering.\n\nQualifications\n\n8+ years of experience in data engineering roles.\nStrong expertise in MS Fabric and related technologies.\nProficiency in SQL and relational database management systems.\nExperience with data warehousing solutions and data modeling.\nHands-on experience in ETL tools and processes.\nKnowledge of cloud computing platforms (Azure, AWS, GCP).\nFamiliarity with Python or similar programming languages.\nAbility to communicate complex concepts clearly to non-technical stakeholders.\nExperience in implementing data quality measures and data governance.\nStrong problem-solving skills and attention to detail.\nAbility to work independently in a remote environment.\nExperience with data visualization tools is a plus.\nExcellent analytical and organizational skills.\nBachelor's degree in Computer Science, Engineering, or related field.\nExperience in Agile methodologies and project management.\n\nSkills: etl processes,sql,python scripting,data integration,performance tuning,cloud technologies,data quality measures,data quality assurance,data modeling,cloud computing (azure, aws, gcp),python,ms fabric,data governance,databricks,data visualization tools,data warehousing","data visualization tools, MS Fabric, ETL processes, Performance Tuning, Data Warehousing, Data Modeling, Sql, Gcp, Data Governance, Azure, Python, AWS, Cloud Computing"
AWS Data Engineer,"GSPANN Technologies, Inc",6-8 Years,,"Bengaluru, India",Login to check your skill match score,"About GSPANN\nGSPANN is a global IT services and consultancy provider headquartered in Milpitas, California (U.S.A.). With five global delivery centers across the globe, GSPANN provides digital solutions that support the customer buying journeys of B2B and B2C brands worldwide.\nWith a strong focus on innovation and client satisfaction, GSPANN delivers cutting-edge solutions that drive business success and operational excellence. GSPANN helps retail, finance, manufacturing, and high-technology brands deliver competitive customer experiences and increased revenues through our solution delivery, technologies, practices, and operations for each client. For more information, visit www.gspann.com\nJD for your reference:\nGSPANN is looking for AWS Data Engineer. As we march ahead on a tremendous growth trajectory, we seek passionate and talented professionals to join our growing family.\nJob Position-\nAWS Data Engineer\nExperience- 6+ years\nLocation- Bangalore\nSkills- AWS+Redshift+Snowflake, SQL, Bigdata, StepFunction, Python/ PySpark, Airflow\nResponsibilities\nActively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing, roll-out, and support.\nSolve complex business problems by utilizing a disciplined development methodology.\nProduce scalable, flexible, efficient, and supportable solutions using appropriate technologies.\nAnalyse the source and target system data. Map the transformation that meets the requirements.\nInteract with the client and onsite coordinators during different phases of a project.\nDesign and implement product features in collaboration with business and Technology stakeholders.\nAnticipate, identify, and solve issues concerning data management to improve data quality.\nClean, prepare, and optimize data at scale for ingestion and consumption.\nSupport the implementation of new data management projects and re-structure the current data architecture.\nImplement automated workflows and routines using workflow scheduling tools.\nUnderstand and use continuous integration, test-driven development, and production deployment frameworks.\nParticipate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards.\nAnalyze and profile data for the purpose of designing scalable solutions.\nTroubleshoot straightforward data issues and perform root cause analysis to proactively resolve product issues.\nRequired Skills\n6+ years experience developing Data and analytic solutions.\nExperience building data lake solutions leveraging one or more of the following AWS, EMR, S3, Hive & PySpark\nExperience with relational SQL\nExperience with scripting languages such as Python\nExperience with source control tools such as GitHub and related dev process\nExperience with workflow scheduling tools such as Airflow\nIn-depth knowledge of AWS Cloud (S3, EMR, Databricks)\nHas a passion for data solutions.\nHas a strong problem-solving and analytical mindset\nWorking experience in the design, Development, and test of data pipelines.\nExperience working with Agile Teams.\nAble to influence and communicate effectively, both verbally and in writing, with team members and business stakeholders\nAble to quickly pick up new programming languages, technologies, and frameworks.\nBachelor's Degree in computer science\nWhy Choose GSPANN\nAt GSPANN, we don't just serve our clientswe co-create. The GSPANNians are passionate technologists who thrive on solving the toughest business challenges, delivering trailblazing innovations for marquee clients. This collaborative spirit fuels a culture where every individual is encouraged to sharpen their skills, feed their curiosity, and take ownership to learn, experiment, and succeed.\nWe believe in celebrating each other's successesbig or smalland giving back to the communities we call home. If you're ready to push boundaries and be part of a close-knit team that's shaping the future of tech, we invite you to carry forward the baton of innovation with us.\nLet's Co-Create the FutureTogether.\nDiscover Your Inner Technologist\nExplore and expand the boundaries of tech innovation without the fear of failure.\nAccelerate Your Learning\nShape your career while scripting the future of tech. Seize the ample learning opportunities to grow at a rapid pace.\nFeel Included\nAt GSPANN, everyone is welcome. Age, gender, culture, and nationality do not matter here, what matters is YOU.\nInspire and Be Inspired\nWhen you work with the experts, you raise your game. At GSPANN, you're in the company of marquee clients and extremely talented colleagues.\nEnjoy Life\nWe love to celebrate milestones and victories, big or small. Ever so often, we come together as one large GSPANN family.\nGive Back\nTogether, we serve communities. We take steps, small and large so we can do good for the environment, weaving in sustainability and social change in our endeavors.\nWe invite you to carry forward the baton of innovation in technology with us.\nLet's Co-Create\nGSPANN | Consulting Services, Technology Services, and IT Services Provider\nGSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.\nGSPANN | Consulting Services, Technology Services, and IT Services Provider\nGSPANN provides consulting services, technology services, and IT services to e-commerce businesses with high technology, manufacturing, and financial services.","Airflow, snowflake, StepFunction, Pyspark, Bigdata, Redshift, Python, Sql, AWS"
Junior Data Engineer (Fabric),PMC,Fresher,,"Vadodara, India",Login to check your skill match score,"Summary Of The Position\n\nThis role supports the integration, transformation, and delivery of data using tools within the Microsoft Fabric platform. The role will work within our data engineering team with responsibility for Data and Insights solutions, delivering high quality data to the business to support our analytics capability.\n\nKey Accountabilities\n\nAssist in building and maintaining ETL pipelines using Azure Data Factory and other Fabric tools.\nWork closely with senior engineers and analysts to gather requirements and build working prototypes.\nSupport data integration from internal, third-party, and public sources.\nParticipate in developing and maintaining Data Warehouse schemas.\nContribute to documentation and testing efforts to ensure data reliability.\nLearn and apply data standards and governance practices as guided by the team.\n\nSkills and Experience | Essential\n\nKnowledge of data engineering concepts and data structures.\nExposure to Microsoft data tools such as Azure Data Factory, OneLake, or Synapse.\nUnderstanding of ETL processes and data pipelines.\nAbility to work collaboratively in an Agile/Kanban team environment.\nMicrosoft certified Fabric DP-600 or DP-700 is big plus , plus any other relevant Azure Data certification is advantage\n\nSkills and Experience | Desirable\n\nFamiliarity with Medallion Architecture principles.\nExposure to MS Purview or other data governance tools.\nUnderstanding of data warehousing and reporting concepts.\nInterest or experience in retail data domains.","Data Warehouse schemas, ETL processes, Azure Data certification, Data pipelines, Microsoft Fabric, Microsoft certified Fabric DP-600, Azure Data Factory"
"Lead, Big Data Engineer",Qualys,10-12 Years,INR 40 - 52 LPA,"Pune, India",Login to check your skill match score,"Come work at a place where innovation and teamwork come together to support the most exciting missions in the world!\n\nJob Description:\n\nWe are seeking a talented Lead Big Data Engineer to deliver roadmap features of Unified Asset Inventory. This is a great opportunity to be an integral part of a team building Qualys next generation Micro-Services based platform processing over a 100 million transactions and terabytes of data per day, leverage open-source technologies, and work on challenging and business-impacting projects.\n\nResponsibilities:\n\nYou will be building the Unified Asset Management product in the cloud\n\nYou will be building highly scalable Micro-services that interacts with Qualys Cloud Platform. Research, evaluate and adopt next generation technologies\n\nProduce high quality software following good architecture and design principles that you and your team will find easy to work with in the future\n\nThis is a fantastic opportunity to be an integral part of a team building Qualys next generation platform using Big Data & Micro-Services based technology to process over billions of transactions data per day, leverage open-source technologies, and work on challenging and business-impacting initiatives.\n\nQualifications:\n\nBachelor's degree in computer science or equivalent\n10+ years of total experience.\n4+ years of relevant experience in design and architecture Big Data solutions using Spark\n3+ years experience in working with engineering resources for innovation.\n4+ years experience in understanding Big Data events flow pipeline.\n3+ years experience in performance testing for large infrastructure.\n3+ In depth experience in understanding various search solutions solr/elastic.\n3+ years experience in Kafka\nIn depth experience in Data lakes and related ecosystems.\nIn depth experience of messing queue\nIn depth experience in giving requirements to build a scalable architecture for Big data and Micro-services environments.\nIn depth experience in understanding caching components or services\nKnowledge in Presto technology.\nKnowledge in Airflow.\nHands-on experience in scripting and automation\nIn depth understanding of RDBMS/NoSQL, Oracle , Cassandra , Kafka , Redis, Hadoop, lambda architecture, kappa , kappa ++ architectures with flink data streaming and rule engines\nExperience in working with ML models engineering and related deployment.\nDesign and implement secure big data clusters to meet many compliances and regulatory requirements.\nExperience in leading the delivery of large-scale systems focused on managing the infrastructure layer of the technology stack.\nStrong experience in doing performance benchmarking testing for Big data technologies.\nStrong troubleshooting skills.\nExperience leading development life cycle process and best practices\nExperience in Big Data services administration would be added value.\nExperience with Agile Management (SCRUM, RUP, XP), OO Modeling, working on internet, UNIX, Middleware, and database related projects.\nExperience mentoring/training the engineering community on complex technical issue.","Airflow, OO Modeling, Caching Components, Flink, ML Models Engineering, Elastic, Data Lakes, agile management, Xp, Micro-services, Performance Benchmarking, Scripting, Cassandra, Kafka, Nosql, RDBMS, Oracle, Hadoop, Solr, Big Data, Scrum, Middleware, Automation, Redis, UNIX, Rup, Lambda Architecture, Presto, Spark"
"Senior Data Engineer( Snowflake, DBT)","Enterprise Minds, Inc",5-7 Years,,India,Login to check your skill match score,"Hiring: Senior Data Engineer (Strong Snowflake, SQL, and DBT Expertise) | Remote\nWe are looking for a senior data engineer with strong expertise in Snowflake and SQL to join our team. If you have a passion for data engineering, this is your perfect role!\nLocation: Remote\nExperience: 5+ Years\nEmployment Type: Full-Time\nKey Responsibilities:\nDevelop and maintain scalable data pipelines using DBT and orchestrate workflows (ideally with Airflow).\nWork with Snowflake to model data and optimize queries for performance.\nApply strong SQL skills to extract, transform, and load data.\nInterpret and understand Python scripts used within data workflows.\nDesign data models with a clear understanding of relational database concepts (primary/foreign keys, normalization, etc.).\nImplement unit testing and data quality tests using DBT best practices.\nCollaborate using Git/GitHub and contribute to a CI/CD pipeline for version control and deployment.\nRequired Skills:\nStrong SQL expertise (query optimization, joins, CTEs, etc.)\nExperience with Snowflake or similar cloud data warehouses.\nWorking knowledge of Python (reading/debugging existing code is sufficient).\nDeep understanding of DBT, including:\nProject structure, models, macros, hooks, and YAML configuration\nData testing: generic, singular, custom, and unit tests\nProficiency in Git/GitHub for version control.\nFamiliarity with CI/CD workflows in data projects.\nNice to Have:\nExperience with Apache Airflow for pipeline orchestration.\nUnderstanding of financial data concepts (useful for data validation and table design).\nApply Now! Send your resume to [HIDDEN TEXT]","snowflake, dbt, Git, Sql, Apache Airflow, Python, Github"
"Senior Data Engineer ( python , Java and SQL , Data pipelines )",NielsenIQ,7-9 Years,,India,IT/Computers - Software,"Job Description\nSenior Data Engineer\nMission of the Role\nYou are a passionate and skilled data engineer who enjoys building scalable, production-grade data pipelines that enable machine learning and AI at scale. You thrive on solving complex data challenges and understand how critical a robust data pipeline is to the success of any predictive model.\nAs a Senior Data Engineer on the Forecasting team, you will be responsible for designing, implementing, and maintaining the data pipelines that power deep neural network models used for global sales and revenue forecasting. You will work closely with ML engineers, data scientists, and product stakeholders to build automated, secure, and cost-efficient pipelines using GCP-native services.\nYou take pride in writing clean, testable, and production-ready code, and are comfortable operating in cloud environments. You know how to make data pipelines observable, recoverable, and performant. You are also comfortable taking architectural ownership and contributing to long-term platform improvements.\nYou will:\nDesign and build robust, scalable, and maintainable data pipelines for training and inference of deep learning models used in forecasting and other Machine Learning based algorithms.\nDevelop and orchestrate end-to-end workflows using GCP-native tools such as Cloud Workflows, Cloud Run, BigQuery, Cloud Functions, and Pub/Sub.\nAutomate and maintain the pipeline lifecycle, including data extraction, transformation, loading (ETL/ELT), and triggering model runs.\nMonitor and Optimise performance, cost-efficiency, and data freshness across all stages of the pipeline.\nCollaborate with data scientists to productionise AI/ML models and integrate them seamlessly into business workflows.\nOwn the data ingestion, validation, and transformation logic powering the forecasting models, ensuring high-quality, consistent input data.\nHandle deployment, versioning, and configuration of pipeline components using Docker, Git, and CI/CD practices.\nImplement logging, monitoring, and alerting to ensure reliability and traceability in a high-scale environment.\nReview code, mentor junior engineers, and help define best practices in our evolving data engineering stack.\nQualifications\nYou have:\n7+ years of experience in data engineering or backend engineering roles.\nStrong expertise in Python and SQL, with experience building production-grade data pipelines.\nSolid understanding of Docker, Git, and shell scripting in Linux environments.\nHands-on experience with GCP services\nExperience in building, deploying, and maintaining data workflows that feed AI/ML models.\nFamiliarity with model lifecycle management and infrastructure challenges in ML pipelines.\nProficiency in setting up CI/CD pipelines for data and ML systems using GitLab CI, Cloud Build, or similar tools.\nExposure to Java for backend services or pipeline components (even if not primary language).\nA proactive, collaborative mindset and strong communication skills across engineering and data science teams.\nNice to have:\nExposure to forecasting or time series modelling pipelines.\nExperience with event-driven architectures.\nFamiliarity with infrastructure-as-code tools like Terraform\nUnderstanding of data quality frameworks and observability tools\nKnowledge of model versioning tools and experiment tracking systems\nAdditional Information\nWhy Join us\nYou'll be working on a high-impact platform that forecasts consumer demand and drives business-critical decisions globally\nYou'll be part of a tight-knit, collaborative, and innovative Forecasting & AI team within a leading global data science organisation\nYou'll have access to top-tier infrastructure, GCP services, and challenging real-world problems in predictive modelling\nFlexible working hours, remote-friendly culture, and strong focus on personal and professional growth\nCompetitive compensation and performance-based bonuses\nOur Benefits\nFlexible working environment\nVolunteer time off\nLinkedIn Learning\nEmployee-Assistance-Program (EAP)\nAbout NIQ\nNIQ is the world's leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights-delivered with advanced analytics through state-of-the-art platforms-NIQ delivers the Full View. NIQ is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world's population.\nFor more information, visit NIQ.com\nWant to keep up with our latest updates\nFollow us on: | | |\nOur commitment to Diversity, Equity, and Inclusion\nNIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. Learn more about how we are driving diversity and inclusion in everything we do by visiting the NIQ News Center:","Cloud Run, Cloud Workflows, GCP-native tools, Pub Sub, Cloud Functions, Python, Sql, BigQuery, Shell scripting, Java, Docker, Git"
Data Engineer -Pharma Commerical Domain,Predigle,3-5 Years,,"Chennai, India",Login to check your skill match score,"We are seeking an experiencedData Engineerwith a strong background inPharma Commercial Datato join our growing data team. The ideal candidate will have hands-on experience withAzure Databricks,Snowflake, and a deep understanding of pharmaceutical commercial datasets, including sales, claims, and HCP-level data.\nKey Responsibilities\nDesign, build, and maintain scalable and efficient data pipelines focused onPharma Commercial datasets\nDevelop data integration workflows usingAzure Databricksand manage data warehousing inSnowflake.\nWork closely with business stakeholders, analytics teams, and data scientists to ensure data solutions support strategic commercial initiatives.\nEnsure data quality, consistency, and governance across all pharma commercial datasets.\nAutomate routine data processes and monitor pipeline performance for production stability.\nParticipate in architecture reviews and recommend improvements for performance and scalability.\nRequired Qualifications\nMinimum3 years of hands-on experience with Pharma Commercial Data, including datasets like sales, prescription, claims, payer data, or CRM/HCP information.\nProven expertise inAzure DatabricksandSnowflake.\nStrong proficiency inSQLandPythonfor data manipulation and transformation.\nSolid understanding of data modeling, ETL/ELT frameworks, and cloud-based data engineering best practices.\nExperience with data orchestration tools (e.g., Airflow, Azure Data Factory).\nAbility to work independently and communicate effectively with business and technical teams.\nPreferred Qualifications\nExposure to commercial analytics use cases like HCP segmentation, field force effectiveness, and targeting.\nFamiliarity with data privacy, HIPAA, and compliance regulations in the pharma domain.\nExperience working in Agile teams and DevOps environments.\nWhat We Offer\nOpportunity to work on impactful commercial data initiatives in the pharmaceutical industry.\nCompetitive compensation and benefits.\nRemote-friendly, flexible work culture.\nSupportive team environment and continuous learning opportunities.\nAbout Predigle:\nPredigle, an EsperGroup company, is an American multinational organization focused on building a disruptive technology platform that revolutionizes the way businesses conduct their daily operations.\nPredigle has grown rapidly to offer multiple products and services,As a growing startup, we offer an entrepreneurial work environment where ideas are valued, creativity is encouraged, and learning opportunities are immense.\nhttps://espergroup.com\n/ https://predigle.com/\nhttps://www.linkedin.com/company/predigle/","Airflow, snowflake, data orchestration tools, Azure Data Factory, Azure Databricks, Python, Sql, Etl, ELT"
Senior Data Engineer,NovusPlatform.io,6-8 Years,,"Hyderabad, India",Login to check your skill match score,"We are hiring Data Platform Engineers for our Hyderabad Office (Hybrid Mode)\nRole: Data Platform Engineer | 5 positions\nLoc.: Hyderabad\nWork Mode: Hybrid (3 days a week)\n- 6 to 8 years of exp.\n- immediate joiner\n- For a Healthcare Data Analytics client\nMust have Hands-on Skills:\nProficient with Python Programming, Scala, SQL, Redshift\nAWS: Experience with the AWS ecosystem\nTerraform: Good Knowledge of Infrastructure as a code using Terraform\nJenkins: CI/CD pipelines using Jenkins\nDocker: Containerization with Docker\nApply at the earliest:\n[HIDDEN TEXT]\n#HiringNow\n#DataPlatformEngineer\n#DataJobs\n#HyderabadJobs\n#ImmediateJoiners\n#PythonJobs\n#DataEngineering\n#ITJobsIndia","Jenkins, Docker, Terraform, Scala, Redshift, Sql, AWS, Python Programming"
Data Engineer,PayPay India,4-6 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Job Description\nPayPay's growth is driving a rapid expansion of PayPay product teams and the need for a robust Data Engineering Platform to support our growing business needs is more critical than ever. The DaaS team's responsibility is to design, implement, and operate this platform using cutting edge technologies such as Spark, Hudi, Delta Lake, Scala, and AWS suite of data tools.\nWe are looking for talented Data Engineers to join our team and help us scale our platform across the organizations.\nMain Responsibilities\nDesign, develop, and maintain scalable data ingestion pipelines using Databricks, Airflow, Kafka, AWS Lambda, and Terraform.\nOptimize and manage large scale data pipelines to ensure high performance, reliability, and efficiency.\nImplement data processing workflows using Delta Lake, Databricks, Python, and Scala.\nDesign and maintain Databricks Unity Catalog for effective data management and discovery.\nCollaborate with cross-functional teams to ensure seamless data flow and integration across the organization.\nImplement best practices for observability, data governance, security, and compliance.\nQualifications\n4+ years experience as a Data Engineer or in a similar role.\nHands-on experience with Delta Lake, Hudi, Spark, and Scala.\nExperience designing, building, and operating a DataLake or Data Warehouse.\nKnowledge of Data Orchestration tools such as Airflow, Dagster, Prefect.\nStrong expertise in AWS services, including Glue, Step Functions, Lambda, and EMR.\nFamiliarity with change data capture tools like Canal, Debezium, and Maxwell.\nExperience with data warehousing tools like Databricks, Snowflake, and BigQuery.\nExperience in at least one primary language (e.g. Scala, Python) and SQL (any variant).\nExperience with data cataloging and metadata management using Databricks Unity Catalog, AWS Glue Data Catalog, or Lakeformation.\nProficiency in Terraform for infrastructure as code (IaC).\nStrong problem-solving skills and ability to troubleshoot complex data issues.\nExcellent communication and collaboration skills.\nAbility to work in a fast-paced, dynamic environment and manage multiple tasks simultaneously.","snowflake, Databricks Unity Catalog, Canal, Airflow, maxwell, Data Catalog, Delta Lake, Hudi, Debezium, Lakeformation, Sql, Databricks, AWS Glue, Aws Lambda, Kafka, BigQuery, AWS, Python, Terraform, Scala, Spark"
