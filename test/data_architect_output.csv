job_title,company_name,experience,salary,location,industry,job_description,skills
Data Architect_Remote,OSI Digital,8-13 Years,,Chennai,"Information Technology, Information Services","We are seeking a highly skilled and experiencedData Architectwith strong expertise indata modelingandSnowflaketo design, develop, and optimize enterprise data architecture. The ideal candidate will play a critical role in shaping data strategy, building scalable models, and ensuring efficient data integration and governance.\nKey Responsibilities:\nDesign and implement end-to-end data architecture using Snowflake\nDevelop and maintain conceptual, logical, and physical data models.\nDefine and enforce data architecture standards, best practices, and policies.\nCollaborate with data engineers, analysts, and business stakeholders to gather requirements and design data solutions.\nOptimize Snowflake performance including data partitioning, caching, and query tuning.\nCreate and manage data dictionaries, metadata, and lineage documentation.\nEnsure data quality, consistency, and security across all data platforms.\nSupport data integration from various sources (cloud/on-premises) into Snowflake.\nRequired Skills and Experience:\n8+ years of experience in data architecture, data modeling, or similar roles.\nHands-on expertise withSnowflakeincluding Snowpipe, Streams, Tasks, and Secure Data Sharing.\nStrong experience withdata modeling tools(e.g., Erwin, ER/Studio, dbt).\nProficiency inSQL,ETL/ELT pipelines, anddata warehousing concepts.\nExperience working with structured, semi-structured (JSON, XML), and unstructured data.\nSolid understanding of data governance, data cataloging, and security frameworks.\nExcellent analytical, communication, and stakeholder management skills.\nPreferred Qualifications:\nExperience with cloud platforms likeAWS,Azure, orGCP.\nFamiliarity with data lakehouse architecture and real-time data processing.\nSnowflake Certification(s) or relevant cloud certifications.\nKnowledge of Python or scripting for data automation is a plus.","snowflake, Data lakehouse, Data Modeling, Data Architecture, Python, Sql, Aws"
Data Architect_Remote,OSI Digital,8-13 Years,,Bengaluru,"Information Technology, Information Services","We are seeking a highly skilled and experiencedData Architectwith strong expertise indata modelingandSnowflaketo design, develop, and optimize enterprise data architecture. The ideal candidate will play a critical role in shaping data strategy, building scalable models, and ensuring efficient data integration and governance.\nKey Responsibilities:\nDesign and implement end-to-end data architecture using Snowflake\nDevelop and maintain conceptual, logical, and physical data models.\nDefine and enforce data architecture standards, best practices, and policies.\nCollaborate with data engineers, analysts, and business stakeholders to gather requirements and design data solutions.\nOptimize Snowflake performance including data partitioning, caching, and query tuning.\nCreate and manage data dictionaries, metadata, and lineage documentation.\nEnsure data quality, consistency, and security across all data platforms.\nSupport data integration from various sources (cloud/on-premises) into Snowflake.\nRequired Skills and Experience:\n8+ years of experience in data architecture, data modeling, or similar roles.\nHands-on expertise withSnowflakeincluding Snowpipe, Streams, Tasks, and Secure Data Sharing.\nStrong experience withdata modeling tools(e.g., Erwin, ER/Studio, dbt).\nProficiency inSQL,ETL/ELT pipelines, anddata warehousing concepts.\nExperience working with structured, semi-structured (JSON, XML), and unstructured data.\nSolid understanding of data governance, data cataloging, and security frameworks.\nExcellent analytical, communication, and stakeholder management skills.\nPreferred Qualifications:\nExperience with cloud platforms likeAWS,Azure, orGCP.\nFamiliarity with data lakehouse architecture and real-time data processing.\nSnowflake Certification(s) or relevant cloud certifications.\nKnowledge of Python or scripting for data automation is a plus.","snowflake, Data lakehouse, Data Modeling, Data Architecture, Python, Sql, Aws"
Principal Data Architect- Data Modelling,Tiger Analytics,16-21 Years,,Delhi NCR,Consulting,"Role & responsibilities\nUnderstand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.\nWork as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.\nLead cross-functional teams, define data strategies, andleveragethe latest technologies in data handling.\nDefine and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.\nSuggest best modelling approach to the client based on their requirement and target architecture.\nAnalyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.\nProfile the Data sets to generate relevant insights.\nOptimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.\nEstablish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.\nDrives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.\nClosely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.\nGuide /mentor team members, and review artifacts.\nContribute to the overall data strategy and roadmaps.\nPropose and execute technical assessments, proofs of concept to promote innovation in the data space.\nPreferred candidate profile\nMinimum 16 years of experience\nDeep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.\nExperience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.\nDecent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server) Good understanding of Relational, Dimensional, Data Vault Modelling\nExperience in implementing 2 or more data models in a database with data security and access controls.\nGood experience in OLTP and OLAP systems\nExcellent Data Analysis skills with demonstrable knowledge on standard datasets and sources.\nGood Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)\nExperience on one or more cloud platforms (e.g. AWS, Azure, GCP)\nUnderstanding of DevOps processes\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python\nExperience in leading the large and complex teams\nGood understanding of agile methodology","Medallion Architecture, Solution Architecting, Data Modeling, Data Architecture, Data Warehousing, Rfp, Sql"
Principal Data Architect- Data Modelling,Tiger Analytics,16-21 Years,,Gurugram,Consulting,"Role & responsibilities\nUnderstand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.\nWork as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.\nLead cross-functional teams, define data strategies, andleveragethe latest technologies in data handling.\nDefine and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.\nSuggest best modelling approach to the client based on their requirement and target architecture.\nAnalyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.\nProfile the Data sets to generate relevant insights.\nOptimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.\nEstablish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.\nDrives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.\nClosely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.\nGuide /mentor team members, and review artifacts.\nContribute to the overall data strategy and roadmaps.\nPropose and execute technical assessments, proofs of concept to promote innovation in the data space.\nPreferred candidate profile\nMinimum 16 years of experience\nDeep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.\nExperience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.\nDecent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server) Good understanding of Relational, Dimensional, Data Vault Modelling\nExperience in implementing 2 or more data models in a database with data security and access controls.\nGood experience in OLTP and OLAP systems\nExcellent Data Analysis skills with demonstrable knowledge on standard datasets and sources.\nGood Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)\nExperience on one or more cloud platforms (e.g. AWS, Azure, GCP)\nUnderstanding of DevOps processes\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python\nExperience in leading the large and complex teams\nGood understanding of agile methodology","Medallion Architecture, Solution Architecting, Data Modeling, Data Architecture, Data Warehousing, Rfp, Sql"
Data Architect-Data Modelling,Tiger Analytics,10-15 Years,,Hyderabad,Consulting,"Role & responsibilities\nAs an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.\nOn a typical day, you might\nEngage the clients & understand the business requirements to translate those into data models.\nAnalyze customer problems, propose solutions from a data structural perspective, and estimate and deliver proposed solutions.\nCreate and maintain a Logical Data Model (LDM) and Physical Data Model (PDM) by applying best practices to provide business insights.\nUse the Data Modelling tool to create appropriate data models\nCreate and maintain the Source to Target Data Mapping document that includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.\nGather and publish Data Dictionaries.\nIdeate, design, and guide the teams in building automations and accelerators\nInvolve in maintaining data models as well as capturing data models from existing databases and recording descriptive information.\nContribute to building data warehouse & data marts (on Cloud) while performing data profiling and quality analysis.\nUse version control to maintain versions of data models.\nCollaborate with Data Engineers to design and develop data extraction and integration code modules.\nPartner with the data engineers & testing practitioners to strategize ingestion logic, consumption patterns & testing.\nIdeate to design & develop the next-gen data platform by collaborating with cross-functional stakeholders.\nWork with the client to define, establish and implement the right modelling approach as per the requirement\nHelp define the standards and best practices\nInvolve in monitoring the project progress to keep the leadership teams informed on the milestones, impediments, etc.\nCoach team members, and review code artifacts.\nContribute to proposals and RFPs\nPreferred candidate profile\n10+ years of experience in Data space.\nDecent SQL knowledge\nAble to suggest modeling approaches for a given problem.\nSignificant experience in one or more RDBMS (Oracle, DB2, and SQL Server)\nReal-time experience working in OLAP & OLTP database models (Dimensional models).\nComprehensive understanding of Star schema, Snowflake schema, and Data Vault Modelling. Also, on any ETL tool, Data Governance, and Data quality.\nEye to analyze data & comfortable with following agile methodology.\nAdept understanding of any of the cloud services is preferred (Azure, AWS & GCP)\nEnthuse to coach team members & collaborate with various stakeholders across the organization and take complete ownership of deliverables.\nExperience in contributing to proposals and RFPs\nGood experience in stakeholder management\nDecent communication and experience in leading the team","Cloud Platforms, Data Modeling, Data Architecture, OLAP, Data Warehousing, Sql, Oltp"
Principal Data Architect - Data Modelling,Tiger Analytics,16-22 Years,,Bengaluru,Consulting,"Role & responsibilities:\nMinimum 15 years of experience\nDeep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.\nExperience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.\nDecent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server)\nGood understanding of Relational, Dimensional, Data Vault Modelling\nExperience in implementing 2 or more data models in a database with data security and access controls. Good experience in OLTP and OLAP systems\nExcellent Data Analysis skills with demonstrable knowledge on standard datasets and sources. Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)\nExperience on one or more cloud platforms (e.g. AWS, Azure, GCP)\nUnderstanding of DevOps processes\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python\nExperience in leading the large and complex teams\nGood understanding of agile methodology\nExtensive expertise in leading data transformation initiatives, driving cultural change, and promoting a data driven mindset across the organization.\nExcellent Communication skills\nRequirements:\nUnderstand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.\nWork as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.\nLead cross-functional teams, define data strategies, and leverage the latest technologies in data handling.\nDefine and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.\nSuggest best modelling approach to the client based on their requirement and target architecture.\nAnalyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.\nProfile the Data sets to generate relevant insights.\nOptimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.\nEstablish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.\nDrives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.\nClosely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.\nGuide /mentor team members, and review artifacts.\nContribute to the overall data strategy and roadmaps.\nPropose and execute technical assessments, proofs of concept to promote innovation in the data space.","Solution Architecting, data vault, Data Modeling, Data Architecture, Dimensional Modeling, Data Warehousing, Oltp"
Principal Data Architect - Data Modelling,Tiger Analytics,16-22 Years,,Chennai,Consulting,"Role & responsibilities:\nMinimum 15 years of experience\nDeep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.\nExperience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.\nDecent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server)\nGood understanding of Relational, Dimensional, Data Vault Modelling\nExperience in implementing 2 or more data models in a database with data security and access controls. Good experience in OLTP and OLAP systems\nExcellent Data Analysis skills with demonstrable knowledge on standard datasets and sources. Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)\nExperience on one or more cloud platforms (e.g. AWS, Azure, GCP)\nUnderstanding of DevOps processes\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python\nExperience in leading the large and complex teams\nGood understanding of agile methodology\nExtensive expertise in leading data transformation initiatives, driving cultural change, and promoting a data driven mindset across the organization.\nExcellent Communication skills\nRequirements:\nUnderstand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.\nWork as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.\nLead cross-functional teams, define data strategies, and leverage the latest technologies in data handling.\nDefine and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.\nSuggest best modelling approach to the client based on their requirement and target architecture.\nAnalyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.\nProfile the Data sets to generate relevant insights.\nOptimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.\nEstablish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.\nDrives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.\nClosely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.\nGuide /mentor team members, and review artifacts.\nContribute to the overall data strategy and roadmaps.\nPropose and execute technical assessments, proofs of concept to promote innovation in the data space.","Solution Architecting, data vault, Data Modeling, Data Architecture, Dimensional Modeling, Data Warehousing, Oltp"
Principal Data Architect - Data Modelling,Tiger Analytics,16-22 Years,,Hyderabad,Consulting,"Role & responsibilities:\nMinimum 15 years of experience\nDeep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.\nExperience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.\nDecent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server)\nGood understanding of Relational, Dimensional, Data Vault Modelling\nExperience in implementing 2 or more data models in a database with data security and access controls. Good experience in OLTP and OLAP systems\nExcellent Data Analysis skills with demonstrable knowledge on standard datasets and sources. Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)\nExperience on one or more cloud platforms (e.g. AWS, Azure, GCP)\nUnderstanding of DevOps processes\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python\nExperience in leading the large and complex teams\nGood understanding of agile methodology\nExtensive expertise in leading data transformation initiatives, driving cultural change, and promoting a data driven mindset across the organization.\nExcellent Communication skills\nRequirements:\nUnderstand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.\nWork as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.\nLead cross-functional teams, define data strategies, and leverage the latest technologies in data handling.\nDefine and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.\nSuggest best modelling approach to the client based on their requirement and target architecture.\nAnalyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.\nProfile the Data sets to generate relevant insights.\nOptimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.\nEstablish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.\nDrives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.\nClosely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.\nGuide /mentor team members, and review artifacts.\nContribute to the overall data strategy and roadmaps.\nPropose and execute technical assessments, proofs of concept to promote innovation in the data space.","Solution Architecting, data vault, Data Modeling, Data Architecture, Dimensional Modeling, Data Warehousing, Oltp"
Sr Data Architect,Houghton Mifflin Harcourt,5-7 Years,,Pune,E-Learning,"Responsibilities\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps\nDesigns the structure and layout of data systems, including databases, warehouses, and lakes\nSelects and designs database management systems that meet the organizations needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures\nDefines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms\nDesigns processes for the ETL process from various sources into the organizations data systems\nTranslates high-level business requirements into data models and appropriate metadata, test data, and data quality standards\nManages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps\nSimplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company\nLeads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums\nDefines and manages standards, guidelines, and processes to ensure data quality\nWorks with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions\nEvaluates and recommends emerging technologies for data management, storage, and analytics\nDesign, create, and implement logical and physical data models for both IT and business solutions to capture the structure, relationships, and constraints of relevant datasets\nBuild and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions\nEffectively collaborate and communicate with various stakeholders to understand data and business requirements and translate them into data models\nCreate entity-relationship diagrams (ERDs), data flow diagrams, and other visualization tools to represent data models\nCollaborate with database administrators and software engineers to implement and maintain data models in databases, data warehouses, and data lakes\nDevelop data modeling best practices, and use these standards to identify and resolve data modeling issues and conflicts\nConduct performance tuning and optimization of data models for efficient data access and retrieval\nIncorporate core data management competencies, including data governance, data security and data quality\nJob Requirements\nEducation:\nA bachelors degree in computer science, data science, engineering, or related field\nExperience:\nAt least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives\nExperience leading projects involving data warehousing, data modeling, and data analysis\nDesign experience in Azure Databricks, PySpark, and Power BI/Tableau\nSkills:\nAbility in programming languages such as Java, Python, and C/C++\nAbility in data science languages/tools such as SQL, R, SAS, or Excel\nProficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)\nExperience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata\nUnderstanding of entity-relationship modeling, metadata systems, and data quality tools and techniques\nAbility to think strategically and relate architectural decisions and recommendations to business needs and client culture\nAbility to assess traditional and modern data architecture components based on business needs\nExperience with business intelligence tools and technologies such as ETL, Power BI, and Tableau\nAbility to regularly learn and adopt new technology, especially in the ML/AI realm\nStrong analytical and problem-solving skills\nAbility to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings\nAbility to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders\nAbility to guide solution design and architecture to meet business needs\nExpert knowledge of data modeling concepts, methodologies, and best practices\nProficiency in data modeling tools such as Erwin or ER/Studio\nKnowledge of relational databases and database design principles\nFamiliarity with dimensional modeling and data warehousing concepts\nStrong SQL skills for data querying, manipulation, and optimization, and knowledge of other data science languages, including JavaScript, Python, and R\nAbility to collaborate with cross-functional teams and stakeholders to gather requirements and align on data models\nExcellent analytical and problem-solving skills to identify and resolve data modeling issues\nStrong communication and documentation skills to effectively convey complex data modeling concepts to technical and business stakeholders","R, Business Analytics, Java, Data Modeling, Cloud Services, C C++, Data Architecture, Sql, Python, Big Data Technologies"
Data Architect-Data Modelling,Tiger Analytics,10-15 Years,,Bengaluru,Consulting,"Role & responsibilities\nAs an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.\nOn a typical day, you might\nEngage the clients & understand the business requirements to translate those into data models.\nAnalyze customer problems, propose solutions from a data structural perspective, and estimate and deliver proposed solutions.\nCreate and maintain a Logical Data Model (LDM) and Physical Data Model (PDM) by applying best practices to provide business insights.\nUse the Data Modelling tool to create appropriate data models\nCreate and maintain the Source to Target Data Mapping document that includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.\nGather and publish Data Dictionaries.\nIdeate, design, and guide the teams in building automations and accelerators\nInvolve in maintaining data models as well as capturing data models from existing databases and recording descriptive information.\nContribute to building data warehouse & data marts (on Cloud) while performing data profiling and quality analysis.\nUse version control to maintain versions of data models.\nCollaborate with Data Engineers to design and develop data extraction and integration code modules.\nPartner with the data engineers & testing practitioners to strategize ingestion logic, consumption patterns & testing.\nIdeate to design & develop the next-gen data platform by collaborating with cross-functional stakeholders.\nWork with the client to define, establish and implement the right modelling approach as per the requirement\nHelp define the standards and best practices\nInvolve in monitoring the project progress to keep the leadership teams informed on the milestones, impediments, etc.\nCoach team members, and review code artifacts.\nContribute to proposals and RFPs\nPreferred candidate profile\n10+ years of experience in Data space.\nDecent SQL knowledge\nAble to suggest modeling approaches for a given problem.\nSignificant experience in one or more RDBMS (Oracle, DB2, and SQL Server)\nReal-time experience working in OLAP & OLTP database models (Dimensional models).\nComprehensive understanding of Star schema, Snowflake schema, and Data Vault Modelling. Also, on any ETL tool, Data Governance, and Data quality.\nEye to analyze data & comfortable with following agile methodology.\nAdept understanding of any of the cloud services is preferred (Azure, AWS & GCP)\nEnthuse to coach team members & collaborate with various stakeholders across the organization and take complete ownership of deliverables.\nExperience in contributing to proposals and RFPs\nGood experience in stakeholder management\nDecent communication and experience in leading the team","Cloud Platforms, Data Modeling, Data Architecture, OLAP, Data Warehousing, Sql, Oltp"
Data Architect-Data Modelling,Tiger Analytics,10-15 Years,,Chennai,Consulting,"Role & responsibilities\nAs an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.\nOn a typical day, you might\nEngage the clients & understand the business requirements to translate those into data models.\nAnalyze customer problems, propose solutions from a data structural perspective, and estimate and deliver proposed solutions.\nCreate and maintain a Logical Data Model (LDM) and Physical Data Model (PDM) by applying best practices to provide business insights.\nUse the Data Modelling tool to create appropriate data models\nCreate and maintain the Source to Target Data Mapping document that includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.\nGather and publish Data Dictionaries.\nIdeate, design, and guide the teams in building automations and accelerators\nInvolve in maintaining data models as well as capturing data models from existing databases and recording descriptive information.\nContribute to building data warehouse & data marts (on Cloud) while performing data profiling and quality analysis.\nUse version control to maintain versions of data models.\nCollaborate with Data Engineers to design and develop data extraction and integration code modules.\nPartner with the data engineers & testing practitioners to strategize ingestion logic, consumption patterns & testing.\nIdeate to design & develop the next-gen data platform by collaborating with cross-functional stakeholders.\nWork with the client to define, establish and implement the right modelling approach as per the requirement\nHelp define the standards and best practices\nInvolve in monitoring the project progress to keep the leadership teams informed on the milestones, impediments, etc.\nCoach team members, and review code artifacts.\nContribute to proposals and RFPs\nPreferred candidate profile\n10+ years of experience in Data space.\nDecent SQL knowledge\nAble to suggest modeling approaches for a given problem.\nSignificant experience in one or more RDBMS (Oracle, DB2, and SQL Server)\nReal-time experience working in OLAP & OLTP database models (Dimensional models).\nComprehensive understanding of Star schema, Snowflake schema, and Data Vault Modelling. Also, on any ETL tool, Data Governance, and Data quality.\nEye to analyze data & comfortable with following agile methodology.\nAdept understanding of any of the cloud services is preferred (Azure, AWS & GCP)\nEnthuse to coach team members & collaborate with various stakeholders across the organization and take complete ownership of deliverables.\nExperience in contributing to proposals and RFPs\nGood experience in stakeholder management\nDecent communication and experience in leading the team","Cloud Platforms, Data Modeling, Data Architecture, OLAP, Data Warehousing, Sql, Oltp"
Principal Data Architect- Data Modelling,Tiger Analytics,16-21 Years,,Pune,Consulting,"Role & responsibilities\nUnderstand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.\nWork as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.\nLead cross-functional teams, define data strategies, andleveragethe latest technologies in data handling.\nDefine and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.\nSuggest best modelling approach to the client based on their requirement and target architecture.\nAnalyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.\nProfile the Data sets to generate relevant insights.\nOptimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.\nEstablish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.\nDrives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.\nClosely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.\nGuide /mentor team members, and review artifacts.\nContribute to the overall data strategy and roadmaps.\nPropose and execute technical assessments, proofs of concept to promote innovation in the data space.\nPreferred candidate profile\nMinimum 16 years of experience\nDeep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.\nExperience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.\nDecent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server) Good understanding of Relational, Dimensional, Data Vault Modelling\nExperience in implementing 2 or more data models in a database with data security and access controls.\nGood experience in OLTP and OLAP systems\nExcellent Data Analysis skills with demonstrable knowledge on standard datasets and sources.\nGood Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)\nExperience on one or more cloud platforms (e.g. AWS, Azure, GCP)\nUnderstanding of DevOps processes\nHands-on experience in one or more Data Modelling Tools\nGood understanding of one or more ETL tool and data ingestion frameworks\nUnderstanding of Data Quality and Data Governance\nGood understanding of NoSQL Database and modeling techniques\nGood understanding of one or more Business Domains\nUnderstanding of Big Data ecosystem\nUnderstanding of Industry Data Models\nHands-on experience in Python\nExperience in leading the large and complex teams\nGood understanding of agile methodology","Medallion Architecture, Solution Architecting, Data Modeling, Data Architecture, Data Warehousing, Rfp, Sql"
Senior Data Architect,Swiss Re,5-7 Years,,"Bengaluru, India",Consulting/Advisory Services,"Our success depends on our ability to build an inclusive culture encouraging fresh perspectives and innovative thinking. We embrace a workplace where everyone has equal opportunities to thrive and develop professionally regardless of their gender, race, ethnicity, gender identity and/or expression, sexual orientation, physical or mental ability, skillset, thought or other characteristics. In our inclusive and flexible environment everyone can bring their authentic selves to work.\nAbout D&A Re\nThe Digital & Tech Re organization is at the forefront of driving the digital transformation for our reinsurance business units across P&C Re, L&H Re and Solutions. We strive to build an inspiring environment for our people to use data and technology to create a sustainable and strategic competitive advantage.\nData & Analytics Re is the data arm of Digital & Tech Re. We create innovative analytics, data science and robust data foundation capabilities to generate data-driven insights that serve the heart of Swiss Re's business. Together with our business counterparts in Property & Casualty and Life & Health, we work daily to deliver differentiating insights, elevate underwriting excellence and effectively select and manage risk pools.\nOur team is composed of an international workforce based in different locations and serving a global customer basis, with a large part of our leadership located in Zurich.\n\nAbout the Role\nAre you an energetic Data Architect who enjoys working with teams to design and deliver exceptional data driven solutions\nAs a Data Architect you will be responsible for supporting the companies ambition to become a data driven commercial insurer. Working with both internal and external customers, you will need to have strong analytical and conceptual thinking, with the ability to recognize competing requirements and constraints, to balance and negotiate trade-offs.\nThe role is a mix of strong solution architecture awareness combined with a strong background in data modelling and canonical modelling. The role is very much focused on constantly improving the data landscape aimed at democratizing our data assets.\nYou thrive working in an agile, collaborative environment providing knowledge and guidance on the data architecture.\nYour responsibilities include:\n.High-level architecture and designs for database and data integrations leveraging Conceptual, logical and physical data models.\n.End to end data analysis, from business analysis and data modelling through to data quality assurance, on a major global program in the Commercial Insurance sector in a fast-moving Agile environment.\n.Complex SQL based data analysis\n.Maintenance of the logical and physical data models within a CASE tool (SAP Sybase Power Designer), supporting both SDLC and Agile application development approaches.\n.Collaborate with your customers, understanding their data needs, finding innovative ways to bring data into their daily process.\n.Work with other members of the data team, including data architects, data analysts, data engineers, and data scientists.\n.Create full transparency of data for customers to know what is available and how to use it\nAbout you\nAre you eager to disrupt the insurance industry with us and make an impact Do you wish to have your talent recognized and rewarded Then join our growing team and become part of the next wave of insurance innovation.\nYour experience\n.Proven track record of delivering results and impact\n.Minimum of 5-7 years as a Data Architect\n.Strong business acumen and a deep strategic mindset\n.Requirements gathering, analysis and prioritization for business processes and data flows, persistence and integration.\n.Conceptual and logical data and physical modeling (Sybase Power Designer)\n.Data retrieval and manipulation using SQL\n.Data architecture: design for integration of complex data flows from multiple applications\n.Data warehousing design, both dimensional and 3NF\n.SDLC and Agile/SCRUM software development processes\n.Experience working with and understanding the needs of customers or clients (internal or external)\n.Ability to understand and internalize an organization's strategy and culture, and contribute to their implementation and reinforcement through technology architecture and data designs\n.A desire and openness to learning and continuous improvement, both of yourself and your team members\n.Proven analytical skills and experience making decisions based on hard and soft data\n.Commitment to the organization's new way of working through greater collaboration and breaking down of siloes\n.English fluency is a requirement, demonstrating excellent verbal and written communication skills\n.Excellent in team and organizational development by providing the highest quality of services\n.Experience in information & data governance would be a plus\n.Self-motivated and good communication skills\n.Dedication and flexibility aptitude for fast learning\n.Ideally experience in insurance/reinsurance or financial industry\nBehavioural Competences\n.Excellent organizational skills.\n.Excellent communication and presentation skills ability to communicate on different levels of seniority.\n.Team player enjoying being part of a cross-functional setup.\n.Ability to perform well on time-critical endeavours and on multiple fronts at the time.\n.Strong dedication to quality and client mindset.\n.A passion for learning and continuous improvement, both of yourself and your team members.\nWe are an equal opportunity employer, and we value diversity at our company. Our aim is to live visible and invisible diversity -diversity of age, race, ethnicity, nationality, gender, gender identity, sexual orientation, religious beliefs, physical abilities, personalities and experiences - at all levels and in all functions and regions. We also collaborate in a flexible working environment, providing you with a compelling degree of autonomy to decide how, when and where to carry out your tasks.\nWe provide feedback to all candidates via email. If you have not heard back from us, please check your spam folder.\nAbout Swiss Re\nSwiss Re is one of the world's leading providers of reinsurance, insurance and other forms of insurance-based risk transfer, working to make the world more resilient. We anticipate and manage a wide variety of risks, from natural catastrophes and climate change to cybercrime. We cover both Property & Casualty and Life & Health. Combining experience with creative thinking and cutting-edge expertise, we create new opportunities and solutions for our clients. This is possible thanks to the collaboration of more than 14,000 employees across the world.\n\nOur success depends on our ability to build an inclusive culture encouraging fresh perspectives and innovative thinking. We embrace a workplace where everyone has equal opportunities to thrive and develop professionally regardless of their age, gender, race, ethnicity, gender identity and/or expression, sexual orientation, physical or mental ability, skillset, thought or other characteristics. In our inclusive and flexible environment everyone can bring their authentic selves to work and their passion for sustainability.\n\nIf you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.\nKeywords:\nReference Code:133976","Canonical Modelling, SAP Sybase Power Designer, Data Modelling, Agile Scrum, Data Architecture, Data Warehousing, Sql, Sdlc"
Data Architect- Snowflake,3Pillar Global,10-12 Years,,"Indi, India",Login to check your skill match score,"Embark on an exciting journey into the realm of data analytics with 3Pillar! We extend an invitation for you to join our team and gear up for a thrilling adventure. As a Snowflake Data Architect you will be at the heart of the organization and support our clients to take control of their data and get value out of it by defining a reference architecture for our customers. This means that you will work closely with business leaders and information management teams to define and implement a roadmap on data management, business intelligence or analytics solutions..\nIf your passion for data analytics solutions that make a real-world impact, consider this your pass to the captivating world of Data Science and Engineering!\n\nRelevant Experience: 10+ years in Data Practice\nMust have Skills: Snowflake, Data Architecture, Engineering, Governance, and Cloud services\n\nResponsibilities\nAssessments of existing data components, Performing POCs, Consulting to the stakeholders\nLead the migration to Snowflake\nIn a strong client-facing role, proposing, advocating, leading implementation of end to end solutions to an enterprise's data specific business problems, and taking care of data collection, extraction, integration, cleansing, enriching and data visualization.\nAbility to design large data platforms to enable Data Engineers, Analysts & scientists\nStrong exposure to different Data architectures, data lake & data warehouse, including migrations, rearchitect and platform modernization\nDefine tools & technologies to develop automated data pipelines, write ETL processes, develop dashboard & report and create insights\nContinually reassess current state for alignment with architecture goals, best practices and business needs\nDB modeling, deciding best data storage, creating data flow diagrams, maintaining related documentation\nTaking care of performance, reliability, reusability, resilience, scalability, security, privacy & data governance while designing a data architecture\nApply or recommend best practices in architecture, coding, API integration, CI/CD pipelines\nCoordinate with data scientists, analysts, and other stakeholders for data-related needs\nHelp the Data Science & Analytics Practice grow by mentoring junior Practice members, leading initiatives, leading Data Practice Offerings\nProvide thought leadership by representing the Practice / Organization on internal / external platforms\n\nQualification:\nTranslate business requirements into data requests, reports and dashboards.\nStrong Database & modeling concepts with exposure to SQL & NoSQL Databases\nStrong data architecture patterns & principles, ability to design secure & scalable data lakes, data warehouse, data hubs, and other event-driven architectures\nExpertise in designing and writing ETL processes.\nStrong experience to Snowflake, and its components.\nKnowledge of Master Data management and related tools\nStrong exposure to data security and privacy regulations (GDPR, HIPAA) and best practices\nSkilled in ensuring data accuracy, consistency, and quality\nExperience of AWS services viz., AWS S3, Redshift, Lambda, DynamoDB, EMR, Glue, Lake formation, Athena, Quicksight, RDS, Kinesis, Managed Kafka, API Gateway, CloudWatch\nAWS S3, Redshift, Lambda, DynamoDB, EMR, Glue, Lake formation, Athena, Quicksight, RDS, Kinesis, Managed Kafka, Elasticsearch and Elastic Cache, API Gateway, CloudWatch\nAbility to implement data validation processes and establish data quality standards.\nExperience in Linux, and scripting\nProficiency in data visualization tools like Tableau, Power BI or similar to create meaningful insights\n\nAdditional Experience Desired:\nExperience working with data ingestion tools such as Fivetran, stitch, or Matillion\nAWS IOT solutions\nApache NiFi, Talend, Informatica\nKnowledge of GCP Data services\nExposure to AI / ML technologies","Engineering Governance, Managed Kafka, NoSQL Databases, GCP Data services, Glue, AI ML technologies, Athena, Lake formation, Snowflake Data Architecture, ETL processes, Tableau, Sql, Emr, Dynamodb, Lambda, Api Gateway, Quicksight, RDS, Power Bi, Cloud Services, Informatica, Apache Nifi, Aws S3, Kinesis, Talend, Cloudwatch, Redshift"
Data Architect,NTT Data,8-10 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324664\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Architect to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\nKey Responsibilities:\nDevelop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.\nUtilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.\nConceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.\nInstitute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.\nImplement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.\nEvaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.\nDevelop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.\nIdentify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.\nFormulate and maintain data models and establish policies and procedures for functional design.\nOffer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.\nStay informed about upgrades and emerging database technologies through continuous research.\nCollaborate with project managers and business leaders on all projects involving enterprise data.\nDocument the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.\nDesign and implement data solutions tailored to meet customer needs and specific use cases.\nProvide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.\nBasic Qualifications:\n8+ years of hands-on experience with various database technologies\n6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.\nExperience with Azure, Databricks, Snowflake\nKnowledgeable on concepts of GenAI\nAbility to travel at least 25%.\nPreferred Skills:\nPossess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.\nDemonstrated expertise with certifications in Snowflake.\nValuable Big 4 Management Consulting experience or exposure to multiple industries.\nUndergraduate or graduate degree preferred.\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","GenAI, snowflake, Databricks, AWS, Azure, Gcp"
Data Architect,Axtria,10-17 Years,,"Gurugram, Bengaluru, Delhi NCR",Data Integration,"To leverage expertise in data architecture and management to design, implement, and optimize a robust data warehousing platform for the pharmaceutical industry.\nThe goal is to ensure seamless integration of diverse data sources, maintain high standards of data quality and governance, and enable advanced analytics through the definition and management of semantic and common data layers.\nUtilizing Axtria DataMAx and generative AI technologies, the aim is to accelerate business insights and support regulatory compliance, ultimately enhancing decision-making and operational efficiency.\nKey Responsibilities\nData Modeling: Design logical and physical data models to ensure efficient data storage and retrieval.\nETL Processes: Develop and optimize ETL processes to accurately and efficiently move data from various sources into the data warehouse.\nInfrastructure Design: Plan and implement the technical infrastructure, including hardware, software, and network components.\nData Governance: Ensure compliance with regulatory standards and implement data governance policies to maintain data quality and security.\nPerformance Optimization: Continuously monitor and improve the performance of the data warehouse to handle large volumes of data and complex queries.\nSemantic Layer Definition: Define and manage the semantic layer architecture and technology stack to manage the lifecycle of semantic constructs including consumption into downstream systems.\nCommon Data Layer Management: Integrate data from multiple sources into a centralized repository, ensuring consistency and accessibility.\nDeep expertise in architecting enterprise-grade software systems that are performant, scalable, resilient, and manageable. Architecting GenAI based systems is an added plus.\nAdvanced Analytics: Enable advanced analytics and machine learning to identify patterns in genomic data, optimize clinical trials, and personalize medication.\nGenerative AI: Should have worked with production-ready use case for GenAI based data.\nStakeholder Engagement: Work closely with business stakeholders to understand their data needs and translate them into technical solutions.\nCross-Functional Collaboration: Collaborate with IT, data scientists, and business analysts to ensure the data warehouse supports various analytical and operational needs.\nQualifications\nProven experience in data architecture and data warehousing, preferably in the pharmaceutical industry.\nStrong knowledge of data modeling, ETL processes, and infrastructure design.\nExperience with data governance and regulatory compliance in the life sciences sector.\nProficiency in using Axtria DataMAx or similar data management products.\nExcellent analytical and problem-solving skills.\nStrong communication and collaboration skills.\nPreferred Skills\nFamiliarity with advanced analytics and machine learning techniques.\nExperience in managing semantic and common data layers.\nKnowledge of FDA guidelines, HIPAA regulations, and other relevant regulatory standards.\nExperience with generative AI technologies and their application in data warehousing.","Gen AI, Infrastructure Design, Data Architecture, Data Warehousing, Data Modelling, Data Governance, Etl"
Data Architect,Kezan Consulting,12-16 Years,,Bengaluru,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,"Delhi, Delhi NCR",Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,Bengaluru,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,Pune,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-22 Years,,Bengaluru,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-22 Years,,Pune,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,Noida,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
AWS -Senior Data Architect - R01543971,Brillio,Fresher,,"Gurugram, India",Login to check your skill match score,"Senior Data Specialist\n\nPrimary Skills\n\nAthena, SQS, CloudWatch, Macie, Spark - Scala, Kinesis, CloudFormation, EMR, Open Search, DynamoDB, Amazon API Gateway, SCT, Redshift, DMS, Oozie\n\nSecondary Skills\n\nPython\n\nJob requirements\n\nAbout Brillio:\n\nBrillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.\n\nBrillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.\n\nWork Location: Gurgaon/Hyderabad/Pune/Bangalore/Chennai\nJob Title: AWS Data Architect/Senior Lead Data Engineer\nJob Type: [Full-time\nJob Summary:\nWe are looking for an experienced and dynamic AWS Data Architect to lead the design and implementation of data solutions in the cloud. This role will focus on leveraging AWS technologies to create scalable, reliable, and optimized data architectures that drive business insights and data-driven decision-making. As an AWS Data Architect, you will play a pivotal role in shaping the data strategy, implementing best practices, and ensuring the seamless integration of AWS-based data platforms, with a focus on services like Amazon Redshift, Aurora, and other AWS data services.\nKey Responsibilities:\nData Architecture Design:\nLead the design and implementation of cloud-native data architectures on AWS, using services like Amazon Redshift, Amazon Aurora, Amazon S3, AWS Glue, and Amazon Athena.\nDefine data management strategies and ensure data models, structures, and systems are aligned with business goals.\nArchitect end-to-end data pipelines for ingesting, processing, storing, and analyzing large datasets using AWS services.\nCloud Data Infrastructure Management:\nDesign and implement highly available, scalable, and secure data infrastructure on AWS.\nLead the migration of on-premise data solutions to AWS cloud platforms, ensuring smooth transitions and minimizing operational impact.\nOversee performance tuning and optimization of data storage and query processing for Amazon Redshift and Aurora environments.\nData Integration and Processing:\nArchitect data integration solutions for combining data from various on-premise and cloud-based systems.\nDesign and implement ETL/ELT workflows utilizing AWS Glue, Lambda, and other relevant AWS tools to transform raw data into consumable formats.\nWork with stakeholders to identify data sources and design appropriate interfaces and APIs for seamless integration into the cloud architecture.\nGovernance and Security:\nEnsure that data architecture adheres to best practices for data security, governance, and compliance (e.g., GDPR, HIPAA).\nDefine and implement data privacy and access control policies using AWS Identity and Access Management (IAM), encryption, and audit logging.\nEstablish data lineage and metadata management strategies to ensure transparency and traceability in data workflows.\nCollaboration and Leadership:\nAct as a technical leader and provide guidance on cloud-based data solutions to internal teams, including Data Engineers, Data Scientists, and DevOps engineers.\nCollaborate with business stakeholders and leadership to understand requirements and design data solutions that meet business needs.\nLead the development of best practices and standards for AWS-based data engineering solutions and ensure these are followed across the organization.\nInnovation and Optimization:\nContinuously evaluate and introduce new AWS data technologies, tools, and features to optimize performance, scalability, and cost-efficiency.\nStay up-to-date with the latest trends in cloud data architecture and AWS service updates to recommend improvements and innovations in existing solutions.\nDocumentation and Knowledge Sharing:\nCreate and maintain clear and detailed documentation for data architecture, design decisions, and system configurations.\nPromote a culture of knowledge sharing by mentoring team members and providing ongoing training on AWS data technologies.","Amazon Athena, Amazon Aurora, Amazon Redshift, Amazon S3, AWS Glue, ELT, Etl"
Data Architect Sr. Advisor,NTT Data,12-15 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:323777\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Architect Sr. Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nJob Duties: The Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.\nThis role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client's technology infrastructure.\n. Key Responsibilities:\n. Ability and experience to have conversations with the CEO, Business owners and CTO/CDO\n. Break down intricate business challenges, devise effective solutions, and focus on client needs.\n. Craft high level innovative solution approach for complex business problems\n. Utilize best practices and creativity to address challenges\n. Leverage market research, formulate perspectives, and communicate insights to clients\n. Establish strong client relationships\n. Interact at appropriate levels to ensure client satisfaction\n. Knowledge and Attributes:\n. Ability to focus on detail with an understanding of how it impacts the business strategically.\n. Excellent client service orientation.\n. Ability to work in high-pressure situations.\n. Ability to establish and manage processes and practices through collaboration and the understanding of business.\n. Ability to create new and repeat business for the organization.\n. Ability to contribute information on relevant vertical markets\n. Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.\nMinimum Skills Required: Academic Qualifications and Certifications:\n. BE/BTech or equivalent in Information Technology and/or Business Management or a related field.\n. Scaled Agile certification desirable.\n. Relevant consulting and technical certifications preferred, for example TOGAF.\nRequired Experience: 12-15 years\n. Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.\n. Very good understanding of Data, AI, Gen AI and Agentic AI\n. Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.\n. Must be able to work on Data & AI RFP responses as Solution Architect\n. 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect\n. Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.\n. Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools\n. Experience with large scale consulting and program execution engagements in AI and data\n. Seasoned multi-technology infrastructure design experience.\n. Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.\n. Additional Job Description\nAdditional Job Description\nAdditional Career Level Description:\nKnowledge and application:\n. Seasoned, experienced professional has complete knowledge and understanding of area of specialization.\n. Uses evaluation, judgment, and interpretation to select right course of action.\nProblem solving:\n. Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\n. Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.\nInteraction:\n. Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\n. Works\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","snowflake, Gen AI, Ai, Cloud-native technical approach, AI tools, Databricks, Ml, Data Architecture, data engineering, Solution Architecting, Data Analytics, AWS, Azure, Gcp"
AWS -Senior Data Architect - R01543971,Brillio,Fresher,,"Gurugram, India",Login to check your skill match score,"Senior Data Specialist\n\nPrimary Skills\n\nAthena, SQS, CloudWatch, Macie, Spark - Scala, Kinesis, CloudFormation, EMR, Open Search, DynamoDB, Amazon API Gateway, SCT, Redshift, DMS, Oozie\n\nSecondary Skills\n\nPython\n\nJob requirements\n\nAbout Brillio:\n\nBrillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.\n\nBrillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.\n\nWork Location: Gurgaon/Hyderabad/Pune/Bangalore/Chennai\nJob Title: AWS Data Architect/Senior Lead Data Engineer\nJob Type: [Full-time\nJob Summary:\nWe are looking for an experienced and dynamic AWS Data Architect to lead the design and implementation of data solutions in the cloud. This role will focus on leveraging AWS technologies to create scalable, reliable, and optimized data architectures that drive business insights and data-driven decision-making. As an AWS Data Architect, you will play a pivotal role in shaping the data strategy, implementing best practices, and ensuring the seamless integration of AWS-based data platforms, with a focus on services like Amazon Redshift, Aurora, and other AWS data services.\nKey Responsibilities:\nData Architecture Design:\nLead the design and implementation of cloud-native data architectures on AWS, using services like Amazon Redshift, Amazon Aurora, Amazon S3, AWS Glue, and Amazon Athena.\nDefine data management strategies and ensure data models, structures, and systems are aligned with business goals.\nArchitect end-to-end data pipelines for ingesting, processing, storing, and analyzing large datasets using AWS services.\nCloud Data Infrastructure Management:\nDesign and implement highly available, scalable, and secure data infrastructure on AWS.\nLead the migration of on-premise data solutions to AWS cloud platforms, ensuring smooth transitions and minimizing operational impact.\nOversee performance tuning and optimization of data storage and query processing for Amazon Redshift and Aurora environments.\nData Integration and Processing:\nArchitect data integration solutions for combining data from various on-premise and cloud-based systems.\nDesign and implement ETL/ELT workflows utilizing AWS Glue, Lambda, and other relevant AWS tools to transform raw data into consumable formats.\nWork with stakeholders to identify data sources and design appropriate interfaces and APIs for seamless integration into the cloud architecture.\nGovernance and Security:\nEnsure that data architecture adheres to best practices for data security, governance, and compliance (e.g., GDPR, HIPAA).\nDefine and implement data privacy and access control policies using AWS Identity and Access Management (IAM), encryption, and audit logging.\nEstablish data lineage and metadata management strategies to ensure transparency and traceability in data workflows.\nCollaboration and Leadership:\nAct as a technical leader and provide guidance on cloud-based data solutions to internal teams, including Data Engineers, Data Scientists, and DevOps engineers.\nCollaborate with business stakeholders and leadership to understand requirements and design data solutions that meet business needs.\nLead the development of best practices and standards for AWS-based data engineering solutions and ensure these are followed across the organization.\nInnovation and Optimization:\nContinuously evaluate and introduce new AWS data technologies, tools, and features to optimize performance, scalability, and cost-efficiency.\nStay up-to-date with the latest trends in cloud data architecture and AWS service updates to recommend improvements and innovations in existing solutions.\nDocumentation and Knowledge Sharing:\nCreate and maintain clear and detailed documentation for data architecture, design decisions, and system configurations.\nPromote a culture of knowledge sharing by mentoring team members and providing ongoing training on AWS data technologies.","Amazon Athena, Amazon Aurora, Amazon Redshift, Amazon S3, AWS Glue, ELT, Etl"
AWS -Data Architect - R01545805,Brillio,10-12 Years,,"Gurugram, India",Login to check your skill match score,"Data Architect\n\nPrimary Skills\n\nAWS, Python, Advanced SQL\nDeep expertise in S3, Redshift, Aurora, Glue and Lambda services.\n\nJob requirements\n\nAbout Brillio:\nBrillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.\nBrillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.\n\nJob Description:\n\n10 years of IT experience with deep expertise in S3, Redshift, Aurora, Glue and Lambda services.\nAt least one instance of proven experience in developing Data platform end to end using AWS\nHands-on programming experience with Data Frames, Python, and unit testing the python as well as Glue code.\nExperience in orchestrating mechanisms like Airflow, Step functions etc.\nExperience working on AWS redshift is Mandatory. Must have experience writing stored procedures, understanding of Redshift data API and writing federated queries\nExperience in Redshift performance tuning. Good in communication and problem solving.\nVery good stakeholder communication and management.","Step Functions, Aurora, Airflow, Glue, S3, Lambda, Advanced Sql, Redshift, Python, AWS"
Data Architect,Kezan Consulting,12-16 Years,,Pune,Information Technology,"Key Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.\nExpertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).\nStrong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kanini Software Solutions,10-15 Years,,"Bengaluru, Chennai",Software,"We are looking forsomeone who has a deep understanding of Big Data and Cloud architecture and isexcited to advance innovative analytics solutions. As a Data Architect, you effectively communicate ideas and concepts to peers and bring onboard the experience of leading projects that support the organization s businessobjectives and goals.\nYou areall set to:\nArchitect andimplement data ingestion, data validation, and data transformation pipelines. Collaboratewith the Product, Development, and Enterprise Data teams to design and maintainbatch and streaming integrations across a variety of data domains andplatforms.\nYou are someone who can:\nTake ownership in building solutions and proposingarchitectural designs related to building efficient and timely data ingestionand transformation.\nprocesses geared towards analytics workloads.\nManage code deployment to various environments.\nBe proficient at positively critiquing andsuggesting improvements via code reviews\nWork with stakeholders to define and develop dataingest, validation, and transform pipelines.\nTroubleshoot data pipelines and resolve issues inalignment with SDLC.\nAbility to diagnose and troubleshoot data issues,recognizing common data integration andtransformation patterns\nEstimate, track, and communicate status of assigneditems to a diverse group of stakeholders required\nYou bring in:\n12 years experience in the industry.\nExperience and knowledge of Big Data Architectures,on cloud and on premise\nProficiency in AWS Collection Services: Kinesis,Kafka, Database Migration Service\nProficiency in AWS main Storage Service: S3, EBS,EFS\nProficiency in AWS main Compute Service: EC2,Lambda, ECS, EKS\nProven experience in: Java, Working experiencewith: AWS Athena and Glue Pyspark, EMR,DynamoDB, Redshift, Kinesis, Lambda, Apache Spark, Databricks on AWS, Snowflakeon AWS\nProficient in AWS Redshift, S3, Glue, Athena, DynamoDB\nAWS Certification: AWS Certified SolutionsArchitect and/or AWS Certified Data Analytics\nWorking experience with Agile Methodology andKanban\nWe Prefer:\nStrong experiencein Azure is preferred with hands-on experience in 2 or more of these skills:Azure SQL DB, Azure SQL Managed Instance, Azure Data Lake Store, Azure CosmosDB, Azure Database for PostgreSQL, Azure Database for MySQL\nYour qualification is:\nB.E/B.Tech/M.C.A/MSc(preferably in Computer Science/IT)\nRole:Data warehouse Architect / Consultant\nIndustry Type:Software Product\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:DBA / Data warehousing","Agile Methodology, MySQL, Data Architect, Cosmos DB, Sql, Database, AWS, Sdlc"
Big Data Architect,World Of Opportunities For Women LLP,8-14 Years,,"Bengaluru, Chennai, Pune",Login to check your skill match score,"Big Data /Cloud Architect\nGood understanding of Data Mesh Architecture\nExperience with Data Vault modelling and data modeling in general\nExperience in data integration and solution designs as Data architect\nUnderstanding of multiple ETL, Reporting and CI/CD tools\nAs part of the Infosys delivery team, your primary role would be to provide best fit architectural solutions for one or more projects.\nYou would also provide technologyconsultation and assist in defining scope and sizing of work\nYou would implement solutions, create technology differentiation and leverage partner technologies.\nAdditionally, you would participate in competency development with the objective of ensuring the best-fit and high-quality technical solutions.\nYou would be a key contributor in creating thought leadership within the area of technology specialization and in compliance with guidelines, policies and norms of Infosys. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nKnowledge of architectural design patterns, performance tuning, database and functional designs","Cloud Platforms, ETL Processes, Machine Learning, Nosql, Hadoop, Data Modeling, Spark, Data Warehousing, Data Governance, Sql"
AEP - Data Architect,Codilar,10-12 Years,,"Noida, Bengaluru",E-Commerce Platforms,"10+ years of strong experience with data transformation & ETL on large datasets.\n5+ years of Data Modeling experience (Relational, Dimensional, Columnar, Big Data).\n5+ years of complex SQL or NoSQL experience.\n5+ years of experience with industry ETL tools (Informatica, Unifi).\nExperience as an enterprise technical or engineer consultant.\nData and Technology:\nExperience with designing customer-centric datasets (CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\nKnowledge of Adobe Experience Platform (AEP) is mandatory.\nAdvanced Data Warehouse concepts.\nExperience with Big Data technologies (Hadoop, Spark, Redshift, Snowflake, Hive, Pig, etc.).\nExperience with Reporting Technologies (Tableau, PowerBI).\nExperience & knowledge with Adobe Experience Cloud solutions.\nExperience & knowledge with Digital Analytics or Digital Marketing.\nSoftware Development and Scripting:\nExperience in professional software development.\nExperience in programming languages (Python, Java, or Bash scripting).\nBusiness and Communication:\nExperience with Business Requirements definition and management, structured analysis, process design, use case documentation.\nStrong verbal & written communication skills to interface with Sales teams and lead customers to successful outcomes.\nDemonstrated exceptional organizational skills and ability to multi-task simultaneous different customer projects.\nMust be self-managed, proactive, and customer-focused.","AEP, Data Modeling, Big Data, Sql, Python, Etl"
Azure Data Architect,iLink Digital,9-14 Years,,"Chennai, Bengaluru, Pune",Financial Services,"At least 12+ years of overall experience mainly in the field of DataWarehousing or DataAnalytics\nSolid experience with Datamodeling using various modeling methodologies/Techniques like dimensional modeling, datavault, 3rdNormalization ..etc\nExperience in designing common/reusable domain specific datasets for DataLakes\nExperience in PowerBI Tabular models for self-service\nExperience in working with Azure dataplatform tools like ADF, Synapse, databricks, Zen2 storage\nSolid understanding of various Datastructures /DataFormats and the best usage for those in Big Data/Hadoop/DataLake environment (Relational vs Parquet vs ORC ..etc)\nGood experience with the datain Supply Chain and Manufacturing domains.\nWorking experience with datafrom SAP ERP systems is preferable\nShould be an expert with SQL queries and dataexploration methods.\nOther technical skills include SparkSQL, Python, PySpark, Any Datamodeling tools like ERWin.","azure data platforms, Data Modeling, Power Bi, Data Warehousing, Tableau, Data Analytics"
Data Architect,Tmf Group,8-13 Years,,"Delhi, Kolkata, Mumbai",BPO,"Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.\nTechnical Expertise:\nStrong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.\nProficiency in SQL, Python, Spark, and PowerShell.\nExperience in data modeling, ETL/ELT, and data warehousing.\nUnderstanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).\nStrong problem-solving and analytical skills.\nExcellent communication and stakeholder management.\nExposure to hybrid cloud environments (Azure on-premises)\nKey Requirements\nKnowledge of data mesh and data fabric architectures.\nWhat s in it for you\nPathways for Career Development\nWork with colleagues and clients around the world on interesting and challenging work.\nWe provide internal career opportunities, so you can take your career further within TMF.\nContinuous development is supported through global learning opportunities from the TMF Business Academy.\nMaking an Impact\nYou ll be helping us to make the world a simpler place to do business for our clients.\nThrough our corporate social responsibility program, you ll also be making a difference in the communities where we work.\nA Supportive Environment\nStrong feedback culture to help build an engaging workplace.\nOur inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.","Powershell, Data Warehousing, Sql, Python, Azure Data, Data Architecture"
Data Architect,Tmf Group,8-13 Years,,Bengaluru,BPO,"Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.\nTechnical Expertise:\nStrong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.\nProficiency in SQL, Python, Spark, and PowerShell.\nExperience in data modeling, ETL/ELT, and data warehousing.\nUnderstanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).\nStrong problem-solving and analytical skills.\nExcellent communication and stakeholder management.\nExposure to hybrid cloud environments (Azure on-premises)\nKey Requirements\nKnowledge of data mesh and data fabric architectures.\nWhat s in it for you\nPathways for Career Development\nWork with colleagues and clients around the world on interesting and challenging work.\nWe provide internal career opportunities, so you can take your career further within TMF.\nContinuous development is supported through global learning opportunities from the TMF Business Academy.\nMaking an Impact\nYou ll be helping us to make the world a simpler place to do business for our clients.\nThrough our corporate social responsibility program, you ll also be making a difference in the communities where we work.\nA Supportive Environment\nStrong feedback culture to help build an engaging workplace.\nOur inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.","Powershell, Azure Data Bricks, Spark, Sql, Python, Azure Data Lake, Data Architecture"
Senior Data Architect-GCP,Reflections Info Systems,7-13 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Pune",IT Management,"Responsibilities include:\nIntegrate machine learning workflows with data pipelines and analytics tools.\nDefine data governance frameworks and manage data lineage.\nLead data modeling efforts to ensure consistency, accuracy, and performance across systems.\nOptimize cloud infrastructure for scalability, performance, and reliability.\nMentor junior team members and ensure adherence to architectural standards.\nCollaborate with DevOps teams to implement Infrastructure as Code (Terraform, Cloud Deployment Manager).\nEnsure high availability and disaster recovery solutions are built into data systems.\nConduct technical reviews, audits, and performance tuning for data solutions.\nDesign solutions for multi-region and multi-cloud data architecture.\nStay updated on emerging technologies and trends in data engineering and GCP.\nDrive innovation in data architecture, recommending new tools and services on GCP.\nPrimary Skills :\n7+ years of experience in data architecture, with at least 3 years in GCP environments.\nExpertise in BigQuery, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage, and related GCP services.\nStrong experience in data warehousing, data lakes, and real-time data pipelines.\nProficiency in SQL, Python, or other data processing languages.\nExperience with cloud security, data governance, and compliance frameworks.\nStrong problem-solving skills and ability to architect solutions for complex data environments.\nGoogle Cloud Certification (Professional Data Engineer, Professional Cloud Architect) preferred.\nLeadership experience and ability to mentor technical teams.\nExcellent communication and collaboration skills.","Analytics, Data Modeling, Sql, Python, Performance Tuning, Machine Learning, Data Architecture"
Data Architect,Reflections Info Systems,10-14 Years,,"Thiruvananthapuram / Trivandrum, Chennai, Bengaluru",IT Management,"Work as part of a team to Design and Architect Large Data Platforms, Analytics and AI solutions\nParticipate in the development of cloud data warehouses, data as a service, business intelligence solutions\nAbility to provide solutions that are forward-thinking in data integration\nProgramming experience in Scala or Python, SQL\nWorking experience in Apache Spark is highly preferred\nFamiliarity with some of these AWS and Azure Services like S3, ADLS Gen2, AWS, Redshift, AWS Glue, Azure Data Factory, Azure Synapse\nCertifications :\nAWS/ other Cloud Services Architect Certifications\nSnowPro Core Certification\nDatabricks Professional level Certification\nPrimary Skills :\nMust Have: Bachelors or Masters degree in Computer Science or a related field\nMust Have: 7+ years of hands-on experience Data Engineering, Building Data Pipelines, Warehouses, Architecting Data platforms, Data Models, Data Science, AI use cases etc.\nProven experience as a Data Architect or in a similar role.\nProficiency in developing and maintaining our data architecture, ensuring its scalability, security, and alignment with business objectives.\nGood experience in MDM/PIM Solution Implementation\nSoft Skills :\nExcellent communication and interpersonal skills, with the ability to articulate ideas and discuss technical concepts with both technical and non-technical team members\nClear and effective documentation, code comments, and the ability to write technical reports or emails are essential.\nCollaboration is often an integral part of software development, Data Projects. Being able to work well with others, share knowledge, and contribute positively to a team is crucial.\nStrong problem-solving and analytical skills, with the ability to make sound decisions under pressure.\nEfficiently managing ones time and meeting deadlines is critical in a fast-paced development environment.\nUnderstanding the needs and expectations of end-users or clients and developing solutions that meet or exceed those expectations. . This is to notify jobseekers that some fraudsters are promising jobs with Reflections Info Systems for a fee. Please note that no payment is ever sought for jobs in Reflections. We contact our candidates only through our official website or LinkedIn and all employment related mails are sent through the official HR email id. for any clarification/ alerts on this subject.","Gcp, Database Management, Data Architect, Data Modeling, Data Analytics, Sql, Python"
Associate Data Architect,Murugappa Group,5-10 Years,,Chennai,Financial Services,"Purpose\nThe candidate is responsible for designing, creating, deploying, and maintaining an organization's data architecture.\nTo ensure that the organization's data assets are managed effectively and efficiently, and that they are used to support the organization's goals and objectives.\nResponsible for ensuring that the organization's data is secure, and that appropriate data governance policies and procedures are in place to protect the organization's data assets.\nKey Responsibilities\nResponsibilities will include but will not be restricted to:\nResponsible for designing and implementing a data architecture that supports the organization's business goals and objectives.\nDeveloping data models, defining data standards and guidelines, and establishing processes for data integration, migration, and management.\nCreate and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organization's data assets.\nEnsure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis.\nOrganization's data is secure, and that appropriate data governance policies and procedures are in place to protect the organization's data assets.\nWork closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organization's data architecture is integrated and aligned with other IT systems and applications.\nStay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organization's data architecture.\nCommunicate with stakeholders across the organization to understand their data needs and ensure that the organization's data architecture is aligned with the organization's strategic goals and objectives.\nMandatory Skills\nAWS Cloud Services\nCompute EC2\nStorage - S3 mandatory Other storage components\nAWS Services Like IAM,S3.\nProgramming Python, Pyspark, Lamda.\nKnowledge on ETL Glue (Mandatory), DMS.\nDatabases Data Bricks , RDBMS skills added advantages like Oracle, SQLServer, MPP databases like Snowflake, Redshift .\nKnowledge on Data modelling and ETL process is mandatory.\nArchitecture Data Mesh, Medallion and EDW - Data Modelling.\nTechnical requirements\nBachelor's or master's degree in Computer Science or a related field.\nCertificates in Database Management will be preferred.\nExpertise in data modeling and design, including conceptual, logical, and physical data models, and must be able to translate business requirements into data models.\nProficient in a variety of data management technologies, including relational databases, NoSQL databases, data warehouses, and data lakes.\nExpertise in ETL processes, including data extraction, transformation, and loading, and must be able to design and implement data integration processes.\nExperience with data analysis and reporting tools and techniques and must be able to design and implement data analysis and reporting processes.\nFamiliar with industry-standard data architecture frameworks, such as TOGAF and Zachman, and must be able to apply them to the organization's data architecture.\nFamiliar with cloud computing technologies, including public and private clouds, and must be able to design and implement data architectures that leverage cloud computing.\nQualitative Requirements\nAble to effectively communicate complex technical concepts to both technical and non-technical stakeholders.\nStrong analytical and problem-solving skills.\nMust be able to inspire and motivate their team to achieve organizational goal.","aws cloud services, Cloud Computing, Databases, Data Modelling, programming, Etl"
Data Architect,Tmf Group,8-12 Years,,"Hyderabad, Chennai, Pune",BPO,"Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.\nTechnical Expertise:\nStrong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.\nProficiency in SQL, Python, Spark, and PowerShell.\nExperience in data modeling, ETL/ELT, and data warehousing.\nUnderstanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).\nStrong problem-solving and analytical skills.\nExcellent communication and stakeholder management.\nExposure to hybrid cloud environments (Azure on-premises)\nKey Requirements\nKnowledge of data mesh and data fabric architectures.\nWhat s in it for you\nPathways for Career Development\nWork with colleagues and clients around the world on interesting and challenging work.\nWe provide internal career opportunities, so you can take your career further within TMF.\nContinuous development is supported through global learning opportunities from the TMF Business Academy.\nMaking an Impact\nYou ll be helping us to make the world a simpler place to do business for our clients.\nThrough our corporate social responsibility program, you ll also be making a difference in the communities where we work.\nA Supportive Environment\nStrong feedback culture to help build an engaging workplace.\nOur inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.","Powershell, Azure Data Bricks, Data Architecture, Datawarehousing, Sql, Python"
Data Architect,Reflections Info Systems,12-20 Years,,"Chennai, Pune",IT Management,"Responsibilities include:\nLead data platforms, engineering, product ownership, and architecture teams to drive business growth through next-generation capabilities.\nFoster innovation and thought leadership by adapting to industry trends, leveraging data for high-value product creation.\nCollaborate with business leaders, effectively communicating the value of data-driven solutions across diverse domains.\nDevelop advanced data analytics, utilizing cutting-edge technology to support various business functions.\nCultivate strong partnerships with internal and external stakeholders, alongside fellow analytics professionals.\nDesign, establish, and maintain cost-effective data platforms and solutions while prioritizing return on investment and talent growth for high performance.\nPrimary Skills :\nAzure , AWS, GCP cloud data engineering\nCloud-based databases (Synapse, Databricks, ,Atacama (MDM), Snowflake, Redshift)\nData integration techniques (API, stream, file) using DBT, SQL/PySpark, Python.\nImplementing data products through data mesh / fabric concepts\nData modeling\nETL development\nDatabase management\nManaging teams budgets, portfolios and driving value.\nAbility to define and implement prototypes and MVPs / RATs","Gcp, Database Management, Data Architect, Data Modeling, Data Analytics, Sql, Python"
Data Architect,Reflections Info Systems,12-20 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru",IT Management,"Responsibilities include:\nLead data platforms, engineering, product ownership, and architecture teams to drive business growth through next-generation capabilities.\nFoster innovation and thought leadership by adapting to industry trends, leveraging data for high-value product creation.\nCollaborate with business leaders, effectively communicating the value of data-driven solutions across diverse domains.\nDevelop advanced data analytics, utilizing cutting-edge technology to support various business functions.\nCultivate strong partnerships with internal and external stakeholders, alongside fellow analytics professionals.\nDesign, establish, and maintain cost-effective data platforms and solutions while prioritizing return on investment and talent growth for high performance.\nPrimary Skills :\nAzure , AWS, GCP cloud data engineering\nCloud-based databases (Synapse, Databricks, ,Atacama (MDM), Snowflake, Redshift)\nData integration techniques (API, stream, file) using DBT, SQL/PySpark, Python.\nImplementing data products through data mesh / fabric concepts\nData modeling\nETL development\nDatabase management\nManaging teams budgets, portfolios and driving value.\nAbility to define and implement prototypes and MVPs / RATs","Gcp, Database Management, Data Architect, Data Modeling, Data Analytics, Sql, Python"
Data Architect,Genzeon Corporation,15-20 Years,,Hyderabad,Information Technology,"We are seeking an experienced Data Architect to join our team. The ideal candidate should have a deep understanding of data architecture principles and practices, as well as experience designing and implementing data solutions. The Data Architect will be responsible for developing and maintaining data models, designing data storage solutions, and ensuring data quality and integrity.\nResponsibilities:\nDesign and develop scalable and efficient data solutions that meet business requirements.\nCreate and maintain data models and documentation.\nDevelop and implement data migration and transformation strategies.\nWork with cross-functional teams to identify and resolve data-related issues\nCollaborate with data engineers and data scientists to ensure data accuracy and consistency.\nEnsure compliance with data security and privacy policies and regulations.\nStay up-to-date with emerging technologies and trends in data architecture.\nCore Skills for Data Engineer:\nProficiency in Python and SQL\nExperience working with Linux and AWS\nStrong analytical and problem-solving skills\nAbility to work independently and in a team environment\nExcellent communication and interpersonal skills\nFamiliarity with data visualization tools (e.g., Tableau, Power BI) is a plus\nExperience with data warehousing, ETL, and data integration is highly desirable\nKnowledge of big data technologies (e.g., Hadoop, Spark) is a plus.\nQualifications:\nBachelor's or Master's degree in computer science, information technology, or related field\nMinimum of 15 years of experience in data architecture or related field\nExperience designing and implementing data solutions for enterprise-level organizations\nStrong knowledge of data modeling and database design principles\nKnowledge of data governance and compliance requirements\nFamiliarity with Agile development methodologies and DevOps practices\nMust have skills:\nExperience working with Linux and AWS\nStrong analytical and problem-solving skills\nAbility to work independently and in a team environment\nExcellent communication and interpersonal skills","python, linux, Data Modeling, Sql, aws, Etl"
Data Architect,Genzeon Corporation,15-20 Years,,Hyderabad,Information Technology,"We are seeking an experienced Data Architect to join our team. The ideal candidate should have a deep understanding of data architecture principles and practices, as well as experience designing and implementing data solutions. The Data Architect will be responsible for developing and maintaining data models, designing data storage solutions, and ensuring data quality and integrity.\nResponsibilities:\nDesign and develop scalable and efficient data solutions that meet business requirements.\nCreate and maintain data models and documentation.\nDevelop and implement data migration and transformation strategies.\nWork with cross-functional teams to identify and resolve data-related issues\nCollaborate with data engineers and data scientists to ensure data accuracy and consistency.\nEnsure compliance with data security and privacy policies and regulations.\nStay up-to-date with emerging technologies and trends in data architecture.\nCore Skills for Data Engineer:\nProficiency in Python and SQL\nExperience working with Linux and AWS\nStrong analytical and problem-solving skills\nAbility to work independently and in a team environment\nExcellent communication and interpersonal skills\nFamiliarity with data visualization tools (e.g., Tableau, Power BI) is a plus\nExperience with data warehousing, ETL, and data integration is highly desirable\nKnowledge of big data technologies (e.g., Hadoop, Spark) is a plus.\nQualifications:\nBachelor's or Master's degree in computer science, information technology, or related field\nMinimum of 15 years of experience in data architecture or related field\nExperience designing and implementing data solutions for enterprise-level organizations\nStrong knowledge of data modeling and database design principles\nKnowledge of data governance and compliance requirements\nFamiliarity with Agile development methodologies and DevOps practices\nMust have skills:\nExperience working with Linux and AWS\nStrong analytical and problem-solving skills\nAbility to work independently and in a team environment\nExcellent communication and interpersonal skills","python, linux, Data Modeling, Sql, aws, Etl"
Data Architect,Purview India Consulting And Services Llp,12-22 Years,,Pune,Banking,"Responsibilities\nPrimarily supporting the design, development and deployment of data and analytics-led solutions\nProduce application specific designs based on a pre-defined high-level global architecture/platform.\nContribute to evolution and development of the global architecture/platform.\nProvide necessary governance to ensure alignment with program principles and agreed architectures, including creating and presenting material to relevant governance bodies.\nSupport both the business and engineering teams regarding elaboration of requirements and lower-level designs documenting key decisions and risks.\nSkills\nStrong data background (data storage, transformation, event processing, APIs, IAM, security)\nBeneficial to have knowledge/experience of analytics solutions (ML/AI & BI) - note though that this is not a Data Scientist role.\nGoogle Cloud Platform & Hadoop experience/skills (as this is where the solutions are being developed/deployed)\nProven strength in conceptual and logical thinking, ability to abstract information and look at the bigger picture. Proven analysis skills. Experience in managing and resolving complex issues.\nAbility to articulate and present solutions at the right level for a wide audience.\nExperience of working within a large, complex and geographically dispersed programme","Data Storage, Gcp, Hadoop, Data Architecture, Api"
Data Architect,Health Catalyst,9-11 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Principal Data Architect / Lead Data Platform Architect / Enterprise Data Architect / Solutions Architect Data Engineering\nAbout Company:\nThe healthcare industry is the next great frontier of opportunity for software development, and Health Catalyst is one of the most dynamic and influential companies in this space. We are working on solving national-level healthcare problems, and this is your chance to improve the lives of millions of people, including your family and friends. Health Catalyst is a fast-growing company that values smart, hardworking, and humble individuals. Each product team is a small, mission-critical team focused on developing innovative tools to support Catalyst's mission to improve healthcare performance, cost, and quality.\nHealth Catalyst is expanding and maintains a large suite of Improvement Apps that contribute to healthcare analytics and process improvement solutions. This includes products that manage the care of health system populations, better serve patients at the point of care, reduce health system costs, and reduce clinician workload.\nJob Overview\nWe are seeking a seasoned Principal Data Architect to design, implement, and maintain scalable data ingestion and processing systems for healthcare and clinical data pipelines. This role is ideal for someone who thrives in building real-time data platforms using Kafka, microservices, and distributed systems on AWS. You will lead architectural decisions, influence data governance practices, and support systems that process billions of patient records daily.\nKey Responsibilities\nArchitect and lead the development of scalable, secure, and high-performance data platforms on AWS.\nOwn and optimize Kafka-based pipelines handling real-time and batch ingestion of clinical data (HL7v2, CCDA).\nDesign and manage Kafka Connect infrastructure with custom SMTs and JDBC, S3, MongoDB, and OpenSearch connectors.\nImplement fault-tolerant systems using DLQs, Prometheus, Grafana, and CloudWatch for observability and alerting.\nDrive deduplication strategies and stream processing (Kafka Streams, Flink) to reduce downstream load.\nCollaborate with parser teams to optimize Java-based services that convert HL7 and CCDA into structured data for Kafka brokers.\nDefine and enforce data quality and governance best practices across the pipeline.\nProvide mentorship and technical leadership to a team of engineers and data professionals.\nEngage with product, compliance, and analytics teams to align the data platform with broader business goals.\nRequired Qualifications\n9+ years of experience in software/data engineering with a focus on distributed systems.\nExpertise in Kafka ecosystem (brokers, Kafka Connect, Streams, Schema Registry).\nDeep knowledge of AWS services (EC2, S3, RDS, CloudWatch, IAM, MSK, etc.).\nStrong Java development skills; familiarity with multithreading, concurrency, and system design patterns.\nExperience with microservices, Docker, and CI/CD pipelines.\nProficiency with clinical data formats like HL7v2 and CCDA.\nExperience in performance tuning large-scale data systems (billions of records/day).\nKnowledge of database technologies: MySQL/PostgreSQL, MongoDB, Snowflake, Cassendra, DynamoDB\nExcellent communication skills and ability to work cross-functionally.\nPreferred Qualifications\nExperience with Flink or Spark for streaming and batch processing.\nKnowledge of FHIR, HIPAA compliance, and healthcare domain challenges.\nExperience with Confluent Kafka\nBackground in data deduplication strategies, ETL optimizations, and distributed caching.\nFamiliarity with Agile and Scrum methodologies.","HL7v2, Flink, FHIR, snowflake, Java, Ccda, PostgreSQL, Dynamodb, Kafka, Hipaa, Docker, MySQL, Spark, MongoDB, AWS"
Data Architect,Technogen India Pvt. Ltd.,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nWe are looking for an experienced Data Architect to design, implement, and oversee scalable, high-performance data architectures that align with business objectives. The ideal candidate will lead data strategy, governance, integration, and architecture, ensuring secure, cost-efficient, and well-optimized data solutions.\n\nThis role includes data warehousing, data mesh principles, ETL, BI platform integration, estimations, and review of implementation, ensuring best practices are followed for data security, governance, and performance.\n\nKey Responsibilities\n\nDefine and implement an enterprise-wide data architecture that supports scalability, performance, and business needs.\nDevelop data models (conceptual, logical, and physical) for structured and unstructured data.\nArchitect Data Warehouses, Data Lakes, and Data Mesh-based solutions based on business requirements.\nEnsure data interoperability across APIs, streaming platforms, and BI tools.\nDefine best practices for data ingestion, transformation, and storage.\nProvide accurate effort estimations for data platform implementation, ETL pipelines, and BI integration.\nOptimize cloud storage and compute costs (AWS/Azure).\nGuide teams on efficient resource utilization to minimize unnecessary expenses.\nWork with project managers to estimate timelines, resources, and infrastructure needs for data projects.\nReview end-to-end implementation of data solutions to ensure architectural compliance.\nConduct design and code reviews for ETL processes, and BI reporting layers.\nWork closely with data engineers, BI developers, and analytics teams to ensure best practices in data modeling, query optimization, and indexing strategies.\nTroubleshoot scalability and performance issues, providing guidance on tuning and refactoring.\nDesign and oversee ETL pipelines in cloud-native solutions.\nOptimize data storage, partitioning, and indexing strategies for better performance.\nImplement Master Data Management (MDM) strategies to maintain a Single Source of Truth (SSOT).\nDefine best practices for data replication, caching, and real-time streaming.\nDesign and implement cloud-native data solutions in AWS/Azure.\nArchitect Data Warehouses, Data Lakes, and Lakehouses using AWS Glue/Snowflake/BigQuery/Azure Synapse.\nLead the adoption of Data Mesh principles for decentralized data ownership.\nIntegrate data pipelines with BI platforms such as Power BI, QuickSight, Tableau or ThoughtSpot.\nEnsure data models are optimized for analytical queries in BI platforms.\nCollaborate with BI teams to ensure self-service analytics enablement.\nOptimize query performance, dashboard responsiveness, and reporting efficiency.\nEstablish and enforce data governance policies, standards, and best practices.\nEnsure data integrity, consistency, and quality across all domains.\nOversee data lineage, metadata management, and cataloging for discoverability.\nEnsure compliance with HIPAA, CCPA, and other regulatory requirements.\nImplement role-based access controls (RBAC), encryption, and anonymization techniques.\nMonitor and audit data access and usage for security compliance.\nEnsure secure data transmission and storage with encryption techniques.\nWork closely with data engineers, BI developers, business analysts, and product teams to understand data requirements.\nAct as a bridge between technical and non-technical teams, ensuring alignment on data strategy.\nProvide technical leadership in data-driven projects and proof-of-concepts (PoCs).\n\nRequired Skills & Experience\n\nMust-Have Skills & Experience\nProven experience in data architecture, data modeling, and database design.\nExpertise in AWS Glue/Azure Data Factory and cloud-native data services.\nStrong knowledge of ETL/ELT frameworks (Informatica/Talend/SSIS/AWS Glue).\nHands-on experience with SQL, NoSQL, and cloud-native databases.\nExperience with Big Data processing technologies (Aws Kinesis, Azure Synapse, Amazon Managed Streaming).\nStrong understanding of BI platform integration and query performance optimization.\nKnowledge of data security, encryption, and compliance frameworks (HIPAA).\nAbility to estimate efforts and optimize costs for cloud-based data solutions.\nExperience in data governance, metadata management, and data lineage tracking.\nStrong analytical and problem-solving skills with the ability to troubleshoot performance issues.\nExcellent collaboration and communication skills to work with technical and business stakeholders.\nNice-to-Have Skills\nFamiliarity with multi-cloud and hybrid data architectures.\nHands-on experience with Data Mesh principles and decentralized data ownership.\nCertifications in AWS Certified Data Analytics, Google Professional Data Engineer, or Azure Data Engineer.","Big Data processing technologies, data lineage tracking, BI platform integration, Amazon Managed Streaming, compliance frameworks, AWS Kinesis, Data Security, Metadata Management, AWS Glue, Informatica, SSIS, Sql, Nosql, Azure Synapse, Encryption, Azure Data Factory, Data Governance, Talend"
Data Architect,NTT DATA North America,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Req ID: 324663\n\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\n\nWe are currently seeking a Data Architect to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nKey Responsibilities:\n\nDevelop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.\nUtilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.\nConceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.\nInstitute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.\nImplement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.\nEvaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.\nDevelop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.\nIdentify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.\nFormulate and maintain data models and establish policies and procedures for functional design.\nOffer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.\nStay informed about upgrades and emerging database technologies through continuous research.\nCollaborate with project managers and business leaders on all projects involving enterprise data.\nDocument the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.\nDesign and implement data solutions tailored to meet customer needs and specific use cases.\nProvide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.\n\nBasic Qualifications:\n\n8+ years of hands-on experience with various database technologies\n6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.\nExperience with Azure, Databricks, Snowflake\nKnowledgeable on concepts of GenAI\nAbility to travel at least 25%.\n\nPreferred Skills:\n\nPossess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.\nDemonstrated expertise with certifications in Snowflake.\nValuable Big 4 Management Consulting experience or exposure to multiple industries.\nUndergraduate or graduate degree preferred.\n\nAbout NTT DATA\n\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com\n\nNTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","GenAI, snowflake, Gcp, Databricks, Azure, AWS"
Data Architect,Affine,Fresher,,"Chennai, India",Login to check your skill match score,"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.\nDevelop scalable ETL pipelines using AWS Glue, Lambda, and PySpark to support data transformation, ingestion, and migration.\nDesign and optimize data models following Medallion architecture, Data Mesh, and Enterprise Data Warehouse (EDW) principles.\nImplement data governance, security, and compliance best practices using IAM policies, encryption, and data masking.\nWork with MPP databases such as Snowflake, Redshift, or Databricks, ensuring performance tuning, indexing, and query optimization.\nCollaborate with cross-functional teams, including data engineers, analysts, and business stakeholders, to design efficient data integration strategies.\nEnsure high availability and reliability of data solutions by implementing monitoring, logging, and automation in AWS.\nEvaluate and recommend best practices for ETL workflows, data pipelines, and cloud-based data warehousing solutions.\nTroubleshoot performance bottlenecks and optimize query execution plans, indexing strategies, and data partitioning.\nRequired Qualifications & Skills:\nStrong expertise in AWS Cloud Services: Compute (EC2), Storage (S3), and security (IAM).\nProficiency in programming languages: Python, PySpark, and AWS Lambda.\nMandatory experience in ETL tools: AWS Glue and DMS for data migration and transformation.\nExpertise in MPP databases: Snowflake, Redshift, or Databricks; knowledge of RDBMS (Oracle, SQL Server) is a plus.\nDeep understanding of data modeling techniques: Medallion architecture, Data Mesh, EDW principles.\nExperience in designing and implementing large-scale, high-performance data solutions.\nStrong analytical and problem-solving skills, with the ability to optimize data pipelines and storage solutions.\nExcellent communication and collaboration skills, with experience working in agile environments.\nPreferred Qualifications:\nAWS Certification (AWS Certified Data Analytics, AWS Certified Solutions Architect, or equivalent).\nExperience with real-time data streaming (Kafka, Kinesis, or similar).\nFamiliarity with Infrastructure as Code (Terraform, CloudFormation).\nUnderstanding of data governance frameworks and compliance standards (GDPR, HIPAA, etc.","ETL processes, snowflake, Data Mesh, Medallion architecture, Monitoring, AWS Cloud Services, Dms, IAM policies, Data partitioning, Indexing, Performance Tuning, Pyspark, AWS Glue, Data Modeling, Data Governance, Encryption, Query Optimization, Python, Logging, Aws Lambda, data masking, Redshift, Automation, Databricks"
Data Architect - R01545805,Brillio,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Data Architect\n\nPrimary Skills\n\nAWS, Python, Advanced SQL\n\nJob requirements\n\nYears of experience- 10+ Years\nJOB role- AWS Architect\n8 year of IT experiences with deep expertise in S3, Redshift, Aurora, Glue and Lambda services.\nAt least one instance of proven experience in developing Data platform end to end using AWS\nHands-on programming experience with Data Frames, Python, and unit testing the python as well as Glue code.\nExperience in orchestrating mechanisms like Airflow, Step functions etc.\nExperience working on AWS redshift is Mandatory. Must have experience writing stored procedures, understanding of Redshift data API and writing federated queries\nExperience in Redshift performance tunning. Good in communication and problem solving.\nVery good stakeholder communication and management.","Step Functions, Aurora, Airflow, Glue, Data Frames, S3, Lambda, Advanced Sql, Redshift, Python, AWS"
"Senior IT Architect, Data Architect, Platinion",Boston Consulting Group (BCG),7-9 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nDesign and implement a data architecture that supports the organization's business goals and objectives.\nDevelop data models, define data standards and guidelines, and establish processes for data integration, migration, and management.\nCreate and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organization's data assets.\nEnsure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis.\nWork closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organization's data architecture is integrated and aligned with other IT systems and applications.\nStay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organization's data architecture.\nCommunicate with stakeholders across the organization to understand their data needs and ensure that the organization's data architecture is aligned with the organization's strategic goals and objectives.\n\nWhat You'll Bring\n\nA BTech / MTech degree in Computer Science or a related field\nAt least 7+ years of experience in working on data architecture\nExpertise in data modeling and design, including conceptual, logical, and physical data models,\n\nand must be able to translate business requirements into data models\n\nProficient in a variety of data management technologies, including relational databases,\n\nNoSQL databases, data warehouses, and data lakes\n\nExpertise in ETL processes, including data extraction, transformation, and loading, and must\n\nbe able to design and implement data integration processes\n\nExperience with data analysis and reporting tools and techniques and must be able to design\n\nand implement data analysis and reporting processes\n\nFamiliar with industry-standard data architecture frameworks, such as TOGAF or Zachman,\n\nand must be able to apply them to the organization's data architecture\n\nFamiliar with cloud computing technologies, including public and private clouds, and must be\n\nable to design and implement data architectures that leverage cloud computing\n\nCertificates in Database Management will be preferred\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","NoSQL databases, data lakes, Relational Databases, zachman, ETL processes, data management technologies, cloud computing technologies, data analysis and reporting tools, Data Architecture, Data Modeling, Togaf, data warehouses"
Consultant (Data Architect),Improzo,7-9 Years,,"Pune, India",Login to check your skill match score,"About Improzo\n\nAt Improzo (Improve + Zoe; meaning Life in Greek), we believe in improving life by empowering our customers. Founded by seasoned Industry leaders, we are laser focused on delivering quality-led commercial analytical solutions to our clients. Our dedicated team of experts in commercial data, technology, and operations has been evolving and learning together since our inception. Here, you won't find yourself confined to a cubicle; instead, you'll be navigating open waters, collaborating with brilliant minds to shape the future. You will work with leading Life Sciences clients, seasoned leaders and carefully chosen peers like you!\n\nPeople are at the heart of our success, so we have defined our CARE values framework with a lot of effort, and we use it as our guiding light in everything we do. We CARE!\n\nCustomer-Centric: Client success is our success. Prioritize customer needs and outcomes in every action.\nAdaptive: Agile and Innovative, with a growth mindset. Pursue bold and disruptive avenues that push the boundaries of possibilities.\nRespect: Deep respect for our clients & colleagues. Foster a culture of collaboration and act with honesty, transparency, and ethical responsibility.\nExecution: Laser focused on quality-led execution; we deliver! Strive for the highest quality in our services, solutions, and customer experiences.\n\nAbout The Role\n\nIntroduction: We are seeking an experienced and highly skilled Data Architect to lead a strategic project focused on Pharma Commercial Data Management Operations. This role demands a professional with 7-9 years of experience in data architecture, data management, ETL, data transformation, and governance, with an emphasis on providing scalable and secure data solutions for the pharmaceutical sector.\n\nThe ideal candidate will bring a deep understanding of data architecture principles, experience with cloud platforms such as Snowflake, and a solid background in driving commercial data management projects. If you're passionate about leading impactful data initiatives, optimizing data workflows, and supporting the pharmaceutical industry's data needs, we invite you to apply.\n\nResponsibilities\n\nKey Responsibilities:\n\nLead Data Architecture and Strategy:\nDesign, develop, and implement the overall data architecture for commercial data management operations within the pharmaceutical business.\nLead the design and operations of scalable and secure data systems that meet the specific needs of the pharma commercial team, including marketing, sales, and operations.\nDefine and implement best practices for data architecture, ensuring alignment with business goals and technical requirements.\nDevelop a strategic data roadmap for efficient data management and integration across multiple platforms and systems.\nData Integration, ETL & Transformation:\nOversee the ETL (Extract, Transform, Load) processes to ensure seamless integration and transformation of data from multiple sources, including commercial, sales, marketing, and regulatory databases.\nCollaborate with data engineers and developers to design efficient and automated data pipelines for processing large volumes of data.\nLead efforts to optimize data workflows and improve data transformation processes to enhance reporting and analytics capabilities.\nData Governance & Quality Assurance:\nImplement and enforce data governance standards across the data management ecosystem, ensuring the consistency, accuracy, and integrity of commercial data.\nDevelop and maintain policies for data stewardship, data security, and compliance with industry regulations, such as HIPAA, GDPR, and other pharma-specific compliance requirements.\nWork closely with business stakeholders to ensure the proper definition of master data and reference data standards.\nCloud Platform Expertise (Snowflake (critical to have), AWS, Azure):\nLead the adoption and utilization of cloud-based data platforms, particularly Snowflake, to support data warehousing, analytics, and business intelligence needs.\nCollaborate with cloud infrastructure teams to ensure efficient management of data storage, compute resources, and performance optimization within cloud environments.\nStay up-to-date with the latest cloud technologies, such as Snowflake, AWS, Azure, or Google Cloud (optional)), and evaluate opportunities for incorporating them into data architectures.\nCollaboration with Cross-functional Teams:\nWork closely with business leaders in commercial operations, analytics, and IT teams to understand their data needs and provide strategic data solutions that enhance business operations.\nCollaborate with data scientists, analysts, and business intelligence teams to ensure data is available for reporting, analysis, and decision-making.\nFacilitate communication between IT, business stakeholders, and external vendors to ensure data architecture solutions align with business requirements.\nContinuous Improvement & Innovation:\nDrive continuous improvement efforts to optimize data pipelines, data storage, and analytics workflows.\nIdentify opportunities to improve data quality, streamline processes, and enhance the efficiency of data management operations.\nAdvocate for the adoption of new data management technologies, tools, and methodologies to improve data processing, security, and integration.\nLeadership and Mentorship:\nLead and mentor a team of data engineers, analysts, and other technical resources, fostering a collaborative and innovative work environment.\nProvide leadership in setting clear goals, performance metrics, and expectations for the team.\nOffer guidance on data architecture best practices, ensuring all team members are aligned with the organization's data strategy.\nRequired Qualifications\n\nBachelor's degree in Computer Science, Data Science, Information Systems, or a related field.\n7-9 years of experience in data architecture, data management, and data governance, with a proven track record of leading commercial data management operations projects.\nExtensive experience in data integration, ETL, and data transformation processes, including familiarity with tools like Informatica, Talend, or Apache NiFi.\nStrong expertise with cloud platforms, particularly Snowflake, AWS, Azure, or Google Cloud.\nStrong knowledge of data governance frameworks, including data security, privacy regulations, and compliance standards in the pharmaceutical industry (e.g., HIPAA, GDPR).\nHands-on experience in designing scalable and efficient data architecture solutions to support business intelligence, analytics, and reporting needs.\nProficient in SQL and other query languages, with a solid understanding of database management and optimization techniques.\nAbility to communicate technical concepts effectively to non-technical stakeholders and align data strategies with business goals.\n\nPreferred Qualifications\n\nExperience in the pharmaceutical or life sciences sector, particularly in commercial data management, sales, marketing, or operations.\nCertification or formal training in cloud platforms (e.g., Snowflake, AWS, Azure) or data management frameworks.\nFamiliarity with data science methodologies, machine learning, and advanced analytics tools.\nKnowledge of Agile methodologies for managing data projects.\n\nKey Skills\n\nData Architecture & Design\nCloud Platforms (Snowflake critical to have)\nData Governance & Quality Assurance\nETL & Data Transformation\nData Integration & Pipelines\nPharmaceutical Data Management (Preferred)\nSQL & Database Optimization\nLeadership & Mentorship\nBusiness & Technical Collaboration\n\nBenefits\n\nCompetitive salary and benefits package.\nOpportunity to work on cutting-edge tech projects, transforming the life sciences industry\nCollaborative and supportive work environment.\nOpportunities for professional development and growth.\n\nSkills: snowflake,database,data governance & quality assurance,data integration & pipelines,etl & data transformation,azure,business & technical collaboration,aws,data management,analytics,sql,sql & database optimization,cloud platforms (snowflake),leadership & mentorship,cloud platforms,data architecture & design,data","Leadership, Business Technical Collaboration, Mentorship, Data Architecture Design, Data Integration Pipelines, snowflake, Database Optimization, Sql, Data Governance, Quality Assurance, Azure, AWS, Data Transformation, Etl"
Senior Analyst - Data Architect,Kraft Heinz,5-7 Years,,"Ahmedabad, India",Login to check your skill match score,"Job Description\n\nGeneral Information\n\nRole Description The data architect is responsible for designing, creating, and managing an organization's data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, structured, accessible, secure, and aligned with business objectives.\n\nResponsibilities\n\nInteract & Influence business stakeholders to secure strong engagement and ensures that the data & analytical product delivery aligns with longer-term strategic roadmaps\nDesign & contribute towards the structure and layout of lake house architecture optimizing data storage, and establishing data access controls and security measures\nImplement the long-term Data & Analytics strategy and deliver functional objectives\nAssess requirement feasibility, translates high-level business requirements into data requirements, appropriate metadata, test data, and data quality standards\nExplore Data Sources by working with Application owners to confirm datasets to be extracted\nContribute to establishing and implementing database structure, including schema design, table definitions, column specifications, and naming conventions\nDesign Data models for Source data products, Master data products & Insight data products.\nDocument Data Architecture artifacts for different Data Products and solutions and perform peer review across various functions.\nSupport Data Engineering and BI Engineering teams during the build phase\nReview Data models development, validate and provide deployment approval\nWork closely with data stewards and governance functions to continuously improve data quality and enhance the reliability of data model(s).\nSimplify the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company\nLeads and participates in the peer review and quality assurance of project architectural artifacts across the Data Architecture group through governance forums\nCollaborate and contribute to the development and enhancement of standards, guidelines, and best practices within Data Architecture discipline.\nWorks with Product owners, Business stewards, Data Scientists and end users to understand data consumers needs and develop data products/ data solutions\nEvaluates and recommends emerging technologies for data management, storage, and analytics\n\nEducation\n\nA bachelor's degree in computer science, data science, engineering, or related field\n\nExperience\n\n\nAt least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives\nTranslate business requirements and ability to guide solution design & architecture in developing Data Products\nDevelop scalable, high-performance, and reusable data models that can be efficiently utilized across different data initiatives and help in generating actionable insights\nWork collaboratively with data stewardship and governance functions to continuously improve data quality and enhance the reliability of data models\nAbility to navigate and collaborate with cross-functional teams involving data scientists, business analysts, and stakeholders\nStrong Business Process and Functional understanding with an Analytical background\nCPG experience with knowledge in domain specific concepts is a plus\nKnowledge on Agile methodologies with experience working on tools such as Jira & Confluence\n\nSkills\n\n\nProficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake)\nExperience with database technologies such as SQL, NoSQL, Snowflake, HANA\nUnderstanding of entity-relationship modeling, metadata systems, and data quality tools and techniques\nExperience building enterprise data models (Logical, Physical, Conceptual) and data modeling tool experience a plus (ERWIN, ER/STUDIO, etc.)\nStrong Business Process and SAP functional understanding with an analytics background (preferred SAP ECC/S4, BW, HANA, BI, ARIBA experience) is a plus\nExpert-level SQL skills\nExperience with enterprise scale data engineering orchestration frameworks/ELT tools and common data engineering Python libraries (dbt, pandas, great expectations, etc.) is a plus\nExperience with business intelligence tools and technologies such as Power BI & Tableau\nStrong analytical and problem-solving skills\nUnderstanding of Data Governance principles and practices including Data Quality, Data Security and compliance\nAbility to think strategically on the use of data within the Organization that support both current and future needs.\nExcellent communication and interpersonal skills for stakeholder management and cross-functional collaboration.\n\nLocation(s)\n\nAhmedabad - Venus Stratum GCC\n\nKraft Heinz is an Equal Opportunity Employer Underrepresented Ethnic Minority Groups/Women/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity and other protected classes.","S4, data quality tools, snowflake, HANA BI, entity-relationship modeling, dbt, great expectations, ARIBA, Erwin, Kafka, Tableau, Hana, Sap Ecc, Nosql, Bw, Python, AWS, Power Bi, Er Studio, Sql, Pandas, Gcp, DataFlow, Azure"
Lead Data Architect,JPMorganChase,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nYour goal is to become a key player among other imaginative thinkers who share a common commitment to continuous improvement and meaningful impact. Don't miss this chance to collaborate with brilliant minds and deliver premier solutions that set a new standard.\n\nAs a Lead Data Architect at JPMorgan Chase within the Risk Technology which is aligned to Enterprise Technology division, you are an integral part of a team that works to develop high-quality data architecture solutions for various software applications on modern cloud-based technologies. As a core technical contributor, you are responsible for carrying out critical data architecture solutions across multiple technical areas within various business functions in support of project goals.\n\nJob Responsibilities\n\nEngages technical teams and business stakeholders to discuss and propose data architecture approaches to meet current and future needs\nDefines the data architecture target state of their product and drives achievement of the strategy\nParticipates in data architecture governance bodies\nEvaluates recommendations and provides feedback for new technologies\nExecutes creative data architecture solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down technical problems\nDevelops secure, high-quality production code and reviews and debugs code written by others\nIdentifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems\nFacilitates evaluation sessions with external vendors, startups, and internal teams to drive outcomes through probing of data architectural designs, technical credentials, and applicability for use within existing systems and information architecture\nLeads data architecture communities of practice to drive awareness and use of modern data architecture technologies\nAdds to team culture of diversity, equity, inclusion, and respect\n\nRequired Qualifications, Capabilities, And Skills\n\nFormal training or certification in Data Architecture concepts and 5+ years of applied experience.\nHands-on practical experience delivering system design, application development, testing, and operational stability\nAdvanced knowledge of architecture and one or more programming languages\nProficiency in automation and continuous delivery methods\nProficiency in all aspects of the Software Development Life Cycle\nDemonstrated proficiency in software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)\nIn-depth knowledge of the financial services industry and their IT systems\nPractical cloud native experience\nAdvanced knowledge of one or more software, application, and architecture disciplines\nAbility to evaluate current and emerging technologies to recommend the best data architecture solutions for the future state architecture\n\nPreferred Qualifications, Capabilities, And Skills\n\nExperience in banking / financial domain\n\nAbout Us\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Operational Stability, Cloud Native Experience, Mobile, Machine Learning, Continuous Delivery, Artificial Intelligence, Automation, Software Development Life Cycle, Cloud, Application Development, Data Architecture, Programming Languages, System Design, Testing"
AWS Data Architect,Knack Consulting Services Pvt Ltd.,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Experience: 10+ years(Candidate must have expiring in doing coding on day to day basis)\nUK Shift 2:00PM to 10:30PM\nMandatory skills : Python, SQL, AWS service, lambda glue, data modelling\nKey Responsibilities:\nLead a team of data engineers and architects, providing technical guidance and mentorship. Develop and execute a strategic roadmap for data processing, storage, and analytics in alignment with organizational goals.\nDesign, implement, and maintain robust data pipelines using Python and Airflow, ensuring efficient data flow and transformation for analytical and operational purposes.\nUtilize AWS services, including S3 for data storage, Glue and EMR for data processing, and orchestrate data workflows that are scalable, reliable, and secure.\nImplement real-time data processing solutions using Kafka, SQS, and Event Bridge, addressing high-volume data ingestion and streaming needs.\nOversee the integration of diverse systems and data sources through AppFlow, APIs, and other integration tools, ensuring seamless data exchange and connectivity.\nLead the development of data warehousing solutions, applying best practices in data modelling to support efficient data storage, retrieval, and analysis.\nContinuously monitor, optimize, and troubleshoot data pipelines and infrastructure, ensuring optimal performance and scalability.\nEnsure adherence to data governance, privacy, and security policies, implementing measures to protect sensitive data and comply with regulatory requirements.\nQualifications: -\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n8-10 years of experience in data engineering, with at least 3 years in a leadership role.\nProficient in Python programming and experience with Airflow for workflow management.\nStrong expertise in AWS cloud services, particularly in data storage, processing, and analytics (S3, Glue, EMR, etc.).\nExperience with real-time streaming technologies like Kafka, SQS, and Event Bridge.\nSolid understanding of API based integrations and familiarity with integration tools such as AppFlow\nDeep knowledge of data warehousing concepts.","Airflow, Event Bridge, AppFlow, Glue, S3, Data Modelling, Kafka, Emr, Sql, Lambda, Sqs, Python, AWS"
Senior Data Architect,Nuivio Ventures Inc.,Fresher,,"Chennai, India",Login to check your skill match score,"Job Summary:\nThe Senior Data Architect will lead the design and implementation of scalable, high-performance data architectures across both on-premise and cloud environments. This role involves advanced data modeling, SQL development, and ETL design, with a strong focus on data quality, integrity, and strategy. The architect will collaborate closely with cross-functional teams to align technical solutions with business needs, perform in-depth data analysis, and drive the development of robust data models that support long-term organizational goals.\nCore Competencies:\nData Architecture & Strategy\nAdvanced Data Modeling (Star, Snowflake)\nSQL Development (Partitioning, Stored Procedures, Recursive Queries)\nData Profiling & Root Cause Analysis\nETL Design & Implementation\nCloud & On-Premise Data Platforms (Oracle, Databricks)\nData Quality & Governance\nStakeholder Engagement\nChange Data Capture & Audit Strategy\nCross-Functional Collaboration\nRoles and Responsibilities:\nAnalyze existing data sources to understand data flow, relationships, and usage.\nDesign and implement scalable data models following best practices in normalization, denormalization, and dimensional modeling.\nDevelop and optimize complex SQL queries, including recursive SQLs and macros, to extract, transform, and analyze data.\nArchitect end-to-end data strategies aligned with future-state objectives, focusing on performance, scalability, and flexibility.\nReverse engineer data models through data profiling, identifying key attributes and relationships.\nCollaborate with business and technical stakeholders to map data patterns to underlying processes and use cases.\nConduct root cause analysis (RCA) to troubleshoot data inconsistencies and propose effective resolutions.\nPerform frequency distribution and statistical analyses to detect data trends and quality issues.\nDesign and implement change data capture (CDC), audit logging, and reference data strategies.\nDefine and create mapping tables and helper tables to support flexible and configurable ETL processes.\nOwn and ensure data integrity, accuracy, and completeness across platforms.\nGeneral Attributes:\nDemonstrates intellectual curiosity and passion for uncovering insights in complex datasets.\nSkilled communicator capable of translating technical concepts into actionable insights for diverse stakeholders.\nEngages effectively with clinicians, researchers, data scientists, and IT teams to gather requirements and align objectives.\nLeads deep-dive data analysis initiatives to support data-driven decision-making.\nCollaborates across departments to deliver robust, business-aligned data models and solutions.\nPreferred Skills\nFamiliarity with data visualization tools (e.g., Tableau, Power BI) is an advantage.\nExperience in the Life Sciences or Pharmaceutical industry is highly desirable.","ETL Design Implementation, Data Quality Governance, Root Cause Analysis, change data capture, Data Architecture Strategy, Advanced Data Modeling, Cloud On-Premise Data Platforms, Sql Development, Data Profiling"
Lead Data Architect,JPMorganChase,5-7 Years,,"Delhi, India",Login to check your skill match score,"Job Description\n\nWe know that people want great value combined with an excellent experience from a bank they can trust, so we launched our digital bank, Chase UK, to revolutionize mobile banking with seamless journeys that our customers love. We're already trusted by millions in the US and we're quickly catching up in the UK but how we do things here is a little different. We're building the bank of the future from scratch, channeling our start-up mentality every step of the way meaning you'll have the opportunity to make a real impact.\n\nAs a Lead Data Architect - Data Scientist at JPMorgan Chase within the International Consumer Bank, you will be a part of a flat-structure organization. Your responsibilities are to deliver end-to-end cutting-edge solutions in the form of cloud-native microservices architecture applications leveraging the latest technologies and the best industry practices. You are expected to be involved in the design and architecture of the solutions while also focusing on the entire SDLC lifecycle stages.\n\nOur Business Analytics team is at the heart of this venture, focused on getting smart ideas into the hands of our customers. We're looking for people who have a curious mindset, thrive in collaborative squads, and are passionate about new technology. By their nature, our people are also solution-oriented, commercially savvy and have a head for fintech. We work in tribes and squads that focus on specific products and projects and depending on your strengths and interests, you'll have the opportunity to move between them.\n\nJob Responsibilities\n\nCollaborating with business partners, research teams and domain experts to understand business problems.\nProviding stakeholders with timely and accurate reporting.\nPerforming ad hoc analysis based on diverse data sources to give decision-makers actionable insights about the performance of the products, customer behavior and market trends.\nPresenting your findings in a clear, logical, and persuasive manner, illustrating them with effective visualizations.\nCollaborating with data engineers, machine learning engineers and dashboard developers to automate and optimize business processes.\nIdentifying unexplored opportunities to change how we do business using data.\n\nRequired Qualifications, Capabilities And Skills\n\nFormal training or certification in Data Analysis using Python and 5+ years applied experience\nAdvanced SQL querying skills.\nExperience in taking open ended business questions, then use big data and statistics to create analysis that can provide an answer to the questions at hand.\nExperience with customer analytics such as user behavioral analysis, campaign analysis, etc.\nDemonstrated ability to think beyond raw data and to understand the underlying business context and sense business opportunities hidden in data.\nAbility to work in a dynamic, agile environment within a geographically distributed team.\nExcellent written and verbal communication skills in English.\n\nPreferred Qualifications, Capabilities And Skills\n\nDistinctive problem-solving skills and impeccable business judgment.\nFamiliarity with machine learning.\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Advanced SQL querying, customer analytics, Data Analysis using Python, Big data and statistics, Machine Learning"
Senior Data Architect,ChaiBu Group,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect\nExperience Required: 510 years\nLocation: Bangalore\nAbout the Role:\nWe're looking for a Senior Data Architect to lead the design and development of our data systems. This includes how we collect, store, move, and use data to help the business make better decisions. You'll work mainly with Azure but may also use other platforms. You'll help build strong data foundations for analytics, AI, and app integration while keeping costs and security in check.\nWhat You'll Do:\nCreate and lead the overall data architecture strategy.\nGuide and mentor junior data team members.\nBuild scalable and high-performing data solutions that fit business needs.\nDesign data models that are secure, reusable, and easy to access.\nCreate and manage data flow diagrams and data dictionaries.\nWork closely with business teams to understand their data requirements and turn them into technical solutions.\nMake sure data is easily accessible for data analysts, data scientists, and developers.\nMaintain high standards of data quality, accuracy, and security.\nLead the design of data warehouses, data lakes, and other storage solutions.\nSet and follow best practices for data governance and compliance.\nKeep up with industry changes and help improve internal standards and practices.\nWhat We're Looking For:\nBachelor's degree in Computer Science, IT, or equivalent experience.\n5+ years of proven experience as a Data Architect or in a similar role.\nStrong knowledge of data modeling, warehousing, and integration.\nHands-on experience with databases like SQL Server, Oracle, PostgreSQL.\nFamiliarity with big data tools like Hadoop, Spark, and data lakes such as Azure Data Lake.\nExperience in creating ER diagrams, designing star/snowflake schemas, and building cost-effective data pipelines.\nAbility to turn business goals into technical solutions.\nComfortable working with cloud platforms like Azure, AWS, or Google Cloud.\nStrong understanding of data security and governance.\nExcellent communication and teamwork skills.\nBonus Skills (Preferred, not mandatory):\nTools: Erwin, Azure Synapse, Azure Databricks, Azure DevOps, Power BI, Spark, Python, R.\nExperience with Azure AI/ML Services, Event Hub, Stream Analytics, and scripting tools like Ansible.\nUnderstanding of machine learning, CI/CD, container tools like Docker/Kubernetes.\nRelevant certifications such as AWS Certified Solutions Architect or IBM Certified Data Architect.","snowflake schema, data pipelines, Event Hub, Azure AI ML Services, R, Stream Analytics, Hadoop, Power Bi, PostgreSQL, SQL Server, Azure Databricks, Data Modeling, Azure Synapse, Docker, Ansible, Er Diagrams, Azure Data Lake, Spark, Oracle, Azure, Python, Kubernetes, Azure DevOps"
Data Architect (Azure & Snowflake),CES,3-5 Years,,"Chennai, India",Login to check your skill match score,"CES has 26+ years of experience in delivering Software Product Development, Quality Engineering, and Digital Transformation Consulting Services to Global SMEs & Large Enterprises. CES has been delivering services to some of the leading Fortune 500 Companies including Automotive, AgTech, Bio Science, EdTech, FinTech, Manufacturing, Online Retailers, and Investment Banks. These are long-term relationships of more than 10 years and are nurtured by not only our commitment to timely delivery of quality services but also due to our investments and innovations in their technology roadmap. As an organization, we are in an exponential growth phase with a consistent focus on continuous improvement, process-oriented culture, and a true partnership mindset with our customers. We are looking for the right qualified and committed individuals to play an exceptional role as well as to support our accelerated growth.\nYou can learn more about us at: http://www.cesltd.com/\nJob Description\nExperience with Azure Synapse Analytics: Hands-on experience in designing, developing, and deploying solutions using Azure Synapse Analytics, including familiarity with its various components such as SQL pools, Spark pools, and Integration Runtimes.\nExpertise in Azure Data Lake Storage: In-depth understanding of Azure Data Lake Storage, including its architecture, features, and best practices for managing a large-scale Data Lake or Lakehouse in an Azure environment.\nExperience with AI Tools: Experience with AI Tools and LLMs (e.g. GitHub Copilot, Copilot, ChatGPT) in automating many of the responsibilities outlined for this role.\nKnowledge of Avro and Parquet: Experience working with Avro and Parquet file formats, including data serialization, compression techniques, and schema evolution. Understanding of their advantages and use cases in a big data environment.\nHealthcare: Prior experience working with data in a healthcare or clinical laboratory environment and a strong understanding of PHI, GDPR & HIPPA/HITRUST is highly desirable.\nCertifications: Relevant certifications such as Azure Data Engineer Associate or Azure Synapse Analytics Developer Associate are highly desirable.\nEssential Functions\nData Integration and ELT Development: Design, develop, and maintain data pipelines for ingestion, transformation, and loading of data into Azure Synapse Analytics. This includes understanding functional and non-functional requirements, performing source data analysis, data profiling, and implementing efficient ELT processes.\nAzure Synapse Development: Work with Azure Synapse Analytics to build and optimize data models, SQL queries, stored procedures, and other artifacts necessary for data processing and analysis.\nData Lake File Handling: Understand the characteristics of various file formats, optimizing data storage, and implementing efficient data reading and writing mechanisms for incremental updates within Azure Synapse Analytics.\nData Governance and Security: Ensure compliance with data governance policies and implement security measures to protect sensitive data stored in Azure. This involves encryption, masking, and access control mechanisms.\nPerformance Optimization: Continuously optimize data pipelines and storage configurations to improve performance, scalability, and reliability. This includes identifying bottlenecks, query tuning, and leveraging Azure Synapse Analytics features for parallel processing.\nMonitoring and Troubleshooting: Implement monitoring solutions to track data pipeline performance, data quality, and system health. Troubleshoot issues related to data ingestion, transformation, or storage, and provide timely resolutions.\nSkills Needed to be Successful\nRelational Database Experience: Proficiency with one or more of the following database platforms; e.g. Oracle, Microsoft SQL Server, PostgreSQL, MySQL/MariaDB\nProficiency in SQL: Strong SQL skills, including experience with complex SQL queries, stored procedures, and performance optimization techniques. Familiarity with T-SQL for Azure Synapse Analytics is a plus.\nELT and Data Integration Skills: Proven experience in building ELT pipelines and data integration solutions using tools like Azure Data Factory, Oracle Golden Gate, or similar platforms.\nAbility to handle a variety of legacy data sources and file formats efficiently.\nData Modeling and Warehousing Concepts: Familiarity with dimensional modeling, star schemas, and data warehousing principles. Experience in designing and implementing data models for analytical workloads.\nAnalytical and Problem-Solving Abilities: Strong analytical skills with the ability to understand complex data requirements, troubleshoot technical issues, and propose effective solutions to meet business needs.\nCommunication and Collaboration: Excellent communication skills with the ability to collaborate effectively with cross-functional teams, including Data Scientists, Reporting Analysts, and DevOps professionals.","Azure Data Lake Storage, Relational Database Experience, Data Modeling and Warehousing Concepts, AI Tools, ELT and Data Integration, Parquet, Sql, Avro, Azure Synapse Analytics"
Data Architect (Databricks or Snowflake Certified ),BDO in India,11-15 Years,,"Chennai, India",Login to check your skill match score,"Job Role - Data Architect\nMust : We need Databricks or Snowflake Certified\nLocation: Chennai (Look only for Chennai Candidates)\nWork Mode: Work From Office\nYrs Of Exp: 11-15 Yrs\nNotice Period: Immediate (Max 20days)\nJob Description:\nMid-level to senior data architects frequently have 3+ years or more of experience.\nResearching data acquisition opportunities.\nEngaging with clients through presentations and demonstrations to showcase data solutions\nExcellent communication skills are required to work with cross-functional teams and convert business objectives into technical solutions.\nTranslating business requirements into databases, data warehouses, and data streams.\nDesign and build scalable & metadata-driven data ingestion framework (For Batch and Streaming Datasets)\nAnalyzing, planning, and defining data architecture framework, including security, reference data, metadata, and master data.\nCreating and implementing data management processes and proceduresto ensure data accuracy and accessibility.\nExperience in building data solution in any cloud platform (Azure/AWS).\nCollaborating with other teams within the organisation to devise and implement data strategies, build models, and assess shareholder needs and goals.\nPrior experience in data modelling, database design, and data administration is required.\nDatabase Expertise: Knowledge of data warehousing ideas and proficiency in various database systems (e.g., SQL, NoSQL).\nUnderstanding on different file formats like Delta Lake, Avro, Parquet, JSON, and CSV.\nUnderstanding on data migration projects and implementation strategies.\nKnowledge of Data Governance: Understanding data governance principles, data security, and regulatory compliance.\nKnowledge of programming languages such as Python, SQL and PySpark.\nExperience in supporting BI and Data Science teams in consuming the data in a secure and governed manner.\nMentoring team members to improve their proficiency/efficiency in data architecture.","Parquet, data administration, snowflake, Delta Lake, Data Modelling, Csv, Pyspark, Json, Avro, Sql, Nosql, Database Design, Databricks, Azure, Python, AWS"
Senior Data Architect,Forbes Advisor,15-17 Years,,"Mumbai, India",Login to check your skill match score,"Job Description\n\nKey Responsibilities\n\nStrategic Data Architecture & Pipeline Leadership\n\nVision & Strategy:\n\nDefine and execute the long-term strategy for our data warehousing\n\nplatform using medallion architecture (Bronze, Silver, Gold layers) and\n\nmodern cloud-based solutions.\n\nEnd-to-End Pipeline Oversight:\n\nOversee data ingestion (via Google Ads, Bing Ads, Facebook Ads, GA,\n\nAPIs, SFTP, etc.), transformation (leveraging DBT, and SQL [via\n\nBigQuery]), and reporting, ensuring that our pipelines are robust and\n\nscalable.\n\nData Modeling Best Practices:\n\nChampion best practices in data modeling, including the effective use\n\nof DBT packages to streamline complex transformations.\n\nData Quality, Governance & Attribution\n\nQuality & Validation:\n\nEstablish and enforce rigorous data quality standards, governance\n\npolicies, and automated validation frameworks across all data streams.\n\nStandardization & Visibility:\n\nCollaborate with the Data Engineering, Insights and BIOps team to\n\nstandardize data definitions (including engagement metrics and\n\nrevenue attribution) and ensure consistency across all reports.\n\nAttribution Focus:\n\nDevelop frameworks to reconcile revenue discrepancies and unify\n\nvalidation across Finance, SEM, and Analytics teams.\n\nEnsure accurate attribution of revenue and paid marketing channel\n\nperformance, working closely with SEM and Digital Experiences teams.\n\nMonitoring & Alerting:\n\nImplement robust monitoring and alerting systems (e.g., Slack and\n\nemail notifications) to quickly identify, diagnose, and resolve data\n\npipeline issues.\n\nTeam Leadership & Cross-Functional Collaboration\n\nPeople & Process:\n\nLead, mentor, and grow a high-performing team of data warehousing\n\nspecialists, fostering a culture of accountability, innovation, and\n\ncontinuous improvement.\n\nStakeholder Engagement:\n\nPartner with RevOps, Analytics, SEM, Finance, and Product teams to align\n\nthe data infrastructure with business objectives.\n\nServe as the primary data warehouse expert in discussions around\n\nrevenue attribution and paid marketing channel performance, ensuring\n\nthat business requirements drive technical solutions.\n\nCommunication:\n\nTranslate complex technical concepts into clear business insights for\n\nboth technical and non-technical stakeholders.\n\nOperational Excellence & Process Improvement\n\nDeployment & QA:\n\nOversee deployment processes, including staging, QA, and rollback\n\nstrategies, to ensure minimal disruption during updates.\n\nContinuous Optimization:\n\nRegularly assess and optimize data pipelines for performance,\n\nscalability, and reliability while reducing operational overhead.\n\nLegacy to Cloud Transition:\n\nLead initiatives to transition from legacy on-premise systems to\n\nmodern cloud-based architectures for improved agility and cost\n\nefficiency.\n\nInnovation & Thought Leadership\n\nEmerging Trends:\n\nStay abreast of emerging trends and technologies in data warehousing,\n\nanalytics, and cloud solutions.\n\nPilot Projects:\n\nPropose and lead innovative projects to enhance our data capabilities,\n\nwith a particular focus on predictive and prescriptive analytics.\n\nExecutive Representation:\n\nRepresent the data warehousing function in senior leadership\n\ndiscussions and strategic planning sessions\n\nQualifications\n\nEducation & Experience\n\nBachelor's or Master's degree in Computer Science, Data Science, Information\n\nSystems, or a related field.\n\n15+ years of experience in data engineering, warehousing, or analytics roles,\n\nwith at least 5+ years in a leadership capacity.\n\nProven track record in designing and implementing scalable data\n\nwarehousing solutions in cloud environments.\n\nTechnical Expertise\n\nDeep experience with medallion architecture and modern data pipeline tools,\n\nincluding DBT (and DBT packages), Databricks, SQL, and cloud-based data\n\nplatforms.\n\nStrong understanding of ETL/ELT best practices, data modeling (logical and\n\nphysical), and large-scale data processing.\n\nHands-on experience with BI tools (e.g., Tableau, Looker) and familiarity with\n\nGoogle Analytics, and other tracking systems.\n\nSolid understanding of attribution models (first-touch, last-touch, multi-\n\ntouch) and experience working with paid marketing channels.\n\nLeadership & Communication\n\nExcellent leadership and team management skills with the ability to mentor\n\nand inspire cross-functional teams.\n\nOutstanding communication skills, capable of distilling complex technical\n\ninformation into clear business insights.\n\nDemonstrated ability to lead strategic initiatives, manage competing\n\npriorities, and deliver results in a fast-paced environment.\n\nPerks & Benefits\n\nFlexible/Remote Working: Enjoy flexible work arrangements in a collaborative,\n\ndistributed team culture.\n\nCompetitive Compensation: Attractive salary, performance-based bonuses,\n\nand comprehensive benefits.\n\nTime Off: Generous paid time off, parental leave policies, and a dedicated day\n\noff on the 3rd Friday of each month.\n\nIf you are a visionary leader with a passion for building resilient data infrastructures,\n\na deep understanding of revenue attribution and paid marketing channels, and a\n\nproven ability to drive strategic business outcomes through data, we invite you to\n\njoin our Data & Analytics team and shape the future of our data warehousing\n\nfunction.","dbt, medallion architecture, Looker, BigQuery, Tableau, Data Modeling, Google Analytics, Sql, Etl, ELT"
Data Architect,OneMagnify,7-9 Years,,"Chennai, India",Login to check your skill match score,"The Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.\n\nTeam: Data Platform, Application & Data\n\nReports to: Engineering Manager, Data Platform, Application & Data\n\nMinimum Education: Bachelor's Degree or Equivalent Experience\n\nRecommended Tenure: 7+ years in data platform engineering or architecture roles, including at least 3 years in a hands-on architecture role\n\nRole Summary:\n\nThe Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.\n\nArchitecture & Design:\n\nTranslate business and technical requirements into data models and architecture specifications\nDesign and document data architecture artifacts, including logical/physical data models, data flow diagrams, and integration patterns\nAlign data models with application architecture and system interaction patterns in partnership with Solution Architects\nEstablish and maintain design patterns for relational, NoSQL, and streaming-based data solutions\n\nSolution Delivery & Support:\n\n\nServe as a hands-on architecture lead during project discovery, planning, and delivery phases\nSupport data engineers in implementing data architecture that aligns with platform and project requirements\nValidate implementation through design reviews and provide guidance throughout the development lifecycle\nContribute to platform evolution by defining and enforcing scalable, reusable architecture practices\n\nData Governance & Quality:\n\n\nDefine and uphold best practices for data modeling, data security, lineage tracking, and performance tuning\nPromote consistency in metadata, naming conventions, and data access standards across environments\nSupport data privacy, classification, and auditability across integrated systems\n\nCross-Functional Collaboration:\n\n\nWork closely with product managers, engineering leads, DevOps, and analytics teams to deliver scalable and future-proof data solutions\nCollaborate with Solution Architects to ensure integrated delivery across application and data domains\nAct as a subject matter expert on data structure, semantics, and lifecycle across key business domains\n\nKey Competencies:\n\n\n7+ years of experience in data engineering or data architecture roles, including 3+ years in a dedicated architecture capacity\nProven experience in cloud platformspreferably GCP and/or Azurewith strong familiarity with native data services\nDeep understanding of data storage paradigms including relational, NoSQL, and object storage\nHands-on experience with databases such as Oracle and Postgres; Python proficiency preferred\nFamiliarity with modern DevOps practices including infrastructure-as-code and CI/CD for data pipelines\nStrong communication skills with the ability to lead through influence across technical and non-technical audiences\nSelf-starter with excellent organization and prioritization skills across multiple initiatives","Data Services, Postgres, Oracle, Python"
Senior Data Architect,Axtria - Ingenious Insights,Fresher,,"Bengaluru, India",Login to check your skill match score,"POSITION: Data Architect (Individual contributor)\nData Architect + Gen AI\nLOCATION: Noida, Gurugram, Pune, Bangalore\n60% through out in academics\nJOB OBJECTIVE: To leverage expertise in data architecture and management to design, implement, and\noptimize a robust data warehousing platform for the pharmaceutical industry. The goal is to ensure\nseamless integration of diverse data sources, maintain high standards of data quality and governance,\nand enable advanced analytics through the definition and management of semantic and common data\nlayers. Utilizing Axtria's product and generative AI technologies, the aim is to accelerate business\ninsights and support regulatory compliance, ultimately enhancing decision-making and operational\nefficiency.\nKey Responsibilities:\nStrong AIML, Gen AI exp\nData Modeling: Design logical and physical data models to ensure efficient data storage and\nretrieval.\nETL Processes: Develop and optimize ETL processes to accurately and efficiently move data\nfrom various sources into the data warehouse.\nInfrastructure Design: Plan and implement the technical infrastructure, including hardware,\nsoftware, and network components.\nData Governance: Ensure compliance with regulatory standards and implement data\ngovernance policies to maintain data quality and security.\nPerformance Optimization: Continuously monitor and improve the performance of the data\nwarehouse to handle large volumes of data and complex queries.\nSemantic Layer Definition: Define and manage the semantic layer architecture and technology\nstack to manage the lifecycle of semantic constructs including consumption into downstream\nsystems.\nCommon Data Layer Management: Integrate data from multiple sources into a centralized\nrepository, ensuring consistency and accessibility.\nDeep expertise in architecting enterprise grade software systems that are performant,\nscalable, resilient and manageable. Architecting GenAI based systems is an added plus.\nAdvanced Analytics: Enable advanced analytics and machine learning to identify patterns in\ngenomic data, optimize clinical trials, and personalize medication.\nGenerative AI: Should have worked with production ready usecase for GenAI based data and\nStakeholder Engagement: Work closely with business stakeholders to understand their data\nneeds and translate them into technical solutions.\nCross-Functional Collaboration: Collaborate with IT, data scientists, and business analysts to\nensure the data warehouse supports various analytical and operational needs.\nQualifications:\nProven experience in data architecture and data warehousing, preferably in the\npharmaceutical industry.\nStrong knowledge of data modeling, ETL processes, and infrastructure design.\nExperience with data governance and regulatory compliance in the life sciences sector.\nExcellent analytical and problem-solving skills.\nStrong communication and collaboration skills.\nPreferred Skills:\nFamiliarity with advanced analytics and machine learning techniques.\nExperience in managing semantic and common data layers.\nKnowledge of FDA guidelines, HIPAA regulations, and other relevant regulatory standards.\nExperience with generative AI technologies and their application in data warehousing\nABOUT AXTRIA\nAxtria (www.axtria.com) is high growth advanced analytics and business information Management\nCompany based out of New Jersey with locations in AZ, GA and VA in USA and Gurgaon in India. We\nhave been named as one of the fastest growing companies in the US by Inc. 5000 in 2014.\nOur broad portfolio of services and solutions help our clients improve their sales, marketing, supply\nchain and distribution planning and operations in various industries such as Pharma, Retail, Banking and\nTechnology. We blend analytics, technology and consulting to help customers gain deep insights from\ntheir customer data, create strategic advantage and drive profitable growth.\nThe leadership team at Axtria brings deep industry experience, expertise in sales, marketing and risk\nmanagement as well as a passion for building cutting-edge analytics and technology solutions.\nOur global team is committed to delivering high quality, high-impact solutions for our clients and to\nbuilding a world-class firm with enduring value. Our unique team combines real-world business\nknowledge, a depth of analytical skill and experience with the latest technologies. Those who excel at\nAxtria share a number of common qualities. They are smart, humble and have the analytical toolkit to\nput their intelligence to work. They are passionate about data and analytics & seek to constantly learn.","Performance Optimization, Generative AI, ETL Processes, Common Data Layer Management, Infrastructure Design, Machine Learning, Data Modeling, Advanced Analytics, Data Architecture, Data Warehousing, Data Governance"
Data Architect,MathCo,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Architect, you will design and implement scalable, cloud-native data solutions that handle petabyte-scale datasets. You will lead architecture discussions, build robust data pipelines, and work closely with cross-functional teams to deliver enterprise-grade data platforms. Your work will directly support analytics, AI/ML, and real-time data processing needs across global clients.\nKey Responsibilities:\nTranslate complex data and analytics requirements into scalable technical architectures.\nDesign and implement cloud-native architectures for real-time and batch data processing.\nBuild and maintain large-scale data pipelines and frameworks using modern orchestration tools (e.g., Airflow, Oozie).\nDefine strategies for data modeling, integration, metadata management, and governance.\nOptimize data systems for cost-efficiency, performance, and scalability.\nLeverage cloud services (AWS, Azure, GCP) including Azure Synapse, AWS Redshift, BigQuery, etc.\nImplement data governance frameworks covering quality, lineage, cataloging, and access control.\nWork with modern big data technologies (e.g., Spark, Kafka, Databricks, Snowflake, Hadoop).\nCollaborate with data engineers, analysts, DevOps, and business stakeholders.\nEvaluate and adopt emerging technologies to improve data architecture.\nProvide architectural guidance in cloud migration and modernization projects.\nLead and mentor engineering teams and provide technical thought leadership.\nRequired Skills and Experience:\nBachelor's or Master's in Computer Science, Engineering, or related field.\n10+ years of experience in data architecture, engineering, or platform roles.\n5+ years of experience with cloud data platforms (Azure, AWS, or GCP).\nProven experience building scalable enterprise data platforms (data lakes/warehouses).\nStrong expertise in distributed computing, data modeling, and pipeline optimization.\nProficiency in SQL and NoSQL databases (e.g., Snowflake, SQL Server, Cosmos DB, DynamoDB).\nExperience with data integration tools like Azure Data Factory, Talend, or Informatica.\nHands-on experience with real-time streaming technologies (Kafka, Kinesis, Event Hub).\nExpertise in scripting/programming languages such as Python, Spark, Java, or Scala.\nDeep understanding of data governance, security, and regulatory compliance (GDPR, HIPAA, CCPA).\nStrong communication, presentation, and stakeholder management skills.\nAbility to lead multiple projects simultaneously in an agile environment.","Airflow, Event Hub, Snowflake SQL Server, CCPA, Cloud-native architectures, snowflake, Data pipelines, Gdpr, Aws Redshift, Kafka, Hipaa, Data Governance, Informatica, Nosql, Azure Synapse, Kinesis, Oozie, Cosmos DB, Talend, Python, AWS, Java, BigQuery, Hadoop, Scala, Dynamodb, Sql, Azure Data Factory, Gcp, Spark, Databricks, Azure"
Data Architect - India,Zywave,5-7 Years,,"Itanagar, India",Login to check your skill match score,"Brief Description\n\nAs a Data Architect at Zywave, you will play a critical role in designing and implementing data solutions that support our financial and business intelligence initiative. The ideal candidate will have a strong background in data architecture, data modeling and data reporting.\n\nEssential Functions:\n\nDesign and develop scalable and efficient data architectures to support financial and BI reporting.\nCollaborate with cross-functional teams to understand data requirements and translate them into technical solutions.\nCreate and maintain data models, schemas, and databases to ensure data integrity and consistency.\nDevelop and implement ETL processes to extract, transform, and load data from various sources.\nOptimize data storage and retrieval processes to enhance performance and scalability.\nEnsure data security and compliance with relevant regulations and standards.\nProvide technical guidance and support to data analysts and BI developers.\nStay up to date with industry trends and best practices in data architecture and management.\n\nFactors for Success:\n\nBachelor's degree in Computer Science, Information Systems, or related field\n5+ years experience as a Data Architect or similar role\nStrong knowledge of data modeling, database design, and data management principles\nFamiliarity with BI tools such as Snowflake, Tableau, PowerBI, etc.\nExcellent problem-solving and analytical skills\nAbility to communicate complex technical concepts to non-technical stakeholders\nExperience with SaaS products and understanding of SaaS business models is a plus\nStrong attention to detail and ability to work independently or collaboratively\n\nWhy pick Zywave\n\nZywave empowers insurers and brokers to drive profitable growth and thrive in today's escalating risk landscape. Only Zywave delivers a powerful Performance Multiplier, bringing together transformative, ecosystem-wide capabilities to amplify impact across data, processes, people, and customer experiences. More than 15,000 insurers, MGAs, agencies, and brokerages trust Zywave to sharpen risk assessment, strengthen client relationships, and enhance operations. Additional information can be found at www.zywave.com.","data reporting, data integrity, snowflake, ETL processes, Data Security, Powerbi, Bi Tools, Database Design, Data Architecture, Tableau, Data Modeling"
Principal Digital Architect-Data Architect,Caterpillar Inc.,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Career Area\n\nTechnology, Digital and Data\n\nJob Description\n\nYour Work Shapes the World at Caterpillar Inc.\n\nWhen you join Caterpillar, you're joining a global team who cares not just about the work we do but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.\n\nJob Purpose\n\nA Principal Digital Architect drives solutions with the core digital platform that underpins many applications that supports Cat Digital's key business domains of eCommerce, advanced condition monitoring, lead generation, service support and equipment management. The solutions will solve big data problems with over 1.4 million connected assets worldwide, advanced analytics, AI capabilities, global marketing and much more. This role is a key leader and accountable to work across the organization for both business and technical alignment to drive the required business outcomes.\n\nJob Duties\n\nKey areas of accountability include leading the development of Architecture Solutions for strategic Cat Digital projects and programs; developing and maintaining global technology roadmaps and application evolution plans; leading in the evaluation of new technology, information or integration standards; and developing digital strategy for a specific technical or business domain.\n\nResponsibilities Include One Or More Of The Following\n\nProvide oversight for architecture assessment and design for infrastructure, information or integration domains that provide core capabilities for the enterprise.\nLead Architecture design of end to end enterprise integrated systems that serves multiple business functions.\nLead the design and implementation of enterprise data model and metadata structures for complex projects.\nInitiate and deliver technology evaluation and recommendations.\nDevelop and maintain current, planned and future state architecture blueprints.\nLead in the identification and analysis of enterprise business drivers and requirements that drive the future state architecture.\n\nBasic Qualifications\n\nPosition requires a four-year degree from an accredited college or university in computer science, information technology, or related field or equivalent work experience.\n15 or more years of a progressive career in distributed system software engineering and architecture\nStrong demonstratable experience delivering product and/or enterprise architecture for enterprise scale solutions in public cloud or hybrid eco-systems\nAt least 5 years working experience in Cloud Technologies such as AWS & Microsoft Azure\nMust have excellent communication skills and be able to deal with sensitive issues, mentor and coach and/or persuade others on new technologies, new applications, or potential solutions.\n\nTop Candidates Will Have\n\nUnderstand the data platform and build new data solutions on the existing data platform. Impact analysis needs to be performed so as not to have unknown impact in other data solutions build on the platform.\n\nUnderstand the current data landscape and build new solutions on top of existing solution.\nTrouble shoots and finds solutions for technical and functional issues identified in the program/project.\nEvaluate, analyse, document and communicate business requirements to stakeholders.\nRun Architecture meetings/discussions and document the solutions on confluence. Complete the solutions and have engineering handover.\nSupport the engineering team during the entire cycle of the build and deploy phases.\nReport on common sources of technical, functional issues and/or questions and make recommendations to architecture team for long term solution.\nOwn and develop relationship with partners (customers, dealers, Technical Product Management, Architect teams), working with them to optimize and enhance the data products.\nProvide guidance on the technical solutions and guidance on new product like optimal database recommendations like dynamo vs Postgre, AWS options like Kinesis and Event bridge.\nOwn the solutions on the Data lake (Snowflake), The solutions should be performant, secure and Cost Optimal.\nOwn some of the data domains in the Data Platform i.e. Any solution on the data domain should be either worked upon or reviewed by the architect.\nProvide ROM (rough order of magnitude) for the solutions and data products.\nBased on understanding of data domains and Business requirements create reusable data products which can be used across applications and teams.\nCreate/Review HLA and TA documentation with reference to a business requirement.\nImprove architecture by tracking emerging technologies and evaluating their applicability to business goals and operational requirements.\nIdentify and solution for business critical business rules for improving the data quality in the platform.\n\nSkills\n\nMust Have:\n\nAWS (EMR, Glue, S3, Fargate, SNS, SQS, Kinesis, AWS EventBridge, RDS, DynamoDB) , Snowflake, SQL, Python, ER Modelling.\n\nGood To Have\n\nMicroservices and API knowledge\n\nPosting Dates\n\nMay 19, 2025 - May 25, 2025\n\nCaterpillar is an Equal Opportunity Employer.\n\nNot ready to apply Join our Talent Community.","AWS EventBridge, Fargate, snowflake, Glue, API knowledge, ER Modelling, S3, RDS, Dynamodb, Emr, Sql, Microservices, Kinesis, Sqs, Sns, Python, AWS"
Sr. Data Architect,WTW,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Description\n\nThe Role:\n\nPartner with other architecture resources to lead the end-to-end architecture of the health and benefits data platform using Azure services, ensuring scalability, flexibility, and reliability.\nDevelop broad understanding of the data lake architecture, including the impact of changes on a whole system, the onboarding of clients and the security implications of the solution.\nDesign a new or improve upon existing architecture including data ingestion, storage, transformation and consumption layers.\nDefine data models, schemas, and database structures optimized for H&B use cases including claims, census, placement, broking and finance sources.\nDesigning solutions for seamless integration of diverse health and benefits data sources.\nImplement data governance and security best practices in compliance with industry standards and regulations using Microsoft Purview.\nEvaluate data lake architecture to understand how technical decisions may impact business outcomes and suggest new solutions/technologies that better align to the Health and Benefits Data strategy.\nDraw on internal and external practices to establish data lake architecture best practices and standards within the team and ensure that they are shared and understood.\nContinuously develop technical knowledge and be recognised as a key resource across the global team.\nCollaborate with other specialists and/or technical experts to ensure H&B Data Platform is delivering to the highest possible standards and that solutions support stakeholder needs and business requirements.\nInitiate practices that will increase code quality, performance and security.\nDevelop recommendations for continuous improvements initiatives, applying deep subject matter knowledge to provide guidance at all levels on the potential implications of changes.\nBuild the team's technical expertise/capabilities/skills through the delivery of regular feedback, knowledge sharing, and coaching.\nHigh learning adaptability, demonstrating understanding of the implications of technical issues on business requirements and / or operations.\nAnalyze existing data design and suggest improvements that promote performance, stability and interoperability.\nWork with product management and business subject matter experts to translate business requirements into good data lake design.\nMaintain the governance model on the data lake architecture through training, design reviews, code reviews, and progress reviews.\nParticipate in the development of Data lake Architecture and Roadmaps in support of business strategies\nCommunication with key stakeholders and development teams on technical solutions. Convince and present proposals by way of high-level solutions to end users and/or stakeholders.\n\nThe Requirement\n\nCandidate must have significant experience in a technology related discipline, such as IT or Engineering with a Bachelor's/College Degree in these areas being beneficial.\nStrong experience in databases, tools and methodologies\nStrong skills across a broad range of database technologies including Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, Azure Data Lake Storage, and other Azure Services.\nWorking knowledge of Microsoft Fabric is preferred.\nData Analysis, Data Modeling, Data Integration, Data Warehousing, Database Design\nExperience with database performance evaluation and remediation\nDevelop strategies for data acquisitions, archive recovery and implementation\nBe able to design and develop Databases, Data Warehouses and Multidimensional Databases\nExperience in Data Governance including Microsoft Purview, Azure Data Catalogue, Azure Data Share, and other Azure tools.\nFamiliarity with legal risks related to data usage and rights.\nExperience in data security, including Azure Key Vault, Azure Data Encryption, Azure Data Masking, Azure Data Anonymization, and Azure Active Directory.\nAbility to develop database strategies for flexible high-performance reporting and business intelligence\nExperience using data modeling tools & methodology\nExperience working within an Agile Scrum Development Life Cycle, across varying levels of Agile maturity\nExperience working with geographically distributed scrum teams\nExcellent verbal and writing skills, including the ability to research, design, and write new documentation, as well as to maintain and improve existing material\n\nTechnical Competencies\n\nSubject Matter Expertise\n\nDeveloping expertise\nYou strengthen your depth and/or breadth of subject matter knowledge and skills across multiple areas.\nYou define the expertise required in your area based on emerging technologies, industry practices. You build the team's capability accordingly.\nApplying expertise\nYou apply subject matter knowledge and skills across multiple areas to assess the impact of complex issues and implement long-term solutions. You foster innovation using subject matter knowledge to enhance tools, practices, and processes for the team.\nSolution Development\n\nSystems thinking\nYou lead and foster collaboration across H&B Data Platform Technology to develop solutions to complex issues.\nYou apply a whole systems approach to evaluating impact, and take ownership for ensuring links between structure, people and processes are made.\nFocusing on quality\nYou instill a quality mindset to the team and ensure the appropriate methods, processes and standards are in place for teams to deliver quality solutions. You create and deliver improvement initiatives.\nTechnical Communication\n\nSimplifying complexity\nYou develop tools, aids and/or original content to support the delivery and/or understanding of complex information. You guide others on best practice.\nQualifications\n\nCandidate must have significant experience in a technology related discipline, such as IT or Engineering with a Bachelor's/College Degree in these areas being beneficial.","Azure Data Encryption, Azure Key Vault, Azure Data Masking, Azure SQL Database, Azure Data Lake Storage, Azure Data Anonymization, Microsoft Purview, Azure Data Share, Azure Data Catalogue, Data Analysis, Data Modeling, Azure Databricks, Data Integration, Database Design, Azure Active Directory, Azure Data Factory, Azure Synapse Analytics, Data Warehousing"
Data Architect,Gramener,Fresher,,"Chennai, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore/Chennai/Noida/Mumbai\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career path, steady growth prospects with great scope to innovate. Our goal is to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\nYou'll architect and build a platform that helps thousands of developers tell data stories! Gramex, our flagship micro services-based low code applications platform, builds custom data applications at supersonic speed.\n\nWe are looking for a technical architect to build domain-specific new solutions and products leveraging the power of Gramex.\n\nYou Will\n\nDesign and implement enterprise-grade data architectures leveraging the medallion architecture (Bronze, Silver, Gold).\nDevelop and enforce data modelling standards, including flattened data models optimized for analytics.\nDefine and implement MDM strategies (Reltio), data governance frameworks (Collibra), and data classification policies.\nLead the development of data landscapes, capturing sources, flows, transformations, and consumption layers.\nCollaborate with domain teams to ensure consistency across decentralized data products in a data mesh architecture.\nGuide best practices for ingesting and transforming data using Fivetran, PySpark, SQL, and Delta Live Tables (DLT).\nDefine metadata and data quality standards across domains.\nProvide architectural oversight for data platform development on Databricks (Lakehouse) and AWS ecosystem.\n\nSkills And Qualifications\n\nExperience in Pharma domain.\nData Modeling (dimensional, flattened, common data model, canonical, and domain-specific, entity level data understanding from business process point of view).\nProven expertise in Data Mesh or Domain-Oriented Data Architecture.\nExperience with medallion/lakehouse architecture.\nAbility to create data blueprints and landscape maps across complex enterprise systems.\nMaster Data Management (MDM) principles and tools (Reltio) (1).\nData Governance and Data Classification frameworks (1).\nStrong experience with Fivetran**, PySpark, SQL, Python.\nDeep understanding of Databricks (Delta Lake, Unity Catalog, Workflows, DLT) .\nExperience with AWS services related to data (e.g., S3, Glue, Redshift, IAM, ).\nExperience on Snowflake.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data acts as the core of decision making. We undertake strategic data consulting for organizations in laying out the roadmap for data-driven decision making. It helps organizations to convert data into a strategic differentiator. Through a host of our product and Solutions, and Service Offerings, we analyze and visualize large amounts of data.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Unity Catalog, snowflake, data classification, Data Mesh, Fivetran, Workflows DLT, Databricks Delta Lake, Domain-Oriented Data Architecture, Medallion Lakehouse Architecture, Data Modeling, Pyspark, Sql, Data Governance, Python"
Data Architect,Gramener,Fresher,,"Hyderabad, India",Login to check your skill match score,"What You'll Do:\nDesign and implement enterprise-grade data architectures leveraging the medallion architecture\nDevelop and enforce data modelling standards, including flattened data models optimized for analytics.\nDefine and implement MDM strategies (using Immuta), data governance\nframeworks (Collibra), and data classification policies.\nLead the development of data landscapes, capturing sources, flows, transformations, and consumption layers.\nProvide architectural oversight for data platform development on Databricks (Lakehouse) and AWS ecosystem.\nWhat We're Looking For:\nData Modeling (dimensional, flattened, canonical, and domain-specific)\nMaster Data Management (MDM) principles and tools\nData Governance and Data Classification frameworks\nStrong experience with Fivetran, Pyspark, SQL, DLT\nDeep understanding of Databricks (Delta Lake, Unity Catalog, Workflows)\nExperience with AWS services related to data (e.g., S3, Glue, Redshift, IAM)\nExperience on Snowflake.\nExperience in Pharma, MedTech, or Life Sciences domains\nFamiliarity with regulatory and compliance frameworks (e.g., GxP, HIPAA, GDPR)\nBackground in data product building\nArchitecture & Design:\nProven expertise in Data Mesh or Domain-Oriented Data Architecture\nExperience with medallion/lake house architecture\nAbility to create data blueprints and landscape maps across complex enterprise systems","Databricks Delta Lake, Domain-Oriented Data Architecture, DLT, Medallion Lake House Architecture, Unity Catalog, snowflake, data classification, Data Mesh, Fivetran, Pyspark, Data Modeling, Sql, Data Governance"
Senior Data Architect,Tredence Inc.,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"Primary Roles and Responsibilities:\nWorking experience in Snowflake; use of Snow SQL CLI, Snow Pipe creation of custom functions and Snowflake stored producers, schema modelling, performance tuning etc.\nExpertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts\nExtensive experience in DBT CLI, DBT Cloud, GitHub version control and repository knowledge, and DBT scripting to design & develop SQL processes to perform complex ELT processes and data pipeline build.\nAbility to independently envision and develop innovative ETL and reporting solutions and execute them through to completion.\nTriage issues to find gaps in existing pipelines and fix the issues\nAnalyze the data quality, align the technical design to data governance, and address all the required non-business but operational requirements during the design and build of data pipelines\nDevelop and maintain data pipelines using DBT\nProvide advice, guidance, and best practices around Snowflake\nProvide guidance on moving data across different environments in Snowflake\nCreate relevant documentation around database objects\nTroubleshoot production support issues post-deployment and come up with solutions as required\nGood Understanding and knowledge of CI/CD process and GitHub->DBT-> Snowflake integrations.\nAdvance SQL knowledge and hands-on experience in complex query writing using Analytical functions, Troubleshooting, problem-solving, and performance tuning of SQL queries accessing data warehouse as well as Strong knowledge of stored procedures.\nExperience in Snowflake advanced concepts such as resource monitors, virtual warehouse sizing, query performance tuning, zero-copy clone, time travel and understanding how to use these features\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussions with client architect and team members\nGood experience in developing scripts for data auditing and automating various database platform manual activities.\nUnderstanding of the full software lifecycle and how development teams should work with DevOps to create more software faster.\nExcellent communication, working in Agile Methodology/Scrum\nSkills and Qualifications:\nBachelor's and/or master's degree in computer science or equivalent experience.\nMust have total 13+ yrs. of IT experience and 3+ years experience in data Integration, ETL/ETL development, and database design or Datawarehouse design\nDeep understanding of Star and Snowflake dimensional modelling.\nStrong knowledge of Data Management principles\nWorking experience in Snowflake; use of Snow SQL CLI, schema modelling, performance tuning etc.\nDevelop and maintain data pipelines using DBT\nExperience with writing complex SQL queries, especially dynamic SQL\nExpertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts\nExperience with performance tuning and optimization of SQL queries\nExperience of working with Retail Data\nExperience with data security and role-based access controls\nComfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects\nShould have experience working in Agile methodology\nStrong verbal and written communication skills.\nStrong analytical and problem-solving skills with high attention to detail.","Snow SQL CLI, snowflake, dbt, Github, Data Modelling, Agile Methodology, Sql, Performance Tuning, Etl"
Google Cloud Data Architect - Remote Work,techolution,Fresher,,India,Login to check your skill match score,"Techolution is looking for highly skilled and innovative hands-on Google Cloud Data Architect who will help Architect & build a highly scalable and reliable platform to match our exponential growth. In this role you will be responsible for Architecting & building a solid back end infrastructure on Google Cloud which will enable data delivery in near real-time using next-gen technologies. You are a technical leader, serving as a liaison among business partners, technical resources, and project stakeholders. Also work closely with our clients, helping them to leverage the power of GCP to unlock the full potential of their data and drive their businesses forward.\nRole: Google Cloud Data Architect\nLocation: Remote\nEmployment Type: Full time\nResponsibilities:\nArchitect, Build & Deploy scalable Data Engineering & Analytics solutions in GCP.\nDesign and development of large scale data solutions using GCP services like DataProc, Dataflow, Cloud Bigtable, Big Query, Cloud SQL, Pub/Sub, Cloud Data Fusion, Cloud Composer, Cloud Functions, Cloud storage, Compute Engine, Looker and Cloud IAM.\nMentor and provide technical oversight and guidance to implementation teams while working in a coordinated manner to deliver and deploy the designed architecture.\nDeploy the data pipeline following data governance and data security requirements, and implement it with the data quality check on Google Cloud Platform.\nCreate and deliver best practices recommendations for On Prem to Cloud or Cloud Native solutions in GCP.\nDesign and creation of data led strategies which provide clients with opportunities to leverage their data for greater insight or performance.\nLeading the technical design and implementation of data solutions, ensuring that they meet our clients business requirements while adhering to best practices and industry standards.\nRequirements:\nExperience in Architecting and designing solutions leveraging services like Cloud Bigquery, Cloud Dataflow, Data Proc, Cloud Composer, Datafusion, Pubsub, Airflow and Cloud BigTable.\nMust have handled projects using ETL / Data Pipeline and Orchestration tools Cloud composer, Cloud data fusion, Dataflow and Dataproc.\nHands-on experience with programming languages such as java/Go/Python.\nExposure with non relational databases such as Mongodb, cassandra, Dynamodb, redis etc..\nKnowledge of messaging queues such as kafka and rabbitmq with strong troubleshooting skills\nCertified Professional Cloud Architect & Official Google Data Engineer Certification is beneficial.\nAbout Techolution:\nTecholution is a next gen AI consulting firm on track to become one of the most admired brands in the world for AI done right. Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better human experience for the communities they serve.\nAt Techolution, we build custom AI solutions that produce revolutionary outcomes for enterprises worldwide. Specializing in AI Done Right, we leverage our expertise and proprietary IP to transform operations and help achieve business goals efficiently.\nWe are honored to have recently received the prestigious Inc 500 Best In Business award, a testament to our commitment to excellence. We were also awarded - AI Solution Provider of the Year by The AI Summit 2023, Platinum sponsor at Advantage DoD 2024 Symposium and a lot more exciting stuff! While we are big enough to be trusted by some of the greatest brands in the world, we are small enough to care about delivering meaningful ROI-generating innovation at a guaranteed price for each client that we serve.\nOur thought leader, Luv Tulsidas, wrote and published a book in collaboration with Forbes, Failing Fast Secrets to succeed fast with AI. Refer here for more details on the content - https://www.luvtulsidas.com/\nLet's explore further!\nUncover our unique AI accelerators with us:\n1. Enterprise LLM Studio: Our no-code DIY AI studio for enterprises. Choose an LLM, connect it to your data, and create an expert-level agent in 20 minutes.\n2. AppMod. AI: Modernizes ancient tech stacks quickly, achieving over 80% autonomy for major brands!\n3. ComputerVision. AI: Our ComputerVision. AI Offers customizable Computer Vision and Audio AI models, plus DIY tools and a Real-Time Co-Pilot for human-AI collaboration!\n4. Robotics and Edge Device Fabrication: Provides comprehensive robotics, hardware fabrication, and AI-integrated edge design services.\n5. RLEF AI Platform: Our proven Reinforcement Learning with Expert Feedback (RLEF) approach bridges Lab-Grade AI to Real-World AI.\nSome videos you wanna watch!\nComputer Vision demo at The AI Summit New York 2023\nLife at Techolution\nGoogleNext 2023\nAi4 - Artificial Intelligence Conferences 2023\nWaWa - Solving Food Wastage\nSaving lives - Brooklyn Hospital\nInnovation Done Right on Google Cloud\nTecholution featured on Worldwide Business with KathyIreland\nTecholution presented by ION World's Greatest\nVisit us @www.techolution.com : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with technology.","Big Query, Go, Pub Sub, Cloud Composer, Cloud SQL, Cloud IAM, Cloud Data Fusion, Cloud Functions, Looker, Orchestration tools, Cloud Bigtable, Cassandra, Kafka, Cloud Storage, Data Pipeline, Python, Java, Dynamodb, Redis, Rabbitmq, Compute Engine, Dataproc, MongoDB, DataFlow, Etl"
Senior Data Architect,ChaiBu Group,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect\nExperience Required: 510 years\nLocation: Bangalore\nAbout the Role:\nWe're looking for a Senior Data Architect to lead the design and development of our data systems. This includes how we collect, store, move, and use data to help the business make better decisions. You'll work mainly with Azure but may also use other platforms. You'll help build strong data foundations for analytics, AI, and app integration while keeping costs and security in check.\nWhat You'll Do:\nCreate and lead the overall data architecture strategy.\nGuide and mentor junior data team members.\nBuild scalable and high-performing data solutions that fit business needs.\nDesign data models that are secure, reusable, and easy to access.\nCreate and manage data flow diagrams and data dictionaries.\nWork closely with business teams to understand their data requirements and turn them into technical solutions.\nMake sure data is easily accessible for data analysts, data scientists, and developers.\nMaintain high standards of data quality, accuracy, and security.\nLead the design of data warehouses, data lakes, and other storage solutions.\nSet and follow best practices for data governance and compliance.\nKeep up with industry changes and help improve internal standards and practices.\nWhat We're Looking For:\nBachelor's degree in Computer Science, IT, or equivalent experience.\n5+ years of proven experience as a Data Architect or in a similar role.\nStrong knowledge of data modeling, warehousing, and integration.\nHands-on experience with databases like SQL Server, Oracle, PostgreSQL.\nFamiliarity with big data tools like Hadoop, Spark, and data lakes such as Azure Data Lake.\nExperience in creating ER diagrams, designing star/snowflake schemas, and building cost-effective data pipelines.\nAbility to turn business goals into technical solutions.\nComfortable working with cloud platforms like Azure, AWS, or Google Cloud.\nStrong understanding of data security and governance.\nExcellent communication and teamwork skills.\nBonus Skills (Preferred, not mandatory):\nTools: Erwin, Azure Synapse, Azure Databricks, Azure DevOps, Power BI, Spark, Python, R.\nExperience with Azure AI/ML Services, Event Hub, Stream Analytics, and scripting tools like Ansible.\nUnderstanding of machine learning, CI/CD, container tools like Docker/Kubernetes.\nRelevant certifications such as AWS Certified Solutions Architect or IBM Certified Data Architect.","snowflake schema, data pipelines, Event Hub, Azure AI ML Services, R, Stream Analytics, Hadoop, Power Bi, PostgreSQL, SQL Server, Azure Databricks, Data Modeling, Azure Synapse, Docker, Ansible, Er Diagrams, Azure Data Lake, Spark, Oracle, Azure, Python, Kubernetes, Azure DevOps"
Data Architect (Azure & Snowflake),CES,3-5 Years,,"Chennai, India",Login to check your skill match score,"CES has 26+ years of experience in delivering Software Product Development, Quality Engineering, and Digital Transformation Consulting Services to Global SMEs & Large Enterprises. CES has been delivering services to some of the leading Fortune 500 Companies including Automotive, AgTech, Bio Science, EdTech, FinTech, Manufacturing, Online Retailers, and Investment Banks. These are long-term relationships of more than 10 years and are nurtured by not only our commitment to timely delivery of quality services but also due to our investments and innovations in their technology roadmap. As an organization, we are in an exponential growth phase with a consistent focus on continuous improvement, process-oriented culture, and a true partnership mindset with our customers. We are looking for the right qualified and committed individuals to play an exceptional role as well as to support our accelerated growth.\nYou can learn more about us at: http://www.cesltd.com/\nJob Description\nExperience with Azure Synapse Analytics: Hands-on experience in designing, developing, and deploying solutions using Azure Synapse Analytics, including familiarity with its various components such as SQL pools, Spark pools, and Integration Runtimes.\nExpertise in Azure Data Lake Storage: In-depth understanding of Azure Data Lake Storage, including its architecture, features, and best practices for managing a large-scale Data Lake or Lakehouse in an Azure environment.\nExperience with AI Tools: Experience with AI Tools and LLMs (e.g. GitHub Copilot, Copilot, ChatGPT) in automating many of the responsibilities outlined for this role.\nKnowledge of Avro and Parquet: Experience working with Avro and Parquet file formats, including data serialization, compression techniques, and schema evolution. Understanding of their advantages and use cases in a big data environment.\nHealthcare: Prior experience working with data in a healthcare or clinical laboratory environment and a strong understanding of PHI, GDPR & HIPPA/HITRUST is highly desirable.\nCertifications: Relevant certifications such as Azure Data Engineer Associate or Azure Synapse Analytics Developer Associate are highly desirable.\nEssential Functions\nData Integration and ELT Development: Design, develop, and maintain data pipelines for ingestion, transformation, and loading of data into Azure Synapse Analytics. This includes understanding functional and non-functional requirements, performing source data analysis, data profiling, and implementing efficient ELT processes.\nAzure Synapse Development: Work with Azure Synapse Analytics to build and optimize data models, SQL queries, stored procedures, and other artifacts necessary for data processing and analysis.\nData Lake File Handling: Understand the characteristics of various file formats, optimizing data storage, and implementing efficient data reading and writing mechanisms for incremental updates within Azure Synapse Analytics.\nData Governance and Security: Ensure compliance with data governance policies and implement security measures to protect sensitive data stored in Azure. This involves encryption, masking, and access control mechanisms.\nPerformance Optimization: Continuously optimize data pipelines and storage configurations to improve performance, scalability, and reliability. This includes identifying bottlenecks, query tuning, and leveraging Azure Synapse Analytics features for parallel processing.\nMonitoring and Troubleshooting: Implement monitoring solutions to track data pipeline performance, data quality, and system health. Troubleshoot issues related to data ingestion, transformation, or storage, and provide timely resolutions.\nSkills Needed to be Successful\nRelational Database Experience: Proficiency with one or more of the following database platforms; e.g. Oracle, Microsoft SQL Server, PostgreSQL, MySQL/MariaDB\nProficiency in SQL: Strong SQL skills, including experience with complex SQL queries, stored procedures, and performance optimization techniques. Familiarity with T-SQL for Azure Synapse Analytics is a plus.\nELT and Data Integration Skills: Proven experience in building ELT pipelines and data integration solutions using tools like Azure Data Factory, Oracle Golden Gate, or similar platforms.\nAbility to handle a variety of legacy data sources and file formats efficiently.\nData Modeling and Warehousing Concepts: Familiarity with dimensional modeling, star schemas, and data warehousing principles. Experience in designing and implementing data models for analytical workloads.\nAnalytical and Problem-Solving Abilities: Strong analytical skills with the ability to understand complex data requirements, troubleshoot technical issues, and propose effective solutions to meet business needs.\nCommunication and Collaboration: Excellent communication skills with the ability to collaborate effectively with cross-functional teams, including Data Scientists, Reporting Analysts, and DevOps professionals.","Azure Data Lake Storage, Relational Database Experience, Data Modeling and Warehousing Concepts, AI Tools, ELT and Data Integration, Parquet, Sql, Avro, Azure Synapse Analytics"
Azure Data Architect (Contract),TalenXis,5-7 Years,,India,Login to check your skill match score,", . .\n- - .\n.\n% ()\nResponsibilities:\nArchitect and model Business Intelligence (BI) and Analytics solutions to support data-driven decision-making.\nCollaborate with stakeholders to understand business requirements and translate them into technical specifications for BI and Analytics solutions.\nDevelop and maintain data models, data integration processes, and data warehousing solutions to support BI and Analytics initiatives.\nImplement and manage ETL (Extract, Transform, Load) processes to ensure accurate and timely data availability for BI and Analytics.\nDesign and implement effective cloud data models in Azure Data Lake & Azure Databricks to store and retrieve company data.\nEnsure data quality and integrity across data models\nDevelop and enforce development standards.\nMust Have Requirements:\nExtensive experience in architecting and modeling BI and Analytics solutions.\nProficiency in designing and implementing ETL processes and data integration workflows.\nStrong background in developing data models and data warehousing solutions to support BI and Analytics.\nHands-on expertise with Databricks for data engineering and SQL warehouse.\nIn-depth understanding of data warehouse's structure principles.\nExpertise in SQL and database management systems (e.g., Oracle, SQL Server, PostgreSQL).\nKnowledge of data visualization tools (e.g., Qlik, Power BI).\nFamiliarity with data warehousing solutions (e.g., Snowflake, Redshift).\nPreferred Qualifications:\nExperience with big data technologies (e.g., Hadoop, Spark).\nCertification in data management or data architecture (e.g., CDMP, TOGAF)","Data warehousing solutions, Analytics Solutions, Data integration workflows, Azure Data Lake, Azure Databricks, Sql"
Consultant (Data Architect),Improzo,7-9 Years,,"Pune, India",Login to check your skill match score,"About Improzo\n\nAt Improzo (Improve + Zoe; meaning Life in Greek), we believe in improving life by empowering our customers. Founded by seasoned Industry leaders, we are laser focused on delivering quality-led commercial analytical solutions to our clients. Our dedicated team of experts in commercial data, technology, and operations has been evolving and learning together since our inception. Here, you won't find yourself confined to a cubicle; instead, you'll be navigating open waters, collaborating with brilliant minds to shape the future. You will work with leading Life Sciences clients, seasoned leaders and carefully chosen peers like you!\n\nPeople are at the heart of our success, so we have defined our CARE values framework with a lot of effort, and we use it as our guiding light in everything we do. We CARE!\n\nCustomer-Centric: Client success is our success. Prioritize customer needs and outcomes in every action.\nAdaptive: Agile and Innovative, with a growth mindset. Pursue bold and disruptive avenues that push the boundaries of possibilities.\nRespect: Deep respect for our clients & colleagues. Foster a culture of collaboration and act with honesty, transparency, and ethical responsibility.\nExecution: Laser focused on quality-led execution; we deliver! Strive for the highest quality in our services, solutions, and customer experiences.\n\nAbout The Role\n\nIntroduction: We are seeking an experienced and highly skilled Data Architect to lead a strategic project focused on Pharma Commercial Data Management Operations. This role demands a professional with 7-9 years of experience in data architecture, data management, ETL, data transformation, and governance, with an emphasis on providing scalable and secure data solutions for the pharmaceutical sector.\n\nThe ideal candidate will bring a deep understanding of data architecture principles, experience with cloud platforms such as Snowflake, and a solid background in driving commercial data management projects. If you're passionate about leading impactful data initiatives, optimizing data workflows, and supporting the pharmaceutical industry's data needs, we invite you to apply.\n\nResponsibilities\n\nKey Responsibilities:\n\nLead Data Architecture and Strategy:\nDesign, develop, and implement the overall data architecture for commercial data management operations within the pharmaceutical business.\nLead the design and operations of scalable and secure data systems that meet the specific needs of the pharma commercial team, including marketing, sales, and operations.\nDefine and implement best practices for data architecture, ensuring alignment with business goals and technical requirements.\nDevelop a strategic data roadmap for efficient data management and integration across multiple platforms and systems.\nData Integration, ETL & Transformation:\nOversee the ETL (Extract, Transform, Load) processes to ensure seamless integration and transformation of data from multiple sources, including commercial, sales, marketing, and regulatory databases.\nCollaborate with data engineers and developers to design efficient and automated data pipelines for processing large volumes of data.\nLead efforts to optimize data workflows and improve data transformation processes to enhance reporting and analytics capabilities.\nData Governance & Quality Assurance:\nImplement and enforce data governance standards across the data management ecosystem, ensuring the consistency, accuracy, and integrity of commercial data.\nDevelop and maintain policies for data stewardship, data security, and compliance with industry regulations, such as HIPAA, GDPR, and other pharma-specific compliance requirements.\nWork closely with business stakeholders to ensure the proper definition of master data and reference data standards.\nCloud Platform Expertise (Snowflake (critical to have), AWS, Azure):\nLead the adoption and utilization of cloud-based data platforms, particularly Snowflake, to support data warehousing, analytics, and business intelligence needs.\nCollaborate with cloud infrastructure teams to ensure efficient management of data storage, compute resources, and performance optimization within cloud environments.\nStay up-to-date with the latest cloud technologies, such as Snowflake, AWS, Azure, or Google Cloud (optional)), and evaluate opportunities for incorporating them into data architectures.\nCollaboration with Cross-functional Teams:\nWork closely with business leaders in commercial operations, analytics, and IT teams to understand their data needs and provide strategic data solutions that enhance business operations.\nCollaborate with data scientists, analysts, and business intelligence teams to ensure data is available for reporting, analysis, and decision-making.\nFacilitate communication between IT, business stakeholders, and external vendors to ensure data architecture solutions align with business requirements.\nContinuous Improvement & Innovation:\nDrive continuous improvement efforts to optimize data pipelines, data storage, and analytics workflows.\nIdentify opportunities to improve data quality, streamline processes, and enhance the efficiency of data management operations.\nAdvocate for the adoption of new data management technologies, tools, and methodologies to improve data processing, security, and integration.\nLeadership and Mentorship:\nLead and mentor a team of data engineers, analysts, and other technical resources, fostering a collaborative and innovative work environment.\nProvide leadership in setting clear goals, performance metrics, and expectations for the team.\nOffer guidance on data architecture best practices, ensuring all team members are aligned with the organization's data strategy.\nRequired Qualifications\n\nBachelor's degree in Computer Science, Data Science, Information Systems, or a related field.\n7-9 years of experience in data architecture, data management, and data governance, with a proven track record of leading commercial data management operations projects.\nExtensive experience in data integration, ETL, and data transformation processes, including familiarity with tools like Informatica, Talend, or Apache NiFi.\nStrong expertise with cloud platforms, particularly Snowflake, AWS, Azure, or Google Cloud.\nStrong knowledge of data governance frameworks, including data security, privacy regulations, and compliance standards in the pharmaceutical industry (e.g., HIPAA, GDPR).\nHands-on experience in designing scalable and efficient data architecture solutions to support business intelligence, analytics, and reporting needs.\nProficient in SQL and other query languages, with a solid understanding of database management and optimization techniques.\nAbility to communicate technical concepts effectively to non-technical stakeholders and align data strategies with business goals.\n\nPreferred Qualifications\n\nExperience in the pharmaceutical or life sciences sector, particularly in commercial data management, sales, marketing, or operations.\nCertification or formal training in cloud platforms (e.g., Snowflake, AWS, Azure) or data management frameworks.\nFamiliarity with data science methodologies, machine learning, and advanced analytics tools.\nKnowledge of Agile methodologies for managing data projects.\n\nKey Skills\n\nData Architecture & Design\nCloud Platforms (Snowflake critical to have)\nData Governance & Quality Assurance\nETL & Data Transformation\nData Integration & Pipelines\nPharmaceutical Data Management (Preferred)\nSQL & Database Optimization\nLeadership & Mentorship\nBusiness & Technical Collaboration\n\nBenefits\n\nCompetitive salary and benefits package.\nOpportunity to work on cutting-edge tech projects, transforming the life sciences industry\nCollaborative and supportive work environment.\nOpportunities for professional development and growth.\n\nSkills: snowflake,database,data governance & quality assurance,data integration & pipelines,etl & data transformation,azure,business & technical collaboration,aws,data management,analytics,sql,sql & database optimization,cloud platforms (snowflake),leadership & mentorship,cloud platforms,data architecture & design,data","Leadership, Business Technical Collaboration, Mentorship, Data Architecture Design, Data Integration Pipelines, snowflake, Database Optimization, Sql, Data Governance, Quality Assurance, Azure, AWS, Data Transformation, Etl"
Enterprise Data Architect - Data & Analytics Delivery Practice Lead,"Aezion, Inc",10-12 Years,,"Bengaluru, India",Login to check your skill match score,"About the Role:\nWe are seeking a highly skilled and results-driven Data & Analytics Delivery Practice Lead to lead our end-to-end data engineering, Big Data, Streaming and business intelligence (BI) solutions. This is a key leadership role for someone who thrives on building scalable, cloud-agnostic data solutions and driving measurable impact through data analytics.\nYou will be responsible for managing a high-performing data team of 20+ members across onsite and offshore locations, primarily working on any Big Data, streaming Platforms Databricks, Snowflake, Redshift, Spark, Kafka, SQL Server, PostgreSQL on any Cloud ( AWS/Azure/GCP). Your expertise in Python, cloud infrastructure, and data architecture will be instrumental in creating intelligent and efficient data pipelines and products.\nIn addition to delivery excellence, you will serve as the primary customer contact for ongoing engagements and play an active role in pre-sales effortsparticipating in prospect calls and leading proposal creation with an exceptional success rate.\nKey Responsibilities:\nTeam Leadership and Development:\nLead, mentor, and develop a high-performing global team of 20+ data professionals across onshore and offshore locations.\nFoster a culture of accountability, innovation, and continuous learning within the data team.\nEstablish career development plans and performance metrics to drive individual and team growth that aligns with organizational KPIs.\nAlign team structure and skill sets with evolving business and technology needs.\nDelivery & Customer Focus:\nServe as the primary point of contact for customers, ensuring successful delivery of data and analytics solutions.\nTranslate complex customer requirements into scalable, high-impact data solutions.\nMaintain a strong focus on stakeholder engagement and satisfaction, ensuring delivery meets or exceeds expectations.\nDrive pre-sales activities, including client discovery calls and proposal development, maintaining close to 100% win rate.\nTechnology & Market Adaptation:\nArchitect and implement scalable, cloud-agnostic data solutions using technologies such as Big Data, streaming Platforms Databricks, Snowflake, Redshift, Spark, Kafka, SQL Server, PostgreSQL on any Cloud ( AWS/Azure/GCP)\nStay ahead of technology trends in data engineering, cloud computing, and big data platforms to ensure competitive offerings.\nEvaluate and adopt emerging tools and frameworks that enhance productivity, performance, and innovation.\nCross-Functional Collaboration with Global Teams:\nCollaborate closely with platform engineering, BI, and AI/ML teams to deliver integrated, end-to-end data solutions.\nAct as a bridge between technical and non-technical stakeholders, translating business needs into technical execution.\nPromote knowledge sharing and alignment across functional and geographical team boundaries.\nProcess Improvement and Reporting:\nDefine and continuously refine best practices in Technology frameworks and operational processes.\nDrive improvements in data quality, performance, and governance across the data lifecycle.\nTrack team utilization and proactively forecast upcoming bench time, ensuring maximum billing efficiency and planning for resource allocation.\nAchieve and maintain 100% billing utilization by aligning team capacity with project demands.\nGenerate and share regular utilization and billing reports with the leadership team to inform resourcing and financial strategies.\nLead initiatives to automate and streamline data processes, reducing manual effort and increasing delivery speed.\nRequired Skills and Qualifications:\n10+ years of experience in Data Engineering and Analytics, with at least 35 years in a leadership role.\nProven experience managing global teams, both onsite and offshore.\nStrong hands-on technical background in Big Data, streaming Platforms, Databricks, Snowflake, Redshift, Spark, Kafka, SQL Server, PostgreSQL on any Cloud ( AWS/Azure/GCP), and modern BI tools.\nDemonstrated ability to design and scale data pipelines and architectures in large, complex environments.\nExcellent soft skills including leadership, client communication, and stakeholder management.\nA successful track record of pre-sales engagement and proposal development with high closure rates.\nExperience working across cross-functional teams including platform and AI/ML engineering.\nStrong problem-solving mindset with the ability to think strategically and execute tactically.\nPreferred Qualifications:\nMaster's or Bachelor's degree in Computer Science, Data Science, Engineering, or related field.\nExperience working in consulting or services-based environments.\nFamiliarity with governance, data security, and compliance best practices.\nExposure to modern data stack tools like dbt, Airflow, and Kafka.","Big Data streaming Platforms, snowflake, PostgreSQL, Data Architecture, SQL Server, Kafka, Redshift, Gcp, Spark, Databricks, Azure, Python, AWS"
Senior Data Architect,Axtria - Ingenious Insights,Fresher,,"Bengaluru, India",Login to check your skill match score,"POSITION: Data Architect (Individual contributor)\nData Architect + Gen AI\nLOCATION: Noida, Gurugram, Pune, Bangalore\n60% through out in academics\nJOB OBJECTIVE: To leverage expertise in data architecture and management to design, implement, and\noptimize a robust data warehousing platform for the pharmaceutical industry. The goal is to ensure\nseamless integration of diverse data sources, maintain high standards of data quality and governance,\nand enable advanced analytics through the definition and management of semantic and common data\nlayers. Utilizing Axtria's product and generative AI technologies, the aim is to accelerate business\ninsights and support regulatory compliance, ultimately enhancing decision-making and operational\nefficiency.\nKey Responsibilities:\nStrong AIML, Gen AI exp\nData Modeling: Design logical and physical data models to ensure efficient data storage and\nretrieval.\nETL Processes: Develop and optimize ETL processes to accurately and efficiently move data\nfrom various sources into the data warehouse.\nInfrastructure Design: Plan and implement the technical infrastructure, including hardware,\nsoftware, and network components.\nData Governance: Ensure compliance with regulatory standards and implement data\ngovernance policies to maintain data quality and security.\nPerformance Optimization: Continuously monitor and improve the performance of the data\nwarehouse to handle large volumes of data and complex queries.\nSemantic Layer Definition: Define and manage the semantic layer architecture and technology\nstack to manage the lifecycle of semantic constructs including consumption into downstream\nsystems.\nCommon Data Layer Management: Integrate data from multiple sources into a centralized\nrepository, ensuring consistency and accessibility.\nDeep expertise in architecting enterprise grade software systems that are performant,\nscalable, resilient and manageable. Architecting GenAI based systems is an added plus.\nAdvanced Analytics: Enable advanced analytics and machine learning to identify patterns in\ngenomic data, optimize clinical trials, and personalize medication.\nGenerative AI: Should have worked with production ready usecase for GenAI based data and\nStakeholder Engagement: Work closely with business stakeholders to understand their data\nneeds and translate them into technical solutions.\nCross-Functional Collaboration: Collaborate with IT, data scientists, and business analysts to\nensure the data warehouse supports various analytical and operational needs.\nQualifications:\nProven experience in data architecture and data warehousing, preferably in the\npharmaceutical industry.\nStrong knowledge of data modeling, ETL processes, and infrastructure design.\nExperience with data governance and regulatory compliance in the life sciences sector.\nExcellent analytical and problem-solving skills.\nStrong communication and collaboration skills.\nPreferred Skills:\nFamiliarity with advanced analytics and machine learning techniques.\nExperience in managing semantic and common data layers.\nKnowledge of FDA guidelines, HIPAA regulations, and other relevant regulatory standards.\nExperience with generative AI technologies and their application in data warehousing\nABOUT AXTRIA\nAxtria (www.axtria.com) is high growth advanced analytics and business information Management\nCompany based out of New Jersey with locations in AZ, GA and VA in USA and Gurgaon in India. We\nhave been named as one of the fastest growing companies in the US by Inc. 5000 in 2014.\nOur broad portfolio of services and solutions help our clients improve their sales, marketing, supply\nchain and distribution planning and operations in various industries such as Pharma, Retail, Banking and\nTechnology. We blend analytics, technology and consulting to help customers gain deep insights from\ntheir customer data, create strategic advantage and drive profitable growth.\nThe leadership team at Axtria brings deep industry experience, expertise in sales, marketing and risk\nmanagement as well as a passion for building cutting-edge analytics and technology solutions.\nOur global team is committed to delivering high quality, high-impact solutions for our clients and to\nbuilding a world-class firm with enduring value. Our unique team combines real-world business\nknowledge, a depth of analytical skill and experience with the latest technologies. Those who excel at\nAxtria share a number of common qualities. They are smart, humble and have the analytical toolkit to\nput their intelligence to work. They are passionate about data and analytics & seek to constantly learn.","Performance Optimization, Generative AI, ETL Processes, Common Data Layer Management, Infrastructure Design, Machine Learning, Data Modeling, Advanced Analytics, Data Architecture, Data Warehousing, Data Governance"
Senior Data Architect,Tredence Inc.,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"Primary Roles and Responsibilities:\nWorking experience in Snowflake; use of Snow SQL CLI, Snow Pipe creation of custom functions and Snowflake stored producers, schema modelling, performance tuning etc.\nExpertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts\nExtensive experience in DBT CLI, DBT Cloud, GitHub version control and repository knowledge, and DBT scripting to design & develop SQL processes to perform complex ELT processes and data pipeline build.\nAbility to independently envision and develop innovative ETL and reporting solutions and execute them through to completion.\nTriage issues to find gaps in existing pipelines and fix the issues\nAnalyze the data quality, align the technical design to data governance, and address all the required non-business but operational requirements during the design and build of data pipelines\nDevelop and maintain data pipelines using DBT\nProvide advice, guidance, and best practices around Snowflake\nProvide guidance on moving data across different environments in Snowflake\nCreate relevant documentation around database objects\nTroubleshoot production support issues post-deployment and come up with solutions as required\nGood Understanding and knowledge of CI/CD process and GitHub->DBT-> Snowflake integrations.\nAdvance SQL knowledge and hands-on experience in complex query writing using Analytical functions, Troubleshooting, problem-solving, and performance tuning of SQL queries accessing data warehouse as well as Strong knowledge of stored procedures.\nExperience in Snowflake advanced concepts such as resource monitors, virtual warehouse sizing, query performance tuning, zero-copy clone, time travel and understanding how to use these features\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussions with client architect and team members\nGood experience in developing scripts for data auditing and automating various database platform manual activities.\nUnderstanding of the full software lifecycle and how development teams should work with DevOps to create more software faster.\nExcellent communication, working in Agile Methodology/Scrum\nSkills and Qualifications:\nBachelor's and/or master's degree in computer science or equivalent experience.\nMust have total 13+ yrs. of IT experience and 3+ years experience in data Integration, ETL/ETL development, and database design or Datawarehouse design\nDeep understanding of Star and Snowflake dimensional modelling.\nStrong knowledge of Data Management principles\nWorking experience in Snowflake; use of Snow SQL CLI, schema modelling, performance tuning etc.\nDevelop and maintain data pipelines using DBT\nExperience with writing complex SQL queries, especially dynamic SQL\nExpertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts\nExperience with performance tuning and optimization of SQL queries\nExperience of working with Retail Data\nExperience with data security and role-based access controls\nComfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects\nShould have experience working in Agile methodology\nStrong verbal and written communication skills.\nStrong analytical and problem-solving skills with high attention to detail.","Snow SQL CLI, snowflake, dbt, Github, Data Modelling, Agile Methodology, Sql, Performance Tuning, Etl"
Solution / Data Architect (Databricks),Koantek,9-11 Years,,"Mumbai, India",Login to check your skill match score,"Location: Mumbai\nWork mode: Hybrid\nMust have skills : Databricks, (Hands on Python, SQL,Pyspark with any cloud platform)\nJob Summary:\nThe Databricks AWS/Azure/GCP Architect at Koantek builds secure, highly scalable big data solutions to\nachieve tangible, data-driven outcomes all the while keeping simplicity and operational effectiveness in\nmind. This role collaborates with teammates, product teams, and cross-functional project teams to lead\nthe adoption and integration of the Databricks Lakehouse Platform into the enterprise ecosystem and\nAWS/Azure/GCP architecture. This role is responsible for implementing securely architected big data\nsolutions that are operationally reliable, performant, and deliver on strategic initiatives.\nRequirements:\nExpert-level knowledge of data frameworks, data lakes and open-source projects such as Apache\nSpark, MLflow, and Delta Lake\nExpert-level hands-on coding experience in Spark/Scala,Python or Pyspar\nIn depth understanding of Spark Architecture including Spark Core, Spark SQL, Data Frames,\nSpark Streaming, RDD caching, Spark MLib\nIoT/event-driven/microservices in the cloud- Experience with private and public cloud\narchitectures, pros/cons, and migration considerations.\nExtensive hands-on experience implementing data migration and data processing using\nAWS/Azure/GCP services\n9+ years in consulting experience with minimum 7+ Years of experience in data engineering,\ndata platform and analytics,\nProjects delivered with hands-on experience in development on databricks\nknowledge of any one cloud platform (AWS or Azure or GCP)\nDeep experience with distributed computing with spark with knowledge of spark runtime\ninternals\nFamiliarity with CI/CD for production deployments\nFamiliarity with optimization for performance and scalabilityCompleted data engineering\nprofessional certification and required classes","Spark MLib, MLflow, Data Frames, CI CD, Delta Lake, RDD caching, Pyspark, Spark SQL, Microservices, Iot, Sql, Spark Core, Spark Streaming, Gcp, Databricks, Azure, Python, AWS"
Principal Digital Architect-Data Architect,Caterpillar Inc.,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Career Area\n\nTechnology, Digital and Data\n\nJob Description\n\nYour Work Shapes the World at Caterpillar Inc.\n\nWhen you join Caterpillar, you're joining a global team who cares not just about the work we do but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.\n\nJob Purpose\n\nA Principal Digital Architect drives solutions with the core digital platform that underpins many applications that supports Cat Digital's key business domains of eCommerce, advanced condition monitoring, lead generation, service support and equipment management. The solutions will solve big data problems with over 1.4 million connected assets worldwide, advanced analytics, AI capabilities, global marketing and much more. This role is a key leader and accountable to work across the organization for both business and technical alignment to drive the required business outcomes.\n\nJob Duties\n\nKey areas of accountability include leading the development of Architecture Solutions for strategic Cat Digital projects and programs; developing and maintaining global technology roadmaps and application evolution plans; leading in the evaluation of new technology, information or integration standards; and developing digital strategy for a specific technical or business domain.\n\nResponsibilities Include One Or More Of The Following\n\nProvide oversight for architecture assessment and design for infrastructure, information or integration domains that provide core capabilities for the enterprise.\nLead Architecture design of end to end enterprise integrated systems that serves multiple business functions.\nLead the design and implementation of enterprise data model and metadata structures for complex projects.\nInitiate and deliver technology evaluation and recommendations.\nDevelop and maintain current, planned and future state architecture blueprints.\nLead in the identification and analysis of enterprise business drivers and requirements that drive the future state architecture.\n\nBasic Qualifications\n\nPosition requires a four-year degree from an accredited college or university in computer science, information technology, or related field or equivalent work experience.\n15 or more years of a progressive career in distributed system software engineering and architecture\nStrong demonstratable experience delivering product and/or enterprise architecture for enterprise scale solutions in public cloud or hybrid eco-systems\nAt least 5 years working experience in Cloud Technologies such as AWS & Microsoft Azure\nMust have excellent communication skills and be able to deal with sensitive issues, mentor and coach and/or persuade others on new technologies, new applications, or potential solutions.\n\nTop Candidates Will Have\n\nUnderstand the data platform and build new data solutions on the existing data platform. Impact analysis needs to be performed so as not to have unknown impact in other data solutions build on the platform.\n\nUnderstand the current data landscape and build new solutions on top of existing solution.\nTrouble shoots and finds solutions for technical and functional issues identified in the program/project.\nEvaluate, analyse, document and communicate business requirements to stakeholders.\nRun Architecture meetings/discussions and document the solutions on confluence. Complete the solutions and have engineering handover.\nSupport the engineering team during the entire cycle of the build and deploy phases.\nReport on common sources of technical, functional issues and/or questions and make recommendations to architecture team for long term solution.\nOwn and develop relationship with partners (customers, dealers, Technical Product Management, Architect teams), working with them to optimize and enhance the data products.\nProvide guidance on the technical solutions and guidance on new product like optimal database recommendations like dynamo vs Postgre, AWS options like Kinesis and Event bridge.\nOwn the solutions on the Data lake (Snowflake), The solutions should be performant, secure and Cost Optimal.\nOwn some of the data domains in the Data Platform i.e. Any solution on the data domain should be either worked upon or reviewed by the architect.\nProvide ROM (rough order of magnitude) for the solutions and data products.\nBased on understanding of data domains and Business requirements create reusable data products which can be used across applications and teams.\nCreate/Review HLA and TA documentation with reference to a business requirement.\nImprove architecture by tracking emerging technologies and evaluating their applicability to business goals and operational requirements.\nIdentify and solution for business critical business rules for improving the data quality in the platform.\n\nSkills\n\nMust Have:\n\nAWS (EMR, Glue, S3, Fargate, SNS, SQS, Kinesis, AWS EventBridge, RDS, DynamoDB) , Snowflake, SQL, Python, ER Modelling.\n\nGood To Have\n\nMicroservices and API knowledge\n\nPosting Dates\n\nMay 19, 2025 - May 25, 2025\n\nCaterpillar is an Equal Opportunity Employer.\n\nNot ready to apply Join our Talent Community.","AWS EventBridge, Fargate, snowflake, Glue, API knowledge, ER Modelling, S3, RDS, Dynamodb, Emr, Sql, Microservices, Kinesis, Sqs, Sns, Python, AWS"
Senior GCP Data Architect,Miracle Software Systems India Private Limited,Fresher,,India,IT/Computers - Software,"Responsibilities\nArchitect and deliver end-to-end data solutions on Google Cloud Platform, aligning with enterprise data strategy and business objectives.\nShould possess strong expertise in core GCP services, including integration, orchestration, and analytics tools.\nCollaborate directly with US-based stakeholders to gather requirements, define architecture, and ensure successful delivery.\nLead and manage offshore development teams, providing technical direction and ensuring adherence to best practices.\nDrive data governance, security, and compliance throughout the data lifecycle in cloud-based environments.\nConduct performance tuning, optimization, and troubleshooting complex data workflows and systems.\nCommunicate complex technical concepts to cross-functional teams, executives, and client stakeholders.\nContinuously evaluate and adopt emerging GCP capabilities to enhance architectural performance and innovation.\nDevelop scalable and maintainable cloud-native data pipelines and solutions tailored to business needs.\nAdditional experience with other cloud platforms, such as AWS or Azure, is advantageous.","Looker, BigQuery, DataFlow"
Data Architect,Innova solutinos,14-20 Years,,"Hyderabad, Chennai",Information Technology,"Responsibilities:\nDesign and implement enterprise data models, ensuring data integrity, consistency, and scalability.\nAnalyse business needs and translate them into technical requirements for data storage, processing, and access.\nIn-memory Cache: Optimizes query performance by storing frequently accessed data in memory.\nQuery Engine: Processes and executes complex data queries efficiently.\nBusiness Rules Engine (BRE): Enforces data access control and compliance with business rules.\nSelect and implement appropriate data management technologies, including databases, data warehouses.\nCollaborate with data engineers, developers, and analysts to ensure seamless integration of data across various systems.\nMonitor and optimize data infrastructure performance, identifying and resolving bottlenecks.\nStay up-to-date on emerging data technologies and trends, recommending and implementing solutions.\nDocument data architecture and processes for clear communication and knowledge sharing, including the integration.\nQualifications:\nProven experience in designing and implementing enterprise data models.\nExpertise in SQL and relational databases (e.g., Oracle, MySQL, PostgreSQL).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) is mandatory.\nWorking experience with ETL tools and data ingestion leveraging any real-time solutions (e.g., Kafka, streaming) is required\nStrong understanding of data warehousing concepts and technologies.\nFamiliarity with data governance principles and best practices.\nExcellent communication, collaboration, and problem-solving skills.\nAbility to work independently and as part of a team.\nStrong analytical and critical thinking skills.\nExperience with data visualization & UI Development is a plus.\nBachelors degree in computer science, Information Technology, or a related fiel\nRole:DBA / Data warehousing - Other\nIndustry Type:IT Services & Consulting\nDepartment:Engineering - Software & QA\nEmployment Type:Full Time, Permanent\nRole Category:DBA / Data warehousing\nEducation\nUG:Graduation Not Required","Data Modeling, Data Architecture, Etl"
Data Architect,Citiustech Healthcare Technology Private Limited,5-10 Years,,"Mumbai City, Bengaluru, Mumbai","Information Technology, Information Services","We are looking for a data architect with below skillset\nHands on skills withImage data (preferably DICOM) parsing\nArchitect and implement data warehousing solutions using AWS (Redshift, S3, Lambda)\nDevelop and maintain ETL pipelines using Informatica PowerCenter or AWS Glue\nCollaborate with cross-functional teams to identify and prioritize data requirements\nAnalyze complex data sets using SQL, Python, and Java and or/Apex languageto inform business decisions\nImplement data visualization tools (Tableau, Power BI) for stakeholder reporting\nEnsure data integrity, security, and compliance with HIPAA, GDPR, and CCPA\nRequirements:> 5 years of experience in data management with focus in image metadata/bigdata\nStrong expertise in:\nPACS& RIS, SYNAPSE/XNAT, systems\nAWS (Redshift, S3, Lambda, Glue)\nData governance, quality, and security\nETL pipelines (Informatica PowerCenter or AWS Glue)\nData warehousing and visualization","Aws, Etl"
Data Architect - Precision Medicine Team,Amgen Inc,6-11 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities:\nArchitect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nSupport development planning by breaking down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation of architectural direction, patterns, and standards\nPresent and train engineers and cross-team collaborators on architecture strategy and patterns\nCollaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDevelop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nProvide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nServe as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.\nCollaborate with stakeholders to define data requirements, architecture specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 6 to 8 years of experience in data management and data solution architecture\nBachelors degree with 8 to 10 years of experience in in data management and data solution architecture\nDiploma and 10 to 12 years of experience in in data management and data solution architecture\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 7 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications(please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.","healthcare data, analytical, Laboratory, Troubleshooting, Data Warehouse, Etl Process"
Data Architect,Grid Dynamics,6-8 Years,,Pune,Information Technology,"Responsibilities:\nTrusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.\nPre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.\nTechnology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing\nassets and solutions strategy across multiple industries.\nEngineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.\nBusiness Development & Partnership:\nManage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.\nRequirements:\nExtensive practical experience in Big Data engineering, data governance, and cloud data\nplatforms.\nStrong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.\nStrong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.\nDeep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.\nExperience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.\nManaged key client relations worldwide and advised global technology and business leaders on innovation and data strategy.\nExperience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,Grid Dynamics,6-8 Years,,Bengaluru,Information Technology,"Responsibilities:\nTrusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.\nPre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.\nTechnology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing\nassets and solutions strategy across multiple industries.\nEngineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.\nBusiness Development & Partnership:\nManage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.\nRequirements:\nExtensive practical experience in Big Data engineering, data governance, and cloud data\nplatforms.\nStrong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.\nStrong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.\nDeep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.\nExperience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.\nManaged key client relations worldwide and advised global technology and business leaders on innovation and data strategy.\nExperience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,Grid Dynamics,6-8 Years,,Chennai,Information Technology,"Responsibilities:\nTrusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.\nPre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.\nTechnology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing\nassets and solutions strategy across multiple industries.\nEngineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.\nBusiness Development & Partnership:\nManage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.\nRequirements:\nExtensive practical experience in Big Data engineering, data governance, and cloud data\nplatforms.\nStrong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.\nStrong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.\nDeep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.\nExperience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.\nManaged key client relations worldwide and advised global technology and business leaders on innovation and data strategy.\nExperience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,INNOVA SOLUTIONS,14-20 Years,,"Hyderabad, Chennai",IT Management,"Data Architect:\nDesign and implement enterprise data models, ensuring data integrity, consistency, and scalability\nResponsibilities:\nDesign and implement enterprise data models, ensuring data integrity, consistency, and scalability.\nAnalyse business needs and translate them into technical requirements for data storage, processing, and access.\nIn-memory Cache: Optimizes query performance by storing frequently accessed data in memory.\nQuery Engine: Processes and executes complex data queries efficiently.\nBusiness Rules Engine (BRE): Enforces data access control and compliance with business rules.\nSelect and implement appropriate data management technologies, including databases, data warehouses.\nCollaborate with data engineers, developers, and analysts to ensure seamless integration of data across various systems.\nMonitor and optimize data infrastructure performance, identifying and resolving bottlenecks.\nStay up-to-date on emerging data technologies and trends, recommending and implementing solutions.\nDocument data architecture and processes for clear communication and knowledge sharing, including the integration.\nQualifications:\nProven experience in designing and implementing enterprise data models.\nExpertise in SQL and relational databases (e.g., Oracle, MySQL, PostgreSQL).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) is mandatory.\nWorking experience with ETL tools and data ingestion leveraging any real-time solutions (e.g., Kafka, streaming) is required\nStrong understanding of data warehousing concepts and technologies.\nFamiliarity with data governance principles and best practices.\nExcellent communication, collaboration, and problem-solving skills.\nAbility to work independently and as part of a team.\nStrong analytical and critical thinking skills.\nExperience with data visualization & UI Development is a plus.\nBachelors degree in computer science, Information Technology, or a related fiel","Data Modeling, Mysql, Data Architecture, Etl, Oracle, Aws"
Data Architect,RARR Technologies,10-18 Years,,"Hyderabad, Noida",Software,"Develop and Implement, Strong indata modellingand pipeline design Experience with metadata driven frameworks and governance practices Strong analytical skills to identify and reduce redundancies Knowledge of Snowflake and Medallion Architecture Objective\nOptimize Data Pipeline Performance and Reliability Expertise in data pipeline optimization and performance tuning Experience with Indexing and efficient orchestration techniques Ability to identify and implement cost-saving measures Knowledge of monitoring tools and processes Objective\nEnhance data modelling and Reusability Strong Communication and training skills Experience in data modeling and reusable asset creation Able to identify and train Subject matter experts Proficiency in gathering and analyzing Stakeholder Feedback Objective\nStrengthen Devops Practices and Documentation Knowledge of version control and release processes Experience of DevOps process and CI/CD pipelines Ability to establish and maintain data asset frameworks Strong Documentation skills Objective\nLead and Develop the Data Engineering Team Leadership and team management skills Experience in conducting performance reviews and skill development plans Ability to establish and lead a center of Excellence(CoE) Proficiency in using tasking and estimation tools like Jira and DevOps\nData Modeler, Data Architect, Snowfake","snowflake, Metadata Governance, Team Leadership, Devops, Data Modeling, Data Pipeline"
Data Architect,Whisk Software Private Limited,6-13 Years,,"Gurugram, Hyderabad, Pune",Software,"Develop architectural strategies for data modelling, design and implementation to meet stated requirements for metadata management, master data management, Data warehouses, ETL and ELT.\nAnalyzing business requirements, designing scalable/ robust data models, documenting conceptual, logical physical data model design, helping developers in development/ creating DB structures and supporting developers throughout the project life cycle.\nLead and Mentor Data Engineers: This role will be responsible for leading and developing a team of data engineers focused on the growth in the teams skills and ability to execute as a team using DevOps and Data Ops principles.\nInvestigate new technologies, data modelling methods and information management systems to determine which ones should be incorporated onto data architectures, and develop implementation timelines and milestones.\nRecognizes and resolves conflicts between models, ensuring that information and data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions).\nParticipates in the design of the information architecture: supports projects, reviews information elements including models, glossary, flows, data usage.\nProvides guidance to the team in achieving the project goals/milestones.\nWorks independently within broad guidelines and policies, with guidance in only the most complex situations.\nContribute as an expert to multiple delivery teams, defining best practices, building reusable design components, capability building, aligning industry trends and actively engaging with wider data communities.\nSkills and Experience\nGraduate or post graduate in Computer science/Electronics/Software engineering.\n6+ years of relevant experience in Data modelling for DW analytics applications (OLAP) / Database related technologies.\nExpert data modelling skills (i.e. conceptual, logical and physical model design, experience with Enterprise Data Warehouses and Data Marts).\nSolid understanding of cloud database technologies and services (eg..GCP, Redshift, Aurora, DynamoDB, etc)\nExperience in working with data governance, data quality, and data security teams.\nExperienced in knowledge-driven data processing techniques like data curation, representation, standardization, normalization and any other type of processing that prepares the data for integration, persistence, analysis, exchange/share and so forth.\nExperience in handling very large DBs and large data volumes\nStrong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, data replication/CDC, message-oriented data movement and upcoming data ingestion and integration technologies such as stream data integration and data virtualization.\nStrong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production.\nKnowledge with popular data discovery, analytics, and BI software tools like MicroStrategy, Tableau, Power BI and others for semantic-layer-based data discovery.\nAbility to lead and mentor teams for effective delivery\nCrisp and effective executive communication skills, including significant experience presenting cross-functionally and across all levels.","Computer Science, Information Management, metadata, Microstrategy, Analytics, virtualization, Data Quality, Olap, Sql, Python"
Data Architect,Grid Dynamics,6-8 Years,,Mumbai,Information Technology,"Responsibilities:\nTrusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.\nPre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.\nTechnology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing\nassets and solutions strategy across multiple industries.\nEngineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.\nBusiness Development & Partnership:\nManage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.\nRequirements:\nExtensive practical experience in Big Data engineering, data governance, and cloud data\nplatforms.\nStrong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.\nStrong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.\nDeep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.\nExperience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.\nManaged key client relations worldwide and advised global technology and business leaders on innovation and data strategy.\nExperience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,Siemens,8-12 Years,,Bengaluru,Manufacturing,"Job description\nAs a Data Architect, you are required to:\nDesign & develop technical solutions which combine disparate information to create meaningful insights for business, using Big-data architectures\nBuild and analyze large, structured and unstructured databases based on scalable cloud infrastructures\nDevelop prototypes and proof of concepts using multiple data-sources and big-data technologies\nProcess, manage, extract and cleanse data to apply Data Analytics in a meaningful way\nDesign and develop scalable end-to-end data pipelines for batch and stream processing\nRegularly scan the Data Analytics landscape to stay up to date with latest technologies, techniques, tools and methods in this field\nStay curious and enthusiastic about using related technologies to solve problems and enthuse others to see the benefit in business domain\nQualification:\nBachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Engineering / Analytics is desirable.\nExperience level:\nMinimum 8 years in software development with at least 2 - 3 years hands-on experience in the area of Big-data / Data Engineering.\nDesired Knowledge & Experience:\nData Engineer -Big Data Developer\nSpark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\nKnowing Spark internals: Catalyst/Tungsten/Photon\nDatabricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\nIDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\nTest: pytest, Great Expectations\nCI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\nBig Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\nLanguages: Python/Functional Programming (FP)\nSQL: TSQL/Spark SQL/HiveQL\nStorage: Data Lake and Big Data Storage Design\nAdditionally it is helpful to know basics of:\nData Pipelines: ADF/Synapse Pipelines/Oozie/Airflow\nLanguages: Scala, Java\nNoSQL: Cosmos, Mongo, Cassandra\nCubes: SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\nSQL Server: TSQL, Stored Procedures\nHadoop: HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\nData Catalog: Azure Purview, Apache Atlas, Informatica\nBig Data Architect\nExpert: in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer\nMentor: mentors/educates Developers in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer\nArchitecture Styles: Lakehouse, Lambda, Kappa, Delta, Data Lake, Data Mesh, Data Fabric, Data Warehouses (e.g. Data Vault)\nApplication Architecture: Microservices, NoSql, Kubernetes, Cloud-native\nExperience: Many years of experience with all kinds of technology in the evolution of data platforms (Data Warehouse -> Hadoop -> Big Data -> Cloud -> Data Mesh)\nCertification: Architect certification (e.g. Siemens Certified Software Architect or iSAQB CPSA)\nRequired Soft-skills & Other Capabilities:\nExcellent communication skills, in order to explain your work to people who don't understand the mechanics behind data analysis\nGreat attention to detail and the ability to solve complex business problems\nDrive and the resilience to try new ideas, if the first ones don't work\nGood planning and organizational skills\nCollaborative approach to sharing ideas and finding solutions\nAbility to work independently and also in a global team environment.","Github Copilot, Git, Sql, Azure Devops, Java"
AWS Data Architect,RARR Technologies,10-15 Years,,Hyderabad,Software,"Design, develop, and implement scalable, secure, and high-performance data architectures on AWS.\nLead the architecture and development of large-scale data platforms utilizing AWS services such as S3, Redshift, RDS, Lambda, Glue, EMR, Athena, and others.\nDevelop and optimize data pipelines and workflows using Python and AWS services.\nDesign and implement data lakes and data warehouses for efficient storage and querying of large datasets.\nCollaborate with business stakeholders and cross-functional teams to gather requirements and translate them into technical solutions.\nEnsure high-quality data availability, integrity, and security across platforms.\nAutomate manual processes, build frameworks, and implement CI/CD pipelines for data workflows.\nMonitor and optimize the performance of data systems, databases, and processing pipelines.\nDevelop and maintain documentation for data models, processes, and infrastructure.\nProvide mentorship and leadership to junior developers and data engineers.\nStay up-to-date with emerging AWS technologies, Python libraries, and industry best practices","Lambda, Data Architecture, Data Warehousing, Python, Aws"
Data Architect - R&D Data Catalyst Team,Amgen Inc,4-6 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities\nArchitect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nSupport development planning by break ing down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new pattern\nCreate robust documentation of architectural direction, patterns, and standards\nPresent and train engineers and cross-team collaborators on architecture strategy and patterns\nCollaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.\nDesign robust data models , and processing layers, that support both analytical processing and operational reporting needs.\nDevelop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.\nensure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nProvide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nServe as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.\nCollaborate with stakeholders to define data requirements, architecture specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions\nBasic Qualifications and Experience\nMaster's degree with 4 to 6 years of experience in data management and data architecture O\nBachelor's degree with 6 to 8 years of experience in data management and data architectur\n\nFunctional\nSkills:\nMust-Have Skills\nMinimum of 3 years of hands-on experience with BI solutions (Preferably Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 7 years of hands-on experience building change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design , DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms ( AWS ), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional h ands - on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have\nSkills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role)\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHigh est degree of initiative and self-motivation\nStrong verbal and written communication skills , including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams , specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nability to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources","Optimization, Etl Process, Dax, aws, Power Query, Powerbi"
Master Data Management Data Architect,Amgen Inc,3-6 Years,,Hyderabad,Pharmaceutical,"Deliver outstanding self-service and automation experiences for our global workforce\nCreate ServiceNow catalog items, workflows, and cross-platform API integrations\nCreate and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.\nCreate and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.\nCreate and maintain data integrations between ServiceNow and other systems\nDevelop system integrations and process automation\nParticipate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives\nCollaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions\nDesign, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner\nParticipate in problem analysis, code review, and system design\nRemain current on new technology and apply innovation to improve functionality\nCollaborate closely with stakeholders and team members to configure, improve and maintain current applications\nWork directly with users to resolve support issues within product team responsibilities\nMonitor health, performance and usage of developed solutions\nBasic Qualifications:\nMaster's degree and 1 to 3 years of experience in computer science, IT, or related field OR\nBachelor's degree and 3 to 5 years of experience in computer science, IT, or related field OR\nDiploma and 7 to 9 years of experience in computer science, IT, or related fiel\nRequired Skills & Qualifications:\n6+ years of deep hands-on experiencewith ServiceNow administration and development in two or more productsITSM, ITBM, ITOM, GRC, HRSD, or Security Operations\nServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;\nStrong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform\nExperience creating and managing Scoped Applications\nWorkflow automation and integration development using REST, SOAP, or MID servers\nScripting skills in Python, Bash, or other programming languages\nWorking in an Agile (SAFe, Scrum, and Kanban) environment\nPreferred Qualifications:\nGood-to-Have\nSkills:\nExperience with other configuration management tools (e.g., Puppet, Chef).\nExperience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)\nExperience with Terraform & CloudFormation for AWS infrastructure automation\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP)\nExperience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)\nProfessional Certifications (preferred):\nService Now Certified System Administrator\nService Now Certified Application Developer\nService Now Certified Technical Architect\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced environment.","Rest Api, Servicenow, Itsm, Javascript, Agile, Automation"
Junior Data Architect,Terralogic Software Solutions,2-7 Years,,Bengaluru,Information Technology,"Junior Data ArchitectLocation :Bangalore\nOverview: Role:\nWe are hiring a Junior Data Architect to support solution design and data architecture tasks, with exposure to Azure cloud services. This role will support senior architects and collaborate with BI and data\nengineering teams to design scalable, secure, and cost-effective data solutions.\nMinimum Requirements:\n2+ years of experience in data engineering, BI, or solution design roles.\nWorking knowledge of Azure Data Services including Azure Data Factory, Azure SQL, and Power BI.\nUnderstanding of data modeling concepts (star schema, snowflake, normalized/denormalized models).\nFamiliarity with data governance and security concepts.\nGood communication and documentation skills.\nAbility to assist with architecture diagrams, metadata documentation, and best practice\nrecommendations.\nNice to Have:\nExposure to Synapse Analytics, Azure Data Lake, or Azure Purview.\nKnowledge of infrastructure components like VNets, Private Endpoints, and Managed Identity.\nWillingness to grow into a senior architect or cloud data strategist role.","data architecture tasks, Azure Data Services, Azure Data Factory, Power Bi Desktop, Azure Sql"
Master Data Management Data Architect,Amgen Inc,7-11 Years,,Hyderabad,Biotechnology,"Job description\nWhat you will do\nLet's do this. Let's change the world. In this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data deliver actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and driving data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills and provides administration support for Master Data Management (MDM) and Data Quality platform, including solution architecture, inbound/outbound data integration (ETL), Data Quality (DQ), and maintenance/tuning of match rules.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing\nCollaborate and communicate with MDM Developers, Data Architects, Product teams, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve complex data-related challenges\nAdhere to standard processes for coding, testing, and designing reusable code/component\nParticipate in sprint planning meetings and provide estimations on technical implementation\nAs a SME, work with the team on MDM related product installation, configuration, customization and optimization\nResponsible for the understanding, documentation, maintenance, and additional creation of master data related data-models (conceptual, logical, and physical) and database structures\nReview technical model specifications and participate in data quality testing\nCollaborate with Data Quality & Governance Analyst and Data Governance Organization to monitor and preserve the master data quality\nCreate and maintain system specific master data data-dictionaries for domains in scope\nArchitect MDM Solutions, including data modeling and data source integrations from proof-of-concept through development and delivery\nDevelop the architectural design for Master Data Management domain development, base object integration to other systems and general solutions as related to Master Data Management\nDevelop and deliver solutions individually or as part of a development team\nApproves code reviews and technical work\nMaintains compliance with change control, SDLC and development standards\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster's degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelor's degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience.\nPreferred Qualifications:\nExpertise in architecting and designing Master Data Management (MDM) solutions.\nPractical experience with AWS Cloud, Databricks, Apache Spark, workflow orchestration, and optimizing big data processing performance.\nFamiliarity with enterprise source systems and consumer systems for master and reference data, such as CRM, ERP, and Data Warehouse/Business Intelligence.\nAt least 2 to 3 years of experience as an MDM developer using Informatica MDM or Reltio MDM, along with strong proficiency in SQL.\nGood-to-Have\nSkills:\nExperience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.\nGood understanding of data modeling, data warehousing, and data integration concepts.\nExperience with development using Python, React JS, cloud data platforms.\nCertified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.","MDM developer, Reltio MDM, Informatica Mdm, Apache Spark, Etl Tools, Python"
Data Architect - Precision Medicine Team,Amgen Inc,6-11 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities:\nArchitect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nSupport development planning by breaking down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation of architectural direction, patterns, and standards\nPresent and train engineers and cross-team collaborators on architecture strategy and patterns\nCollaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDevelop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nProvide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nServe as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.\nCollaborate with stakeholders to define data requirements, architecture specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 6 to 8 years of experience in data management and data solution architecture\nBachelors degree with 8 to 10 years of experience in in data management and data solution architecture\nDiploma and 10 to 12 years of experience in in data management and data solution architecture\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 7 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications(please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.","healthcare data, analytical, Laboratory, Troubleshooting, Data Warehouse, Etl Process"
Azure Data Architect,RARR Technologies,7-12 Years,,Mumbai,Software,"A leading software consultancy company in Mumbai is seeking a highly skilled Azure Data Architect. The ideal candidate should have extensive experience in data architecture, with a strong focus on transforming legacy systems to Azure Graph DB. The role requires excellent communication skills, teamwork, and the ability to mentor and support other team members.\nResponsibilities:\nData Architecture Design:Design and implement robust data architecture solutions on Azure, ensuring scalability, performance, and security.\nLegacy System Transformation:Lead the transformation of legacy systems to modern Azure Graph DB, ensuring minimal disruption and maximum efficiency.\nData Migration:Plan and execute data migration strategies, ensuring data integrity and consistency throughout the process & remapping source to target model.\nAzure Data Services:Utilize Azure data services such as Azure Cosmos DB (Graph API) to build and manage graph database solutions.\nData Modeling:Develop and maintain graph data models to support business requirements and optimize data storage and retrieval.\nCollaboration:Work closely with stakeholders, including business analysts, developers, and operations teams, to ensure data solutions meet business needs.\nFramework Utilization:Prioritize the use of out-of-the-box framework features over custom solutions to enhance efficiency and maintainability.\nRequired Qualifications:\nMinimum 7 years of experience in data architecture and database management.\nSpecialist in Azure data services with proven experience in transforming legacy systems to Azure Graph DB.\nExtensive experience with data migration strategies and tools.\nProficient in data modeling and database design, specifically for graph databases.\nExperience with Azure Cosmos DB (Graph API) and other graph database technologies.\nExcellent communication skills and a strong team player.\nProven experience in mentoring and leading data architecture teams.\nTechnical Skills:\nAzure Cosmos DB (Graph API)\nData Modeling for Graph Databases\nData Migration Tools and Strategies\nScripting (PowerShell, Bash, Python)\nGood to Have Skills:\nKnowledge of data governance and security best practices\nKnowledge of Virtuoso Graph Database & SPARQL Protocol and RDF Query Language.","Powershell And Bash, Graph Db, Graph Database, Azure Cosmos DB, Data Migration, Azure, Python"
Data Architect - R&D Data Catalyst Team,Amgen Inc,4-6 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities\nArchitect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nSupport development planning by break ing down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new pattern\nCreate robust documentation of architectural direction, patterns, and standards\nPresent and train engineers and cross-team collaborators on architecture strategy and patterns\nCollaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.\nDesign robust data models , and processing layers, that support both analytical processing and operational reporting needs.\nDevelop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.\nensure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nProvide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nServe as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.\nCollaborate with stakeholders to define data requirements, architecture specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions\nBasic Qualifications and Experience\nMaster's degree with 4 to 6 years of experience in data management and data architecture O\nBachelor's degree with 6 to 8 years of experience in data management and data architectur\n\nFunctional\nSkills:\nMust-Have Skills\nMinimum of 3 years of hands-on experience with BI solutions (Preferably Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 7 years of hands-on experience building change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design , DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms ( AWS ), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional h ands - on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have\nSkills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role)\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHigh est degree of initiative and self-motivation\nStrong verbal and written communication skills , including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams , specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nability to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources","Optimization, Etl Process, Dax, aws, Power Query, Powerbi"
Master Data Management Data Architect,Amgen Inc,3-6 Years,,Hyderabad,Pharmaceutical,"Deliver outstanding self-service and automation experiences for our global workforce\nCreate ServiceNow catalog items, workflows, and cross-platform API integrations\nCreate and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.\nCreate and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.\nCreate and maintain data integrations between ServiceNow and other systems\nDevelop system integrations and process automation\nParticipate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives\nCollaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions\nDesign, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner\nParticipate in problem analysis, code review, and system design\nRemain current on new technology and apply innovation to improve functionality\nCollaborate closely with stakeholders and team members to configure, improve and maintain current applications\nWork directly with users to resolve support issues within product team responsibilities\nMonitor health, performance and usage of developed solutions\nBasic Qualifications:\nMaster's degree and 1 to 3 years of experience in computer science, IT, or related field OR\nBachelor's degree and 3 to 5 years of experience in computer science, IT, or related field OR\nDiploma and 7 to 9 years of experience in computer science, IT, or related fiel\nRequired Skills & Qualifications:\n6+ years of deep hands-on experiencewith ServiceNow administration and development in two or more productsITSM, ITBM, ITOM, GRC, HRSD, or Security Operations\nServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;\nStrong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform\nExperience creating and managing Scoped Applications\nWorkflow automation and integration development using REST, SOAP, or MID servers\nScripting skills in Python, Bash, or other programming languages\nWorking in an Agile (SAFe, Scrum, and Kanban) environment\nPreferred Qualifications:\nGood-to-Have\nSkills:\nExperience with other configuration management tools (e.g., Puppet, Chef).\nExperience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)\nExperience with Terraform & CloudFormation for AWS infrastructure automation\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP)\nExperience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)\nProfessional Certifications (preferred):\nService Now Certified System Administrator\nService Now Certified Application Developer\nService Now Certified Technical Architect\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced environment.","Rest Api, Servicenow, Itsm, Javascript, Agile, Automation"
AWS Big data Architect,Birlasoft Limited,15-20 Years,,Bengaluru,Software Engineering,"Key Responsibilities\nDesign and architect end to end solutions on AWS and create the HLD and LLD documents\nExperience in real-time Data Ingestion and Processing\nHands-on experience in AWS S3, Kafka, Glue, PySpark, Python, Redshift, AWS Cloud best practices and optimization\nExperience with integration of different data sources with Data Lake is required\nExperience in creating data lakes for Reporting, AI and Machine Learning\nExperience of data modelling and data architecture concepts\nGood in Creating Technical Specifications and Data Flow document\nTo be able to clearly articulate pros and cons of various technologies and platforms\nExperience in create the Technical Specification Design.\nSkills Required\n15 years of IT experience with 4+ years of AWS Big Data Services Architecture experience\nClient facing experience.\nIn-depth knowledge of domain Industry and business environment\nAnalytical and problem-solving capabilities","AWS Big Data Services Architecture, AWS Big Data, AWS"
Sr IT Architect- Data Architect IICS,Honeywell,8-15 Years,,Bengaluru,Consumer Electronics,"Role Responsibilities:\nDesign and implement data architecture integrating Salesforce with other platforms.\nDevelop and maintain ETL processes using Informatica tools.\nEnsure data integrity, security, and compliance with governance standards.\nCollaborate with stakeholders to define and meet business data requirements.\nJob Requirements:\nGraduate with 7+ years of experience in data architecture and integration.\nHands-on expertise in Informatica and Salesforce APIs.\nStrong data modeling and cloud ETL knowledge.\nCertifications in Informatica and Salesforce preferred.","Salesforce Integration, Data Architecture, Informatica, Data Governance, Etl Development"
Data Architect,Motivity Labs Inc,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Roles and Responsibilities\n10+ years of relevant work experience, including previous experience leading Data related projects in the field of Reporting and Analytics.\nDesign, build & maintain scalable data lake and data warehouse in cloud (GCP)\nExpertise in gathering business requirements, analysing business needs, defining the BI/DW architecture to support and help deliver technical solutions to complex business and technical requirements Creating solution prototype and participating in technology selection. Perform POC and technical presentations Architect, develop and test scalable data warehouses and data pipelines architecture in Cloud Technologies (GCP)\nExperience in SQL and No SQL DBMS like MS SQL Server, MySQL, PostgreSQL, DynamoDB, Cassandra, MongoDB.\nDesign and develop scalable ETL processes, including error handling. Expert in Query and program languages MS SQL Server, T-SQL, PostgreSQL, MY SQL, Python, R.\nPreparing data structures for advanced analytics and self-service reporting using MS SQL, SSIS, SSRS\nWrite scripts for stored procedures, database snapshots backups and data archiving. Experience with any of these cloud-based technologies: o PowerBI/Tableau, Azure Data Factory, Azure Synapse, Azure Data Lake o AWS RedShift, Glue, Athena, AWS Quicksight o Google Cloud Platform Good to have:\nAgile development environment pairing DevOps with CI/CD pipelines\nAI/ML background Interview Rounds: 2 Technical Rounds followed by HR Round.","MS SQL SSIS, R, Glue, No SQL DBMS, Athena, AWS Quicksight, T-sql, Ssrs, Ms Sql Server, Cassandra, PostgreSQL, Dynamodb, Tableau, Sql, Azure Synapse, Azure Data Factory, Gcp, Powerbi, MySQL, Azure Data Lake, Aws Redshift, MongoDB, Python"
Data Architect,OneMagnify,7-9 Years,,"Chennai, India",Login to check your skill match score,"The Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.\n\nTeam: Data Platform, Application & Data\n\nReports to: Engineering Manager, Data Platform, Application & Data\n\nMinimum Education: Bachelor's Degree or Equivalent Experience\n\nRecommended Tenure: 7+ years in data platform engineering or architecture roles, including at least 3 years in a hands-on architecture role\n\nRole Summary:\n\nThe Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.\n\nArchitecture & Design:\n\nTranslate business and technical requirements into data models and architecture specifications\nDesign and document data architecture artifacts, including logical/physical data models, data flow diagrams, and integration patterns\nAlign data models with application architecture and system interaction patterns in partnership with Solution Architects\nEstablish and maintain design patterns for relational, NoSQL, and streaming-based data solutions\n\nSolution Delivery & Support:\n\n\nServe as a hands-on architecture lead during project discovery, planning, and delivery phases\nSupport data engineers in implementing data architecture that aligns with platform and project requirements\nValidate implementation through design reviews and provide guidance throughout the development lifecycle\nContribute to platform evolution by defining and enforcing scalable, reusable architecture practices\n\nData Governance & Quality:\n\n\nDefine and uphold best practices for data modeling, data security, lineage tracking, and performance tuning\nPromote consistency in metadata, naming conventions, and data access standards across environments\nSupport data privacy, classification, and auditability across integrated systems\n\nCross-Functional Collaboration:\n\n\nWork closely with product managers, engineering leads, DevOps, and analytics teams to deliver scalable and future-proof data solutions\nCollaborate with Solution Architects to ensure integrated delivery across application and data domains\nAct as a subject matter expert on data structure, semantics, and lifecycle across key business domains\n\nKey Competencies:\n\n\n7+ years of experience in data engineering or data architecture roles, including 3+ years in a dedicated architecture capacity\nProven experience in cloud platformspreferably GCP and/or Azurewith strong familiarity with native data services\nDeep understanding of data storage paradigms including relational, NoSQL, and object storage\nHands-on experience with databases such as Oracle and Postgres; Python proficiency preferred\nFamiliarity with modern DevOps practices including infrastructure-as-code and CI/CD for data pipelines\nStrong communication skills with the ability to lead through influence across technical and non-technical audiences\nSelf-starter with excellent organization and prioritization skills across multiple initiatives","Postgres, Oracle, Python, Data Services"
Data Architect,Blackbaud,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"About The Role\n\nWe are seeking a highly skilled and experienced Data Architect to join our team. The ideal candidate will have at least 12 years of experience in software & data engineering and analytics and a proven track record of designing and implementing complex data solutions. You will be expected to design, create, deploy, and manage Blackbaud's data architecture. This role has considerable technical influence within the Data Platform, Data Engineering teams, and the Data Intelligence Center of Excellence atBlackbaud. Thisindividual acts as an evangelist for proper data strategy with other teams at Blackbaud and assists with the technical direction, specifically with data, of other projects.\n\nWhat You'll Be Doing\n\nDevelop and direct the strategy for all aspects of Blackbaud's Data and Analytics platforms, products and services\nSet, communicate and facilitate technical directionmore broadly for the AI Center of Excellence and collaboratively beyond the Center of Excellence\nDesign and develop breakthrough products, services or technological advancements in the Data Intelligence space that expand our business\nWork alongside product management to craft technical solutions to solve customer business problems.\nOwn the technical data governance practices and ensures data sovereignty, privacy, security and regulatory compliance.\nContinuously challenging the status quo of how things have been done in the past.\nBuild data access strategy to securely democratize data and enable research, modelling, machine learning and artificial intelligence work.\nHelp define the tools and pipeline patterns our engineers and data engineers use to transform data and support our analytics practice\nWork in a cross-functional team to translate business needs into data architecture solutions.\nEnsure data solutions are built for performance, scalability, and reliability.\nMentor junior data architects and team members.\nKeep current on technology: distributed computing, big data concepts and architecture.\nPromote internally how data within Blackbaud can help change the world.\n\nWhat We Want You To Have\n\n10+ years of experience in data and advanced analytics\nAt least 8 years of experience working on data technologies in Azure/AWS\nExperience building modern products and infrastructure\nExperience working with .Net/Java and Microservice Architecture\nExpertise in SQL and Python\nExpertise in SQL Server, Azure Data Services, and other Microsoft data technologies.\nExpertise in Databricks, Microsoft Fabric\nStrong understanding of data modeling, data warehousing, data lakes, data mesh and data products.\nExperience with machine learning\nExcellent communication and leadership skills.\nAble to work flexible hours as required by business priorities\nAbility to deliver software that meets consistent standards of quality, security and operability.\n\nStay up to date on everything Blackbaud, follow us on Linkedin, X, Instagram, Facebook and YouTube\n\nBlackbaud is a digital-first company which embraces a flexible remote or hybrid work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!\n\nBlackbaud is proud to be an equal opportunity employer and is committed to maintaining an inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.\n\nR0012317","Data Lakes, Analytics, Data Mesh, Microsoft Fabric, Azure Data Services, Java, data engineering, Machine Learning, Data Modeling, .NET, SQL Server, Microservice Architecture, Sql, Databricks, Data Warehousing, Azure, Python, AWS"
Data Architect,Evoke HR Solutions Pvt. Ltd.,10-13 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title : GCP Data architect\nLocation : Pune,Benguluru (Local)\nExp : 10 to 13 Years\nMinimum of 5 years of experience in a Data Architect role, supporting warehouse and Cloud data platforms/environments.\nExperience with common GCP services such as BigQuery, Dataflow, GCS, Service Accounts, cloud function\nExtremely strong in BigQuery design, development\nExtensive knowledge and implementation experience in data management, governance, and security frameworks.\nProven experience in creating high-level and detailed data architecture and design documentation.\nStrong aptitude for business analysis to understand domain data requirements.\nProficiency in Data Modelling using any Modelling tool for Conceptual, Logical, and Physical models is preferred\nHands-on experience with architecting end-to-end data solutions for both batch and real-time designs.\nAbility to collaborate effectively with clients, developers, and architecture teams to implement enterprise-level data solutions.\nFamiliarity with Data Fabric and Data Mesh architecture is a plus.\nExcellent verbal and written communication skills.","Service Accounts, GCS, Data Fabric, Data Mesh, BigQuery, Data Modelling, Gcp, Data Management, DataFlow"
Data Architect,BNP Paribas,7-9 Years,,"Mumbai, India",Login to check your skill match score,"About BNP Paribas India Solutions\n\nEstablished in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union's leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.\n\nAbout BNP Paribas Group\n\nBNP Paribas is the European Union's leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group's commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability\n\nCommitment to Diversity and Inclusion\n\nAt BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.\n\nPosition Purpose\n\nThe Data Architect is to support the work for ensuring that systems are designed, upgraded, managed, de-commissioned and archived in compliance with data policy across the full data life cycle. This includes complying with the data strategy and undertaking the design of data models and supporting the management of metadata. The Data Architect mission will integrate a focus on GDPR law, with the contribution to the privacy impact assessment and Record of Process & Activities relating to personal Data.\n\nThe scope is CIB EMEA and CIB ASIA\n\nResponsibilities\n\nDirect Responsibilities\n\nEngage with key business stakeholders to assist with establishing fundamental data governance processes\n\nDefine key data quality metrics and indicators and facilitate the development and implementation of supporting standards\n\nHelp to identify and deploy enterprise data best practices such as data scoping, metadata standardization, data lineage, data deduplication, mapping and transformation and business validation\n\nStructures the information in the Information System (any data modelling tool like Abacus), i.e. the way information is grouped, as well as the navigation methods and the terminology used within the Information Systems of the entity, as defined by the lead data architects.\n\nCreates and manages data models (Business Flows of Personal Data with process involved) in all their forms, including conceptual models, functional database designs, message models and others in compliance with the data framework policy\n\nAllows people to step logically through the Information System (be able to train them to use tools like Abacus)\n\nContribute and enrich the Data Architecture framework through the material collected during analysis, projects and IT validations Update all records in Abacus collected from stakeholder interviews/ meetings.\n\nSkill Area\n\nExpected\n\nCommunicating between the technical and the non-technical\n\nIs able to communicate effectively across organisational, technical and political boundaries, understanding the context. Makes complex and technical information and language simple and accessible for non- technical audiences. Is able to advocate and communicate what a team does to create trust and authenticity, and can respond to challenge.\n\nAble to effectively translate and accurately communicate across technical and non- technical stakeholders as well as facilitating discussions within a multidisciplinary team, with potentially difficult dynamics.\n\nData Modelling (Business Flows of Data in Abacus)\n\nProduces data models and understands where to use different types of data models. Understands different tools and is able to compare between different data models.\n\nAble to reverse engineer a data model from a live system. Understands industry recognized data modelling patterns and standards.\n\nUnderstands the concepts and principles of data modelling and is able to produce, maintain and update relevant data models for specific business needs.\n\nData Standards (Rules defined to manage/ maintain Data)\n\nDevelops and sets data standards for an organisation.\n\nCommunicates the business benefit of data standards, championing and governing those standards across the organisation.\n\nDevelops data standards for a specific component. Analyses where data standards have been applied or breached and undertakes an impact analysis of that breach.\n\nMetadata Management\n\nUnderstands a variety of metadata management tools. Designs and maintains the appropriate metadata repositories to enable the organization to understand their data assets.\n\nWorks with metadata repositories to complete and Maintains it to ensure information remains accurate and up to date.\n\nThe objective is to manage own learning and contribute to domain knowledge building\n\nTurning business problems into data design\n\nWorks with business and technology stakeholders to translate business problems into data designs. Creates optimal designs through iterative processes, aligning user needs with organisational objectives and system requirements.\n\nDesigns data architecture by dealing with specific business problems and aligning it to enterprise-wide standards and principles. Works within the context of well understood architecture and identifies appropriate patterns.\n\nContributing Responsibilities\n\nIt is expected that the data architect applies knowledge and experience of the capability, including tools and technique and adopts those that are more appropriate for the environment.\n\nThe Data Architect Needs To Have The Knowledge Of\n\nThe Functional & Application Architecture, Enterprise Architecture and Architecture rules and principles\nThe activities Global Market and/or Global Banking\nMarket meta-models, taxonomies and ontologies (such as FpML, CDM, ISO2022)\n\nSkill Area\n\nExpected\n\nData Communication\n\nUses the most appropriate medium to visualise data to tell compelling and actionable stories relevant for business goals.\n\nPresents, communicates and disseminates data appropriately and with high impact.\n\nAble to create basic visuals and presentations.\n\nData Governance\n\nUnderstands data governance and how it works in relation to other organisational governance structures. Participates in or delivers the assurance of a service.\n\nUnderstands what data governance is required and contribute to these data governance.\n\nData Innovation\n\nRecognises and exploits business opportunities to ensure more efficient and effective performance of organisations. Explores new ways of conducting business and organisational processes\n\nAware of opportunities for innovation with new tools and uses of data\n\nTechnical & Behavioral Competencies\n\nAble to effectively translate and accurately communicate across technical and non- technical stakeholders as well as facilitating discussions within a multidisciplinary team, with potentially difficult dynamics.\nAble to create basic visuals and presentations.\nExperience in working with Enterprise Tools (like Abacus, informatica, big data, collibra, etc)\nExperience in working with BI Tools (Like Power BI)\nGood understanding of Excel (formulas and Functions)\n\nSpecific Qualifications (if Required)\n\nPreferred: BE/ BTech, BSc-IT, BSc-Comp, MSc-IT, MSc Comp, MCA\n\nSkills Referential\n\nBehavioural Skills: (Please select up to 4 skills)\n\nCommunication skills - oral & written\n\nAbility to collaborate / Teamwork\n\nAbility to deliver / Results driven\n\nCreativity & Innovation / Problem solving\n\nTransversal Skills: (Please select up to 5 skills)\n\nAnalytical Ability\n\nAbility to understand, explain and support change\n\nAbility to develop and adapt a process\n\nAbility to anticipate business / strategic evolution\n\nChoose an item.\n\nEducation Level\n\nBachelor Degree or equivalent\n\nExperience Level\n\nAt least 7 years\n\nOther/Specific Qualifications (if Required)\n\nExperience in GDPR (General Data Protection Regulation) or in Privacy by Design would be preferred\nDAMA Certified","Data Innovation, Abacus, data standards, Data Communication, Big Data, Power Bi, Excel, Metadata Management, Informatica, Data Governance, Collibra, Data Modelling"
Data Architect,Siemens Healthineers,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Architect, you are required to:\n\nDesign & develop technical solutions which combine disparate information to create meaningful insights for business, using Big-data architectures\n\nBuild and analyze large, structured and unstructured databases based on scalable cloud infrastructures\nDevelop prototypes and proof of concepts using multiple data-sources and big-data technologies\nProcess, manage, extract and cleanse data to apply Data Analytics in a meaningful way\nDesign and develop scalable end-to-end data pipelines for batch and stream processing\nRegularly scan the Data Analytics landscape to stay up to date with latest technologies, techniques, tools and methods in this field\nStay curious and enthusiastic about using related technologies to solve problems and enthuse others to see the benefit in business domain\n\nQualification:\n\nBachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Engineering / Analytics is desirable.\n\nExperience level:\n\nMinimum 8 years in software development with at least 2 - 3 years hands-on experience in the area of Big-data / Data Engineering.\n\nDesired Knowledge & Experience:\n\nData Engineer - Big Data Developer\n\nSpark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\nKnowing Spark internals: Catalyst/Tungsten/Photon\nDatabricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\nIDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\nTest: pytest, Great Expectations\nCI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\nBig Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\nLanguages: Python/Functional Programming (FP)\nSQL: TSQL/Spark SQL/HiveQL\nStorage: Data Lake and Big Data Storage Design\n\nAdditionally it is helpful to know basics of:\n\nData Pipelines: ADF/Synapse Pipelines/Oozie/Airflow\nLanguages: Scala, Java\nNoSQL: Cosmos, Mongo, Cassandra\nCubes: SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\nSQL Server: TSQL, Stored Procedures\nHadoop: HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\nData Catalog: Azure Purview, Apache Atlas, Informatica\nBig Data Architect\nExpert: in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer\nMentor: mentors/educates Developers in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer\nArchitecture Styles: Lakehouse, Lambda, Kappa, Delta, Data Lake, Data Mesh, Data Fabric, Data Warehouses (e.g. Data Vault)\nApplication Architecture: Microservices, NoSql, Kubernetes, Cloud-native\nExperience: Many years of experience with all kinds of technology in the evolution of data platforms (Data Warehouse -> Hadoop -> Big Data -> Cloud -> Data Mesh)\nCertification: Architect certification (e.g. Siemens Certified Software Architect or iSAQB CPSA)\n\nRequired Soft-skills & Other Capabilities:\n\nExcellent communication skills, in order to explain your work to people who don't understand the mechanics behind data analysis\nGreat attention to detail and the ability to solve complex business problems\nDrive and the resilience to try new ideas, if the first ones don't work\nGood planning and organizational skills\nCollaborative approach to sharing ideas and finding solutions\nAbility to work independently and also in a global team environment.","Big Data Storage Design, Nosql, Hadoop, Spark, Data Lake, Databricks, Sql, Python, Azure DevOps"
Celonis Data Architect,"Sky Systems, Inc. (SkySys)",Fresher,,India,Login to check your skill match score,"Job Title: Celonis Data Architect\nJob Type & Location: India - Remote\nJob Description:\nWe are seeking a meticulous and experienced Data Architect to design, develop, and implement data architectures and solutions. The ideal candidate will have a strong understanding of database management systems and data modelling techniques, as well as experience in data warehousing, ETL processes, and data governance.\nJob Responsibilities:\nCollaborate with business stakeholders and technical teams to understand data requirements and develop data architecture solutions.\nDesign, develop, and maintain data models, data dictionaries, and data flow diagrams, and implement data architecture solutions\nCreate and implement data warehouse strategies and structures to support reporting and analytics.\nDevelop and maintain data integration processes, including Extract, Transform, Load (ETL) processes.\nEnsure data quality and integrity by establishing and enforcing data governance practices and data stewardship responsibilities.\nPrepare and deliver presentations to business stakeholders and technical teams, explaining data architecture concepts and solutions.\nCollaborate with software developers and DBAs to optimize database performance and ensure scalability and reliability.\nStay updated with industry trends and emerging technologies related to data architecture and data management.\nDefine and enforce data security and privacy standards and policies.\nProvide guidance and mentoring to junior data professionals.\nJob Requirements:\nBachelor's degree in computer science, Information Technology, or a related field.\nProven experience as a Data Architect or in a similar role.\nStrong knowledge of data modelling principles and techniques.\nProficiency in database management systems such as Oracle, SQL Server, or MySQL.\nExperience with data warehousing concepts and tools such as ETL processes, star schemas, and dimensional modeling.\nFamiliarity with data governance and data stewardship practices.\nExcellent analytical and problem-solving skills.\nStrong communication and presentation skills.\nAbility to work well in a team environment and collaborate with stakeholders from various departments.\nFamiliarity with cloud-based data storage and data management platforms is a plus.\nAs a Data Architect, you will play a critical role in designing and implementing data architectures that support the organization's data needs. Your expertise in data modelling, data warehousing, and ETL processes will contribute to the successful management and utilization of data within the organization.\nSupport in technical feasibility of various source systems with Celonis EMS. Understand details of the proposed integration pattern to help in effort & timeline estimation.\nEnsure that detailed designs match high level designs and are traceable to requirements in functional specification\nEnsure designs produced adhere to architectural roadmap and support the development, execution and operations of solutions\nEnsure that solutions meet requirements outlined in the design documentation. Ensure the overall user experience is taken into account when designing and deploying new solutions and services. Ensure that developed solutions are peer reviewed, formally documented and signed off by business\nEnsure that all work is delivered to agreed time, cost and quality constraints. Initiate solution testing to ensure they meet quality standards\nEstablish standardized design and development processes to enable cost effective delivery. Authorize and conduct service handover and lead the go-live authorization discussions with the other work streams\nEnsure that all release and deployment packages can be tracked, installed, tested, verified and backed out (if required). Ensure that release delivers the expected outcomes and value for the customers\nTake accountability to ensure adherence with Security and Compliance policies and procedures within Solution Delivery scope.","ETL Processes, data stewardship, Cloud-based Data Storage, Data Modelling, MySQL, Database Management Systems, Data Architecture, SQL Server, Data Warehousing, Data Governance, Oracle"
Data Engineer / Data Architect,Soul AI,Fresher,,India,Login to check your skill match score,"About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are seeking a Data Engineer / Data Architect who will be responsible for designing, building, and maintaining scalable data infrastructure and systems for a client. You'll play a key role in enabling efficient data flow, storage, transformation, and access across our organization or client ecosystems.\nWhether you're just beginning or already an expert, we value strong technical skills, curiosity, and the ability to translate complex requirements into reliable data pipelines.\nResponsibilities:\nDesign and implement scalable, robust, and secure data pipelines.\nBuild ETL/ELT frameworks to collect, clean, and transform structured and unstructured data.\nCollaborate with data scientists, analysts, and backend engineers to enable seamless data access and model integration.\nMaintain data integrity, schema design, lineage, and quality monitoring.\nOptimize performance and ensure reliability of data workflows in production environments.\nDesign and manage data warehousing and lakehouse architecture.\nSet up and manage infrastructure using IaC (Infrastructure as Code) when applicable.\nRequired Skills:\nStrong programming skills in Python, SQL, and Shell scripting.\nHands-on experience with ETL tools and orchestration frameworks (e.g., Airflow, Luigi, dbt).\nProficiency in relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Redis).\nExperience with big data technologies: Apache Spark, Kafka, Hive, Hadoop, etc.\nDeep understanding of data modeling, schema design, and data warehousing concepts.\nProficient with cloud platforms (AWS/GCP/Azure) and services like Redshift, BigQuery, S3, Dataflow, or Databricks.\nKnowledge of DevOps and CI/CD tools relevant to data infrastructure.\nNice to Have:\nExperience working in real-time streaming environments.\nFamiliarity with containerization and Kubernetes.\nExposure to MLOps and collaboration with ML teams.\nExperience with security protocols, data governance, and compliance frameworks.\nEducational Qualifications:\nBachelor's or Master's in Computer Science, Data Engineering, Information Systems, or a related technical field.","Airflow, Luigi, CI CD tools, dbt, S3, PostgreSQL, Data Warehousing, Kafka, Schema Design, Data Modeling, MySQL, Shell scripting, Etl Tools, Python, AWS, BigQuery, Hadoop, Apache Spark, Redshift, Redis, Sql, Devops, Hive, Gcp, Databricks, MongoDB, DataFlow, Azure"
Data Architect,TMF Group,8-10 Years,,"Pune, India",Login to check your skill match score,"We never ask for payment as part of our selection process, and we always contact candidates via our corporate accounts and platforms. If you are approached for payment, this is likely to be fraudulent. Please check to see whether the role you are interested in is posted on our career website.\nExperience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.\nTechnical Expertise:\nStrong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.\nProficiency in SQL, Python, Spark, and PowerShell.\nExperience in data modeling, ETL/ELT, and data warehousing.\nUnderstanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).\nStrong problem-solving and analytical skills.\nExcellent communication and stakeholder management.\nExposure to hybrid cloud environments (Azure & on-premises)\nKnowledge of data mesh and data fabric architectures.","data fabric architectures, data mesh, Infrastructure-as-Code, PowerShell, Data Warehousing, Azure Databricks, Data Modeling, Sql, Azure Sql, ELT, Devops, Azure Data Factory, ARM templates, Terraform, Azure Synapse Analytics, Azure Data Lake, Spark, Cosmos DB, Python, Etl"
AWS Data Architect,DataPMI Inc,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Data Architect AWS Cloud\nLocation: Bangalore\nClient: Deloitte India\nExperience: 8+ years\nJob Summary:\nWe are looking for a highly skilled Data Architect with a strong background in AWS cloud to design and oversee enterprise data architectures. The ideal candidate will be responsible for establishing data standards, modeling strategies, and optimizing data flows and repositories on AWS cloud infrastructure.\nKey Responsibilities:\nDesign and implement end-to-end data architecture solutions on AWS.\nDefine enterprise data models and frameworks aligned with business needs.\nArchitect scalable data pipelines and data lakes using AWS services (e.g., S3, Glue, Redshift, Lake Formation).\nLead data governance, metadata management, and data quality initiatives.\nCollaborate with business and technology stakeholders to capture requirements and translate them into architectural blueprints.\nEvaluate and recommend tools and platforms to support data strategy.\nProvide technical leadership and mentoring to data engineering teams.\nRequired Skills:\nStrong experience with AWS data ecosystem: S3, Redshift, Glue, Lambda, Athena, EMR, Lake Formation.\nProficiency in data modeling techniques (conceptual, logical, physical) for OLTP and OLAP.\nExperience with ERwin, PowerDesigner, or similar modeling tools.\nDeep understanding of data governance, security, and compliance on AWS.\nAbility to communicate complex technical concepts to non-technical stakeholders.\nPreferred Qualifications:\nAWS Certified Solutions Architect or equivalent certification.\nExperience with real-time/streaming data (e.g., Kinesis, Kafka).\nBackground in BI, analytics, and data warehousing","powerdesigner, data modeling techniques, Compliance, Lake Formation, Security, Glue, Athena, S3, Erwin, Emr, Redshift, Lambda, Data Governance, AWS"
"Lead IT Architect, Data Architect, Platinion",Boston Consulting Group (BCG),7-9 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nAbout BCG Platinion\n\nBCG Platinion's presence spans across the globe, with offices in Asia, Europe, and South and North America. We achieve digital excellence for clients with sustained solutions to the most complex and time-sensitive challenge. We guide clients into the future to push the status quo, overcome tech limitations, and enable our clients to go further in their digital journeys than what has ever been possible in the past. At BCG Platinion, we deliver business value through the innovative use of technology at a rapid pace. We roll up our sleeves to transform business, revolutionize approaches, satisfy customers, and change the game through Architecture, Cybersecurity, Digital Transformation, Enterprise Application and Risk functions. We balance vision with a pragmatic path to change transforming strategies into leading-edge tech platforms, at scale.\n\nWhat You'll Do\n\nDesign and implement a data architecture that supports the organization's business goals and objectives\nDevelop data models, define data standards and guidelines, and establish processes for data integration, migration, and management\nCreate and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organization's data assets\nEnsure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis\nWork closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organization's data architecture is integrated and aligned with\n\nother IT systems and applications\n\nStay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organization's data architecture\nCommunicate with stakeholders across the organization to understand their data needs and ensure that the organization's data architecture is aligned with the organization's strategic\n\ngoals and objectives\n\nWhat You'll Bring\n\nA BTech / MTech degree in Computer Science or a related field\nAt least 7+ years of experience in working on data architecture\nExpertise in data modeling and design, including conceptual, logical, and physical data models,\n\nand must be able to translate business requirements into data models\n\nProficient in a variety of data management technologies, including relational databases,\n\nNoSQL databases, data warehouses, and data lakes\n\nExpertise in ETL processes, including data extraction, transformation, and loading, and must\n\nbe able to design and implement data integration processes\n\nExperience with data analysis and reporting tools and techniques and must be able to design\n\nand implement data analysis and reporting processes\n\nFamiliar with industry-standard data architecture frameworks, such as TOGAF or Zachman,\n\nand must be able to apply them to the organization's data architecture\n\nFamiliar with cloud computing technologies, including public and private clouds, and must be\n\nable to design and implement data architectures that leverage cloud computing\n\nCertificates in Database Management will be preferred\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","NoSQL databases, data lakes, Relational Databases, zachman, ETL processes, data management technologies, cloud computing technologies, data analysis and reporting tools, Data Architecture, Data Modeling, Togaf, data warehouses"
GCP Data Architect,techolution,5-7 Years,,India,Login to check your skill match score,"Techolution is seeking an experienced GCP Data Architect who brings deep technical expertise in cloud-native data architecture, a strong grasp of Google Cloud Platform (GCP) services, and the ability to deliver enterprise-grade solutions. The ideal candidate will lead the end-to-end design and implementation of data pipelines and platforms, working closely with cross-functional teams to drive innovation and ensure scalable, secure, and efficient data infrastructure.\nDesignation: GCP Data Architect\nLocation: Remote\nEmployment Type: Full-time\nWork Timings: 2 PM to 11 PM IST\nJob Description:\nArchitect, implement, and optimize data platforms using GCP services like BigQuery, Pub/Sub, Dataflow, and Cloud Storage.\nLead the design of modern data lake/lakehouse solutions, integrating real-time and batch data pipelines.\nCollaborate with business stakeholders to translate requirements into technical architectures.\nLeverage tools like Cloud Composer, Apache Beam, and Informatica for ETL/ELT workflows.\nApply security best practices including IAM, VPC SC, and encryption mechanisms.\nManage infrastructure through IaC using Terraform and DevOps pipelines (Cloud Build, GitOps).\nOversee implementation of monitoring and logging tools to ensure data platform health.\nSupport and guide development teams with code reviews, architecture best practices, and documentation.\nEngage in Agile delivery processes and contribute to process improvement.\nMandatory Skills:\n5+ years in data architecture, including 3+ years on GCP.\nExperience in handling data architecture projects on GCP.\nHands-on with GCP services: BigQuery, Cloud Storage, Pub/Sub, Dataflow, Dataproc.\nStrong ETL/ELT development experience using Cloud Composer, Apache Beam, and Python.\nStrong skills in SQL and Python; experience in JavaScript/TypeScript is a plus.\nProven experience in data lake/lakehouse and microservices architecture.\nHands-on with Terraform, CI/CD pipelines, and secure cloud deployments.\nExperience with security & governance: IAM, encryption, Data Catalog.\nSoft skills: excellent communication, documentation, stakeholder collaboration.\nPreferred Skills:\nGoogle Cloud Professional Data Engineer or Architect certification.\nFamiliarity with BI tools like Looker, Tableau, or Power BI.\nExperience with Kafka, Hadoop, or Spark.\nLeadership and mentorship qualities; team player attitude.\nAbility to work in a fast-paced Agile environment.\nAbout Techolution:\nTecholution is a leading innovation consulting company on track to become one of the most admired brands in the world for innovation done right. Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better human\nexperience for the communities they serve. With that, we are now fully committed to helping our clients build the enterprise of tomorrow by making the leap from Lab Grade AI to Real World AI. In 2019, we won the prestigious Inc. 500 Fastest-Growing Companies in America award, only 4 years after its formation. In 2022, Techolution was honored with the Best-in-Business title by Inc. for Innovation Done Right. Most recently, we received the AIConics trophy for being the Top AI Solution Provider of the Year at the AI Summit in New York.\nLet's give you more insights!\nOne of our amazing products with Artificial Intelligence:\n1. https://faceopen.com/ :Our proprietary and powerful AI Powered user identification system which is built on artificial intelligence technologies such as image recognition, deep neural networks, and robotic process automation. (No more touching keys, badges or fingerprint scanners ever again!)\nSome videos you wanna watch!\nLife at Techolution\nGoogleNext 2023\nAi4 - Artificial Intelligence Conferences 2023\nWaWa - Solving Food Wastage\nSaving lives - Brooklyn Hospital\nInnovation Done Right on Google Cloud\nTecholution featured on Worldwide Business with KathyIreland\nTecholution presented by ION World's Greatest\nVisit us @www.techolution.com : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with technology.","Data Catalog, Pub Sub, Cloud Composer, CI CD pipelines, GCP services, BigQuery, Dataproc, Sql, Cloud Storage, Encryption, Terraform, Iam, Apache Beam, DataFlow, Python"
Azure Data Architect,Delphi Consulting Middle East,Fresher,,India,Login to check your skill match score,"This role requires a blend of technical expertise in data architecture, ETL processes, data engineering, and data analytics, along with strong communication and leadership skills to effectively drive data and AI initiatives within the organization.\nKey Responsibilities\nData Architecture & Engineering\nDesign and architect end-to-end data and artificial intelligence solutions that meet business requirements and align with organizational goals.\nLead the development and implementation of data architecture strategies, including data modeling, data warehousing, and data governance.\nDesign and implement efficient ETL processes to extract, transform, and load data from various sources into data lakes, data warehouses, and analytical platforms.\nDevelop and maintain data models and schemas to support data analysis, reporting, and visualization needs across the organization.\nEnsure data security, compliance, and governance requirements are met throughout the data lifecycle.\nTechnology & Tools\nUtilize Databricks for data engineering tasks such as data preparation, data transformation, and batch/stream processing.\nImplement and optimize data workflows using Data Factory for orchestrating data movement and data processing tasks.\nDesign and implement scalable and high-performance data storage solutions using Microsoft Fabric.\nCollaborate with data engineers and data scientists to design and optimize data pipelines for machine learning model development and deployment.\nLeadership & Collaboration\nProvide technical leadership and guidance to cross-functional teams on best practices for data architecture, ETL processes, and data analytics.\nCollaborate with business stakeholders to understand requirements, define project scope, and prioritize initiatives to deliver actionable insights and drive business outcomes.\nStay updated with the latest trends and advancements in data and AI technologies and evaluate emerging tools and platforms for potential integration into existing solutions.\nWhat You'll Bring\nStrong expertise in data architecture principles and best practices.\nProficiency in ETL/ELT (Extract, Transform, Load) processes and tools.\nExperience with data engineering techniques and technologies.\nDeep understanding of data modeling concepts and techniques.\nHands-on experience with data warehousing solutions.\nExpertise in Databricks for data engineering and analytics.\nFamiliarity with Azure Data Factory for data integration and orchestration.\nExperience with Microsoft Fabric or Azure Databricks for scalable data storage and analytics.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration skills.\nAbility to work effectively in cross-functional teams and influence technical decision-making.\nWhat We Offer\nAt Delphi, we are dedicated to creating an environment where you can thrive, both professionally and personally. Our competitive compensation package, performance-based incentives, and health benefits are designed to ensure you're well-supported.","ETL processes, Microsoft Fabric, data engineering, Azure Data Factory, Data Modeling, Data Architecture, Data Warehousing, Databricks, Data Analytics"
GCP Data Architect,techolution,5-7 Years,,India,Login to check your skill match score,"Techolution is seeking an experienced GCP Data Architect who brings deep technical expertise in cloud-native data architecture, a strong grasp of Google Cloud Platform (GCP) services, and the ability to deliver enterprise-grade solutions. The ideal candidate will lead the end-to-end design and implementation of data pipelines and platforms, working closely with cross-functional teams to drive innovation and ensure scalable, secure, and efficient data infrastructure.\nDesignation: GCP Data Architect\nLocation: Remote\nEmployment Type: Full-time\nWork Timings: 2 PM to 11 PM IST\nJob Description:\nArchitect, implement, and optimize data platforms using GCP services like BigQuery, Pub/Sub, Dataflow, and Cloud Storage.\nLead the design of modern data lake/lakehouse solutions, integrating real-time and batch data pipelines.\nCollaborate with business stakeholders to translate requirements into technical architectures.\nLeverage tools like Cloud Composer, Apache Beam, and Informatica for ETL/ELT workflows.\nApply security best practices including IAM, VPC SC, and encryption mechanisms.\nManage infrastructure through IaC using Terraform and DevOps pipelines (Cloud Build, GitOps).\nOversee implementation of monitoring and logging tools to ensure data platform health.\nSupport and guide development teams with code reviews, architecture best practices, and documentation.\nEngage in Agile delivery processes and contribute to process improvement.\nMandatory Skills:\n5+ years in data architecture, including 3+ years on GCP.\nExperience in handling data architecture projects on GCP.\nHands-on with GCP services: BigQuery, Cloud Storage, Pub/Sub, Dataflow, Dataproc.\nStrong ETL/ELT development experience using Cloud Composer, Apache Beam, and Python.\nStrong skills in SQL and Python; experience in JavaScript/TypeScript is a plus.\nProven experience in data lake/lakehouse and microservices architecture.\nHands-on with Terraform, CI/CD pipelines, and secure cloud deployments.\nExperience with security & governance: IAM, encryption, Data Catalog.\nSoft skills: excellent communication, documentation, stakeholder collaboration.\nPreferred Skills:\nGoogle Cloud Professional Data Engineer or Architect certification.\nFamiliarity with BI tools like Looker, Tableau, or Power BI.\nExperience with Kafka, Hadoop, or Spark.\nLeadership and mentorship qualities; team player attitude.\nAbility to work in a fast-paced Agile environment.\nAbout Techolution:\nTecholution is a leading innovation consulting company on track to become one of the most admired brands in the world for innovation done right. Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better human\nexperience for the communities they serve. With that, we are now fully committed to helping our clients build the enterprise of tomorrow by making the leap from Lab Grade AI to Real World AI. In 2019, we won the prestigious Inc. 500 Fastest-Growing Companies in America award, only 4 years after its formation. In 2022, Techolution was honored with the Best-in-Business title by Inc. for Innovation Done Right. Most recently, we received the AIConics trophy for being the Top AI Solution Provider of the Year at the AI Summit in New York.\nLet's give you more insights!\nOne of our amazing products with Artificial Intelligence:\n1. https://faceopen.com/ :Our proprietary and powerful AI Powered user identification system which is built on artificial intelligence technologies such as image recognition, deep neural networks, and robotic process automation. (No more touching keys, badges or fingerprint scanners ever again!)\nSome videos you wanna watch!\nLife at Techolution\nGoogleNext 2023\nAi4 - Artificial Intelligence Conferences 2023\nWaWa - Solving Food Wastage\nSaving lives - Brooklyn Hospital\nInnovation Done Right on Google Cloud\nTecholution featured on Worldwide Business with KathyIreland\nTecholution presented by ION World's Greatest\nVisit us @www.techolution.com : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with technology.","Data Catalog, Pub Sub, Cloud Composer, CI CD pipelines, GCP services, BigQuery, Dataproc, Sql, Cloud Storage, Encryption, Terraform, Iam, Apache Beam, DataFlow, Python"
Data Architect,Horizontal Digital,7-9 Years,,"Jaipur, India",Login to check your skill match score,"Horizontal Digital is an experience-forward consultancy. So, what does this mean We help organizations meet ever-increasing customer expectations and set the bar higher in the process. And we deliver on this promise by putting customers at the absolute center of everything we do, helping them build stronger possibilities with our clients in the process.\n\nOur solutions are driven by strategy, creativity, and execution and powered by Sitecore, Salesforce, and other enterprise platforms. Get a deeper look at our expertise with some sample case studies.\n\nBut enough about us. Let's talk about you.\n\nAs a Data and Technical Architect for Salesforce Data Cloud at Horizontal Digital, you will be a demonstrated leader in technical and data architecture aspects of customer and partner engagements that lead to the successful delivery of Data Cloud Projects. In this key leadership role, you will play the critical role for setting customers up for success by prescriptively helping to shape and then lead the project teams within the Salesforce Data Cloud space. You will collaborate with stakeholders to define technical vision, drive solution and data architecture, and ensure seamless integration of Salesforce Data Cloud with enterprise systems, aligning technology solutions with business objectives. This role requires deep technical expertise in data architecture, integration, and advanced data strategies, enabling organizations to unlock the full potential of their customer data.\n\nWhat you'll do:\n\nFacilitate and lead technical discovery workshops to document detailed data architecture, integration requirements, and data ingestion strategies.\nSynthesize complex requirements to create clear and comprehensive technical solution designs and collaborate with technical teams to document and implement them.\nAssess current-state data ecosystems, contact/subscriber management, and identity resolution processes, while defining the future-state architecture and performing gap analysis across data, platform and technology.\nLead the refinement, design, and configuration of complex data models, ensuring alignment with business processes, scalability, and Salesforce Data Cloud best practices.\nCollaborate with cross-functional data teams to design and implement data integration and migration strategies leveraging ETL tools, APIs, and middleware solutions.\nGuide and oversee the execution of user acceptance testing (UAT), ensuring the delivery of solutions that meet client expectations and quality standards.\nServe as the primary technical point of contact for client stakeholders, providing enablement training on Salesforce Data Cloud and driving adoption across their teams.\nAdvocate for data governance best practices, including data privacy, quality assurance, and regulatory compliance frameworks.\nCollaborate with Sales, Go-to-Market, and professional services teams in pre-sales activities, including scoping, solution estimation, and proposal development.\nContribute to internal growth by developing thought leadership, building best practices, delivering training, and mentoring teams to scale Data Cloud expertise across the organization.\nManage multiple engagements simultaneously, ensuring a balance between billable utilization and team leadership objectives.\n\nWhat you bring:\n\n7+ years of client-facing consulting/professional services experience delivering enterprise-grade data solutions.\n3+ years of experience implementing Salesforce Data Cloud or equivalent Customer Data Platforms (CDPs) (e.g., Adobe AEP, Segment, Tealium, Arm Treasure Data, BlueShift).\nStrong background in data architecture, data modeling, ETL/ELT pipelines, data integration, and API-driven solutions.\nCertifications in Salesforce Data Cloud and a solid understanding of the Salesforce ecosystem (Sales Cloud, Service Cloud, Marketing Cloud).\nExperience implementing data governance, data security, and regulatory compliance (e.g., GDPR, CCPA) frameworks.\nExpertise in identity resolution, subscriber management, and harmonizing data across systems to enable a single customer view.\nDemonstrated success in facilitating technical workshops, delivering solution documentation, and leading cross-functional technical teams.\nStrong analytical and problem-solving skills with expertise in agile delivery methodologies and complex solution lifecycles.\nExcellent written and verbal communication skills for engaging with technical and non-technical stakeholders alike.\nIndustry experience in one or more of the following: Financial Services, Health and Life Sciences, Manufacturing, Retail, or Hospitality.\nBachelor's degree preferred; Master's degree is a plus.\n\nWho you are:\n\nYou are a data integration strategy and architecture enthusiast with deep expertise in integrating enterprise-scale data solutions.\nYou excel at simplifying and leading discussions around complex data challenges, data integration, and technical solutioning for diverse stakeholders.\nYou are passionate about leveraging data to transform customer experiences, create unified data ecosystems, and enable actionable insights.\nYou thrive in fast-paced, agile environments and demonstrate the flexibility to manage shifting priorities, ambiguity, and multiple client engagements.\nYou are recognized as a trusted advisor, known for your strong stakeholder relationships, empathetic communication, and accountability.\nYou bring a growth mindset, with a commitment to professional development, mentoring teams, and delivering excellence in every engagement.\nYou are optimistic and results-driven, operating with a focus on collaboration, innovation, and successful outcomes for clients.","subscriber management, identity resolution, Regulatory Compliance, API-driven solutions, middleware solutions, Salesforce Data Cloud, Data Security, Apis, Data Architecture, Data Modeling, Etl Tools, Data Governance, Data Integration"
Lead Data Architect,Encora Inc.,5-7 Years,,"Noida, India",Login to check your skill match score,"Encora is seeking a full-time Lead Data Engineer to support our manufacturing clients large scale digital transformation.\nThe Lead Data Engineer is responsible for ensuring the day-to-day leadership and guidance of the local, India-based, data team. This role will be the primary interface with the management team of the client and will work cross functionally with various IT functions to streamline project delivery.\nDuties and Responsibilities:\nWork with a distributed team with primary focus on the management of resources, team assignments, and fostering career growth\nInput into the hiring of team members for the locally based team\nSupport compliance with best practices related to data availability and security procedures\nApply proven communication and problem-solving skills to resolve support data issues as they arise\nDemonstrate skills in abstract development frameworks\nDemonstrate accountability to assigned work & timelines\nDemonstrated leadership skills.\nDemonstrated technical competency for the role.\nDemonstrated timely delivery on assigned tasks and effective communication on project status and escalation when needed.\nEducation and Experience:\n5+ years of experience in a management role dealing with resourcing, career growth or similar activities and/or responsibilities.\n12+ Years or equivalent degree + experience\nRequired Skills/Certifications:\nStrong analytical and problem-solving skills - Ability to determine data patterns and perform root cause analysis to resolve production reporting bugs\nFamiliarity with the data, analytics and BI space and knowledge of Power BI\nExperience working with Microsoft data integration platforms such as Azure Data Factory, SSIS, Synapse\nExperience with SQL (Python experience a plus)\nFamiliarity with Azure DevOps for source code management a plus\nUnderstanding of data management (e. g. permissions, security, and monitoring)\nExperience working with and building strong relationships with customers and external partners\nExcellent written and verbal communication skills, including logical structuring and delivering presentations\nKnowledge of working with helpdesk tools or platforms like ServiceNow is a plus\nExperience with cloud services (Azure) is a plus\nExperience in working with an onshore client is desirable\nEagerness to learn and think critically to design improved processes\nPreferred Experience with:\nEstimating and Planning\nWork breakdown for the team\nData pipelines\nData Transformations\nCode/Unit Test\nPR & Code Reviews\nPerformance & Load Testing\nFunctional & Regression Testing\nTier 2 support/consulting\nProduction Support\nFunctional ask to Technical design\nData Engineering\nAgile Methodologies\nCI/CD\nMetadata driven frameworks.\nTechnical documentation\nAzure Synapse Spark\nPython\nComplex SQL\nParquet & Delta file processing\nETL Optimizations\nAzure DevOps\nLocation: Noida, India\nMandatory Skills:\n1. Data Engineering :\n2. Data Pipeline:\n3. Pyspark:\n4. Spark SQL:\n5. Python:\n6. SQL:\n7. Datawarehousing:\n8. Azure Synapse:\n9. Enterprise level Data Architecture:\n10. Logistic Domain:\n11. MS Fabric:\n12. Azure Databricks:\nAbout Encora:\nEncora is the preferred digital engineering and modernization partner of some of the world's leading enterprises and digital native companies. With over 9,000 experts in 47+ offices and innovation labs worldwide, Encora's technology practices include Product Engineering & Development, Cloud Services, Quality Engineering, DevSecOps, Data & Analytics, Digital Experience, Cybersecurity, and AI & LLM Engineering.\nAt Encora, we hire professionals based solely on their skills and qualifications, and do not discriminate based on age, disability, religion, gender, sexual orientation, socioeconomic status, or nationality.","Parquet Delta file processing, Complex SQL, Logistic Domain, CI CD, Synapse, Enterprise level Data Architecture, Metadata driven frameworks, MS Fabric, ETL Optimizations, Pyspark, data engineering, Spark SQL, Power Bi, Azure Databricks, SSIS, Sql, Azure Synapse, Azure Data Factory, Data Pipeline, Cloud Services, Datawarehousing, Python, Azure DevOps"
Senior Data Engineer/Data Architect,Spaulding Ridge,8-10 Years,,"Pune, India",Login to check your skill match score,"Spaulding Ridge is an advisory and IT implementation firm. We help global organizations get financial clarity into the complex, daily sales, and operational decisions that impact profitable revenue generations, efficient operational performance, and reliable financial management.\n\nAt Spaulding Ridge, we believe all business is personal. Core to our values is our relationships with our clients, our business partners, our team, and the global community. Our employees dedicate their time to helping our clients transform their business, from strategy through implementation and business transformation.\n\nWhat You Will Do And Learn\n\nAs a Data Architect/ Manager in Data Solutions, you'll be responsible for designing, implementing, and testing proposed modern analytic solutions. Working closely with our client partners and architects, you'll develop relationships with key technical resources while delivering tangible business outcomes.\n\nManage the Data engineering lifecycle including research, proof of concepts, architecture, design, development, test, deployment, and maintenance\nCollaborate with team members to design and implement technology that aligns with client business objectives\nBuild proof of concepts for a modern analytics stack supporting a variety of Cloud-based Business Systems for potential clients\nTeam management experience and ability to manage, mentor and develop talent of assigned junior resources\nCreate actionable recommendations based on identified platform, structural and/or logic problems\nCommunicate and demonstrate a clear understanding of client business needs, goals, and objectives\nCollaborate with other architects on solution designs and recommendations.\n\nQualifications:\n\n8+ years experience developing industry leading business intelligence and analytic solutions\nMust have thorough knowledge of data warehouse concepts and dimensional modelling\nMust have experience in writing advanced SQL\nMust have at least 5+ years of hands-on experience on DBT (Data Build Tool). Mandatory to have most recent hands-on experience on DBT.\nMust have experience working with DBT on one or more of the modern databases like Snowflake / Amazon Redshift / BigQuery / Databricks / etc.\nHands-on experience with Snowflake would carry higher weightage\nSnowflake SnowPro Core certification would carry higher weightage\nExperience working in AWS, Azure, GCP or similar cloud data platform would be an added advantage\nHands-on experience on Azure would carry higher weightage\nMust have experience in setting up DBT projects\nMust have experience in understanding / creating / modifying & optimizing YML files within DBT\nMust have experience in implementing and managing data models using DBT, ensuring efficient and scalable data transformations\nMust have experience with various materialization techniques within DBT\nMust have experience in writing & executing DBT Test cases\nMust have experience in setting up DBT environments\nMust have experience in setting up DBT Jobs\nMust have experience with writing DBT Jinja and Macros\nMust have experience in creating DBT Snapshots\nMust have experience in creating & managing incremental models using DBT\nMust have experience with DBT Docs\nShould have a good understanding of DBT Seeds\nMust have experience with DBT Deployment\nMust Experience with architecting data pipelines using DBT, utilizing advanced DBT features\nProficiency in version control systems and CI/CD\nMust have hands-on experience configuring DBT with one or more version control systems like Azure DevOps / Github / Gitlab / etc.\nMust have experience in PR approval workflow\nParticipate in code reviews and best practices for SQL and DBT development\nExperience working with visualization tools such as Tableau, PowerBI, Looker and other similar analytic tools would be an added advantage\n2+ years of Business Data Analyst experience\n2+ years of experience writing Business requirements, Use cases and/or user stories, for data warehouse or data mart initiatives.\nUnderstanding and experience on ETL/ELT is an added advantage\n2+ years of consulting experience working on project-based delivery using Software Development Life Cycle (SDLC)\n2+ years of years of experience with relational databases (Postgres, MySQL, SQL Server, Oracle, Teradata etc.)\n2+ years of experience creating functional test cases and supporting user acceptance testing\n2+ years of experience in Agile/Kanban/DevOps Delivery\nOutstanding analytical, communication, and interpersonal skillsAbility to manage projects and teams against planned work\nResponsible for managing the day-to-day client relationship on projects\n\nSpaulding Ridge's Commitment to an Inclusive Workplace\n\n\nWhen we engage the expertise, insights, and creativity of people from all walks of life, we become a better organization, we deliver superior services to clients, and we transform our communities and world for the better.\n\nAt Spaulding Ridge, we believe our team should reflect the rich diversity of society and we take seriously the responsibility to cultivate a workplace where every bandmate feels accepted, respected, and valued for who they are. We do this by creating a culture of trust and belonging, through practices and policies that support inclusion, and through our employee led Employee Resource Groups (ERGs): CRE (Cultural Race and Ethnicity), Women Elevate, PROUD and Mental Wellness Alliance.\n\nThe company is committed to offering Equal Employment Opportunity and to providing reasonable accommodation to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Spaulding Ridge and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to our VP of Human Resources, Cara Halladay ([HIDDEN TEXT]). Requests for reasonable accommodation will be considered on a case-by-case basis.\n\nQualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, gender, sexual orientation, gender identity, protected veteran status or disability.","snowflake, Looker, Tableau, Sql, ELT, AWS, Etl, Powerbi, Version Control Systems, Azure, Gcp"
Celonis Data Architect,"Sky Systems, Inc. (SkySys)",Fresher,,India,Login to check your skill match score,"Job Title: Celonis Data Architect\nJob Type & Location: India - Remote\nJob Description:\nWe are seeking a meticulous and experienced Data Architect to design, develop, and implement data architectures and solutions. The ideal candidate will have a strong understanding of database management systems and data modelling techniques, as well as experience in data warehousing, ETL processes, and data governance.\nJob Responsibilities:\nCollaborate with business stakeholders and technical teams to understand data requirements and develop data architecture solutions.\nDesign, develop, and maintain data models, data dictionaries, and data flow diagrams, and implement data architecture solutions\nCreate and implement data warehouse strategies and structures to support reporting and analytics.\nDevelop and maintain data integration processes, including Extract, Transform, Load (ETL) processes.\nEnsure data quality and integrity by establishing and enforcing data governance practices and data stewardship responsibilities.\nPrepare and deliver presentations to business stakeholders and technical teams, explaining data architecture concepts and solutions.\nCollaborate with software developers and DBAs to optimize database performance and ensure scalability and reliability.\nStay updated with industry trends and emerging technologies related to data architecture and data management.\nDefine and enforce data security and privacy standards and policies.\nProvide guidance and mentoring to junior data professionals.\nJob Requirements:\nBachelor's degree in computer science, Information Technology, or a related field.\nProven experience as a Data Architect or in a similar role.\nStrong knowledge of data modelling principles and techniques.\nProficiency in database management systems such as Oracle, SQL Server, or MySQL.\nExperience with data warehousing concepts and tools such as ETL processes, star schemas, and dimensional modeling.\nFamiliarity with data governance and data stewardship practices.\nExcellent analytical and problem-solving skills.\nStrong communication and presentation skills.\nAbility to work well in a team environment and collaborate with stakeholders from various departments.\nFamiliarity with cloud-based data storage and data management platforms is a plus.\nAs a Data Architect, you will play a critical role in designing and implementing data architectures that support the organization's data needs. Your expertise in data modelling, data warehousing, and ETL processes will contribute to the successful management and utilization of data within the organization.\nSupport in technical feasibility of various source systems with Celonis EMS. Understand details of the proposed integration pattern to help in effort & timeline estimation.\nEnsure that detailed designs match high level designs and are traceable to requirements in functional specification\nEnsure designs produced adhere to architectural roadmap and support the development, execution and operations of solutions\nEnsure that solutions meet requirements outlined in the design documentation. Ensure the overall user experience is taken into account when designing and deploying new solutions and services. Ensure that developed solutions are peer reviewed, formally documented and signed off by business\nEnsure that all work is delivered to agreed time, cost and quality constraints. Initiate solution testing to ensure they meet quality standards\nEstablish standardized design and development processes to enable cost effective delivery. Authorize and conduct service handover and lead the go-live authorization discussions with the other work streams\nEnsure that all release and deployment packages can be tracked, installed, tested, verified and backed out (if required). Ensure that release delivers the expected outcomes and value for the customers\nTake accountability to ensure adherence with Security and Compliance policies and procedures within Solution Delivery scope.","ETL Processes, data stewardship, Cloud-based Data Storage, Data Modelling, MySQL, Database Management Systems, Data Architecture, SQL Server, Data Warehousing, Data Governance, Oracle"
Senior Data Architect,Decision Minds,Fresher,,"Bengaluru, India",Login to check your skill match score,"Company Description\nDecision Minds is a leading company in Data Cloud, Big Data, Cloud Analytics, AI/ML, and Multi-Cloud deployments. The team consists of passionate thought leaders and industry experts dedicated to revolutionizing Data Analytics, Artificial Intelligence, Cloud Computing, and Robotic Process Automation. The company's focus on utilizing technology for positive change and creating innovative solutions sets it apart in the industry.\nRole Description\nThis is a full-time hybrid role for a Senior Data Architect at Decision Minds. The role will be based in Bengaluru with some opportunities for remote work. The Senior Data Architect will be responsible for data governance, data architecture, data modeling, ETL processes, and data warehousing on a daily basis.\nQualifications\nData Governance and Data Architecture skills\nData Modeling and ETL (Extract Transform Load) skills\nExperience in Data Warehousing\nStrong analytical and problem-solving skills\nExcellent communication and stakeholder management abilities\nKnowledge of AI/ML technologies is a plus\nBachelor's or Master's degree in Computer Science, Data Science, or related field","AI ML technologies, Data Modeling, Data Architecture, Data Warehousing, Data Governance"
CDC Data Architect,LSEG (London Stock Exchange Group),8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Design and Development: Build, enhance, and lead the company's logical, conceptual, and physical data models, demonstrating Snowflake's advanced capabilities.\n\nData Integration: Lead all aspects of the development of comprehensive data integration processes using ETL for various data types, including structured, semi-structured, and unstructured data, ensuring seamless integration with Snowflake.\n\nPerformance Tuning: Implement and fine-tune Snowflake features such as resource monitors, RBAC controls, scalable virtual warehouses, SQL performance tuning, zero copy clone, and time travel to optimize performance.\n\nData Security: Ensure robust data security and handle access controls effectively within the Snowflake environment.\n\nCloud Integration: Deploy cloud-based enterprise data warehouse solutions, Leverage AWS services such as S3, Glue, Athena, CloudWatch, and EMR to enhance data storage, processing, and analytics capabilities and integrate seamlessly with platforms like AWS and applying Snowflake's cloud-native architecture.\n\nData Governance: Uphold consistent data governance, testing, and continuous delivery practices, ensuring data integrity and compliance within Snowflake.\n\nAI Integration: Incorporate AI and machine learning models into the data architecture, demonstrating Snowflake's capabilities to handle large-scale data processing and real-time analytics.\n\nSnowpark/Python Development: Use Python/Snowpark for developing data pipelines, ETL processes, and automation scripts, ensuring efficient data handling and processing within the Snowflake environment\n\nTeamwork: Serve as a data domain expert, working closely with various teams to ensure standard methodologies in data management are followed, and facilitate the integration of AI insights into business processes !\n\nCandidate Profile / Key skills\n\nExperience: At least 8 years in Data Engineering or Data Management Solutions, with a proven track record of improving data pipeline processes and leading initiatives.\n\nSnowflake Expertise: A minimum of 5 years of meaningful experience with Snowflake.\n\nClient Leadership: Skilled in leading data-centric client engagements.\n\nTechnical Proficiency: Demonstrable skills in sophisticated SQL, Unix Shell/Python scripting, performance tuning, and database optimization.\n\nCloud Technologies: Expertise in AWS services, including S3, EC2, Lambda, and Redshift.\n\nData Handling & Migration: Proficient in managing semi-structured data (JSON, XML) and using Snowflake's VARIANT attribute. Experience in migrating data from on-premises databases to Snowflake.\n\nAutomation: Skilled in crafting and developing automated data pipelines using Snowpipe and other relavant tools !\n\nDatabase Experience: Hands-on experience with databases and data warehousing solutions such as Oracle, Microsoft SQL Server, AWS Redshift, or Snowflake.\n\nCloud Experience: Experience with AWS or Azure is a plus.\n\nSQL Analysis: Strong SQL analysis skills and familiarity with tools like JIRA, Asana, or other relevant defect tracking tools. Experience in implementing data quality frameworks is an added advantage.\n\nProgramming Skills: Proficiency in Python, PySpark, or Snowpark is helpful. Additional expertise in Cortex or AI capabilities is a big plus.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","Data Handling, snowflake, Automation, Data Integration, Sql, Performance Tuning, Cloud Technologies, Unix Shell, Data Security, Data Governance, Python, AWS, Etl"
Data Engineer / Data Architect,Soul AI,Fresher,,India,Login to check your skill match score,"About Us:\nSoul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.\nRole Overview:\nWe are seeking a Data Engineer / Data Architect who will be responsible for designing, building, and maintaining scalable data infrastructure and systems for a client. You'll play a key role in enabling efficient data flow, storage, transformation, and access across our organization or client ecosystems.\nWhether you're just beginning or already an expert, we value strong technical skills, curiosity, and the ability to translate complex requirements into reliable data pipelines.\nResponsibilities:\nDesign and implement scalable, robust, and secure data pipelines.\nBuild ETL/ELT frameworks to collect, clean, and transform structured and unstructured data.\nCollaborate with data scientists, analysts, and backend engineers to enable seamless data access and model integration.\nMaintain data integrity, schema design, lineage, and quality monitoring.\nOptimize performance and ensure reliability of data workflows in production environments.\nDesign and manage data warehousing and lakehouse architecture.\nSet up and manage infrastructure using IaC (Infrastructure as Code) when applicable.\nRequired Skills:\nStrong programming skills in Python, SQL, and Shell scripting.\nHands-on experience with ETL tools and orchestration frameworks (e.g., Airflow, Luigi, dbt).\nProficiency in relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Redis).\nExperience with big data technologies: Apache Spark, Kafka, Hive, Hadoop, etc.\nDeep understanding of data modeling, schema design, and data warehousing concepts.\nProficient with cloud platforms (AWS/GCP/Azure) and services like Redshift, BigQuery, S3, Dataflow, or Databricks.\nKnowledge of DevOps and CI/CD tools relevant to data infrastructure.\nNice to Have:\nExperience working in real-time streaming environments.\nFamiliarity with containerization and Kubernetes.\nExposure to MLOps and collaboration with ML teams.\nExperience with security protocols, data governance, and compliance frameworks.\nEducational Qualifications:\nBachelor's or Master's in Computer Science, Data Engineering, Information Systems, or a related technical field.","Airflow, Luigi, CI CD tools, dbt, S3, PostgreSQL, Data Warehousing, Kafka, Schema Design, Data Modeling, MySQL, Shell scripting, Etl Tools, Python, AWS, BigQuery, Hadoop, Apache Spark, Redshift, Redis, Sql, Devops, Hive, Gcp, Databricks, MongoDB, DataFlow, Azure"
Data Architect - DX1,Maruti Suzuki,5-10 Years,,Gurugram,Automotive/Automobile/Ancillaries,"Job Title: Data Architect\nExperience: 5-10 Years\nJob Summary:\nWe are looking for an experienced and highly motivated Data Architect to join our team. The ideal candidate will have a strong background in architecture design, and implementing enterprise data solutions. You will play a critical role in shaping our data infrastructure, ensuring scalability, performance, and security across data platforms.\nKey Responsibilities:\n.Design and implement scalable data architectures for enterprise applications.\n.Develop and maintain conceptual, logical, and physical data models.\n.Define data governance policies and ensure data integrity and security.\n.Collaborate with stakeholders to identify data requirements and translate them into architectural solutions.\n.Lead the evaluation and selection of database technologies and tools.\n.Oversee data integration, data warehousing, and ETL/ELT processes.\n.Optimize database performance and manage data storage solutions.\n.Ensure alignment of data architecture with business and technology strategies.\nRequired Skills & Qualifications:\n.Bachelor's or Master's degree in Computer Science, Information Systems, or related field.\n.5-10 years of experience in data architecture, and database design.\n.Strong knowledge of relational (e.g., SQL Server).\n.Expertise in data warehousing, ETL tools (e.g., Informatica, Talend), and big data platforms (e.g., Hadoop, Spark).\n.Strong understanding of data governance, security, and compliance standards.\n.Experience with cloud data platforms (e.g., AWS Redshift, Azure Synapse, Google BigQuery) is a plus.\n.Excellent communication and stakeholder management skills.\nPreferred Certifications (optional):\n.AWS Certified Data Analytics - Specialty\n.Google Professional Data Engineer\n.Microsoft Certified: Azure Data Engineer Associate","Google BigQuery, SQL Server, Aws Redshift, Talend, Azure Synapse, Informatica, Hadoop, Spark"
Data Architect,DHL,Fresher,,"Indore, India",Login to check your skill match score,"Your IT Future, Delivered\n\nSolutions Architect\n\nWith a global team of 5800 IT professionals, DHL IT Services connects people and keeps the global economy running by continuously innovating and creating sustainable digital solutions. We work beyond global borders and push boundaries across all dimensions of logistics. You can leave your mark shaping the technology backbone of the biggest logistics company of the world. Our offices in Cyberjaya, Prague, and Chennai have earned #GreatPlaceToWork certification, reflecting our commitment to exceptional employee experiences.\n\nDigitalization. Simply delivered.\n\nAt IT Services, we are passionate about Solution Architect in datawarehouse and business intelligence space. Our Customer Service Complex Data Solution team is continuously expanding. No matter your level of Solution Architect proficiency, you can always grow within our diverse environment.\n\n#DHL #DHLITServices #GreatPlace #ppmt #Kart #cscombine\n\nGrow together.\n\nWe strive to deliver efficient and optimized business solutions in the Area of Customer Service Complex Data Solutions for our business. You will work as Solutions Architect for existing and new applications to provide end to end Architecture expertise on wide range of technologies like Snowflake, Teradata, Power BI, Matillion, Azure Cloud and many more.\n\nYou will be our main Architect providing guidance and direction on the implementation of Analytics, Data Warehousing & Reporting products. You will ensure that the Analytics & Reporting solutions meets the required performance benchmark and adheres to standards & guidelines.\n\nYou will work with project teams to ensure Business Requirements are delivered keeping in mind the end-to-end Solution & Data Architecture. You will get to work with some of the complex data structures that will need your expertise to Data Modelling & Design. You will be involved in Optimizing the performance and resource utilization of the existing solutions.\n\nYou will guide the development team with technical expertise for ensuring business requirements are implemented as expected. This would mean you sometime have to get down to coding and provide a solution or high-level approach to achieve the requirement to give direction to the Dev Team.\n\nAs a senior member in the team, you will collaborate with business users on Requirements and ensure that the requirements are well defined before assigning for development. Lead discussion with Business during UAT Defects review.\n\nYou will be working on latest technologies like Snowflake, Matilllion, Teradata, ERWIN, Microservices, Data pipelines, Jenkins, Jira/Confluence, Splunk etc. You will get ample opportunities to grow within the organization and with focus on continuous learning will get opportunity to work & learn many different technologies.\n\nReady to embark on the journey Here's what we are looking for:\n\nAs a Solution Architect, having excellent skill in understanding the latest technology relation to the business knowledge of customer service experience is a huge plus. Very good knowledge of data modeling will also be an integral part of this role and experience in implementation of customer facing application. Been part of the Agile / Scrum team experience is useful. Well versed in Architecture design, software development experiences especially in Python, familiarity of development framework and also analytics and problem solving skills.\n\nYou are a business intelligence technology aficionado, therefore you have a good understanding of latest analytics skill sets and experience in implementation of MVP and POC rapid prototyping experience is good to have also in the AI space of new technology adoptions. You are able to work independently prioritize and organize your tasks under time and workload pressure. Working in a multinational environment, you can expect cross-region collaboration with teams around the globe, thus being advanced in spoken and written English will be certainly useful. Basic certification / knowledge of AWS / Azure/ Snowflake/ Teradata/ Power BI related too is a plus.\n\nAn array of benefits for you:\n\nHybrid work arrangements to balance in-office collaboration and home flexibility.\nAnnual Leave: 42 days off apart from Public / National Holidays.\nMedical Insurance: Self + Spouse + 2 children. An option to opt for Voluntary\nParental Insurance (Parents / Parent -in-laws) at a nominal premium covering pre existing disease.\nIn House training programs: professional and technical training certifications.","Matillion, Reporting, Teradata, Analytics, snowflake, Data pipelines, Data Modelling, Power Bi, Jira, Microservices, Jenkins, Confluence, Azure Cloud, Data Warehousing, Splunk, Python"
IN_Senior Manager_ Big Data Architect _D&A_Advisory_Noida,PwC India,10-12 Years,,"Noida, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nFS X-Sector\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Manager\n\nJob Description & Summary\n\nA career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nCreating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nKey Responsibilities:\n\nProvide technical leadership regarding data strategy and roadmap exercises, data architecture definition, business intelligence/data warehouse product selection, design and implementation for the enterprise.\nProven track record of success in implementations for Data Lake, Business Intelligence and DW architecture.\nHands on experience in leading large-scale global data warehousing and analytics projects. Demonstrated industry leadership in the fields of database, data warehousing or data sciences\nBe accountable for creating end-to-end solution design and development approach in a big data environment including hardware and software recommendations\nShould have Deep technical expertise with Big Data technologies like Hadoop, Hive, Hbase, Spark, and 2-3 years of experience of cloud technologies. Real time streaming technologies and time series with tools such as Spark, Flink, Samza etc. Caching and queueing technologies Kafka/Kinesis etc.\nNoSQL understanding and use case application Cassandra, HBase, DynamoDB\nShould have worked extensively in creating re-usable assets for Data Integration, transformation, auditing and validation frameworks\nKnowledge of any Scripting/Programming skills Python, Java, Scala, Go\nImplementation and tuning experience of data warehousing platforms, including knowledge of data warehouse schema design, query tuning and optimization, and data migration and integration. Experience of requirements for the analytics presentation layer including dashboards, reporting, and OLAP\nExtensive experience in designing Data architecture, data modeling, design, development, data migration and data integration aspects of SDLC.\nParticipate and/or lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates\nShould have experience designing new or enhancing existing architecture frameworks and implementing them in a cooperative and collaborative setting\nTroubleshooting skills, ability to determine impacts, ability to resolve complex issues, and initiative in stressful situations\nContributed in Business Development activities.\nStrong oral and written communication and interpersonal skills\nWorking experience on Agile & Scrum methods\nDevelop documentation and maintain as needed\nSupport projects by providing SME knowledge to project teams in the areas of Enterprise Data Management\n\nMandatory Skill Sets\n\n\nBig Data Architect\n\nPreferred Skill Sets\n\nBig Data Architect\n\nYears Of Experience Required\n\n10+\n\nEducation Qualification\n\nBTech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nBig Data Architecture\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Intellectual Curiosity, Learning Agility, Optimism + 24 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Go, Flink, Samza, Real time streaming technologies, Data warehousing platforms, Cassandra, Kafka, Data Modeling, Nosql, Kinesis, Data Integration, Agile, Scrum, Python, Java, Data Migration, Hadoop, Scala, Dynamodb, Cloud Technologies, Data Architecture, HBase, Big Data Technologies, Hive, Spark"
Azure Data Architect,HDFC Bank,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Job Name: Azure Data Architect\nExperience: 12 years and above\nLocation: Mumbai ,Bangalore & Gurgaon\nKey Skills:\nFundamentals of DevOps, DevSecOps, CD / CI Pipeline using ADO\nGood understanding of MPP Architecture, MySQL, RDS, MS-SQL DB, Oracle ,Postgres DB\nELT - Trino, Azure Data factory, Azure Databricks, PySpark, Python, Iceberg, Parquet\nCDC Tool like Qlik/ Golden Gate/Dbsium/IBM CDC, Kafka/ Solace\nScripting Shell, Python, Java\n5 or more years of experience of software or application development and implementation\nExperience with data integration concepts\nDevelopment skill using Trino, PySpark and Databricks\nJob Role\nWork with Retail, Corporates, SME's and Fintech partners to implement and or co- develop cloud applications for the banks business, with particular emphasis on MS Azure, understanding competitive landscape, and prioritizing projects based on client impact\nDevelopment and Implement Data Engineering practice (Batch,CDC & Stream) by identifying the capabilities and enhancements to meet client-driven needs, leverage market opportunities, counter competitive threats, and comply with regulatory requirements.\nLead engagement with Business Analysis and core dev teams to create business requirements documentation to feed into the development process.\nStudy interfaces and providing consulting inputs for remediation and include changes if any interfaces","Dbsium, solace, CDC Tool, Trino, MS-SQL DB, CD CI Pipeline using ADO, Parquet, IBM CDC, MPP Architecture, Iceberg, Postgres DB, Oracle, Java, Devops, RDS, Pyspark, Kafka, Azure Databricks, MySQL, Qlik, Golden Gate, ELT, DevSecOps, Azure Data Factory, Python"
IN_Senior Manager_ Big Data Architect _D&A_Advisory_Noida,PwC India,10-12 Years,,"Noida, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nFS X-Sector\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Manager\n\nJob Description & Summary\n\nA career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nCreating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nKey Responsibilities:\n\nProvide technical leadership regarding data strategy and roadmap exercises, data architecture definition, business intelligence/data warehouse product selection, design and implementation for the enterprise.\nProven track record of success in implementations for Data Lake, Business Intelligence and DW architecture.\nHands on experience in leading large-scale global data warehousing and analytics projects. Demonstrated industry leadership in the fields of database, data warehousing or data sciences\nBe accountable for creating end-to-end solution design and development approach in a big data environment including hardware and software recommendations\nShould have Deep technical expertise with Big Data technologies like Hadoop, Hive, Hbase, Spark, and 2-3 years of experience of cloud technologies. Real time streaming technologies and time series with tools such as Spark, Flink, Samza etc. Caching and queueing technologies Kafka/Kinesis etc.\nNoSQL understanding and use case application Cassandra, HBase, DynamoDB\nShould have worked extensively in creating re-usable assets for Data Integration, transformation, auditing and validation frameworks\nKnowledge of any Scripting/Programming skills Python, Java, Scala, Go\nImplementation and tuning experience of data warehousing platforms, including knowledge of data warehouse schema design, query tuning and optimization, and data migration and integration. Experience of requirements for the analytics presentation layer including dashboards, reporting, and OLAP\nExtensive experience in designing Data architecture, data modeling, design, development, data migration and data integration aspects of SDLC.\nParticipate and/or lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates\nShould have experience designing new or enhancing existing architecture frameworks and implementing them in a cooperative and collaborative setting\nTroubleshooting skills, ability to determine impacts, ability to resolve complex issues, and initiative in stressful situations\nContributed in Business Development activities.\nStrong oral and written communication and interpersonal skills\nWorking experience on Agile & Scrum methods\nDevelop documentation and maintain as needed\nSupport projects by providing SME knowledge to project teams in the areas of Enterprise Data Management\n\nMandatory Skill Sets\n\n\nBig Data Architect\n\nPreferred Skill Sets\n\nBig Data Architect\n\nYears Of Experience Required\n\n10+\n\nEducation Qualification\n\nBTech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nBig Data Architecture\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Intellectual Curiosity, Learning Agility, Optimism + 24 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Go, Flink, Samza, Real time streaming technologies, Data warehousing platforms, Cassandra, Kafka, Data Modeling, Nosql, Kinesis, Data Integration, Agile, Scrum, Python, Java, Data Migration, Hadoop, Scala, Dynamodb, Cloud Technologies, Data Architecture, HBase, Big Data Technologies, Hive, Spark"
Data Architect,Siemens Healthineers,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Architect, you are required to:\n\nDesign & develop technical solutions which combine disparate information to create meaningful insights for business, using Big-data architectures\n\nBuild and analyze large, structured and unstructured databases based on scalable cloud infrastructures\nDevelop prototypes and proof of concepts using multiple data-sources and big-data technologies\nProcess, manage, extract and cleanse data to apply Data Analytics in a meaningful way\nDesign and develop scalable end-to-end data pipelines for batch and stream processing\nRegularly scan the Data Analytics landscape to stay up to date with latest technologies, techniques, tools and methods in this field\nStay curious and enthusiastic about using related technologies to solve problems and enthuse others to see the benefit in business domain\n\nQualification:\n\nBachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Engineering / Analytics is desirable.\n\nExperience level:\n\nMinimum 8 years in software development with at least 2 - 3 years hands-on experience in the area of Big-data / Data Engineering.\n\nDesired Knowledge & Experience:\n\nData Engineer - Big Data Developer\n\nSpark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming\nKnowing Spark internals: Catalyst/Tungsten/Photon\nDatabricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader\nIDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot\nTest: pytest, Great Expectations\nCI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing\nBig Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction\nLanguages: Python/Functional Programming (FP)\nSQL: TSQL/Spark SQL/HiveQL\nStorage: Data Lake and Big Data Storage Design\n\nAdditionally it is helpful to know basics of:\n\nData Pipelines: ADF/Synapse Pipelines/Oozie/Airflow\nLanguages: Scala, Java\nNoSQL: Cosmos, Mongo, Cassandra\nCubes: SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model\nSQL Server: TSQL, Stored Procedures\nHadoop: HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka\nData Catalog: Azure Purview, Apache Atlas, Informatica\nBig Data Architect\nExpert: in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer\nMentor: mentors/educates Developers in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer\nArchitecture Styles: Lakehouse, Lambda, Kappa, Delta, Data Lake, Data Mesh, Data Fabric, Data Warehouses (e.g. Data Vault)\nApplication Architecture: Microservices, NoSql, Kubernetes, Cloud-native\nExperience: Many years of experience with all kinds of technology in the evolution of data platforms (Data Warehouse -> Hadoop -> Big Data -> Cloud -> Data Mesh)\nCertification: Architect certification (e.g. Siemens Certified Software Architect or iSAQB CPSA)\n\nRequired Soft-skills & Other Capabilities:\n\nExcellent communication skills, in order to explain your work to people who don't understand the mechanics behind data analysis\nGreat attention to detail and the ability to solve complex business problems\nDrive and the resilience to try new ideas, if the first ones don't work\nGood planning and organizational skills\nCollaborative approach to sharing ideas and finding solutions\nAbility to work independently and also in a global team environment.","Big Data Storage Design, Nosql, Hadoop, Spark, Data Lake, Databricks, Sql, Python, Azure DevOps"
Assistant Manager - Data Architect,KPMG India,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nAbout KPMG in India\n\nKPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.\n\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\n\nThe person will work on a variety of projects in a highly collaborative, fast-paced environment. The person will be responsible for software development activities of KPMG, India. Part of the development team, he/she will work on the full life cycle of the process, develop code and unit testing. He/she will work closely with Technical Architect, Business Analyst, user interaction designers, and other software engineers to develop new product offerings and improve existing ones. Additionally, the person will ensure that all development practices are in compliance with KPMG's best practices policies and procedures. This role requires quick ramp up on new technologies whenever required.\n\nResponsibilities\n\nRole Fabric Data E ngineer\n\nLocation Bangalore\n\nExperience 6 to 8 Years\n\nKey Responsibilities :-\n\n6+ years of experience as a Data Platform Architect, with demonstrated expertise in designing, managing, and optimizing data warehouses, lakes, and lakehouses.\nProficiency with data storage formats such as Parquet, ORC, and AVRO, as well as storage layers like Delta Lake and other transactional storage solutions\nIn-depth knowledge of data technologies, including Databricks, Snowflake, and similar platforms\nStrong understanding of Business Intelligence (BI) tools such as Power BI, Tableau, and other analytics tools\nExperience with data integration and ETL tools such as Azure Data Factory, Talend, Ab Initio, or equivalent\nProven experience with Microsoft Fabric or similar data platforms\nPython + Spark is must.\nAdvanced SQL skills and solid expertise in database design principles\nExperience in data modeling, particularly in data warehouse and lakehouse design\nKnowledge of the Azure Cloud Platform, especially in relation to data warehousing and storage solutions\nBackground in data integration and engineering, including familiarity with data pipelines and storage optimization.\nStrong grasp of data governance, data security, and compliance requirements\nProblem-solving skills with a track record of resolving complex technical issues.\nExcellent communication skills, capable of conveying technical concepts to both technical and non-technical stakeholders.\nAbility to work independently and as part of a collaborative team.\nMicrosoft certifications in data-related fields are preferred.\n\nQualifications\n\n\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n\nEqual Opportunity Employer\n\nKPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","orc, Compliance, snowflake, Azure Cloud Platform, Parquet, Data Platform Architect, Delta Lake, Microsoft Fabric, Data Modeling, Tableau, Data Integration, Database Design, Data Governance, Talend, Python, Power Bi, Avro, Sql, Azure Data Factory, Spark, Databricks, Data Warehousing, Ab Initio, Data Security"
IN-Director_Senior Data Architect_Data & Analytics_Advisory_ Bangalore,PwC India,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nDirector\n\nJob Description & Summary\n\nAt PwC, our people in business application consulting specialise in consulting services for a variety of business applications, helping clients optimise operational efficiency. These individuals analyse client needs, implement software solutions, and provide training and support for seamless integration and utilisation of business applications, enabling clients to achieve their strategic objectives.\n\nAs a business application consulting generalist at PwC, you will provide consulting services for a wide range of business applications. You will leverage a broad understanding of various software solutions to assist clients in optimising operational efficiency through analysis, implementation, training, and support.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary: We are seeking an experienced Senior Data Architect to lead the design and development of our data architecture, leveraging cloud-based technologies, big data processing frameworks, and DevOps practices. The ideal candidate will have a strong background in data warehousing, data pipelines, performance optimization, and collaboration with DevOps teams.\n\nResponsibilities\n\nDesign and implement end-to-end data pipelines using cloud-based services (AWS/ GCP/Azure) and conventional data processing frameworks.\nLead the development of data architecture, ensuring scalability, security, and performance.\nCollaborate with cross-functional teams, including DevOps, to design and implement data lakes, data warehouses, and data ingestion/extraction processes.\nDevelop and optimize data processing workflows using PySpark, Kafka, and other big data processing frameworks.\nEnsure data quality, integrity, and security across all data pipelines and architectures.\nProvide technical leadership and guidance to junior team members.\nDesign and implement data load strategies, data partitioning, and data storage solutions.\nCollaborate with stakeholders to understand business requirements and develop data solutions to meet those needs.\nWork closely with DevOps team to ensure seamless integration of data pipelines with overall system architecture.\nParticipate in design and implementation of CI/CD pipelines for data workflows.\n\nDevOps Requirements\n\n\nKnowledge of DevOps practices and tools, such as Jenkins, GitLab CI/CD, or Apache Airflow.\nExperience with containerization using Docker.\nUnderstanding of infrastructure as code (IaC) concepts using tools like Terraform or AWS CloudFormation.\nFamiliarity with monitoring and logging tools, such as Prometheus, Grafana, or ELK Stack.\n\nRequirements\n\n\n12-14 years of experience for Senior Data Architect\n\nin data architecture, data warehousing, and big data processing.\n\nStrong expertise in cloud-based technologies (AWS/ GCP/ Azure) and data processing frameworks (PySpark, Kafka, Flink , Beam etc.).\nExperience with data ingestion, data extraction, data warehousing, and data lakes.\nStrong understanding of performance optimization, data partitioning, and data storage solutions.\nExcellent leadership and communication skills.\nExperience with NoSQL databases is a plus.\n\nMandatory Skill Sets\n\n\nExperience with agile development methodologies.\nCertification in cloud-based technologies (AWS / GCP/ Azure) or data processing frameworks.\nExperience with data governance, data quality, and data security.\n\nPreferred Skill Sets\n\n\nKnowledge of AgenticAI and GenAI is added advantage\n\nYears Of Experience Required\n\n12 to 14 syears\n\nEducation Qualification\n\nGraduate Engineer or Management Graduate\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Innovation, Intellectual Curiosity, Learning Agility + 28 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date","Flink, Beam, GitLab CI CD, Pyspark, Prometheus, Kafka, Grafana, Elk Stack, Apache Airflow, Jenkins, Gcp, Docker, Terraform, AWS CloudFormation, Azure, AWS"
IN_Senior Manager_ Big Data Architect _D&A_Advisory_Noida,PwC India,10-12 Years,,"Noida, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nFS X-Sector\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nSenior Manager\n\nJob Description & Summary\n\nA career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\n\nCreating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nKey Responsibilities:\n\nProvide technical leadership regarding data strategy and roadmap exercises, data architecture definition, business intelligence/data warehouse product selection, design and implementation for the enterprise.\nProven track record of success in implementations for Data Lake, Business Intelligence and DW architecture.\nHands on experience in leading large-scale global data warehousing and analytics projects. Demonstrated industry leadership in the fields of database, data warehousing or data sciences\nBe accountable for creating end-to-end solution design and development approach in a big data environment including hardware and software recommendations\nShould have Deep technical expertise with Big Data technologies like Hadoop, Hive, Hbase, Spark, and 2-3 years of experience of cloud technologies. Real time streaming technologies and time series with tools such as Spark, Flink, Samza etc. Caching and queueing technologies Kafka/Kinesis etc.\nNoSQL understanding and use case application Cassandra, HBase, DynamoDB\nShould have worked extensively in creating re-usable assets for Data Integration, transformation, auditing and validation frameworks\nKnowledge of any Scripting/Programming skills Python, Java, Scala, Go\nImplementation and tuning experience of data warehousing platforms, including knowledge of data warehouse schema design, query tuning and optimization, and data migration and integration. Experience of requirements for the analytics presentation layer including dashboards, reporting, and OLAP\nExtensive experience in designing Data architecture, data modeling, design, development, data migration and data integration aspects of SDLC.\nParticipate and/or lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates\nShould have experience designing new or enhancing existing architecture frameworks and implementing them in a cooperative and collaborative setting\nTroubleshooting skills, ability to determine impacts, ability to resolve complex issues, and initiative in stressful situations\nContributed in Business Development activities.\nStrong oral and written communication and interpersonal skills\nWorking experience on Agile & Scrum methods\nDevelop documentation and maintain as needed\nSupport projects by providing SME knowledge to project teams in the areas of Enterprise Data Management\n\nMandatory Skill Sets\n\n\nBig Data Architect\n\nPreferred Skill Sets\n\nBig Data Architect\n\nYears Of Experience Required\n\n10+\n\nEducation Qualification\n\nBTech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nBig Data Architecture\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Intellectual Curiosity, Learning Agility, Optimism + 24 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","Go, Flink, Samza, Real time streaming technologies, Data warehousing platforms, Cassandra, Kafka, Data Modeling, Nosql, Kinesis, Data Integration, Agile, Scrum, Python, Java, Data Migration, Hadoop, Scala, Dynamodb, Cloud Technologies, HBase, Data Architecture, Big Data Technologies, Hive, Spark"
IN_Director_Data Architect_D&A_Advisory_Gurgaon,PwC India,15-17 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nDirector\n\nJob Description & Summary\n\nAt PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.\n\nIn business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nResponsibilities\n\nAbout the Role:\n\nWe are seeking a highly experienced and motivated Tech Director to lead our data engineering team and spearhead the architecture, development, and implementation of our next-generation data platform, with a strong emphasis on Databricks and the Hadoop ecosystem. The ideal candidate is a proven technical leader with a deep understanding of big data technologies, distributed systems, and a passion for building scalable, high-performance data pipelines.\n\nKey Responsibilities\n\nLeadership & Strategy:\nHire, lead and mentor a team of data engineers, providing technical guidance and fostering a culture of innovation and collaboration.\nDefine the strategic vision and roadmap for the data platform, aligning with business objectives and industry best practices.\nStay abreast of emerging technologies and trends in the big data and distributed computing landscape.\nBuild POVs around modern Data platforms.\nGuide team to build accelerators/prototypes.\nMandatory Skill Sets\n\nData Architect\n\nPreferred Skill Sets\n\n\nData Architect\n\nYears Of Experience Required\n\n\n15+\n\nEducation Qualification\n\nBE/BTech/MBA/MCA\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Master of Business Administration, Bachelor of Engineering\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nDatabase Architecture\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Coaching and Feedback, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion + 24 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nAvailable for Work Visa Sponsorship\n\nGovernment Clearance Required\n\nJob Posting End Date","Data Analysis and Interpretation, Data Pipeline, Databricks, Data Architecture, Data Quality, Hadoop Ecosystem, Data Visualization"
Data Architect,Compunnel Technology India Private Limited,7-12 Years,,"Hyderabad, Chandigarh, Noida",Software,"Analyse current state of inventory of data sources, discover ETL activities & processes adopted, understand data quality & lineage.\nDiscover and analyse Metadata, Master data and Reference data\nAnalyse Data Access Management, self-service BI Practices in a system\nAnalyse data life cycle (Important points of access, acquire, transport, store, query, manage, secure, and share)\nDiscover current state of Databases Availability, Backup, Recovery Mechanism\nUnderstand the system of data virtualisation and related tools like Denodo\nEvaluate current state and recommend future roadmap for the organisation data lake, data warehousing and cloud storage requirements (Azure, Snowflake, AWS for semi-structured and unstructured data)\nDesign data pipeline architecture for data lake & data warehouse (like Azure Data Factory)\nEvaluate ETL run-time and recommend suitable schema designs.\nRecommend cloud-based data warehouse and analytics solution (Azure Data Factory, Synapse Analytics and Data bricks, AWS services, Snowflake)\nAnalyse structural requirements for new software, hardware and applications\nUnderstand business & technical requirements to migrate data from legacy systems to new solutions\nBuild data models for database structures, analytics and AI applications.\nImprove & elaborate system performance parameters\nEvaluate & Optimize new and current database systems\nProvide insight into the changing database storage and utilization requirements and offer suggestions\nIntegrate new systems and functions like security, performance, scalability, reliability and data recovery.\nCollaborate in a data strategy that meets the industry requirements\nEnvision data pipelines and how data will flow through the enterprise\nEvaluate current data management technologies, policies and suggest improvements\nDesign, document, build and implement database architectures and applications.\nDevelop measures that ensure data accuracy, integrity and accessibility.\nEnsure that the data architecture is scalable and maintainable\nSuggest suitable data architecture landscape for integration and scaling\nUnderstand granular details of current MIS\nRequirements and skills\nBachelor's/Master's Degree in Computer Engineering or degree/certification in data architecture\nProven work experience as a Data Architect, Data warehouse designer, Data Engineer, Data Scientist, Data Analyst or similar role\nIn-depth understanding of database structure principles\nFamiliarity with data virtualisation tools\nData management and reporting technologies, data visualization and structured/unstructured data management\nStrong business and communication skills\nGood understanding of key architecture concerns such as availability, scalability, operability and maintainability","Data Architect, Etl, AWS"
Sr. Staff Data Architect,Warner Bros. Discovery,6-10 Years,,Hyderabad,Media and Entertainment,"Roles and Responsibilities\nDesign and implement enterprise data architecture solutions.\nDevelop and maintain data models, including canonical data models, to standardize data across the organization.\nImplement and manage Master Data Management (MDM) solutions to ensure data consistency and accuracy.\nDevelop and enforce data governance frameworks to ensure data quality, security, and compliance.\nEnsure data quality by implementing best practices and tools for data validation, cleansing, and enrichment.\nCollaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.\nCreate conceptual and logical data models and flowcharts.\nProvide technical expertise and guidance on data architecture best practices and emerging technologies.\nOversee the integration of data from various sources into a unified data architecture.\nEnsure the scalability and performance of data architecture solutions to support growing data volumes and complexity.\nOptimize resources to ensure cost-effective and efficient data architecture solutions.\nDesign and implement different data architectures, including data warehouses, data lakes, and real-time data processing systems.\nLeverage AI capabilities to enhance data processing, analysis, and decision-making.\nCollaborate with business, management, and data scientists to align data architecture with organizational goals.\nStay up to date with the latest trends and technologies in data architecture and AI and apply them to improve our data infrastructure.\nQualifications & Experiences\nAdvanced degree in Computer Science, Data Science, Engineering, or a related field\n1 2 + years experience with data platforms, cloud services (e.g., AWS, Azure, Google Cloud), and big data technologies\nExtensive experience with AWS services , Snowflake and its ecosystem\n1 0 + years experience in data architecture, data modeling, and database design\nLead the design and implementation of data architecture from scratch at the enterprise level, ensuring scalability, reliability, and performance\nProficiency in data warehousing, ETL processes, data integration, and analytics tools\nExperience in Media and Entertainment industry is preferred\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication skills.\nAbility to work effectively with cross-functional teams.","Computer Science, Data Science, Data Modelling, Azure, Google Cloud, Aws, Etl Process, Data Architecture"
Data Architect,SBM Offshore,2-5 Years,,Bengaluru,Oil and Gas,"SBM Offshore created an Operations Data Architect Team under the Operations Solutions Management department to govern the data from entry to its destruction. The goal is to embrace more digitally automated functions across the entire asset lifecycle. The team is responsible to manage the process related data structuration, enrichment, consolidation, contextualization, and quality through the full operations lifecycle. The team also coordinate with business specialists to guarantee the data modeling is fit for purpose and aligned with the Smart Services ITS Data Strategy.\nAmong digital applications, the suite of tools has been put in place in order to leverage on the data intelligence for reporting, analysis and prediction purpose:\nIFS as ERP solution covering HSSE, Finance, CMMS areas\nAveva PI suite for timeseries historian and structuration\nFieldbox (vendor) for display front end and manual entry, analytics and predictive algorithms\nMS Power BI for reporting\nMicrosoft Azure technologies\nSeeq\nData Architect with expertise in Aveva PI and PI Asset Framework. The Data Architect will be responsible for designing and maintaining data architectures, structures, timeseries analytics, creating data models, implementing data governance policies, and ensuring the efficient storage and retrieval of data.\nRESPONSIBILITIES\nDevelop and maintain data architecture and data models for all Digital Solutions within the Operations Portfolio such as: Aveva PI and PI Asset Framework, Seeq, Cognite\nCollaborate with Business stakeholders to understand their data needs and requirements, and design appropriate solutions.\nDesign, implement, and maintain data governance policies and procedures to ensure data quality, security, and compliance.\nDevelop, test and deploy and maintain simple and complex analyses\nWork with the development team to ensure efficient data storage and retrieval and optimize system performance.\nCreate data mapping and integration strategies to ensure seamless data flow across multiple systems.\nConduct data analysis and provide insights to support decision-making and business strategy.\nStay up-to-date with the latest industry trends, technologies, and best practices related to data architecture.\nJOB REQUIREMENTS\nEducation: Bachelors degree in Engineering, Computer Science, Information Systems, or related field\nExperience: 2-5 of experience in Aveva PI, Time series\nSpecific competencies\nSolid understanding of data modeling, database design, and data governance principles.\nProficient in SQL, database programming, and data integration technologies.\nStrong analytical and problem-solving skills, with the ability to analyze complex data sets and provide insights.\nExcellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nFamiliarity with industry standards like CFIHOS.\nExperience with cloud-based data platforms such as AWS, Azure, or Google Cloud is a plus.\nCertification in Aveva PI or PI Asset Framework is highly desirable.\nExperience in Seeq is desirable\nExperience in MS Azure IOT framework would be a differentiator","Database Programming, Data Architect, Azure, Iot, Sql, AWS"
Data Architect,Norstella,8-10 Years,,India,Login to check your skill match score,"Description\n\nAbout Norstella\n\nAt Norstella, our mission is simple: to help our clients bring life-saving therapies to market quickerand help patients in need.\n\nFounded in 2022, but with history going back to 1939, Norstella unites best-in-class brands to help clients navigate the complexities at each step of the drug development life cycle and get the right treatments to the right patients at the right time.\n\nEach Organization (Citeline, Evaluate, MMIT, Panalgo, The Dedham Group) Delivers Must-have Answers For Critical Strategic And Commercial Decision-making. Together, Via Our Market-leading Brands, We Help Our Clients\n\nCiteline accelerate the drug development cycle\nEvaluate bring the right drugs to market\nMMIT identify barrier to patient access\nPanalgo turn data into insight faster\nThe Dedham Group think strategically for specialty therapeutics\n\nBy combining the efforts of each organization under Norstella, we can offer an even wider breadth of expertise, cutting-edge data solutions and expert advisory services alongside advanced technologies such as real-world data, machine learning and predictive analytics.\n\nAs one of the largest global pharma intelligence solution providers, Norstella has a footprint across the globe with teams of experts delivering world class solutions in the USA, UK, The Netherlands, Japan, China and India.\n\nJob Description\n\nHave you wondered how life saving drugs and therapies are created, tested, marketed and\n\nmade available to patients in need Have you wondered how clinical trials are conducted at\n\na global scale How governments and health authorities regulate various organizations\n\nparticipating in this marketplace Have you wondered how those companies and insurance\n\nproviders price a certain drug, and how a care provider determines the right treatment for\n\na given patient If yes, Norstella could the next step in your career.\n\nWe are looking for a Data Engineer with a strong background in cloud data warehousing,\n\ndata pipelines, and ETL development. The ideal candidate will have extensive experience\n\nwith AWS services, Python, and advanced SQL, coupled with a solid understanding of data\n\nmodeling and ETL testing. The role requires a candidate who is proactive, detail-oriented,\n\nand capable of leading projects within a collaborative team environment.\n\nKey Requirements\n\nCloud Data Architecture Design:\n\nDesign and implement scalable, high-performance data models and architectures\n\nusing cloud data warehousing concepts.\n\nDevelop and maintain data models (including Snowflake and Star Schema) for both\n\nstructured and unstructured data, ensuring optimal performance and reliability\n\nacross AWS services (e.g., S3, Redshift, Glue, and Lambda).\n\nData Pipeline And ETL Development\n\nBuild and manage data pipelines to ensure efficient data ingestion, processing, and\n\nintegration, utilizing tools like AWS Glue, Airflow, and Pyspark.\n\nImplement ETL processes to transform and load data from various sources into\n\nSnowflake, Redshift, PostgreSQL, and other platforms, ensuring data completeness\n\nand quality.\n\nAdvanced SQL And RDBMS Management\n\nLeverage advanced SQL (including joins, subqueries, CTEs) and RDBMS concepts to\n\ndevelop and optimize complex queries, with a preference for RDS SQL Server.\n\nManage AWS RDS instances, specifically PostgreSQL, ensuring robust data storage\n\nand retrieval processes.\n\nCollaboration With Data Science Team\n\nWork closely with Data Scientists to understand their data needs, ensuring data\n\navailability and quality for real-world data (RWD) analysis and modeling.\n\nProvide Python and Pyspark-based data support, troubleshooting, and performance\n\ntuning for data science projects.\n\nLarge Data Set Management\n\nHandle large data sets with a focus on performance optimization, including\n\nimplementing strategies for data partitioning, indexing, and caching within AWS\n\necosystems.\n\nOptimize the querying of large data sets to enhance performance and ensure\n\nefficient data processing.\n\nPerformance Tuning And ETL Testing\n\nMonitor and optimize data systems for performance, including query optimization,\n\nresource management, and AWS DevOps CI/CD pipelines.\n\nPerform ETL testing to validate data completeness and quality across various data\n\nfeeds, resolving any bottlenecks in data processing and retrieval.\n\nData Delivery And Governance Ownership\n\nTake ownership of data delivery processes, ensuring data is accurate, timely, and\n\naccessible, while establishing and maintaining robust data governance policies and\n\nprocedures.\n\nEnsure data infrastructure is scalable, cost-effective, and aligns with industry best\n\npractices, particularly in the life sciences/pharma domain.\n\nLife Science Data Expertise\n\nApply deep knowledge of life science data and industry-specific requirements to\n\ninform data architecture and modeling decisions, ensuring compliance with relevant\n\nregulations and standards.\n\nDemonstrate strong leadership and a positive attitude, embodying Norstella's\n\nprinciples in collaboration and project execution.\n\nRequired Skills And Qualifications\n\nCloud Data Warehousing Concepts: Strong understanding of cloud data warehousing architectures and best practices.\nData Pipelines/ETL Development: Proven experience in designing and implementing data pipelines and ETL processes.\nRDS Postgres: Hands-on experience with AWS RDS, specifically Postgres.\nPython & Pyspark: Proficiency in Python and Pyspark for data manipulation and transformation.\nAWS Services: Experience with AWS ECS, Lambda, API Gateway, S3, RDS, Glue, and Airflow.\nRDBMS & Advanced SQL: Expertise in RDBMS and advanced SQL, including joins, subqueries, CTEs, and complex query writing, with a preference for RDS SQL Server.\nData Modeling: Understanding of data modeling concepts, including Snowflake and Star Schema.\nETL Testing: Experience with ETL testing, focusing on data completeness and quality.\nAWS DevOps CI/CD: Experience with AWS DevOps tools and CI/CD pipelines.\nLife Sciences/Pharma Domain Knowledge: Familiarity with the life sciences or pharmaceutical domain.\nSoft Skills: Strong leadership attitude, aligns with Norstella principles, and exhibits a positive and collaborative work attitude.\nEducation: Minimum bachelor's degree in computer science and engineering or related field of study, or equivalent experience. 8+ years of experience as a Data Architect or in a similar role, with demonstrated expertise in the required skills.\n\nThe guiding principles for success at Norstella\n\n01: Bold, Passionate, Mission-First\n\nWe have a lofty mission to Smooth Access to Life Saving Therapies and we will get there by being bold and passionate about the mission and our clients. Our clients and the mission in what we are trying to accomplish must be in the forefront of our minds in everything we do.\n\n02: Integrity, Truth, Reality\n\nWe make promises that we can keep, and goals that push us to new heights. Our integrity offers us the opportunity to learn and improve by being honest about what works and what doesn't. By being true to the data and producing realistic metrics, we are able to create plans and resources to achieve our goals.\n\n03: Kindness, Empathy, Grace\n\nWe will empathize with everyone's situation, provide positive and constructive feedback with kindness, and accept opportunities for improvement with grace and gratitude. We use this principle across the organization to collaborate and build lines of open communication.\n\n04: Resilience, Mettle, Perseverance\n\nWe will persevere even in difficult and challenging situations. Our ability to recover from missteps and failures in a positive way will help us to be successful in our mission.\n\n05: Humility, Gratitude, Learning\n\nWe will be true learners by showing humility and gratitude in our work. We recognize that the smartest person in the room is the one who is always listening, learning, and willing to shift their thinking.\n\nBenefits\n\nHealth Insurance\nProvident Fund\nLife Insurance\nReimbursement of Certification Expenses\nGratuity\n24x7 Health Desk\n\nNorstella is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people's differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual's abilities, skills, performance and behavior and our business requirements. Norstella operates a zero tolerance policy to any form of discrimination, abuse or harassment.\n\n\nSometimes the best opportunities are hidden by self-doubt. We disqualify ourselves before we have the opportunity to be considered. Regardless of where you came from, how you identify, or the path that led you here- you are welcome. If you read this job description and feel passion and excitement, we're just as excited about you.","snowflake, Airflow, Aws Rds, Etl Development, Data Modeling, Aws Services, AWS Glue, Pyspark, Star Schema, Data Governance, Python, PostgreSQL, Advanced Sql"
Data Architect,Grid Dynamics,12-16 Years,,"Bengaluru, India",Login to check your skill match score,"Experience: 12-16Years(only)\nLocation: Bangalore/Hyderabad\nA result- oriented thought-leader to drive the development of the data engineering practice\nA trusted advisor and business partner to customers across verticals, and consulting team leader who establishes engineering processes and skill development.\nResponsibilities:\nTrusted Advisor: Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.\nPre-Sales and Consulting: Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.\nTechnology Strategy and R&D: Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing\nassets and solutions strategy across multiple industries.\nEngineering: Working with Grid Dynamics's delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.\nBusiness Development & Partnership:\nManage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.\nRequirements:\nExtensive practical experience in Big Data engineering, data governance, and cloud data\nplatforms.\nStrong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.\nStrong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.\nDeep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.\nExperience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.\nManaged key client relations worldwide and advised global technology and business leaders on innovation and data strategy.\nExperience with Big 4 consulting is a plus.\nTechnology skills (any of below):\nData engineering: analytical data platforms, streaming, big data, EDW, data lakes, data governance, data mesh, Spark, Kafka, Snowflake, (2 from below: AWS, GCP, Azure) Transactional databases: Redis, Cassandra, MongoDB, (2 from below: AWS, GCP, Azure) ML and MLOps: VertexAI, Sagemaker, Dataiku, Databricks, mlflow\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, and advanced analytics services. Fusing technical vision with business acumen, we enable positive business outcomes for enterprise companies undergoing business transformation by solving their most pressing technical challenges. A key differentiator for Grid Dynamics is our 7+ years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization, and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India. Follow us on LinkedIn.","Big Data engineering, data aggregation, cloud-based architectures, ML platforms, data lakes, Sagemaker, snowflake, ML and MLOps, Reporting, streaming big data, Data Collection, mlflow, analytical data platforms, Dataiku, Transactional databases, data mesh, VertexAI, Cassandra, BI, Kafka, Edw, AWS, Redis, data engineering, Gcp, Spark, Data Governance, Databricks, MongoDB, Azure"
Data Architect,eInfochips (An Arrow Company),10-12 Years,,"Ahmedabad, India",Login to check your skill match score,"Role: Data Architect Data Science and Azure Cloud\nYears of Experience: 10+ Years\nLocation: Ahmedabad\nWhat You Will Be Doing:\nCollaborate with stakeholders to translate business requirements into scalable data architecture solutions.\nDesign data systems that support machine learning, advanced data science, and generative AI models.\nBuild and maintain data pipelines, integration flows, and storage solutions for efficient data handling.\nDefine and uphold data governance standards ensuring data quality, compliance, and security.\nMentor junior data scientists, data engineers, and developers on building end-to-end data solutions.\nProvide architectural guidance and best practices across technical teams.\nStay updated with trends in Azure Cloud, Data Science, and Generative AI to introduce relevant technologies.\nPerform data profiling, detect quality issues, and recommend solutions for better data reliability.\nWork with IT to optimize Azure cloud environments for data storage, retrieval, and processing.\nWhat Are We Looking For:\nBachelor's or Master's in Computer Science, Data Science, or a related field.\nDeep expertise in data modeling, statistical analysis, and machine learning.\nHands-on experience with Azure Data Factory, Azure Databricks, Azure OpenAI, and Azure ML.\nKnowledge of generative AI (GANs, VAEs) is a strong plus.\nProven track record of designing scalable, enterprise-grade data architectures.\nStrong grasp of data governance, management, and security principles.\nProficiency in Python, R, or Scala for data manipulation and analysis.\nFamiliarity with modern storage solutions data lakes, warehouses, and relational DBs.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and cross-team collaboration skills.\nCertifications in Azure, Data Science, ML, or Generative AI are highly preferred.","relational DBs, R, data lakes, Azure OpenAI, Machine Learning, Azure Databricks, Data Modeling, Python, Azure Data Factory, Statistical Analysis, Scala, Azure ML"
Data Architect,VLink Inc,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"VLink Inc Hiring: Data Architect | Bangalore | Hybrid Work Mode\nWe're looking for expert Data Architects with 1215 years of experience to join us immediately in Bangalore.\nWe are seeking experienced Data Architects to design and implement enterprise data solutions, ensuring data governance, quality, and advanced analytics capabilities. The ideal candidate will have expertise in defining data policies, managing metadata, and leading data migrations from legacy systems to Microsoft Fabric/DataBricks/Snowflake. Experience and deep knowledge about at least one of these 3 platforms is critical. Additionally, they will play a key role in identifying use cases for advanced analytics and developing machine learning models to drive business insights.\nKey Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems. Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP). Strong communication skills with the ability to translate complex data concepts into business insights.\nPreferred candidate profile\nData Modeling (Conceptual, Logical, Physical)- Minimum 5 years\nDatabase Technologies (SQL Server, Oracle, PostgreSQL, NoSQL)- Minimum 5 years\nCloud Platforms (AWS, Azure, GCP) - Minimum 3 Years\nETL Tools (Informatica, Talend, Apache Nifi) - Minimum 3 Years\nBig Data Technologies (Hadoop, Spark, Kafka) - Minimum 5 Years\nData Governance & Compliance (GDPR, HIPAA) - Minimum 3 years\nMaster Data Management (MDM) - Minimum 3 years\nData Warehousing (Snowflake, Redshift, BigQuery)- Minimum 3 years\nAPI Integration & Data Pipelines - Good to have.\nPerformance Tuning & Optimization - Minimum 3 years\nbusiness Intelligence (Power BI, Tableau)- Minimum 3 years\nInterested candidate can share their updated profile on below mentioned mail:-\n[HIDDEN TEXT]\nRegards As Ever\nAnkit Malik","snowflake, Microsoft Fabric, Master Data Management, Data Quality Management, Oracle, Apache Nifi, Hipaa, Data Warehousing, Hadoop, Tableau, Kafka, BigQuery, Power Bi, Etl, Nosql, Informatica, PostgreSQL, SQL Server, Gdpr, Metadata Management, Machine Learning, Talend, Data Governance, Data Modeling, Redshift, Spark, DataBricks"
AI & Data Architect - Azure,CloudFronts - Microsoft Solutions Partner,5-7 Years,,"Mumbai, India",Login to check your skill match score,"AI & Data Architect - Azure\nJob Summary:\nAI & Data Architect will lead all aspects of AI, Business Intelligence, and Data Architecture with expertise in Azure services, including AI/ML solutions, data engineering, and analytics. The role involves designing, configuring, and managing BI services, aggregating data from multiple sources into a data warehouse, implementing AI-driven insights, and deploying AI models while adhering to best practices.\nSkills, Experience & Key Responsibilities :\n5+ years of experience in AI, Data Engineering, and Analytics with Azure cloud technologies.\n5+ years of integration experience with the Azure Platform (Azure Data Factory, Azure Data Lake, Azure Synapse, Azure Databricks, Azure Machine Learning, Azure Cognitive Services, Logic Apps, API Management).\nHands-on experience with AI/ML models, NLP, Computer Vision, and Predictive Analytics using Azure AI services.\nArchitect and manage AI-driven BI solutions (portals, dashboards, standard, and ad-hoc reporting) with a focus on data management and AI-driven insights.\nExperience in developing and deploying AI models into production environments.\nDeep analytical experience and understanding of data modeling and big data solutions.\nStrong knowledge of requirements gathering, documentation processes, and stakeholder management.\nProficiency in programming languages such as Python, R, or SQL for AI/ML model development.\nDesigning & developing dimensions, hierarchies & cubes with Azure Synapse & Databricks.\nProvide technical direction and mentoring to a team of AI, BI, and Data Engineers working with enterprise data tools (SSRS, SSIS, SQL Server, Power BI, Azure ML, Databricks).\nEnsures best practices in AI model development, testing, and deployment while overseeing, planning, and estimating project needs.\nProvide technical leadership on client AI & Data projects and during the sales cycle.\nLocation: Mumbai\nJob Type: Full Time (5days WFO)\nWorking Hours: 8.30 am to 5.00 pm (Monday to Friday)\nExperience Range: 5+ years of experience (Relevant)\nABOUTCLOUDFRONTS:\nCloudFronts is a 100% Dynamics 365 focused Microsoft Solutions Partner helping Teams & Organizations worldwide solve their Complex Business Challenges with Microsoft Cloud. Our head office and robust delivery center are based out of Mumbai, India along with branch offices in Singapore & U.S.\nCloudFronts was established in 2012 by a former Microsoft CRM Solution Architect Anil Shah with a mission to help other businesses scale up their productivity and reduce their costs concurrently with Microsoft Dynamics. Since its inception, the CloudFronts team has successfully served over 500+ small and medium-sized clients all over the world such as North America, Europe, Australia, Maldives & India with diverse experiences in sectors ranging from Professional services, Finances, Pharmaceutical, Manufacturing, F&B, Retail, Logistics, Energy, Automotive and non-profits.\nOur customer success stories and testimonials speak for us. We urge you to look at https://www.cloudfronts.com/dynamics-365-customer-success-stories/\nExplore the power of Microsoft Dynamics at www.cloudfronts.com","Analytics, Azure Cognitive Services, R, Logic Apps, AI ML solutions, Sql, SQL Server, SSIS, Computer Vision, Azure Data Factory, Ssrs, Azure Machine Learning, Power Bi, Azure Databricks, data engineering, Python, Azure Synapse, Azure, Api Management, Azure Data Lake, Nlp, Predictive Analytics"
Data Architect,Impetus,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Qualifications\nDegree Graduates/Postgraduate in CSE or related field\nlooking for candidates with hands on experience in Big Data and Cloud Technologies.\n10+ Years of experience Expertise in designing and developing applications using Big Data and Cloud technologies Must Have\nExpertise and hands-on experience* on Spark, and Hadoop echo system components Must Have\nExpertise and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of Shell script & Java/Python Must Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engines like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Must Have\nAutomation approach - Good to Have\nResponsibilities\nDefine Data Warehouse modernization approach and strategy for the customer\nAlign the customer on the overall approach and solution\nDesign systems for meeting performance SLA\nResolve technical queries and issues for team\nWork with the team to establish an end-to-end migration approach for one use case so that the team can replicate the same for other iterations","Java, Hadoop, Big Data, Autosys, Cloud Technologies, Gcp, Spark, Shell script, Oozie, Azure, Python, AWS"
Data Architect,DATAECONOMY,15-17 Years,,"Hyderabad, India",Login to check your skill match score,"About Us\n\nAbout DATAECONOMY: We are a fast-growing data & analytics company headquartered in Dublin with offices inDublin, OH, Providence, RI, and an advanced technology center in Hyderabad,India. We are clearly differentiated in the data & analytics space via our suite of solutions, accelerators, frameworks, and thought leadership.\n\nJob Description\n\n15+ years of professional experience in the field of Data Architecture, Design and Development of Data Lifecycle Management (DLM) and Data Governance projects\nExperience should include hands-on experience in implementing a Customer Data Hub Customer Information Management system or Customer Data Mart which includes Modeling Customer Data Model in the 3NF / Dimensional Model, defining and developing integration patterns like Batch and real-time, defining & developing the physical storage layer, defining & developing the consumption pattern\nExperience with Data Modeling is a MUST, experience in defining standards and data architecture best practices is a must\nExperience with Data Management/Architecture Practices like Data Integration, Data Governance, Data Quality, Metadata management, Data Security, Data Encryption etc. is a must\nMust have hands-on experience with developing large Data Mart or Data warehouse or Data hub involving data modelling, data ingestion, data processing and extract generation to the downstream systems\nMust have experience with any ETL tool like Informatica or Data Stage or other for data processing requirements.\nExperience with Identity resolution products for Name and Address standardization like Informatica IDQ / Address Doctor /IBM Quality stage is required.\nExperience with any Data Quality product like Informatica IDQ, Quality Stage or Trillium will be an added advantage\nMust have strong experience with writing SQL for pulling and analyzing source/data platforms\nExperience with Data Science models, model validation, model tuning and management will be an added advantage.\nProactively communicate and collaborate with business stakeholders to understand the business requirements, translate them into design, and develop the respective module based on the requirements.\nMust have experience working in an Agile implementation set-up with Product Backlogs, User Stories, Scrum and sprint planning etc.\nStrong verbal and written communication and English language skills\nStrong consulting skills and consulting experience are strongly desired.\n\nRequirements\n\nDeveloping Data Architecture and Design documents for Data Lake, Data warehouse & Data Marts corresponding to Customer Information Management Systems\nExperience in Data Lifecycle Management (DLM)\nConfiguring Solutions and Coding any customization required for the Data platform including Data Lake, Data warehouse and Data Marts\nWorking with the clients to understand the requirements. Develop the required codebase for the functional needs\nDevelop Source to Target mapping document for all the attributes that are required to be captured for Data Lake, Data warehouse and Data Marts\nConfigure and develop code required for Upstream and downstream system communication in a Batch and real-time mode\nProvide clarifications for any questions that the Business / SME team or any other stakeholder has on Data implementations.\nParticipate in system and acceptance testing along with the stakeholders\n\nBenefits\n\nStandard Company Benefits","Agile implementation, Data Encryption, Customer Data Hub, Data Science models, Customer Data Mart, Customer Information Management, Metadata Management, Data Modeling, Data Integration, Sql, Data Quality, Data Architecture, Data Security, Data Governance"
Data Architect,Accurate Background,12-15 Years,,"Hyderabad, India",Login to check your skill match score,"When you join Accurate Background, you're an integral part of making every hire the start of a success story. Your contributions will help us fulfill our mission of advancing the background screening experience through visibility and insights, empowering our clients to make smarter, unbiased decisions.\nAccurate Background is a fast-growing organization, focused on providing employment background screenings and building trustful relationships with our clients. Accurate Background continues to exceed expectations by offering an array of innovative and cutting-edge background check and credentialing products to meet the needs of human resource, loss prevention, and security/legal professionals in employment screening and vendor certification.\nLead Data/BI Architect, where you will take charge of developing and executing the enterprise-wide data & analytics strategy. As a senior leader within the organization, you will guide a small data team across integration and BI functions, shaping the data landscape and driving the company towards a single source of truth. This role demands a highly skilled and experienced individual who can deliver a comprehensive data strategy while ensuring robust governance, architecture, and integration across the accurate enterprise.\nLead Data Architect will oversee data governance, security, integration, and data quality initiatives. You will be instrumental in defining architectural design patterns, Data standards, and best practices while leading the team in implementing scalable, optimized, and secure data solutions that support business intelligence objectives.\nResponsibilities:\nShould be able to understand different cloud Platforms/Architecture, preferably on AWS Platform Services for Data & Analytics\nArchitect end-to-end data solutions on data lakes, warehouses, and real-time analytics.\nShould have Designed/Architected Medium to Large Data Warehouses\nShould have created Conceptual, Logical, Physical Data Models, OLTP, OLAP/Dimensional Data Models, Data Analysis\nShould be aware of Data Architecture/Design Patterns in the areas of Data Ingestion / Curation / Data Consumption / Reporting Semantic Models\nShould have participated in Defining Data Strategy/Roadmaps in Data & Analytics\nShould be hands on in some of the areas of the required Technology tools.\nShould be aware of Data fabric, Data Ingestion Tools, Data Quality Management, Metadata Management, Data Lineage, Data Security\nShould have good experience in Designing Reusable utilities\nShould have good experience with Data Warehouse Migrations\nShould be aware of Agile Methodologies/Data Products, leading teams technically from Design to Development to Deployment, through DevOps, DataOps\nAutomate data quality checks and validations to maintain high data integrity. Monitor, troubleshoot, and resolve issues across data platforms\nQualifications:\nShould have at least 12-15 years of total IT experience in Software Development, with 5 years exclusively in Design & Architecting Data Warehousing projects\nShould have good hands-on below tools & technologies\nMust Have:\nData Lake Architecture / Data Fabric\nSnowflake (Tasks, Streams, Stored Procs, Snow pipes), SQL Server\nData Modeling tools (like Erwin)\nData Warehouse Migrations\nAgile Methodologies\nReplication tools (like AWS DMS, Qlik Replicate)\nOLAP/Dimensional Data Models\nNoSQL Databases (like MongoDB)\nGood to have\nETL Tools (like SSIS, dBT)\nBI/Reporting tools (Power BI, Tableau)\nCloud Platforms (AWS, Microsoft Fabric)\nReal-Time databases (Cassandra, DynamoDB)\nSolid understanding of the Agile development process and software release processes.\nMust be a self-starter who is highly organized, hands-on, and a team player.\nShould be able to create Design Documents/Mapping documents (either PPT or Word document)\nShould be able to communicate & Collaborate with all stakeholders (Director level, Business Units, other Architects, Product Managers, Scrum Stay updated on industry trends to continuously improve data systems and processes.\nThe Accurate Way:\nWe offer a fun, fast-paced environment, with lots of room for growth. We have an unwavering commitment to diversity, ensuring everyone has a complete sense of belonging here. To do this, we follow four guiding principles Take Ownership, Be Open, Stay Curious, Work as One core values that dictate what we stand for, and how we behave.\nTake ownership.\nBe accountable for your actions, your team, and the company. Accept responsibility willingly, especially when it's what's best for our customers. Give others every reason to trust you, believe in you, and count on you. Rise to every occasion with your personal best.\nBe open.\nBe open to new ideas. Be inclusive of people and ways of doing things. Make yourself accessible and approachable, and communicate with genuineness, transparency, honesty, and respect. Embrace differences.\nStay curious.\nStay curious even as you move forward. Tirelessly ask questions and challenge the status quo in your pursuit of new ideas, ways to solve problems, and to continually grow and improve.\nWork as one.\nWork together to create the best customer and workplace experience. Put our customers and employees firstbefore individual or departmental agendas. Make sure they get the help they need to succeed.\nAbout Accurate Background:\nAccurate Background's vision is to make every hire the start of a success story. As a trusted provider of employment background screening and workforce monitoring services, Accurate Background gives companies of all sizes the confidence to make smarter, unbiased hiring decisions at the speed of demand. Experience a new standard of support with a dedicated team, comprehensive technology and insight, and the most extensive coverage and search options to advance your business while keeping your brand and people safe.\nSpecial Notice:\nAccurate is aware of schemes involving fraudulent job postings/offers and/or individuals or entities claiming to be employees of Accurate. Those involved are offering fabricated employment opportunities to applicants, often asking for sensitive personal and financial information. If you believe you have been contacted by anyone misrepresenting themselves as an employee of Accurate, please contact [HIDDEN TEXT].\nPlease be advised that all legitimate correspondence from an Accurate employee will come from @accurate.com email accounts.\nAccurate will not interview candidates via text or email. Our interviews are conducted by recruiters and leaders via the phone, Zoom/Teams or in an in-person format.\nAccurate will never ask candidates to make any type of personal financial investment related to gaining employment with the Company.","snowflake, Data Lake Architecture, Data Fabric, OLAP Dimensional Data Models, Data Warehouse Migrations, Replication tools like AWS DMS Qlik Replicate, NoSQL Databases like MongoDB, BI Reporting tools Power BI Tableau, Real-Time databases Cassandra DynamoDB, Cloud Platforms AWS Microsoft Fabric, ETL Tools like SSIS dBT, Data Modeling tools like Erwin, SQL Server, Agile Methodologies"
Data Architect,eInfochips (An Arrow Company),10-12 Years,,"Ahmedabad, India",Login to check your skill match score,"Role: Data Architect Data Science and Azure Cloud\nYears of Experience: 10+ Years\nLocation: Ahmedabad\nWhat You Will Be Doing:\nCollaborate with stakeholders to translate business requirements into scalable data architecture solutions.\nDesign data systems that support machine learning, advanced data science, and generative AI models.\nBuild and maintain data pipelines, integration flows, and storage solutions for efficient data handling.\nDefine and uphold data governance standards ensuring data quality, compliance, and security.\nMentor junior data scientists, data engineers, and developers on building end-to-end data solutions.\nProvide architectural guidance and best practices across technical teams.\nStay updated with trends in Azure Cloud, Data Science, and Generative AI to introduce relevant technologies.\nPerform data profiling, detect quality issues, and recommend solutions for better data reliability.\nWork with IT to optimize Azure cloud environments for data storage, retrieval, and processing.\nWhat Are We Looking For:\nBachelor's or Master's in Computer Science, Data Science, or a related field.\nDeep expertise in data modeling, statistical analysis, and machine learning.\nHands-on experience with Azure Data Factory, Azure Databricks, Azure OpenAI, and Azure ML.\nKnowledge of generative AI (GANs, VAEs) is a strong plus.\nProven track record of designing scalable, enterprise-grade data architectures.\nStrong grasp of data governance, management, and security principles.\nProficiency in Python, R, or Scala for data manipulation and analysis.\nFamiliarity with modern storage solutions data lakes, warehouses, and relational DBs.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and cross-team collaboration skills.\nCertifications in Azure, Data Science, ML, or Generative AI are highly preferred.","relational DBs, R, data lakes, Azure OpenAI, Machine Learning, Azure Databricks, Data Modeling, Python, Azure Data Factory, Statistical Analysis, Scala, Azure ML"
Data Architect,VLink Inc,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"VLink Inc Hiring: Data Architect | Bangalore | Hybrid Work Mode\nWe're looking for expert Data Architects with 1215 years of experience to join us immediately in Bangalore.\nWe are seeking experienced Data Architects to design and implement enterprise data solutions, ensuring data governance, quality, and advanced analytics capabilities. The ideal candidate will have expertise in defining data policies, managing metadata, and leading data migrations from legacy systems to Microsoft Fabric/DataBricks/Snowflake. Experience and deep knowledge about at least one of these 3 platforms is critical. Additionally, they will play a key role in identifying use cases for advanced analytics and developing machine learning models to drive business insights.\nKey Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems. Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP). Strong communication skills with the ability to translate complex data concepts into business insights.\nPreferred candidate profile\nData Modeling (Conceptual, Logical, Physical)- Minimum 5 years\nDatabase Technologies (SQL Server, Oracle, PostgreSQL, NoSQL)- Minimum 5 years\nCloud Platforms (AWS, Azure, GCP) - Minimum 3 Years\nETL Tools (Informatica, Talend, Apache Nifi) - Minimum 3 Years\nBig Data Technologies (Hadoop, Spark, Kafka) - Minimum 5 Years\nData Governance & Compliance (GDPR, HIPAA) - Minimum 3 years\nMaster Data Management (MDM) - Minimum 3 years\nData Warehousing (Snowflake, Redshift, BigQuery)- Minimum 3 years\nAPI Integration & Data Pipelines - Good to have.\nPerformance Tuning & Optimization - Minimum 3 years\nbusiness Intelligence (Power BI, Tableau)- Minimum 3 years\nInterested candidate can share their updated profile on below mentioned mail:-\n[HIDDEN TEXT]\nRegards As Ever\nAnkit Malik","snowflake, Microsoft Fabric, Master Data Management, Data Quality Management, Oracle, Apache Nifi, Hipaa, Data Warehousing, Hadoop, Tableau, Kafka, BigQuery, Power Bi, Etl, Nosql, Informatica, PostgreSQL, SQL Server, Gdpr, Metadata Management, Machine Learning, Talend, Data Governance, Data Modeling, Redshift, Spark, DataBricks"
GridOS Data Architect,GE Vernova,Fresher,,"Hyderabad, India",Login to check your skill match score,"Job Description Summary\n\nAs a Data Architect, you will play a pivotal role in defining and implementing common data models, API standards, and leveraging the Common Information Model (CIM) standard across a portfolio of products deployed in Critical National Infrastructure (CNI) environments globally.\n\nGE Vernova is the leading software provider for the operations of national and regional electricity grids worldwide. Our software solutions range from supporting electricity markets, enabling grid and network planning, to real-time electricity grid operations.\n\nIn this senior technical role, you will collaborate closely with lead software architects to ensure secure, performant, and composable designs and implementations across our portfolio.\n\nJob Description\n\nGrid Software (a division of GE Vernova) is driving the vision of GridOS - a portfolio of software running on a common platform to meet the fast-changing needs of the energy sector and support the energy transition. Grid Software has extensive and well-established software stacks that are progressively being ported to a common microservice architecture, delivering a composable suite of applications. Simultaneously, new applications are being designed and built on the same common platform to provide innovative solutions that enable our customers to accelerate the energy transition.\n\nResponsibilities\n\nThis role is for a senior data architect who understands the core designs, principles, and technologies of GridOS. Key responsibilities include:\n\nFormalizing Data Models and API Standards: Lead the formalization and standardization of data models and API standards across products to ensure interoperability and efficiency.\nLeveraging CIM Standards: Implement and advocate for the Common Information Model (CIM) standards to ensure consistent data representation and exchange across systems.\nArchitecture Reviews and Coordination: Contribute to architecture reviews across the organization as part of Architecture Review Boards (ARB) and the Architecture Decision Record (ADR) process.\nKnowledge Transfer and Collaboration: Work with the Architecture SteerCo and Developer Standard Practices team to establish standard pratcise around data modeling and API design.\nDocumentation: Ensure that data modeling and API standards are accurately documented and maintained in collaboration with documentation teams.\nBacklog Planning and Dependency Management: Work across software teams to prepare backlog planning, identify, and manage cross-team dependencies when it comes to data modeling and API requirements.\n\nKey Knowledge Areas and Expertise\n\nData Architecture and Modeling: Extensive experience in designing and implementing data architectures and common data models.\nAPI Standards: Expertise in defining and implementing API standards to ensure seamless integration and data exchange between systems.\nCommon Information Model (CIM): In-depth knowledge of CIM standards and their application within the energy sector.\nData Mesh and Data Fabric: Understanding of data mesh and data fabric principles, enabling software composability and data-centric design trade-offs.\nMicroservice Architecture: Understandig of microservice architecture and software development\nKubernetes: Understanding of Kubernetes, including software development in an orchestrated microservice architecture. This includes Kubernetes API, custom resources, API aggregation, Helm, and manifest standardization.\nCI/CD and DevSecOps: Experience with CI/CD pipelines, DevSecOps practices, and GitOps, especially in secure, air-gapped environments.\nMobile Software Architecture: Knowledge of mobile software architecture for field crew operations, offline support, and near-realtime operation.\n\nAdditional Knowledge (Advantageous But Not Essential)\n\nEnergy Industry Technologies: Familiarity with key technologies specific to the energy industry, such as Supervisory Control and Data Acquisition (SCADA), Geospatial network modeling, etc.\n\nThis is a critical role within Grid Software, requiring a broad range of knowledge and strong organizational and communication skills to drive common architecture, software standards, and principles across the organization.\n\nAdditional Information\n\nRelocation Assistance Provided: No","Data Mesh and Data Fabric, Data Architecture and Modeling, CI CD and DevSecOps, API Standards, Common Information Model CIM, Mobile Software Architecture, Microservice Architecture, Kubernetes"
Data Architect,EverExpanse,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"About Us\n\nEverExpanse is a dynamic technology-driven organization specializing in modern web and e-commerce solutions. We pride ourselves on building scalable, high-performance applications that drive user engagement and business success. Our development team thrives on innovation and collaboration, delivering impactful digital experiences across diverse industries. About Us\n\nJob Overview\n\nclip0_599_14767\n\nEverExpanse Pvt. Ltd.\n\nBangalore ,India\n\n8 + Years\n\nFull Time\n\nBachelor's degree in Computer Science, IT, or a related field.\n\nJob Description\n\nWe are seeking experienced Data Architects to lead the design, governance, and implementation of large-scale enterprise data solutions. The ideal candidate will have hands-on expertise in Microsoft Fabric, Databricks, or Snowflake, and strong capabilities in data governance, metadata management, and analytics. This role requires the ability to engage with senior stakeholders, define strategic data use cases, and drive the adoption of modern data architectures and advanced analytics solutions.\n\nKey Responsibilities\n\nData Governance & Management\n\nEstablish and maintain a Data Usage Hierarchy to enable structured access to enterprise data.\nDefine and enforce data policies, standards, and governance frameworks to maintain data consistency and compliance.\nImplement Data Quality Management (DQM) practices to enhance data reliability and integrity.\nOversee Metadata Management and Master Data Management (MDM) to enable unified data integration across platforms.\n\nData Architecture & Migration\n\nLead end-to-end data migration projects from legacy systems to Microsoft Fabric, Databricks, or Snowflake.\nDesign scalable and high-performance data architectures that support business intelligence and real-time analytics.\nCollaborate with engineering and IT teams to define and implement robust data pipelines and ETL frameworks.\n\nAdvanced Analytics & Machine Learning\n\nIdentify and prioritize advanced analytics use cases aligned with business objectives.\nDesign and develop machine learning models to derive actionable business insights.\nCollaborate with data scientists and business teams to operationalize ML models for real-world applications.\n\nRequired Qualifications\n\nProven experience as a Data Architect or similar role with deep knowledge in data management, governance, and analytics.\nHands-on expertise in at least one of: Microsoft Fabric, Databricks, or Snowflake (must-have).\nIn-depth understanding of data governance frameworks, DQM, metadata management, and MDM.\nExperience designing and implementing machine learning models and AI-powered solutions.\nProficiency with cloud platforms like Azure, AWS, or GCP and experience in data modeling and ETL development.\nStrong communication skills, with the ability to engage senior-level stakeholders and translate complex data concepts into actionable insights.\n\nPreferred Qualifications\n\nExperience with enterprise-level data migration projects.\nBackground in AI/ML model development in cloud environments.\nFamiliarity with data visualization tools such as Power BI or Tableau.\nKnowledge of modern data pipeline orchestration tools (e.g., Azure Data Factory, Apache Airflow).\n\nSoft Skills\n\nStrategic thinking and problem-solving abilities.\nStrong stakeholder management and interpersonal communication.\nAbility to work independently and as part of a cross-functional team.\nCuriosity and drive to stay updated with emerging data technologies.\n\nWhy Join Us\n\nBe at the forefront of data transformation with platforms like Microsoft Fabric, Databricks, and Snowflake.\nWork in a hybrid model from our Bangalore office, offering flexibility and collaboration.\nOpportunity to interact with senior stakeholders and influence enterprise data strategies.\nCompetitive compensation and career growth in a future-focused environment.\n\nTo Apply send your Resume to [HIDDEN TEXT]","snowflake, Microsoft Fabric, Etl Development, Databricks, Data Migration, Metadata Management, Data Governance, Machine Learning, Data Modeling"
Data Architect,Gainwell Technologies,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Summary\n\nWe're looking for a Solution Architect to join our product development team to drive a cloud based data platform architecture that will power BI and advanced analytics at Gainwell Technologies. You will have a direct impact on the design, architecture, and implementation of BI and data science use cases. The ideal candidate will have industry-leading programming skills and established knowledge of implementing, designing, deploying, and maintaining big data analytics platforms in a cloud environment.\n\nThe solution architect will use knowledge of healthcare data to influence the implementation and governance of our data architecture. Your designs will account for data movement, storage, compute and BI consumption, and will comply with security and data governance standards. You will work closely with a variety of partners within the Data and Analytics organization.\n\nYour role in our mission\n\nProvide thought leadership and technical direction to the data engineering team in building analytic data products\nUnderstand and translate business requirements to data strategies that align with overall technology vision\nDesign, develop and enforce standards for the data storage, processing and governance across all environments\nWork closely with enterprise and application architects to align the data engineering team to the overall company SDLC standards, practices, and data access patterns\nDevelop, document and maintain overall view of data platform architecture, data acquisition, data quality, and data retention\nProvide formal and informal training for data engineers, platform engineers and ETL developers\nMaintain knowledge of emerging technologies and architectures\nDocument and publish technical principles and standards, and mentor the data engineering team to incorporate them into their daily practices\nChampion and present the technical vision to the executive team and business stakeholders\n\nWhat we're looking for\n\nBasic Qualifications\n\nBachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields\n8+ years of overall experience in big data, database and enterprise data architecture and delivery\n8+ years of programming proficiency in a subset of Python, Java, and Scala\n5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks\n5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms\n3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred\nStrong SQL skills to create/maintain DB objects, query/load required data using data governance (e.g. business glossary, data dictionary, data catalog, data quality, master data management, etc.) and visualization tools to bring data literacy to the organization.\nPractical experience on workload management, monitoring, and performance tuning Apache Spark jobs\nBroad knowledge of data technologies, tools, and disciplines including data modeling, dimensional models, third-normal-form structures, ETL/ELT, change data capture and slowly changing dimensions\nExperience with healthcare data a big plus\nExperience with Machine Learning & MLOPs is a big plus\n\nWhat you should expect in this role\n\nClient or office environment / may work remotely\nOccasional evening and weekend work\n\nReq. ID: 26672","Java, Machine Learning, Hadoop, Scala, Data Modeling, Sql, ELT, MLops, Spark, Data Governance, Databricks, Python, Etl"
Senior Data Architect,Volvo Group,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.\n\nWhat You Will Do\n\nAs Senior Data Architect, your mission is to define and design secured and adaptable data environments for data management and analytics in order to provide access to high quality, consistent data in an easy and convenient manner to all authorized end users.\n\nYou will interact with various stakeholders such as Data Analyst, Solution Architects, Business process Developer, capability leads and global teams to provide them with the right architecture for their data solutions based on ITSM and Foundation data.\nYou will support product delivery teams, providing architecture leadership and support within the end to end life-cycle of managing data.\nYou will also work with data modeling and data transformation, and by that define and deploy data management and analytical services. This could include reporting and analytics, machine learning, artificial intelligence, data quality, master data management and data provisioning.\nYou will be part of IT Performance Management Team. This organization is gathering professionals in performance analytics using platforms such as Power BI or ServiceNow and expertise in architecture with goal of defining and supporting healthy and sustainable development of our IT ecosystem.\n\nFor this position we also expect high focus on interpersonal skills and attitude. You are passionate about people, customer value and have a strong digital era leadership, meaning that you are able to lead in pluriverse and changing environment.\n\nWho are you\n\nDo you dream big We do too, and we are excited to grow together.\n\nFor this position, we are looking for a person with teamwork and innovation spirit. You have proven experience as Data Architect, have already an experience in supporting a team and a good understanding and knowledge of an enterprise with a comparable volume and complexity.\n\nYou have strong experience in mentoring and coaching other Architects and you have a good knowledge of ITSM data. You are used to be the Go-to person for your team on the full scope of the delivery and strong contributor cross teams. You can demonstrate effective technical and technology trends dialog with business stakeholders and define right priorities and approach for team to prepare technical solutions that can maximize value delivered to customers. You are able to define the roadmap and the vision for the data environment in accordance with the Department strategy and support the team in taking actions aligned to this strategy. You are already part of various technology communities and cultivate continuous learning core habits.\nYou have critical thinking, sense making and complex problem solving with ownership mindset.\nYou share core values according to the Volvo Way. You appreciate diversity and work in cross-functional teams.\nYou actively collaborate with various stakeholders and navigate with ease in a multi-cultural environment.\nYou get an opportunity to work together with highly skilled colleagues in an exciting, global environment which provides opportunities to develop both professionally and personally.\n\nRequired Competencies\n\nExtended Architect experience on ITSM Data (ServiceNow or in a similar environment)\nProfessional skills\nSecurity competencies and mindset\nAdvanced Communication Skills including the ability to effectively communicate with technical and non-technical audiences.\nFamiliar with Agile environment\nGood understanding of the (business) processes and requirements\nExcellent analytical skills.\nInterest in technology, passionate about ITSM platforms curiosity about the market\nHave already successfully supported an organization in a global context with ITSM Data Architecture and managing good relationships with business partners and stakeholders.\nKnowledge of Volvo environment (ITSM related) is nice to have\nStrong leadership and committed to getting the job done\nFamiliarity and commitment to work in a global and multinational environment\nMinimum 7 years as Data Architect\nRelevant university degree\nComplete proficiency in English is a requirement\n\nJoining our group means becoming part of a global organization that values innovation, collaboration, and continuous improvement. We offer a dynamic and inclusive work environment where your ideas and contributions truly matter. You'll have the opportunity to work with cutting-edge technologies and develop your skills through continuous learning and professional development programs. We believe in empowering our employees to take ownership of their careers, and we support work-life balance.\n\nIf you want to make a real impact in your career, the transportation business is where you want to be. We look forward to meeting you.\n\nWe value your data privacy and therefore do not accept applications via mail.\n\nWho We Are And What We Believe In\n\nOur focus on Inclusion, Diversity, and Equity allows each of us the opportunity to bring our full authentic self to work and thrive by providing a safe and supportive environment, free of harassment and discrimination. We are committed to removing the barriers to entry, which is why we ask that even if you feel you may not meet every qualification on the job description, please apply and let us decide.\n\nApplying to this job offers you the opportunity to join Volvo Group. Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group's leading brands and entities.\n\nGroup Digital & IT is the hub for digital development within Volvo Group. Imagine yourself working with cutting-edge technologies in a global team, represented in more than 30 countries. We are dedicated to leading the way of tomorrow's transport solutions, guided by a strong customer mindset and high level of curiosity, both as individuals and as a team. Here, you will thrive in your career in an environment where your voice is heard and your ideas matter.","Data Provisioning, Master Data Management, Servicenow, Machine Learning, Data Management, Itsm, Data Modeling, Artificial Intelligence, Power Bi, Data Quality, Data Architecture, Agile, Data Transformation"
Senior Data Architect,Volvo Group,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.\n\nWhat You Will Do\n\nAs Senior Data Architect, your mission is to define and design secured and adaptable data environments for data management and analytics in order to provide access to high quality, consistent data in an easy and convenient manner to all authorized end users.\n\nYou will interact with various stakeholders such as Data Analyst, Solution Architects, Business process Developer, capability leads and global teams to provide them with the right architecture for their data solutions based on ITSM and Foundation data.\nYou will support product delivery teams, providing architecture leadership and support within the end to end life-cycle of managing data.\nYou will also work with data modeling and data transformation, and by that define and deploy data management and analytical services. This could include reporting and analytics, machine learning, artificial intelligence, data quality, master data management and data provisioning.\nYou will be part of IT Performance Management Team. This organization is gathering professionals in performance analytics using platforms such as Power BI or ServiceNow and expertise in architecture with goal of defining and supporting healthy and sustainable development of our IT ecosystem.\n\nFor this position we also expect high focus on interpersonal skills and attitude. You are passionate about people, customer value and have a strong digital era leadership, meaning that you are able to lead in pluriverse and changing environment.\n\nWho are you\n\nDo you dream big We do too, and we are excited to grow together.\n\nFor this position, we are looking for a person with teamwork and innovation spirit. You have proven experience as Data Architect, have already an experience in supporting a team and a good understanding and knowledge of an enterprise with a comparable volume and complexity.\n\nYou have strong experience in mentoring and coaching other Architects and you have a good knowledge of ITSM data. You are used to be the Go-to person for your team on the full scope of the delivery and strong contributor cross teams. You can demonstrate effective technical and technology trends dialog with business stakeholders and define right priorities and approach for team to prepare technical solutions that can maximize value delivered to customers. You are able to define the roadmap and the vision for the data environment in accordance with the Department strategy and support the team in taking actions aligned to this strategy. You are already part of various technology communities and cultivate continuous learning core habits.\nYou have critical thinking, sense making and complex problem solving with ownership mindset.\nYou share core values according to the Volvo Way. You appreciate diversity and work in cross-functional teams.\nYou actively collaborate with various stakeholders and navigate with ease in a multi-cultural environment.\nYou get an opportunity to work together with highly skilled colleagues in an exciting, global environment which provides opportunities to develop both professionally and personally.\n\nRequired Competencies\n\nExtended Architect experience on ITSM Data (ServiceNow or in a similar environment)\nProfessional skills\nSecurity competencies and mindset\nAdvanced Communication Skills including the ability to effectively communicate with technical and non-technical audiences.\nFamiliar with Agile environment\nGood understanding of the (business) processes and requirements\nExcellent analytical skills.\nInterest in technology, passionate about ITSM platforms curiosity about the market\nHave already successfully supported an organization in a global context with ITSM Data Architecture and managing good relationships with business partners and stakeholders.\nKnowledge of Volvo environment (ITSM related) is nice to have\nStrong leadership and committed to getting the job done\nFamiliarity and commitment to work in a global and multinational environment\nMinimum 7 years as Data Architect\nRelevant university degree\nComplete proficiency in English is a requirement\n\nJoining our group means becoming part of a global organization that values innovation, collaboration, and continuous improvement. We offer a dynamic and inclusive work environment where your ideas and contributions truly matter. You'll have the opportunity to work with cutting-edge technologies and develop your skills through continuous learning and professional development programs. We believe in empowering our employees to take ownership of their careers, and we support work-life balance.\n\nIf you want to make a real impact in your career, the transportation business is where you want to be. We look forward to meeting you.\n\nWe value your data privacy and therefore do not accept applications via mail.\n\nWho We Are And What We Believe In\n\nOur focus on Inclusion, Diversity, and Equity allows each of us the opportunity to bring our full authentic self to work and thrive by providing a safe and supportive environment, free of harassment and discrimination. We are committed to removing the barriers to entry, which is why we ask that even if you feel you may not meet every qualification on the job description, please apply and let us decide.\n\nApplying to this job offers you the opportunity to join Volvo Group. Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group's leading brands and entities.\n\nGroup Digital & IT is the hub for digital development within Volvo Group. Imagine yourself working with cutting-edge technologies in a global team, represented in more than 30 countries. We are dedicated to leading the way of tomorrow's transport solutions, guided by a strong customer mindset and high level of curiosity, both as individuals and as a team. Here, you will thrive in your career in an environment where your voice is heard and your ideas matter.","ITSM Data, Reporting and Analytics, Data Provisioning, Master Data Management, Servicenow, Machine Learning, Data Management, Data Modeling, Artificial Intelligence, Power Bi, Data Quality, Data Architecture, Agile, Data Transformation"
Data Architect,Tezo,7-12 Years,,"Hyderabad, India",Login to check your skill match score,"Tezo is a new generation Digital & AI solutions provider, with a history of creating remarkable outcomes for our customers. We bring exceptional experiences using cutting-edge analytics, data proficiency, technology, and digital excellence.\nAbout the Role\nJob Title: Data Engineering Architect\nLocation: Hyderabad\nEmployment Type: Full-time\nExperience: 7 -12 years\nSeeking a highly skilled and modern Data Engineering Lead/Architect to lead technical teams in architecting and delivering cutting-edge data solutions across multiple cloud platforms.\nThis role requires deep expertise in Azure, Snowflake, and Databricks, along with a strong background in data engineering, architecture, and analytics.\nAs a Architect, you will drive end-to-end data solutioning, oversee data pipeline development, and ensure scalability, performance, and security while aligning solutions with business objectives.\nResponsibilities\nDesign and implement modern, scalable, and high-performance data architectures across Azure cloud platforms\nDevelop, optimize, and manage ETL/ELT pipelines, data lakes, and real-time streaming solutions using Snowflake, Databricks, and cloud-native tools.\nDeploy and manage data warehousing, analytics, and Lakehouse solutions on Azure Synapse, Azure Data factory, Databricks\nCollaborate with data scientists to integrate AI/ML models into data pipelines and optimize analytics workflows.\nImplement data governance frameworks, compliance (GDPR, CCPA), role-based access controls, and best practices for security across multi-cloud environments.\nLead and mentor a team of data engineers, define best practices, and drive innovation in data engineering strategies.\nEnsure cost-efficient and high-performance data processing, leveraging Spark, DBT, and cloud-native tools.\nCollaborate with business leaders, data analysts, and engineering teams to deliver data-driven solutions aligned with business needs.","Ai, real-time streaming solutions, data lakes, snowflake, dbt, Ml, ELT, Azure Synapse, Azure Data Factory, Spark, Databricks, Azure, Etl"
Senior Data Architect,Milacron,10-12 Years,,"Coimbatore, India",Login to check your skill match score,"We are seeking a highly experienced Senior Business Intelligence (BI) Data Architect to join our team. The ideal candidate will have a minimum of 10 years of experience in data architecture, data management, and data analysis. The candidate will be responsible for designing, creating, and managing our company's business intelligence data architecture to drive business decision-making and growth.\n\nThe role reports to the head of global business intelligence, and may be based in Coimbatore, India offices or remote.\n\nResponsibilities\n\nDesign and develop a scalable and efficient data architecture to support business intelligence needs.\nCollaborate with stakeholders to understand and translate business needs into data models supporting long-term solutions.\nWork with the development team to design and implement data strategies, build data flows, and develop conceptual data models.\nCreate logical and physical data models using best practices to ensure high data quality and reduced redundancy.\nOptimize and update logical and physical data models to support new and existing projects.\nMaintain data architecture standards across the organization.\nIdentify opportunities to optimize data flows and data management to improve overall efficiency and performance.\nProvide technical guidance and support to the BI team members.\n\nQualifications\n\nBachelor's degree in Computer Science, Information Systems, or a related field. A Master's degree is preferred.\n\nMinimum of 10 years of experience in data architecture, data management, and data analysis.\n\nProven experience in BI tools and technologies, data modeling, data warehousing, ETL tools, SQL, and data governance. Expertise in Azure Data Factory, Azure Data Bricks, Azure Warehouse, and PowerBI is a must.\n\nStrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\n\nExcellent communication skills with the ability to explain complex technical concepts to non-technical stakeholders.\n\nStrong problem-solving skills, with a proven ability to design creative solutions and make critical decisions.\n\nExperience in project management and team leadership. Familiarity with cloud-based data strategies and architectures. Certification in Azure Data Engineering or similar is a plus.\n\nWe offer a competitive salary and benefits package, a challenging and rewarding work environment, and the opportunity to be part of a team that is shaping the future of our company. If you meet the above qualifications and are ready to take the next step in your career, we invite you to apply today.\n\nWho We Are\n\nMilacron is a global leader in the manufacture, distribution and service of highly engineered and customized systems within the $27 billion plastic technology and processing industry. We are the only global company with a full-line product portfolio that includes hot runner systems, injection molding, extrusion equipment. We maintain strong market positions across these products, as well as leading positions in process control systems, mold bases and components, maintenance, repair and operating (MRO) supplies for plastic processing equipment. Our strategy is to deliver highly customized equipment, components and service to our customers throughout the lifecycle of their plastic processing technology systems. Milacron is an Operating Company of Hillenbrand.\n\nHillenbrand (NYSE: HI) is a global industrial company that provides highly-engineered, mission-critical processing equipment and solutions to customers in over 100 countries around the world. Our portfolio is composed of leading industrial brands that serve large, attractive end markets, including durable plastics, food, and recycling. Guided by our Purpose Shape What Matters For Tomorrow we pursue excellence, collaboration, and innovation to consistently shape solutions that best serve our associates, customers, communities, and other stakeholders. To learn more, visit: www.Hillenbrand.com.\n\nEEO: The policy of Hillenbrand Inc. is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity/expression or veteran status. Additionally, Hillenbrand Inc. and our operating companies are committed to being an Equal Employment Opportunity (EEO) Employer and offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email us at [HIDDEN TEXT] . In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying. At Hillenbrand, everyone is welcome to apply and Shape What Matters for Tomorrow.","Data Analysis, BI tools and technologies, Azure Warehouse, Data Management, Azure Data Bricks, Data Warehousing, Data Architecture, Data Modeling, Sql, Azure Data Factory, Powerbi, Data Governance, Etl Tools"
Data Architect,66degrees,Fresher,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.\n\nOverview of Role\n:We are seeking an experienced Data Architect to design, develop, and maintain our google cloud data architecture. The ideal candidate will have a strong background in data architecture, data engineering, and cloud technologies, with experience in managing data across google cloud platforms\n.Responsibilities\n:GCP Cloud Architecture: Design, implement, and manage robust, scalable, and cost-effective cloud-based data architectures on Google Cloud Platform (GCP), leveraging services like BigQuery, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage, Cloud DataProc, Cloud Run, and Cloud Composer. Experience designing cloud architectures on Oracle Cloud is a plus\n.Data Modeling: Develop and maintain conceptual, logical, and physical data models to support various business needs\n.Big Data Processing: Design and implement solutions for processing large datasets using technologies such as Spark and Hadoop\n.Data Governance: Establish and enforce data governance policies, including data quality, security, compliance, and metadata management\n.Data Pipelines: Build and optimize data pipelines for efficient data ingestion, transformation, and loading\n.Performance Optimization: Monitor and tune data systems to ensure high performance and availability\n.Collaboration: Work closely with data engineers, data scientists, and other stakeholders to understand data requirements and provide architectural guidance\n.Innovation: Stay current with the latest technologies and trends in data architecture and cloud computing\n.Qualifications\n:GCP Core Services: In-depth knowledge of GCP data services, including BigQuery, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage, Cloud DataProc, Cloud Run, and Cloud Composer\n.Data Modeling: Expertise in data modeling techniques and best practices\n.Big Data Technologies: Hands-on experience with Spark and Hadoop\n.Cloud Architecture: Proven ability to design scalable, reliable, and cost-effective cloud architectures\n.Data Governance: Understanding of data quality, security, compliance, and metadata management\n.Programming: Proficiency in SQL, Python, and DBT (Data Build Tool)\n.Problem-Solving: Strong analytical and problem-solving skills\n.Communication: Excellent written and verbal communication skills\n.A Bachelor's degree in Computer Science, Computer Engineering, Data or related or equivalent work experience required\n.GCP Professional Data Engineer or Cloud Architect certification is a plus\n.66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class\n.","Cloud Dataflow, Cloud Composer, Cloud Run, Cloud DataProc, DBT Data Build Tool, BigQuery, Hadoop, Data Modeling, Sql, Cloud Storage, Spark, Python"
GCP BIG DATA ARCHITECT- Chennai,Grid Dynamics,Fresher,,"Chennai, India",Login to check your skill match score,"Details on tech stack\nGCP Services: BigQuery, Cloud Dataflow, Pub/Sub, Dataproc, Cloud Storage.\nData Processing: Apache Beam (batch/stream), Apache Kafka, Cloud Dataprep.\nProgramming: Python, Java/Scala, SQL.\nOrchestration: Apache Airflow (Cloud Composer), Terraform.\nSecurity: IAM, Cloud Identity, Cloud Security Command Center.\nContainerization: Docker, Kubernetes (GKE).\nMachine Learning: Google AI Platform, TensorFlow, AutoML.\nCertifications: Google Cloud Data Engineer, Cloud Architect (preferred).\nProven ability to design scalable and robust AI/ML systems in production, with a focus on high-performance and cost-effective solutions.\nStrong experience with cloud platforms (Google Cloud, AWS, Azure) and cloud-native AI/ML services (e.g., Vertex AI, SageMaker).\nExpertise in implementing MLOps practices, including model deployment, monitoring, retraining, and version control.\nStrong leadership skills with the ability to guide teams, mentor engineers, and collaborate with cross-functional teams to meet business objectives.\nDeep understanding of frameworks like TensorFlow, PyTorch, and Scikit-learn for designing, training, and deploying models.\nExperience with data engineering principles, scalable pipelines, and distributed systems (e.g., Apache Kafka, Spark, Kubernetes).\nNice to have requirements to the candidate\nStrong leadership and mentorship capabilities, guiding teams toward best practices and high-quality deliverables.\nExcellent problem-solving skills, with a focus on designing efficient, high-performance systems.\nEffective project management abilities to handle multiple initiatives and ensure timely delivery.\nStrong emphasis on collaboration and teamwork, fostering a positive and productive work environment.","Cloud Dataprep, Cloud Dataflow, Google AI Platform, Pub Sub, Cloud Security Command Center, Cloud Composer, Cloud Identity, Vertex AI, GCP Services, GKE, Scikit-learn, SageMaker, AutoML, Apache Airflow, Tensorflow, Cloud Storage, Pytorch, Docker, Terraform, Python, AWS, Java, BigQuery, Scala, Dataproc, Sql, MLops, Iam, Apache Kafka, Apache Beam, Azure, Kubernetes"
Data Architect - Enterprise Architecture,Viasat,Fresher,,"Hyderabad, India",Login to check your skill match score,"About Us\n\nOne team. Global challenges. Infinite opportunities. At Viasat, we're on a mission to deliver connections with the capacity to change the world. For more than 35 years, Viasat has helped shape how consumers, businesses, governments and militaries around the globe communicate. We're looking for people who think big, act fearlessly, and create an inclusive environment that drives positive impact to join our team.\n\nWhat You'll Do\n\nThe Enterprise Architecture team is focused on providing solutions to enable an effective software engineering workforce that can scale to the business needs. This includes exploring how the business needs map to the application portfolio, business processes, APIs, and data elements across the organization. As a member of this team, you will build up a vast knowledge in software development, cloud application engineering, automation, and container orchestration. Our ideal candidate values communication, learning, adaptability, creativity, and ingenuity. They also enjoy working on challenging technical issues and use creative, innovative techniques to develop and automate solutions. This team is focused on providing our executive and business leadership with visibility into how the software organization is functioning and what opportunities lie to transform the business. In this position you will be an integral part of the Enterprise Architecture team and make meaningful impacts in our journey towards digital transformation.\n\nThe day-to-day\n\nA Strong Data-Model and High-Quality Data are pre-requisites to provide better insights and enable solid data-driven decision making. This is also key to take advantage of various technological advances in Artificial Intelligence and Machine Learning. Your responsibilities will involve build out of data models for various aspects of our enterprise in conjunction with domain experts. Examples include but are not limited to Network, Capacity, Finance, Business Support Systems etc. Responsibilities also include working with software product teams to improve data quality across the organization.\n\nWhat You'll Need\n\nBachelor's degree or higher in Computer Science & Applications, Computer Science and Computer & Systems Engineering, Computer Science & Engineering, Computer Science & Mathematics, Computer Science & Network Security and Math & Computer Science, and/or a related field\nSolid understanding of Data Architecture and Data Engineering principles\nExperience building out data models\nExperience performing data analysis and presenting data in easy to comprehend manner.\nExperience in working with Relational Databases, NoSQL, Large Scale Data technologies (Kafka, Big Query, Snowflake etc)\nExperience with digital transformation across multiple cloud platforms like AWS and GCP.\nExperience in modernizing data platforms especially in GCP is highly preferred.\nPartner with members of Data Platform team and others to build out Data Catalog and map to the data model\nDetail Oriented to ensure that the catalog represents quality data\nSolid communication skills and ability to work on a distributed team\nTenacity to remain focused on the mission and overcome obstacles\nAbility to perform hands-on work with development teams and guide them to building necessary data models.\nExperience setting up governance structure and changing the organization culture by influence\n\nWhat Will Help You On The Job\n\n\nExperience with Cloud Technologies: AWS, GCP, and/or Azure, etc.\nExpertise in GCP data services like Cloud Pub/Sub, Dataproc, Dataflow, BigQuery, and related technologies preferred.\nExperience with Airflow, DBT and SQL.\nExperience with Open-source software like Logstash, ELK stack, Telegraf, Prometheus and OpenTelemetry is a plus.\nPassionate to deliver solutions that improve developer experience and promote API-first principles and microservices architecture.\nExperience with Enterprise Architecture and related principles\n\nEEO Statement\n\n\nViasat is proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, ancestry, physical or mental disability, medical condition, marital status, genetics, age, or veteran status or any other applicable legally protected status or characteristic. If you would like to request an accommodation on the basis of disability for completing this on-line application, please click here.","Large Scale Data technologies, Big Query, Airflow, Relational Databases, OpenTelemetry, snowflake, dbt, Telegraf, Cloud Pub Sub, Data Analysis, data engineering, Enterprise Architecture, Prometheus, Kafka, Elk Stack, Nosql, Data Architecture, AWS, Logstash, Dataproc, Sql, Cloud Technologies, Gcp, DataFlow"
Senior Data Architect- Snowflake (Remote),Reflections Info Systems,12-14 Years,,India,Login to check your skill match score,"We are looking for an 12+years experienced candidates for this role and a minimum of 8-10 years of experience in data engineering, encompassing the development and scaling of data warehouse and data lake platforms.\nWorking hours - 8 hours , with a few hours of overlap during EST Time zone. This overlap hours is mandatory as meetings happen during this overlap hours. Working hours will be 12 PM - 9 PM\nPrimary Skills :\nExtensive experience in designing and implementing data solutions using Snowflake. DBT,\nProficiency in data modeling, schema design, and optimization within Snowflake environments.\nStrong understanding of cloud data warehousing concepts and best practices, particularly with Snowflake.\nExpertise in Dimension Modeling is a must\nExpertise in python/java/scala, SQL, ETL processes, and data integration techniques, with a focus on Snowflake.\nFamiliarity with other cloud platforms and data technologies (e.g., AWS, Azure, GCP )\nDemonstrated experience in implementing data governance frameworks and DataOps practices.\nWorking experience in SAP environments\nFamiliarity with realtime streaming technologies and Change Data Capture (CDC) mechanisms.\nKnowledge of data governance principles and DataOps methodologies\nProven track record of architecting and delivering complex data solutions in cloud platforms/ Snowflake.","realtime streaming technologies, snowflake, cloud data warehousing, data governance principles, data integration techniques, SAP environments, Dimension Modeling, Optimization, dbt, ETL processes, data governance frameworks, DataOps practices, AWS, Sql, Data Modeling, Schema Design, Java, Azure, python, Scala, Gcp"
Chief Technical Manager-Data Architect,NuRe FutureTech,15-17 Years,,"Noida, India",Login to check your skill match score,"Data Governance Senior Consultant Job Description\n\nThe opportunity\n\nWe're looking for Senior Consultant with expertise in Data Governance with 15 years of experience. Experience in Banking Industries and a good understanding of Core Banking systems/flow is preferred.\n\nKey Responsibilities\n\nAssess clients current data governance maturity and develop recommendations for improvement.\nDevelop data governance strategies and roadmaps aligned with clients business objectives.\nDevelop and implement data governance policies, procedures, and standards to ensure data quality.\nCollaborate with cross-functional teams to define and document data governance requirements and document them in DG tools.\nManaged metadata for data assets, ensuring accurate documentation and integrating technical metadata with business metadata.\nConduct data governance assessments and audits to identify areas for improvement.\nDevelop and implement data quality, metadata management, and data security policies and procedures.\nCollaborate with IT teams to implement data governance technologies and tools.\nDevelop and manage data governance metrics and reporting frameworks.\n\nMandatory Requirements\n\n15 years of relevant experience in data governance, data management, or a related field.\n\nStrong understanding of data governance principles, practices, and technologies.\n\nExperience with data governance tools and technologies, such as data catalog, metadata management tools, and data quality software.\n\nExcellent communication, project management, and stakeholder management skills.\n\nDesirable\n\nExperience of cloud-based data governance solutions.\nKnowledge of data privacy regulations, such as GDPR, CCPA, or HIPAA.\nExperience with agile project management methodologies.\nStrong business acumen and understanding of business operations.\n\nQualifications\n\nGraduate, preference for degree in Computer Science (MCA/BS/BE) with industry recognized certifications\nStrong customer service orientation ability to connect with global customers and work with Global teams.",", Data Security Policies, Metadata Management Tools, Data Quality Software, Data Catalog, Data Management, Technologies"
Lead Data Architect Snowflake & Airflow,Mogi I/O : OTT/Podcast/Short Video Apps for you,3-5 Years,,India,Login to check your skill match score,"Note: After your resume is shortlisted, our recruiting team will contact you for initial screening and interview scheduling.\n\nJob Purpose\n\nThe Data Engineer will play a crucial role in designing, building, and maintaining modern data pipelines and infrastructure that power business intelligence, reporting, and analytics across the organization. Working with tools like Snowflake, DBT, Airflow, and Looker/Power BI, you will collaborate closely with Data Analysts and cross-functional teams to ensure timely, reliable, and scalable data delivery within a modern cloud-based environment (AWS/GCP). You will be instrumental in translating business needs into technical solutions and ensuring data quality, governance, and best practices are consistently applied.\n\nRoles And Responsibilities\n\nDesign and develop robust ETL/ELT pipelines for business analytics and reporting needs.\nWork with business analysts to understand data requirements and translate them into scalable technical implementations.\nImplement and maintain scalable and automated processes for data ingestion, transformation, and delivery in a Data Mesh architecture.\nCollaborate with data stakeholders to ensure optimal data feed performance, including change data capture and delta loading strategies.\nConduct exploratory data analysis to proactively identify and resolve data quality issues, and implement automated data validation tests.\nDevelop and manage data models and build dashboards using Looker or Power BI as needed.\nEnsure adherence to data governance standards including testing, peer reviews, and coding best practices.\nMentor junior engineers and act as a subject matter expert for data engineering tools and workflows.\nProvide hands-on troubleshooting and solutions to architecture and design challenges in the data ecosystem.\nContinuously evaluate and improve the performance and efficiency of existing pipelines and data workflows.\n\nMust-Have Qualifications\n\nMinimum 3 years of hands-on experience with Snowflake or equivalent cloud data warehouse platforms.\nProficiency in SQL with proven experience in writing and tuning complex queries.\nExperience with DBT for data modeling and transformation.\nFamiliarity with modern data stack tools such as Airflow, Fivetran, and git.\nDashboarding experience using Looker or Power BI.\nSolid understanding of data ingestion methods and best practices.\nBackground in data modeling and data warehouse design principles.\nExperience working with large-scale datasets and implementing scalable ETL processes.\nComfortable working in an agile environment (SCRUM) and collaborating with cross-functional teams.\nBachelor's degree in Engineering or related field.\nMust be able to join within 30 days.\nThis is a fully remote position.","snowflake, Airflow, dbt, Looker, Fivetran, Power Bi, Sql, Git"
Staff Software Engineer- Data Architect,Convera,10-15 Years,,"Pune, India",Login to check your skill match score,"As a Staff Software Engineer- Data Architect with expertise in Data Modelling & Architecture along with SQL, Snowflake & AWS with Convera, you will be responsible for to oversee the development and utilization of data systems. You will be reporting to the Manager Data Architect, to join our dynamic team in the Foreign Exchange payments processing industry. The ideal candidate is responsible for defining and implementing the enterprise data architecture strategy and ensuring robust data governance across the organization. This role requires a deep understanding of business processes, technology, data management, and regulatory compliance. The successful candidate will work closely with business and IT leaders to ensure that the enterprise data architecture supports business goals, and that data governance policies and standards are adhered to across the organization. Your responsibilities will include working closely with data modelers, data engineers, analysts, cross-functional teams, and other stakeholders to ensure that our data platform meets the needs of our organization and supports our data-driven initiatives. It also includes building a new data platform, integrating data from various sources, and ensuring data availability for various application and reporting needs. Additionally, the candidate should have experience working with AI/ML technologies and collaborating with data scientists to meet their data requirements.\n\nIn your role as a Staff Data Architect, you will:\n\nArchitect and Develop Data Architecture Solutions: Design and implement scalable and efficient data architecture solutions on enterprise data platform. Lead the end-to-end architecture, design and modeling using Snowflake, Tableau and AWS.\nCollaborate and Design Data Models: Partner with stakeholders and business units to understand data requirements and business goals. Lead the architecture and designing the data models, schemas, data mappings, data transformations and metadata that align with business needs, analytical requirements, and industry standards. Ability to analyze the legacy data models within legacy platforms involving SQL Server, Oracle and Stored Procedures. Possess strong hands-on SQL and Python skills.\nData Integration, Governance and Security: Collaborate with internal and external teams to design, implement, and maintain data integration solutions, ensuring high data integrity, consistency, and accuracy. Ensure data security, integrity, and compliance with regulations. Oversee the integration of data from multiple sources into a unified system.\nBusiness Intelligence and Reporting: Develop and implement semantic models to support business intelligence and reporting needs. Design and manage data models involving dimensions, facts, metrics, and measures to ensure accurate and efficient reporting. Work closely with business analysts and BI developers to create and maintain semantic layers that facilitate self-service reporting and analytics. Ensure the semantic models align with business requirements and support key performance indicators (KPIs) and metrics.\nImplementation and Troubleshooting: Oversee the implementation of data solutions from initial concept through to production. Troubleshoot and resolve complex issues to ensure data pipeline stability and high performance. Monitor and optimize the performance of data systems and processes.\nLeadership and Mentorship: Provide guidance and leadership to data modelers and engineering teams, promoting a culture of continuous improvement, knowledge sharing, and technical excellence. Mentor data modelers and engineers and foster their professional growth.\nAI and ML Capabilities: Stay updated with industry trends and advancements in AI and ML and integrate these technologies into the data architecture to enhance data processing and analytics capabilities. Leverage AI and ML to automate data integration, cleansing, and transformation processes, improving efficiency and accuracy. Implement AI-driven analytics and predictive modeling to provide deeper insights and support data-driven decision-making.\nInnovation and Strategy: Drive technical innovation by staying abreast of industry trends and emerging technologies. Influence technical strategies and decisions to align with organizational goals and objectives. Evaluate and recommend data technologies, tools, and platforms.\nDocumentation and Best Practices: Develop and maintain comprehensive documentation for data architecture, pipelines, and processes. Establish and enforce best practices for data engineering and quality assurance.\n\nA Successful Candidate For This Position Should Have:\n\nBachelor's degree or equivalent in Computer Science, Engineering, or a related field with proven experience as a Data Architect or in a similar role. in architecting, designing, deploying, and managing data models on cloud-based infrastructure, preferably for data platforms.\nMinimum of 10-15 years of experience in enterprise data architecture, data management and data governance, or a related field.\nStrong knowledge of database design, data modeling, and ETL processes. Experience in designing and implementing data models involving dimensions, facts, metrics, and measures.\nHands-on experience with Snowflake and SQL. Proficiency with data modeling tools such as ER/Studio, Erwin Data Modeler, Lucidchart, MySQL Workbench, and Oracle SQL Developer Data Modeler. Possess strong hands-on SQL and Python skills.\nStrong experience with data architecture principles, including data modelling, ETL/ELT processes, and data management and hands on experience with Big Data technologies.\nFamiliarity with database systems such as Snowflake, SQL Server, PostgreSQL, or NoSQL databases\nAbility to translate business requirements into effective semantic models that support reporting and analytics using Tableau.\nUnderstanding of current industry trends in AI and ML, and their application in data architecture.\nKnowledge of data governance and compliance standards.\nExcellent problem-solving and analytical skills.\nStrong communication and collaboration abilities.\n\nNice To Have Qualifications:\n\nExperience with regulatory compliance related to data management (e.g., GDPR, HIPAA).\nKnowledge of emerging technologies such as AI, machine learning, and data analytics.\nCertifications in cloud platforms (e.g., AWS).\n\nAbout Convera\n\nConvera is the largest non-bank B2B cross-border payments company in the world. Formerly Western Union Business Solutions, we leverage decades of industry expertise and technology-led payment solutions to deliver smarter money movements to our customers helping them capture more value with every transaction. Convera serves more than 30,000 customers ranging from small business owners to enterprise treasurers to educational institutions to financial institutions to law firms to NGOs.\n\nOur teams care deeply about the value we bring to our customers which makes Convera a rewarding place to work. This is an exciting time for our organization as we build our team with growth-minded, results-oriented people who are looking to move fast in an innovative environment.\n\nAs a truly global company with employees in over 20 countries, we are passionate about diversity; we seek and celebrate people from different backgrounds, lifestyles, and unique points of view. We want to work with the best people and ensure we foster a culture of inclusion and belonging.\n\nWe offer an abundance of competitive perks and benefits including:\n\nCompetitive salary\nOpportunity to earn an annual bonus.\nGreat career growth and development opportunities in a global organization\nA flexible approach to work\n\nThere are plenty of amazing opportunities at Convera for talented, creative problem solvers who never settle for good enough and are looking to transform Business to Business payments. Apply now if you're ready to unleash your potential.\n\nAbout Convera\n\nConvera is the largest non-bank B2B cross-border payments company in the world. Formerly Western Union Business Solutions, we leverage decades of industry expertise and technology-led payment solutions to deliver smarter money movements to our customers helping them capture more value with every transaction. Convera serves more than 30,000 customers ranging from small business owners to enterprise treasurers to educational institutions to financial institutions to law firms to NGOs.\n\nOur teams care deeply about the value we bring to our customers which makes Convera a rewarding place to work. This is an exciting time for our organization as we build our team with growth-minded, results-oriented people who are looking to move fast in an innovative environment.\n\nAs a truly global company with employees in over 20 countries, we are passionate about diversity; we seek and celebrate people from different backgrounds, lifestyles, and unique points of view. We want to work with the best people and ensure we foster a culture of inclusion and belonging.\n\nWe offer an abundance of competitive perks and benefits including:\n\nCompetitive salary\nOpportunity to earn an annual bonus.\nGreat career growth and development opportunities in a global organization\nA flexible approach to work\n\nThere are plenty of amazing opportunities at Convera for talented, creative problem solvers who never settle for good enough and are looking to transform Business to Business payments. Apply now if you're ready to unleash your potential.","snowflake, Ai, Data Modelling Architecture, Tableau, Ml, Sql, ELT, Big Data Technologies, AWS, Etl, Data Management, Data Governance, Python, Data Security, Data Integration"
GCP Data Architect,Lingaro,10-12 Years,,India,Login to check your skill match score,"Job Title: Senior Data Architect (GCP)\nLocation: India (Remote)\nAbout Lingaro:\nLingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.\nSince 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.\nAbout Data Management Competency: Focused on Data Governance and Quality Management, establishing and enforcing policies, processes, and practices to ensure the integrity, availability, and reliability of data across the organization.\nDuties:\nFormulate and communicate the organization's data strategy, including data quality standards, data flow, and data security measures.\nProvides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements.\nDefine and implement data governance policies, procedures, and frameworks to ensure data integrity and compliance.\nCollaborate with stakeholders to align data strategy with business goals and objectives, document current and target state in the form of business process and data journey diagrams.\nDesign and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.\nDefine data standards, naming conventions, and data classification guidelines. Ensure data models are scalable, efficient, and optimized for performance.\nEvaluate and select appropriate database technologies and solutions based on organizational needs and requirements.\nDesign and oversee the implementation of data platforms, including relational databases, NoSQL databases, data warehousing, and Big Data solutions.\nOptimize database performance, ensure data security, and implement backup and recovery strategies.\nDesign data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, document source to target mappings.\nCollaborate with IT team and data experts to identify opportunities for data acquisition.\nUnderstand and follow data architecture patterns for various types of data systems, e.g. data lakehouse platforms, master data management systems, ML enriched data flows.\nImplement data profiling and data cleansing processes to identify and resolve data quality issues.\nEstablish data quality standards and implement processes to measure, monitor, and improve data quality.\nFacilitate discussions and workshops to gather requirements and align data initiatives with business goals, prepare data inventory documentation.\nCommunicate complex technical concepts effectively to both technical and non-technical stakeholders.\nStay abreast of industry trends and emerging technologies in data management, analytics, and security.\nEvaluate and recommend new tools, technologies, and frameworks to enhance data architecture capabilities.\nProvide guidance and support to developers and other team members on data-related topics.\nConduct knowledge sharing sessions and training programs to promote understanding and adoption of data architecture best practices.\nRequirements:\nBachelor's or master's degree in computer science, Information Systems, or a related field.\n10+ years experience as a Data Architect or in a similar role, ideally in a complex organizational setting.\nStrong understanding of data management principles, data modeling techniques, database design and data integration flows.\nExperience in developing and implementing data governance policies, procedures, and frameworks to ensure data integrity and compliance.\nFamiliarity with industry best practices and emerging trends in data management and governance.\nAbility to design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.\nStrong understanding of data modeling best-practices, e.g. data normalization, denormalization, generalization, and performance optimization techniques.\nExpertise in working with various database technologies, such as relational databases (e.g., Oracle, SQL Server, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB).\nKnowledge of database management systems, data warehousing, data integration technologies, and Big Data solutions.\nFamiliarity with cloud-based database, warehouse, and lakehouse platforms.\nExperience in designing and implementing data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, understanding of data integration patterns and best practices.\nUnderstanding of Business Intelligence and data analytics requirements, optimization of data storage and processing for reporting needs.\nExcellent communication and presentation skills to effectively articulate complex technical concepts to both technical and non-technical stakeholders.\nAbility to collaborate with cross-functional teams, including business analysts, developers, and data scientists, to understand requirements and drive data-related initiatives.\nStrong analytical and problem-solving abilities to identify data-related issues, propose solutions, and make data-driven decisions.\nFamiliarity with data profiling, data cleansing, data harmonization, and data quality assessment techniques.\nKnowledge of data security and privacy regulations, such as GDPR or CCPA, understanding of data encryption, access controls, and data masking techniques to ensure the security of sensitive data.\nProfessional certification in data management or related field would be advantageous.\nWhy join us:\nStable employment. On the market since 2008, 1300+ talents currently on board in 7 global sites.\n100% remote.\nFlexibility regarding working hours.\nFull-time position\nComprehensive online onboarding program with a Buddy from day 1.\nCooperation with top-tier engineers and experts.\nUnlimited access to the Udemylearning platform from day 1.\nCertificate training programs. Lingarians earn 500+ technology certificates yearly.\nUpskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.\nGrow as we grow as a company. 76% of our managers are internal promotions.\nA diverse, inclusive, and values-driven community.\nAutonomy to choose the way you work. We trust your ideas.\nCreate our community together. Refer your friends to receive bonuses.\nActivities to support your well-being and health.\nPlenty of opportunities to donate to charities and support the environment.","Relational Databases, data quality assessment, Big Data solutions, NoSQL databases, ETL processes, Data Integration, Data Modeling, Data Governance, Database Design, Data Profiling, Data Warehousing, Data Security, Data Cleansing"
CFIN Data Architect,ABB,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN Data Architect\n\nAt ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.\n\nWrite the next chapter of your ABB story.\n\nThis position reports to\n\nHead of Central Finance\n\nYour role and responsibilities\n\nWe are looking for an experienced and technically proficient Data Architect to lead the design, integration, and optimization of the technical solutions within the Central Finance (CFIN) landscape. The Data architect will be responsible for ensuring that data replication and technical activities are fully aligned with business needs, effectively integrated with other enterprise applications, and supported by automated solutions to enhance operational efficiency. This role involves close collaboration with various internal teams, including Finance, IS Architecture, and external vendors, to maintain and evolve the data architecture, ensuring it meets business requirements and is fully compliant with ABB's standards.\n\nThe work model for the role is:\n\nThis role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.\n\nYou will be mainly accountable for:\n\nSolution Design & Validation: Review and validate the design of all Data & Technical related solutions within the CFIN framework, ensuring they are aligned with business goals and technical requirements.\nOwnership of Data Architecture: Define, document, and own the overall data architecture within the CFIN ecosystem, including technical components, modules, and integration with other applications.\nData Replication and automation: Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG and Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data\nIntegration with other processes: Collaborate with other business streams (O2C, P2P, P2D, R2R, TAX, Treasury) to ensure data standards are maintained and design comprehensive data solutions that incorporate all work streams\nMaintain Solution Roadmap: Keep the target Data solution architecture up-to-date, documenting changes to the roadmap and their impact on the broader enterprise architecture. Collaboration with Stakeholders: Work closely with the CFIN solution team, IS architects, vendors, and business stakeholders (including Finance, Process, Data, and Systems Finance teams) to configure, maintain, and enhance the CFIN landscape, ensuring business continuity.\nBusiness Process Alignment: Collaborate with Data Global Process Owners (GPOs) and business teams to define and implement robust Data solutions that align with business requirements and global best practices. Automation & Innovation: Drive the regular implementation of automation solutions within the CFIN system to streamline Data processes, reduce manual effort, and improve efficiency.\nRequirements Validation: Support the validation of business and functional requirements alongside Process Owners, FPDS team, and Technical Leads, ensuring processes are allocated to the appropriate applications and technologies.\nCompliance & Standards: Ensure that all Data & technical solutions and work processes are compliant with ABB's internal standards, policies, and regulatory requirements. Continuous Improvement: Maintain and enhance domain expertise in Data and related technologies, keeping abreast of industry trends and ABB standards to drive continuous improvement within the organization.\n\nQualifications for the role\n\nEducation: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in FICO SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.\nAt least 7-10 years of experience in Data Architect, SAP Architect, or a similar role, with deep knowledge of Data processes and system integration.\nAdvanced expertise in SAP Central Finance (CFIN), SAP S/4HANA, or other ERP systems. Proficient in data process automation tools and strategies.\nExtensive experience with data migration and replication between SAP systems. In-depth knowledge of SAP Business Technology Platform (BTP), FIORI, and other related applications.\nStrong understanding of real-time data replication and automation standards. Strong leadership and team management skills, with the ability to motivate and guide cross-functional teams.\nExcellent collaboration skills with the ability to coordinate between different stakeholders, including business leaders, technical teams, and external partners.\nA strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.\nExperience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.\n\nMore about us\n\nFinance Services is ABB's shared services organization which delivers operational and expert services in Finance, with employees based in five main hubs and front offices, finance service provides mainly Business services to ABB teams across the globe as well as supports with external customer inquiries.\n\nWe value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory\n\nIt has come to our attention that the name of ABB is being used for asking candidates to make payments for job opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made available on our career portal for all fitting the criteria to apply. ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions. For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Real-time data replication and automation standards, Data migration and replication between SAP systems, Data process automation tools, Fiori"
Lead Data Architect,Chevron,10-15 Years,,"Bengaluru, India",Login to check your skill match score,"About The Position\n\nLead Data architects lead the design and implementation of data collection, storage, transformation, orchestration (movement) and consumption to achieve optimum value from data. They are the technical leaders within data delivery teams. They play a key role in modeling data for optimal reuse, interoperability, security and accessibility as well as in the design of efficient ingestion and transformation pipelines. They ensure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration. And they instill trust through the employment of data quality frameworks and tools.\n\nThe data architect at Chevron predominantly works within the Azure Data Analytics Platform, but they are not limited to it. The Senior Data architect is responsible for optimizing costs for delivering data. They are also responsible for ensuring compliance to enterprise standards and are expected to contribute to the evolution of those standards resulting from changing technologies and best practices.\n\nKey Responsibilities\n\nDesign and overseeing the entire data architecture strategy\nMentor junior data architects to ensure skill development in alignment with the team strategy\nDesign and implement complex scalable, high-performance data architectures that meet business requirements\nModel data for optimal reuse, interoperability, security and accessibility\nDevelop and maintain data flow diagrams, and data dictionaries\nCollaborate with stakeholders to understand data needs and translate them into technical solutions\nEnsure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration\nEnsure data quality, integrity, and security across all data systems\n\nRequired Qualifications\n\nBachelor's degree in computer science, Information Technology, or a related field (or equivalent experience)\nOverall 10-15 years of experience with at least 5 years of proven experience as a Data Architect or similar role\nStrong knowledge of data modeling, data warehousing, and data integration techniques\nProficiency in database management systems (e.g., SQL Server, Oracle, PostgreSQL)\nExperience with big data technologies (e.g., Hadoop, Spark) and data lake solutions (e.g., Azure Data Lake, AWS Lake Formation)\nExperience with big data technologies data lake solutions DBMS and cloud platforms\nExperience in data modeling, ERDs, Star and/or Snowflake, and physical model design for analytics and application integration\nExperience in designing data pipelines for optimal performance, resiliency, and cost efficiency\nExperience translating business objectives and goals into technical architecture for data solutions\nFamiliarity with cloud platforms (e.g., Microsoft Azure, AWS, Google Cloud Platform)\nStrong understanding of data governance and security best practices\nExcellent problem-solving skills and attention to detail\nStrong communication and collaboration skills\nTrack record for defining/implementing data architecture framework and governance around master data, meta data, modeling\n\nPreferred Qualifications\n\nExperience in Erwin, Azure Synapse, Azure Databricks, Azure DevOps, SQL, Power BI, Spark, Python, R\nAbility to drive business results by building optimal cost data landscapes\nFamiliarity with Azure AI/ML Services, Azure Analytics: Event Hub, Azure Stream Analytics, Scripting: Ansible\nExperience with machine learning and advanced analytics\nFamiliarity with containerization and orchestration tools (e.g., Docker, Kubernetes)\nUnderstanding of CI/CD pipelines and automated testing frameworks\nCertifications such as AWS Certified Solutions Architect, IBM certified data architect or similar are a plus\n\nChevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm.\n\nChevron participates in E-Verify in certain locations as required by law.","CI CD pipelines, data integration techniques, Azure Analytics, Azure AI ML Services, Azure Stream Analytics, data pipelines, R, Event Hub, AWS Lake Formation, Hadoop, Erwin, Power Bi, Azure Databricks, Data Warehousing, PostgreSQL, Azure DevOps, SQL Server, Data Modeling, Data Governance, Ansible, AWS, Oracle, Kubernetes, Python, Azure Synapse, Docker, Azure Data Lake, Microsoft Azure, Google Cloud Platform, Spark"
Data Architect III - Workforce Technology,JPMorganChase,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nBe a part of a dynamic team and excel in an environment that values diversity and creativity. Continue to sharpen your skills and ambition while pushing the industry forward.\n\nAs a Data Architect at JPMorgan Chase within the Employee Platforms, you serve as a seasoned member of a team to develop high-quality data architecture solutions for various software applications and platforms. By incorporating leading best practices and collaborating with teams of architects, you are an integral part of carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.\n\nIn this role, you will be responsible for designing and implementing data models that support our organization's data strategy. You will work closely with Data Product Managers, Engineering teams, and Data Governance teams to ensure the delivery of high-quality data products that meet business needs and adhere to best practices.\n\nJob Responsibilities\n\nExecutes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems\nCollaborate with Data Product Managers to understand business requirements and translate them into data modeling specifications. Conduct interviews and workshops with stakeholders to gather detailed data requirements.\nCreate and maintain data dictionaries, entity-relationship diagrams, and other documentation to support data models.\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development\nEvaluates data architecture designs and provides feedback on recommendations\nRepresents their team in architectural governance bodies\nLeads the data architecture team in evaluating new technologies to modernize the architecture using existing data standards and framework\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems\nProactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to data architecture communities of practice and events that explore new and emerging technologies\n\nRequired Qualifications, Capabilities, And Skills\n\nFormal training or certification on Data Architect and 3+ years applied experience\nHands on experience in data platforms, cloud services (eg, AWS, Azure or Google Cloud) and big data technologies\nStrong understanding of database management systems, data warehousing, and ETL processes.\nExcellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.\nKnowledge of data governance principles and best practices.\nAbility to evaluate current technologies to recommend ways to optimize data architecture\nHands-on practical experience in system design, application development, testing, and operational stability\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming and database querying languages\nOverall knowledge of the Software Development Life Cycle\nSolid understanding of agile methodologies such as continuous integration and delivery, application resiliency, and security\nDemonstrated knowledge of software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)\n\nPreferred Qualifications, Capabilities, And Skills\n\nExperience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud).\nFamiliarity with big data technologies (e.g., Hadoop, Spark).\nCertification in data modeling or data architecture.\n\nAbout Us\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","application resiliency, Security, ETL processes, data governance principles, Delivery, Mobile, operational stability, database querying languages, Continuous Integration, Data Architect, Data Warehousing, Big Data Technologies, Database Management Systems, Artificial Intelligence, Software Development Life Cycle, cloud, AWS, Machine Learning, Testing, Application Development, Google Cloud, System Design, Agile Methodologies, Azure"
Principal Data Architect - Data Engineering,Minfy,Fresher,,"Hyderabad, India",Login to check your skill match score,"Role & Responsibilities\n\nWe are seeking a highly skilled and experienced Architect to lead the design, implementation, and optimization Intelligent data Platform. The ideal candidate will have a strong background in cloud-based data processing systems, data warehousing, and big data technologies. They will work closely with our data engineering team to ensure that the data platform is optimized for performance, scalability, and reliability.\n\nCollaborate with stakeholders to understand business objectives and translate them into data architecture requirements.\nDesign and implement data models, develop Data attribute maps, database schemas, and data integration strategies that comply with regulatory requirements and industry best practices.\nDevelop and maintain data governance policies and procedures to ensure the confidentiality, integrity, and availability of sensitive data.\nImplement data security measures and access controls to protect against unauthorized access and mitigate potential risks.\nArchitect and optimize data storage and retrieval processes to meet the performance and scalability demands of banking applications.\nLeverage AWS services such as Amazon Redshift, Amazon RDS, Amazon Aurora, and AWS Glue to build scalable and cost-effective data solutions.\nArchitect and design solutions to meet functional and non-functional requirements.\nLead the design, implementation, and optimization of Intelligent Data platform.\nDevelop and maintain a comprehensive understanding of data pipeline and data architecture.\nDevelop and maintain documentation for our Intelligent data platform including architecture diagrams, deployment guides, and operational procedures.\nProvide guidance and support to our data engineering team.\nCreate and review architecture and solution design artifacts.\nEvangelize re-use through the implementation of shared assets.\nEnforce adherence to architectural standards/principles, global product-specific guidelines, usability design standards, etc.\nProactively guide engineering methodologies, standards, and leading practices.\nIdentify, communicate, and mitigate Risks, Assumptions, Issues, and Decisions throughout the full lifecycle.\nConsiders the art of the possible, compares various architectural options based on feasibility and impact, and proposes actionable plans.\nDemonstrate strong analytical and technical problem-solving skills.\nAbility to analyze and operate at various levels of abstraction.\nAbility to balance what is strategically right with what is practically realistic.\nGrowing the Data Engineering business by helping customers identify opportunities to deliver improved business outcomes, designing and driving the implementation of those solutions.\nLeading team in the definition of best practices & repeatable methodologies in Cloud Data Engineering, including Data Storage, ETL, Data Integration & Migration, Data Warehousing and Data Governance\nShould have Technical Experience in AWS Cloud Data Engineering services and solutions.\nContributing to Sales & Pre-sales activities including proposals, pursuits, demonstrations, and proof of concept initiatives\nEvangelizing the Data Engineering service offerings to both internal and external stakeholders\nDevelopment of Whitepapers, blogs, webinars and other though leadership material\nDevelopment of Go-to-Market and Service Offering definitions for Data Engineering\nExpand the business within existing accounts and help clients, by building and sustaining strategic executive relationships, doubling up as their trusted business technology advisor.\nPosition differentiated and custom solutions to clients, based on the market trends, specific needs of the clients and the supporting business cases.\nBuild new Data capabilities, solutions, assets, accelerators, and team competencies.\n\nMandatory Skills Description\n\n- Provide technical leadership and mentorship to junior members.\n- Proven experience as a Data Architect with a deep understanding of Enterprise systems and processes.\n- Strong proficiency in SQL and database technologies, with experience in designing and optimizing data models for core applications.\n- Hands-on experience with AWS cloud services, particularly those relevant to data architecture such as Amazon Redshift, Amazon RDS, AWS Glue, and AWS Lambda.\n- Technology Agnostic approach for multitude of data source systems and API gateways\n\nNice-to-Have Skills\n\nFamiliarity with compliance requirements such as GDPR, PII protection frameworks, PCI DSS.\nExcellent communication and interpersonal skills, with the ability to effectively collaborate with diverse teams and stakeholders.\nAWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified Data Analytics Specialty) are highly desirable.\n\nMinimum Qualifications\n\nExcellent technical architecture skills, enabling the creation of future-proof, complex global Platform solutions.\n\nExcellent interpersonal communication and organizational skills are required to operate as a leading member of global, distributed teams that deliver quality services and solutions.\nAbility to rapidly gain knowledge of the organizational structure of the firm to facilitate work with groups outside of the immediate technical team.\nKnowledge and experience in IT methodologies and life cycles that will be used.\nFamiliar with solution implementation/management, service/operations management, etc.\nMaintains close awareness of new and emerging technologies and their potential application for service offerings and products.\nBachelor's Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline) or equivalent work experience.\nExperience in architecting and designing technical solutions for cloud-centric solutions based on industry standards using IaaS, PaaS, and SaaS capabilities.\nMust have strong hands-on experience on various cloud services like Lambda, S3, Security, Monitoring, Governance & Compliance.\nMust have good knowledge of Data Engineering concept and related services of cloud.\nMust have good experience in Python and Spark.\nMust have good experience in setting up development best practices.\nExperience with claims-based authentication (SAML/OAuth/OIDC), MFA,RBAC, SSO etc.\nKnowledge of cloud security controls including tenant isolation, encryption at rest, encryption in transit, key management, vulnerability assessments, application firewalls, SIEM, etc.\nExperience building and supporting mission-critical technology components with DR capabilities.\nExperience with multi-tier system and service design and development for large enterprises\nExtensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.\nExposure to infrastructure and application security technologies and approaches\nFamiliarity with requirements gathering techniques.\nPreferred Qualifications\nMust have experience in DevSecOps working closely on Data Engineering based project\nStrong expertise in Data platform component\nDelta lake\ndb API 2.0\nSQL Endpoint Photon engine\nDelta Sharing\nUnity Catalog\nSecurity management\nPlatform governance, Auditing & Compliance\nData Security\nProficiency in AWS services including but not limited to S3, EC2, IAM, VPC, EKS, Lambda, Glue, Private Link, KMS, CloudWatch, EMR etc.\nMust know how to enable geo redundancy and DR capabilities on databricks.\nProficient in designing and implementing\nEverything as a code\nInfrastructure as a code\nConfiguration as a code\nConfiguration as a code\nSecurity configuration as a code\nMust have strong expertise in designing platform with strong observability and Monitoring standards.\nProficient in developing and setting best practices of various DevSecOps activities including CI/CD.\nGood to have Rest API knowledge.\nGood to have understanding around cost distribution.\nGood to have if worked on migration project to build Unified data platform.\nGood to have knowledge of DBT.\nSoftware development full lifecycle methodologies, patterns, frameworks, libraries, and tools\nKnowledge of programming and scripting languages such as JavaScript, Bash, SQL, Python, etc.\nExperience in distilling complex technical challenges to actionable decisions for stakeholders and guiding project teams by building consensus and mediating compromises when necessary.\nExperience coordinating the intersection of complex system dependencies and interactions\nExperience in solution delivery using common methodologies especially SAFe Agile but also Waterfall, Iterative, etc.","Auditing, Compliance, Unity Catalog, SQL Endpoint Photon engine, Delta Sharing, Platform governance, Delta lake, db API 2.0, Data Security, Data Governance, Data Warehousing, Data Architecture, Sql, Security Management, DevSecOps, Data Integration, Spark, Python, Etl"
Data Architect,Gainwell Technologies,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Summary\n\nWe're looking for a Solution Architect to join our product development team to drive a cloud based data platform architecture that will power BI and advanced analytics at Gainwell Technologies. You will have a direct impact on the design, architecture, and implementation of BI and data science use cases. The ideal candidate will have industry-leading programming skills and established knowledge of implementing, designing, deploying, and maintaining big data analytics platforms in a cloud environment.\n\nThe solution architect will use knowledge of healthcare data to influence the implementation and governance of our data architecture. Your designs will account for data movement, storage, compute and BI consumption, and will comply with security and data governance standards. You will work closely with a variety of partners within the Data and Analytics organization.\n\nYour role in our mission\n\nProvide thought leadership and technical direction to the data engineering team in building analytic data products\nUnderstand and translate business requirements to data strategies that align with overall technology vision\nDesign, develop and enforce standards for the data storage, processing and governance across all environments\nWork closely with enterprise and application architects to align the data engineering team to the overall company SDLC standards, practices, and data access patterns\nDevelop, document and maintain overall view of data platform architecture, data acquisition, data quality, and data retention\nProvide formal and informal training for data engineers, platform engineers and ETL developers\nMaintain knowledge of emerging technologies and architectures\nDocument and publish technical principles and standards, and mentor the data engineering team to incorporate them into their daily practices\nChampion and present the technical vision to the executive team and business stakeholders\n\nWhat we're looking for\n\nBasic Qualifications\n\nBachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields\n8+ years of overall experience in big data, database and enterprise data architecture and delivery\n8+ years of programming proficiency in a subset of Python, Java, and Scala\n5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks\n5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms\n3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred\nStrong SQL skills to create/maintain DB objects, query/load required data using data governance (e.g. business glossary, data dictionary, data catalog, data quality, master data management, etc.) and visualization tools to bring data literacy to the organization.\nPractical experience on workload management, monitoring, and performance tuning Apache Spark jobs\nBroad knowledge of data technologies, tools, and disciplines including data modeling, dimensional models, third-normal-form structures, ETL/ELT, change data capture and slowly changing dimensions\nExperience with healthcare data a big plus\nExperience with Machine Learning & MLOPs is a big plus\n\nWhat you should expect in this role\n\nClient or office environment / may work remotely\nOccasional evening and weekend work\n\nReq. ID: 26672","Java, Machine Learning, Hadoop, Scala, Data Modeling, Sql, ELT, MLops, Spark, Data Governance, Databricks, Python, Etl"
Data Architect - Data Warehousing,ParentPay Group - India,Fresher,,"Pune, India",Login to check your skill match score,"Department: Development\nLocation: Pune, India\nDescription\n\nParentPay Group is Europe's leading software product company and the UK's largest education technology business. We are on a mission to bring next-generation innovation to positively impact on the lives of millions of parents, teachers, and students every day in over 49 countries.\n\nOur market leading products use cutting edge cloud-based technology to streamline school processes, including secure web and mobile apps that enable secure online payments for school items such as meals, trips, clubs and uniform, improve parental engagement, simplify meal management and - through our product SIMS - collect and manage a database of student information and core school operations.\n\nParentPay Group's new offices in Pune are a fantastic tech hub for those looking to boost their careers in software product development.\n\nOur bright team FastTrack their career with international exposure and ways of working based on agile development best practices from globally renowned technology consultancies.\n\nKey Responsibilities\n\nResponsibilities: Data Architect\n\n\nCreating data models that specify how data is formatted, stored, and retrieved inside an organisation. This comprises data models that are conceptual, logical, and physical.\n\nCreating and optimising databases, including the selection of appropriate database management systems (DBMS) and the standardisation and indexing of data.\n\nCreating and maintaining data integration processes, ETL (Extract, Transform, Load) workflows, and data pipelines to seamlessly transport data between systems.\n\nCollaborating with business analysts, data scientists, and other stakeholders to understand data requirements and align architecture with business objectives.\n\nStay current with industry trends, best practices, and advancements in data management through continuous learning and professional development.\n\nEstablishing processes for monitoring and improving the quality of data within the organisation.\n\nImplement data quality tools and practices to detect and resolve data issues.\n\n\n\n\nRequirements and Skills: Data Architect\n\n\nPrior experience in designing Data Warehouse, data modelling, database design, and data administration is required.\n\nDatabase Expertise: Knowledge of data warehousing ideas and proficiency in various database systems (e.g., SQL).\n\nKnowledge of data modelling tools such as Visual Paradigm is required.\n\nKnowledge of ETL methods and technologies (for example, Azure ADF, Events).\n\nExpertise writing complex stored procedures.\n\nGood understanding of Data Modelling Concepts like Star Schema ,SnowFlake etc\n\nStrong problem-solving and analytical skills are required to build effective data solutions.\n\nExcellent communication skills are required to work with cross-functional teams and convert business objectives into technical solutions.\n\nKnowledge of Data Governance: Understanding data governance principles, data security, and regulatory compliance.\n\nKnowledge of programming languages such as .net can be advantageous.","data administration, Stored procedures, Database Design, .NET, Sql, Data Governance"
Data Architect,Blackbaud,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"About The Role\n\nWe are seeking a highly skilled and experienced Data Architect to join our team. The ideal candidate will have at least 12 years of experience in software & data engineering and analytics and a proven track record of designing and implementing complex data solutions. You will be expected to design, create, deploy, and manage Blackbaud's data architecture. This role has considerable technical influence within the Data Platform, Data Engineering teams, and the Data Intelligence Center of Excellence atBlackbaud. Thisindividual acts as an evangelist for proper data strategy with other teams at Blackbaud and assists with the technical direction, specifically with data, of other projects.\n\nWhat You'll Be Doing\n\nDevelop and direct the strategy for all aspects of Blackbaud's Data and Analytics platforms, products and services\nSet, communicate and facilitate technical directionmore broadly for the AI Center of Excellence and collaboratively beyond the Center of Excellence\nDesign and develop breakthrough products, services or technological advancements in the Data Intelligence space that expand our business\nWork alongside product management to craft technical solutions to solve customer business problems.\nOwn the technical data governance practices and ensures data sovereignty, privacy, security and regulatory compliance.\nContinuously challenging the status quo of how things have been done in the past.\nBuild data access strategy to securely democratize data and enable research, modelling, machine learning and artificial intelligence work.\nHelp define the tools and pipeline patterns our engineers and data engineers use to transform data and support our analytics practice\nWork in a cross-functional team to translate business needs into data architecture solutions.\nEnsure data solutions are built for performance, scalability, and reliability.\nMentor junior data architects and team members.\nKeep current on technology: distributed computing, big data concepts and architecture.\nPromote internally how data within Blackbaud can help change the world.\n\nWhat We Want You To Have\n\n10+ years of experience in data and advanced analytics\nAt least 8 years of experience working on data technologies in Azure/AWS\nExperience building modern products and infrastructure\nExperience working with .Net/Java and Microservice Architecture\nExpertise in SQL and Python\nExpertise in SQL Server, Azure Data Services, and other Microsoft data technologies.\nExpertise in Databricks, Microsoft Fabric\nStrong understanding of data modeling, data warehousing, data lakes, data mesh and data products.\nExperience with machine learning\nExcellent communication and leadership skills.\nAble to work flexible hours as required by business priorities\nAbility to deliver software that meets consistent standards of quality, security and operability.\n\nStay up to date on everything Blackbaud, follow us on Linkedin, X, Instagram, Facebook and YouTube\n\nBlackbaud is a digital-first company which embraces a flexible remote or hybrid work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!\n\nBlackbaud is proud to be an equal opportunity employer and is committed to maintaining an inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.\n\nR0012317","Microsoft Fabric, Data Mesh, Data Lakes, Azure Data Services, Analytics, Java, Databricks, Sql, SQL Server, data products, data engineering, Data Warehousing, .NET, Machine Learning, AWS, Python, Azure, Microservice Architecture, Data Modeling"
Data Architect,McCain Foods,5-8 Years,,"Delhi, India",Login to check your skill match score,"Position Title: Data Architect\n\nPosition Type: Regular - Full-Time\n\nPosition Location: New Delhi\n\nGrade: Grade 05\n\nRequisition ID: 34658\n\nJob Purpose\n\nReporting to the Data Architect Lead, Global Data Architect will take a lead role in creating the enterprise data model for McCain Foods, bringing together data assets across agriculture, manufacturing, supply chain and commercial. This data model will be the foundation for our analytics program that seeks to bring together McCain's industry-leading operational data sets, with 3rd party data sets, to drive world-class analytics. Working with a diverse team of data governance experts, data integration architects, data engineers and our analytics team including data scientist, you will play a key role in creating a conceptual, logical and physical data model that underpins the Global Digital & Data team's activities. .\n\nJob Responsibilities\n\nDevelop an understanding of McCain's key data assets and work with data governance team to document key data sets in our enterprise data catalog\nWork with business stakeholders to build a conceptual business model by understanding the business end to end process, challenges, and future business plans.\nCollaborate with application architects to bring in the analytics point of view when designing end user applications.\nDevelop Logical data model based on business model and align with business teams\nWork with technical teams to build physical data model, data lineage and keep all relevant documentations\nDevelop a process to manage to all models and appropriate controls\nWith a use-case driven approach, enhance and expand enterprise data model based on legacy on-premises analytics products, and new cloud data products including advanced analytics models\nDesign key enterprise conformed dimensions and ensure understanding across data engineering teams (including third parties); keep data catalog and wiki tools current\nPrimary point of contact for new Digital and IT programs, to ensure alignment to enterprise data model\nBe a clear player in shaping McCain's cloud migration strategy, enabling advanced analytics and world-leading Business Intelligence analytics\nWork in close collaboration with data engineers ensuring data modeling best practices are followed\n\nMeasures Of Success\n\nDemonstrated history of driving change in a large, global organization\nA true passion for well-structured and well-governed data; you know and can explain to others the real business risk of too many mapping tables\nYou live for a well-designed and well-structured conformed dimension table\nFocus on use-case driven prioritization; you are comfortable pushing business teams for requirements that connect to business value and also able to challenge requirements that will not achieve the business goals\nDeveloping data models that are not just elegant, but truly optimized for analytics, both advanced analytics use cases and dashboarding / BI tools\nA coaching mindset wherever you go, including with the business, data engineers and other architects\nA infectious enthusiasm for learning: about our business, deepening your technical knowledge and meeting our teams\nHave a get things done attitude. Roll up the sleeves when necessary; work with and through others as needed\n\nKey Qualification & Experiences\n\nData Design and Governance\nAt least 5 years of experience with data modeling to support business process\nAbility to design complex data models to connect and internal and external data\nNice to have: Ability profile the data for data quality requirements\nAt least 8 years of experience with requirement analysis; experience working with business stakeholders on data design\nExperience on working with real-time data.\nNice to have: experience with Data Catalog tools\nAbility to draft accurate documentation that supports the project management effort and coding\n\nTechnical skills\nAt least 5 years of experience designing and working in Data Warehouse solutions building data models; preference for having S4 hana knowledge.\nAt least 2 years of experience in visualization tools preferably Power BI or similar tools.e\nAt least 2 years designing and working in Cloud Data Warehouse solutions; preference for Azure Databricks, Azure Synapse or earlier Microsoft solutions\nExperience Visio, Power Designer, or similar data modeling tools\nNice to have: Experience in data profiling tools informatica, Collibra or similar data quality tools\nNice to have: Working experience on MDx\nExperience in working in Azure cloud environment or similar cloud environment\nMust have : Ability to develop queries in SQL for assessing , manipulating, and accessing data stored in relational databases , hands on experience in PySpark, Python\nNice to have: Ability to understand and work with unstructured data\nNice to have at least 1 successful enterprise-wide cloud migration being the data architect or data modeler. - mainly focused on building data models.\nNice to have: Experience on working with Manufacturing /Digital Manufacturing.\nNice to have: experience designing enterprise data models for analytics, specifically in a PowerBI environment\nNice to have: experience with machine learning model design (Python preferred)\nBehaviors and Attitudes\nComfortable working with ambiguity and defining a way forward.\nExperience challenging current ways of working\nA documented history of successfully driving projects to completion\nExcellent interpersonal skills\nAttention to the details.\nGood interpersonal and communication skills\nComfortable leading others through change\n\nMcCain Foods is an equal opportunity employer. We see value in ensuring we have a diverse, antiracist, inclusive, merit-based, and equitable workplace. As a global family-owned company we are proud to reflect the diverse communities around the world in which we live and work. We recognize that diversity drives our creativity, resilience, and success and makes our business stronger.\n\nMcCain is an accessible employer. If you require an accommodation throughout the recruitment process (including alternate formats of materials or accessible meeting rooms), please let us know and we will work with you to meet your needs.\n\nYour privacy is important to us. By submitting personal data or information to us, you agree this will be handled in accordance with the Global Employee Privacy Policy\n\nJob Family: Information Technology\n\nDivision: Global Digital Technology\n\nDepartment: Data Architect\n\nLocation(s): IN - India : Haryana : Gurgaon\n\nCompany: McCain Foods(India) P Ltd","Visio, Data Catalog Tools, power designer, Data Profiling Tools, Data Warehouse Solutions, Data Modeling, Power Bi, Pyspark, Azure Databricks, Sql, Azure Synapse, Python"
AI & Data Architect - Azure,CloudFronts - Microsoft Solutions Partner,5-7 Years,,"Mumbai, India",Login to check your skill match score,"AI & Data Architect - Azure\nJob Summary:\nAI & Data Architect will lead all aspects of AI, Business Intelligence, and Data Architecture with expertise in Azure services, including AI/ML solutions, data engineering, and analytics. The role involves designing, configuring, and managing BI services, aggregating data from multiple sources into a data warehouse, implementing AI-driven insights, and deploying AI models while adhering to best practices.\nSkills, Experience & Key Responsibilities :\n5+ years of experience in AI, Data Engineering, and Analytics with Azure cloud technologies.\n5+ years of integration experience with the Azure Platform (Azure Data Factory, Azure Data Lake, Azure Synapse, Azure Databricks, Azure Machine Learning, Azure Cognitive Services, Logic Apps, API Management).\nHands-on experience with AI/ML models, NLP, Computer Vision, and Predictive Analytics using Azure AI services.\nArchitect and manage AI-driven BI solutions (portals, dashboards, standard, and ad-hoc reporting) with a focus on data management and AI-driven insights.\nExperience in developing and deploying AI models into production environments.\nDeep analytical experience and understanding of data modeling and big data solutions.\nStrong knowledge of requirements gathering, documentation processes, and stakeholder management.\nProficiency in programming languages such as Python, R, or SQL for AI/ML model development.\nDesigning & developing dimensions, hierarchies & cubes with Azure Synapse & Databricks.\nProvide technical direction and mentoring to a team of AI, BI, and Data Engineers working with enterprise data tools (SSRS, SSIS, SQL Server, Power BI, Azure ML, Databricks).\nEnsures best practices in AI model development, testing, and deployment while overseeing, planning, and estimating project needs.\nProvide technical leadership on client AI & Data projects and during the sales cycle.\nLocation: Mumbai\nJob Type: Full Time (5days WFO)\nWorking Hours: 8.30 am to 5.00 pm (Monday to Friday)\nExperience Range: 5+ years of experience (Relevant)\nABOUTCLOUDFRONTS:\nCloudFronts is a 100% Dynamics 365 focused Microsoft Solutions Partner helping Teams & Organizations worldwide solve their Complex Business Challenges with Microsoft Cloud. Our head office and robust delivery center are based out of Mumbai, India along with branch offices in Singapore & U.S.\nCloudFronts was established in 2012 by a former Microsoft CRM Solution Architect Anil Shah with a mission to help other businesses scale up their productivity and reduce their costs concurrently with Microsoft Dynamics. Since its inception, the CloudFronts team has successfully served over 500+ small and medium-sized clients all over the world such as North America, Europe, Australia, Maldives & India with diverse experiences in sectors ranging from Professional services, Finances, Pharmaceutical, Manufacturing, F&B, Retail, Logistics, Energy, Automotive and non-profits.\nOur customer success stories and testimonials speak for us. We urge you to look at https://www.cloudfronts.com/dynamics-365-customer-success-stories/\nExplore the power of Microsoft Dynamics at www.cloudfronts.com","Analytics, Azure Cognitive Services, R, Logic Apps, AI ML solutions, Sql, SQL Server, SSIS, Computer Vision, Azure Data Factory, Ssrs, Azure Machine Learning, Power Bi, Azure Databricks, data engineering, Python, Azure Synapse, Azure, Api Management, Azure Data Lake, Nlp, Predictive Analytics"
Data Architect (Snowflake),P99SOFT,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"We are seeking a highly motivated and experienced Data Architect to join our growing team. The ideal candidate will have a strong understanding of data warehousing principles and extensive experience with Snowflake, a leading cloud-based data warehouse platform.\n\nResponsibilities\n\nDesign, develop, and implement data warehouse solutions using Snowflake for various business needs.\nLead the migration of existing data pipelines and projects from legacy systems to Snowflake.\nDevelop and maintain ETL processes (Extract, Transform, Load) using DBT (Data Build Tool) and/or Informatica Power Center.\nEnsure data quality through the implementation of robust data quality rules and monitoring frameworks.\nOwn and manage Snowflake administration activities for assigned projects.\nCollaborate with business users to understand their requirements and translate them into actionable data solutions.\nMentor and guide team members on best practices for data warehousing and Snowflake utilization.\n\nQualifications\n\n10+ years of experience in data warehousing and data management.\n5+ years of experience with Snowflake, including a strong understanding of its architecture, capabilities, and best practices.\nExpertise in DBT (Data Build Tool) or Informatica Power Center for building ETL pipelines.\nFamiliarity with cloud platforms such as AWS S3 and Azure.\nExperience with relational databases like Oracle, SQL Server, and MySQL.\nExperience with data governance and security best practices.\nExcellent communication and collaboration skills.\nStrong analytical and problem-solving abilities.\nAbility to work independently and as part of a team.\n\nBenefits\n\nCompetitive salary and performance-based bonuses.\nComprehensive health insurance.\nPaid time off and flexible working hours.\nProfessional development opportunities and support for continuing education.\nUdemy Learning Licenses to enhance skills and stay up-to-date with the latest technology.\n\nP99soft is an equal-opportunity employer\n\nP99soft is an equal-opportunity employer. At P99soft, we are committed to providing equal employment opportunities regardless of job history, disability, gender identity, religion, race, color, caste, marital/parental status, veteran status, or any other special status. We stand against the discrimination of employees and individuals and are proud to be an equitable workplace that welcomes individuals from all walks of life if they fit the designated roles and responsibilities.\n\nLife@P99soft\n\nAt P99soft, we believe in creating a work environment that balances professional excellence with personal well-being. Our vibrant office culture is built on mutual respect, inclusivity, and a shared passion for innovation. We host regular team-building activities, workshops, and social events to foster a sense of community and collaboration among our employees. Join us and be part of a team that values your contributions and supports your growth\n\nAbout Us\n\nP99soft is a leading IT Services and IT Consulting company dedicated to making our customers business operations seamless by providing them with digital services and applications using the latest technologies. We are committed to bringing excellence to all our creations. We need passion, empathy, and being customer-focused to achieve technical excellence. We believe in transparency through open communication and work with honesty and integrity while dealing with our customers and partners.\n\nOur Vision: To lead digital transformation for our customers, delighting them at every step, while empowering our employees and fostering success.\n\nOur Mission: To attain our objectives within a framework of integrity, transparency, and respect for our clients, employees, partners, and the community, fostering mutual trust and sustainable growth.\n\nJourney towards digital transformations begins here. Our talented teams help you make informed decisions, enhance operation efficiencies, create state-of-the-art visual interfaces, and design dynamic workflows to bring a better customer experience. Our engineering teams take complete ownership of assigning the right talent and architecting solutions using the latest technologies.","snowflake, DBT Data Build Tool, Informatica Power Center, MySQL, SQL Server, Azure, Oracle, Aws S3"
AWS Data Architect Lead,Thoucentric,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"About Us\n\nThoucentric is the Consulting arm of Xoriant, a prominent digital engineering services company with 5000+ employees. We are headquartered in Bangalore with presence across multiple locations in India, US, UK, Singapore & Australia Globally.\n\nAs the Consulting business of Xoriant, We help clients with Business Consulting, Program & Project Management, Digital Transformation, Product Management, Process & Technology Solutioning and Execution including Analytics & Emerging Tech areas cutting across functional areas such as Supply Chain, Finance & HR, Sales & Distribution across US, UK, Singapore and Australia. Our unique consulting framework allows us to focus on execution rather than pure advisory. We are working closely with marquee names in the global consumer & packaged goods (CPG) industry, new age tech and start-up ecosystem. Xoriant (Parent entity) started in 1990 and is a Sunnyvale, CA headquartered digital engineering firm with offices in the USA, Europe, and Asia. Xoriant is backed by ChrysCapital, a leading private equity firm. Our strengths are now combined with Xoriant's capabilities in AI & Data, cloud, security and operations services proven for 30 years.\n\nWe have been certified as Great Place to Work by AIM and have been ranked as 50 Best Firms for Data Scientists to Work For.\n\nWe have an experienced consulting team of over 450+ world-class business and technology consultants based across six global locations, supporting clients through their expert insights, entrepreneurial approach and focus on delivery excellence. We have also built point solutions and products through Thoucentric labs using AI/ML in the supply chain space.\n\nJob Description\n\nPosition Overview\n\nWe are looking for an experienced AWS Data Engineer to design, develop, and maintain data solutions using core AWS services. The ideal candidate will have hands-on experience with tools like Amazon S3, Redshift, AWS Glue, and DynamoDB, and will build scalable, efficient, and secure data pipelines and architectures. The role also requires strong expertise in PySpark, SQL, and workflow orchestration tools like Airflow.\n\nKey Responsibilities\n\nData Pipeline Development\nDevelop and manage ETL/ELT workflows using AWS Glue and PySpark to process large datasets.\nAutomate data workflows using Apache Airflow and other orchestration tools.\nData Storage and Management\nArchitect and manage data storage in Amazon S3 , ensuring performance, cost-efficiency, and security.\nCreate and optimize Amazon Redshift clusters for data warehousing and analytics workloads.\nDesign scalable NoSQL solutions using Amazon DynamoDB for real-time data needs.\nCompute and Serverless\nBuild and deploy serverless solutions using AWS Lambda for event-driven data processing.\nConfigure and manage virtual machine instances using Amazon EC2 for custom data processing tasks.\nSecurity and Monitoring\nImplement fine-grained access controls with AWS IAM to ensure data security.\nSet up monitoring, logging, and alerts using AWS CloudWatch for proactive system health management.\nData Integration and Transformation\nCreate efficient SQL queries to handle data transformations and analytics.\nIntegrate and process structured and unstructured data from multiple sources.\nDesign data models and implement them in Redshift or DynamoDB for optimized query performance.\nCollaboration and Optimization\nCollaborate with data scientists, analysts, and stakeholders to gather requirements and deliver solutions.\nContinuously optimize data processing workflows to improve performance and reduce costs.\n\nRequirements\n\nRequired Skills and Qualifications\n\nCore AWS Expertise\nStrong knowledge of Amazon S3 , Redshift , AWS Glue , Lambda , DynamoDB , EC2 , IAM , and CloudWatch .\nTechnical Proficiency\nHands-on experience with PySpark for big data processing.\nProficient in writing complex SQL queries for data manipulation and analysis.\nExpertise in workflow orchestration using Apache Airflow or similar tools.\nExperience\n10+ years in data engineering with a focus on AWS technologies.\nExperience in designing scalable, fault-tolerant data pipelines.\nFamiliarity with data modeling, ETL/ELT, and data warehousing principles.\nSoft Skills\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration abilities.\nAbility to manage priorities in a fast-paced environment\n\nBenefits\n\nWhat a Consulting role at Thoucentric will offer you\n\nOpportunity to define your career path and not as enforced by a manager\nA great consulting environment with a chance to work with Fortune 500 companies and startups alike.\nA dynamic but relaxed and supportive working environment that encourages personal development.\nBe part of One Extended Family. We bond beyond work - sports, get-togethers, common interests etc. Work in a very enriching environment with Open Culture, Flat Organization and Excellent Peer Group.\nBe part of the exciting Growth Story of Thoucentric!","Aws Lambda, Amazon Ec2, Amazon S3, Pyspark, Dynamodb, AWS Glue, AWS CloudWatch, Redshift, Sql, Apache Airflow, AWS IAM, AWS"
Principal Cloud Data Architect,Algonomy,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Principal Cloud Data Architect (Big Data, PySpark, Databricks stack)\nJob Description\nAlgonomy is seeking a highly experienced Senior Cloud Data Architect specializing in Azure, Databricks, and Spark to drive digital transformation for clients by designing scalable information architectures. You have technical depth and business knowledge and can drive complex technology discussions which express the value of the these technologies, throughout the lifecycle of the engagement (sales / pre-sales, architecture development, implementation). You will understand the business use cases, research technologies, recommend solutions, define short term-tactical to long term strategic information architecture roadmap, contribute to best practices, and provide architectural guidance to project teams, ensuring high-quality technical solutions within a learning-oriented culture. Partner with customers, leveraging your technical and business acumen to drive complex technology discussions and become a trusted advisor.\nKey Responsibilities\nContribute to planning activities, particularly for price/performance engineering, and participate in customer discussions for requirement analysis. You will utilize architecture frameworks (e.g., TOGAF, Zachman) to recommend solutions using various Databricks stack & other Big Data technologies, implementation approaches, and deployment options.\nYou will perform comparative analysis of technologies and anchor Proof of Concept (PoC) development to validate solutions and mitigate risks\nDevelop architecture & transition plans which meet functional and non-functional requirements, applying knowledge of multiple build tools and provide expert guidance to project teams on quality, process, and architectural frameworks to enhance technical quality\nCollaborate with project teams to resolve complex technical issues.\nContribute to technology and architectural frameworks and deliver impactful presentations to customers showcasing thought leadership. You will also coordinate tasks within teams, providing feedback to ensure deliverables meet standards.\nWork with Sales to develop account strategies, establish standard data architectures (Databricks Lakehouse architecture), and build/present reference architectures and demos to prospects. Your focus will be to capture technical wins by consulting on big data, data engineering, and data science projects.\nRequired Qualifications\nBachelor's degree or equivalent experience.\nMinimum 12 years of IT experience, including 7+ years hands-on experience architecting solutions on Azure, Databricks, and Spark.\nExperience in handling TBs of data across use cases - batch, streaming, information reporting\n5+ years in a customer-facing role (pre-sales, technical architecture, or consulting) with expertise in: big data engineering (e.g., Spark, Hadoop, Kafka, pandas), data warehousing & ETL (e.g., SQL, OLTP/OLAP/DSS), data science/machine learning (nice to have)\nProven experience designing and implementing complex distributed systems, leading and mentoring teams, and fluency in Python, SQL. and\nExperience with Master Data Management, ETL, Data Quality, metadata management, data profiling, and handling batch and streaming data.\nExtensive experience with CI/CD platforms (e.g., GitLab CI, GitHub Actions, Azure Pipelines, Jenkins).\nDebug and development experience of Python\nAbility to translate business needs to technical solutions and establish buy-in with stakeholders.\nExperienced at designing, architecting, and presenting data systems for customers and managing the delivery of production solutions of those data architectures.","Azure Pipelines, DSS, GitLab CI, GitHub Actions, SQL OLTP, Master Data Management, Machine Learning, Hadoop, Metadata Management, OLAP, Kafka, Azure Databricks, Sql, Big Data Technologies, Jenkins, Data Quality, Pandas, Data Science, Spark, Python, Data Profiling, Etl"
Cloud Data Architect,Go Digital Technology Consulting LLP,10-12 Years,,"Mumbai, India",Login to check your skill match score,"Job Description\n\nAt Go Digital Technology Consulting LLP (GDTC), we are redefining the future of data consulting and services. Our expertise spans Data Engineering, Analytics, and Data Science, enabling us to craft cutting-edge cloud data solutions tailored to our clients unique needs. Specializing in AWS, Azure, and Snowflake, we empower organizations to harness the full potential of their data, transforming it into actionable insights that drive impact.\n\nOur team is a dynamic mix of technologists, product managers, thinkers, and architects, unified by the vision to deliver exceptional value. By blending technological expertise with a deep understanding of business needs, we create solutions that not only meet expectations but set new industry benchmarks.\n\nJoin us on a transformative journey where your skills and vision will directly shape the future of cloud data architecture.\n\nRole Summary\n\nAs a Cloud Data Architect, you will play a pivotal role in designing and implementing scalable, high-performance cloud-native data architectures. You will lead the architecture of complex, large-scale data environments while fostering innovation and delivering robust solutions that drive business value.\n\nKey Technologies / Skills\n\nMastery of SQL, Python, PySpark, and Shell scripting.\nExpertise in data modeling and big data ecosystems (e.g., Hadoop, Hive) and ETL pipelines.\nDeep understanding of cloud-native data solutions across AWS, Azure, or GCP, with experience in Snowflake.\nStrong knowledge of modern data architectures, including serverless computing, data lakes, and analytics solutions.\n\nResponsibilities\n\nDesign and implement innovative cloud-native data architectures, translating business needs into scalable and sustainable solutions.\nLead technology selection, ensuring alignment with client needs and organizational standards.\nDevelop comprehensive high-level design documents and frameworks to guide project execution.\nArchitect and optimize ETL pipelines, ensuring efficient data ingestion, transformation, and storage in the cloud.\nChampion best practices for data governance, security, and compliance in cloud environments.\nConduct performance tuning and optimization of cloud-based data platforms.\nCollaborate with stakeholders to align architectural solutions with business objectives.\nSupport pre-sales efforts by providing technical insights and creating compelling proposals.\nMentor and coach technical leads, enabling their growth into future data architects.\nStay ahead of emerging technologies and trends, driving continuous innovation in data engineering and architecture.\n\nRequired Qualifications\n\n10+ years of experience in data engineering and architecture, with a focus on cloud hyperscalers (AWS, Azure, GCP).\nProven expertise in cloud-native data solutions, including Snowflake and modern data lake architectures.\nAdvanced proficiency in Python, PySpark, and SQL, with experience in NoSQL databases.\nExtensive experience with data warehousing, ETL pipelines, and big data ecosystems.\nStrong knowledge of ITIL service management practices and design processes.\nDemonstrated leadership and collaboration skills, with the ability to engage diverse stakeholders.\nA bachelor's or master's degree in Computer Science, Engineering, or a related field.\n\nWhy Join Us\n\nBe part of a forward-thinking organization that values innovation, collaboration, and excellence.\nWork on pioneering projects using state-of-the-art technologies in cloud data engineering.\nEnjoy competitive compensation, comprehensive benefits, and flexible work arrangements.\nThrive in an environment that encourages personal growth and career advancement.\n\nAs a Cloud Data Architect at GDTC, your expertise will shape the future of data solutions, empowering clients to unlock the full potential of their data. Together, let's build a smarter, data-driven world.","ITIL service management practices, NoSQL databases, Analytics Solutions, data lakes, snowflake, ETL pipelines, Serverless Computing, Hadoop, Pyspark, Data Warehousing, Data Modeling, Sql, Hive, Gcp, Shell scripting, Azure, Python, AWS"
Data Architect,NewVision Software,Fresher,,"Pune, India",Login to check your skill match score,"Primary Skills:\nData Engineering using Azure stack - Data Handling, Data Modeling, Data Integration, Data Governance\nAzure Data Lake Analytics,\nAzure Synapse Analytics Engineering,\nAzure Data Factory,\nAzure Databricks,\nAzure Dataflows,\nPower BI(Expert in DAX)\nScala or Python,\nPyspark, Hadoop, Spark, Hive, Kafka, Sqoop\nT-SQL,\nNoSQL,\nCertified Azure Solution Architect\nSecondary Skills:\nSource code control systems such as GIT, AzureDevops\nDevOps Basics.\nKey Responsibilities:\n1. Designing Solutions related to data to handle data silos of our customers.\n2. Creating Data Models as part of solutions to suffice customer's needs while following best practices.\n3. Handling Data Integration with structured and Unstructured sources of data.\n4. Implementing Data Governance and security on top of the existing DWH to enhance reliability.\n5. Monitoring the existing data flows and maintaining them thereby helping to resolve bugs.\n6. Collaborating with a team whenever needed and providing business intelligence solutions.\n7. Actively participating in PI Planning and assisting the team during Sprints.\n8. Work as part of a team to develop Cloud Data and Analytics solutions.","Certified Azure Solution Architect, DevOps Basics, AzureDevops, Azure Synapse Analytics Engineering, Azure Data Lake Analytics, Azure Dataflows, Pyspark, T-sql, Hadoop, Power Bi, Scala, Kafka, Azure Databricks, Nosql, Git, Azure Data Factory, Hive, Sqoop, Spark, Dax, Python"
Freelance Data Architect,Flexing It®,Fresher,,"Mumbai, India",Login to check your skill match score,"Our Client is a leading management consulting firm and is looking for a Data Architect Consultant. The client want someone with experience working with banking and core banking systems.\nKey Deliverables:-\n1. Design the data lake, data products, and data lakehouse for the bank.\n2. Design data models, information flow, ingestion, and consumption architecture.\n3. Guide a team of data engineers.\n4. Collaborate with partners to deliver the designed patterns.\nLocation:- Mumbai, Onsite\nDuration:- 6 Months\nSkills Required\n1.Hands on experience in Hadoop and Cloudera hadoop\nExperience as an architect in distribution on premise\n3. Experience with programming languages like UNIX shell scripting, Python etc.\n4. Exposure to data modeling, and distributed computing Strong analytical and\nproblem-solving skills\n5. Good communication and collaboration skills\n6. Experience working in agile environments\n7. Experience working with banking and core banking systems is preferred","Cloudera Hadoop, Distributed Computing, Hadoop, Data Modeling, Unix Shell Scripting, Python"
Senior Data Architect,Sysmedac,Fresher,,"Chennai, India",Login to check your skill match score,"Data Architect\n\nJob location - Dubai\n\nContract\n\nJob Summary:\n\nWe are seeking a highly skilled Senior Data Architect to lead the strategic design, development, and documentation of our enterprise data architecture. The ideal candidate will play a pivotal role in evaluating the current data environment, identifying architectural gaps, and designing a future-state architecture that supports scalable integration, governance, and analytics.\n\n\n\nKey Responsibilities:\n\n1. Current State Analysis\nConduct in-depth assessments of existing data assets, systems, flows, and infrastructure.\nDocument the current state data architecture, including data sources, data models, integration methods, and reporting platforms.\nCreate detailed data flow diagrams, system interaction maps, and metadata documentation.\n\n2. Gap Analysis & Future-State Design\nAnalyze gaps between the current and desired future data architecture.\nIdentify inefficiencies, technical debt, and areas lacking governance or integration.\nDefine and document a scalable, secure, and business-aligned future state data architecture.\n\n3. Enterprise Data Modeling\nDesign and maintain the Enterprise Data Model (conceptual, logical, and physical layers).\nEnsure alignment between business definitions, data structures, and analytic/reporting requirements.\nCollaborate with business and data stewards to validate data domains and relationships.\n\n4. Integration Data Architecture\nDesign integration architecture for real-time and batch data movement using APIs, ETL/ELT tools, and streaming platforms.\nEnsure consistency, performance, and scalability of data flows across systems.\nWork with application teams and data engineers to enforce standardized integration practices.\n\n5. Architecture Roadmap & Governance\nDevelop a comprehensive roadmap to transition from current to target data architecture.\nDefine milestones, dependencies, tools/platforms, and required skillsets.\nSupport governance efforts by embedding architectural principles into data policies and standards.\n\n6. Data Quality Framework Design\nDefine a Data Quality Framework aligned with business rules and operational KPIs.\nIdentify critical data elements (CDEs), define quality dimensions, thresholds, and remediation processes.\nCollaborate with data governance and engineering teams to implement quality checks, dashboards, and monitoring mechanisms.","Streaming Platforms, Data Quality Framework, Enterprise Data Architecture, Apis, Data Modeling, Integration Architecture, ELT, Etl"
Data Architect,Wavicle Data Solutions,5-7 Years,,India,Login to check your skill match score,"Multiple years of hands-on experience working as a data architect on Azure or AWS or Snowflake cloud ecosystems.\nDesigning & implementing the organization's data architecture, including data models, data flows, and data storage solutions.\nCreating and maintaining data dictionaries and data maps to document data sources, data types, and data relationships.\nCollaborating with business analysts to understand business requirements and develop data solutions that meet those requirements.\nWorking with data scientists and data analysts to ensure that data is available, accurate, and accessible for analysis.\nDeveloping data security and privacy policies and procedures to ensure that sensitive data is protected.\nEvaluating new data technologies and tools to determine if they are appropriate for the organization's needs.\nWorking with IT teams to implement data solutions and ensure that they integrate with existing systems and applications.\nProviding technical guidance and support to other teams and stakeholders to ensure that they can effectively use and manage\nNeed to have a strong background in data management, database design, and programming.\nGood experience with data modeling and data visualization tools and familiar with data warehousing and business intelligence concepts\nAWS certifications, such as AWS Certified Solutions Architect and AWS Certified Data Analytics - Specialty, are preferred.\nSnowflake certifications, such as SnowPro Core and SnowPro Advanced, are preferred.\nAzure certifications, such as Azure Solutions Architect Expert and Azure Data Engineer Associate, are preferred.\nExperience in designing and implementing data solutions on Azure cloud platforms, such as Azure SQL Database, Azure Cosmos DB, and Azure Synapse Analytics.\nExperience in designing and implementing data solutions on the Snowflake cloud platform, including data modeling, data flow, and data storage solutions.\nExperience in designing and implementing data solutions on the AWS cloud platform, such as AWS Redshift, AWS S3, and AWS RDS.","Business intelligence, snowflake, Data visualization tools, Database Design, Data Security, Data Management, Data Modeling, Programming, Data Warehousing, Azure, AWS"
Staff Specialist IT - PLM Data Architect,Infineon Technologies,8-10 Years,,"Ahmedabad, India",Semiconductor Manufacturing,"As a PLM Data Architect, you will be responsible for designing, implementing, and maintaining the data architecture of our Product Lifecycle Management (PLM) system. You will work closely with cross-functional teams to ensure data consistency, integrity, and quality across the entire product lifecycle. If you have a strong background in data architecture, PLM systems, and a passion for data-driven decision-making, we encourage you to apply for this exciting opportunity.\n\nJob Description\n\nIn your new role you will:\n\nDesign and Create the framework for managing the organization's enterprise data architecture.\nFocus on identifying and using the right tools and technologies, technical methodologies, technical guardrails and guidelines ,integration with broader technical environment, etc.\nDesign and implement a scalable and flexible data architecture for the PLM system, ensuring data consistency, integrity, and quality across the entire product lifecycle.\nDevelop and maintain data models, data flows, and data governance policies to ensure data accuracy, completeness, and compliance with industry standards and regulations.\nCollaborate with cross-functional teams, including engineering, manufacturing, and quality, to ensure data requirements are met and data is properly integrated across systems.\nDevelop and maintain data interfaces, APIs, and data migration strategies to ensure seamless data exchange between PLM and other systems.\nEnsure data security, access controls, and auditing mechanisms are in place to protect sensitive product data.\nDevelop and maintain data analytics and reporting capabilities to support business decision-making and product development.\n\nYour Profile\n\nYou are best equipped for this task if you have:\n\nBachelor's or Master's degree (Computer Science or Related) with 8+years of relevant experience.\nProven experience as Data Architecture or in a similar role in an R&D environment.\nExperience in implementing data management, data integration and reporting technologies.\nGood Knowledge of data governance, data quality, and data security best practices.\nKnowledge in enterprise systems like PLM, ERP, MD systems.\nProficiency in data modelling and design.\nExperience working with Data Platforms, Data Catalogues, API Management and Event Driven Architecture.\nKnowledge of programming languages Python or Java , NoSQL databases, data visualization, data virtualization.\nSolid understanding of cloud services, architectures, and storage solutions.\nExcellent problem-solving and communication skills.\n\n#WeAreIn for driving decarbonization and digitalization.\n\nAs a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.\n\nAre you in\n\nWe are on a journey to create the best Infineon for everyone.\n\nThis means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicants experience and skills.\n\nPlease let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.\n\nClick here for more information about Diversity & Inclusion at Infineon.","cloud services architectures, Storage Solutions, NoSQL databases, reporting technologies, data virtualization, Data Platforms, Event Driven Architecture, enterprise systems, Data Catalogues, Data Management, Java, Api Management, Data Security, Data Modelling, Data Architecture, Data Quality, Data Governance, Data Visualization, Data Integration, Python"
Data Architect - Enterprise Architecture,Viasat,Fresher,,"Chennai, India",Login to check your skill match score,"About Us\n\nOne team. Global challenges. Infinite opportunities. At Viasat, we're on a mission to deliver connections with the capacity to change the world. For more than 35 years, Viasat has helped shape how consumers, businesses, governments and militaries around the globe communicate. We're looking for people who think big, act fearlessly, and create an inclusive environment that drives positive impact to join our team.\n\nWhat You'll Do\n\nThe Enterprise Architecture team is focused on providing solutions to enable an effective software engineering workforce that can scale to the business needs. This includes exploring how the business needs map to the application portfolio, business processes, APIs, and data elements across the organization. As a member of this team, you will build up a vast knowledge in software development, cloud application engineering, automation, and container orchestration. Our ideal candidate values communication, learning, adaptability, creativity, and ingenuity. They also enjoy working on challenging technical issues and use creative, innovative techniques to develop and automate solutions. This team is focused on providing our executive and business leadership with visibility into how the software organization is functioning and what opportunities lie to transform the business. In this position you will be an integral part of the Enterprise Architecture team and make meaningful impacts in our journey towards digital transformation.\n\nThe day-to-day\n\nA Strong Data-Model and High-Quality Data are pre-requisites to provide better insights and enable solid data-driven decision making. This is also key to take advantage of various technological advances in Artificial Intelligence and Machine Learning. Your responsibilities will involve build out of data models for various aspects of our enterprise in conjunction with domain experts. Examples include but are not limited to Network, Capacity, Finance, Business Support Systems etc. Responsibilities also include working with software product teams to improve data quality across the organization.\n\nWhat You'll Need\n\nBachelor's degree or higher in Computer Science & Applications, Computer Science and Computer & Systems Engineering, Computer Science & Engineering, Computer Science & Mathematics, Computer Science & Network Security and Math & Computer Science, and/or a related field\nSolid understanding of Data Architecture and Data Engineering principles\nExperience building out data models\nExperience performing data analysis and presenting data in easy to comprehend manner.\nExperience in working with Relational Databases, NoSQL, Large Scale Data technologies (Kafka, Big Query, Snowflake etc)\nExperience with digital transformation across multiple cloud platforms like AWS and GCP.\nExperience in modernizing data platforms especially in GCP is highly preferred.\nPartner with members of Data Platform team and others to build out Data Catalog and map to the data model\nDetail Oriented to ensure that the catalog represents quality data\nSolid communication skills and ability to work on a distributed team\nTenacity to remain focused on the mission and overcome obstacles\nAbility to perform hands-on work with development teams and guide them to building necessary data models.\nExperience setting up governance structure and changing the organization culture by influence\n\nWhat Will Help You On The Job\n\n\nExperience with Cloud Technologies: AWS, GCP, and/or Azure, etc.\nExpertise in GCP data services like Cloud Pub/Sub, Dataproc, Dataflow, BigQuery, and related technologies preferred.\nExperience with Airflow, DBT and SQL.\nExperience with Open-source software like Logstash, ELK stack, Telegraf, Prometheus and OpenTelemetry is a plus.\nPassionate to deliver solutions that improve developer experience and promote API-first principles and microservices architecture.\nExperience with Enterprise Architecture and related principles\n\nEEO Statement\n\n\nViasat is proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, ancestry, physical or mental disability, medical condition, marital status, genetics, age, or veteran status or any other applicable legally protected status or characteristic. If you would like to request an accommodation on the basis of disability for completing this on-line application, please click here.","Large Scale Data technologies, Big Query, Airflow, Relational Databases, OpenTelemetry, dbt, snowflake, Telegraf, Cloud Pub Sub, Data Analysis, data engineering, Enterprise Architecture, Prometheus, Elk Stack, Kafka, Nosql, Data Architecture, AWS, Logstash, Dataproc, Sql, Cloud Technologies, Gcp, DataFlow"
Sr. Data Architect- SnowFlake,DYNE,8-10 Years,,"Chennai, India",Login to check your skill match score,"Job Description:A minimum of 8-10 years of experience in data engineering, encompassing the development and scaling of data\n\nwarehouse and data lake platforms.Working hours-8 hours , with a few hours of overlap during EST Time zone. This overlap hours is mandatory asmeetings happen during this overlap hours. Working hours will be 12 PM-9 PM\n\nResponsibilities\n\nMandatory Skills: Snowflake experiance, Data Architectureexperiance, ETL process experiance, Large Datamigration solutioning experiance\n\nLead the design and architecture of data solutions leveraging Snowflake, ensuring scalability, performance, andreliability.\nCollaborate with stakeholders to understand business requirements and translate them into technicalspecifications and data models.\nDevelop and maintain data architecture standards, guidelines, and best practices, including data governanceprinciples and DataOps methodologies.\nOversee the implementation of data pipelines, ETL processes, and data governance frameworks withinSnowflake environments.\nProvide technical guidance and mentorship to data engineering teams, fostering skill development andknowledge sharing.\nConduct performance tuning and optimization of Snowflake databases and queries.\nStay updated on emerging trends and advancements in Snowflake, cloud data technologies, data governance ,and Data Ops practices.\n\nPrimary Skills:\n\nExtensive experience in designing and implementing data solutions using Snowflake. DBT,\nProficiency in data modeling, schema design, and optimization within Snowflake environments.\nStrong understanding of cloud data warehousing concepts and best practices, particularly with Snowflake. Expertise in Dimension Modeling is a must\nExpertise in python/java/scala, SQL, ETL processes, and data integration techniques, with a focus on Snowflake.\nFamiliarity with other cloud platforms and data technologies (e.g., AWS, Azure, GCP )\nDemonstrated experience in implementing data governance frameworks and DataOps practices.\nWorking experience in SAP environments\nFamiliarity with realtime streaming technologies and Change Data Capture (CDC) mechanisms.\nKnowledge of data governance principles and DataOps methodologies\nProven track record of architecting and delivering complex data solutions in cloud platforms/ Snowflake.\n\nSecondarySkills (If Any):\n\nExperience with data visualization tools (e.g., Tableau, Power BI) is a plus.\nKnowledge of data security and compliance standards\nExcellent communication and presentation skills, with the ability to convey complex technical concepts tojuniors, non-technical stakeholders.\nStrong problem-solving and analytical skills-Ability to work effectively in a collaborative team environment andlead cross-functional initiatives.\n\nEducationalQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or related field.\n\nCertifications Required(If Any):\n\nCertifications related Snowflake (e.g., SnowPro core/Snowpro advanced Architect/Snowpro advance Data\n\nEngineer ) are desirable but not mandatory.","snowflake, Data governance frameworks, Data integration techniques, Dimension Modeling, ETL processes, DataOps practices, Sql, Data Modeling, Data Architecture, Java, Schema Design, AWS, Python, Azure, Gcp, Scala"
Data Architect,CG-VAK Software & Exports Ltd.,10-15 Years,,India,Login to check your skill match score,"Company Size\n\nMid-Sized\n\nExperience Required\n\n10 - 15 years\n\nWorking Days\n\n5 days/week\n\nOffice Location\n\nRemote Working\n\nRole & Responsibilities\n\nLead and mentor a team of data engineers, ensuring high performance and career growth.\nArchitect and optimize scalable data infrastructure, ensuring high availability and reliability.\nDrive the development and implementation of data governance frameworks and best practices.\nWork closely with cross-functional teams to define and execute a data roadmap.\nOptimize data processing workflows for performance and cost efficiency.\nEnsure data security, compliance, and quality across all data platforms.\nFoster a culture of innovation and technical excellence within the data team.\n\nIdeal Candidate\n\n10+ years of experience in software/data engineering, with at least 3+ years in a leadership role.\nExpertise in backend development with programming languages such as Java, PHP, Python, Node.JS, GoLang, JavaScript, HTML, and CSS.\nProficiency in SQL, Python, and Scala for data processing and analytics.\nStrong understanding of cloud platforms (AWS, GCP, or Azure) and their data services.\nStrong foundation and expertise in HLD and LLD, as well as design patterns, preferably using Spring Boot or Google Guice\nExperience in big data technologies such as Spark, Hadoop, Kafka, and distributed computing frameworks.\nHands-on experience with data warehousing solutions such as Snowflake, Redshift, or BigQuery\nDeep knowledge of data governance, security, and compliance (GDPR, SOC2, etc.).\nExperience in NoSQL databases like Redis, Cassandra, MongoDB, and TiDB.\nFamiliarity with automation and DevOps tools like Jenkins, Ansible, Docker, Kubernetes, Chef, Grafana, and ELK.\nProven ability to drive technical strategy and align it with business objectives.\nStrong leadership, communication, and stakeholder management skills.\n\nPreferred Qualifications\n\nExperience in machine learning infrastructure or MLOps is a plus.\nExposure to real-time data processing and analytics.\nInterest in data structures, algorithm analysis and design, multicore programming, and scalable architecture.\nPrior experience in a SaaS or high-growth tech company.\n\nPerks, Benefits and Work Culture\n\nTestimonial from a designer:One of the things I love about the design team at Wingify is the fact that every designer has a style which is unique to them. The second best thing is non-compliance to pre-existing rules for new products. So I just don't follow guidelines, I help create them.\n\nSkills: scala,hld,hadoop,kafka,python,javascript,lld,big data technologies,azure,sql,gdpr,google guice,kubernetes,gcp,soc2,compliance,aws,chef,data warehousing,docker,apache,php,leadership,golang,ansible,redshift,data,tidb,mongodb,grafana,machine learning infrastructure,css,cassandra,data architect,backend development,elk,snowflake,spring boot,bigquery,nosql,real-time data processing,html,spark,redis,automation tools,data engineering,devops tools,data governance,node.js,jenkins,cloud platforms,mlops,java","Chef, SOC2, snowflake, TiDB, Hld, Gdpr, Elk, Lld, Cassandra, Kafka, Spring Boot, Grafana, HTML, Javascript, Docker, Php, Python, AWS, Java, BigQuery, Hadoop, CSS, Scala, Node.JS, google guice, Redshift, Redis, Sql, Jenkins, Gcp, Ansible, Spark, MongoDB, Azure, Kubernetes, Golang"
Data Architect,DATAECONOMY,15-17 Years,,"Hyderabad, India",Login to check your skill match score,"About Us\n\nAbout DATAECONOMY: We are a fast-growing data & analytics company headquartered in Dublin with offices inDublin, OH, Providence, RI, and an advanced technology center in Hyderabad,India. We are clearly differentiated in the data & analytics space via our suite of solutions, accelerators, frameworks, and thought leadership.\n\nJob Description\n\n15+ years of professional experience in the field of Data Architecture, Design and Development of Data Lifecycle Management (DLM) and Data Governance projects\nExperience should include hands-on experience in implementing a Customer Data Hub Customer Information Management system or Customer Data Mart which includes Modeling Customer Data Model in the 3NF / Dimensional Model, defining and developing integration patterns like Batch and real-time, defining & developing the physical storage layer, defining & developing the consumption pattern\nExperience with Data Modeling is a MUST, experience in defining standards and data architecture best practices is a must\nExperience with Data Management/Architecture Practices like Data Integration, Data Governance, Data Quality, Metadata management, Data Security, Data Encryption etc. is a must\nMust have hands-on experience with developing large Data Mart or Data warehouse or Data hub involving data modelling, data ingestion, data processing and extract generation to the downstream systems\nMust have experience with any ETL tool like Informatica or Data Stage or other for data processing requirements.\nExperience with Identity resolution products for Name and Address standardization like Informatica IDQ / Address Doctor /IBM Quality stage is required.\nExperience with any Data Quality product like Informatica IDQ, Quality Stage or Trillium will be an added advantage\nMust have strong experience with writing SQL for pulling and analyzing source/data platforms\nExperience with Data Science models, model validation, model tuning and management will be an added advantage.\nProactively communicate and collaborate with business stakeholders to understand the business requirements, translate them into design, and develop the respective module based on the requirements.\nMust have experience working in an Agile implementation set-up with Product Backlogs, User Stories, Scrum and sprint planning etc.\nStrong verbal and written communication and English language skills\nStrong consulting skills and consulting experience are strongly desired.\n\nRequirements\n\nDeveloping Data Architecture and Design documents for Data Lake, Data warehouse & Data Marts corresponding to Customer Information Management Systems\nExperience in Data Lifecycle Management (DLM)\nConfiguring Solutions and Coding any customization required for the Data platform including Data Lake, Data warehouse and Data Marts\nWorking with the clients to understand the requirements. Develop the required codebase for the functional needs\nDevelop Source to Target mapping document for all the attributes that are required to be captured for Data Lake, Data warehouse and Data Marts\nConfigure and develop code required for Upstream and downstream system communication in a Batch and real-time mode\nProvide clarifications for any questions that the Business / SME team or any other stakeholder has on Data implementations.\nParticipate in system and acceptance testing along with the stakeholders\n\nBenefits\n\nStandard Company Benefits","Agile implementation, Data Encryption, Customer Data Hub, Data Science models, Customer Data Mart, Customer Information Management, Metadata Management, Data Modeling, Data Integration, Sql, Data Quality, Data Architecture, Data Security, Data Governance"
Technical Architect- (Data Architect),Simpplr,8-10 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Who We Are\n\nSimpplr is the AI-powered platform that unifies the digital workplace bringing together engagement, enablement, and services to transform the employee experience. It streamlines communication, simplifies interactions, automates workflows, and elevates the everyday experience of work. The platform is intuitive, highly extensible, and built to integrate seamlessly with your existing technology.\n\nMore than 1,000 leading organizations including AAA, the NHS, Penske, and Moderna trust Simpplr to foster a more aligned and productive workforce. Headquartered in Silicon Valley with global offices, Simpplr is backed by Norwest Ventures, Sapphire Ventures, Salesforce Ventures, and Tola Capital. Learn more at simpplr.com.\n\nJob Title: Technical Architect - Analytics\n\nLocation: Gurgaon Or Bangalore - Hybrid India\n\nEmployment Type: Full-Time\n\nThe opportunity\n\nWe are looking for a hands-on Technical Architect Analytics who will be responsible for designing, developing, and optimizing our data and analytics architecture. You will play a critical role in defining the data strategy, designing scalable data pipelines, and implementing best practices for real-time and batch analytics solutions. This role requires a strong technical leader who is passionate about data engineering, analytics, and driving data-driven decision-making across the organization.\n\nKey Responsibilities\n\nData Architecture & Design: Define and own the architecture for data processing, analytics, and reporting systems, ensuring scalability, reliability, and performance.\nData Engineering: Design and implement highly efficient, scalable, and reliable data pipelines for structured and unstructured data.\nBig Data & Real-Time Analytics: Architect and optimize data processing workflows for batch, real-time, and streaming analytics.\nCross-Functional Collaboration: Work closely with Product Managers, Data Scientists, Analysts, and Software Engineers to translate business requirements into scalable data architectures.\nTechnology & Best Practices: Stay ahead of industry trends, introduce modern data technologies, and drive best practices in data architecture, governance, and security.\nCode Reviews & Mentorship: Review code, enforce data engineering best practices, and mentor engineers to build a high-performance analytics team.\nData Governance & Compliance: Ensure data security, integrity, and compliance with regulations (GDPR, CCPA, etc.).\nOptimization & Performance Tuning: Identify performance bottlenecks in data pipelines and analytics workloads, optimizing for cost, speed, and efficiency.\nCloud & Infrastructure: Lead cloud-based data platform initiatives, ensuring high availability, fault tolerance, and cost optimization.\n\nWhat Makes You a Great Fit for Us\n\n\nExperience: 8+ years of experience in data architecture, analytics, and big data processing.\nProven Track Record: Experience designing and implementing end-to-end data platforms for high-scale applications.\nStrong Data Engineering Background: Expertise in ETL/ELT pipelines, data modeling, data warehousing, and stream processing.\nAnalytics & Reporting Expertise: Experience working with BI tools, data visualization, and reporting platforms.\nDeep Knowledge of Modern Data Technologies:\nBig Data & Analytics: Spark, Kafka, Hadoop, Druid, ClickHouse, Presto, Snowflake, Redshift, BigQuery.\nDatabases: PostgreSQL, MongoDB, Cassandra, ElasticSearch.\nCloud Platforms: AWS, GCP, Azure (experience with cloud data warehouses like AWS Redshift, Snowflake is a plus).\nProgramming & Scripting: Python, SQL, Java, Scala.\nMicroservices & Event-Driven Architecture: Understanding of real-time event processing architectures.\nStrategic Thinking: Ability to design and implement long-term data strategies aligned with business goals.\nProblem-Solving & Optimization: Strong analytical skills with a deep understanding of performance tuning for large-scale data systems.\nVisionary Leadership: Ability to think strategically and drive engineering excellence within the team.\nCommunication Skills: Strong interpersonal and communication skills to collaborate effectively across teams.\nAttention to Detail: An eye for detail with the ability to translate ideas into tangible, impactful outcomes.\nAgility: Comfortable managing and delivering work in a fast-paced, dynamic environment.\n\nPreferred Skills (Good To Have)\n\nHands-on experience with AWS Public Cloud.\nExperience with Machine Learning Pipelines and AI-driven analytics.\nHands-on experience with Kubernetes, Terraform, and Infrastructure-as-Code (IaC) for data platforms.\nCertifications in AWS Data Analytics, Google Professional Data Engineer, or equivalent.\nExperience with data security, encryption, and access control mechanisms.\nExperience in Event/Data Streaming platforms\nExperience in risk management and compliance frameworks\n\nSimpplr's Hub-Hybrid-Remote Model\n\n\nAt Simpplr we believe that when work is good, life is better and that belief guides all we do. Including how we approach our flexible work model. Simpplr operates with a Hub-Hybrid-Remote model. This model is role-based with exceptions and provides employees with the flexibility that many have told us they want.\n\nHub - 100% work from Simpplr office. Role requires Simpplifier to be in the office full-time.\nHybrid - Hybrid work from home and office. Role dictates the ability to work from home, plus benefit from in-person collaboration on a regular basis.\nRemote - 100% remote. Role can be done anywhere within your country of hire, as long as the requirements of the role are met.","Event-Driven Architecture, stream processing, Druid, Real-Time Analytics, snowflake, ClickHouse, Data Architecture Design, Bi Tools, data engineering, Data Modeling, Cassandra, PostgreSQL, Kafka, ELT, Microservices, Elasticsearch, Python, AWS, Java, BigQuery, Hadoop, Scala, Big Data, Redshift, Sql, Gcp, Presto, Spark, Data Visualization, Data Warehousing, MongoDB, Azure, Etl"
Sr Data Architect,HMH Tech India,5-7 Years,,"Pune, India",Login to check your skill match score,"HMH is a learning technology company committed to delivering connected solutions that engage learners, empower educators and improve student outcomes. As a leading provider of K12 core curriculum, supplemental and intervention solutions, and professional learning services, HMH partners with educators and school districts to uncover solutions that unlock students potential and extend teachers capabilities.\n\nHMH serves more than 50 million students and 4 million educators in 150 countries. HMH Technology India Pvt. Ltd. is our technology and innovation arm in India focused on developing novel products and solutions using cutting-edge technology to better serve our clients globally. HMH aims to help employees grow as people, and not just as professionals. For more information, visit www.hmhco.com\n\nThe data architect is responsible for designing, creating, and managing an organization's data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, accessible, secure, and aligned with business objectives. The data architect designs data models, warehouses, file systems and databases, and defines how data will be collected and organized.\n\nResponsibilities\n\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps\nDesigns the structure and layout of data systems, including databases, warehouses, and lakes\nSelects and designs database management systems that meet the organization's needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures\nDefines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms\nDesigns processes for the ETL process from various sources into the organization's data systems\nTranslates high-level business requirements into data models and appropriate metadata, test data, and data quality standards\nManages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps\nSimplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company\nLeads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums\nDefines and manages standards, guidelines, and processes to ensure data quality\nWorks with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions\nEvaluates and recommends emerging technologies for data management, storage, and analytics\nDesign, create, and implement logical and physical data models for both IT and business solutions to capture the structure, relationships, and constraints of relevant datasets\nBuild and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions\nEffectively collaborate and communicate with various stakeholders to understand data and business requirements and translate them into data models\nCreate entity-relationship diagrams (ERDs), data flow diagrams, and other visualization tools to represent data models\nCollaborate with database administrators and software engineers to implement and maintain data models in databases, data warehouses, and data lakes\nDevelop data modeling best practices, and use these standards to identify and resolve data modeling issues and conflicts\nConduct performance tuning and optimization of data models for efficient data access and retrieval\nIncorporate core data management competencies, including data governance, data security and data quality\n\nEducation\n\nJob Requirements\n\nA bachelor's degree in computer science, data science, engineering, or related field\n\nExperience\n\nAt least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives\nExperience leading projects involving data warehousing, data modeling, and data analysis\nDesign experience in Azure Databricks, PySpark, and Power BI/Tableau\n\nSkills\n\nAbility in programming languages such as Java, Python, and C/C++\nAbility in data science languages/tools such as SQL, R, SAS, or Excel\nProficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)\nExperience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata\nUnderstanding of entity-relationship modeling, metadata systems, and data quality tools and techniques\nAbility to think strategically and relate architectural decisions and recommendations to business needs and client culture\nAbility to assess traditional and modern data architecture components based on business needs\nExperience with business intelligence tools and technologies such as ETL, Power BI, and Tableau\nAbility to regularly learn and adopt new technology, especially in the ML/AI realm\nStrong analytical and problem-solving skills\nAbility to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings\nAbility to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders\nAbility to guide solution design and architecture to meet business needs\nExpert knowledge of data modeling concepts, methodologies, and best practices\nProficiency in data modeling tools such as Erwin or ER/Studio\nKnowledge of relational databases and database design principles\nFamiliarity with dimensional modeling and data warehousing concepts\nStrong SQL skills for data querying, manipulation, and optimization, and knowledge of other data science languages, including JavaScript, Python, and R\nAbility to collaborate with cross-functional teams and stakeholders to gather requirements and align on data models\nExcellent analytical and problem-solving skills to identify and resolve data modeling issues\nStrong communication and documentation skills to effectively convey complex data modeling concepts to technical and business stakeholders\n\nHMH Technology Private Limited is an Equal Opportunity Employer and considers applicants for all positions without regard to race, colour, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. We are committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit https://careers.hmhco.com/ . Follow us on Twitter, Facebook, LinkedIn, and YouTube.","snowflake, R, Teradata, Tableau, Databricks, Sql, Java, Excel, Er Studio, SAS, DataFlow, Hadoop, Erwin, Pyspark, Kafka, Power Bi, Etl, Azure Databricks, AWS, Oracle, Python, Nosql, Azure, Gcp"
Data Architect,VLink Inc,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"VLink Inc Hiring: Data Architect | Bangalore | Hybrid Work Mode\nWe're looking for expert Data Architects with 1215 years of experience to join us immediately in Bangalore.\nWe are seeking experienced Data Architects to design and implement enterprise data solutions, ensuring data governance, quality, and advanced analytics capabilities. The ideal candidate will have expertise in defining data policies, managing metadata, and leading data migrations from legacy systems to Microsoft Fabric/DataBricks/Snowflake. Experience and deep knowledge about at least one of these 3 platforms is critical. Additionally, they will play a key role in identifying use cases for advanced analytics and developing machine learning models to drive business insights.\nKey Responsibilities:\n1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.\n2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.\n3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.\nRequired Qualifications:\nProven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems. Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP). Strong communication skills with the ability to translate complex data concepts into business insights.\nPreferred candidate profile\nData Modeling (Conceptual, Logical, Physical)- Minimum 5 years\nDatabase Technologies (SQL Server, Oracle, PostgreSQL, NoSQL)- Minimum 5 years\nCloud Platforms (AWS, Azure, GCP) - Minimum 3 Years\nETL Tools (Informatica, Talend, Apache Nifi) - Minimum 3 Years\nBig Data Technologies (Hadoop, Spark, Kafka) - Minimum 5 Years\nData Governance & Compliance (GDPR, HIPAA) - Minimum 3 years\nMaster Data Management (MDM) - Minimum 3 years\nData Warehousing (Snowflake, Redshift, BigQuery)- Minimum 3 years\nAPI Integration & Data Pipelines - Good to have.\nPerformance Tuning & Optimization - Minimum 3 years\nbusiness Intelligence (Power BI, Tableau)- Minimum 3 years\nInterested candidate can share their updated profile on below mentioned mail:-\n[HIDDEN TEXT]\nRegards As Ever\nAnkit Malik","snowflake, Microsoft Fabric, Master Data Management, Data Quality Management, Oracle, Apache Nifi, Hipaa, Data Warehousing, Hadoop, Tableau, Kafka, BigQuery, Power Bi, Etl, Nosql, Informatica, PostgreSQL, SQL Server, Gdpr, Metadata Management, Machine Learning, Talend, Data Governance, Data Modeling, Redshift, Spark, DataBricks"
Azure Data Architect,Veracity Software Inc,10-12 Years,,"Pune, India",Login to check your skill match score,"Job Details:\n\nPosition: Data Architect\n\nExperience: 10-12 years\n\nWork Mode: Onsite\n\nLocation: Pune\n\nNotice Period: Immediate\n\nJob Responsibilities:\n\nThe ideal profile should have a strong foundation in data concepts, design, and strategy, with the ability to\n\nwork across diverse technologies in an agnostic manner.\n\nTransactional Database Architecture\n\nDesign and implement high-performance, reliable, and scalable transactional database architectures.\nCollaborate with cross-functional teams to understand transactional data requirements and create\n\nsolutions that ensure data consistency, integrity, and availability.\n\nOptimize database designs and recommend best practices and technology stacks.\nOversee the management of entire transactional databases, including modernization and de-\n\nduplication initiatives.\n\nData Lake Architecture\n\nDesign and implement data lakes that consolidate data from disparate sources into a unified, scalable\n\nstorage solution.\n\nArchitect and deploy cloud-based or on-premises data lake infrastructure.\nEnsure self-service capabilities across the data engineering space for the business.\nWork closely with Data Engineers, Product Owners, and Business teams.\n\nData Integration & Governance:\n\nUnderstand ingestion and orchestration strategies.\nImplement data sharing, data exchange, and assess data sensitivity and criticality to recommend\n\nappropriate designs.\n\nBasic understanding of data governance practices.\n\nInnovation\n\nEvaluate and implement new technologies, tools, and frameworks to improve data accessibility,\n\nperformance, and scalability.\n\nStay up to date with industry trends and best practices to continuously innovate and enhance the data\n\narchitecture strategy.","Data Lake Architecture, Data accessibility, Data governance practices, Transactional Database Architecture, Data Integration, Scalability"
Senior Manager - Enterprise Data Architect,METRO Global Solution Center IN,10-12 Years,,"Pune, India",Login to check your skill match score,"Metro Global Solution Center (MGSC) is internal solution partner for METRO, a 30.5 Billion international wholesaler with operations in 31 countries through 625 stores & a team of 93,000 people globally. Metro operates in a further 10 countries with its Food Service Distribution (FSD) business and it is thus active in a total of 34 countries.\nMGSC, location wise is present in Pune (India), Dsseldorf (Germany) and Szczecin (Poland). We provide HR, Finance, IT & Business operations support to 31 countries, speak 24+ languages and process over 18,000 transactions a day. We are setting tomorrow's standards for customer focus, digital solutions, and sustainable business models. For over 10 years, we have been providing services and solutions from our two locations in Pune and Szczecin. This has allowed us to gain extensive experience in how we can best serve our internal customers with high quality and passion. We believe that we can add value, drive efficiency, and satisfy our customers.\nWebsite: https://www.metro-gsc.in\n\nCompany Size: 600-650\n\nHeadquarters: Pune, Maharashtra, India\n\nType: Privately Held\n\nInception: 2011\nJob Description\n\nRole & Responsibility:\n\nOwn the data Architecture Principles; enterprise data flow, know how on enterprise data objects and their dependency to business processes;\nContribute to the Data Strategy;\nCo-own the enterprise data model on the Conceptional Level;\nOwn the enterprise data model on Logical Level;\nConsult and oversee solution architect on the Physical Level;\nCollaborate with data governance, Quality, Security, Privacy to define the data architectural principles.\nShould Be able to communicate the value of data on all levels and be actively shaping how Metro is handling data today and tomorrow;\nHave strong communication skills as a key task is to connect people from different business units and different roles;\nBe able to work conceptually and be able to communicate the impact of this type of work;\nBe able to define and foster data architecture principles;\nOwn enterprise data flow (safeguard + design new when tech is introduced/decommissioned), know and understand enterprise data objects and the processes that are supported by data, design migration strategy in large scale projects\nCollaborate with business process architects and enterprise architecture to define the data architecture principles and act as quality gate in change and run the business;\n\nQualifications\n\nBachelors in the domain of computer science.\nHave minimum 10+ years of experience in the data field (engineering and data architecting);\nHave minimum 2 years of experience in the position of Enterprise Data Architect or minimum 3 years experience in the position of Data Architect conducting large data transformation programs;\nHave a considerable interest in the business processes and business challenges a company like Metro is facing; experience in the wholesale/retail domain is a plus but not mandatory;\nHave deep knowledge in data processing technologies; data models and interdependencies between data objects;\nUnderstanding /know how of different database technologies and public cloud Have a consulting mindset;\nShould be familiar with agile software delivery by teams across various locations.","Data Strategy, agile software delivery, data models, data architecture principles, enterprise data flow, enterprise data model, data processing technologies, Data Governance, Database Technologies, Public Cloud"
Big Data Architect,Airtel Digital,8-17 Years,,"Pune, India",Login to check your skill match score,"Key Responsibilities\nDesign, build, and maintain scalable big data architectures on Azure and AWS - Select and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )\nLead data migration from legacy systems to cloud-based solutions - Develop and optimize ETL pipelines and data processing workflows.\nEnsure data infrastructure meets performance, scalability, and security requirements.\nCollaborate with development teams to implement microservices and backend solutions for big data applications.\nOversee the end-to-end SDLC for big data projects, from planning to deployment.\nMentor junior engineers and contribute to architectural best practices.\nPrepare architecture documentation and technical reports.\nRequired Skills & Qualifications\nBachelor's/Master's degree in Computer Science, Engineering, or related field.\n817 years of experience in big data and cloud architecture.\nProven hands-on expertise with Azure and AWS big data services (e.g., Azure Synapse, AWS Redshift, S3, Glue, Data Factory).\nStrong programming skills in Python, Java, or Scala[9].\nSolid understanding of SDLC and agile methodologies.\nExperience in designing and deploying microservices, preferably for backend data systems.\nKnowledge of data storage, database management (relational and NoSQL), and data security best practices\nExcellent problem-solving, communication, and team leadership skills","Glue, Java, S3, Aws Redshift, Hadoop, Scala, Kafka, Microservices, Nosql, Azure Synapse, Azure Data Factory, Database Management, Spark, Data Security, Azure, Python, AWS"
Data Architect,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Responsibilities:\n\nDesign, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.\n\nEnhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.\n\nImplement best practices for data management, storage and security to ensure data integrity and compliance with regulations.\n\nOwn the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.\n\nParticipate in code reviews to ensure code quality and share knowledge.\n\nLead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.\n\nDefine and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.\n\nMentor junior members of the team, providing guidance and support in their professional development.\n\nCollaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration\n\nA little more about you:\n\nBachelor's degree or higher in Computer Science, Engineering, or a related field.\n\n10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.\n\nProficient in SQL and Python, with the ability to translate complexity into efficient code.\n\nExperience with data workflow development and management tools (dbt, Airflow).\n\nSolid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.\n\nStrong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.\n\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\n\nExperience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Gcp, Azure, Python, Sql, AWS"
Data Architect,Impelsys,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Software Skills\n\nTechnical Skill Data Architect, ETL, Data Engineer, Python, SQL and Any\n\nCloud\n\nJob Description\n\nDevelop architectural strategies for data modelling, design and implementation to meet stated requirements for metadata management, master data management, Data warehouses, ETL and ELT.\nAnalyzing business requirements, designing scalable/ robust data models, documenting conceptual, logical & physical data model design, helping developers in development/ creating DB structures and supporting developers throughout the project life cycle.\nLead and Mentor Data Engineers: This role will be responsible for leading and developing a team of data engineers focused on the growth in the team's skills and ability to execute as a team using DevOps and Data Ops principles.\nInvestigate new technologies, data modelling methods and information management systems to determine which ones should be incorporated onto data architectures, and develop implementation timelines and milestones.\nRecognizes and resolves conflicts between models, ensuring that information and data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions).\nParticipates in the design of the information architecture: supports projects, reviews information elements including models, glossary, flows, data usage.\nProvides guidance to the team in achieving the project goals/milestones.\nWorks independently within broad guidelines and policies, with guidance in only the most complex situations.\nContribute as an expert to multiple delivery teams, defining best practices, building reusable design & components, capability building, aligning industry trends and actively engaging with wider data communities.\n\nCandidate Profile\n\n10+ years of relevant experience in Data modelling for DW & analytics applications / Database related technologies.\nExpert data modelling skills (i.e. conceptual, logical and physical model design, experience with Enterprise Data Warehouses and Data Marts).\nSolid understanding of cloud database technologies and services (eg..AWS, Azure , Redshift, Aurora, DynamoDB, Snowflake etc).\nExperience in data lake technologies involving S3, Databricks Delta Lake, etc.\nExperience in working with data governance, data quality, and data security teams.\nExperienced in knowledge-driven data processing techniques like data curation, representation, standardization, normalization and any other type of processing that prepares the data for integration, persistence, analysis, exchange/share and so forth.\nExperience in handling very large DBs and large data volumes .\nStrong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production.\nAbility to lead and mentor teams for effective delivery.\nCrisp and effective executive communication skills, including significant experience. presenting cross-functionally and across all levels.\n\nEducation\n\nGraduate or post graduate in Computer science/Electronics/Software engineering","Aurora, data marts, ETL processes, snowflake, Enterprise Data Warehouses, Delta Lake, Data lake technologies, S3, Data Architect, Data Governance, data curation, data preparation, Database Technologies, Data Integration, Data Engineer, Data Modelling, Python, AWS, Data Security, Dynamodb, Data Quality, Redshift, Sql, Cloud, Databricks, Azure, Etl"
Cloud Data Architect,PURVIEW,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nJob Description\nA Cloud Data Architect who has experience working on Google Cloud Platform and have built solutions using Data Services of GCP\nExperience of 10+ years and who understands the data principles and experience dealing with migrating BI data warehouses, BigData platforms into Google Cloud\nHaving experience on how to extract, transform, load the data into BI systems and experience with Datawarehousing will help\nExperience building solutions using Google BigQuery, Google Dataflow, Google Cloud Storage, Google Pubsub which we use on our data platform\nExperience working with business, engineering, data modeling teams in defining the architecture solution\nUnderstands the architecture risks, design principles and suggest ways to business/engineering teams on tactical (vs) strategic solutions depending on various scenarios one would see\nAbout Company :\n\nPurview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.\n\nWe have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.\n\nIn\n\nCompany Info:\n\n3rd Floor, Sonthalia Mind Space\n\nNear Westin Hotel, Gafoor Nagar\n\nHitechcity, Hyderabad\n\nPhone: +91 40 48549120 / +91 8790177967\n\nUk\n\nGyleview House, 3 Redheughs Rigg,\n\nSouth Gyle, Edinburgh, EH12 9DQ.\n\nPhone: +44 7590230910\n\nEmail: [HIDDEN TEXT]\n\nLogin to Apply !","Google Dataflow, Google Cloud Storage, Google Pubsub, Google BigQuery, Data Services of GCP, Google Cloud Platform"
Manager_Data Architect,VOIS,Fresher,,"Pune, India",Login to check your skill match score,"Join Us\n\nAt Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.\n\nAbout VOIS\n\nVOIS (Vodafone Intelligent Solutions) is a strategic arm of Vodafone Group Plc, creating value and enhancing quality and efficiency across 28 countries, and operating from 7 locations: Albania, Egypt, Hungary, India, Romania, Spain and the UK.\n\nOver 29,000 highly skilled individuals are dedicated to being Vodafone Group's partner of choice for talent, technology, and transformation. We deliver the best services across IT, Business Intelligence Services, Customer Operations, Business Operations, HR, Finance, Supply Chain, HR Operations, and many more.\n\nEstablished in 2006, VOIS has evolved into a global, multi-functional organisation, a Centre of Excellence for Intelligent Solutions focused on adding value and delivering business outcomes for Vodafone.\n\nAbout VOIS India\n\nIn 2009, VOIS started operating in India and now has established global delivery centres in Pune, Bangalore and Ahmedabad. With more than 14,500 employees, VOIS India supports global markets and group functions of Vodafone, and delivers best-in-class customer experience through multi-functional services in the areas of Information Technology, Networks, Business Intelligence and Analytics, Digital Business Solutions (Robotics & AI), Commercial Operations (Consumer & Business), Intelligent Operations, Finance Operations, Supply Chain Operations and HR Operations and more.\n\nWhat You'll Do\n\nStrong understanding of end-to-end impact assessment across all subject areas.\nCreating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications\nCreating and maintaining data models for databases, data warehouses, and data lakes, defining relationships between data entities to optimize data retrieval and analysis.\nDesigning and implementing data pipelines to integrate data from multiple sources, ensuring data consistency and quality across systems.\nCollaborating with business stakeholders to define the overall data strategy, aligning data needs with business requirements.\nSupport migration of new & changed software, elaborate and perform production checks\nNeed to effectively communicate complex data concepts to both technical and non-technical stakeholders.\nGCP Knowledge/exp with Cloud Composer, BigQuery, Pub/Sub, Cloud Functions.\n\nWho You Are\n\nE2E Impact assessment should be done for all demands.\nCreating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications\nManage database related refresh and decommissioning programs whilst maintaining the highest level of Service availability to Vodafone customers\nAssure correct database configuration and proper documentation of all relevant changes within the DB infrastructure\nSupport supplier's 3rd level and engineering in root cause analysis and remediation of problems in their products\nTo come up with ideas to enhance the system\n\nVOIS Equal Opportunity Employer Commitment India\n\nVOIS is proud to be an Equal Employment Opportunity Employer. We celebrate differences and we welcome and value diverse people and insights. We believe that being authentically human and inclusive powers our employees growth and enables them to create a positive impact on themselves and society. We do not discriminate based on age, colour, gender (including pregnancy, childbirth, or related medical conditions), gender identity, gender expression, national origin, race, religion, sexual orientation, status as an individual with a disability, or other applicable legally protected characteristics.\n\nAs a result of living and breathing our commitment, our employees have helped us get certified as a Great Place to Work in India for four years running. We have been also highlighted among the Top 5 Best Workplaces for Diversity, Equity, and Inclusion, Top 10 Best Workplaces for Women, Top 25 Best Workplaces in IT & IT-BPM and 14th Overall Best Workplaces in India by the Great Place to Work Institute in 2023. These achievements position us among a select group of trustworthy and high-performing companies which put their employees at the heart of everything they do.\n\nBy joining us, you are part of our commitment. We look forward to welcoming you into our family which represents a variety of cultures, backgrounds, perspectives, and skills!\n\nApply now, and we'll be in touch!\n\nNot a perfect fit\n\nWorried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.\n\nWhat's In It For You\n\nWho we are\n\nWe are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.\n\nBelonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.\n\nIf you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.\n\nTogether we can.","Pub Sub, data models, Cloud Composer, Cloud Functions, data pipelines, Data Flow Diagrams, data architecture documentation, GCP Knowledge, Technical Specifications, data consistency, BigQuery"
Senior Data Architect,Tredence Inc.,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Designation: Senior Manager (Databricks Architect)\nLocation: Bengaluru, Chennai, Kolkata, Gurugram, Pune and Hyderabad.\nProfile Summary: We are seeking an experienced professional who apart from the required mathematical and statistical expertise also possesses the natural curiosity and creative mind to ask questions, connect the dots, and uncover opportunities that lie hidden with the ultimate goal of realizing the data's full potential.\nRoles and Responsibilities:\nDeveloping Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack\nAbility to provide solutions that are forward-thinking in data engineering and analytics space\nCollaboration with DW/BI leads to understanding new ETL pipeline development requirements.\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop datamodel to fulfill reporting needs\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussion with client architect and team members\nOrchestrate the data pipelines in scheduler via Airflow Skills\nQualifications:\nBachelor's and/or master's degree in computer science or equivalent experience.\nMust have total 12+ yrs. of IT experience and 3+ years experience in Data warehouse/ETL projects.\nDeep understanding of Star and Snowflake dimensional modelling.\nStrong knowledge of Data Management principles\nGood understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture\nShould have hands-on experience in SQL, Python and Spark (PySpark)\nCandidate must have experience in AWS/ Azure stack\nDesirable to have ETL with batch and streaming (Kinesis).\nExperience in building ETL / data warehouse transformation processes\nExperience with Apache Kafka for use with streaming data / event-based data\nExperience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)\nExperience with Open Source non-relational/ NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.\nProficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot\nDatabricks Certified Data Engineer Associate/Professional Certification (Desirable).\nShould have experience working in Agile methodology. Job Description:\nGood Communication and presentations skills.\nAt least 3 years experience in Databricks implementations, 2large scale data warehouse end-to-end implementation experience.","Azure Stack, CircleCI, Hadoop, Cassandra, Pyspark, Pl Sql, Impala, Sql, Pig, Git, Hive, Kinesis, RDBMS, Neo4j, Terraform, Unix Shell Scripting, Apache Kafka, Spark, Databricks, MongoDB, Python, Etl, AWS"
Data Architect,ECI,12-15 Years,,"Indore, India",Login to check your skill match score,"ECI is the leading global provider of managed services, cybersecurity, and business transformation for mid-market financial services organizations across the globe. From its unmatched range of services, ECI provides stability, security and improved business performance, freeing clients from technology concerns and enabling them to focus on running their businesses. More than 1,000 customers worldwide with over $3 trillion of assets under management put their trust in ECI.\n\nAt ECI, we believe success is driven by passion and purpose. Our passion for technology is only surpassed by our commitment to empowering our employees around the world.\n\nThe Opportunity:\n\nECI has an exciting opportunity for an experienced Data Architect, who will work with our clients in building robust data centric applications. Client satisfaction is our primary objective; all available positions are customer facing requiring EXCELLENT communication and people skills. A positive attitude, rigorous work habits and professionalism in the work place are a must. Fluency in English, both written and verbal are required.\n\nThis is an onsite role with work timings, 1 PM IST 10 PM IST / 2 PM IST 11 PM IST.\n\nWhat you will do:\n\nDesign and develop data architecture for large enterprise application\nShould be able to build and demonstrate quick POC\nReview customer environment for master data processes and help with overall data solution & governance model\nWork closely with team business and IT stakeholders to understand master data requirements and current constraints\nShould be able to mentor technically to junior resources\nShould be able to set industry standards with his own work.\n\nWho you are:\n\n12 to 15 years of experience as a Data Architect\nHands on experience in full life cycle Master Data Management\nHands of experience in ADF, Azure Purview, Databricks, Azure Fabric Services\nLead Data architecture roadmaps, defined business cases and implementations for clients\nExperience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture\nReview customer environment for master data processes and help with overall data governance model\nHands on experience in building cloud based later enterprise data warehouses.\nExperience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture\nImplementing best practices for data governance, data modeling, and data migrations\nShould be a good team player\n\nBonus points if you have:\n\nDeep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies\nStrong knowledge of ETL and Data Modeling\nDeep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies\n\n\nECI's culture is all about connection - connection with our clients, our technology and most importantly with each other. In addition to working with an amazing team around the world, ECI also offers a competitive compensation package and so much more! If you believe you would be a great fit and are ready for your best job ever, we would like to hear from you!\n\nLove Your Job, Share Your Technology Passion, Create Your Future Here!","Azure Fabric Services, Data Migrations, Azure Purview, Master Data Management, Data Modeling, Adf, Data Architecture, Databricks, Data Governance"
MS Azure Data Architect,3Pillar,8-10 Years,,India,Login to check your skill match score,"We are seeking a highly experienced Data Architect with a strong background in designing scalable data solutions and leading data engineering teams. The ideal candidate will have deep expertise in Microsoft Azure, ETL processes, and modern data architecture principles. This role involves close collaboration with stakeholders, engineering teams, and business units to design and implement robust data pipelines and architectures.\n\nAssessments of existing data components, Performing POCs, Consulting to the stakeholders\nProposing end to end solutions to an enterprise's data specific business problems, and taking care of data collection, extraction, integration, cleansing, enriching and data visualization\nAbility to design large data platforms to enable Data Engineers, Analysts & scientists\nStrong exposure to different Data architectures, data lake & data warehouse\nDesign and implement end-to-end data architecture solutions on Azure cloud platform.\nLead the design and development of scalable ETL/ELT pipelines using tools such as Azure Data Factory (ADF).\nArchitect data lakes using Azure Data Lake Storage (ADLS) and integrate with Azure Synapse Analytics for enterprise-scale analytics.\nCollaborate with business analysts, data scientists, and engineers to understand data needs and deliver high-performing solutions.\nDefine data models, metadata standards, data quality rules, and security protocols.\nDefine tools & technologies to develop automated data pipelines, write ETL processes, develop dashboard & report and create insights\nContinually reassess current state for alignment with architecture goals, best practices and business needs\nDB modeling, deciding best data storage, creating data flow diagrams, maintaining related documentation\nTaking care of performance, reliability, reusability, resilience, scalability, security, privacy & data governance while designing a data architecture\nApply or recommend best practices in architecture, coding, API integration, CI/CD pipelines\nCoordinate with data scientists, analysts, and other stakeholders for data-related needs\nHelp the Data Science & Analytics Practice grow by mentoring junior Practice members, leading initiatives, leading Data Practice Offerings\nProvide thought leadership by representing the Practice / Organization on internal / external platforms\n\nQualificatons:\n\n8+ years of experience in data architecture, data engineering, or related roles.\nTranslate business requirements into data requests, reports and dashboards.\nStrong Database & modeling concepts with exposure to SQL & NoSQL Databases\nExpertise in designing and writing ETL processes in Python/PySpark\nStrong data architecture patterns & principles, ability to design secure & scalable data lakes, data warehouse, data hubs, and other event-driven architectures\nProven expertise in Microsoft Azure data services, especially ADF, ADLS, Synapse Analytics.\nStrong hands-on experience in designing and building ETL/ELT pipelines.\nProficiency in data modeling, SQL, and performance tuning.\nDemonstrated leadership experience, with the ability to manage and mentor technical teams.\nExcellent communication and stakeholder management skills.\nProficiency in data visualization tools like Tableau, Power BI or similar to create meaningful insights.\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n\nGood to have:\n\nAzure certifications (e.g., Azure Data Engineer Associate, Azure Solutions Architect).\nExperience with modern data platforms, data governance frameworks, and real-time data processing tools.\n\nBenefits:\n\nImagine a flexible work environment whether it's the office, your home, or a blend of both. From interviews to onboarding, we embody a remote-first approach.\nYou will be part of a global team, learning from top talent around the world and across cultures, speaking English everyday. Our global workforce enables our team to leverage global resources to accomplish our work in efficient and effective teams.\nWe're big on your well-being as a company, we spend a whole trimester in our annual cycle focused on wellbeing. Whether it is taking advantage of fitness offerings, mental health plans (country-dependent), or simply leveraging generous time off, we want all of our team members operating at their best.\nOur professional services model enables us to accelerate career growth and development opportunities - across projects, offerings, and industries.\nWe are an equal opportunity employer. It goes without saying that we live by values like Intrinsic Dignity and Open Collaboration to create cutting-edge technology AND reinforce our commitment to diversity - globally and locally.\nJoin us and be a part of a global tech community! Check out our Linkedin site and Careers page to learn more about what it's like to be part of our #oneteam!","NoSQL Databases, ETL processes, Azure Synapse Analytics, Pyspark, Microsoft Azure, Data Modeling, Data Architecture, Sql, Python"
Senior Data Architect (Technical),PURVIEW,5-7 Years,,"Pune, India",Login to check your skill match score,"Job Description\n\nPrincipal Responsibilities:\nEvolve WCS IT landscape with Data-driven approach, define, design and drive Data architecture roadmap for WCS IT including enterprise data architecture.\nClosely work with Product Owners, Business & IT stakeholders across WCS IT landscape to understand the requirements and provide solutions fit-for-purpose\nCreate DFDs and data lineage diagrams to understand the current state and work with stakeholders to align to the future roadmap adhering to the HSBC defined standards\nUnderstand system integrations across and outside WCS IT applications, API integration (and others), understand and document JSON, XML or any other formats being used and decode the interfaces\nUnderstand business requirements on data flows across applications / systems and work with Product owners and POD teams for development aligning to data architecture and designs\nDesign and Develop Tools for automation and scripting for data extraction, transformation and loading as per business needs.\nWork with Program, Department and Enterprise architects of HSBC to drive data architecture and to deliver expected business outcome by designing, developing and implementing solutions\nFollow DevOps Model in day-to-day delivery\nInnovation is part of team's culture and everyone expected to come up with their own ideas / thoughts / solutions, develop PoC, explore new technologies, participate in hackathon events, attend forums, sessions\nWays of working is of prime importance and must follow the best practices and guidelines laid down including Data security standards of BFS\nDevelop deep know-how of WCIT applications - jou eys, features, platform, further develop understanding of other applications in Wholesale Client Services IT landscape\nGuide junior members in the team and help other members when needed\n\nMust Have Requirements\n\nReach experience with Data architecture, defining and driving enterprise data architecture\nUnderstanding of micro services architecture and other architecture patte s, REST APIs\nReading, parsing of JSON, XML and other structures and develop understanding of data attributes and values between two systems\nUse of various data modelling, DFD, Data lineage tools like Visio, reverse engineering tools\nMust have experience working in Banking and Financial service industry specifically with a MNC Bank of size of HSBC\nMust have experience analysing large size data, JSONs, XMLs, other formats and writing scripts to extract, transform and load using tools or developed automation tools\nMust have worked on PostgreSQL, Oracle, MongoDB and such databases\nMust have experience working in Agile and DevSecOps.\nMust have an inclination to explore new technologies, explore and innovate other than project work\nMust have excellent communication skills written and verbal and should be able to articulate thoughts clearly\nAbout Company :\n\nPurview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.\n\nWe have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.\n\nIn\n\nCompany Info:\n\n3rd Floor, Sonthalia Mind Space\n\nNear Westin Hotel, Gafoor Nagar\n\nHitechcity, Hyderabad\n\nPhone: +91 40 48549120 / +91 8790177967\n\nUk\n\nGyleview House, 3 Redheughs Rigg,\n\nSouth Gyle, Edinburgh, EH12 9DQ.\n\nPhone: +44 7590230910\n\nEmail: [HIDDEN TEXT]\n\nLogin to Apply !","DFD, Data lineage tools, PostgreSQL, Json, Data Architecture, DevSecOps, Automation Tools, Xml, Agile, MongoDB, Rest Apis, Oracle"
Data Architect,Gramener,Fresher,,"Hyderabad, India",Login to check your skill match score,"Location: Hyderabad/ Bengaluru, India (Hybrid Mode 3 Days/Week in Office)\n\nJob Description\n\nCollaborate with stakeholders to develop a data strategy that meets enterprise needs and industry requirements.\nCreate an inventory of the data necessary to build and implement a data architecture.\nEnvision data pipelines and how data will flow through the data landscape.\nEvaluate current data management technologies and what additional tools are needed.\nDetermine upgrades and improvements to current data architectures.\nDesign, document, build and implement database architectures and applications. Should have hands-on experience in building high scale OLAP systems.\nBuild data models for database structures, analytics, and use cases.\nDevelop and enforce database development standards with solid DB/ Query optimizations capabilities.\nIntegrate new systems and functions like security, performance, scalability, governance, reliability, and data recovery.\nResearch new opportunities and create methods to acquire data.\nDevelop measures that ensure data accuracy, integrity, and accessibility.\nContinually monitor, refine, and report data management system performance.\n\nRequired Qualifications And Skillset\n\nExtensive knowledge of Azure, GCP clouds, and DataOps Data Eco-System (super strong in one of the two clouds and satisfactory in the other one)\nHands-on expertise in systems like Snowflake, Synapse, SQL DW, BigQuery, and Cosmos DB. (Expertise in any 3 is a must)\nAzure Data Factory, Dataiku, Fivetran, Google Cloud Dataflow (Any 2)\nHands-on experience in working with services/technologies like Apache Airflow, Cloud Composer, Oozie, Azure Data Factory, and Cloud Data Fusion (Expertise in any 2 is required)\nWell-versed with Data services, integration, ingestion, ELT/ETL, Data Governance, Security, and Meta-driven Development.\nExpertise in RDBMS (relational database management system) writing complex SQL logic, DB/Query optimization, Data Modelling, and managing high data volume for mission-critical applications.\nStrong grip on programming using Python and PySpark.\nClear understanding of data best practices prevailing in the industry.\nPreference to candidates having Azure or GCP architect certification. (Either of the two would suffice)\nStrong networking and data security experience.\n\nAwareness Of The Following\n\nApplication development understanding (Full Stack)\nExperience on open-source tools like Kafka, Spark, Splunk, Superset, etc.\nGood understanding of Analytics Platform Landscape that includes AI/ML\nExperience in any Data Visualization tool like PowerBI / Tableau / Qlik /QuickSight etc.\n\nAbout Us\n\nGramener is a design-led data science company. We build custom Data & AI solutions that help solve complex business problems with actionable insights and compelling data stories. We partner with enterprise data and digital transformation teams to improve the data-driven decision-making culture across the organization. Our open standard low-code platform, Gramex, rapidly builds engaging Data & AI solutions across multiple business verticals and use cases. Our solutions and technology have been recognized by analysts such as Gartner and Forrester and have won several awards.\n\nWe offer you\n\na chance to try new things & take risks.\nmeaningful problems you'll be proud to solve.\npeople you will be comfortable working with.\ntransparent and innovative work environment.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","DB Query optimization, Cloud Composer, DataOps, Dataiku, snowflake, Synapse SQL DW, Google Cloud Dataflow, Cloud Data Fusion, Fivetran, BigQuery, Data Modelling, Pyspark, Sql, Apache Airflow, Azure Data Factory, Gcp, RDBMS, Oozie, Cosmos DB, Azure, Python"
Data Architect,DigitalXNode,5-10 Years,,India,Login to check your skill match score,"No. Of Position: 4\n\nExperience: 5Y to 10Y\n\nSalary: Competitive as per market standards\n\nNotice period: Immediate to 30 days\n\nSkills : Azure, ADLS, Kafka, Apache Delta, Databricks/Spark, Hadoop ecosystem, SQL, RDBMS, Data Lakes and Warehouses\n\nLocation: Delhi NCR/Pune/Bangalore/Chennai/Hyderabad/Remote\n\nRole & Responsibilities\n\nSubject matter knowledge in creating data models for corporate analytics that are in compliance with standards, allowing for usability and conformance throughout the enterprise.\nIn charge of creating data strategies, vocabulary consistency, and transformations via intricate analytical relationships and access paths, as well as data mappings at the data-field level.\nCollaborate with Product Management and Business stakeholders to ascertain and assess the data sources required to fulfill project and business objectives.\nCollaborate with Tech Leads and Product Architects to comprehend end-to-end data implications, data integration, and functioning business systems.\nWork with DQ Leads, for data integrity improvement/quality resolution so that these improvements are addressed at the source.\nDomain knowledge in supply chain, retail, or inventory management.\n\nCritical Skills To Have\n\nFive or more years of experience in the field of information technology\nHas a general understanding of several software platforms and development technologies\nHas experience with SQL, RDBMS, Data Lakes, and Warehouses Knowledge of the Hadoop ecosystem, Azure, ADLS, Kafka, Apache Delta, and Databricks/Spark.\nPossessing knowledge of any data modeling tool, such as ERStudio or Erwin, is advantageous.\nCollaboration history with Product Managers, Technology teams, and Business Partners\nStrong familiarity with Agile and DevOps techniques\nExcellent communication skills both in writing and speaking\n\nPreferred Qualifications\n\nA bachelor's degree in business information technology, computer science, or a similar discipline.\n\nPlease apply for a resume online, and the digitalxnode evaluation team will reach out to you in case your profile gets screen-selected. We will keep your data in our repository, and our team may reach out to you for other positions.\n\nTechnology: IT\n\nJob Type: Full Time\n\nJob Location: Bangalore Bengaluru Delhi Kolkata Navi Mumbai","Azure ADLS, Data Lakes, Warehouses, Apache Delta, RDBMS, Hadoop Ecosystem, Spark, Kafka, Databricks, Sql"
GCP BIG DATA ARCHITECT- Chennai,Grid Dynamics,Fresher,,"Chennai, India",Login to check your skill match score,"Details on tech stack\nGCP Services: BigQuery, Cloud Dataflow, Pub/Sub, Dataproc, Cloud Storage.\nData Processing: Apache Beam (batch/stream), Apache Kafka, Cloud Dataprep.\nProgramming: Python, Java/Scala, SQL.\nOrchestration: Apache Airflow (Cloud Composer), Terraform.\nSecurity: IAM, Cloud Identity, Cloud Security Command Center.\nContainerization: Docker, Kubernetes (GKE).\nMachine Learning: Google AI Platform, TensorFlow, AutoML.\nCertifications: Google Cloud Data Engineer, Cloud Architect (preferred).\nProven ability to design scalable and robust AI/ML systems in production, with a focus on high-performance and cost-effective solutions.\nStrong experience with cloud platforms (Google Cloud, AWS, Azure) and cloud-native AI/ML services (e.g., Vertex AI, SageMaker).\nExpertise in implementing MLOps practices, including model deployment, monitoring, retraining, and version control.\nStrong leadership skills with the ability to guide teams, mentor engineers, and collaborate with cross-functional teams to meet business objectives.\nDeep understanding of frameworks like TensorFlow, PyTorch, and Scikit-learn for designing, training, and deploying models.\nExperience with data engineering principles, scalable pipelines, and distributed systems (e.g., Apache Kafka, Spark, Kubernetes).\nNice to have requirements to the candidate\nStrong leadership and mentorship capabilities, guiding teams toward best practices and high-quality deliverables.\nExcellent problem-solving skills, with a focus on designing efficient, high-performance systems.\nEffective project management abilities to handle multiple initiatives and ensure timely delivery.\nStrong emphasis on collaboration and teamwork, fostering a positive and productive work environment.","Cloud Dataprep, Cloud Dataflow, Google AI Platform, Pub Sub, Cloud Security Command Center, Cloud Composer, Cloud Identity, Vertex AI, GCP Services, GKE, Scikit-learn, SageMaker, AutoML, Apache Airflow, Tensorflow, Cloud Storage, Pytorch, Docker, Terraform, Python, AWS, Java, BigQuery, Scala, Dataproc, Sql, MLops, Iam, Apache Kafka, Apache Beam, Azure, Kubernetes"
Data Architect,Grid Dynamics,12-16 Years,,"Hyderabad, India",Login to check your skill match score,"Grid Dynamics Hiring Data/Cloud Architect\nExperience: 12- 16 Years\nNotice period: Immediate - 30 Days\nLocation : Hyderabad\nRole:\nA result- oriented thought-leader to drive the development of the data engineering practice\nA trusted advisor and business partner to customers across verticals, and consulting team leader who establishes engineering processes and skill development.\nResponsibilities:\nTrusted Advisor:\nProvide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.\nPre-Sales and Consulting:\nLead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.\nTechnology Strategy and R&D:\nCreate and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing assets and solutions strategy across multiple industries.\nEngineering:\nWorking with Grid Dynamics's delivery organisation to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.\nBusiness Development & Partnership:\nManage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.\nRequirements:\nExtensive practical experience in Big Data engineering, data governance, and cloud data\nplatforms.\nStrong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.\nStrong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.\nDeep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.\nExperience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.\nManaged key client relations worldwide and advised global technology and business leaders on innovation and data strategy.\nGood experience with Pre-sales activities.\nTechnology skills (any of below):\nData engineering: analytical data platforms, streaming, big data, EDW, data lakes, data governance, data mesh, Spark, Kafka, Snowflake, (2 from below: AWS, GCP, Azure) Transactional databases: Redis, Cassandra, MongoDB, (2 from below: AWS, GCP, Azure) ML and MLOps: VertexAI, Sagemaker, Dataiku, Databricks, mlflow.\nAbout Grid :\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, and advanced analytics services. Fusing technical vision with business acumen, we enable positive business outcomes for enterprise companies undergoing business transformation by solving their most pressing technical challenges. A key differentiator for Grid Dynamics is our 7+ years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization, and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India. Follow us on LinkedIn.","ML and MLOps, Sagemaker, snowflake, Data Collection, cloud-based architectures, Big Data engineering, Transactional databases, mlflow, Dataiku, data mesh, streaming big data, Reporting, ML platforms, analytical data platforms, data aggregation, data lakes, VertexAI, Databricks, Kafka, Cassandra, data engineering, Gcp, Data Governance, BI, AWS, Redis, Edw, Azure, MongoDB, Spark"
Data Architect,66degrees,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.\nOverview of Role\nAs a Data Architect with 66degrees, you are responsible for assisting customers in their journey to Google Cloud through the design and implementation of a migration strategy. The Data Architect should be comfortable translating business and technical requirements into scalable and cost-effective database solutions.\nResponsibilities\nFacilitate, guide, and influence the client and teams towards an effective architectural pattern and becoming an interface between business leadership, technology leadership and the delivery teams.\nPerform Migration Assessments and Produce Migration Plans that include\nTotal Cost of Ownership (TCO)\nMigration Architecture\nMigration Timelines\nApplication Waves\nDesigning of solution architecture on Google Cloud to support critical workloads\nHeterogenous Oracle Migrations to Postgres or Spanner\nDesign a migration path that accounts for the conversion of:\nApplication Dependencies\nDatabase objects\nData\nData Pipelines\nOrchestration\nUsers and Security\nOversee migration activities and provide troubleshooting support including:\nTranslation of DDL and DML\nExecuting data transfers using native Google Cloud and 3rd party tools\nSetup and configuration of relative Google Cloud components\nEngage with customer teams as a Google Cloud expert to provide:\nEducation Workshops\nArchitectural Recommendations\nTechnology reviews and recommendations\nQualifications\n5+ years of Oracle database management and IT experience.\nExperience with Oracle Database adjacent products like Golden Gate and Data Guard.\n3+ years of PostgreSQL experience\n5+ years Consulting experience\nProven experience in performing performance testing and applying remediations to address performance issues.\nExperience in designing data models\nAdvanced SQL skills, including the ability to write, tune, and interpret SQL queries; tool specific experience in the database platforms listed above is ideal.\nProven experience in migrating and or implementing cloud databases like Cloud SQL, Spanner, and Bigtable\n2+ years of cloud experience; Google Cloud preferred\nExperience in defining technical architecture in cloud environments; Google Cloud preferred\nGoogle Cloud Professional Architect and or Data Engineer Certification is preferred\nA Bachelor's degree in Computer Science, Computer Engineering, or related or equivalent work experience required.\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","bigtable, Cloud SQL, Spanner, Advanced Sql, Data Guard, Oracle Database, PostgreSQL, Google Cloud, Golden Gate"
Data Architect,Impelsys,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Software Skills\n\nTechnical Skill Data Architect, ETL, Data Engineer, Python, SQL and Any\n\nCloud\n\nJob Description\n\nDevelop architectural strategies for data modelling, design and implementation to meet stated requirements for metadata management, master data management, Data warehouses, ETL and ELT.\nAnalyzing business requirements, designing scalable/ robust data models, documenting conceptual, logical & physical data model design, helping developers in development/ creating DB structures and supporting developers throughout the project life cycle.\nLead and Mentor Data Engineers: This role will be responsible for leading and developing a team of data engineers focused on the growth in the team's skills and ability to execute as a team using DevOps and Data Ops principles.\nInvestigate new technologies, data modelling methods and information management systems to determine which ones should be incorporated onto data architectures, and develop implementation timelines and milestones.\nRecognizes and resolves conflicts between models, ensuring that information and data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions).\nParticipates in the design of the information architecture: supports projects, reviews information elements including models, glossary, flows, data usage.\nProvides guidance to the team in achieving the project goals/milestones.\nWorks independently within broad guidelines and policies, with guidance in only the most complex situations.\nContribute as an expert to multiple delivery teams, defining best practices, building reusable design & components, capability building, aligning industry trends and actively engaging with wider data communities.\n\nCandidate Profile\n\n10+ years of relevant experience in Data modelling for DW & analytics applications / Database related technologies.\nExpert data modelling skills (i.e. conceptual, logical and physical model design, experience with Enterprise Data Warehouses and Data Marts).\nSolid understanding of cloud database technologies and services (eg..AWS, Azure , Redshift, Aurora, DynamoDB, Snowflake etc).\nExperience in data lake technologies involving S3, Databricks Delta Lake, etc.\nExperience in working with data governance, data quality, and data security teams.\nExperienced in knowledge-driven data processing techniques like data curation, representation, standardization, normalization and any other type of processing that prepares the data for integration, persistence, analysis, exchange/share and so forth.\nExperience in handling very large DBs and large data volumes .\nStrong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production.\nAbility to lead and mentor teams for effective delivery.\nCrisp and effective executive communication skills, including significant experience. presenting cross-functionally and across all levels.\n\nEducation\n\nGraduate or post graduate in Computer science/Electronics/Software engineering","Aurora, data marts, ETL processes, snowflake, Enterprise Data Warehouses, Delta Lake, Data lake technologies, S3, Data Architect, Data Governance, data curation, data preparation, Database Technologies, Data Integration, Data Engineer, Data Modelling, Python, AWS, Data Security, Dynamodb, Data Quality, Redshift, Sql, Cloud, Databricks, Azure, Etl"
Data Architect,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Responsibilities:\n\nDesign, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.\n\nEnhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.\n\nImplement best practices for data management, storage and security to ensure data integrity and compliance with regulations.\n\nOwn the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.\n\nParticipate in code reviews to ensure code quality and share knowledge.\n\nLead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.\n\nDefine and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.\n\nMentor junior members of the team, providing guidance and support in their professional development.\n\nCollaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration\n\nA little more about you:\n\nBachelor's degree or higher in Computer Science, Engineering, or a related field.\n\n10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.\n\nProficient in SQL and Python, with the ability to translate complexity into efficient code.\n\nExperience with data workflow development and management tools (dbt, Airflow).\n\nSolid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.\n\nStrong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.\n\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\n\nExperience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Gcp, Azure, Python, Sql, AWS"
Data Architect,ACL Digital,8-10 Years,,"Pune, India",Login to check your skill match score,"Total yrs of Exp: 8+ yrs\nLocation:Balewadi, Pune\nTechnical skills and core competencies\nStrong understanding of Data Architecture and models and leading data driven projects.\nSolid expertise and strong opinions on Data Modelling paradigms such as Kimball, Inmon, Data Marts, Data Vault, Medallion etc.\nStrong experience with Cloud Based data strategies and big data technologies AWS Preferred.\nSolid experience in designing data pipelines for ETL with expert knowledge on ingestion, transformation, and data quality is a must\nHands-on experience in SQL is a must.\nDeep understanding of PostGreSQL development, query optimization and designing indexes is a must.\nAn ability to understand and manipulate intermediate to complex level of SQL\nThorough knowledge of Postgres PL/SQL to work with complex warehouse workflows.\nAbility to use advanced SQL concepts such as RANK, DENSE_RANK along with applying advanced statistical concepts through SQL is required.\nWorking experience with PostGres SQL extensions like PostGIS is desired.\nExpertise writing ETL pipelines combining Python + SQL is required.\nUnderstanding of data manipulation libraries in python like Pandas, Polars, DuckDB is desired\nExperience in designing Data visualization with different tools such as Tableau and PowerBI is desirable.\nResponsibilities\nParticipate in the design and developing features in the existing Data Warehouse.\nProvide leadership in establishing connection between Engineering, product and analytics/data scientists team.\nDesign, implement, update existing/new batch ETL pipelines\nDefine and implement data architecture.\nPartner with both engineers and data analysts to build reliable datasets that can be trusted, understood, and used by the rest of the company.\nWork with various data orchestration tools (Apache Airflow, Dagster, Prefect and others)\nEmbrace a fast-paced start-up environment.\nYou should be passionate about your job and enjoy a fast-paced international working environment.\nBackground or experience in the telecom industry is a plus but not a requirement.\nLove automating and enjoy monitoring","Cloud Based data strategies, PostGreSQL development, Postgres PL SQL, Query Optimization, Sql, Data Architecture"
Senior Data Architect (Technical),PURVIEW,5-7 Years,,"Pune, India",Login to check your skill match score,"Job Description\n\nPrincipal Responsibilities:\nEvolve WCS IT landscape with Data-driven approach, define, design and drive Data architecture roadmap for WCS IT including enterprise data architecture.\nClosely work with Product Owners, Business & IT stakeholders across WCS IT landscape to understand the requirements and provide solutions fit-for-purpose\nCreate DFDs and data lineage diagrams to understand the current state and work with stakeholders to align to the future roadmap adhering to the HSBC defined standards\nUnderstand system integrations across and outside WCS IT applications, API integration (and others), understand and document JSON, XML or any other formats being used and decode the interfaces\nUnderstand business requirements on data flows across applications / systems and work with Product owners and POD teams for development aligning to data architecture and designs\nDesign and Develop Tools for automation and scripting for data extraction, transformation and loading as per business needs.\nWork with Program, Department and Enterprise architects of HSBC to drive data architecture and to deliver expected business outcome by designing, developing and implementing solutions\nFollow DevOps Model in day-to-day delivery\nInnovation is part of team's culture and everyone expected to come up with their own ideas / thoughts / solutions, develop PoC, explore new technologies, participate in hackathon events, attend forums, sessions\nWays of working is of prime importance and must follow the best practices and guidelines laid down including Data security standards of BFS\nDevelop deep know-how of WCIT applications - jou eys, features, platform, further develop understanding of other applications in Wholesale Client Services IT landscape\nGuide junior members in the team and help other members when needed\n\nMust Have Requirements\n\nReach experience with Data architecture, defining and driving enterprise data architecture\nUnderstanding of micro services architecture and other architecture patte s, REST APIs\nReading, parsing of JSON, XML and other structures and develop understanding of data attributes and values between two systems\nUse of various data modelling, DFD, Data lineage tools like Visio, reverse engineering tools\nMust have experience working in Banking and Financial service industry specifically with a MNC Bank of size of HSBC\nMust have experience analysing large size data, JSONs, XMLs, other formats and writing scripts to extract, transform and load using tools or developed automation tools\nMust have worked on PostgreSQL, Oracle, MongoDB and such databases\nMust have experience working in Agile and DevSecOps.\nMust have an inclination to explore new technologies, explore and innovate other than project work\nMust have excellent communication skills written and verbal and should be able to articulate thoughts clearly\nAbout Company :\n\nPurview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.\n\nWe have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.\n\nIn\n\nCompany Info:\n\n3rd Floor, Sonthalia Mind Space\n\nNear Westin Hotel, Gafoor Nagar\n\nHitechcity, Hyderabad\n\nPhone: +91 40 48549120 / +91 8790177967\n\nUk\n\nGyleview House, 3 Redheughs Rigg,\n\nSouth Gyle, Edinburgh, EH12 9DQ.\n\nPhone: +44 7590230910\n\nEmail: [HIDDEN TEXT]\n\nLogin to Apply !","DFD, Data lineage tools, PostgreSQL, Json, Data Architecture, DevSecOps, Automation Tools, Xml, Agile, MongoDB, Rest Apis, Oracle"
Senior Data Architect,Tredence Inc.,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Designation: Senior Manager (Databricks Architect)\nLocation: Bengaluru, Chennai, Kolkata, Gurugram, Pune and Hyderabad.\nProfile Summary: We are seeking an experienced professional who apart from the required mathematical and statistical expertise also possesses the natural curiosity and creative mind to ask questions, connect the dots, and uncover opportunities that lie hidden with the ultimate goal of realizing the data's full potential.\nRoles and Responsibilities:\nDeveloping Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack\nAbility to provide solutions that are forward-thinking in data engineering and analytics space\nCollaboration with DW/BI leads to understanding new ETL pipeline development requirements.\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop datamodel to fulfill reporting needs\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussion with client architect and team members\nOrchestrate the data pipelines in scheduler via Airflow Skills\nQualifications:\nBachelor's and/or master's degree in computer science or equivalent experience.\nMust have total 12+ yrs. of IT experience and 3+ years experience in Data warehouse/ETL projects.\nDeep understanding of Star and Snowflake dimensional modelling.\nStrong knowledge of Data Management principles\nGood understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture\nShould have hands-on experience in SQL, Python and Spark (PySpark)\nCandidate must have experience in AWS/ Azure stack\nDesirable to have ETL with batch and streaming (Kinesis).\nExperience in building ETL / data warehouse transformation processes\nExperience with Apache Kafka for use with streaming data / event-based data\nExperience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)\nExperience with Open Source non-relational/ NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.\nProficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot\nDatabricks Certified Data Engineer Associate/Professional Certification (Desirable).\nShould have experience working in Agile methodology. Job Description:\nGood Communication and presentations skills.\nAt least 3 years experience in Databricks implementations, 2large scale data warehouse end-to-end implementation experience.","Azure Stack, CircleCI, Hadoop, Cassandra, Pyspark, Pl Sql, Impala, Sql, Pig, Git, Hive, Kinesis, RDBMS, Neo4j, Terraform, Unix Shell Scripting, Apache Kafka, Spark, Databricks, MongoDB, Python, Etl, AWS"
Data Architect,ECI,12-15 Years,,"Indore, India",Login to check your skill match score,"ECI is the leading global provider of managed services, cybersecurity, and business transformation for mid-market financial services organizations across the globe. From its unmatched range of services, ECI provides stability, security and improved business performance, freeing clients from technology concerns and enabling them to focus on running their businesses. More than 1,000 customers worldwide with over $3 trillion of assets under management put their trust in ECI.\n\nAt ECI, we believe success is driven by passion and purpose. Our passion for technology is only surpassed by our commitment to empowering our employees around the world.\n\nThe Opportunity:\n\nECI has an exciting opportunity for an experienced Data Architect, who will work with our clients in building robust data centric applications. Client satisfaction is our primary objective; all available positions are customer facing requiring EXCELLENT communication and people skills. A positive attitude, rigorous work habits and professionalism in the work place are a must. Fluency in English, both written and verbal are required.\n\nThis is an onsite role with work timings, 1 PM IST 10 PM IST / 2 PM IST 11 PM IST.\n\nWhat you will do:\n\nDesign and develop data architecture for large enterprise application\nShould be able to build and demonstrate quick POC\nReview customer environment for master data processes and help with overall data solution & governance model\nWork closely with team business and IT stakeholders to understand master data requirements and current constraints\nShould be able to mentor technically to junior resources\nShould be able to set industry standards with his own work.\n\nWho you are:\n\n12 to 15 years of experience as a Data Architect\nHands on experience in full life cycle Master Data Management\nHands of experience in ADF, Azure Purview, Databricks, Azure Fabric Services\nLead Data architecture roadmaps, defined business cases and implementations for clients\nExperience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture\nReview customer environment for master data processes and help with overall data governance model\nHands on experience in building cloud based later enterprise data warehouses.\nExperience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture\nImplementing best practices for data governance, data modeling, and data migrations\nShould be a good team player\n\nBonus points if you have:\n\nDeep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies\nStrong knowledge of ETL and Data Modeling\nDeep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies\n\n\nECI's culture is all about connection - connection with our clients, our technology and most importantly with each other. In addition to working with an amazing team around the world, ECI also offers a competitive compensation package and so much more! If you believe you would be a great fit and are ready for your best job ever, we would like to hear from you!\n\nLove Your Job, Share Your Technology Passion, Create Your Future Here!","Azure Fabric Services, Data Migrations, Azure Purview, Master Data Management, Data Modeling, Adf, Data Architecture, Databricks, Data Governance"
Manager_Data Architect,VOIS,Fresher,,"Pune, India",Login to check your skill match score,"Join Us\n\nAt Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.\n\nAbout VOIS\n\nVOIS (Vodafone Intelligent Solutions) is a strategic arm of Vodafone Group Plc, creating value and enhancing quality and efficiency across 28 countries, and operating from 7 locations: Albania, Egypt, Hungary, India, Romania, Spain and the UK.\n\nOver 29,000 highly skilled individuals are dedicated to being Vodafone Group's partner of choice for talent, technology, and transformation. We deliver the best services across IT, Business Intelligence Services, Customer Operations, Business Operations, HR, Finance, Supply Chain, HR Operations, and many more.\n\nEstablished in 2006, VOIS has evolved into a global, multi-functional organisation, a Centre of Excellence for Intelligent Solutions focused on adding value and delivering business outcomes for Vodafone.\n\nAbout VOIS India\n\nIn 2009, VOIS started operating in India and now has established global delivery centres in Pune, Bangalore and Ahmedabad. With more than 14,500 employees, VOIS India supports global markets and group functions of Vodafone, and delivers best-in-class customer experience through multi-functional services in the areas of Information Technology, Networks, Business Intelligence and Analytics, Digital Business Solutions (Robotics & AI), Commercial Operations (Consumer & Business), Intelligent Operations, Finance Operations, Supply Chain Operations and HR Operations and more.\n\nWhat You'll Do\n\nStrong understanding of end-to-end impact assessment across all subject areas.\nCreating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications\nCreating and maintaining data models for databases, data warehouses, and data lakes, defining relationships between data entities to optimize data retrieval and analysis.\nDesigning and implementing data pipelines to integrate data from multiple sources, ensuring data consistency and quality across systems.\nCollaborating with business stakeholders to define the overall data strategy, aligning data needs with business requirements.\nSupport migration of new & changed software, elaborate and perform production checks\nNeed to effectively communicate complex data concepts to both technical and non-technical stakeholders.\nGCP Knowledge/exp with Cloud Composer, BigQuery, Pub/Sub, Cloud Functions.\n\nWho You Are\n\nE2E Impact assessment should be done for all demands.\nCreating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications\nManage database related refresh and decommissioning programs whilst maintaining the highest level of Service availability to Vodafone customers\nAssure correct database configuration and proper documentation of all relevant changes within the DB infrastructure\nSupport supplier's 3rd level and engineering in root cause analysis and remediation of problems in their products\nTo come up with ideas to enhance the system\n\nVOIS Equal Opportunity Employer Commitment India\n\nVOIS is proud to be an Equal Employment Opportunity Employer. We celebrate differences and we welcome and value diverse people and insights. We believe that being authentically human and inclusive powers our employees growth and enables them to create a positive impact on themselves and society. We do not discriminate based on age, colour, gender (including pregnancy, childbirth, or related medical conditions), gender identity, gender expression, national origin, race, religion, sexual orientation, status as an individual with a disability, or other applicable legally protected characteristics.\n\nAs a result of living and breathing our commitment, our employees have helped us get certified as a Great Place to Work in India for four years running. We have been also highlighted among the Top 5 Best Workplaces for Diversity, Equity, and Inclusion, Top 10 Best Workplaces for Women, Top 25 Best Workplaces in IT & IT-BPM and 14th Overall Best Workplaces in India by the Great Place to Work Institute in 2023. These achievements position us among a select group of trustworthy and high-performing companies which put their employees at the heart of everything they do.\n\nBy joining us, you are part of our commitment. We look forward to welcoming you into our family which represents a variety of cultures, backgrounds, perspectives, and skills!\n\nApply now, and we'll be in touch!\n\nNot a perfect fit\n\nWorried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.\n\nWhat's In It For You\n\nWho we are\n\nWe are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.\n\nBelonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.\n\nIf you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.\n\nTogether we can.","Pub Sub, data models, Cloud Composer, Cloud Functions, data pipelines, Data Flow Diagrams, data architecture documentation, GCP Knowledge, Technical Specifications, data consistency, BigQuery"
Data Architect,Jio,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Skills:\nData Architect, On-premise, Python, Kafka, Cloudera, Spark, Hadoop, Hive,\n\nCompany Overview\n\nJio Platforms Limited is the driving force behind India's leading telecom operator Jio, serving 400M+ customers. We offer digital apps & services, end-to-end 5G solutions, AI/ML platforms, and cloud-native OSS/BSS solutions.\n\nJob Overview\n\nExperienced Data Architect role at Jio Platforms Limited, Mumbai, Bangalore India. Full-Time position with more than 10 years of experience. Join a dynamic company leading India's telecom market with 400M+ customers and innovative digital solutions for B2C and B2B sectors.\n\nQualifications And Skills\n\nExpertise in Hadoop, Hive, Apache Spark, Python, Kafka, Cloudera\nExperience with On-premise and Microsoft Azure environments\nStrong data management and data governance skills\nExcellent problem-solving and analytical abilities\nEffective communication and collaboration skills\nBachelors or masters degree in computer science, Information Technology or related field of study. MCA preferable.\n12 - 15 years of overall experience.\nCertification MS Azure AZ-304, AZ-303,\n6+ years of large-scale software development or application engineering with recent coding experience in two or more of the following: Java, JavaScript, Node.js, .NET, Python, MSSQL.\n4+ years of experience as a technical specialist in customer-facing roles.\nExperience architecting highly available systems that utilize load balancing, horizontal scalability and high availability.\nGood exposure to Agile software development and DevOps practices such as Infrastructure as Code (IaC), Continuous Integration and automated deployment Continuous Integration (CI) tools (e.g. Jenkins).\nStrong, in-depth and demonstrable hands-on experience with the following technologies: Microsoft Azure and its relevant build, deployment, automation, networking and security technologies in cloud and hybrid environments.\nExposure to Agile development methodologies and deployment strategies.\nStrong practical application development experience on Linux and Windows-based systems.\nExcellent knowledge of cloud computing technologies and current computing trends.\nExperience working directly with customers, partners or third-party developers.\nEffective communication skills (written and verbal) to properly articulate complicated cloud reports to management and other IT development partners.\nPositive attitude and a strong commitment to delivering quality work.\n\nRoles And Responsibilities\n\nDesign and implement data architecture solutions to meet business needs.\nDevelop data models, database design, and data migration strategies.\nCollaborate with cross-functional teams to ensure data integration and data quality.\nImplement data security and compliance measures.\nOptimize data infrastructure and performance for scalable solutions.\nArchitect, design, and develop Products on the Azure platform.\nDesign and develop solutions for Data Platforms ranging from Batch Data management to real-time data feeds.\nLeverage new technology paradigms (e.g., serverless, containers, microservices, Api Management, Data Storage).\nDevelop solutions for the cloud and for Azure storage.\nDesign identity & security and data platform solutions.\nDesign Azure infrastructure strategy.\nWork closely with Business Analysts, Product Managers, Data Managers and other team members to ensure successful production of application software.\nWork closely with IT security to monitor the company's cloud privacy.\nRespond to technical issues in a professional and timely manner.\nOffer guidance in infrastructure movement techniques including bulk application transfers into the cloud.\nIdentify the top cloud architecture solution patterns to successfully meet the strategic needs of the company.\n\nLocation: - Mumbai, Bangalore","Java, Hadoop, .NET, Apache Spark, Node.js, Kafka, Windows, Mssql, Jenkins, Hive, Javascript, Linux, Cloudera, Microsoft Azure, Python"
Senior Data Architect,Uplers,15-17 Years,,"Thiruvananthapuram, Thiruvananthapuram / Trivandrum, India",Login to check your skill match score,"Experience: 13.00 + years\n\nSalary: Confidential (based on experience)\n\nShift: (GMT+05:30) Asia/Kolkata (IST)\n\nOpportunity Type: Remote\n\nPlacement Type: Full time Permanent Position\n\n(*Note: This is a requirement for one of Uplers client - Forbes Advisor)\n\nWhat do you need for this opportunity\n\nMust have skills required:\n\nDBT, GCP, DWH, Data Modelling, Data Governance, data quality, data monitoring, Cost Management, Multi Cloud\n\nForbes Advisor is Looking for:\n\nCompany Description\n\nForbes Advisor, part of the Forbes Marketplace family, provides consumers with\n\nexpert-written insights, news, and reviews on personal finance, health, business, and\n\neveryday life decisions. We empower our audience with data-driven knowledge so\n\nthey can make informed choices confidentlybalancing the agility of a startup with\n\nthe stability of a seasoned enterprise\n\nRole Overview\n\nThe Senior Data Architect is a strategic, senior leadership role responsible for\n\nsetting the vision and direction of our data warehousing function. You will architect,\n\nimplement, and maintain a state-of-the-art data warehouse that drives actionable\n\ninsights across revenue, subscriptions, paid marketing channels, and operational\n\nfunctions. Your leadership will ensure data quality, robust pipeline design, and\n\nseamless integration with business intelligence tools. This role requires a strong mix\n\nof technical acumen, team management, and cross-functional collaboration\n\nespecially with teams focused on SEM, Digital Experiences, and revenue attribution\n\nJob Description\n\nKey Responsibilities\n\nStrategic Data Architecture & Pipeline Leadership\n\nVision & Strategy:\n\nDefine and execute the long-term strategy for our data warehousing\n\nplatform using medallion architecture (Bronze, Silver, Gold layers) and\n\nmodern cloud-based solutions.\n\nEnd-to-End Pipeline Oversight:\n\nOversee data ingestion (via Google Ads, Bing Ads, Facebook Ads, GA,\n\nAPIs, SFTP, etc.), transformation (leveraging DBT, and SQL [via\n\nBigQuery]), and reporting, ensuring that our pipelines are robust and\n\nscalable.\n\nData Modeling Best Practices:\n\nChampion best practices in data modeling, including the effective use\n\nof DBT packages to streamline complex transformations.\n\nData Quality, Governance & Attribution\n\nQuality & Validation:\n\nEstablish and enforce rigorous data quality standards, governance\n\npolicies, and automated validation frameworks across all data streams.\n\nStandardization & Visibility:\n\nCollaborate with the Data Engineering, Insights and BIOps team to\n\nstandardize data definitions (including engagement metrics and\n\nrevenue attribution) and ensure consistency across all reports.\n\nAttribution Focus:\n\nDevelop frameworks to reconcile revenue discrepancies and unify\n\nvalidation across Finance, SEM, and Analytics teams.\n\nEnsure accurate attribution of revenue and paid marketing channel\n\nperformance, working closely with SEM and Digital Experiences teams.\n\nMonitoring & Alerting:\n\nImplement robust monitoring and alerting systems (e.g., Slack and\n\nemail notifications) to quickly identify, diagnose, and resolve data\n\npipeline issues.\n\nTeam Leadership & Cross-Functional Collaboration\n\nPeople & Process:\n\nLead, mentor, and grow a high-performing team of data warehousing\n\nspecialists, fostering a culture of accountability, innovation, and\n\ncontinuous improvement.\n\nStakeholder Engagement:\n\nPartner with RevOps, Analytics, SEM, Finance, and Product teams to align\n\nthe data infrastructure with business objectives.\n\nServe as the primary data warehouse expert in discussions around\n\nrevenue attribution and paid marketing channel performance, ensuring\n\nthat business requirements drive technical solutions.\n\nCommunication:\n\nTranslate complex technical concepts into clear business insights for\n\nboth technical and non-technical stakeholders.\n\nOperational Excellence & Process Improvement\n\nDeployment & QA:\n\nOversee deployment processes, including staging, QA, and rollback\n\nstrategies, to ensure minimal disruption during updates.\n\nContinuous Optimization:\n\nRegularly assess and optimize data pipelines for performance,\n\nscalability, and reliability while reducing operational overhead.\n\nLegacy to Cloud Transition:\n\nLead initiatives to transition from legacy on-premise systems to\n\nmodern cloud-based architectures for improved agility and cost\n\nefficiency.\n\nInnovation & Thought Leadership\n\nEmerging Trends:\n\nStay abreast of emerging trends and technologies in data warehousing,\n\nanalytics, and cloud solutions.\n\nPilot Projects:\n\nPropose and lead innovative projects to enhance our data capabilities,\n\nwith a particular focus on predictive and prescriptive analytics.\n\nExecutive Representation:\n\nRepresent the data warehousing function in senior leadership\n\ndiscussions and strategic planning sessions\n\nQualifications\n\nEducation & Experience\n\nBachelor's or Master's degree in Computer Science, Data Science, Information\n\nSystems, or a related field.\n\n15+ years of experience in data engineering, warehousing, or analytics roles,\n\nwith at least 5+ years in a leadership capacity.\n\nProven track record in designing and implementing scalable data\n\nwarehousing solutions in cloud environments.\n\nTechnical Expertise\n\nDeep experience with medallion architecture and modern data pipeline tools,\n\nincluding DBT (and DBT packages), Databricks, SQL, and cloud-based data\n\nplatforms.\n\nStrong understanding of ETL/ELT best practices, data modeling (logical and\n\nphysical), and large-scale data processing.\n\nHands-on experience with BI tools (e.g., Tableau, Looker) and familiarity with\n\nGoogle Analytics, and other tracking systems.\n\nSolid understanding of attribution models (first-touch, last-touch, multi-\n\ntouch) and experience working with paid marketing channels.\n\nLeadership & Communication\n\nExcellent leadership and team management skills with the ability to mentor\n\nand inspire cross-functional teams.\n\nOutstanding communication skills, capable of distilling complex technical\n\ninformation into clear business insights.\n\nDemonstrated ability to lead strategic initiatives, manage competing\n\npriorities, and deliver results in a fast-paced environment.\n\nPerks & Benefits\n\nFlexible/Remote Working: Enjoy flexible work arrangements in a collaborative,\n\ndistributed team culture.\n\nCompetitive Compensation: Attractive salary, performance-based bonuses,\n\nand comprehensive benefits.\n\nTime Off: Generous paid time off, parental leave policies, and a dedicated day\n\noff on the 3rd Friday of each month.\n\nIf you are a visionary leader with a passion for building resilient data infrastructures,\n\na deep understanding of revenue attribution and paid marketing channels, and a\n\nproven ability to drive strategic business outcomes through data, we invite you to\n\njoin our Data & Analytics team and shape the future of our data warehousing\n\nfunction.\n\nHow to apply for this opportunity\n\nStep 1: Click On Apply! And Register or Login on our portal.\nStep 2: Complete the Screening Form & Upload updated Resume\nStep 3: Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Uplers:\n\n\nOur goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement.\n\n(Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).\n\nSo, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!","Multi Cloud, Cost Management, Looker, dbt, Data Modelling, BigQuery, Tableau, Sql, Data Quality, data monitoring, Gcp, Dwh, Google Analytics, Data Governance"
Data Architect - Senior Manager,PwC Acceleration Centers in India,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in integration and platform architecture focus on designing and implementing seamless integration solutions and robust platform architectures for clients. They enable efficient data flow and optimise technology infrastructure for enhanced business performance. Those in solution architecture at PwC will design and implement innovative technology solutions to meet clients business needs. You will leverage your experience in analysing requirements, developing technical designs to enable the successful delivery of solutions.\n\nGrowing as a strategic advisor, you leverage your influence, expertise, and network to deliver quality results. You motivate and coach others, coming together to solve complex problems. As you increase in autonomy, you apply sound judgment, recognising when to take action and when to escalate. You are expected to solve through complexity, ask thoughtful questions, and clearly communicate how things fit together. Your ability to develop and sustain high performing, diverse, and inclusive teams, and your commitment to excellence, contributes to the success of our Firm.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\nCraft and convey clear, impactful and engaging messages that tell a holistic story.\nApply systems thinking to identify underlying problems and/or opportunities.\nValidate outcomes with clients, share alternative perspectives, and act on client feedback.\nDirect the team through complexity, demonstrating composure through ambiguous, challenging and uncertain situations.\nDeepen and evolve your expertise with a focus on staying relevant.\nInitiate open and honest coaching conversations at all levels.\nMake difficult decisions and take action to resolve issues hindering team effectiveness.\nModel and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.\n\nSenior Data Architect\n\nJob Description\n\nSenior Data Modeler with experience in leading and delivering data modeling solutions involving complex data landscapes and legacy modernization projects. The ideal candidate will have extensive experience with industry-standard data modeling methodologies, cloud data lake platforms, and NoSQL data modeling. This role requires a strong leader capable of guiding data teams and driving the design and implementation of scalable data architectures.\n\nKey Responsibilities\n\nDesign and implement scalable and efficient data models for complex systems, ensuring alignment with business and technical requirements.\nApply industry-standard data modeling methodologies to create conceptual, logical, and physical data models for structured, semi-structured, and unstructured data.\nDevelop and optimize data models for cloud data lake platforms (e.g., AWS, Azure, Google Cloud).\nCreate and manage NoSQL data models tailored to non-relational databases (e.g., MongoDB, Cassandra, DynamoDB).\nLead cross-functional data teams, overseeing the delivery of large-scale, multi-technology projects.\nCollaborate with stakeholders to define data architecture strategies and ensure models align with enterprise data governance standards.\nIntegrate and manage data across diverse systems, ensuring consistency, integrity, and accuracy.\nEvaluate emerging technologies and tools to enhance data modeling practices and infrastructure.\nProvide technical guidance and mentorship to junior team members.\nDocument data modeling processes and create reusable frameworks for future projects.\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\n13+ years of experience in data modeling, including conceptual, logical, and physical data design.\n5 8 years of experience in cloud data lake platforms such as AWS Lake Formation, Delta Lake, Snowflake or Google Big Query.\nProven experience with NoSQL databases and data modeling techniques for non-relational data.\nExperience with data warehousing concepts, ETL/ELT processes, and big data frameworks (e.g., Hadoop, Spark).\nHands-on experience delivering complex, multi-module projects in diverse technology ecosystems.\nStrong understanding of data governance, data security, and compliance best practices.\nProficiency with data modeling tools (e.g., ER/Studio, ERwin, PowerDesigner).\nExcellent leadership and communication skills, with a proven ability to manage teams and collaborate with stakeholders.\n\nPreferred Skills\n\nExperience with modern data architectures, such as data fabric or data mesh.\nKnowledge of graph databases and modeling for technologies like Neo4j.\nProficiency with programming languages like Python, Scala, or Java.\nUnderstanding of CI/CD pipelines and DevOps practices in data engineering.","Data modeling methodologies, Data Security, Data Governance, Data Warehousing Concepts"
Senior Data Architect,Uplers,15-17 Years,,"Thiruvananthapuram, Thiruvananthapuram / Trivandrum, India",Login to check your skill match score,"Experience: 13.00 + years\n\nSalary: Confidential (based on experience)\n\nShift: (GMT+05:30) Asia/Kolkata (IST)\n\nOpportunity Type: Remote\n\nPlacement Type: Full time Permanent Position\n\n(*Note: This is a requirement for one of Uplers client - Forbes Advisor)\n\nWhat do you need for this opportunity\n\nMust have skills required:\n\nDBT, GCP, DWH, Data Modelling, Data Governance, data quality, data monitoring, Cost Management, Multi Cloud\n\nForbes Advisor is Looking for:\n\nCompany Description\n\nForbes Advisor, part of the Forbes Marketplace family, provides consumers with\n\nexpert-written insights, news, and reviews on personal finance, health, business, and\n\neveryday life decisions. We empower our audience with data-driven knowledge so\n\nthey can make informed choices confidentlybalancing the agility of a startup with\n\nthe stability of a seasoned enterprise\n\nRole Overview\n\nThe Senior Data Architect is a strategic, senior leadership role responsible for\n\nsetting the vision and direction of our data warehousing function. You will architect,\n\nimplement, and maintain a state-of-the-art data warehouse that drives actionable\n\ninsights across revenue, subscriptions, paid marketing channels, and operational\n\nfunctions. Your leadership will ensure data quality, robust pipeline design, and\n\nseamless integration with business intelligence tools. This role requires a strong mix\n\nof technical acumen, team management, and cross-functional collaboration\n\nespecially with teams focused on SEM, Digital Experiences, and revenue attribution\n\nJob Description\n\nKey Responsibilities\n\nStrategic Data Architecture & Pipeline Leadership\n\nVision & Strategy:\n\nDefine and execute the long-term strategy for our data warehousing\n\nplatform using medallion architecture (Bronze, Silver, Gold layers) and\n\nmodern cloud-based solutions.\n\nEnd-to-End Pipeline Oversight:\n\nOversee data ingestion (via Google Ads, Bing Ads, Facebook Ads, GA,\n\nAPIs, SFTP, etc.), transformation (leveraging DBT, and SQL [via\n\nBigQuery]), and reporting, ensuring that our pipelines are robust and\n\nscalable.\n\nData Modeling Best Practices:\n\nChampion best practices in data modeling, including the effective use\n\nof DBT packages to streamline complex transformations.\n\nData Quality, Governance & Attribution\n\nQuality & Validation:\n\nEstablish and enforce rigorous data quality standards, governance\n\npolicies, and automated validation frameworks across all data streams.\n\nStandardization & Visibility:\n\nCollaborate with the Data Engineering, Insights and BIOps team to\n\nstandardize data definitions (including engagement metrics and\n\nrevenue attribution) and ensure consistency across all reports.\n\nAttribution Focus:\n\nDevelop frameworks to reconcile revenue discrepancies and unify\n\nvalidation across Finance, SEM, and Analytics teams.\n\nEnsure accurate attribution of revenue and paid marketing channel\n\nperformance, working closely with SEM and Digital Experiences teams.\n\nMonitoring & Alerting:\n\nImplement robust monitoring and alerting systems (e.g., Slack and\n\nemail notifications) to quickly identify, diagnose, and resolve data\n\npipeline issues.\n\nTeam Leadership & Cross-Functional Collaboration\n\nPeople & Process:\n\nLead, mentor, and grow a high-performing team of data warehousing\n\nspecialists, fostering a culture of accountability, innovation, and\n\ncontinuous improvement.\n\nStakeholder Engagement:\n\nPartner with RevOps, Analytics, SEM, Finance, and Product teams to align\n\nthe data infrastructure with business objectives.\n\nServe as the primary data warehouse expert in discussions around\n\nrevenue attribution and paid marketing channel performance, ensuring\n\nthat business requirements drive technical solutions.\n\nCommunication:\n\nTranslate complex technical concepts into clear business insights for\n\nboth technical and non-technical stakeholders.\n\nOperational Excellence & Process Improvement\n\nDeployment & QA:\n\nOversee deployment processes, including staging, QA, and rollback\n\nstrategies, to ensure minimal disruption during updates.\n\nContinuous Optimization:\n\nRegularly assess and optimize data pipelines for performance,\n\nscalability, and reliability while reducing operational overhead.\n\nLegacy to Cloud Transition:\n\nLead initiatives to transition from legacy on-premise systems to\n\nmodern cloud-based architectures for improved agility and cost\n\nefficiency.\n\nInnovation & Thought Leadership\n\nEmerging Trends:\n\nStay abreast of emerging trends and technologies in data warehousing,\n\nanalytics, and cloud solutions.\n\nPilot Projects:\n\nPropose and lead innovative projects to enhance our data capabilities,\n\nwith a particular focus on predictive and prescriptive analytics.\n\nExecutive Representation:\n\nRepresent the data warehousing function in senior leadership\n\ndiscussions and strategic planning sessions\n\nQualifications\n\nEducation & Experience\n\nBachelor's or Master's degree in Computer Science, Data Science, Information\n\nSystems, or a related field.\n\n15+ years of experience in data engineering, warehousing, or analytics roles,\n\nwith at least 5+ years in a leadership capacity.\n\nProven track record in designing and implementing scalable data\n\nwarehousing solutions in cloud environments.\n\nTechnical Expertise\n\nDeep experience with medallion architecture and modern data pipeline tools,\n\nincluding DBT (and DBT packages), Databricks, SQL, and cloud-based data\n\nplatforms.\n\nStrong understanding of ETL/ELT best practices, data modeling (logical and\n\nphysical), and large-scale data processing.\n\nHands-on experience with BI tools (e.g., Tableau, Looker) and familiarity with\n\nGoogle Analytics, and other tracking systems.\n\nSolid understanding of attribution models (first-touch, last-touch, multi-\n\ntouch) and experience working with paid marketing channels.\n\nLeadership & Communication\n\nExcellent leadership and team management skills with the ability to mentor\n\nand inspire cross-functional teams.\n\nOutstanding communication skills, capable of distilling complex technical\n\ninformation into clear business insights.\n\nDemonstrated ability to lead strategic initiatives, manage competing\n\npriorities, and deliver results in a fast-paced environment.\n\nPerks & Benefits\n\nFlexible/Remote Working: Enjoy flexible work arrangements in a collaborative,\n\ndistributed team culture.\n\nCompetitive Compensation: Attractive salary, performance-based bonuses,\n\nand comprehensive benefits.\n\nTime Off: Generous paid time off, parental leave policies, and a dedicated day\n\noff on the 3rd Friday of each month.\n\nIf you are a visionary leader with a passion for building resilient data infrastructures,\n\na deep understanding of revenue attribution and paid marketing channels, and a\n\nproven ability to drive strategic business outcomes through data, we invite you to\n\njoin our Data & Analytics team and shape the future of our data warehousing\n\nfunction.\n\nHow to apply for this opportunity\n\nStep 1: Click On Apply! And Register or Login on our portal.\nStep 2: Complete the Screening Form & Upload updated Resume\nStep 3: Increase your chances to get shortlisted & meet the client for the Interview!\n\nAbout Uplers:\n\n\nOur goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement.\n\n(Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).\n\nSo, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!","Multi Cloud, Cost Management, Looker, dbt, Data Modelling, BigQuery, Tableau, Sql, Data Quality, data monitoring, Gcp, Dwh, Google Analytics, Data Governance"
Data Architect,G10X,15-17 Years,,India,Login to check your skill match score,"Data Architect\nExp: 15 yrs\nLocation: Remote / Kochi, Kerala\nNotice Period: Immediate to 30 days\nArchitect and implement data ingestion, data validation, and data transformation pipelines. Collaborate with the Data teams to design and maintain batch and streaming integrations across a variety of data domains and platforms. Take ownership in building solutions and proposing architectural designs related to building efficient and timely data ingestion and transformation processes geared towards analytics workloads. Manage code deployment to various environments. Be proficient at positively critiquing and suggesting improvements via code reviews Work with stakeholders to define and develop data ingest, validation, and transform pipelines. Troubleshoot data pipelines and resolve issues in alignment with SDLC. Ability to diagnose and troubleshoot data issues, recognizing common data integration and transformation patterns Estimate, track, and communicate status of assigned items to a diverse group of stakeholders required\nCollaborate with application architects, database administrators, and business stakeholders to ensure seamless data transition and alignment Design ETL (Extract, Transform, Load) processes to support data migration, ensuring high availability and performance. Define data mapping, transformation, and validation strategies during migration. Provide leadership and mentorship to other members of the data team and stakeholders","Data ingestion, Code deployment, Streaming integrations, Data Mapping, Data Validation, Data Migration, Data Transformation"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Gurugram, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata\n\nwork Mode- Hybrid\n\nRoles And Responsibilities\n\nMandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization\n\nSolution Architect for Data modelling Understanding of Enterprise datasets Sales,\n\nProcurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle\n\netc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building\n\nData lake foundation, Maintenance etc)\n\nCollaborate with the product/business teams, understand related business processes and\n\ndocument business requirements and then write high level specifications/requirements for DEs\n\nDevelop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout\n\nfor right grain of data either in True source systems or in Data WHs and build reusable data\n\nmodels in intermediary layers before creating physical consumable views from data mart\n\nUnderstand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data\n\nGovernance, Data Quality frameworks, Data Observality and the candidate should be:\n\nFamiliar with DevOps process\nKnowing how to check existing tables, data dictionary, table structures\nExperienced with normalizing tables\nHaving good understanding of Landing, Bronze, Silver and Gold layers and concepts\nFamiliar with Agile techniques\nCreate business process map, user journey map and data flow integration diagrams; Understand\n\nIntegration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of\n\nmodels\n\nStakeholder management with data engineering, product owners, central data modelling team,\n\ndata governance & stewards, Scrum master, project team and sponsor.\n\nAbility to handle large implementation program with multiple projects spanning over an year.\n\nSkills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Data Observability, Data lake foundation, DataOps, Data Quality frameworks, Agile techniques, data vault, FTP, SAP, Data Architect, Data Modeling, Data Modeler, Dimensional Modeling, Sql, Cloud Architecture, Devops, Sftp, Data Governance, Azure, Data Warehouse, Oracle"
Data Architect,VidPro Consultancy Services,8-15 Years,,India,Login to check your skill match score,"Role: Data Architect\nExperience: 8-15 years\nLocation: Bangalore, Chennai, Gurgaon, Pune, and Kolkata\nMandatory Skills: Python, Pyspark, SQL, ETL, Pipelines, Azure Databricks, Azure Data Factory, & Architect Designing.\nPrimary Roles and Responsibilities:\nDeveloping Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack\nAbility to provide solutions that are forward-thinking in data engineering and analytics space\nCollaborate with DW/BI leads to understand new ETL pipeline development requirements.\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop data model to fulfill reporting needs\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussion with client architect and team members\nOrchestrate the data pipelines in scheduler via Airflow\nSkills and Qualifications:\nBachelor's and/or master's degree in computer science or equivalent experience.\nMust have total 8+ yrs. of IT experience and 5+ years experience in Data warehouse/ETL projects.\nDeep understanding of Star and Snowflake dimensional modelling.\nStrong knowledge of Data Management principles\nGood understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture\nShould have hands-on experience in SQL, Python and Spark (PySpark)\nCandidate must have experience in AWS/ Azure stack\nDesirable to have ETL with batch and streaming (Kinesis).\nExperience in building ETL / data warehouse transformation processes\nExperience with Apache Kafka for use with streaming data / event-based data\nExperience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)\nExperience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.\nProficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot\nDatabricks Certified Data Engineer Associate/Professional Certification (Desirable).\nComfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects\nShould have experience working in Agile methodology\nStrong verbal and written communication skills.\nStrong analytical and problem-solving skills with a high attention to detail.","Airflow, CircleCI, Apache Kafka, Databricks, Sql, Pig, Pl Sql, Impala, Azure Data Factory, RDBMS, Hadoop, Pyspark, Etl, AWS, Unix Shell Scripting, Hive, Cassandra, Python, Neo4j, Terraform, Git, MongoDB"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nDefine and design future state data architecture for HR reporting, forecasting and analysis products.\nPartner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.\nEngage with line of business, operations, and project partners to gather process improvements.\nLead to design / build new models to efficiently deliver the financial results to senior management.\nEvaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.\nCollaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing POCs to ensure those standards are implemented.\nProvide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.\nDevelop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.\nCreate and maintain conceptual / logical data models to identify key business entities and visual relationships.\nWork with business and IT teams to understand data requirements.\nMaintain a data dictionary consisting of table and column definitions.\nReview data models with both technical and business audience\n\nWhat You'll Bring\n\nEssential Education\n\nMinimum of a Bachelor's degree in Computer science, Engineering or a similar field\nAdditional Certification in Data Management or cloud data platforms like Snowflake preferred\n\nEssential Experience & Job Requirements\n\n12+ years of IT experience with major focus on data warehouse/database related projects\nExpertise in cloud databases like Snowflake, Redshift etc.\nExpertise in Data Warehousing Architecture; BI/Analytical systems; Data cataloguing; MDM etc\nProficient in Conceptual, Logical, and Physical Data Modelling\nProficient in documenting all the architecture related work performed.\nProficient in data storage, ETL/ELT and data analytics tools like AWS Glue, DBT/Talend, FiveTran, APIs, Tableau, Power BI, Alteryx etc\nExperience in building Data Solutions to support Comp Benchmarking, Pay Transparency / Pay Equity and Total Rewards use cases preferred.\nExperience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake a plus\nExperience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs) a plus\nExcellent written, oral communication and presentation skills to present architecture, features, and solution recommendations is a must\n\nAdditional info\n\nYOU'RE GOOD AT\n\nDesign, document & train the team on the overall processes and process flows for the Data architecture.\nResolve technical challenges in critical situations that require immediate resolution.\nDevelop relationships with external stakeholders to maintain awareness of data and security issues and trends.\nReview work from other tech team members and provide feedback for growth.\nImplement Data security policies that align with governance objectives and regulatory requirements.\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","Cloud Databases, Cloud Big Data Technologies, FiveTran, Physical Data Modelling, Data Analytics Tools, dbt, MDM, Meta Scrum, Data Cataloguing, Kanban, Data Warehousing Architecture, BI Analytical Systems, Alteryx, AWS Glue, Tableau, ELT, Data Architecture, Scrum, Talend, AWS, Apis, Power Bi, Agile Methodologies, Gcp, Azure, Etl"
Data Architect,CriticalRiver Inc.,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Role: Data Architect\nExperience:- 10 Years\nWork Location :- Hyderabad\nKey Responsibilities:\nDesign and implement scalable, high-performance data architectures using Snowflake.\nDevelop ETL/ELT pipelines using dbt (Data build tool), FiveTran and Airflow to ingest, transform, and process large volumes of data.\nStrong experience of data modelling using dbt\nOptimize data pipelines for efficiency, reliability, and scalability.\nEnsure data integrity, governance, and security across the data ecosystem.\nStrong understanding of dimensional data modelling and data warehousing concepts.\nStrong experience in SQL and Python\nCollaborate with data engineers, analysts, and business stakeholders to define data requirements and architecture.\nWork with cloud platforms (AWS, GCP or Azure) to manage and scale data infrastructure.\nImplement reverse ETL solutions using Hightouch for data activation and operational analytics.\nMonitor and troubleshoot data pipelines and workflows to ensure smooth operations.\nDrive best practices in data modeling, performance tuning, and cost optimization.\nStay updated with emerging technologies and trends in cloud-based data engineering.\nUnderstanding of ERP (NetSuite) data is added advantage.\nStrong communication skills","Airflow, snowflake, dbt, Hightouch, dimensional data modelling, data integrity governance, FiveTran, Sql, AWS, Data Warehousing, Python, Azure, Gcp"
Senior Manager - Data Architect - Pune,Telecoms Management,12-15 Years,,"Pune, India",Login to check your skill match score,"Hybrid\n\nSenior Manager - Data Architect - Pune\n\nPune, Maharashtra, India\n\nApply Now\n\nFind out how well you match with this job\n\nRequisition ID\n\n260603\n\nDate posted\n\n04/23/2025\n\nOrganisational Unit\n\nData & Analytics\n\nJoin Us\n\nAt Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.\n\nWhat You'll Do\n\nPerform ETL design, data architecture, data management and data analysis.\n\nDevelop and enforce data management policies, standards, and best practices.\n\nWork with stakeholders to define data models, metadata, and integration strategies.\n\nExperience in Architecting and designing solutions leveraging capabilities of ETL tools like Ab-Initio.\n\nAccountable to business and technology management for end to end application scoping, planning, development and delivery that meets and exceeds quality standards\n\nOptimize data storage, processing, and retrieval to improve performance and cost efficiency.\n\nEnsure system alignment to Enterprise Architecture policies and best practices; ensure that process methodologies are followed in system development\n\nAssist Project manager with the estimation of technical timelines and allocation of the technical resources to specific task\n\nContribution to the organization in terms of process adoption, resource optimization, tools adoption to bring in efficiency and uplift quality\n\nContribution to the Guilds with the technical expertise\n\nWho You Are\n\nExperience on ETL solutions using Ab-Initio & Teradata, Data Modelling, GCP.\nExposure on Data Modelling tools eg. Power Designer, Erwin etc.\nKnowledge on Data Warehousing, Data Modelling, Data Profiling, GCP Cloud migration etc.\nOverall experience of 12-15 years\nRelevant experience of 4-5 years\n\nNot a perfect fit\n\nWorried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.\n\nWhat's In It For You\n\nWho we are\n\nWe are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.\n\nBelonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.\n\nIf you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.\n\nTogether we can.\n\nInsights from previous hires\n\nTop skills\n\nAgile\n\nBusiness and Commercial Acumen\n\nCommunication\n\nCoaching\n\nBusiness Partnering\n\nBusiness Development\n\nEngineering\n\nChange and Adaptability\n\nBudgeting\n\nBudget Management\n\nPreviously worked at\n\nPreviously worked as\n\nSenior Manager\nData Architect\nSenior Data Architect\nSenior Manager Technology\nLead Specialist\n\nRequisition ID\n\n260603\n\nDate posted\n\n04/23/2025\n\nOrganisational Unit\n\nData & Analytics\n\nJoin Us\n\nAt Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.\n\nWho You Are\n\nExperience on ETL solutions using Ab-Initio & Teradata, Data Modelling, GCP.\nExposure on Data Modelling tools eg. Power Designer, Erwin etc.\nKnowledge on Data Warehousing, Data Modelling, Data Profiling, GCP Cloud migration etc.\nOverall experience of 12-15 years\nRelevant experience of 4-5 years\n\nNot a perfect fit\n\nWorried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.\n\nWho We Are\n\nWe are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.\n\nBelonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.\n\nIf you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.\n\nTogether we can.","power designer, Teradata, Erwin, Data Modelling, Gcp, Data Profiling, Data Warehousing, Cloud Migration, Ab-initio, Etl"
Big Data Architect,Skyhigh Security,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title:\n\nBig Data Architect\n\nAbout Skyhigh Security:\n\nSkyhigh Security is a dynamic, fast-paced, cloud company that is a leader in the security industry. Our mission is to protect the world's data, and because of this, we live and breathe security. We value learning at our core, underpinned by openness and transparency.\n\nSince 2011, organizations have trusted us to provide them with a complete, market-leading security platform built on a modern cloud stack. Our industry-leading suite of products radically simplifies data security through easy-to-use, cloud-based, Zero Trust solutions that are managed in a single dashboard, powered by hundreds of employees across the world. With offices in Santa Clara, Aylesbury, Paderborn, Bengaluru, Sydney, Tokyo and more, our employees are the heart and soul of our company.\n\nSkyhigh Security Is more than a company; here, when you invest your career with us, we commit to investing in you. We embrace a hybrid work model, creating the flexibility and freedom you need from your work environment to reach your potential. From our employee recognition program, to our Blast Talks learning series, and team celebrations (we love to have fun!), we strive to be an interactive and engaging place where you can be your authentic self.\n\nWe are on these too! Follow us on LinkedIn and Twitter@SkyhighSecurity.\n\nRole Overview:\n\nThe Big Data Architect will be responsible for the design, implementation, and management of the organization's big data infrastructure. The ideal candidate will have a strong technical background in big data technologies, excellent problem-solving skills, and the ability to work in a fast-paced environment. The role requires a deep understanding of data architecture, data modeling, and data integration techniques.\n\nAbout the Role:\n\nDesign and implement scalable and efficient big data architecture solutions to meet business requirements.\nDevelop and maintain data pipelines, ensuring the availability and quality of data.\nCollaborate with data scientists, data engineers, and other stakeholders to understand data needs and provide technical solutions.\nLead the evaluation and selection of big data tools and technologies.\nEnsure data security and privacy compliance.\nOptimize and tune big data systems for performance and cost-efficiency.\nDocument data architecture, data flows, and processes.\nStay up-to-date with the latest industry trends and best practices in big data technologies.\n\nAbout You:\n\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\nover all 10+ years exp with 5+ years of experience in big data architecture and engineering.\nProficiency in big data technologies such as Hadoop mapredue, Spark batch and streaming, Kafka, HBase, Scala, Elastic Search and others.\nExperience with AWS cloud platform.\nStrong knowledge of data modeling, ETL processes, and data warehousing.\nProficiency in programming languages such as Java, Scala, Spark\nFamiliarity with data visualization tools and techniques.\nExcellent communication and collaboration skills.\nStrong problem-solving abilities and attention to detail.\n\nCompany Benefits and Perks:\n\nWe work hard to embrace diversity and inclusion and encourage everyone to bring their authentic selves to work every day. We offer a variety of social programs, flexible work hours and family-friendly benefits to all of our employees.\n\nRetirement Plans\nMedical, Dental and Vision Coverage\nPaid Time Off\nPaid Parental Leave\nSupport for Community Involvement\n\nWe're serious about our commitment to diversity which is why we prohibit discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.","ETL processes, Data visualization tools, Java, Hadoop, Scala, Kafka, Data Modeling, Data Warehousing, HBase, Spark, Elastic Search, AWS"
Cloud Data Architect,myCloudDoor,3-5 Years,,"Jodhpur, India",Login to check your skill match score,"Do you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!\n\nWho we are\n\nmyCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.\n\nTasks\n\nThe Selected Person Will Do The Following Tasks\n\nDefinition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...\nMaintenance of data solutions, failure analysis and solution proposal.\nCommunication with customers: proposal solutions, technical trainings...\n\nThe profile\n\nWe are looking for a person who fit the following requirements:\n\n+3 years of experience years of experience in a similar role.\nExperience in Azure projects\nReal experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...\nReal Experience in AI\nExperience in presales and proposals\n\nWhat we offer you\n\nCareer Path\nRemote working\nTraining: Internal and technical certifications\n\nThink you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.","Presales, Proposals, Ai, Data Lakes, Azure, Data Architecture, Data Warehouse"
Data Architect,LSEG (London Stock Exchange Group),12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Data Architect Corporate Engineering\n\nCompany Profile\n\nLSEG (London Stock Exchange Group) is a world-leading financial markets infrastructure and data business. We are dedicated, open-access partners with a commitment to excellence in delivering services across Data & Analytics, Capital Markets, and Post Trade.\n\nBacked by three hundred years of experience, innovative technologies, and a team of over 23,000 people in 70 countries, our purpose is driving financial stability, empowering economies, and enabling customers to create sustainable growth.\n\nRole Profile\n\nIn this role, you'll be joining our CRM, External Digital and Marketing team within Data and Integrations Team, Corporate Engineering (CE) as a Data Architect. This team works on data and integration requests by guiding customers on data migration cleansing, data quality techniques. This role impacts all divisional users of CRM Technology and downstream systems reliant on Customer Data 7k users across Capital Markets, Post Trade and Data & Analytics.\n\nThe data and integrations team do this by:\n\nConducting data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion/AWS Glue).\nBuilding Enterprise Data Architecture or Data Lake for the Migration projects\nDefining the data quality of the sources and lists the data quality metrics of the source systems.\nWorking on detailed scoping and requirements working with business customers, SMEs, Technology Partner organizations and internal CRM Technology and Corporate Technology teams.\nBuilding and maintaining Customer Master and Product Master.\nContinuously review operating metrics and data to find opportunities to improve.\n\nTech Profile/Essential Skills\n\n12 years of technical experience.\n7 years of experience on data migration and reporting using ETL and Reporting Tools\n5 years of experience on ETL/Database Development\n2 years of experience on Salesforce Data Migration projects\nExpert level skill on Informatica IICS or Matillion or AWS Glue or equivalent ETL tool which covers from extracting the source to building the complex mappings.\nProficient in the use or extract of data from Salesforce.\nMust have knowledge on the Salesforce Data modelling.\nMust have Experience on Snowflake Storage and Database.\nExpert level coding knowledge on Python to do ETL.\nAble to translate business requirements into data solutions.\nUnderstanding of Salesforce concepts.\nSnowflake development.\nExperience with providing technical solutions and supporting documentation.\n\nPreferred Skills And Experience\n\nExperience of Tableau and/or Power BI reporting preferred for management reporting.\nMust have experience of working with Cloud Native based applications.\nUnderstanding of the SDLC and agile delivery methodology.\nExperience working with databases and data, performing data cleanup, and/or data manipulation and migration to and from Salesforce.com.\nShould have experience with Enterprise Architect or Erwin Data modeler\nAbility to handle own work and multitask to meet tight deadlines without losing sight of priorities under minimum supervision.\nHighly motivated, self-directed individual with a positive & pro-active demeanor to work.\nCustomer and service focused, with determination to meet their needs and expectations.\nBe driven and committed to the goals and objectives of the team and organization.\n\nEducation and Professional Skills\n\nProfessional qualification or equivalent.\nBS/MS degree in Computer Science, Software Engineering or STEM degree (Desirable).\nCurious about new technologies and tools, creative thinking and initiative taking.\nAgile related certifications preferable.\n\nDetailed Responsibilities\n\nProficient in data discovery, data migration, data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion Preferable).\nAnalyses the data quality of the sources and lists the data quality metrics of the source systems.\nDomain expert in MDM and Customer data.\nLead the Single Customer View and Customer 360 implementations.\nLead the data migration strategies for large scale programs.\nData mining to uncover patterns, anomalies, and correlations in large data sets.\nData management to efficiently and cost-effectively collect, store, and use data.\nCoding languages like Python and Java to develop applications for data analysis.\nMachine learning to build scalable systems for handling Big Data Systems.\nStructured query language (SQL) to manipulate data.\nData modelling tools like Erwin or Visio to visualize metadata and database schema.\nCreating and implementing data management processes and procedures\nResearching data acquisition opportunities.\nDeveloping application programming interfaces (APIs) to retrieve data.\nDevelops and improves data governance and business data processes within the Technology and business organizations and understands client requirements, specifying and analyzing these to a sufficient level of detail to ensure transparency of definition and ability for technical teams to translate to a technical solution design.\nWorks with developers, architects, and solution designers to translate sophisticated business requirements and provides feedback on technical solutions proposed.\nResponsible for building a relationship with partners, collaborators and impacted users.\nDemonstrates proposed solutions and seeks and addresses feedback.\nProactively identifies, recommends, and implements improvements to the process as it relates to assigned projects.\nFlexible in approach, adapting plans and strategies to help handle risks around ambiguity.\nStrategic problem solver with strong intuition for business and well-versed in current technological trends and business concepts.\n\nLSEG Benefits\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate based on anyone's race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","IICS, Salesforce Data Migration, Snowflake Storage and Database, Matillion, Power Bi, AWS Glue, Tableau, Sql, Python, Etl"
CDC Data Architect,LSEG (London Stock Exchange Group),8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Design and Development: Build, enhance, and lead the company's logical, conceptual, and physical data models, demonstrating Snowflake's advanced capabilities.\n\nData Integration: Lead all aspects of the development of comprehensive data integration processes using ETL for various data types, including structured, semi-structured, and unstructured data, ensuring seamless integration with Snowflake.\n\nPerformance Tuning: Implement and fine-tune Snowflake features such as resource monitors, RBAC controls, scalable virtual warehouses, SQL performance tuning, zero copy clone, and time travel to optimize performance.\n\nData Security: Ensure robust data security and handle access controls effectively within the Snowflake environment.\n\nCloud Integration: Deploy cloud-based enterprise data warehouse solutions, Leverage AWS services such as S3, Glue, Athena, CloudWatch, and EMR to enhance data storage, processing, and analytics capabilities and integrate seamlessly with platforms like AWS and applying Snowflake's cloud-native architecture.\n\nData Governance: Uphold consistent data governance, testing, and continuous delivery practices, ensuring data integrity and compliance within Snowflake.\n\nAI Integration: Incorporate AI and machine learning models into the data architecture, demonstrating Snowflake's capabilities to handle large-scale data processing and real-time analytics.\n\nSnowpark/Python Development: Use Python/Snowpark for developing data pipelines, ETL processes, and automation scripts, ensuring efficient data handling and processing within the Snowflake environment\n\nTeamwork: Serve as a data domain expert, working closely with various teams to ensure standard methodologies in data management are followed, and facilitate the integration of AI insights into business processes !\n\nCandidate Profile / Key skills\n\nExperience: At least 8 years in Data Engineering or Data Management Solutions, with a proven track record of improving data pipeline processes and leading initiatives.\n\nSnowflake Expertise: A minimum of 5 years of meaningful experience with Snowflake.\n\nClient Leadership: Skilled in leading data-centric client engagements.\n\nTechnical Proficiency: Demonstrable skills in sophisticated SQL, Unix Shell/Python scripting, performance tuning, and database optimization.\n\nCloud Technologies: Expertise in AWS services, including S3, EC2, Lambda, and Redshift.\n\nData Handling & Migration: Proficient in managing semi-structured data (JSON, XML) and using Snowflake's VARIANT attribute. Experience in migrating data from on-premises databases to Snowflake.\n\nAutomation: Skilled in crafting and developing automated data pipelines using Snowpipe and other relavant tools !\n\nDatabase Experience: Hands-on experience with databases and data warehousing solutions such as Oracle, Microsoft SQL Server, AWS Redshift, or Snowflake.\n\nCloud Experience: Experience with AWS or Azure is a plus.\n\nSQL Analysis: Strong SQL analysis skills and familiarity with tools like JIRA, Asana, or other relevant defect tracking tools. Experience in implementing data quality frameworks is an added advantage.\n\nProgramming Skills: Proficiency in Python, PySpark, or Snowpark is helpful. Additional expertise in Cortex or AI capabilities is a big plus.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","snowflake, Snowpark, Data Handling Migration, AI Integration, Automation, Sql, Cloud Technologies, Performance Tuning, AWS, Data Security, Etl, Data Integration, Python, Data Governance, Unix Shell"
Data Architect,Tarento Group,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Tarento Technologies is a dynamic and innovative technology solutions provider, specializing in delivering cutting-edge IT services and solutions. With a strong focus on software development, data analytics, cloud computing, and enterprise applications, Tarento aims to empower businesses to thrive in the digital age. The company's team of experts combines industry knowledge with technical expertise to deliver tailored solutions that meet the unique needs of each client.\n\nAt Tarento, we believe in fostering a collaborative, growth-oriented environment, where creativity and continuous learning are encouraged. As we expand, we are looking for passionate individuals to join our team and help shape the future of technology-driven solutions. If you're ready to take on exciting challenges and grow professionally, Tarento Technologies offers the ideal platform to advance your career.\n\nWe are looking for Technically Data Architects with hands-on experience in Apache Spark and Databricks skills with expertise on big data processing, data warehousing, and cloud platforms like Azure, AWS, or GCP.\n\nKey Responsibilities\n\nDesign, develop, and optimize ETL/ELT pipelines using Databricks and Apache Spark.\nImplement data lakes, data warehouses, and lakehouses using Delta Lake.\nDevelop scalable, high-performance data solutions for batch and streaming data processing.\nOptimize Spark jobs for performance and cost efficiency.\nImplement data governance, security, and compliance best practices.\nWork with CI/CD pipelines for data workflows using tools like Terraform, Git, and DevOps practices.\nCollaborate with data analysts, scientists, and business teams to understand data requirements.\n\nRequired Skills & Experience\n\n10 years of experience in data engineering.\nStrong expertise in Databricks, Apache Spark (PySpark/Scala/Java), and Delta Lake.\nProficiency in SQL, Python, or Scala.\nHands-on experience with ETL/ELT pipeline development.\nExperience with cloud platforms (Azure Data Factory, AWS Glue, GCP Dataflow).\nKnowledge of data modeling, data lakes, and data warehousing\nUnderstanding of CI/CD, Git, and DevOps tools.\nStrong troubleshooting and performance optimization skills.\n\nWork Location : Bangalore","GCP Dataflow, CI CD, Delta Lake, Scala, AWS Glue, Apache Spark, Sql, ELT, Devops, Git, Azure Data Factory, Databricks, Python, Etl"
Data Architect,Velotio Technologies,6-9 Years,,"Pune, India",Login to check your skill match score,"Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.\nAs a Data Architect you'll be leading the effort to establish world-class data foundations in the world while working with real-time streaming plant data, cutting-edge ETL pipeline software tools, and solving complex data problems.\nResponsibilities :\nLead the technical delivery of the company for large enterprises through the customer lifecycle: discovery, implementation, expansion, and ongoing support.\nDesign and build complex, streaming data pipelines that synthesize disparate manufacturing data sources using a proprietary ETL engine.\nCollaborate with Data Scientists to design and implement advanced analytics using ML/AI.\nAnalyze key customer use cases, identify data sources, and architect common data solutions that unlock insights to enable data-driven continuous improvement.\nPartner with customer teams to establish trust.\nDrive adoption through internal and external data validation exercises.\nContribute to data dictionaries and process flow diagrams for complex data solutions.\nInvestigate, diagnose, and resolve data challenges using common data mining techniques that often involve creating custom Python notebooks or constructing complex SQL queries.\nCollaborate with product and platform engineering teams to define and optimize new features at scale.\nDesired Skills & Experience:\n6 to 9 years of experience as a Data Architect.\nAbility to work independently and collaboratively with other teams to achieve goals and represent the business.\nGood to have experience working with batch or streaming data processes.\nStrong analytical ability and problem-solving skills.\nMust be comfortable working with customers to relate the concerns and suggest best possible practices.\nGood to Have :\nExperience with technologies such as Python, Java, Git, Pandas, or R\nExperience working in a manufacturing environment or on 6 sigma Projects.\nExperience in data analysis with SQL or NoSQL databases.\nOur Culture:\nWe have an autonomous and empowered work culture encouraging individuals to take ownership and grow quickly\nFlat hierarchy with fast decision making and a startup-oriented get things done culture\nA strong, fun & positive environment with regular celebrations of our success. We pride ourselves in creating an inclusive, diverse & authentic environment\nAt Velotio, we embrace diversity. Inclusion is a priority for us, and we are eager to foster an environment where everyone feels valued. We welcome applications regardless of ethnicity or cultural background, age, gender, nationality, religion, disability or sexual orientation.","R, Nosql, Java, Git, Pandas, Sql, Python, Etl"
Cloud Data Architect,PURVIEW,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nJob Description\nA Cloud Data Architect who has experience working on Google Cloud Platform and have built solutions using Data Services of GCP\nExperience of 10+ years and who understands the data principles and experience dealing with migrating BI data warehouses, BigData platforms into Google Cloud\nHaving experience on how to extract, transform, load the data into BI systems and experience with Datawarehousing will help\nExperience building solutions using Google BigQuery, Google Dataflow, Google Cloud Storage, Google Pubsub which we use on our data platform\nExperience working with business, engineering, data modeling teams in defining the architecture solution\nUnderstands the architecture risks, design principles and suggest ways to business/engineering teams on tactical (vs) strategic solutions depending on various scenarios one would see\nAbout Company :\n\nPurview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.\n\nWe have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.\n\nIn\n\nCompany Info:\n\n3rd Floor, Sonthalia Mind Space\n\nNear Westin Hotel, Gafoor Nagar\n\nHitechcity, Hyderabad\n\nPhone: +91 40 48549120 / +91 8790177967\n\nUk\n\nGyleview House, 3 Redheughs Rigg,\n\nSouth Gyle, Edinburgh, EH12 9DQ.\n\nPhone: +44 7590230910\n\nEmail: [HIDDEN TEXT]\n\nLogin to Apply !","Google Dataflow, Google Cloud Storage, Google Pubsub, Google BigQuery, Data Services of GCP, Google Cloud Platform"
Data Architect,ACL Digital,8-10 Years,,"Pune, India",Login to check your skill match score,"Total yrs of Exp: 8+ yrs\nLocation:Balewadi, Pune\nTechnical skills and core competencies\nStrong understanding of Data Architecture and models and leading data driven projects.\nSolid expertise and strong opinions on Data Modelling paradigms such as Kimball, Inmon, Data Marts, Data Vault, Medallion etc.\nStrong experience with Cloud Based data strategies and big data technologies AWS Preferred.\nSolid experience in designing data pipelines for ETL with expert knowledge on ingestion, transformation, and data quality is a must\nHands-on experience in SQL is a must.\nDeep understanding of PostGreSQL development, query optimization and designing indexes is a must.\nAn ability to understand and manipulate intermediate to complex level of SQL\nThorough knowledge of Postgres PL/SQL to work with complex warehouse workflows.\nAbility to use advanced SQL concepts such as RANK, DENSE_RANK along with applying advanced statistical concepts through SQL is required.\nWorking experience with PostGres SQL extensions like PostGIS is desired.\nExpertise writing ETL pipelines combining Python + SQL is required.\nUnderstanding of data manipulation libraries in python like Pandas, Polars, DuckDB is desired\nExperience in designing Data visualization with different tools such as Tableau and PowerBI is desirable.\nResponsibilities\nParticipate in the design and developing features in the existing Data Warehouse.\nProvide leadership in establishing connection between Engineering, product and analytics/data scientists team.\nDesign, implement, update existing/new batch ETL pipelines\nDefine and implement data architecture.\nPartner with both engineers and data analysts to build reliable datasets that can be trusted, understood, and used by the rest of the company.\nWork with various data orchestration tools (Apache Airflow, Dagster, Prefect and others)\nEmbrace a fast-paced start-up environment.\nYou should be passionate about your job and enjoy a fast-paced international working environment.\nBackground or experience in the telecom industry is a plus but not a requirement.\nLove automating and enjoy monitoring","Cloud Based data strategies, PostGreSQL development, Postgres PL SQL, Query Optimization, Sql, Data Architecture"
Enterprise Data Architect,ACA Group,7-10 Years,,"Pune, India",Login to check your skill match score,"About ACA:\n\nACA was founded in 2002 by four former SEC regulators and one former state regulator. The founders saw a need for investment advisers to receive expert guidance on existing and new regulations. Over the years, ACA has grown both organically and by acquisition to expand our GRC business and technology solutions. Our services now include GIPS standards verification, cybersecurity and technology risk, regulatory technology, ESG advisory, AML and financial crimes, financial and regulatory reporting, and Mirabella for establishing EU operations.\n\nACA is an equal opportunity employer that values diversity. We conduct our business without regard to actual or perceived age, race, color, religion, disability, caregiver, marital or partnership status, pregnancy (including childbirth, breastfeeding, or related medical conditions), ancestry, national origin and citizenship, sex, gender identity and expression, sexual orientation, sexual and reproductive health decisions, military or veteran status, creed, genetic predisposition, carrier status or any other category protected by federal, state and local law.\n\nPosition Summary:\n\nAs an Enterprise Architect at ACA you will be responsible for overseeing the development and use of data systems. You will discover efficient ways to organize, store, and analyze data with attention to security and confidentiality. A great asset for a data-driven company, your strategic planning and oversight will help us improve our operational efficiency and drive business growth.\n\nJob Duties:\n\nDevelop and implement data management strategies that align with company goals.\nOversee the collection, storage, management, quality, and protection of data.\nEnsure data accuracy and accessibility and reduce data redundancy.\nCollaborate with IT teams and management to devise a data strategy that addresses industry requirements.\nLead and mentor a team of data professionals\nContribute to the design and development of the core Enterprise Data Services hub\nWork directly with internal business stakeholders to drive out requirements and gain alignment on solution options.\n\n\nEducation, Experience and Skills:\n\nBachelor's degree in Computer Science, Data Science or related field and a minimum of seven (7) years of experience OR a minimum of ten (10) years of applicable industry experience (if no bachelor's degree)\nKnowledge of data modeling practices with experience building enterprise data solutions\nExperience designing and building reporting and dashboarding solutions\nWorking understanding of Microsoft Azure cloud environment and applicable data technologies Azure Data Factory, Event Hubs, Synapse etc.\nWillingness to learn and experiment with new technologies to provide best in class solutions\nGood communication skills ability to discuss technical topics with business stakeholders\nWorking experience to pull and push data from various APIs, including REST\nGood understanding of using Postman\nWorking experience to read and write SQL, stored procedures and functions\nGood understanding and demonstrated experience with Azure Data Factory pipelines\nGood understanding and demonstrated experience with Azure data landscape.\nExperience with DAX and M query formations.\n\n\nPreferred Education and Experience:\n\nExperience working in Power BI\nExperience building SSAS cubes, particularly tabular modeling\nC#, Python or other object-oriented programming experience a plus used currently to write Azure\nFunctions as needed\nAI/ML knowledge in support of long-term strategy\n\n\nRequired Skills and Attributes:\n\nExperienced in Data Modeling\nExperience with Azure Cloud and relevant data technologies\nExperience building reports and dashboards preferably in Power BI.\nAbility to work directly with and communicate well with all levels of leadership and business partners.\nMust be comfortable balancing several projects at once and able to pivot as business need arise\n\n\nPreferred Licenses and Certification(s):\n\nMicrosoft Certification in Azure or Equivalent\nData Science Certifications\nData management Certification\n\n\nWhy join our team\n\nWe are the leading governance, risk, and compliance (GRC) advisor in financial services. When you join ACA, you'll become part of a team whose unique combination of talent includes the industry's largest team of former regulators, compliance professionals, legal professionals, and GIPS standards verifiers in the industry, along with practitioners in cybersecurity, ESG, and regulatory technology.\n\nOur team enjoys an entrepreneurial work environment by offering innovative and tailored solutions for our clients. We encourage creative thinking and making the most of your experience at ACA by offering multiple career paths. We foster a culture of growth by focusing on continuous learning through inquiry and curiosity, and transparency. If you're ready to be part of an award-winning, global team of thoughtful, talented, and committed professionals, you've come to the right place.","SSAS cubes, SQL stored procedures and functions, Synapse, Event Hubs, Reporting and dashboarding solutions, Data modeling practices, Enterprise data solutions, M query formations, Azure Data Factory, Power Bi, Dax, Postman, Python, Microsoft Azure"
GCP Data Architect,Lingaro,10-12 Years,,India,Login to check your skill match score,"Job Title: Senior Data Architect (GCP)\nLocation: India (Remote)\nAbout Lingaro:\nLingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.\nSince 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.\nAbout Data Management Competency: Focused on Data Governance and Quality Management, establishing and enforcing policies, processes, and practices to ensure the integrity, availability, and reliability of data across the organization.\nDuties:\nFormulate and communicate the organization's data strategy, including data quality standards, data flow, and data security measures.\nProvides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements.\nDefine and implement data governance policies, procedures, and frameworks to ensure data integrity and compliance.\nCollaborate with stakeholders to align data strategy with business goals and objectives, document current and target state in the form of business process and data journey diagrams.\nDesign and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.\nDefine data standards, naming conventions, and data classification guidelines. Ensure data models are scalable, efficient, and optimized for performance.\nEvaluate and select appropriate database technologies and solutions based on organizational needs and requirements.\nDesign and oversee the implementation of data platforms, including relational databases, NoSQL databases, data warehousing, and Big Data solutions.\nOptimize database performance, ensure data security, and implement backup and recovery strategies.\nDesign data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, document source to target mappings.\nCollaborate with IT team and data experts to identify opportunities for data acquisition.\nUnderstand and follow data architecture patterns for various types of data systems, e.g. data lakehouse platforms, master data management systems, ML enriched data flows.\nImplement data profiling and data cleansing processes to identify and resolve data quality issues.\nEstablish data quality standards and implement processes to measure, monitor, and improve data quality.\nFacilitate discussions and workshops to gather requirements and align data initiatives with business goals, prepare data inventory documentation.\nCommunicate complex technical concepts effectively to both technical and non-technical stakeholders.\nStay abreast of industry trends and emerging technologies in data management, analytics, and security.\nEvaluate and recommend new tools, technologies, and frameworks to enhance data architecture capabilities.\nProvide guidance and support to developers and other team members on data-related topics.\nConduct knowledge sharing sessions and training programs to promote understanding and adoption of data architecture best practices.\nRequirements:\nBachelor's or master's degree in computer science, Information Systems, or a related field.\n10+ years experience as a Data Architect or in a similar role, ideally in a complex organizational setting.\nStrong understanding of data management principles, data modeling techniques, database design and data integration flows.\nExperience in developing and implementing data governance policies, procedures, and frameworks to ensure data integrity and compliance.\nFamiliarity with industry best practices and emerging trends in data management and governance.\nAbility to design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.\nStrong understanding of data modeling best-practices, e.g. data normalization, denormalization, generalization, and performance optimization techniques.\nExpertise in working with various database technologies, such as relational databases (e.g., Oracle, SQL Server, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB).\nKnowledge of database management systems, data warehousing, data integration technologies, and Big Data solutions.\nFamiliarity with cloud-based database, warehouse, and lakehouse platforms.\nExperience in designing and implementing data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, understanding of data integration patterns and best practices.\nUnderstanding of Business Intelligence and data analytics requirements, optimization of data storage and processing for reporting needs.\nExcellent communication and presentation skills to effectively articulate complex technical concepts to both technical and non-technical stakeholders.\nAbility to collaborate with cross-functional teams, including business analysts, developers, and data scientists, to understand requirements and drive data-related initiatives.\nStrong analytical and problem-solving abilities to identify data-related issues, propose solutions, and make data-driven decisions.\nFamiliarity with data profiling, data cleansing, data harmonization, and data quality assessment techniques.\nKnowledge of data security and privacy regulations, such as GDPR or CCPA, understanding of data encryption, access controls, and data masking techniques to ensure the security of sensitive data.\nProfessional certification in data management or related field would be advantageous.\nWhy join us:\nStable employment. On the market since 2008, 1300+ talents currently on board in 7 global sites.\n100% remote.\nFlexibility regarding working hours.\nFull-time position\nComprehensive online onboarding program with a Buddy from day 1.\nCooperation with top-tier engineers and experts.\nUnlimited access to the Udemylearning platform from day 1.\nCertificate training programs. Lingarians earn 500+ technology certificates yearly.\nUpskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.\nGrow as we grow as a company. 76% of our managers are internal promotions.\nA diverse, inclusive, and values-driven community.\nAutonomy to choose the way you work. We trust your ideas.\nCreate our community together. Refer your friends to receive bonuses.\nActivities to support your well-being and health.\nPlenty of opportunities to donate to charities and support the environment.","Relational Databases, data quality assessment, Big Data solutions, NoSQL databases, ETL processes, Data Integration, Data Modeling, Data Governance, Database Design, Data Profiling, Data Warehousing, Data Security, Data Cleansing"
CFIN Data Architect,ABB,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN Data Architect\n\nAt ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.\n\nWrite the next chapter of your ABB story.\n\nThis position reports to\n\nHead of Central Finance\n\nYour role and responsibilities\n\nWe are looking for an experienced and technically proficient Data Architect to lead the design, integration, and optimization of the technical solutions within the Central Finance (CFIN) landscape. The Data architect will be responsible for ensuring that data replication and technical activities are fully aligned with business needs, effectively integrated with other enterprise applications, and supported by automated solutions to enhance operational efficiency. This role involves close collaboration with various internal teams, including Finance, IS Architecture, and external vendors, to maintain and evolve the data architecture, ensuring it meets business requirements and is fully compliant with ABB's standards.\n\nThe work model for the role is:\n\nThis role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.\n\nYou will be mainly accountable for:\n\nSolution Design & Validation: Review and validate the design of all Data & Technical related solutions within the CFIN framework, ensuring they are aligned with business goals and technical requirements.\nOwnership of Data Architecture: Define, document, and own the overall data architecture within the CFIN ecosystem, including technical components, modules, and integration with other applications.\nData Replication and automation: Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG and Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data\nIntegration with other processes: Collaborate with other business streams (O2C, P2P, P2D, R2R, TAX, Treasury) to ensure data standards are maintained and design comprehensive data solutions that incorporate all work streams\nMaintain Solution Roadmap: Keep the target Data solution architecture up-to-date, documenting changes to the roadmap and their impact on the broader enterprise architecture. Collaboration with Stakeholders: Work closely with the CFIN solution team, IS architects, vendors, and business stakeholders (including Finance, Process, Data, and Systems Finance teams) to configure, maintain, and enhance the CFIN landscape, ensuring business continuity.\nBusiness Process Alignment: Collaborate with Data Global Process Owners (GPOs) and business teams to define and implement robust Data solutions that align with business requirements and global best practices. Automation & Innovation: Drive the regular implementation of automation solutions within the CFIN system to streamline Data processes, reduce manual effort, and improve efficiency.\nRequirements Validation: Support the validation of business and functional requirements alongside Process Owners, FPDS team, and Technical Leads, ensuring processes are allocated to the appropriate applications and technologies.\nCompliance & Standards: Ensure that all Data & technical solutions and work processes are compliant with ABB's internal standards, policies, and regulatory requirements. Continuous Improvement: Maintain and enhance domain expertise in Data and related technologies, keeping abreast of industry trends and ABB standards to drive continuous improvement within the organization.\n\nQualifications for the role\n\nEducation: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in FICO SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.\nAt least 7-10 years of experience in Data Architect, SAP Architect, or a similar role, with deep knowledge of Data processes and system integration.\nAdvanced expertise in SAP Central Finance (CFIN), SAP S/4HANA, or other ERP systems. Proficient in data process automation tools and strategies.\nExtensive experience with data migration and replication between SAP systems. In-depth knowledge of SAP Business Technology Platform (BTP), FIORI, and other related applications.\nStrong understanding of real-time data replication and automation standards. Strong leadership and team management skills, with the ability to motivate and guide cross-functional teams.\nExcellent collaboration skills with the ability to coordinate between different stakeholders, including business leaders, technical teams, and external partners.\nA strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.\nExperience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.\n\nMore about us\n\nFinance Services is ABB's shared services organization which delivers operational and expert services in Finance, with employees based in five main hubs and front offices, finance service provides mainly Business services to ABB teams across the globe as well as supports with external customer inquiries.\n\nWe value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory\n\nIt has come to our attention that the name of ABB is being used for asking candidates to make payments for job opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made available on our career portal for all fitting the criteria to apply. ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions. For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Real-time data replication and automation standards, Data migration and replication between SAP systems, Data process automation tools, Fiori"
Lead Data Architect,Chevron,10-15 Years,,"Bengaluru, India",Login to check your skill match score,"About The Position\n\nLead Data architects lead the design and implementation of data collection, storage, transformation, orchestration (movement) and consumption to achieve optimum value from data. They are the technical leaders within data delivery teams. They play a key role in modeling data for optimal reuse, interoperability, security and accessibility as well as in the design of efficient ingestion and transformation pipelines. They ensure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration. And they instill trust through the employment of data quality frameworks and tools.\n\nThe data architect at Chevron predominantly works within the Azure Data Analytics Platform, but they are not limited to it. The Senior Data architect is responsible for optimizing costs for delivering data. They are also responsible for ensuring compliance to enterprise standards and are expected to contribute to the evolution of those standards resulting from changing technologies and best practices.\n\nKey Responsibilities\n\nDesign and overseeing the entire data architecture strategy\nMentor junior data architects to ensure skill development in alignment with the team strategy\nDesign and implement complex scalable, high-performance data architectures that meet business requirements\nModel data for optimal reuse, interoperability, security and accessibility\nDevelop and maintain data flow diagrams, and data dictionaries\nCollaborate with stakeholders to understand data needs and translate them into technical solutions\nEnsure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration\nEnsure data quality, integrity, and security across all data systems\n\nRequired Qualifications\n\nBachelor's degree in computer science, Information Technology, or a related field (or equivalent experience)\nOverall 10-15 years of experience with at least 5 years of proven experience as a Data Architect or similar role\nStrong knowledge of data modeling, data warehousing, and data integration techniques\nProficiency in database management systems (e.g., SQL Server, Oracle, PostgreSQL)\nExperience with big data technologies (e.g., Hadoop, Spark) and data lake solutions (e.g., Azure Data Lake, AWS Lake Formation)\nExperience with big data technologies data lake solutions DBMS and cloud platforms\nExperience in data modeling, ERDs, Star and/or Snowflake, and physical model design for analytics and application integration\nExperience in designing data pipelines for optimal performance, resiliency, and cost efficiency\nExperience translating business objectives and goals into technical architecture for data solutions\nFamiliarity with cloud platforms (e.g., Microsoft Azure, AWS, Google Cloud Platform)\nStrong understanding of data governance and security best practices\nExcellent problem-solving skills and attention to detail\nStrong communication and collaboration skills\nTrack record for defining/implementing data architecture framework and governance around master data, meta data, modeling\n\nPreferred Qualifications\n\nExperience in Erwin, Azure Synapse, Azure Databricks, Azure DevOps, SQL, Power BI, Spark, Python, R\nAbility to drive business results by building optimal cost data landscapes\nFamiliarity with Azure AI/ML Services, Azure Analytics: Event Hub, Azure Stream Analytics, Scripting: Ansible\nExperience with machine learning and advanced analytics\nFamiliarity with containerization and orchestration tools (e.g., Docker, Kubernetes)\nUnderstanding of CI/CD pipelines and automated testing frameworks\nCertifications such as AWS Certified Solutions Architect, IBM certified data architect or similar are a plus\n\nChevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm.\n\nChevron participates in E-Verify in certain locations as required by law.","CI CD pipelines, data integration techniques, Azure Analytics, Azure AI ML Services, Azure Stream Analytics, data pipelines, R, Event Hub, AWS Lake Formation, Hadoop, Erwin, Power Bi, Azure Databricks, Data Warehousing, PostgreSQL, Azure DevOps, SQL Server, Data Modeling, Data Governance, Ansible, AWS, Oracle, Kubernetes, Python, Azure Synapse, Docker, Azure Data Lake, Microsoft Azure, Google Cloud Platform, Spark"
Principal Data Architect,JPMorganChase,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nYour unmatched expertise and unrelenting quest for outcomes are the driving forces for transformation that inspire high-quality solutions. You are a crucial member of a diverse team of thought leaders committed to leaving a positive mark on the industry.\n\nAs a Principal Data Architect at JPMorgan Chase within the Consumer and Community Banking, you will provide expertise to enhance and develop data architecture platforms based on modern cloud-based technologies as well as support the adoption of strategic global solutions. You will leverage your advanced data architecture capabilities and collaborate with colleagues across the organization to drive best-in-class outcomes to achieve the target state architecture goals.\n\nJob Responsibilities\n\nAdvises cross-functional teams on data architecture solutions to achieve the target state architecture and improve current technologies.\nLead the design, implementation, and maintenance of scalable, high-performance data architectures, including data lakes, data warehouses, and data integration solutions.\nCollaborate with cross-functional teams, including IT, business units, and analytics teams, to ensure data architecture supports business needs and enables data-driven decision-making.\nDrive the adoption of emerging data technologies and methodologies to enhance data capabilities and improve efficiency\nDevelops multi-year roadmaps aligned with business and data architecture strategy and priorities\nCreates complex and scalable data frameworks using appropriate software design\nReviews and debugs code written by others to deliver secure, high-quality production code\nServes as the function's go-to subject matter expert\nContributes to the development of technical methods in specialized fields in line with the latest product development methodologies\nCreates durable, reusable data frameworks using new technology to meet the needs of the business\nChampions the firm's culture of diversity, equity, inclusion, and respect. Mentors and coaches junior architects and technologists\n\nRequired Qualifications, Capabilities, And Skills\n\nFormal training or certification on data management concepts and 10+ years applied experience. In addition, 5+ years of experience leading technologists to manage, anticipate and solve complex technical items within your domain of expertise.\nHands-on practical experience delivering system design, application development, testing, and operational stability\nExpert in one or more architecture disciplines and programming languages\nDeep knowledge of data architecture, best practices, and industry trends\nExperience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud) and big data technologies (e.g., Hadoop, Spark).\nAdvanced knowledge of application development and technical processes with considerable in-depth knowledge in one or more technical disciplines (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)\nExperience applying expertise and new methods to determine solutions for complex architecture problems in one or more technical disciplines\nAbility to present and effectively communicate with Senior Leaders and Executives\n\nAbout Us\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We're proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions all while ranking first in customer satisfaction.","cloud-based technologies, data integration solutions, Mobile, data lakes, Hadoop, Google Cloud, Machine Learning, data warehouses, AWS, Data Architecture, Azure, Application Development, Artificial Intelligence, Spark"
Big Data Architect,Airtel Digital,8-17 Years,,"Pune, India",Login to check your skill match score,"Key Responsibilities\nDesign, build, and maintain scalable big data architectures on Azure and AWS - Select and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )\nLead data migration from legacy systems to cloud-based solutions - Develop and optimize ETL pipelines and data processing workflows.\nEnsure data infrastructure meets performance, scalability, and security requirements.\nCollaborate with development teams to implement microservices and backend solutions for big data applications.\nOversee the end-to-end SDLC for big data projects, from planning to deployment.\nMentor junior engineers and contribute to architectural best practices.\nPrepare architecture documentation and technical reports.\nRequired Skills & Qualifications\nBachelor's/Master's degree in Computer Science, Engineering, or related field.\n817 years of experience in big data and cloud architecture.\nProven hands-on expertise with Azure and AWS big data services (e.g., Azure Synapse, AWS Redshift, S3, Glue, Data Factory).\nStrong programming skills in Python, Java, or Scala[9].\nSolid understanding of SDLC and agile methodologies.\nExperience in designing and deploying microservices, preferably for backend data systems.\nKnowledge of data storage, database management (relational and NoSQL), and data security best practices\nExcellent problem-solving, communication, and team leadership skills","Glue, Java, S3, Aws Redshift, Hadoop, Scala, Kafka, Microservices, Nosql, Azure Synapse, Azure Data Factory, Database Management, Spark, Data Security, Azure, Python, AWS"
Data Architect,NTT DATA Global Delivery Services Limited,1-5 Years,,Pune,Information Technology,"NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Architect to join our team in Pune, Mahrshtra (IN-MH), India (IN).\nJob Duties: Role: Data Architect\nJob Description:\nWork as Data architect/Senior data engineer to design and develop cost effective and reliable data solutions on any cloud platform like Azure or AWS.\nShould be able to understand client requirements and convert them into technical solution leveraging cloud capabilities and modern technologies.\nShould be able to contribute into company's internal innovation projects by conducting proof of concepts and developing frameworks using state of the art technologies.\nShould have prior experience in developing data pipelines using Pyspark, SQL and Python\nShould have good understanding of Snowflake and Azure cloud services like Synapse, Azure Databricks, Azure Data Factory, ADLS etc.\nShould have prior understanding of applying ETL and ELT concepts and principals especially when migrating data from any legacy system to modern cloud platform.\nAny experience in real time data processing using PySpark, Python and Kafka is an advantage.\nWork as individual contributor and spend 70% of the time writing code in different languages, frameworks, and technology stacks.\nShould be familiar with emerging technologies like GenAI, Snowflake Cortex and Databricks\nMinimum Skills Required: Mandatory skills: Any cloud experience (mainly Azure/AWS), SQL, Python, Pyspark, Snowflake\nGood to have: AI/ML, GenAI, Databricks","Data Pipeline, Azure, Aws"
Data Architect,HDFC,12-17 Years,,"Gurugram, Bengaluru, Mumbai",Banking,"Responsible for building data lake, data foundation and analytical solution with standard design and modern cloud/hybrid architecture pattern\nWork with cross functional teams to make the data usable for functional users and applications to enable delivery of business values to customers\nProvide architectural leadership and vision for Bank's Next Gen Data Platform and Data Lakehouse\nDevelop and maintain architectural roadmap for data products and data services plus ensuring alignment with the business and enterprise architecture strategies and standards\nDrive design and architecture for Data Transformation & aggregations, design and development of a roadmap, and implementation based upon current vs. future state in a cohesive architecture viewpoint\nReview and understand business requirements and technical designs for physical data design, data pipelines (ETL) and other technical integrations\nBuild data pipelines for multiple storage solutions, including distributed platforms such as Databricks, Trino, Hadoop and MPP databases and cloud Data warehouse\nDesign and implement low latency analytical platform services leveraging open source and cloud technology\nDesign and implement data governance and MDM tools\nDesigning and building data capabilities to support cloud data strategy, including fully automated data Pipelines, data curation and consumption\nExperience:\nExperienced technology leader with a minimum of 12+ years of software development experience including 10+ years of data application or data platform architecture experience with deep technology expertise\nDeep knowledge and hands on experience in design and implementation of Big Data technologies (Apache Spark, Apache Airflow, Apache Flink, Streaming data, ADLS, S3, Object Data store, Trino, Databricks, Unity Catalog, Data governance tools) and familiarity with data architecture patterns (data warehouse, data lake, data lakehouse, data ingestion, curation and consumption)","Apache Airflow, Apache, Azure"
Data Architect,NTT DATA Global Delivery Services Limited,3-6 Years,,Pune,"IT Management, Consulting","Job Duties: Role: Data Architect\nJob Description:\nWork as Data architect/Senior data engineer to design and develop cost effective and reliable data solutions on any cloud platform like Azure or AWS.\nShould be able to understand client requirements and convert them into technical solution leveraging cloud capabilities and modern technologies.\nShould be able to contribute into company's internal innovation projects by conducting proof of concepts and developing frameworks using state of the art technologies.\nShould have prior experience in developing data pipelines using Pyspark, SQL and Python\nShould have good understanding of Snowflake and Azure cloud services like Synapse, Azure Databricks, Azure Data Factory, ADLS etc.\nShould have prior understanding of applying ETL and ELT concepts and principals especially when migrating data from any legacy system to modern cloud platform.\nAny experience in real time data processing using PySpark, Python and Kafka is an advantage.\nWork as individual contributor and spend 70% of the time writing code in different languages, frameworks, and technology stacks.\nShould be familiar with emerging technologies like GenAI, Snowflake Cortex and Databricks\nMinimum Skills Required: Mandatory skills: Any cloud experience (mainly Azure/AWS), SQL, Python, Pyspark, Snowflake\nGood to have: AI/ML, GenAI, Databricks","Pyspark, Sql, Python"
Data Architect,Adobe,10-12 Years,,Bengaluru,Software,"Data Architect AEP Competency\nPosition Summary\nExperienced data modelers, SQL, ETL, with some development background to provide defining new data schemas, data ingestion for Adobe Experience Platform customers. Interface directly with enterprise customers and collaborate with internal teams.\nWhat you'll do\nInterface with Adobe customers to gather requirements, design solutions & make recommendations\nLead customer project conference calls or interface with a Project Manager\nDeliver Technical Specifications documents for customer review\nStrong collaboration with team software engineer consultants onshore & offshore\nLeverage understanding of data relationships and schemas to structure data to allow clients to perform dynamic customer-level analysis\nConstruct processes to build Customer ID mapping files for use in building 360 degree view of customer across data sources.\nLeverage scripting languages to automate key processes governing data movement, cleansing, and processing activities\nBill & forecast time toward customer projects\nInnovate on new ideas to solve customer needs\nRequirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process design, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nExperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects\nStrong verbal & written communication skills to interface with Sales team & lead customers to successful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field\nSpecial Consideration given for\nExperience & knowledge with Adobe Experience Cloud solutions\nExperience & knowledge with Digital Analytics or Digital Marketing\nExperience in programming languages (Python, Java, or Bash scripting)\nExperience with Big Data technologies (i.e., Hadoop, Spark, Redshift, Snowflake, Hive, Pig etc.)\nExperience as an enterprise technical or engineer consultant","Java, Hive, Hadoop, Redshift, Python"
Data Architect,Wipro Limited,11-13 Years,,"Gurugram, Gurugram",IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\nJob Description\nRole:\nService Desk Manager\nBand C1 Role (Data Architect)\nLocation Chennai, Noida\nTotal exp 11+ Years\nThe candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.\nMust have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop\nMust have hands on in writing complex use case driven SQLs\nShould have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.\nShould have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3\nShould have been involved in On-Prem to Cloud Migration process.\nShould have good knowledge with HIVE / Spark / Scala scripts\nShould have good knowledge on Unix Shell scripting\nShould be flexible to overlap US business hours\nShould be able to drive technical design on Cloud applications\nShould be able to guide & drive the team members for cloud implementations\nShould be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.\nAWS Certified applicants preferable\nCompetencies\nClient Centricity\nExecution Excellence\nCollaborative Working\nProblem Solving & Decision Making\nEffective communication\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, S3, Sql, Unix Shell Scripting, Aws Cloud, Emr, Hadoop, Etl, Sqoop, Hive, Scala, Data Warehouse, Redshift, Spark"
Data Architect,Wipro Limited,11-13 Years,,Chennai,IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\nJob Description\nRole:\nService Desk Manager\n\nBand C1 Role (Data Architect)\nLocation Chennai, Noida\nTotal exp 11+ Years\nThe candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.\nMust have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop\nMust have hands on in writing complex use case driven SQLs\nShould have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.\nShould have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3\nShould have been involved in On-Prem to Cloud Migration process.\nShould have good knowledge with HIVE / Spark / Scala scripts\nShould have good knowledge on Unix Shell scripting\nShould be flexible to overlap US business hours\nShould be able to drive technical design on Cloud applications\nShould be able to guide & drive the team members for cloud implementations\nShould be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.\nAWS Certified applicants preferable\nCompetencies\nClient Centricity\nPassion for Results\nCollaborative Working\nProblem Solving & Decision Making\nEffective communication\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, On-Prem to Cloud Migration, Data Warehouse, Sql, S3, Unix Shell Scripting, Emr, Aws Cloud, Hadoop, Etl, Sqoop, Hive, Scala, Redshift, Spark"
Data Modelling Architect (CoE),NTT Data,10-12 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:312265\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\nWe are currently seeking a Data Modelling Architect (CoE) to join our team in Bangalore, Karntaka (IN-KA), India (IN).\nTitle: Data Modelling Architect (CoE)\nPosition Overview:\nWe are seeking a highly skilled and experienced Data Modelling Architect to join our dynamic team. The ideal candidate will have a strong background in full data modelling life cycle, e.g. design, implement, and maintain complex data models that align with organisational goals and industry standards. This role requires a deep understanding of data architecture, data modelling methodologies, and ideally in real-time data integrations. The successful candidate will collaborate with cross-functional teams to ensure optimal data structures that support business intelligence, analytics, and operational requirements. This role is primarily within the Centre of Excellence of the Data Products Factory, creating, assuring, and overseeing the implementation of data models within analytical and real-time streaming domains.\nKey Responsibilities\nDevelop conceptual, logical, and physical data models to support data analytics, streaming and data products implementation.\nDefine and maintain data architecture standards, principles, and best practices.\nEnsure data models are aligned with business requirements and scalable for future needs.\nWork closely with business stakeholders, data engineers, data solution architects, and data product teams to gather requirements and design solutions.\nProvide guidance on data integration, transformation, and migration strategies.\nEstablish and maintain enterprise data models, data dictionaries, metadata repositories, and data lineage documentation.\nEnsure data models comply with organisational policies and regulatory requirements.\nOptimise data products and their components for performance, scalability, and reliability.\nEvaluate and recommend data modelling tools and technologies.\nStay updated on industry trends and emerging technologies in data architecture.\nIdentify and resolve data inconsistencies, redundancies, and performance issues.\nProvide technical leadership in addressing complex data-related challenges.\nRequired Skills and Qualifications\n10+ years of experience in data architecture and modelling.\nProven experience in data modelling, data architecture, and data products design.\nProven experience and expertise in data modelling standards, techniques (e.g. dimensional model, 3NF, Vault 2.0)\nFamiliarity with both analytical and real-time/ streaming data solutions (Kafka/Airflow).\nHands-on experience with data modelling tools (e.g., Erwin, Lucidchart, SAP PowerDesigner).\nExpertise in Python and SQL (e.g., Snowflake, Kafka).\nExperience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway\nKnowledge of data warehouse design, ETL/ ELT processes, and big data technologies (e.g., Snowflake, Spark).\nFamiliarity with data governance and compliance frameworks (e.g., GDPR, HIPAA).\nStrong communication and stakeholder management skills.\nAnalytical mindset with attention to detail.\nAbility to lead and mentor teams on best practices in data modelling.\nEducation: Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.\nPreferred Skills and Qualifications\n- Certifications in data modelling, cloud platforms, or database technologies.\n- Experience in developing and implementing enterprise data models.\n- Experience with Interface/ API data modelling.\n- Experience with CI/CD GITHUB Actions (or similar)\n- AWS fundamentals (e.g., AWS Certified Data Engineer)\n- Knowledge of Snowflake/ SQL\n- Knowledge of Apache Airflow\n- Knowledge of DBT\n- Familiarity with Atlan for data catalog and metadata management\n- Understanding of iceberg tables\nAbout NTT DATA\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at\nNTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","snowflake, data warehouse design, EKS, 3NF, Airflow, SAP PowerDesigner, Lucidchart, data products design, Vault 2.0, AWS cloud services, dimensional model, Sql, Hipaa, Lambda, Api Gateway, Erwin, Kafka, Etl, Data Architecture, S3, ELT, Big Data Technologies, Gdpr, Data Governance, Sns, Python, Data Modelling, Spark"
Network Data Architect,Aspire Systems India Private Limited,12-16 Years,,Chennai,Software,"Minimum 12+ years of relevant hands on experience in configuring and troubleshooting Network devices, Cisco routers, ACI, Switches, Firewalls and F5 Load Balancers.\nAbility to troubleshoot and upgrade Cisco routers, Switches, Firewalls and F5 Load Balancers.\nMust be skilled to perform router/switch network hardware/software upgrades.\nMust have hands on experience in Routing Protocol Development (BGP, OSPF, VPN and Static.), providing IP Addressing Strategy - NAT, Re-Addressing, WAN - IP Routed Network Integration Wireless LAN technologies.\nExperience with Cisco LAN routing, switching, Security, Voice, and Wireless LAN products.\nExperience in network protocols VPN, OSPF, BGP, TCP/IP.\nExperience in Cisco nexus and Datacenter support.\nExperience and hands on with Cisco Client/ Cisco Prime/ Public DNS services.\nExperience in providing Network Engineering services in support of the large enterprise customer data networks.\nConfigure, administer firewall infrastructure, working with Cisco and Palo alto.\nWork with customer technical teams to provide solutions for customers internetworking communications requirements.\nExperience in proposal presentation and participating in Pre-sales engagement is an added advantage.\nProvide valuable solutions for colleagues and stakeholders, both tactically and strategically and ensure compliance to process, procedure, and tools within the operations.\nExcellent written and verbal communications skills are essential.\nProven analytic skills and the ability to isolate and resolve complex issues.\nGood understanding of Migration Effort, Resources and Timelines estimation.\nDesign public cloud infrastructure and DevOps solutions, architectures and roadmaps.\nEvolve and build best practice materials for Infrastructure-as-Code and Configuration Management.\nEnable sales including knowledge transfer, architecture and design, as well as participate in the sales cycle as needed.\nParticipate in community and market activities including participation in trade shows.\nCreate, build and grow partnerships with various organizations relevant to the practice,\nEnable development teams to leverage cloud and cloud-native architectures with new and existing applications.","Network Data Architect, Tcp/ip, BGP, OSPF"
Senior Data Architect,Adobe,10-12 Years,,Noida,Software,"Position Summary\nExperienced data modelers, SQL, ETL, with some development background to provide defining new data schemas, data ingestion for Adobe Experience Platform customers. Interface directly with enterprise customers and collaborate with internal teams.\nWhat you'll do\nInterface with Adobe customers to gather requirements, design solutions & make recommendations\nLead customer project conference calls or interface with a Project Manager\nDeliver Technical Specifications documents for customer review\nStrong collaboration with team software engineer consultants onshore & offshore\nLeverage understanding of data relationships and schemas to structure data to allow clients to perform dynamic customer-level analysis\nConstruct processes to build Customer ID mapping files for use in building 360 degree view of customer across data sources.\nLeverage scripting languages to automate key processes governing data movement, cleansing, and processing activities\nBill & forecast time toward customer projects\nInnovate on new ideas to solve customer needs\nRequirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience\nExperience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process design, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI)\nExperience in professional software development\nDemonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects\nStrong verbal & written communication skills to interface with Sales team & lead customers to successful outcome\nMust be self-managed, proactive and customer focused\nDegree in Computer Science, Information Systems, Data Science, or related field\nSpecial Consideration given for\nExperience & knowledge with Adobe Experience Cloud solutions\nExperience & knowledge with Digital Analytics or Digital Marketing\nExperience in programming languages (Python, Java, or Bash scripting)\nExperience with Big Data technologies (i.e., Hadoop, Spark, Redshift, Snowflake, Hive, Pig etc.)\nExperience as an enterprise technical or engineer consultant","data ingestion, Big Data Technologies, Data Warehousing, Sql, Etl"
Cloud Data Architect,Birlasoft Limited,13-23 Years,,Pune,Consulting,"About Birlasoft:\nBirlasoft, a powerhouse where domain expertise, enterprise solutions, and digital technologies converge to redefine business processes. We take pride in our consultative and design thinking approach, driving societal progress by enabling our customers to run businesses with unmatched efficiency and innovation. As part of the CK Birla Group, a multibillion-dollar enterprise, we boast a 12,500+ professional team committed to upholding the Group's 162-year legacy. Our core values prioritize Diversity, Equity, and Inclusion (DEI) initiatives, along with Corporate Sustainable Responsibility (CSR) activities, demonstrating our dedication to building inclusive and sustainable communities. Join us in shaping a future where technology seamlessly aligns with purpose.\nAbout the Job:\nBirlasoft is seeking a visionary Senior/Lead Cloud and Data Architect with proven leadership capabilities to drive enterprise-level data solutions. The ideal candidate will have expertise in cloud platforms (AWS, Azure, or GCP), mandatory experience in Databricks or Snowflake, and a strong foundation in data warehousing, data modelling, DevOps, and DataOps. Additionally, the role demands a strategic leader with an understanding of machine learning (ML) concepts and the ability to function as an Enterprise Architect, providing guidance and alignment across diverse teams and initiatives.\nTitle: Senior/Lead Cloud and Data Architect\nLocation:Pune/Mumbai\nEducational Background:Any Graduation\nKey Responsibilities:\nLeadership and Collaboration:\nLead and inspire cross-functional teams, including data engineers, data scientists, ML engineers, and BI analysts, fostering a culture of collaboration, innovation, and continuous improvement.\nAct as the central technical point of contact for enterprise-wide data and analytics initiatives, ensuring alignment with business objectives.\nCollaborate with senior leadership and business stakeholders to define and deliver on data strategy, ensuring scalability and alignment with enterprise goals.\nMentor and upskill team members, providing guidance on best practices, emerging technologies, and professional development.\nEnterprise Architecture:\nDevelop and maintain enterprise-level architectural blueprints for cloud and data platforms, ensuring interoperability and scalability.\nCreate a cohesive architecture that integrates data platforms, ML systems, and business intelligence tools while adhering to governance and compliance requirements.\nProvide strategic input on technology roadmaps, ensuring alignment with organizational vision and future-proofing investments.\nEvaluate and recommend emerging technologies and frameworks to enhance enterprise data capabilities.\nCloud and Data Platform Engineering:\nDesign and implement cutting-edge cloud architectures using AWS, Azure, or GCP, focusing on scalability, security, and cost optimization.\nBuild and optimize modern data platforms using Databricks or Snowflake for advanced analytics and real-time data processing.\nLead the development of end-to-end data pipelines, ensuring robust integration between data sources, platforms, and analytics tools.\nData and ML Integration:\nCollaborate with ML teams to deploy machine learning pipelines and operationalize AI models within enterprise systems.\nProvide architectural support for ML initiatives, including data preparation, feature engineering, and model lifecycle management.\nEnable seamless integration of ML and analytics into business workflows, enhancing decision-making and operational efficiency.\nData Warehousing and Modeling:\nArchitect enterprise data warehouses with robust data models, ensuring high performance and reliability for analytics workloads.\nLead the development of advanced data models that cater to both analytical and operational requirements, emphasizing scalability and data quality.\nDevOps and DataOps Practices:\nEstablish and enforce DevOps and DataOps pipelines to automate deployments, enhance agility, and ensure operational excellence.\nDrive initiatives for continuous improvement in data delivery, ensuring high availability, data quality, and scalability.\nGovernance, Security, and Compliance:\nDefine and enforce data governance policies, including data security, lineage, and compliance with industry regulations.\nImplement robust security measures to protect sensitive data across platforms, adhering to privacy standards such as GDPR and CCPA.\nSkill Required:\n10+ years in data architecture, cloud platforms, and advanced analytics.\nHands-on experience with at least two cloud platforms (AWS, Azure, or GCP) is mandatory.\nExpertise in Databricks or Snowflake is required.\nComprehensive understanding of data warehousing, data modelling, and data integration.\nExperience implementing ML pipelines or integrating ML solutions with enterprise systems.\nProven track record in enterprise architecture, aligning technology solutions with business goals.\nPreferred Qualifications:\nCertifications in AWS, Azure, GCP, Databricks, or Snowflake.\nExperience working in enterprise-scale environments with complex data landscapes.\nExposure to industry-specific data challenges in domains like finance, insurance, or healthcare.\nAdditional Requirements\nThis role offers the opportunity to lead transformative data initiatives at an enterprise level, combining cutting-edge cloud and data technologies with leadership and innovation. If you have a passion for building scalable data solutions and leading diverse teams to success, we encourage you to apply.","Cloud Analytics, Big Data, Azure, Cloud Architect"
Data Warehouse Architect,Icici Bank Limited,10-12 Years,,Hyderabad,Banking,"Key Responsibilities:\nData Pipeline Design: Responsible for designing and developing ETL data pipelines that can help in organising large volumes of data. Use of data warehousing technologies to ensure that the data warehouse is efficient, scalable, and secure.\nIssue Management: Responsible for ensuring that the data warehouse is running smoothly. Monitor system performance, diagnose and troubleshoot issues, and make necessary changes to optimize system performance.\nCollaboration: Collaborate with cross-functional teams to implement upgrades, migrations and continuous improvements.\nData Integration and Processing: Responsible for processing, cleaning, and integrating large data sets from various sources to ensure that the data is accurate, complete, and consistent.\nData Modelling: Responsible for designing and implementing data modelling solutions to ensure that the organization's data is properly structured and organized for analysis.\nKey Qualifications & Skills:\nEducation Qualification: B.E./B. Tech. in Computer Science, Information Technology or equivalent domain with 10 to 12 years of experience and at least 5 years or relevant work experience in Datawarehouse/mining/BI/MIS.\nExperience in Data Warehousing: Knowledge on ETL and data technologies like OLTP, OLAP (Oracle / MSSQL). Data Modelling, Data Analysis and Visualization experience (Analytical tools experience like Power BI / SAS / ClickView / Tableau etc.). Good to have exposure to Azure Cloud Data platform services like COSMOS, Azure Data Lake, Azure Synapse, and Azure Data factory.\nCertification: Azure certified DP 900, PL 300, DP 203 or any other Data platform/Data Analyst certifications.","Data Analysis, Data Warehousing, Data Modelling, Etl"
Data Engineer Architect,TechnoGen,10-19 Years,,Hyderabad,"Consulting, Information Services","Basic Qualifications\nBachelors degree in computer science, engineering or a related field\nData: 8+ years of experience with data analytics and warehousing inInvestment& Finance Domain\nSQL: Deep knowledge of SQL and query optimization\nELT: Good understanding of ELT methodologies and tools\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers\n3+ years of coding and scripting (Python, Java, Scala) and design experience.","Java, Scala, Data Analytics, Sql, Python, data engineering, Data Architecture"
Data Architect,Spectral Consultants,10-13 Years,,Chennai,Consulting,"10-13 years of experience in Business Intelligence application development and support\nHands-on experience on the data pipeline setup, data management, importing and cleansing data, preparing enterprise wide data catalog and data mining\nWorked on data analysis methodologies and modern data warehouse implementation to capture analytics and metrics\nExposure to ETL process, data visualization tools and statutory report development\nExperience on Implementation with large scale data sets, handling XML, JSON structures\nStrong program/project management skills from initiation to implementation in an enterprise\nTrack record of implementing innovative solutions to address business challenges\nStrong exposure to Microsoft Azure and Google native Cloud services for Big data pipeline implementation, data processing, data transformation etc.\nStrong business analytical and communication skills\nGood influential skills and ability to operate in a fast-paced global environment with urgency, ownership, and accountability\nStrong oral and written skills\nStrong knowledge of data warehouse concepts\nKnowledge of SAP\nKnowledge of Microsoft Business Intelligence stack\nExperience with Big data concepts a plus\nEducation:\nBE Degree in Information Systems, Computer Science or related technical discipline or equivalent","Data Information Architecture, Data Pipeline, Data Governance, Azure, DataFlow"
Data Architect,Right Advisors Private Limited,10-14 Years,,Bengaluru,Software,"Requirements\n10+ years of strong experience with data transformation & ETL on large data sets\nExperience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale etc.)\n5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)\n5+ years of complex SQL or NoSQL experience Experience in advanced Data Warehouse concepts\nExperience in industry ETL tools (i.e., Informatica, Unifi)\nExperience with Business Requirements definition and management, structured analysis, process design, use case documentation\nExperience with Reporting Technologies (i.e., Tableau, PowerBI) Experience in professional software development Demonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects\nStrong verbal & written communication skills to interface with Sales team & lead customers to successful outcome Must be self-managed, proactive and customer focused Degree in Computer Science, Information Systems, Data Science, or related field","Data Modelling, Power Bi, Sql, Etl, Data Transformation"
Data Architect,Gainwell Technologies LLC,8-12 Years,,Bengaluru,IT Management,"Job description\nSummary\nWere looking for a Solution Architect to join our product development team to drive a cloud based data platform architecture that will power BI and advanced analytics at Gainwell Technologies. You will have a direct impact onthe design, architecture, and implementation of BI and data science use cases. The ideal candidate will have industry-leading programming skills and established knowledge of implementing, designing, deploying, and maintaining big data analytics platforms in a cloud environment.\nThe solution architect will use knowledge of healthcare data to influence the implementation and governance of our data architecture. Your designs will account for data movement, storage, compute and BI consumption, and will comply with security and data governance standards. You will work closely with a variety of partners within the Data and Analytics organization.\nYour role in our mission\nProvide thought leadership and technical direction to the data engineering team in building analytic data products\nUnderstand and translate business requirements to data strategies that align with overall technology vision\nDesign, develop and enforce standards for the data storage, processing and governance across all environments\nWork closely with enterprise and application architects to align the data engineering team to the overall company SDLC standards, practices, and data access patterns\nDevelop, document and maintain overall view of data platform architecture, data acquisition, data quality, and data retention\nProvide formal and informal training for data engineers, platform engineers and ETL developers\nMaintain knowledge of emerging technologies and architectures\nDocument and publish technical principles and standards, and mentor the data engineering team to incorporate them into their daily practices\nChampion and present the technical vision to the executive team and business stakeholders\nWhat we're looking for\nBasic Qualifications\nBachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields\n8+ years of overall experience in big data, database and enterprise data architecture and delivery\n8+ years of programming proficiency in a subset of Python, Java, and Scala\n5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks\n5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms\n3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred\nStrong SQL skills to create/maintain DB objects, query/load required data using data governance (e.g. business glossary, data dictionary, data catalog, data quality, master data management, etc.) and visualization tools to bring data literacy to the organization.\nPractical experience on workload management, monitoring, and performance tuning Apache Spark jobs\nBroad knowledge of data technologies, tools, and disciplines including data modeling, dimensional models, third-normal-form structures, ETL/ELT, change data capture and slowly changing dimensions\nExperience with healthcare data a big plus\nExperience with Machine Learning & MLOPs is a big plus","Java, Scala, Aws"
Data Architect,Citiustech Healthcare Technology Private Limited,5-10 Years,,"Mumbai City, Bengaluru, Mumbai",Health Care,"We are looking for a data architect with below skillset\nHands on skills withImage data (preferably DICOM) parsing\nArchitect and implement data warehousing solutions using AWS (Redshift, S3, Lambda)\nDevelop and maintain ETL pipelines using Informatica PowerCenter or AWS Glue\nCollaborate with cross-functional teams to identify and prioritize data requirements\nAnalyze complex data sets using SQL, Python, and Java and or/Apex languageto inform business decisions\nImplement data visualization tools (Tableau, Power BI) for stakeholder reporting\nEnsure data integrity, security, and compliance with HIPAA, GDPR, and CCPA\nRequirements:> 5 years of experience in data management with focus in image metadata/bigdata\nStrong expertise in:\nPACS& RIS, SYNAPSE/XNAT, systems\nAWS (Redshift, S3, Lambda, Glue)\nData governance, quality, and security\nETL pipelines (Informatica PowerCenter or AWS Glue)\nData warehousing and visualization","Data Warehousing, Tableau, Python, AWS"
Data Architect,Ifintalent Global Private Limited,5-10 Years,,Mumbai,Financial Services,"Job Description\nDirect Responsibilities\nEngage with key business stakeholders to assist with establishing fundamental data governance processes\nDefine key data quality metrics and indicators and facilitate the development and implementation of supporting standards\nHelp to identify and deploy enterprise data best practices such as data scoping, metadata standardization, data lineage, data deduplication, mapping and transformation and business validation\nStructures the information in the Information System (any data modelling tool like Abacus), i.e. the way information is grouped, as well as the navigation methods and the terminology used within the Information Systems of the entity, as defined by the lead data architects.\nCreates and manages data models (Business Flows of Personal Data with process involved) in all their forms, including conceptual models, functional database designs, message models and others in compliance with the data framework policy\nAllows people to step logically through the Information System (be able to train them to use tools like Abacus)\nContribute and enrich the Data Architecture framework through the material collected during analysis, projects and IT validations Update all records in Abacus collected from stakeholder interviews/ meetings.\nSkill Area\nCommunicating between the technical and the non-technical\nIs able to communicate effectively across organisational, technical and political boundaries, understanding the context. Makes complex and technical information and language simple and accessible for non- technical audiences. Is able to advocate and communicate what a team does to create trust and authenticity, and can respond to challenge.\nData Modelling (Business Flows of Data in Abacus)\nProduces data models and understands where to use different types of data models. Understands different tools and is able to compare between different data models.\nAble to reverse engineer a data model from a live system. Understands industry recognized data modelling patterns and standards.\nData Standards (Rules defined to manage/ maintain Data)\nDevelops and sets data standards for an organisation.\nCommunicates the business benefit of data standards, championing and governing those standards across the organisation.\nMetadata Management\nUnderstands a variety of metadata management tools. Designs and maintains the appropriate metadata repositories to enable the organization to understand their data assets.\nTurning business problems into data design\nWorks with business and technology stakeholders to translate business problems into data designs. Creates optimal designs through iterative processes, aligning user needs with organisational objectives and system requirements.","Data Process, Data Architect, Data Governance"
Data Architect,Paypal,10-15 Years,,Bengaluru,Financial Services,"PayPal Data Architecture team seeks a highly skilled Senior Data Architect to lead the modernization of our applications towards a Kappa or Lambda architecture. The ideal candidate will have a deep understanding of data modeling, domain-driven design, and CQRS, as well as experience with real-time data processing, analytics and distributed systems.Job Description:\nMeet our team : PayPal Data Architecture team is on a mission to build a robust and scalable data platform that empowers our customers. We're looking for a passionate Data Architect to join us in designing and implementing secure, high-performing data solutions that leverage a variety of technologies.\nWhat you need to know about the role:- In this role, you'll be a trusted advisor to various PayPal business units, partnering with their product engineering teams to architect data solutions that fuel high-priority initiatives. You'll be embedded with these teams, playing a key role throughout the entire data lifecycle.\nYour day to day:\nDevelop and implement a long-term data strategy aligned with business objectives, focusing on event-driven architectures and real-time analytics.\nDemonstrate deep expertise in Kappa and Lambda architectures, data modeling, domain-driven design, CQRS, and stream processing frameworks.\nAddress complex data challenges, such as large-scale data integration, real-time analytics, and data governance.\nDesign and model data structures, schemas, and relationships to optimize performance and scalability.\nLead the transition of existing applications to event-driven architectures, leveraging event modeling, domain-driven design, and CQRS.\nCollaborate with development teams to ensure seamless integration of data models and architectures.\nMentor and guide junior data professionals, fostering a culture of innovation and excellence.\nStay updated on emerging technologies and evaluate their suitability for our organization.\nYour way to impact:-\nHybrid Data Architecture Expertise. Craft solutions that seamlessly integrate on-premises, cloud-based, and hybrid data storage options, ensuring optimal data management across the organization.\nCompliance & Governance. Ensure your data architectures and designs adhere to PayPal's specific enterprise data architecture standards for each data store type.\nData Access Pattern & Structure Design. Design data structures and access patterns that meet both functional and non-functional requirements.\nFunctional. Align with the specific business needs of the project.\nNon-Functional. Prioritize security, availability, performance, scalability to create a robust and user-friendly data platform.\nAs a Customer Champion, you'll collaborate with domain architects to identify opportunities for data platform improvements.\nData Security. Enhance data security posture by minimizing risks and vulnerabilities.\nFault Tolerance. Design data solutions to be highly available and resilient to failures.\nScalability. Ensure solutions can accommodate future growth.\nQuery Performance. Optimize data structures and access patterns for faster queries, ultimately improving the customer experience.\nDrive Improvement Implementation. Not only identify improvement opportunities but also actively work towards their implementation.\nWhat do you need to bring-\n10+ years of experience as a Data Architect or a similar role.\nProven track record of leading successful data modernization projects.\nDeep understanding of data modeling techniques.\nExpertise in domain-driven design principles and their application to data modeling.\nProficiency in Kappa and Lambda architectures, including their benefits, challenges, and best practices.\nStrong experience with stream processing frameworks (e.g., Apache Flink, Apache Spark Streaming).\nKnowledge of data lake and data warehouse concepts.\nExperience with cloud platforms (e.g., AWS, Azure, GCP) and their data services.\nCertifications related to data architecture, cloud platforms, or data analytics.\nExcellent communication and collaboration skills.","stream processing, Data Modeling, Data Architect, cloud platform"
Data Architect,STATS PERFORM,16-20 Years,,Hyderabad,Fitness,"DAZN Group is looking for Data Architect to join our dynamic team and embark on a rewarding career journey A Data Architect is a professional who is responsible for designing, building, and maintaining an organization's data architecture\nDesigning and implementing data models, data integration solutions, and data management systems that ensure data accuracy, consistency, and security\nDeveloping and maintaining data dictionaries, metadata, and data lineage documents to ensure data governance and compliance\nData Architect should have a strong technical background in data architecture and management, as well as excellent communication skills\nStrong problem-solving skills and the ability to think critically are also essential to identify and implement solutions to complex data issues","Data Management, Data Architecture, Data Integration"
Data Architect,Aeries Technology,10-15 Years,,Bengaluru,Information Technology,"Job description\nWe are looking for a Senior Data Engineer with an architect s mindset and thought leadership approach on the Data & Analytics team. This role goes beyond development its about end-to-end thinking, mentoring and collaborating with a global team.\nAs a key technical leader, you will align data strategies with business needs, and drive best practices in data modeling, governance, and performance optimization. you will play a pivotal role in streamlining tooling for Democratized Development within Snowflake, DBT related data platforms and build high impact data models.\nYou will drive performance optimization, cost efficiency, data governance, and model architecture, ensuring that our data infrastructure is scalable, secure, and high-performing.\nThis role requires deep expertise in cloud-based data engineering, a strong problem-solving mindset, and the ability to collaborate with business and analytics teams . You will work on optimizing compute resources, improving data quality, and enforcing governance standards , support code reviews, and mentor team members, fostering a high-performing data engineering culture.\nKey Responsibilities\nData Acquisition & Pipeline Development\nDevelop and maintain scalable data pipelines for efficient data ingestion, transformation, and integration.\nWork with Fivetran, Python, and other ETL/ELT tools to automate and optimize data acquisition from various sources.\nEnsure reliable data movement from SaaS platforms (eg, Salesforce, Gong, Google Analytics) and operational databases into Snowflake.\nMonitor and enhance pipeline performance, identifying areas for optimization and fault tolerance.\nEvaluate and recommend new technologies for data ingestion, transformation, and orchestration.\nDemocratized Development & Tooling\nEnable and streamline self-service data development for analysts and data practitioners.\nDrive best practices in DBT and Snowflake for modular, reusable, and governed data modeling.\nDesign and maintain CI/CD pipelines to support version control, testing, and deployment in a modern data stack.\nPerformance Optimization & Cost Efficiency\nOptimize and tune Snowflake queries and workloads for performance and cost efficiency.\nImplement warehouse resource scaling strategies to reduce compute costs while maintaining SLAs.\nMonitor and analyze query performance, storage consumption, and data usage patterns to identify optimization opportunities.\nData Quality, Governance & Security\nEstablish data quality monitoring frameworks, integrating automated validation and anomaly detection.\nEnforce data governance policies, including access controls, lineage tracking, and compliance standards.\nWork closely with security teams to implement data protection strategies in Snowflake.\nData Model Development & Architecture Review\nDesign and develop scalable, well-structured data models in Snowflake.\nPerform data model reviews to ensure consistency, efficiency, and alignment with business needs.\nCollaborate with Analytics & BI teams to define metrics layers and transformation logic in DBT.\nCollaboration & Agile Execution\nCollaborate with BI teams to ensure data models meet reporting requirements in Power BI.\nPartner with cross-functional teams including Sales, Marketing, Customer Support, Finance, and Product to deliver trusted, high-quality data solutions.\nWork within an Agile framework, delivering iterative improvements to data infrastructure.\nStay ahead of industry trends in modern data engineering, Snowflake, and DBT to drive innovation.\nQualifications\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.\n5+ years of experience in Data Engineering with strong expertise in Snowflake.\n2+ years of hands-on experience in DBT for data modeling and transformation.\n2+ years of experience in Python, particularly for data pipelines and automation.\nStrong expertise in SQL optimization and performance tuning.\nDeep understanding of ETL/ELT architectures, data warehousing, and cloud data management best practices.\nExperience implementing cost monitoring and optimization techniques in Snowflake.\nStrong problem-solving skills and ability to troubleshoot complex data issues.\nExcellent communication skills and ability to work in collaborative, cross-functional teams.\nExperience with Agile methodologies and iterative data development processes is a plus.\nRole:Data Science & Machine Learning - Other\nIndustry Type:IT Services & Consulting\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Consultant, Automation, Information Technology, Performance Tuning"
Data Architect,GlobalLogic Inc,10-15 Years,,"Bengaluru, Noida",Software,"Description\nMandatory Skills: SQL or No SQL database\nExperience in any cloud (AWS or Azure)\nCan have : Any scripting language (Python, Java or others)\nRequirements\nProficient in database technologies (SQL, NoSQL, and data warehousing solutions).\nFamiliarity with cloud platforms (AWS, Azure, Google Cloud).\nExperience with data integration tools, ETL processes, and data modeling.\nKnowledge of programming/scripting languages like Python, Java, or Scala.\nExperience with big data technologies (Hadoop, Spark, Kafka) is a plus\nDesign and implement scalable and secure data architectures.\nDevelop and optimize data models for structured and unstructured data.\nDefine data governance policies and best practices.\nCollaborate with business and IT teams to understand data needs and implement solutions.\nSelect and implement appropriate database technologies (SQL, NoSQL, etc.).\nWork with ETL/ELT processes to integrate and transform data from multiple sources.\nEnsure data integrity, security, and compliance with regulations.\nImprove data accessibility and performance across systems.\nGuide development teams in best practices for data management and storage.\nJob responsibilities\nExpertise in SQL, NoSQL, and data modeling techniques.\nStrong knowledge of cloud platforms (AWS, Azure, GCP) and data services.\nExperience with ETL/ELT pipelines, data lakes, and data warehouses.\nUnderstanding of big data technologies (Hadoop, Spark, Kafka) is a plus.\nKnowledge of data security, compliance (GDPR, HIPAA, etc.), and governance.\nStrong problem-solving and analytical skills.\nExcellent communication and stakeholder management abilities.\nWhat we offer\nCulture of caring.At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you'll experience an inclusive culture of acceptance and belonging, where you'll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders.\nLearning and development.We are committed to your continuous learning and development. You'll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally.\nInteresting & meaningful work.GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you'll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what's possible and bring new solutions to market. In the process, you'll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today.\nBalance and flexibility.We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way!\nHigh-trust organization.We are a high-trust organization where integrity is key.By joining GlobalLogic, you're placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do.\nAbout GlobalLogic\nGlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world's largest and most forward-thinking companies. Since 2000, we've been at the forefront of the digital revolution helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.","Data Base, Programming Language, Ms Sql"
Data Architect,Citiustech Healthcare Technology Private Limited,10-15 Years,,"Mumbai City, Mumbai, Pune",Health Care,"Role & responsibilities\nDemonstrate problem-solving abilities and strategic thinking to drive continuous improvement in client system and applications Identify the improvement areas in application and provide the solution.\nCreation of designing and implementing effective documentation templates.\nMentor and develop new embers in accounts for, problem solving and customer centricity. Periodically review projects and Identify opportunities for process optimization, areas of improvement, and efficiency improvements within the systems\nStrategic Thinking\nUnderstanding of Agile/Scrum processes and capability to write User stories and acceptance criteria & work closely with the scrum teams at all levels\nEnsure compliance with HIPAA regulations and requirements.\nDemonstrate commitment to the Companys core values.\nExcellent communication skills\nGood to have skills Development/Data analyst:\nexperience SQL, PySpark, Databricks, delta lake, delta table, Azure data factory in SQL, PySpark Exposure on the CICD pipelines for the azure Databricks, delta lake, delta table, Azure data factory, Data Warehousing/data understanding","Databricks, Sql"
Data Architect I,Bottom Line,10-15 Years,,Bengaluru,Software,"Job description\nDesign and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.\nCollaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.\nHelp estimate the size, scope, and timeframes for deliverables.\nEvaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.\nDefine and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.\nDesign and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.\nDevelop and implement data governance policies and procedures to ensure data integrity and security.\nDevelop and maintain documentation for data architecture, standards, policies, and procedures.\nProvide technical guidance and mentorship to other members of the data and analytics team.\nStay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.\nRequirements\nBachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.\n10+ years of demonstrated experience as a Data Architect or similar role.\nStrong knowledge of data architecture principles, data modeling techniques, and best practices.\nProficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.\nHands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.\nFamiliarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).\nStrong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.\nExcellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.\nProven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.","data architecture principles, snowflake, data modeling techniques, SQL Server"
Data Architect I,Bottom Line,10-15 Years,,Kolkata,Software,"Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.\nCollaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.\nHelp estimate the size, scope, and timeframes for deliverables.\nEvaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.\nDefine and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.\nDesign and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.\nDevelop and implement data governance policies and procedures to ensure data integrity and security.\nDevelop and maintain documentation for data architecture, standards, policies, and procedures.\nProvide technical guidance and mentorship to other members of the data and analytics team.\nStay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.\nRequirements\nBachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.\n10+ years of demonstrated experience as a Data Architect or similar role.\nStrong knowledge of data architecture principles, data modeling techniques, and best practices.\nProficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.\nHands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.\nFamiliarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).\nStrong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.\nExcellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.\nProven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.\nRole:Data Science & Machine Learning - Other\nIndustry Type:Software Product\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Machine Learning, Data Management, My Sql, Data Architecture, Aws"
Data Architect I,Bottom Line,10-15 Years,,"Delhi, Mumbai, Pune",Software,"Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.\nCollaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.\nHelp estimate the size, scope, and timeframes for deliverables.\nEvaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.\nDefine and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.\nDesign and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.\nDevelop and implement data governance policies and procedures to ensure data integrity and security.\nDevelop and maintain documentation for data architecture, standards, policies, and procedures.\nProvide technical guidance and mentorship to other members of the data and analytics team.\nStay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.\nRequirements\nBachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.\n10+ years of demonstrated experience as a Data Architect or similar role.\nStrong knowledge of data architecture principles, data modeling techniques, and best practices.\nProficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.\nHands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.\nFamiliarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).\nStrong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.\nExcellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.\nProven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.\nRole:Data Science & Machine Learning - Other\nIndustry Type:Software Product\nDepartment:Data Science & Analytics\nEmployment Type:Full Time, Permanent\nRole Category:Data Science & Machine Learning\nEducation\nUG:Any Graduate\nPG:Any Postgraduate","Machine Learning, Data Management, My Sql, Data Architecture, Aws"
Data Architect I,Bottom Line,10-15 Years,,"Hyderabad, Chennai, Pune",Software,"Job description\nDesign and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.\nCollaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.\nHelp estimate the size, scope, and timeframes for deliverables.\nEvaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.\nDefine and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.\nDesign and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.\nDevelop and implement data governance policies and procedures to ensure data integrity and security.\nDevelop and maintain documentation for data architecture, standards, policies, and procedures.\nProvide technical guidance and mentorship to other members of the data and analytics team.\nStay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.\nRequirements\nBachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.\n10+ years of demonstrated experience as a Data Architect or similar role.\nStrong knowledge of data architecture principles, data modeling techniques, and best practices.\nProficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.\nHands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.\nFamiliarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).\nStrong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.\nExcellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.\nProven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.","data architecture principles, snowflake, data modeling techniques, SQL Server"
Data Architect I,Bottom Line,10-15 Years,,"Delhi, Kolkata, Mumbai",Software,"Job description\nDesign and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.\nCollaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.\nHelp estimate the size, scope, and timeframes for deliverables.\nEvaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.\nDefine and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.\nDesign and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.\nDevelop and implement data governance policies and procedures to ensure data integrity and security.\nDevelop and maintain documentation for data architecture, standards, policies, and procedures.\nProvide technical guidance and mentorship to other members of the data and analytics team.\nStay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.\nRequirements\nBachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.\n10+ years of demonstrated experience as a Data Architect or similar role.\nStrong knowledge of data architecture principles, data modeling techniques, and best practices.\nProficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.\nHands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.\nFamiliarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).\nExperience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).\nStrong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.\nExcellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.\nProven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.","data architecture principles, snowflake, data modeling techniques, SQL Server"
"Vice President, Data Tech Solutions Architect",Genpact,Fresher,,"Gurugram, Gurugram",IT/Computers - Hardware & Networking,"Ready to shape the future of work\n\n\n\nAt Genpact, we don&rsquot just adapt to change&mdashwe drive it. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.\n\n\n\nIf you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that&rsquos shaping the future, this is your moment.\n\n\n\nGenpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.\n\n\n\n\n\n\nInviting applications for the role of Vice President, Data Tech Solutions Architect!\n\n\n\nIn this role, we are seeking candidates who have deep technical knowledge and a strategic bent to leverage technology for better business outcomes for the Banking and Capital Markets vertical. The candidate must have detailed understanding of a modern Tech stack, hyperscale offerings, key digital technologies Generative AI, AI/ML, etc.\n\n\n\nRole will be part of the Global Banking and Capital Markets Solutions team, as the evangelist for Alliance Partner Technology, Data/ AI led transformation opportunities in all Solutions created for clients. The role will constantly identify solution gaps, understand emerging client needs and market trends, to create and own a mid to long term Data/ Tech solution roadmap. All of this will be done in close partnership with rest of Genpact ecosystem, i.e., Alliance Partners, Service Lines, Data and Tech Practices in order to deliver top notch solutions.\n\n\n\n\n\n\nResponsibilities\n\n\n\n\n\nProvide technical leadership to evaluate and adopt Data, Tech and AI solutions from Alliance Partners and Genpact offerings\n\n\n\n\n\n\n\nRun the solution management cadence, build and own a periodically refreshed roadmap of Data/ Tech new solutionsidentified and prioritized through Client and Sales feedback\n\n\n\n\n\n\n\nAim for unified solutioning through consolidated business needs to avoid multiple design and build efforts\n\n\n\n\n\n\n\nCollaborate with Alliance Partners and internal teams to create a Tech driven PoV for all solutions, facilitating higher conversions with Clients\n\n\n\n\n\n\n\nFully own first and second level solutioning, orchestrating initial conversations and engage more specialized Genpact teams and Alliance Partners for deeper solutioning\n\n\n\n\n\n\n\nProvide thought leadership to Lead Solution Architects on RFPs, with an objective to steer Tech and Data led transformation agenda\n\n\n\n\n\n\n\nBuild and maintain a comprehensive tech stack view for priority clients to facilitate proactive engagements\n\n\n\n\n\n\n\nCreate architectural artifacts for new solutions developed, and drive consistency to elevate the Tech quotient in Client engagements\n\n\n\n\n\nQualifications we seek in you!\n\n\n\nMinimum Qualifications\n\n\n\n\n\n\n\nBachelor's degree in Computer Science, Information Technology, or a related field.\n\n\n\n\n\n\n\nRelevant years of experience in IT architecture, with a focus on financial services.\n\n\n\n\n\n\n\nTechnical Skills:\n\n\n\n\n\n\n\nThe mandate for this role will be the same as rest of the Global Solutions team, to cover all geographies and to include all Banking and Capital Markets clients where we are responding to Request for Proposals or working on contract renewals or proactively stocking the shelf for targeted solutions.\n\n\n\n\n\n\n\nRequires strong organizational, technical and communication skills. Requires strong stakeholder management and ability to influence\n\n\n\n\n\n\n\nProficiency in designing IT solutions.\n\n\n\n\n\n\n\nExperience with cloud platforms (e.g., AWS, Azure, Google Cloud).\n\n\n\n\n\n\n\nExperience in Data Platforms, AI/ GenAI Solutions\n\n\n\n\n\n\n\nExperience in architecting and designing solutions using Microsoft Power Platform, including Power Apps, Power Automate, and Power BI.\n\n\n\n\n\n\n\nMust have a understanding of low-code/no-code environments and the ability to create scalable, secure and efficient solutions that align with business requirements and best practices.\n\n\n\n\n\n\n\nFamiliarity with DevOps practices and tools.\n\n\n\n\n\n\n\nKnowledge of concept and principles of agile methodology ability to apply appropriate agile approaches in the processes of software development and delivery.\n\n\n\n\n\n\n\nThe mandate for this role will be the same as rest of the Global Solutions team, to cover all geographies and to include all Banking and Capital Markets clients where we are responding to Request for Proposals or working on contract renewals or proactively stocking the shelf for targeted solutions.\n\n\n\n\n\n\n\nRequires strong organizational, technical and communication skills. Requires strong stakeholder management and ability to influence across multiple levels of leadership, while leveraging the different capabilities at Genpact to deliver maximum value for Clients. Must work well in a dynamic, complex environment and under deadline pressures.\n\n\n\n\n\nPreferred Qualifications/ Skills\n\n\n\n\n\nConsulting experience - demonstrated ability to convey value proposition, differentiators in a compelling manner to win new relationships/opportunities\n\n\n\n\n\n\n\nTransformation experience - demonstrated ability to design and execute operational transformation. Experience of running day to day operations would be an added advantage\n\n\n\n\n\n\n\n\nWhy join Genpact\n\n\n\n\n\nBe a transformation leader - Work at the cutting edge of AI, automation, and digital innovation\n\n\n\n\n\n\n\n\n\nMake an impact - Drive change for global enterprises and solve business challenges that matter\n\n\n\n\n\n\n\nAccelerate your career - Get hands-on experience, mentorship, and continuous learning opportunities\n\n\n\n\n\n\n\nWork with the best - Join 140,000+ bold thinkers and problem-solvers who push boundaries every day\n\n\n\n\n\n\n\nThrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress\n\n\n\n\n\nCome join the tech shapers and growth makers at Genpact and take your career in the only direction that matters: Up.\n\n\n\nLet&rsquos build tomorrow together.\n\n\n\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Data warehouse Architect / Consultant,Robotics Technologies,5-15 Years,INR 12.5 - 22.5 LPA,"Navi Mumbai, Mumbai, Pune",Software Engineering,"Description\nWe are seeking a skilled Data Warehouse Architect / Consultant to design and implement robust data warehouse solutions. The ideal candidate will have extensive experience in data warehousing concepts, ETL processes, and data modeling, with a proven track record of delivering high-quality data solutions.\nResponsibilities\nDesign and implement data warehouse solutions that meet business needs.\nCollaborate with stakeholders to gather requirements and translate them into technical specifications.\nDevelop ETL processes to extract, transform, and load data from multiple sources into the data warehouse.\nOptimize data models for performance and scalability.\nEnsure data quality and integrity throughout the data lifecycle.\nProvide technical guidance and support to data engineers and analysts.\nStay updated with industry trends and best practices in data warehousing.\nSkills and Qualifications\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n5-15 years of experience in data warehousing, data modeling, and ETL development.\nStrong knowledge of SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).\nProficiency in data warehousing tools such as Amazon Redshift, Snowflake, or Google BigQuery.\nExperience with ETL tools like Informatica, Talend, or Apache NiFi.\nFamiliarity with data visualization tools such as Tableau, Power BI, or Looker.\nUnderstanding of data governance and data management best practices.\nExcellent analytical and problem-solving skills.\nStrong communication and interpersonal skills to collaborate with cross-functional teams.","Business Intelligence, Data Modeling, Cloud Services, Big Data, Data Warehousing, Data Governance, Sql, Data Integration, Etl, Performance Tuning"
Data warehouse Architect / Consultant,Robotics Technologies,5-15 Years,INR 18.5 - 25 LPA,"Noida, Delhi NCR, Pune",Software,"Description\nWe are seeking an experienced Data Warehouse Architect/Consultant to design, implement, and maintain robust data warehousing solutions. The ideal candidate will have a strong background in ETL processes, data modeling, and performance optimization to support our data-driven decision-making.\nResponsibilities\nDesign and implement data warehousing solutions based on business requirements.\nDevelop and maintain ETL processes to ensure data integrity and availability.\nCollaborate with data analysts and business stakeholders to gather requirements and translate them into technical specifications.\nOptimize data warehouse performance and troubleshoot issues as they arise.\nCreate documentation for data warehouse architecture, ETL processes, and data modeling.\nEnsure compliance with data governance and security policies.\nSkills and Qualifications\n5-15 years of experience in data warehousing and ETL development.\nProficient in SQL and experience with database management systems like Oracle, SQL Server, or PostgreSQL.\nExperience with ETL tools such as Informatica, Talend, or Apache Nifi.\nStrong understanding of data modeling concepts and methodologies (dimensional modeling, star schema, snowflake schema).\nFamiliarity with cloud data warehousing solutions such as Amazon Redshift, Google BigQuery, or Snowflake.\nKnowledge of data visualization tools like Tableau, Power BI, or Looker is a plus.\nExcellent analytical and problem-solving skills.\nStrong communication skills to work effectively with team members and stakeholders.","Business Intelligence, Cloud Computing, Etl Tools, Data Modeling, Big Data, Data Warehousing, Data Governance, Sql, Data Integration, Performance Tuning"
"Senior Principal Consultant - Data Architect, Data Engineering",Genpact,Fresher,,Bengaluru,IT/Computers - Hardware & Networking,"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose - the relentless pursuit of a world that works better for people - we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\n\n\n\n\n\n\nInviting Applications for Senior Principal Consultant - Data Architect, Data Engineering!\n\n\n\nA Data Architect who can design and implement data modernization solutions in the cloud is responsible for developing and implementing data architecture strategies for organizations transitioning their data systems to the cloud. They collaborate with stakeholders to understand business requirements and translate them into scalable, secure, and high-performing data solutions. The Data Solution Architect works closely with data engineers, data scientists, and other IT professionals to design and deliver robust data platforms and architectures.\n\n\n\nResponsibilities:\n\n\n\nCollaborate with stakeholders to understand business requirements and translate them into data architecture strategies and solutions.\n\n\n\nDesign and develop scalable, secure, and high-performing data architectures in the cloud, leveraging cloud technologies and services.\n\n\n\nModernize legacy data systems and migrate them to cloud-based architectures, ensuring smooth transition and minimal disruption.\n\n\n\nCollaborate with data engineers, data scientists, and other IT professionals to design and implement data pipelines, data transformation processes, and data integration solutions.\n\n\n\nEnsure data governance principles and best practices are implemented, including data quality controls, data privacy, and compliance.\n\n\n\nOptimize data platforms for performance, scalability, and cost efficiency, monitoring and troubleshooting any issues that may arise.\n\n\n\nStay updated with the latest trends and advancements in cloud technologies and data architecture, continuously enhancing knowledge and skills.\n\n\n\nProvide guidance and mentorship to junior team members, fostering a culture of learning and growth.\n\n\n\nAct as a subject matter expert in data architecture and cloud technologies, providing recommendations and insights to stakeholders and senior management\n\n\n\nLead RFP responses by working with the Sales, Vertical and competency teams. Groom the team members in the pre-sales area.\n\n\n\nQualifications we seek in you!\n\n\n\nMinimum Qualifications\n\n\n\nData Architecture: In-depth knowledge and experience in designing and implementing scalable, secure, and high-performing data architectures in the cloud.\n\n\n\nCloud Technologies: Proficiency in cloud platforms such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP), including their data services like AWS Glue, Azure Data Factory, or Google BigQuery.\n\n\n\nData Modernization: Expertise in modernizing legacy data systems and transitioning them to cloud-based architectures using industry best practices.\n\n\n\nData Integration: Strong understanding of data integration techniques, including ETL (Extract, Transform, Load) processes, data pipelines, and data streaming using Python , Kafka for streams ,Pyspark , DBT , and ETL services provided by cloud providers and SQL/PLSQL\n\n\n\nDatabase Technologies: Familiarity with various database technologies, both relational and non-relational, such as SQL databases, NoSQL databases, data lakes, and data warehouses. Should have strong hands-on knowledge in Data Modelling techniques using ERWIN\n\n\n\nData Governance: Knowledge of data governance principles and practices, including data quality, data lineage, data privacy, and compliance.\n\n\n\nData Security: Understanding of data security principles, including encryption, access controls, and data masking techniques.\n\n\n\nProgramming and Scripting: Proficiency in programming languages such as Python, Java, or Scala, and experience with scripting languages like SQL or Shell scripting.\n\n\n\nCommunication and Collaboration: Excellent communication and collaboration skills to effectively work with stakeholders, business users, and technical teams.\n\n\n\nPre-Sales :Ability to lead/Coordinate the medium to large scale RFPs and construct the end to end big-data, analytics technical solutions and articulate the same using Power point or word document. Participate the client presentations and present the solutions prepared.\n\n\n\n\n\n\nPreferred Qualifications\n\n\n\nHybrid cloud domain exposure including end to end data architecture experience.\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation. Get to know us at and on , , , and .\n\n\n\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Data Architect,Wipro Limited,11-13 Years,,Chennai,IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\nJob Description\nRole:\nService Desk Manager\n\nBand C1 Role (Data Architect)\nLocation Chennai, Noida\nTotal exp 11+ Years\nThe candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.\nMust have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop\nMust have hands on in writing complex use case driven SQLs\nShould have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.\nShould have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3\nShould have been involved in On-Prem to Cloud Migration process.\nShould have good knowledge with HIVE / Spark / Scala scripts\nShould have good knowledge on Unix Shell scripting\nShould be flexible to overlap US business hours\nShould be able to drive technical design on Cloud applications\nShould be able to guide & drive the team members for cloud implementations\nShould be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.\nAWS Certified applicants preferable\nCompetencies\nClient Centricity\nPassion for Results\nCollaborative Working\nProblem Solving & Decision Making\nEffective communication\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, On-Prem to Cloud Migration, Data Warehouse, Sql, S3, Unix Shell Scripting, Emr, Aws Cloud, Hadoop, Etl, Sqoop, Hive, Scala, Redshift, Spark"
Data Architect,Wipro Limited,11-13 Years,,"Gurugram, Gurugram",IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\nJob Description\nRole:\nService Desk Manager\nBand C1 Role (Data Architect)\nLocation Chennai, Noida\nTotal exp 11+ Years\nThe candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.\nMust have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop\nMust have hands on in writing complex use case driven SQLs\nShould have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.\nShould have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3\nShould have been involved in On-Prem to Cloud Migration process.\nShould have good knowledge with HIVE / Spark / Scala scripts\nShould have good knowledge on Unix Shell scripting\nShould be flexible to overlap US business hours\nShould be able to drive technical design on Cloud applications\nShould be able to guide & drive the team members for cloud implementations\nShould be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.\nAWS Certified applicants preferable\nCompetencies\nClient Centricity\nExecution Excellence\nCollaborative Working\nProblem Solving & Decision Making\nEffective communication\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, S3, Sql, Unix Shell Scripting, Aws Cloud, Emr, Hadoop, Etl, Sqoop, Hive, Scala, Data Warehouse, Redshift, Spark"
Data Architect,DBiz.ai,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"About the Company:\nDbiz is a high-performing product and engineering company that partners with organizations to build out digital solutions using the right technology at the right time. We pride ourselves on our innovative use of technology in various ways.\nRole Summary:\nWe are seeking a Solution Architect with deep expertise in microservices architecture and AWS cloud technologies. Knowledge of Azure is a plus. The ideal candidate will be passionate about big data, analytics, and AI, with a strong background in designing scalable, secure, and cost-effective data architectures.\nResponsibilities:\nData Architecture Design: Create and implement scalable data architectures using AWS, supporting BI, analytics, and AI initiatives.\nCloud Data Solutions: Lead AWS-based data ecosystems using services like S3, Redshift, Athena, AWS Data Lakehouse, and more.\nMicroservices Architecture: Design and implement microservices for distributed data processing, ensuring efficient communication and scalability.\nData Modelling & Integration: Develop data models (relational & non-relational), integrate diverse data sources (databases, APIs, real-time streaming).\nETL & Data Pipelines: Architect end-to-end pipelines with tools like AWS Glue, Lambda, ensuring seamless ingestion and transformation.\nData Governance & Security: Implement policies for data security, privacy, and integrity; leverage AWS services like IAM and encryption.\nPerformance Optimization: Enhance performance and cost efficiency for data storage, retrieval, and processing.\nCollaboration & Stakeholder Engagement: Work cross-functionally with product, data engineering, and business teams to align architecture with goals.\nEmerging Technologies: Stay updated on cloud tech, data management, and adopt new tools/frameworks.\nLeadership: Provide technical guidance, mentor data engineers, and lead architectural reviews and decisions.\nRequirements:\nExperience: Minimum 10 years in data architecture focusing on AWS cloud services.\nMicroservices Expertise: Proven ability to design scalable data solutions on AWS using microservices.\nAWS Technologies: Proficiency with S3, Redshift, Athena, AWS Glue, etc.\nCollaboration: Strong communication skills for working with diverse teams.\nLeadership: Experience guiding architectural decisions and mentoring technical teams.\nGood to Have:\nETL Pipelines: Experience with AWS Glue, Lambda, or similar.\nData Modeling: Skills in both relational and NoSQL databases.\nData Governance: Understanding of data security, compliance, and governance.\nOptimization: Track record of cost-effective, high-performance data architecture.\nProblem-Solving: Strategic mindset for building resilient data solutions.\nLife at Dbiz:\nCompetitive salary and attractive benefits\nDynamic and innovative work environment\nOpportunities for personal growth and development\nEngaging and collaborative company culture","microservices architecture, data architecture design, Ai, AWS cloud technologies, ETL data pipelines, AWS Data Lakehouse, Big Data Analytics, Data Modeling, Data Governance, AWS Glue, Encryption"
Microsoft Fabric Data Architect - Manager,KPMG India,9-11 Years,,"Pune, India",Login to check your skill match score,"Job Description\n\nAbout KPMG in India\n\nKPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.\n\nKPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.\n\nThe person will work on a variety of projects in a highly collaborative, fast-paced environment. The person will be responsible for software development activities of KPMG, India. Part of the development team, he/she will work on the full life cycle of the process, develop code and unit testing. He/she will work closely with Technical Architect, Business Analyst, user interaction designers, and other software engineers to develop new product offerings and improve existing ones. Additionally, the person will ensure that all development practices are in compliance with KPMG's best practices policies and procedures. This role requires quick ramp up on new technologies whenever required.\n\nResponsibilities\n\nRole : Microsoft Fabric Data Architect\n\nLocation: Pune\n\nExperience: 9 to 11 years\n\nResponsibilities\n\nData Architecture: Design end-to-end data architecture leveraging Microsoft Fabric's capabilities.\nData Flows: Design data flows within the Microsoft Fabric environment.\nStorage Strategies: Implement OneLake storage strategies.\nAnalytics Configuration: Configure Synapse Analytics workspaces.\nIntegration Patterns: Establish Power BI integration patterns.\nData Integration: Architect data integration patterns between systems using Azure Databricks and Microsoft Fabric.\nDelta Lake Architecture: Design Delta Lake architecture and implement medallion architecture (Bronze/Silver/Gold layers).\nReal-Time Data Ingestion: Create real-time data ingestion patterns and establish data quality frameworks.\nData Governance: Establish data governance frameworks incorporating Microsoft Purview for data quality, lineage, and compliance.\nSecurity: Implement row-level security, data masking, and audit logging mechanisms.\nPipeline Development: Design scalable data pipelines using Azure Databricks for ETL/ELT processes and real-time data integration.\nPerformance Optimization: Implement performance tuning strategies for large-scale data processing and analytics workloads.\nExperience: Proven experience in data architecture, particularly with Microsoft Fabric and Azure Databricks.\nTechnical Skills: Proficiency in Microsoft Fabric, Azure Databricks, Synapse Analytics, and data modeling.\nAnalytical Skills: Strong analytical and problem-solving skills.\nCommunication: Excellent communication and teamwork skills.\nCertifications: Relevant certifications in Microsoft data platforms are a plus\n\nQualifications\n\n\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n\nEqual Opportunity Employer\n\nKPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","Real-Time Data Ingestion, Data Flows, Microsoft Fabric, Power BI Integration Patterns, Microsoft Purview, Audit Logging, Synapse Analytics, pipeline development, Delta Lake Architecture, Performance Optimization, Row-Level Security, OneLake Storage Strategies, ELT, data masking, Data Architecture, Azure Databricks, Etl, Data Governance, Data Modeling"
Data Architect,DHL,Fresher,,"Indore, India",Login to check your skill match score,"Start your IT career with us!\n\nor\n\nYour IT Future, Delivered\n\nSolutions Architect\n\nOpen to all PAN India candidates.\n\nWith a global team of 6000+ IT professionals, DHL IT Services connects people and keeps the global economy running by continuously innovating and creating sustainable digital solutions. We work beyond global borders and push boundaries across all dimensions of logistics. You can leave your mark shaping the technology backbone of the biggest logistics company of the world. Our offices in Cyberjaya, Prague, and Chennai have earned #GreatPlaceToWork certification, reflecting our commitment to exceptional employee experiences.\n\nDigitalization. Simply delivered.\n\nAt IT Services, we are passionate about Solution Architect in datawarehouse and business intelligence space. Our Customer Service Complex Data Solution team is continuously expanding. No matter your level of Solution Architect proficiency, you can always grow within our diverse environment.\n\n#DHL #DHLITServices #GreatPlace #ppmt #Kart #cscombine\n\nGrow together\n\nWe strive to deliver efficient and optimized business solutions in the Area of Customer Service Complex Data Solutions for our business. You will work as Solutions Architect for existing and new applications to provide end to end Architecture expertise on wide range of technologies like Snowflake, Teradata, Power BI, Matillion, Azure Cloud and many more.\n\nYou will be our main Architect providing guidance and direction on the implementation of Analytics, Data Warehousing & Reporting products. You will ensure that the Analytics & Reporting solutions meets the required performance benchmark and adheres to standards & guidelines.\n\nYou will work with project teams to ensure Business Requirements are delivered keeping in mind the end-to-end Solution & Data Architecture. You will get to work with some of the complex data structures that will need your expertise to Data Modelling & Design. You will be involved in Optimizing the performance and resource utilization of the existing solutions.\n\nYou will guide the development team with technical expertise for ensuring business requirements are implemented as expected. This would mean you sometime have to get down to coding and provide a solution or high-level approach to achieve the requirement to give direction to the Dev Team.\n\nAs a senior member in the team, you will collaborate with business users on Requirements and ensure that the requirements are well defined before assigning for development. Lead discussion with Business during UAT Defects review.\n\nYou will be working on latest technologies like Snowflake, Matilllion, Teradata, ERWIN, Microservices, Data pipelines, Jenkins, Jira/Confluence, Splunk etc. You will get ample opportunities to grow within the organization and with focus on continuous learning will get opportunity to work & learn many different technologies.\n\nReady to embark on the journey Here's what we are looking for:\n\nAs a Solution Architect, having excellent skill in understanding the latest technology relation to the business knowledge of customer service experience is a huge plus. Very good knowledge of data modeling will also be an integral part of this role and experience in implementation of customer facing application. Been part of the Agile / Scrum team experience is useful. Well versed in Architecture design, software development experiences especially in Python, familiarity of development framework and also analytics and problem solving skills.\n\nYou are a business intelligence technology aficionado, therefore you have a good understanding of latest analytics skill sets and experience in implementation of MVP and POC rapid prototyping experience is good to have also in the AI space of new technology adoptions. You are able to work independently prioritize and organize your tasks under time and workload pressure. Working in a multinational environment, you can expect cross-region collaboration with teams around the globe, thus being advanced in spoken and written English will be certainly useful. Basic certification / knowledge of AWS / Azure/ Snowflake/ Teradata/ Power BI related too is a plus.\n\nAn array of benefits for you:\n\nHybrid work arrangements to balance in-office collaboration and home flexibility.\nAnnual Leave: 42 days off apart from Public / National Holidays.\nMedical Insurance: Self + Spouse + 2 children. An option to opt for Voluntary\nParental Insurance (Parents / Parent -in-laws) at a nominal premium covering pre existing disease.\nIn House training programs: professional and technical training certifications.","Matillion, Teradata, Analytics, snowflake, Data pipelines, Data Modelling, Power Bi, Jira, Microservices, Jenkins, Confluence, Azure Cloud, Splunk, Python"
Data Center Architect - Delivery,Lenovo India,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Resource who is expert in Data Center System Integration (SI) services discovery, assessment, Planning, Design, Setup, Configure, and Migration Services with Data Center products & solutions such as VMware, VCF, Nutanix, Azure Stack HCI, backup, Storage, SDDC, containers, Red hat KVM, Open-shift etc.\nTraining & Certifications: VMware , Nutanix, Server, Backup, Storage, HCI\nExperience Required: 12-15 Years of Experience in Data Center services, Hybrid Cloud deployment on premises experience is mandatory.","Azure Stack HCI, Backup Storage, Data Center System Integration, SDDC, Red Hat KVM, VMware VCF, Nutanix, Containers"
AWS Data & AI Architect,Cloud202,3-5 Years,,India,Login to check your skill match score,"Location: Remote (India)\nJob Type: Full-time | Permanent | Remote\nAbout Cloud202:\nAs an AWS Advanced Tier Services Partner, we specialize in helping organizations accelerate innovation through modern cloud-native solutions, data platforms, and AI applicationsincluding Generative AI. With a global presence and a deep commitment to business impact, we work with leading enterprises to design intelligent, ethical, and scalable systems.\nRole Overview:\nWe are looking for a Data & AI Architect with deep AWS expertise to lead the design and implementation of modern, cloud-native data and AI platforms. This role will combine architectural vision with hands-on technical delivery, driving end-to-end solutions that enable advanced analytics, data governance, and enterprise-scale AIincluding Generative AI use cases.\nKey Responsibilities:\nDesign and lead scalable data and AI architectures using AWS services such as SageMaker, Bedrock, Glue, Redshift, Athena, EMR, and Lake Formation.\nArchitect end-to-end data pipelines that support AI/ML workloads, data lakehouse strategies, and real-time analytics.\nDrive implementation of Retrieval-Augmented Generation (RAG) pipelines, vector databases, and LLM-based inference workflows.\nCollaborate with business and technical stakeholders to understand use cases, data requirements, and success metrics.\nEmbed data governance, security, lineage, and MLOps best practices into platform designs.\nMentor engineering teams and provide technical leadership across data and AI initiatives.\nWork closely with partner and client teams to co-create Generative AI MVPs, production-grade applications, and reusable blueprints.\nEnsure architectural alignment with AWS Well-Architected principles and sustainability guidelines.\nRequired Qualifications:\n3+ years of experience in data engineering, analytics, or AI roles, with at least 1+ years in architecture.\nStrong expertise in AWS analytics and AI services, including SageMaker, Bedrock, Glue, Lake Formation, Redshift, and Athena.\nProficient in SQL, Python, and PySpark, with hands-on experience in building data pipelines and model workflows.\nSolid understanding of Generative AI patterns (e.g., RAG, prompt engineering, fine-tuning, grounding).\nExperience with data governance, metadata management, and secure data sharing practices.\nStrong stakeholder management and communication skills.\nPreferred Qualifications:\nAWS Certified (Solutions Architect Professional, Data Analytics Specialty, or Machine Learning Specialty).\nExperience with LangChain, vector databases (e.g., FAISS, Pinecone, OpenSearch), and MLOps tools.\nFamiliarity with open-source LLMs and foundation models, fine-tuning workflows, and responsible AI practices.\nBackground in consulting or customer-facing delivery roles is a plus.\nWhy Join Cloud202:\n100% remote work from anywhere in India with flexible hours.\nWork on impactful, AWS Generative AI and data modernization projects.\nAccess to global clients and innovation programs across sectors.\nFlat, transparent, and collaborative culture that values continuous learning.\nCertifications, mentorship, and growth opportunities in cutting-edge AI and cloud domains.","OpenSearch, Lake Formation, Athena, vector databases, SageMaker, Glue, bedrock, LangChain, Pinecone, FAISS, Generative AI, Sql, Emr, Pyspark, MLops, AWS, Python, Redshift"
Data Analytics Architect (Looker),66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.\nOverview of Role\nWe're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.\nResponsibilities\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nWork with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.\nAdvise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.\nUtilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.\nWrite clean, efficient, and scalable code (LookML, Python as applicable)\nConduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.\nStay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.\nCollaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.\nProvide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.\nQualifications\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.\nExcellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.\nExperience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.\nStrong proficiency with SQL required.\nDemonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.\nExcellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.\nSolid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.\nStrong problem-solving skills and the ability to translate business requirements into technical solutions.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nGoogle Cloud certifications and any analytics platform certifications are a plus.\nA desire to stay ahead of the curve and continuously learn new technologies and techniques.\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, Data Modeling, Python, Sql, Google Cloud"
Data center Architect,Kyndryl India,7-9 Years,,"Mumbai, India",Login to check your skill match score,"Who We Are\n\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\n\nJob Description\n\nInfrastructure Specialists at Kyndryl are project-based subject matter experts in all things infrastructure good at providing analysis, documenting and diagraming work for hand-off, offering timely solutions, and generally figuring it out. This is a hands-on role where your feel for the interaction between a system and its environment will be invaluable to every one of your clients.\n\nThere are two halves to this role: First, contributing to current projects where you analyze problems and tech issues, offer solutions, and test, modify, automate, and integrate systems. And second, long-range strategic planning of IT infrastructure and operational execution. This role isn't specific to any one platform, so you'll need a good feel for all of them. And because of this, you'll experience variety and growth at Kyndryl that you won't find anywhere else.\n\nYou'll be involved early to offer solutions, help decide whether something can be done, and identify the technical and timeline risks up front. This means dealing with both client expectations and internal challenges in other words, there are plenty of opportunities to make a difference, and a lot of people will witness your contributions. In fact, a frequent sign of success for our Infrastructure Specialists is when clients come back to us and ask for the same person by name. That's the kind of impact you can have!\n\nThis is a project-based role where you'll enjoy deep involvement throughout the lifespan of a project, as well as the chance to work closely with Architects, Technicians, and PMs. Whatever your current level of tech savvy or where you want your career to lead, you'll find the right opportunities and a buddy to support your growth. Boredom Trust us, that won't be an issue.\n\nYour future at Kyndryl\n\nThere are lots of opportunities to gain certification and qualifications on the job, and you'll continuously grow as a Cloud Hyperscaler. Many of our Infrastructure Specialists are on a path toward becoming either an Architect or Distinguished Engineer, and there are opportunities at every skill level to grow in either of these directions.\n\nWho You Are\n\nYou're good at what you do and possess the required experience to prove it. However, equally as important you have a growth mindset; keen to drive your own personal and professional development. You are customer-focused someone who prioritizes customer success in their work. And finally, you're open and borderless naturally inclusive in how you work with others.\n\nRequired Technical and Professional Expertise\n\nMinimum of 7 years in IT infrastructure design and management, with demonstrable experience in on-premises solutions.\nExperience in developing and maintaining high-quality documentation for complex technical systems.\nIn-depth understanding of on-premises infrastructure components including servers, storage, networking, virtualization, and security.\nFamiliarity with automation, monitoring, and management tools.\nArchitect and deploy scalable, secure, and resilient on-premises systems including servers, storage, networking, and virtualization.\nDevelop technical blueprints and solution designs that align with business objectives and industry best practices.\nEvaluate emerging technologies and recommend enhancements to improve system performance and security.\nDevelop, maintain, and update detailed technical documentation, including system architecture diagrams, design specifications, standard operating procedures (SOPs), and disaster recovery plans.\nCreate training materials and conduct technical sessions for internal teams to ensure proper system usage and maintenance.\n\nPreferred Technical And Professional Experience\n\nBachelor's degree in computer science, Information Technology, or a related field. Master's degree is a plus.\nRelevant certifications such as TOGAF, ITIL, PMP, or similar are preferred.\nEnsure all infrastructure solutions meet relevant regulatory requirements, security standards, and compliance guidelines.\nMonitor system performance and implement optimization measures to enhance efficiency and reliability.\nCoordinate with external vendors and service providers as needed to support infrastructure initiatives.\n\nBeing You\n\nDiversity is a whole lot more than what we look like or where we come from, it's how we think and who we are. We welcome people of all cultures, backgrounds, and experiences. But we're not doing it single-handily: Our Kyndryl Inclusion Networks are only one of many ways we create a workplace where all Kyndryls can find and provide support and advice. This dedication to welcoming everyone into our company means that Kyndryl gives you and everyone next to you the ability to bring your whole self to work, individually and collectively, and support the activation of our equitable culture. That's the Kyndryl Way.\n\nWhat You Can Expect\n\nWith state-of-the-art resources and Fortune 100 clients, every day is an opportunity to innovate, build new capabilities, new relationships, new processes, and new value. Kyndryl cares about your well-being and prides itself on offering benefits that give you choice, reflect the diversity of our employees and support you and your family through the moments that matter wherever you are in your life journey. Our employee learning programs give you access to the best learning in the industry to receive certifications, including Microsoft, Google, Amazon, Skillsoft, and many more. Through our company-wide volunteering and giving platform, you can donate, start fundraisers, volunteer, and search over 2 million non-profit organizations. At Kyndryl, we invest heavily in you, we want you to succeed so that together, we will all succeed.\n\nGet Referred!\n\nIf you know someone that works at Kyndryl, when asked How Did You Hear About Us during the application process, select Employee Referral and enter your contact's Kyndryl email address.","management tools, on-premises solutions, documentation for complex technical systems"
Data Platform Architect,SpurTree Technologies,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Platform Architect, you will play a pivotal role in the design, implementation, and optimization of scalable data architecture and systems. You will be responsible for creating data-driven solutions that enable effective data storage, integration, processing, and analytics. You will collaborate closely with data engineers, data scientists, and business stakeholders to build a robust data infrastructure that supports the organization's data strategy.\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, Data Engineering, or a related field.\nProven experience (typically 7+ years) in designing and implementing complex data architectures and platforms.\nExpertise in cloud platforms (AWS, Azure, GCP) and their data services (e.g., Amazon Redshift, Azure Synapse, BigQuery).\nStrong knowledge of data integration, ETL/ELT tools, and data pipelines.\nHands-on experience with big data technologies (e.g., Hadoop, Spark, Kafka).\nProficiency in database technologies (SQL, NoSQL, columnar, graph, etc.).\nFamiliarity with data modeling techniques and practices.\nStrong understanding of data security, governance, and privacy practices.\nExperience with data visualization and reporting tools (e.g., Power BI, Tableau).\nExcellent problem-solving, analytical, and troubleshooting skills.\nStrong communication and collaboration skills with technical and non-technical teams.\n\nPreferred Skills\n\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Microsoft Certified: Azure Data Engineer Associate).\nExperience with containerized environments (e.g., Docker, Kubernetes).\nFamiliarity with DevOps practices and CI/CD pipelines.\nKnowledge of machine learning and artificial intelligence applications in data platforms.\nExperience with Agile methodologies and project management tools.","BigQuery, Hadoop, Power Bi, Kafka, Tableau, Sql, ELT, Nosql, Azure Synapse, Gcp, Docker, Amazon Redshift, Spark, Data Visualization, Azure, Kubernetes, AWS, Etl"
Data Modelling Architect (CoE),NTT DATA North America,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Req ID: 312265\n\nNTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\n\nWe are currently seeking a Data Modelling Architect (CoE) to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nTitle: Data Modelling Architect (CoE)\n\nPosition Overview:\n\nWe are seeking a highly skilled and experienced Data Modelling Architect to join our dynamic team. The ideal candidate will have a strong background in full data modelling life cycle, e.g. design, implement, and maintain complex data models that align with organisational goals and industry standards. This role requires a deep understanding of data architecture, data modelling methodologies, and ideally in real-time data integrations. The successful candidate will collaborate with cross-functional teams to ensure optimal data structures that support business intelligence, analytics, and operational requirements. This role is primarily within the Centre of Excellence of the Data Products Factory, creating, assuring, and overseeing the implementation of data models within analytical and real-time streaming domains.\n\nKey Responsibilities\n\nDevelop conceptual, logical, and physical data models to support data analytics, streaming and data products implementation.\nDefine and maintain data architecture standards, principles, and best practices.\nEnsure data models are aligned with business requirements and scalable for future needs.\nWork closely with business stakeholders, data engineers, data solution architects, and data product teams to gather requirements and design solutions.\nProvide guidance on data integration, transformation, and migration strategies.\nEstablish and maintain enterprise data models, data dictionaries, metadata repositories, and data lineage documentation.\nEnsure data models comply with organisational policies and regulatory requirements.\nOptimise data products and their components for performance, scalability, and reliability.\nEvaluate and recommend data modelling tools and technologies.\nStay updated on industry trends and emerging technologies in data architecture.\nIdentify and resolve data inconsistencies, redundancies, and performance issues.\nProvide technical leadership in addressing complex data-related challenges.\n\nRequired Skills And Qualifications\n\n10+ years of experience in data architecture and modelling.\nProven experience in data modelling, data architecture, and data products design.\nProven experience and expertise in data modelling standards, techniques (e.g. dimensional model, 3NF, Vault 2.0)\nFamiliarity with both analytical and real-time/ streaming data solutions (Kafka/Airflow).\nHands-on experience with data modelling tools (e.g., Erwin, Lucidchart, SAP PowerDesigner).\nExpertise in Python and SQL (e.g., Snowflake, Kafka).\nExperience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway\nKnowledge of data warehouse design, ETL/ ELT processes, and big data technologies (e.g., Snowflake, Spark).\nFamiliarity with data governance and compliance frameworks (e.g., GDPR, HIPAA).\nStrong communication and stakeholder management skills.\nAnalytical mindset with attention to detail.\nAbility to lead and mentor teams on best practices in data modelling.\nEducation: Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.\n\nPreferred Skills And Qualifications\n\nCertifications in data modelling, cloud platforms, or database technologies.\nExperience in developing and implementing enterprise data models.\nExperience with Interface/ API data modelling.\nExperience with CI/CD GITHUB Actions (or similar)\nAWS fundamentals (e.g., AWS Certified Data Engineer)\nKnowledge of Snowflake/ SQL\nKnowledge of Apache Airflow\nKnowledge of DBT\nFamiliarity with Atlan for data catalog and metadata management\nUnderstanding of iceberg tables\n\nAbout NTT DATA\n\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com\n\nNTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Airflow, AWS cloud services, Lucidchart, snowflake, dimensional model, Vault 2.0, data warehouse design, 3NF, EKS, SAP PowerDesigner, data products design, Gdpr, S3, Data Architecture, Kafka, Big Data Technologies, Hipaa, ELT, Lambda, Sns, Python, Api Gateway, Data Modelling, Sql, Erwin, Spark, Data Governance, Etl"
Analytics Head / Data Model Architect,Deepak Group Co,10-12 Years,,"Vadodara, India",Login to check your skill match score,"Job Title: Analytics Head / Data Model Architect\nJob Summary:\nWe are seeking a seasoned Analytics Head / Data Model Architect to lead our data strategy, design scalable data models, and drive analytical innovation across the organization. This role combines leadership in data science and business analytics with deep technical expertise in data architecture and modelling. The ideal candidate will be a strategic thinker, technical expert, and effective communicator capable of aligning data initiatives with business objectives.\nKey Responsibilities:\nLeadership & Strategy\nLead and manage a team of data scientists, analysts, and data architects.\nDefine and drive the enterprise analytics strategy aligned with business goals.\nCollaborate with executive leadership to identify data-driven growth opportunities.\nData Architecture & Modeling\nDesign and implement robust, scalable, and high-performance data models (OLAP/OLTP, dimensional, relational, NoSQL).\nDevelop enterprise data architecture standards, policies, and best practices.\nOversee data governance, data quality, and metadata management initiatives.\nAdvanced Analytics & Insights\nBuild advanced analytics solutions including predictive modeling, statistical analysis, and machine learning frameworks.\nTranslate complex data into actionable insights for stakeholders across departments.\nEvaluate and implement modern BI tools and platforms (e.g., Power BI, Tableau, Looker).\nCollaboration & Integration\nPartner with data engineering teams to ensure efficient ETL/ELT pipelines and data lake/warehouse infrastructure.\nWork closely with business units to understand needs and deliver customized data solutions.\nSupport data privacy, security, and compliance initiatives (e.g., GDPR, HIPAA, SOC 2).\nRequired Qualifications:\nBachelor's or master's in computer science, Data Science, Statistics, or related field. PhD is a plus.\n10+ years of experience in analytics, data architecture, or related roles.\nStrong knowledge of data modeling techniques (3NF, Star Schema, Snowflake, Data Vault, etc.).\nExpertise in SQL, Python, R, and at least one cloud platform (AWS, Azure, GCP).\nExperience with modern data warehousing tools (Snowflake, BigQuery, Redshift) and orchestration (Airflow, DBT).\nProven leadership and team-building skills.\nPreferred Skills:\nExperience with AI/ML model deployment in production.\nFamiliarity with data mesh, data fabric, or modern data stack concepts.\nKnowledge of industry-specific data standards (e.g., HL7 for healthcare, ACORD for insurance).","Airflow, dimensional, Looker, R, relational, dbt, snowflake, BigQuery, Machine Learning, Power Bi, OLAP, Tableau, Redshift, Sql, Nosql, Gcp, Predictive Modeling, Azure, Python, AWS, Oltp, Statistical Analysis"
Data Analytics Architect (Looker),66degrees,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees\n\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\n\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.\n\nOverview of Role\n\nWe're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.\n\nResponsibilities\n\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nWork with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.\nAdvise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.\nUtilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.\nWrite clean, efficient, and scalable code (LookML, Python as applicable)\nConduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.\nStay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.\nCollaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.\nProvide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.\nExcellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.\nExperience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.\nStrong proficiency with SQL required.\nDemonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.\nExcellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.\nSolid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.\nStrong problem-solving skills and the ability to translate business requirements into technical solutions.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nGoogle Cloud certifications and any analytics platform certifications are a plus.\nA desire to stay ahead of the curve and continuously learn new technologies and techniques.\n\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, Microstrategy, Looker, LookML, Azure, Python, Sql, Google Cloud, AWS"
Big Data-Cloud Architect,Airtel Digital,8-12 Years,,"Pune, India",Login to check your skill match score,"We are seeking an experienced Big Data Cloud Architect with 812 years in designing and implementing scalable big data solutions on Azure and AWS. The ideal candidate will have deep hands-on expertise in cloud-based big data technologies, a strong background in the software development life cycle (SDLC), and experience in microservices and backend development.\n\n**Key Responsibilities**\n\nDesign, build, and maintain scalable big data architectures on Azure and AWS\nSelect and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )\nLead data migration from legacy systems to cloud-based solutions\nDevelop and optimize ETL pipelines and data processing workflows.\nEnsure data infrastructure meets performance, scalability, and security requirements.\nCollaborate with development teams to implement microservices and backend solutions for big data applications.\nOversee the end-to-end SDLC for big data projects, from planning to deployment.\nMentor junior engineers and contribute to architectural best practices.\nPrepare architecture documentation and technical reports.\n\n#ADL","Azure Data Factory, Hadoop, Spark, Kafka, Azure, Etl, AWS"
Celonis Data Architect,Harman Connected Services Corporation India Private Limited,12-15 Years,INR 2 - 3 LPA,Bengaluru,Software,"A Celonis data architect is responsible for designing and implementing the data architecture required for effective utilization of the Celonis process mining tool within an organization. They work closely with various stakeholders, including data engineers, business analysts, and process owners, to define data requirements, ensure data integrity, and enable seamless data integration.\nThe role requires a strong understanding of data architecture principles, data modeling techniques, and relational and non-relational databases. Candidate should also be proficient in ETL processes and tools, as well as have experience in data integration and data governance. Additionally, candidate should have a good understanding of business processes, data visualization, and analytics to effectively implement process mining initiatives within the customer organization\nRole and Responsibilities\nOver all 12-15 years of IT experience with minimum 5+ years experience in Process Mining using Celonis\nStrong experience in Data Integration, Data Modeling, Data quality, Data pipeline management, Dashboard design\nExperience in delivering at least 1 to 2 large scale end-to-end Celonis process mining implementations as Data Architect\nExperience in leading team of data modelers, data engineers\nStrong experience in providing multiple solutions and reviewing the implementations in parallel\nExpertise in defining the governance, security, roles around Data pools and dashboards in Celonis\nExperience in implementing object centric process mining\nMajor accountabilities:\nCollaborating with business stakeholders to understand their data requirements and process mining goals.\nEngage with customers C-level their strategic objectives with the Celonis technical strategy\nDesigning and implementing data models, schemas, and structures that align with the organization's data governance policies and standards.\nIdentifying and documenting data sources, including databases, systems, and applications, that need to be integrated with Celonis.\nEnsuring proper data extraction, transformation, and loading (ETL) processes to populate the necessary data into Celonis.\nSupervising data engineers in the development and maintenance of data pipelines and workflows.\nAssessing data quality, consistency, and accuracy and taking proactive measures to resolve issues. Implementing data security measures, including access controls and data encryption, to protect sensitive information within Celonis.\nProviding technical support and guidance to business analysts and end-users in data preparation, analysis, and reporting using Celonis.\nStaying updated with the latest trends and advancements in data management and process mining technologies, including Celonis updates and new features.","Celonis Data Architect, Etl"
Big Data Architect,Persistent,10-12 Years,,Hyderabad,Information Technology,"We are looking for a Data Architect with creativity and results-oriented critical thinking to meet complex challenges and develop new strategies for acquiring, analyzing, modeling and storing data.\nIn this role you will guide the company into the future and utilize the latest technology and information management methodologies to meet our requirements for effective logical data modeling, metadata management and database warehouse domains. You will be working with experts in a variety of industries, including computer science and software development, as well as department heads and senior executives to integrate new technologies and refine system performance.\nWe reward dedicated performance with exceptional pay and benefits, as well as tuition reimbursement and career growth opportunities.\nWhat Youll Do\nDefine data retention policies\nMonitor performance and advise any necessary infrastructure changes\nMentor junior engineers and work with other architects to deliver best in class solutions\nImplement ETL / ELT process and orchestration of data flows\nRecommend and drive adoption of newer tools and techniques from the big data ecosystem\nExpertise Youll Bring\n10+ years in industry, building and managing big data systems\nBuilding, monitoring, and optimizing reliable and cost-efficient pipelines for SaaS is a must\nBuilding stream-processing systems, using solutions such as Storm or Spark-Streaming\nDealing and integrating with data storage systems like SQL and NoSQL databases, file systems and object storage like s3\nReporting solutions like Pentaho, PowerBI, Looker including customizations\nDeveloping high concurrency, high performance applications that are database-intensive and have interactive, browser-based clients\nWorking with SaaS based data management products will be an added advantage\nProficiency and expertise in Cloudera / Hortonworks\nSpark\nHDF and NiFi\nRDBMS, NoSQL like Vertica, Redshift, Data Modelling with physical design and SQL performance optimization\nMessaging systems, JMS, Active MQ, Rabbit MQ, Kafka\nBig Data technology like Hadoop, Spark, NoSQL based data-warehousing solutions\nData warehousing, reporting including customization, Hadoop, Spark, Kafka, Core java, Spring/IOC, Design patterns\nBig Data querying tools, such as Pig, Hive, and Impala\nOpen-source technologies and databases (SQL & NoSQL)\nProficient understanding of distributed computing principles\nAbility to solve any ongoing issues with operating the cluster\nScale data pipelines using open-source components and AWS services\nCloud (AWS), provisioning, capacity planning and performance analysis at various levels\nWeb-based SOA architecture implementation with design pattern experience will be an added advantage","Databases, Big Data, Etl"
Big Data Architect,Trellix,10-15 Years,,Bengaluru,Software,"Role Overview:\nThe Big Data Architect will be responsible for the design, implementation, and management of the organizations big data infrastructure. The ideal candidate will have a strong technical background in big data technologies, excellent problem-solving skills, and the ability to work in a fast-paced environment. The role requires a deep understanding of data architecture, data modeling, and data integration techniques.\nAbout the Role:\nDesign and implement scalable and efficient big data architecture solutions to meet business requirements.\nDevelop and maintain data pipelines, ensuring the availability and quality of data.\nCollaborate with data scientists, data engineers, and other stakeholders to understand data needs and provide technical solutions.\nLead the evaluation and selection of big data tools and technologies.\nEnsure data security and privacy compliance.\nOptimize and tune big data systems for performance and cost-efficiency.\nDocument data architecture, data flows, and processes.\nStay up to date with the latest industry trends and best practices in big data technologies.\nAbout You:\nBachelors or master's degree in computer science, Information Technology, or a related field.\nOverall 10+ years exp with 5+ years of experience in big data architecture and engineering.\nProficiency in big data technologies such as Hadoop MapReduce, Spark batch and streaming, Kafka, HBase, Scala, Elastic Search and others.\nExperience with AWS cloud platform.\nStrong knowledge of data modeling, ETL processes, and data warehousing.\nProficiency in programming languages such as Java, Scala, Spark\nFamiliarity with data visualization tools and techniques.\nExcellent communication and collaboration skills.\nStrong problem-solving abilities and attention to detail.","ETL processes, Java, Scala, Data Warehousing, Big Data, Kafka, Data Modeling"
Data Analytics Architect (Looker),66degrees,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees\n\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\n\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.\n\nOverview of Role\n\nWe're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.\n\nResponsibilities\n\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nWork with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.\nAdvise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.\nUtilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.\nWrite clean, efficient, and scalable code (LookML, Python as applicable)\nConduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.\nStay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.\nCollaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.\nProvide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.\nExcellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.\nExperience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.\nStrong proficiency with SQL required.\nDemonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.\nExcellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.\nSolid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.\nStrong problem-solving skills and the ability to translate business requirements into technical solutions.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nGoogle Cloud certifications and any analytics platform certifications are a plus.\nA desire to stay ahead of the curve and continuously learn new technologies and techniques.\n\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, Microstrategy, Looker, LookML, Azure, Python, Sql, Google Cloud, AWS"
Analytics Head / Data Model Architect,Deepak Group Co,10-12 Years,,"Vadodara, India",Login to check your skill match score,"Job Title: Analytics Head / Data Model Architect\nJob Summary:\nWe are seeking a seasoned Analytics Head / Data Model Architect to lead our data strategy, design scalable data models, and drive analytical innovation across the organization. This role combines leadership in data science and business analytics with deep technical expertise in data architecture and modelling. The ideal candidate will be a strategic thinker, technical expert, and effective communicator capable of aligning data initiatives with business objectives.\nKey Responsibilities:\nLeadership & Strategy\nLead and manage a team of data scientists, analysts, and data architects.\nDefine and drive the enterprise analytics strategy aligned with business goals.\nCollaborate with executive leadership to identify data-driven growth opportunities.\nData Architecture & Modeling\nDesign and implement robust, scalable, and high-performance data models (OLAP/OLTP, dimensional, relational, NoSQL).\nDevelop enterprise data architecture standards, policies, and best practices.\nOversee data governance, data quality, and metadata management initiatives.\nAdvanced Analytics & Insights\nBuild advanced analytics solutions including predictive modeling, statistical analysis, and machine learning frameworks.\nTranslate complex data into actionable insights for stakeholders across departments.\nEvaluate and implement modern BI tools and platforms (e.g., Power BI, Tableau, Looker).\nCollaboration & Integration\nPartner with data engineering teams to ensure efficient ETL/ELT pipelines and data lake/warehouse infrastructure.\nWork closely with business units to understand needs and deliver customized data solutions.\nSupport data privacy, security, and compliance initiatives (e.g., GDPR, HIPAA, SOC 2).\nRequired Qualifications:\nBachelor's or master's in computer science, Data Science, Statistics, or related field. PhD is a plus.\n10+ years of experience in analytics, data architecture, or related roles.\nStrong knowledge of data modeling techniques (3NF, Star Schema, Snowflake, Data Vault, etc.).\nExpertise in SQL, Python, R, and at least one cloud platform (AWS, Azure, GCP).\nExperience with modern data warehousing tools (Snowflake, BigQuery, Redshift) and orchestration (Airflow, DBT).\nProven leadership and team-building skills.\nPreferred Skills:\nExperience with AI/ML model deployment in production.\nFamiliarity with data mesh, data fabric, or modern data stack concepts.\nKnowledge of industry-specific data standards (e.g., HL7 for healthcare, ACORD for insurance).","Airflow, dimensional, Looker, R, relational, dbt, snowflake, BigQuery, Machine Learning, Power Bi, OLAP, Tableau, Redshift, Sql, Nosql, Gcp, Predictive Modeling, Azure, Python, AWS, Oltp, Statistical Analysis"
Data Center Architect - Delivery,Lenovo India,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Resource who is expert in Data Center System Integration (SI) services discovery, assessment, Planning, Design, Setup, Configure, and Migration Services with Data Center products & solutions such as VMware, VCF, Nutanix, Azure Stack HCI, backup, Storage, SDDC, containers, Red hat KVM, Open-shift etc.\nTraining & Certifications: VMware , Nutanix, Server, Backup, Storage, HCI\nExperience Required: 12-15 Years of Experience in Data Center services, Hybrid Cloud deployment on premises experience is mandatory.","Azure Stack HCI, Backup Storage, Data Center System Integration, SDDC, Red Hat KVM, VMware VCF, Nutanix, Containers"
Data Analytics Architect (Looker),66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees\n66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.\nAt 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.\nOverview of Role\nWe're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.\nResponsibilities\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nWork with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.\nAdvise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.\nUtilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.\nWrite clean, efficient, and scalable code (LookML, Python as applicable)\nConduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.\nStay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.\nCollaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.\nProvide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.\nQualifications\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.\nExcellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.\nExperience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.\nStrong proficiency with SQL required.\nDemonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.\nExcellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.\nSolid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.\nStrong problem-solving skills and the ability to translate business requirements into technical solutions.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nGoogle Cloud certifications and any analytics platform certifications are a plus.\nA desire to stay ahead of the curve and continuously learn new technologies and techniques.\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, Data Modeling, Python, Sql, Google Cloud"
Big Data-Cloud Architect,Airtel Digital,8-12 Years,,"Pune, India",Login to check your skill match score,"We are seeking an experienced Big Data Cloud Architect with 812 years in designing and implementing scalable big data solutions on Azure and AWS. The ideal candidate will have deep hands-on expertise in cloud-based big data technologies, a strong background in the software development life cycle (SDLC), and experience in microservices and backend development.\n\n**Key Responsibilities**\n\nDesign, build, and maintain scalable big data architectures on Azure and AWS\nSelect and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )\nLead data migration from legacy systems to cloud-based solutions\nDevelop and optimize ETL pipelines and data processing workflows.\nEnsure data infrastructure meets performance, scalability, and security requirements.\nCollaborate with development teams to implement microservices and backend solutions for big data applications.\nOversee the end-to-end SDLC for big data projects, from planning to deployment.\nMentor junior engineers and contribute to architectural best practices.\nPrepare architecture documentation and technical reports.\n\n#ADL","Azure Data Factory, Hadoop, Spark, Kafka, Azure, Etl, AWS"
Big Data Architect,Trellix,10-15 Years,,Bengaluru,Software,"Role Overview:\nThe Big Data Architect will be responsible for the design, implementation, and management of the organizations big data infrastructure. The ideal candidate will have a strong technical background in big data technologies, excellent problem-solving skills, and the ability to work in a fast-paced environment. The role requires a deep understanding of data architecture, data modeling, and data integration techniques.\nAbout the Role:\nDesign and implement scalable and efficient big data architecture solutions to meet business requirements.\nDevelop and maintain data pipelines, ensuring the availability and quality of data.\nCollaborate with data scientists, data engineers, and other stakeholders to understand data needs and provide technical solutions.\nLead the evaluation and selection of big data tools and technologies.\nEnsure data security and privacy compliance.\nOptimize and tune big data systems for performance and cost-efficiency.\nDocument data architecture, data flows, and processes.\nStay up to date with the latest industry trends and best practices in big data technologies.\nAbout You:\nBachelors or master's degree in computer science, Information Technology, or a related field.\nOverall 10+ years exp with 5+ years of experience in big data architecture and engineering.\nProficiency in big data technologies such as Hadoop MapReduce, Spark batch and streaming, Kafka, HBase, Scala, Elastic Search and others.\nExperience with AWS cloud platform.\nStrong knowledge of data modeling, ETL processes, and data warehousing.\nProficiency in programming languages such as Java, Scala, Spark\nFamiliarity with data visualization tools and techniques.\nExcellent communication and collaboration skills.\nStrong problem-solving abilities and attention to detail.","ETL processes, Java, Scala, Data Warehousing, Big Data, Kafka, Data Modeling"
Sr Data Architect,PHOTON,8-13 Years,,Chennai,Information Technology,"Job Summary\nExperience Architecting Enterprise Data Platforms for large Enterprises\nEvolved point of view and wide ranging experience in Data Governance,. Data Quality, Data Lineage\nVery strong, hands-on expertise in Data Supply Chain, Data Pipelines\nStrong experience in Google Cloud, Cloud Data Platforms\nExperience creating Data Analytics workspaces\nLeading and Guiding the team on all aspects of Data Engineering, Data Analytics\nExperience working Banking Domain, especially Private Wealth Management strongly preferred.\nTech Skill Sets: Google Cloud Data Platform, NoSQL\nAbInitio\nReporting: Looker, Tableau, Cognos","Google Cloud Data, Cloud Data Platforms, Nosql, Abinitio, Tableau"
Azure Data Lake Architect,Expleo Solutions Limited,10-13 Years,,Chennai,Information Technology,"Job Title : Azure Data Lake Architect\nExperience:10+ Years\nWork Location Candidate should work in Client Location (Egmore - Chennai) (Monday to Friday) (General Shift Timings) (100% Compulsory Work From Office)\nJob Description :\nAzure Data Lake\nAzure Databricks\nAzure Data factory\nAzure Synapse Analytics\nBlob Storage\nETL\nSQL\nJob Overview:\nWe are seeking a highly skilled and motivated individual for the role of Azure Data Lake Architect. You will be responsible for the end-to-end management and optimization of our Azure-based data lake ecosystem. This includes overseeing ETL jobs in ADF, Azure Data Lake, Azure Databricks, Azure Synapse Analytics, Blob Storage to ensure a robust and efficient data management infrastructure.\nPreferred Qualifications:\nMinimum 5 to 10 years of relevant experience\nProven experience in designing, implementing, and managing Azure Data Lake solutions.\nStrong expertise in ETL processes using ADF, Azure Databricks, Azure Synapse Analytics, Blob Storage, Proficient in SQL,\nExcellent problem-solving and troubleshooting skills.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.\nDesirable to have:\nExpertise in Python programming language with Pandas and NumPy\nCertifications: Microsoft Certified: Azure Data Engineer Associate, Microsoft Certified: Azure Enterprise Data Analyst Associate","Azure Data Lake, Azure Databricks, Azure Data Factory, Sql, Etl"
Celonis Data Architect,Harman Connected Services Corporation India Private Limited,12-15 Years,INR 2 - 3 LPA,Bengaluru,Software,"A Celonis data architect is responsible for designing and implementing the data architecture required for effective utilization of the Celonis process mining tool within an organization. They work closely with various stakeholders, including data engineers, business analysts, and process owners, to define data requirements, ensure data integrity, and enable seamless data integration.\nThe role requires a strong understanding of data architecture principles, data modeling techniques, and relational and non-relational databases. Candidate should also be proficient in ETL processes and tools, as well as have experience in data integration and data governance. Additionally, candidate should have a good understanding of business processes, data visualization, and analytics to effectively implement process mining initiatives within the customer organization\nRole and Responsibilities\nOver all 12-15 years of IT experience with minimum 5+ years experience in Process Mining using Celonis\nStrong experience in Data Integration, Data Modeling, Data quality, Data pipeline management, Dashboard design\nExperience in delivering at least 1 to 2 large scale end-to-end Celonis process mining implementations as Data Architect\nExperience in leading team of data modelers, data engineers\nStrong experience in providing multiple solutions and reviewing the implementations in parallel\nExpertise in defining the governance, security, roles around Data pools and dashboards in Celonis\nExperience in implementing object centric process mining\nMajor accountabilities:\nCollaborating with business stakeholders to understand their data requirements and process mining goals.\nEngage with customers C-level their strategic objectives with the Celonis technical strategy\nDesigning and implementing data models, schemas, and structures that align with the organization's data governance policies and standards.\nIdentifying and documenting data sources, including databases, systems, and applications, that need to be integrated with Celonis.\nEnsuring proper data extraction, transformation, and loading (ETL) processes to populate the necessary data into Celonis.\nSupervising data engineers in the development and maintenance of data pipelines and workflows.\nAssessing data quality, consistency, and accuracy and taking proactive measures to resolve issues. Implementing data security measures, including access controls and data encryption, to protect sensitive information within Celonis.\nProviding technical support and guidance to business analysts and end-users in data preparation, analysis, and reporting using Celonis.\nStaying updated with the latest trends and advancements in data management and process mining technologies, including Celonis updates and new features.","Celonis Data Architect, Etl"
Big Data Architect,Persistent,10-12 Years,,Hyderabad,Information Technology,"We are looking for a Data Architect with creativity and results-oriented critical thinking to meet complex challenges and develop new strategies for acquiring, analyzing, modeling and storing data.\nIn this role you will guide the company into the future and utilize the latest technology and information management methodologies to meet our requirements for effective logical data modeling, metadata management and database warehouse domains. You will be working with experts in a variety of industries, including computer science and software development, as well as department heads and senior executives to integrate new technologies and refine system performance.\nWe reward dedicated performance with exceptional pay and benefits, as well as tuition reimbursement and career growth opportunities.\nWhat Youll Do\nDefine data retention policies\nMonitor performance and advise any necessary infrastructure changes\nMentor junior engineers and work with other architects to deliver best in class solutions\nImplement ETL / ELT process and orchestration of data flows\nRecommend and drive adoption of newer tools and techniques from the big data ecosystem\nExpertise Youll Bring\n10+ years in industry, building and managing big data systems\nBuilding, monitoring, and optimizing reliable and cost-efficient pipelines for SaaS is a must\nBuilding stream-processing systems, using solutions such as Storm or Spark-Streaming\nDealing and integrating with data storage systems like SQL and NoSQL databases, file systems and object storage like s3\nReporting solutions like Pentaho, PowerBI, Looker including customizations\nDeveloping high concurrency, high performance applications that are database-intensive and have interactive, browser-based clients\nWorking with SaaS based data management products will be an added advantage\nProficiency and expertise in Cloudera / Hortonworks\nSpark\nHDF and NiFi\nRDBMS, NoSQL like Vertica, Redshift, Data Modelling with physical design and SQL performance optimization\nMessaging systems, JMS, Active MQ, Rabbit MQ, Kafka\nBig Data technology like Hadoop, Spark, NoSQL based data-warehousing solutions\nData warehousing, reporting including customization, Hadoop, Spark, Kafka, Core java, Spring/IOC, Design patterns\nBig Data querying tools, such as Pig, Hive, and Impala\nOpen-source technologies and databases (SQL & NoSQL)\nProficient understanding of distributed computing principles\nAbility to solve any ongoing issues with operating the cluster\nScale data pipelines using open-source components and AWS services\nCloud (AWS), provisioning, capacity planning and performance analysis at various levels\nWeb-based SOA architecture implementation with design pattern experience will be an added advantage","Databases, Big Data, Etl"
Big Data Architect,Persistent,10-13 Years,,Bengaluru,Information Technology,"We are looking for a Data Architect with creativity and results-oriented critical thinking to meet complex challenges and develop new strategies for acquiring, analyzing, modeling and storing data.\nIn this role you will guide the company into the future and utilize the latest technology and information management methodologies to meet our requirements for effective logical data modeling, metadata management and database warehouse domains. You will be working with experts in a variety of industries, including computer science and software development, as well as department heads and senior executives to integrate new technologies and refine system performance.\nWe reward dedicated performance with exceptional pay and benefits, as well as tuition reimbursement and career growth opportunities.\nWhat Youll Do\nDefine data retention policies\nMonitor performance and advise any necessary infrastructure changes\nMentor junior engineers and work with other architects to deliver best in class solutions\nImplement ETL / ELT process and orchestration of data flows\nRecommend and drive adoption of newer tools and techniques from the big data ecosystem\nExpertise Youll Bring\n10+ years in industry, building and managing big data systems\nBuilding, monitoring, and optimizing reliable and cost-efficient pipelines for SaaS is a must\nBuilding stream-processing systems, using solutions such as Storm or Spark-Streaming\nDealing and integrating with data storage systems like SQL and NoSQL databases, file systems and object storage like s3\nReporting solutions like Pentaho, PowerBI, Looker including customizations\nDeveloping high concurrency, high performance applications that are database-intensive and have interactive, browser-based clients\nWorking with SaaS based data management products will be an added advantage\nProficiency and expertise in Cloudera / Hortonworks\nSpark\nHDF and NiFi\nRDBMS, NoSQL like Vertica, Redshift, Data Modelling with physical design and SQL performance optimization\nMessaging systems, JMS, Active MQ, Rabbit MQ, Kafka\nBig Data technology like Hadoop, Spark, NoSQL based data-warehousing solutions\nData warehousing, reporting including customization, Hadoop, Spark, Kafka, Core java, Spring/IOC, Design patterns\nBig Data querying tools, such as Pig, Hive, and Impala\nOpen-source technologies and databases (SQL & NoSQL)\nProficient understanding of distributed computing principles\nAbility to solve any ongoing issues with operating the cluster\nScale data pipelines using open-source components and AWS services\nCloud (AWS), provisioning, capacity planning and performance analysis at various levels\nWeb-based SOA architecture implementation with design pattern experience will be an added advantage","Databases, Big Data, cloud platform, Sql"
Data Engineer - Architect,HR Addons,10-13 Years,,Pune,Information Technology,"Job Description: The Senior Data & AI Architect designs, develops, and implements complex data architectures, machine learning (ML) models, and AI-driven solutions using Azure (preferred), AWS, and GCP. This role requires hands-on expertise in data engineering, ML model development, big data frameworks, and cloud infrastructure. The architect will manage large-scale data processing, optimize ML pipelines, and deploy AI solutions while ensuring security, scalability, and performance.\nKey Responsibilities:\nData & AI Architecture Design:\nDesign scalable, reliable data pipelines using Azure Data Factory, Synapse, and Data Lake.\nArchitect ML models with cloud integration for business use cases using Azure ML.\nDesign data lakes/warehouses for batch and real-time processing.\nIntegrate AI models with real-time data systems (Azure Event Hub, Stream Analytics).\nCloud Architecture:\nBuild AI solutions using Azure AI services (Cognitive Services, ML, etc.).\nImplement strong data governance with Azure Data Catalog, Policy, and Security Center.\nEnsure high availability, fault tolerance, and disaster recovery using Azure services.\nPerformance Optimization:\nOptimize distributed data processing with Apache Spark on Azure Databricks.\nImprove query performance in Synapse with partitioning, indexing, and caching.\nOptimize ML model efficiency and inference times using Azure ML AutoML.\nML Lifecycle Management:\nImplement CI/CD pipelines for ML models using Azure DevOps.\nApply MLOps for model monitoring, retraining, and lifecycle management.\nEnsure scalable model serving with Azure Kubernetes Service (AKS).\nReal-time Data & Integration:\nDeploy real-time data pipelines with Azure Event Hub, Stream Analytics, and Kafka.\nDesign data integration strategies using Azure Data Factory and APIs.\nAI Solution Delivery:\nLead AI product development (recommendation engines, predictive analytics).\nDrive end-to-end AI solution delivery, integrating models into business workflows.\nCandidate Profile:\n10+ years of experience in data architecture, data engineering, AI/ML, and cloud computing.\nExpertise in Azure (Data Factory, Synapse, ML, AKS); AWS/GCP experience is a plus.\nStrong knowledge of big data frameworks (Apache Spark, Databricks), Python, SQL, Terraform, and Kubernetes.\nKey Attributes:\nExcellent problem-solving skills with a focus on optimizing performance, cost, and scalability.\nExperience designing secure, cloud-native AI systems.\nAbility to manage complex challenges independently in a fast-paced environment.","AI Architecture Design, Synapse, AI/ML, Google Cloud Platform, Cloud Architecture, Scala, Kafka, Apache, Data Architecture, Spark, Azure, Python, Kubernetes"
Solution Cloud Data Architect,CIGNEX Technologies Private Limited,12-18 Years,,Bengaluru,"Recruiting, Staffing Agency","Responsibilities\nDevelop and deliver detailed technology solutions through consulting project activities.\nEvaluate and recommend emerging cloud data technologies and tools to drive innovation and competitive advantage, with a focus on Azure services such as Azure Data Lake, Azure Synapse Analytics, and Azure Databricks.\nLead the design and implementation of cloud-based data architectures using Azure and Databricks to support the company's strategic initiatives and exploratory projects.\nCollaborate with cross-functional teams to understand business requirements, architect data solutions, and drive the development of innovative data platforms and analytics solutions.\nDefine cloud data architecture standards, best practices, and guidelines to ensure scalability, reliability, and security across the organization.\nDesign and implement data pipelines, ETL processes, and data integration solutions to ingest, transform, and load structured and unstructured data from multiple sources into Azure data platforms.\nProvide technical leadership and mentorship to junior team members, fostering a culture of collaboration, continuous learning, and innovation in cloud data technologies.\nCollaborate with Azure and Databricks experts within the organization and the broader community to stay abreast of the latest developments and best practices in cloud data architecture and analytics.","snowflake, Solutioning Cloud, Azure Data, Azure Data Bricks"
Azure Data Engineer/Architect,Quinnox Consultancy Services Limited,12-18 Years,,"Bengaluru, Mumbai",Software,"We are seeking a highly skilled and experiencedAzure Data Engineerto join our team. The ideal candidate will have a strong background in cloud-based data integration, data transformation, and analytics solutions, with a focus on Azure services. This role involves designing, implementing, and maintaining robust data pipelines and analytics frameworks to support business intelligence and advanced analytics initiatives.\nKey Responsibilities:\nData Engineering and Development:\nDesign and implement scalable and efficient ETL/ELT processes usingAzure Data FactoryandAzure Databricks (PySpark).\nManage and optimize data storage and access inAzure Data Lake Storage.\nData Architecture:\nDevelop and maintain aLakehouse architecturefor data analytics and reporting.\nEnsure best practices in data organization, partitioning, and access control in Azure Data Lake.\nCollaborate on the design and implementation ofMicrosoft Fabric architectureto enhance data accessibility and governance.\nData Analysis and Reporting:\nCreate and optimize reports and dashboards usingPower BIto enable data-driven decision-making.\nDevelop T-SQL scripts and stored procedures for data transformation and analysis.\nCloud Infrastructure and DevOps:\nLeverage knowledge ofAzure IaaS servicesfor data platform setup and maintenance.\nImplement and manage CI/CD pipelines usingAzure DevOpsto automate data pipeline deployments.\nTechnical Expertise and Collaboration:\nProvide strong technical guidance inSpark architectureand ensure best practices for PySpark code development.\nCollaborate with cross-functional teams to align on data strategies and solutions.\nTroubleshoot and resolve complex data engineering challenges efficiently.\nMandatory Skills:\nProficiency inAzure Data Factory,Azure Data Lake Storage, andAzure Databricks (PySpark).\nStrong understanding ofETL and ELTprinciples.\nDeep knowledge ofLakehouse architectureand its implementation.\nExpertise inPySparkandSpark architecture.\nSolid understanding ofAzure Data Lake architectureand access control mechanisms\nStrong command ofT-SQLfor advanced querying and data manipulation..\nGood to Have:\nExperience withPower BIfor visualization and reporting.\nFamiliarity withMicrosoft Fabric architecture.\nKnowledge ofAzure IaaS services.\nUnderstanding ofCI/CD processesand tools likeAzure DevOps.\nQualifications:\nBachelors degree in Computer Science, Information Technology, or a related field.\n5+ years of experience in data engineering with a focus on Azure services.\nProven expertise in designing and implementing data pipelines and architectures in cloud environments.\nStrong problem-solving skills and ability to work collaboratively in a team.\nPreferred Certifications:\nMicrosoft Certified: Azure Data Engineer Associate.\nMicrosoft Certified: Azure Solutions Architect Expert.\nMicrosoft Certified: Power BI Data Analyst Associate","Architect, Power Bi, Azure"
Sr Data Architect,PHOTON,8-13 Years,,Chennai,Information Technology,"Job Summary\nExperience Architecting Enterprise Data Platforms for large Enterprises\nEvolved point of view and wide ranging experience in Data Governance,. Data Quality, Data Lineage\nVery strong, hands-on expertise in Data Supply Chain, Data Pipelines\nStrong experience in Google Cloud, Cloud Data Platforms\nExperience creating Data Analytics workspaces\nLeading and Guiding the team on all aspects of Data Engineering, Data Analytics\nExperience working Banking Domain, especially Private Wealth Management strongly preferred.\nTech Skill Sets: Google Cloud Data Platform, NoSQL\nAbInitio\nReporting: Looker, Tableau, Cognos","Google Cloud Data, Cloud Data Platforms, Nosql, Abinitio, Tableau"
Azure Data Lake Architect,Expleo Solutions Limited,10-13 Years,,Chennai,Information Technology,"Job Title : Azure Data Lake Architect\nExperience:10+ Years\nWork Location Candidate should work in Client Location (Egmore - Chennai) (Monday to Friday) (General Shift Timings) (100% Compulsory Work From Office)\nJob Description :\nAzure Data Lake\nAzure Databricks\nAzure Data factory\nAzure Synapse Analytics\nBlob Storage\nETL\nSQL\nJob Overview:\nWe are seeking a highly skilled and motivated individual for the role of Azure Data Lake Architect. You will be responsible for the end-to-end management and optimization of our Azure-based data lake ecosystem. This includes overseeing ETL jobs in ADF, Azure Data Lake, Azure Databricks, Azure Synapse Analytics, Blob Storage to ensure a robust and efficient data management infrastructure.\nPreferred Qualifications:\nMinimum 5 to 10 years of relevant experience\nProven experience in designing, implementing, and managing Azure Data Lake solutions.\nStrong expertise in ETL processes using ADF, Azure Databricks, Azure Synapse Analytics, Blob Storage, Proficient in SQL,\nExcellent problem-solving and troubleshooting skills.\nAbility to work collaboratively in a team environment and communicate effectively with stakeholders.\nDesirable to have:\nExpertise in Python programming language with Pandas and NumPy\nCertifications: Microsoft Certified: Azure Data Engineer Associate, Microsoft Certified: Azure Enterprise Data Analyst Associate","Azure Data Lake, Azure Databricks, Azure Data Factory, Sql, Etl"
Data Engineer - Architect,HR Addons,10-13 Years,,Pune,Information Technology,"Job Description: The Senior Data & AI Architect designs, develops, and implements complex data architectures, machine learning (ML) models, and AI-driven solutions using Azure (preferred), AWS, and GCP. This role requires hands-on expertise in data engineering, ML model development, big data frameworks, and cloud infrastructure. The architect will manage large-scale data processing, optimize ML pipelines, and deploy AI solutions while ensuring security, scalability, and performance.\nKey Responsibilities:\nData & AI Architecture Design:\nDesign scalable, reliable data pipelines using Azure Data Factory, Synapse, and Data Lake.\nArchitect ML models with cloud integration for business use cases using Azure ML.\nDesign data lakes/warehouses for batch and real-time processing.\nIntegrate AI models with real-time data systems (Azure Event Hub, Stream Analytics).\nCloud Architecture:\nBuild AI solutions using Azure AI services (Cognitive Services, ML, etc.).\nImplement strong data governance with Azure Data Catalog, Policy, and Security Center.\nEnsure high availability, fault tolerance, and disaster recovery using Azure services.\nPerformance Optimization:\nOptimize distributed data processing with Apache Spark on Azure Databricks.\nImprove query performance in Synapse with partitioning, indexing, and caching.\nOptimize ML model efficiency and inference times using Azure ML AutoML.\nML Lifecycle Management:\nImplement CI/CD pipelines for ML models using Azure DevOps.\nApply MLOps for model monitoring, retraining, and lifecycle management.\nEnsure scalable model serving with Azure Kubernetes Service (AKS).\nReal-time Data & Integration:\nDeploy real-time data pipelines with Azure Event Hub, Stream Analytics, and Kafka.\nDesign data integration strategies using Azure Data Factory and APIs.\nAI Solution Delivery:\nLead AI product development (recommendation engines, predictive analytics).\nDrive end-to-end AI solution delivery, integrating models into business workflows.\nCandidate Profile:\n10+ years of experience in data architecture, data engineering, AI/ML, and cloud computing.\nExpertise in Azure (Data Factory, Synapse, ML, AKS); AWS/GCP experience is a plus.\nStrong knowledge of big data frameworks (Apache Spark, Databricks), Python, SQL, Terraform, and Kubernetes.\nKey Attributes:\nExcellent problem-solving skills with a focus on optimizing performance, cost, and scalability.\nExperience designing secure, cloud-native AI systems.\nAbility to manage complex challenges independently in a fast-paced environment.","AI Architecture Design, Synapse, AI/ML, Google Cloud Platform, Cloud Architecture, Scala, Kafka, Apache, Data Architecture, Spark, Azure, Python, Kubernetes"
Solution Cloud Data Architect,CIGNEX Technologies Private Limited,12-18 Years,,Bengaluru,"Recruiting, Staffing Agency","Responsibilities\nDevelop and deliver detailed technology solutions through consulting project activities.\nEvaluate and recommend emerging cloud data technologies and tools to drive innovation and competitive advantage, with a focus on Azure services such as Azure Data Lake, Azure Synapse Analytics, and Azure Databricks.\nLead the design and implementation of cloud-based data architectures using Azure and Databricks to support the company's strategic initiatives and exploratory projects.\nCollaborate with cross-functional teams to understand business requirements, architect data solutions, and drive the development of innovative data platforms and analytics solutions.\nDefine cloud data architecture standards, best practices, and guidelines to ensure scalability, reliability, and security across the organization.\nDesign and implement data pipelines, ETL processes, and data integration solutions to ingest, transform, and load structured and unstructured data from multiple sources into Azure data platforms.\nProvide technical leadership and mentorship to junior team members, fostering a culture of collaboration, continuous learning, and innovation in cloud data technologies.\nCollaborate with Azure and Databricks experts within the organization and the broader community to stay abreast of the latest developments and best practices in cloud data architecture and analytics.","snowflake, Solutioning Cloud, Azure Data, Azure Data Bricks"
Azure Data Engineer/Architect,Quinnox Consultancy Services Limited,12-18 Years,,"Bengaluru, Mumbai",Software,"We are seeking a highly skilled and experiencedAzure Data Engineerto join our team. The ideal candidate will have a strong background in cloud-based data integration, data transformation, and analytics solutions, with a focus on Azure services. This role involves designing, implementing, and maintaining robust data pipelines and analytics frameworks to support business intelligence and advanced analytics initiatives.\nKey Responsibilities:\nData Engineering and Development:\nDesign and implement scalable and efficient ETL/ELT processes usingAzure Data FactoryandAzure Databricks (PySpark).\nManage and optimize data storage and access inAzure Data Lake Storage.\nData Architecture:\nDevelop and maintain aLakehouse architecturefor data analytics and reporting.\nEnsure best practices in data organization, partitioning, and access control in Azure Data Lake.\nCollaborate on the design and implementation ofMicrosoft Fabric architectureto enhance data accessibility and governance.\nData Analysis and Reporting:\nCreate and optimize reports and dashboards usingPower BIto enable data-driven decision-making.\nDevelop T-SQL scripts and stored procedures for data transformation and analysis.\nCloud Infrastructure and DevOps:\nLeverage knowledge ofAzure IaaS servicesfor data platform setup and maintenance.\nImplement and manage CI/CD pipelines usingAzure DevOpsto automate data pipeline deployments.\nTechnical Expertise and Collaboration:\nProvide strong technical guidance inSpark architectureand ensure best practices for PySpark code development.\nCollaborate with cross-functional teams to align on data strategies and solutions.\nTroubleshoot and resolve complex data engineering challenges efficiently.\nMandatory Skills:\nProficiency inAzure Data Factory,Azure Data Lake Storage, andAzure Databricks (PySpark).\nStrong understanding ofETL and ELTprinciples.\nDeep knowledge ofLakehouse architectureand its implementation.\nExpertise inPySparkandSpark architecture.\nSolid understanding ofAzure Data Lake architectureand access control mechanisms\nStrong command ofT-SQLfor advanced querying and data manipulation..\nGood to Have:\nExperience withPower BIfor visualization and reporting.\nFamiliarity withMicrosoft Fabric architecture.\nKnowledge ofAzure IaaS services.\nUnderstanding ofCI/CD processesand tools likeAzure DevOps.\nQualifications:\nBachelors degree in Computer Science, Information Technology, or a related field.\n5+ years of experience in data engineering with a focus on Azure services.\nProven expertise in designing and implementing data pipelines and architectures in cloud environments.\nStrong problem-solving skills and ability to work collaboratively in a team.\nPreferred Certifications:\nMicrosoft Certified: Azure Data Engineer Associate.\nMicrosoft Certified: Azure Solutions Architect Expert.\nMicrosoft Certified: Power BI Data Analyst Associate","Architect, Power Bi, Azure"
Azure Data Engineer/Architect,Quinnox Consultancy Services Limited,12-18 Years,,"Bengaluru, Mumbai",Information Technology,"Description\nWe are looking for an experienced Azure Data Engineer/Architect to join our team. The ideal candidate should have 12-18 years of experience in the job market context of India and should be well-versed in Azure cloud technologies. The candidate should have strong technical skills, a problem-solving mindset, and an ability to work independently and as part of a team.\nResponsibilities\nDesign and build scalable data pipelines on Azure cloud platform\nCreate and maintain data models, data flows, and data integration processes\nDevelop and maintain ETL processes\nDesign and implement data security and compliance policies\nImplement and maintain data quality standards\nCollaborate with cross-functional teams to identify and solve complex data-related problems\nProvide technical guidance to junior team members\nStay up-to-date with emerging data technologies and trends\nSkills and Qualifications\n12-18 years of experience in data engineering and architecture\nExpertise in Azure cloud platform, including Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Azure SQL Database\nStrong programming skills in Python and/or Scala\nExperience in data modeling, data warehousing, and data integration\nExperience in ETL development and maintenance\nKnowledge of data security and compliance policies\nExperience in implementing data quality standards\nExcellent problem-solving and analytical skills\nAbility to work independently and as part of a team\nExcellent communication and interpersonal skills\nBachelor's or Master's degree in Computer Science or a related field","Nosql, Data Migration, Data Security, Data Integration, Data Modeling, Data Governance, Data Warehousing, Azure, Sql, Etl"
Data Solution Architect (Data Engineering),Amgen Inc,7-11 Years,,Hyderabad,Pharmaceutical,"Design and implement scalable, modular, and future-proof data architectures that support enterprise data lakes, data warehouses, and real-time analytics.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have Skills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\nDoctorate Degree with 6-8 + years of experience in Computer Science, IT or related field\nMaster's degree with 8-10 + years of experience in Computer Science, IT or related field\nBachelor's degree with 10-12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.","CI/CD, Data Architecture, Apache Spark, Databricks, Data Governance, Etl"
Data Modeller/Architect,Coforge,10-16 Years,,"Hyderabad, Noida, Pune",Information Technology,"Design and develop data models, architecture, and metadata for large-scale enterprise systems using Erwin tools.\nCollaborate with cross-functional teams to identify business requirements and translate them into technical designs.\nDevelop complex data models using Snowflake as the primary database management system.\nEnsure compliance with industry standards, best practices, and company policies for data governance.\nJob Requirements :\n10-16 years of experience in Data Modeling, Architecture, or related field.\nStrong expertise in Erwin tools (e.g., Erwin Modeler) for designing and developing data models.\nProficiency in Snowflake as a primary database management system.\nExperience working on large-scale enterprise projects involving multiple stakeholders.","enterprise systems, Architecture, snowflake, Erwin, Data Modeling, Data Governance"
AWS Dig Data Solution Architect,Birlasoft Limited,15-20 Years,,"Bengaluru, Noida",Software,"As an Amazon Web Services (AWS) certified solutions architect you would work as cloud network specialist who designs, creates, and maintains their employer's AWS-based services. Companies can use Amazon Web Services to run many business systems, such as analytics, security, storage, or database services.\nTitle: AWS Dig Data Solution Architect\nLocation: Pune/Mumbai/Bangalore/Noida\nEducational Background : Masters/Professional Degree\nKey Responsibilities :\nDesign and architect end to end solutions on AWS and create the HLD and LLD documents\nExperience in real-time Data Ingestion and Processing\nHands-on experience in AWS S3, Kafka, Glue, PySpark, Python, Redshift, AWS Cloud best practices and optimization\nExperience with integration of different data sources with Data Lake is required\nExperience in creating data lakes for Reporting, AI and Machine Learning\nExperience of data modelling and data architecture concepts\nGood in Creating Technical Specifications and Data Flow document\nTo be able to clearly articulate pros and cons of various technologies and platforms\nExperience in create the Technical Specification Design.\nSkills Required:\n15 years of IT experience with 4+ years of AWS Big Data Services Architecture experience\nClient facing experience.\nIn-depth knowledge of domain Industry and business environment\nAnalytical and problem-solving capabilities","AWS Dig Data Solution Architect, Pyspark, AWS"
AWS Dig Data Solution Architect,Birlasoft Limited,15-20 Years,,"Mumbai, Pune",Software,"As an Amazon Web Services (AWS) certified solutions architect you would work as cloud network specialist who designs, creates, and maintains their employer's AWS-based services. Companies can use Amazon Web Services to run many business systems, such as analytics, security, storage, or database services.\nTitle: AWS Dig Data Solution Architect\nLocation: Pune/Mumbai/Bangalore/Noida\nEducational Background : Masters/Professional Degree\nKey Responsibilities :\nDesign and architect end to end solutions on AWS and create the HLD and LLD documents\nExperience in real-time Data Ingestion and Processing\nHands-on experience in AWS S3, Kafka, Glue, PySpark, Python, Redshift, AWS Cloud best practices and optimization\nExperience with integration of different data sources with Data Lake is required\nExperience in creating data lakes for Reporting, AI and Machine Learning\nExperience of data modelling and data architecture concepts\nGood in Creating Technical Specifications and Data Flow document\nTo be able to clearly articulate pros and cons of various technologies and platforms\nExperience in create the Technical Specification Design.\nSkills Required:\n15 years of IT experience with 4+ years of AWS Big Data Services Architecture experience\nClient facing experience.\nIn-depth knowledge of domain Industry and business environment\nAnalytical and problem-solving capabilities","AWS Dig Data Solution Architect, Pyspark, AWS"
Sr Data Migration Architect (Windchill),Birlasoft Limited,3-5 Years,,Bengaluru,Software Engineering,"Educational Background\nUG. - B. Tech /B. E in any specialization\nPG. - MCA/MSC/MTech in Computers\nKey Responsibilities -\nHands-on experience as Data Migration Architect with PLM Enterprise Systems, mainly Windchill and Thing Worx.\nHas strong experience as a Solution Architect as well for Windchill and Thing Worx applications.\nStrong Experience in UDI and other Medical Devices aspects.\nStrong Experience in Medical Devices Industry with deep insights into regulatory, change control, compliances, and other nuggets.\nHas done windchill migrations on cloud as a target system.\nWhile being hands-on for Migration on Windchill PLM, the person also can help with data migration strategies; also show and tell the customer on best practices and strategies.\nStrong hands-on with data migration and has handled large business transformation programs.\nHas worked directly with onsite and offshore teams from execution standpoint.\nShould be expert in Windchill Migration using Windchill Bulk Migrator (WBM) - at least have executed 3-4 Windchill migration project using WBM.\nShould be expert in WBM tool execution (Extraction, Transformation & Loading).\nSkills Required -\nExperience in data migration including CAD Data migration.\nExperience in at least one non-Windchill to Windchill data migration.\nShould have good understanding of Windchill Architecture, database etc.\nShould have good understanding of Windchill object models, relationships, content.\nShould have experience on working with Customer for Migration Requirements Gathering, Source Data Analysis and Data Mapping.\nScripting Knowledge on Database - Oracle/SQL Server with large data set analysis.","Data Migration Architect, Windchill, Windchill PLM"
GCP Data architect,EAGateway Services India Private Limited,10-19 Years,,Remote,Information Technology,"Dear Candidate,\nWe identified your profile as a Job Seeker.\nWe found your profile suitable for one of Permanent Position.\nIf you are interested and the JD suits your profile, Please revert with the details requested below along with your latest CV and we will process your candidature.\nPlease Share below details:-\nLinkedIn ID:\nTotal Experience:\nRelevant Hands-on Experience in:\nCurrent Salary (Fixed + Variables):\nExpected Salary:\nCurrent Company:\nPayroll Company:\nAre you an Immediate joiner / Serving any Notice Period:\nOfficial Notice Period:\nLast working day, if you have already resigned:\nReason for Relieving:\nCurrent Location:\nPreferred Location :\nIs there any OFFER in your Hand:\nCould you please share your relieving letter or Resignation acceptance screenshot for Confirmation.\nJOB DESCRIPTION:\nRole: Data Architect (GCP)\nTotal Exp: 10+ yrs.\nLocation: Remote\nJob Type: Permanent with Lingaro\nNotice Period: Immediate- 30 Days\nAbout Lingaro:\nLingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.\nSince 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.\nAbout DS/AI Competency Center:\nFocuses on leveraging data, analytics, and artificial intelligence (AI) technologies to extract insights, build predictive models, and develop AI powered solutions. Utilizes Exploratory Data Analysis, Statistical Modeling and Machine Learning, Model Deployment, and Integration as well as Model Monitoring and Maintenance. Delivers business solutions using multiple AI techniques and tools.\nWebsite:\nhttps://lingarogroup.com/\nMandatory :\nGCP cloud platform (at least 6+ years), Data modelling (5+ years)\nExperience in Dataflow, Cloud composer, Pub/Sub, Cloud storage, Big query\nRequirements:\nBachelor's or master's degree in computer science, Information Systems, or a related field.\n10+ years experience as a Data Architect or in a similar role, ideally in a complex organizational setting.\nStrong understanding of data management principles, data modeling techniques, database design and data integration flows.\nExperience in developing and implementing data governance policies, procedures, and frameworks to ensure data integrity and compliance.\nFamiliarity with industry best practices and emerging trends in data management and governance.\nAbility to design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.\nStrong understanding of data modeling best-practices, e.g. data normalization, denormalization, generalization, and performance optimization techniques.\nExpertise in working with various database technologies, such as relational databases (e.g., Oracle, SQL Server, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB).\nKnowledge of database management systems, data warehousing, data integration technologies, and Big Data solutions.\nFamiliarity with cloud-based database, warehouse, and lakehouse platforms.\nExperience in designing and implementing data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, understanding of data integration patterns and best practices.\nUnderstanding of Business Intelligence and data analytics requirements, optimization of data storage and processing for reporting needs.\nExcellent communication and presentation skills to effectively articulate complex technical concepts to both technical and non-technical stakeholders.\nAbility to collaborate with cross-functional teams, including business analysts, developers, and data scientists, to understand requirements and drive data-related initiatives.\nStrong analytical and problem-solving abilities to identify data-related issues, propose solutions, and make data-driven decisions.\nFamiliarity with data profiling, data cleansing, data harmonization, and data quality assessment techniques.\nKnowledge of data security and privacy regulations, such as GDPR or CCPA, understanding of data encryption, access controls, and data masking techniques to ensure the security of sensitive data.\nProfessional certification in data management or related field would be advantageous.\nThanks & Regards\nNikhil Dasyam\nEAGateway Services India Pvt Ltd\nCell: +91 6305027687\nemail: nikhil@eagateways.net |Web: http://www.eagateways.net","Pub/Sub, Cloud composer, BigQuery, Gcp, Data Modeling, Cloud Storage, Data Architect, DataFlow"
Azure Data Architect,Nityo Infotech Services Pte. Ltd,5-10 Years,,India,IT/Computers - Software,"Job Description\nAzure Data Architect: 1. Minimum of 5 years of experience in designing and implementing data solutions using Microsoft Azure. 2. Experience in data modeling, integration, and performance optimization. 3. Familiarity with Azure security features and cloud migration. 4. Thorough understanding and working knowledge of Azure data services, such as Azure Data Lake Storage, Azure SQL Database, and Azure Synapse Analytics, Azure Data Factory (ADF). 5. Security features of Azure data services and ability to design and implement security solutions to ensure that data is protected from unauthorized access. 6. Must be able to migrate on-premises data to the cloud and capable of planning, designing, and implementing the migration process while minimizing downtime.\nExperience Required\n5-10 years\nIndustry Type\nIT\nEmployment Type\nPermanent","Azure Data Lake Storage, Azure security features, performance optimization, Azure SQL Database, Cloud Migration, Azure Synapse Analytics, Integration, Microsoft Azure, Security Solutions, Data Modeling"
Enterprise Data Architect - Ramboll Tech,Ramboll,5-7 Years,,"Chennai, India",Login to check your skill match score,"Company Description\n\nAbout Ramboll\n\nFounded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.\n\nEquality, diversity, and inclusion are at the heart of what we do\n\nWe believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.\n\nJob Description\n\nWere passionate about using data to drive sustainability. You too Join us as an Enterprise Data Architect at Ramboll Tech, and shape our architecture to be data centric.\n\nWe are looking for a highly skilled and experienced Data Architect to join our team. You will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth. This role is vital to ensuring the seamless integration, management and deployment of data and analytics solutions that support our business needs. You will identify, analyse and proactively recommend how information assets drive business outcomes, to share consistent data throughout Ramboll.\n\nYou will be a part of our exciting digital transformation journey and play an active role in turning our data ambitions into concrete actions to deliver optimisations and revenue growth targets!\n\nWhat You Will Do\n\nYou will join Technology & Data Architecture in Ramboll Tech. You will be working across Ramboll, with focus on enterprise data layer, in collaboration with Domain Enterprise Architects, Data Strategy and Data Platform teams. You will partner with Innovation and Digital Transformation Directorswho drive digitalisation, innovation, and scaling of digital solutions & products across their respective business domains.\n\nAs our Enterprise Data Architect, you will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth.\n\nYour focus will be on delivering value by shaping data strategies, roadmaps, and solutions that directly address the challenges and opportunities of our business areas. This role requires expertise in modern data management and analysis technologies, alongside a deep understanding of corporate data management best practices.\n\nAs we operate in Architecture, Engineering and Consulting (AEC), you can expect a considerable focus on Building Information Modelling (BIM). We are looking for a candidate eager to engage in applying data-centric principles to BIM data to leverage generating valuable business insights.\n\nWhere you will make an impact:\n\nBusiness needs: Translate complex business requirements into robust, scalable, and secure data architectures that are aligned with enterprise-wide data strategy. We want to unlock new revenue streams and optimise operational efficiency, by leveraging our Enterprise Data Architecture to transform our data into valuable business assets, and implement Al-enabled solutions.\nDesign and implement modern data architectures: Lead the design and implementation of scalable, robust and secure data systems using modern data stack technologies. Ensure architecture is aligned with business objectives.\nImprove and ensure the overall quality of data, including ease of access and use of data assets. Define information and data strategies as a part of an overall business and technology strategy. Design and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.\nSupport data platform teams: Work with data platform teams to design and implement operating models that support efficient data processing, integrations, reporting and AI model deployment and execution.\nData modelling: Design and develop data models that support business processes, analytics and reporting requirements. Ensure that data models are optimised for performance and scalability.\nTechnology roadmaps: Actively engage with technology providers to understand critical, data relevant, technology roadmaps. Explore innovative solutions and evaluate their alignment with business goals and enterprise-wide data strategy\n\nKey responsibilities:\n\nWork with solution architects, developers and engineers to operationalise Data Strategy and build robust and scalable technology stacks and platforms for D&A solutions\nPartner with our Domain Enterprise Architects to understand business objectives. Develop and manage cloud-based data solutions on platforms such as Microsoft Azure, GCP or AWS. Optimise cloud infrastructure for performance, cost and scalability.\nDesign and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.\nWork closely with cross-functional teams, including business stakeholders, data scientists, data engineer and other SME colleagues in Ramboll Tech, to understand data requirements and deliver solutions that meet business needs.\nMaintaining repositories for representing the data elements including the entities, relationships and attributes, the information flows, and business glossary\nProvide mentorship and guidance to junior data engineers and other team members. Foster a culture of continuous learning and improvement within the team\nContinuously evaluate and recommend new tools and technologies that can improve the efficiency and effectiveness of data engineering processing\nCoach and mentor other architects, product teams and business stakeholders to instil architectural thinking, with focus on business, data/information and solution architecture.\nEffectively provide guiding principles, standard and minimal viable architectures, technology governance model informed by business strategy and corporate governance.\n\nQualifications\n\nEducation:\n\nBachelors or Masters degree in Computer Science, Engineering, or a related field.\n\nExperience:\n\n5+ years of professional experience in data architecture, with a strong focus on cloud architecture, data integration and data modelling.\nDeep understanding of the modern data stack and its components.\nProven experience with cloud platforms such as Microsoft Azure, GCP or AWS.\nPrevious engagement in establishing data strategy and data governance.\nExperience in leading data modelling, database design, data warehousing, master data management, data governance practices. Strong skills in data modelling, ETL processes and data integration, having worked with data platform teams.\nExperience in securing and protection of sensitive information\nExperience in AEC industry is great to have. An ideal candidate will have a strong understanding of Building Information Modelling (BIM) principles and applications. Applied data-centric principles to BIM data, enabling integration with enterprise systems (e.g., improved project management, cost control, and facilities management). Developed data pipelines to extract, transform, and load BIM data for analysis and reporting. Explored opportunities to leverage BIM data for generating valuable insights, predictive and prescriptive maintenance\n\nSkills:\n\nExceptional analytical and problem-solving skills with the ability to design innovative solutions to complex data challenges.\nData Modelling (Conceptual, Logical, Physical), Data Warehousing, Data Lakes, Data Mesh, Data Governance, Metadata Management, Master Data Management.\nFamiliarity with the DAMA Data Management Framework (DAMA-DMBOK) and its application in managing data as a strategic asset\nUnderstanding of BIM principles, data structures (IFC), BIM software APIs, and BIM data integration with enterprise systems. (Great to have skill)\nAdvanced knowledge of data modelling tools, data lakes, SQL, and pipeline design\nExcellent communication and interpersonal skills, with the ability to convey technical concepts to non-technical stakeholders\nA strategic mindset with the ability to navigate and influence within a matrixed organization\nProven ability to lead projects and mentor junior team members\nRelevant certifications in cloud technologies are a plus, such as: Microsoft Certified Azure Solutions Architect Expert, or Data Engineering certifications, Snowflake certifications, Databricks certifications, Kafka certifications\n\nWe encourage you to apply even if your profile does not meet all the requirements for the role. We are looking for a diverse range of experiences, skills, and interests to enrich our team.\n\nAdditional Information\n\nWhat defines us: CURIOSITY, OPTIMISM, AMBITION, EMPATHY\n\nOur team at Ramboll Tech is currently on a steep growth trajectory while maintaining a strong team culture.\n\nWe are curious about other people and their motivations; about new business models and technologies; about each other and the future.\nWe are optimistic, focusing on solutions rather than problems; we plan for success and are willing to take calculated risks instead of playing it safe.\nWe are ambitious, setting our own standards higher than others expectations, and we celebrate each other&aposs successes.\nWe are empathetic, taking ourselves, others, and each other seriously without prejudgment, and we help each other and our clients, colleagues, and the world.\n\nHow We Work As a Team\n\nOur team culture is crucial to us; that&aposs why we take time daily to exchange ideas and discuss our work priorities. We support each other when facing challenges and foster a strong team spirit. We aim to learn and grow continuously from one another. We value diversity, and although we&aposre not perfect, we regularly engage in open discussions about how we can improve in this area.\n\nOur current hybrid work approach focuses on adapting to different needs, including increased flexibility that works best for our global team and individuals, with as much autonomy as possible.\n\nWho Is Ramboll\n\nRamboll is a global architecture, engineering, and consultancy firm. We believe sustainable change&aposs aim is to create a livable world where people thrive in healthy nature. Our strength is our employees, and our history is rooted in a clear vision of how a responsible company should act. Openness and curiosity are cornerstones of our corporate culture, fostering an inclusive mindset that seeks new, diverse, and innovative perspectives. We respect and welcome all forms of diversity and focus on creating an inclusive environment where everyone can thrive and reach their full potential.\n\nWhat Does Ramboll Tech Do\n\nRamboll Tech / GBA Innovation and Digital Transformation accelerates innovation and digital transformation for the entire Ramboll Group/ GBA and directs all AI initiatives within the company. We digitally enable and co-create with Rambolls world-class experts to deliver solutions that drive sustainable change for our clients and society. This includes collaborating with Global Business Areas and Enabling Functions at Ramboll on their digital journey, developing proprietary AI and digital products for Ramboll and our clients, following our product life cycle. Ramboll Tech also works on larger technology projects within Ramboll and provides world-class Technology Operations. Ramboll Tech currently has over 300 employees globally, with strongholds across Nordics, Germany, the USA, and India. We are looking to quickly expand in key areas across Europe and the globe.\n\nImportant Information\n\nWe don&apost require a cover letterjust send us your current CV through our application tool, and we&aposre eager to get to know you better in a conversation.\nDo you have any questions Feel free to contact Head of Technology & Data Architecture: Elvira Janas ([HIDDEN TEXT]). Thanks\nShow more Show less","Data Lakes, ETL processes, Master Data Management, Data Modelling, Data Integration, Data Warehousing, Data Architecture, Data Governance"
Cloud Data Architect,owow,8-16 Years,,"Bengaluru, India",Login to check your skill match score,"Technical Skills:\nSkillsets we are looking for:\n8 to 16 years of working experience in data engineering.\n7+ years exp in PySprak\n7+ years exp in AWS Glue\n7+ years exp in AWS Redshift\n7+ years exp in in AWS CI/CD pipeline like codebuild, codecommit, codedeploy and codepipeline\nStrong proficiency in AWS services such as S3, EC2, EMR, SNS, Lambda, StepFunctions\nExperience implementing automated testing platforms like PyTest\nStrong proficiency in Python, Hadoop, Spark and or PySpark is required\nSkill of writing clean, readable, commented and easily maintainable code\nUnderstanding of fundamental design principles for building a scalable solution\nSkill for writing reusable libraries\nProficiency in understanding code versioning tools such as Git, SVN, TFS etc.,\nInterpersonal skills:\nExcellent communication and collaboration skills.\nAbility as part of a team.\nStrong problem-solving and analytical skills.\nMust have worked with US customers and should have provided at least 3-4 hours overlap with Pacific Time (PT)\nBonus points:\nCertifications in cloud platforms\nShow more Show less","Hadoop, Aws Redshift, Pyspark, Spark, AWS Glue, Python"
Senior Azure Data Architect,MonoSpear Technologies LLC,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"Experience: 7+ Years\nWork Mode: Remote\nWork Type: Contract\nShift Timings: 2 Pm to 11 Pm\nAvailability: Immediate\n\nFreelancers may also apply.\n\nKey Responsibilities:\nData Architecture Design:\nDesign, develop, and maintain the enterprise data architecture, including data models, database schemas, and data flow diagrams.\nDevelop a data strategy and roadmap that aligns with business objectives and ensures the scalability of data systems.\nArchitect both transactional (OLTP) and analytical (OLAP) databases, ensuring optimal performance and data consistency.\nData Integration & Management:\nOversee the integration of disparate data sources into a unified data platform, leveraging ETL/ELT processes and data integration tools.\nDesign and implement data warehousing solutions, data lakes, and/or data marts that enable efficient storage and retrieval of large datasets.\nEnsure proper data governance, including the definition of data ownership, security, and privacy controls in accordance with compliance standards (GDPR, HIPAA, etc.).\nCollaboration with Stakeholders:\nWork closely with business stakeholders, including analysts, developers, and executives, to understand data requirements and ensure that the architecture supports analytics and reporting needs.\nCollaborate with DevOps and engineering teams to optimize database performance and support large-scale data processing pipelines.\nTechnology Leadership:\nGuide the selection of data technologies, including databases (SQL/NoSQL), data processing frameworks (Hadoop, Spark), cloud platforms (Azure is a must), and analytics tools.\nStay updated on emerging data management technologies, trends, and best practices, and assess their potential application within the organization.\nData Quality & Security:\nDefine data quality standards and implement processes to ensure the accuracy, completeness, and consistency of data across all systems.\nEstablish protocols for data security, encryption, and backup/recovery to protect data assets and ensure business continuity.\nMentorship & Leadership:\nLead and mentor data engineers, data modelers, and other technical staff in best practices for data architecture and management.\nProvide strategic guidance on data-related projects and initiatives, ensuring that all efforts are aligned with the enterprise data strategy.\n\nRequired Skills & Experience:\nExtensive Data Architecture Expertise:\nOver 7 years of experience in data architecture, data modeling, and database management.\nProficiency in designing and implementing relational (SQL) and non-relational (NoSQL) database solutions.\nStrong experience with data integration tools (Azure Tools are a must + any other third party tools), ETL/ELT processes, and data pipelines.\nAdvanced Knowledge of Data Platforms:\nExpertise in Azure cloud data platform is a must. Other platforms such as AWS (Redshift, S3), Azure (Data Lake, Synapse), and/or Google Cloud Platform (Big Query, Dataproc) is a bonus.\nExperience with big data technologies (Hadoop, Spark) and distributed systems for large-scale data processing.\nHands-on experience with data warehousing solutions and BI tools (e.g., Power BI, Tableau, Looker).\nData Governance & Compliance:\nStrong understanding of data governance principles, data lineage, and data stewardship.\nKnowledge of industry standards and compliance requirements (e.g., GDPR, HIPAA, SOX) and the ability to architect solutions that meet these standards.\nTechnical Leadership:\nProven ability to lead data-driven projects, manage stakeholders, and drive data strategies across the enterprise.\nStrong programming skills in languages such as Python, SQL, R, or Scala.\n\nCertification:\nAzure Certified Solution Architect, Data Engineer, Data Scientist certifications are mandatory.\n\nPre-Sales Responsibilities:\nStakeholder Engagement: Work with product stakeholders to analyze functional and non-functional requirements, ensuring alignment with business objectives.\nSolution Development: Develop end-to-end solutions involving multiple products, ensuring security and performance benchmarks are established, achieved, and maintained.\nProof of Concepts (POCs): Develop POCs to demonstrate the feasibility and benefits of proposed solutions.\nClient Communication: Communicate system requirements and solution architecture to clients and stakeholders, providing technical assistance and guidance throughout the pre-sales process.\nTechnical Presentations: Prepare and deliver technical presentations to prospective clients, demonstrating how proposed solutions meet their needs and requirements.\n\nAdditional Responsibilities:\nStakeholder Collaboration: Engage with stakeholders to understand their requirements and translate them into effective technical solutions.\nTechnology Leadership: Provide technical leadership and guidance to development teams, ensuring the use of best practices and innovative solutions.\nIntegration Management: Oversee the integration of solutions with existing systems and third-party applications, ensuring seamless interoperability and data flow.\nPerformance Optimization: Ensure solutions are optimized for performance, scalability, and security, addressing any technical challenges that arise.\nQuality Assurance: Establish and enforce quality assurance standards, conducting regular reviews and testing to ensure robustness and reliability.\nDocumentation: Maintain comprehensive documentation of the architecture, design decisions, and technical specifications.\nMentoring: Mentor fellow developers and team leads, fostering a collaborative and growth-oriented environment.\n\nQualifications:\nEducation: Bachelors or masters degree in computer science, Information Technology, or a related field.\nExperience: Minimum of 7 years of experience in data architecture, with a focus on developing scalable and high-performance solutions.\nTechnical Expertise: Proficient in architectural frameworks, cloud computing, database management, and web technologies.\nAnalytical Thinking: Strong problem-solving skills, with the ability to analyze complex requirements and design scalable solutions.\nLeadership Skills: Demonstrated ability to lead and mentor technical teams, with excellent project management skills.\nCommunication: Excellent verbal and written communication skills, with the ability to convey technical concepts to both technical and non-technical stakeholders.",", Looker, R, ELT, Sql, Data Warehousing, Hadoop, Tableau, Power Bi, Etl, Database Management, Nosql, Python, Azure, Scala, Data Modeling, Spark"
"Data Architect (Financial Crime), Director",NatWest Group,Fresher,,"Bengaluru, India",Login to check your skill match score,"Join us as a Data Architect (Financial Crime)\n\nFor someone with a background in defining business and application architectures and roadmaps for complex enterprises, this is an excellent opportunity to join our business. You'll be defining the intentional architecture foryour assigned scope in order to ensure that the current architecture being delivered by engineers best supports the enterprise and its long term strategy\nYou'll provide advisory support and embed governance to ensure projects align to our simplification strategy and comply with data standards while promoting and supporting effective data modelling, metadata management and alignment to enterprise data model and data controls\nWith valuable exposure, you'll be building and leveraging relationships with colleagues across the bank to ensure commercially focused decisions and to create long term value for the bank\nWe're offering this role at director level\n\nWhat you'll do\n\nAs a Data Architect, you'll be defining and communicating the current, resultant and target state data architecture for your assigned scope. You'll advise domain and project teams to ensure alignment with data architecture standards, data principles, data controls, target data architectures, data patterns and the banks simplification strategy.\n\nWe'll look to you to influence the development of business strategies at an organisational level, identifying transformational opportunities for Financial Crime associated with both new and existing data solutions.\n\nAs Well As This, You'll Be\n\nTranslating architecture roadmaps into packages of work that allow frequent incremental delivery of value to be included in product backlog\nDefining, creating and maintaining architecture models, roadmaps, standards and outcomes, using architecture strategies to ensure alignment to adjacent and higher level model\nCollaborate and support data management SMEs and data stewards ensuring metadata management aligns with enterprise data model\nConduct data design reviews to ensure solutions meet data standards, data principles, data controls, data patterns and target data architectures\nSeeking out and utilising continuous feedback, fostering adaptive design and engineering practices to drive the collaboration of programmes and teams around a common technical vision\n\nThe skills you'll need\n\nTo succeed in this role, you'll need expert knowledge of application architecture, and at least one business, data or infrastructure architecture with working knowledge of the remaining disciplines. You'll have excellent communication skills with the ability to clearly communicate complex technical concepts to colleagues, up to senior leadership level, along with a good understanding of Agile methodologies with experience of working in an Agile team.\n\nYou'll Also Demonstrate\n\nGood collaboration and stakeholder management skills\nExperience of developing, syndicating and communicating architectures, designs and proposals for action\nAn understanding of industry architecture frameworks, such as TOGAF and ArchiMate\nExperience in data modelling methodologies (3NF, Dimensional, Data Vault, Star Schema, etc). Knowledge of data related regulations such as GDPR, CCPA,PCI DSS, BCBS 239\nExperience of working with business solution vendors, technology vendors and products within the market\nA background in systems development change life cycles, best practices and approaches\nKnowledge of hardware, software, application and systems engineering","BCBS 239, Systems engineering, archimate, CCPA, data modelling methodologies, Pci Dss, Gdpr, Togaf, Agile Methodologies, Application Architecture, Infrastructure Architecture"
GridOS Data Architect,GE Vernova,Fresher,,"Hyderabad, India",Login to check your skill match score,"Job Description Summary\n\nAs a Data Architect, you will play a pivotal role in defining and implementing common data models, API standards, and leveraging the Common Information Model (CIM) standard across a portfolio of products deployed in Critical National Infrastructure (CNI) environments globally.\n\nGE Vernova is the leading software provider for the operations of national and regional electricity grids worldwide. Our software solutions range from supporting electricity markets, enabling grid and network planning, to real-time electricity grid operations.\n\nIn this senior technical role, you will collaborate closely with lead software architects to ensure secure, performant, and composable designs and implementations across our portfolio.\n\nJob Description\n\nGrid Software (a division of GE Vernova) is driving the vision of GridOS - a portfolio of software running on a common platform to meet the fast-changing needs of the energy sector and support the energy transition. Grid Software has extensive and well-established software stacks that are progressively being ported to a common microservice architecture, delivering a composable suite of applications. Simultaneously, new applications are being designed and built on the same common platform to provide innovative solutions that enable our customers to accelerate the energy transition.\n\nResponsibilities\n\nThis role is for a senior data architect who understands the core designs, principles, and technologies of GridOS. Key responsibilities include:\n\nFormalizing Data Models and API Standards: Lead the formalization and standardization of data models and API standards across products to ensure interoperability and efficiency.\nLeveraging CIM Standards: Implement and advocate for the Common Information Model (CIM) standards to ensure consistent data representation and exchange across systems.\nArchitecture Reviews and Coordination: Contribute to architecture reviews across the organization as part of Architecture Review Boards (ARB) and the Architecture Decision Record (ADR) process.\nKnowledge Transfer and Collaboration: Work with the Architecture SteerCo and Developer Standard Practices team to establish standard pratcise around data modeling and API design.\nDocumentation: Ensure that data modeling and API standards are accurately documented and maintained in collaboration with documentation teams.\nBacklog Planning and Dependency Management: Work across software teams to prepare backlog planning, identify, and manage cross-team dependencies when it comes to data modeling and API requirements.\n\nKey Knowledge Areas and Expertise\n\nData Architecture and Modeling: Extensive experience in designing and implementing data architectures and common data models.\nAPI Standards: Expertise in defining and implementing API standards to ensure seamless integration and data exchange between systems.\nCommon Information Model (CIM): In-depth knowledge of CIM standards and their application within the energy sector.\nData Mesh and Data Fabric: Understanding of data mesh and data fabric principles, enabling software composability and data-centric design trade-offs.\nMicroservice Architecture: Understandig of microservice architecture and software development\nKubernetes: Understanding of Kubernetes, including software development in an orchestrated microservice architecture. This includes Kubernetes API, custom resources, API aggregation, Helm, and manifest standardization.\nCI/CD and DevSecOps: Experience with CI/CD pipelines, DevSecOps practices, and GitOps, especially in secure, air-gapped environments.\nMobile Software Architecture: Knowledge of mobile software architecture for field crew operations, offline support, and near-realtime operation.\n\nAdditional Knowledge (Advantageous But Not Essential)\n\nEnergy Industry Technologies: Familiarity with key technologies specific to the energy industry, such as Supervisory Control and Data Acquisition (SCADA), Geospatial network modeling, etc.\n\nThis is a critical role within Grid Software, requiring a broad range of knowledge and strong organizational and communication skills to drive common architecture, software standards, and principles across the organization.\n\nAdditional Information\n\nRelocation Assistance Provided: No","Data Mesh and Data Fabric, Data Architecture and Modeling, CI CD and DevSecOps, API Standards, Common Information Model CIM, Mobile Software Architecture, Microservice Architecture, Kubernetes"
Data Architect,DATAECONOMY,15-17 Years,,"Hyderabad, India",Login to check your skill match score,"About Us\n\nAbout DATAECONOMY: We are a fast-growing data & analytics company headquartered in Dublin with offices inDublin, OH, Providence, RI, and an advanced technology center in Hyderabad,India. We are clearly differentiated in the data & analytics space via our suite of solutions, accelerators, frameworks, and thought leadership.\n\nJob Description\n\n15+ years of professional experience in the field of Data Architecture, Design and Development of Data Lifecycle Management (DLM) and Data Governance projects\nExperience should include hands-on experience in implementing a Customer Data Hub Customer Information Management system or Customer Data Mart which includes Modeling Customer Data Model in the 3NF / Dimensional Model, defining and developing integration patterns like Batch and real-time, defining & developing the physical storage layer, defining & developing the consumption pattern\nExperience with Data Modeling is a MUST, experience in defining standards and data architecture best practices is a must\nExperience with Data Management/Architecture Practices like Data Integration, Data Governance, Data Quality, Metadata management, Data Security, Data Encryption etc. is a must\nMust have hands-on experience with developing large Data Mart or Data warehouse or Data hub involving data modelling, data ingestion, data processing and extract generation to the downstream systems\nMust have experience with any ETL tool like Informatica or Data Stage or other for data processing requirements.\nExperience with Identity resolution products for Name and Address standardization like Informatica IDQ / Address Doctor /IBM Quality stage is required.\nExperience with any Data Quality product like Informatica IDQ, Quality Stage or Trillium will be an added advantage\nMust have strong experience with writing SQL for pulling and analyzing source/data platforms\nExperience with Data Science models, model validation, model tuning and management will be an added advantage.\nProactively communicate and collaborate with business stakeholders to understand the business requirements, translate them into design, and develop the respective module based on the requirements.\nMust have experience working in an Agile implementation set-up with Product Backlogs, User Stories, Scrum and sprint planning etc.\nStrong verbal and written communication and English language skills\nStrong consulting skills and consulting experience are strongly desired.\n\nRequirements\n\nDeveloping Data Architecture and Design documents for Data Lake, Data warehouse & Data Marts corresponding to Customer Information Management Systems\nExperience in Data Lifecycle Management (DLM)\nConfiguring Solutions and Coding any customization required for the Data platform including Data Lake, Data warehouse and Data Marts\nWorking with the clients to understand the requirements. Develop the required codebase for the functional needs\nDevelop Source to Target mapping document for all the attributes that are required to be captured for Data Lake, Data warehouse and Data Marts\nConfigure and develop code required for Upstream and downstream system communication in a Batch and real-time mode\nProvide clarifications for any questions that the Business / SME team or any other stakeholder has on Data implementations.\nParticipate in system and acceptance testing along with the stakeholders\n\nBenefits\n\nStandard Company Benefits","Data Encryption, Customer Data Hub, Data Science Models, Customer Data Mart, Customer Information Management, Metadata Management, Data Modeling, Sql, Data Integration, Data Quality, Data Architecture, Data Security, Data Governance"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Pune, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata\n\nwork Mode- Hybrid\n\nRoles And Responsibilities\n\nMandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization\n\nSolution Architect for Data modelling Understanding of Enterprise datasets Sales,\n\nProcurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle\n\netc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building\n\nData lake foundation, Maintenance etc)\n\nCollaborate with the product/business teams, understand related business processes and\n\ndocument business requirements and then write high level specifications/requirements for DEs\n\nDevelop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout\n\nfor right grain of data either in True source systems or in Data WHs and build reusable data\n\nmodels in intermediary layers before creating physical consumable views from data mart\n\nUnderstand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data\n\nGovernance, Data Quality frameworks, Data Observality and the candidate should be:\n\nFamiliar with DevOps process\nKnowing how to check existing tables, data dictionary, table structures\nExperienced with normalizing tables\nHaving good understanding of Landing, Bronze, Silver and Gold layers and concepts\nFamiliar with Agile techniques\nCreate business process map, user journey map and data flow integration diagrams; Understand\n\nIntegration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of\n\nmodels\n\nStakeholder management with data engineering, product owners, central data modelling team,\n\ndata governance & stewards, Scrum master, project team and sponsor.\n\nAbility to handle large implementation program with multiple projects spanning over an year.\n\nSkills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Data Quality frameworks, Agile techniques, DataOps, Data lake foundation, Data Observability, SAP, Sql, Data Modeling, Cloud Architecture, Data Governance, Devops, Dimensional Modeling, FTP, Data Modeler, Data Visualization, Oracle, Data Architect, Data Warehouse, Azure, Data Governance, Sftp, data vault"
Data Architect,Aumni Techworks,8-10 Years,,"Pune, India",Login to check your skill match score,"Position Summary:\n\nWe are looking for a highly skilled and experienced Data Engineer who focus on leading the development, and implementation of our Data Warehouse/Lakehouse solution, ensuring it serves as the foundation for scalable, high-performance analytics.\n\nResponsibilities:\n\nLakehouse Design & Implementation:\n\nLead the end-to-end development and deployment of a scalable and secure Lakehouse architecture.\nDefine best practices for data ingestion, storage, transformation, and processing using modern cloud technologies.\nArchitect data pipelines using ETL/ELT frameworks to support structured, semi-structured, and unstructured data.\nOptimize data modeling strategies to meet the analytical and performance needs of stakeholders.\nEvaluate and select appropriate cloud technologies, frameworks, and architectures.\n\nRequirement:\n\nExperience:\n\n8+ years of experience in data engineering, with a proven track record of implementing large-scale data solutions.\nExtensive experience with cloud platforms (AWS, GCP, or Azure), specifically in data warehouse/lakehouse implementations.\nExpertise in modern data architectures with tools like Databricks, Snowflake, or BigQuery.\nStrong background in SQL, Python, and distributed computing frameworks (Spark, Dataflow, etc.).\nIn-depth knowledge of data modeling principles (e.g., Star Schema, Snowflake Schema).\nExperience in enabling AI tools to consume data from the Lakehouse.\n\nAbout Aumni Techworks:\n\nEstablished in 2016, Aumni Techworks partners with its multinational clients to incubate and operate remote teams in India using the AumniBOT model. With a team of 250 and growing, our mission is to provide a quality alternative to project-based outsourcing.\n\nBenefits of working at Aumni Techworks:\n\nWork within a product team on cutting edge tech with one of the best pay packages.\nNo politics, no bench, voice your opinion, flat hierarchy, and global exposure\nWork environment to re-live our fun college days (awarded as Best culture by Pune Mirror)\nRecharge frequently with Friday socials, dance classes, theme parties and monsoon picnic.\nBreakout spaces at the office Gym, Pool, TT, Foosball and Carrom\nHealth focused Insurance coverage and get in shape with AumniFit (Do not miss our 4 PM plank!)","Snowflake Schema, Data ingestion, snowflake, ELT frameworks, Lakehouse architecture, BigQuery, Data Modeling, Cloud Technologies, Sql, Gcp, Spark, Databricks, DataFlow, Azure, Star Schema, Python, AWS"
Cloud Data Architect,Allianz Services,7-12 Years,,"Pune, India",Insurance,"Designation - Senior Cloud Data Architect\nExperience Range - 7 to 12 Years\nJob Location - Pune OR Trivandrum\nKey Responsibilities:\nCollaborate with agile and analytics data teams to design and implement scalable data products using cutting-edge technologies.\nProvide strategic insights and expertise on cloud capabilities, focusing on AWS or Azure, ensuring compliance with global and local regulations, security, and risk management.\nServe as a Subject Matter Expert for end-to-end cloud data architecture at Allianz.\nApply Agile and DevOps methodologies and implementation approaches in project delivery.\nUtilize strong communication skills to understand and deliver change and BAU requirements effectively.\nDrive innovative engineering, design, and strategy with your experience and ideas.\nMentor and upskill team members, fostering growth and knowledge sharing.\nContribute to internal networks and special interest groups, enhancing our knowledge base and community.\nDesired Experience:\nWe are looking for a new team member with strong technical skills, but equally value strong communication skills and adaptability, and a willingness to learn and respond to emerging needs:\nExpertise in managing Kubernetes components, scalability, and cost/consumption monitoring.\nDeep understanding of managing relational and non-relational data stores, scalability, and cost/consumption monitoring.\nProvide advisory and thought leadership on cloud-based analytics environments and big data technologies, integrating with existing data and analytics platforms.\nHands-on experience with:\nBig Data stack (e.g., SPARK, Kafka)\nRelational/non-relational stores (e.g., Postgres, MongoDB, Cassandra)\nRelated open-source software platforms and languages (e.g., Java, Apache, Python, Scala)\nDevelop solutions architecture and evaluate architectural alternatives for private, public, and hybrid cloud models, including IaaS, PaaS, and other cloud services.\nEnsure timely delivery of solutions, working with project sponsors to size, estimate (capacity planning), and manage scope and risk.\nProvide support and technical governance, offering expertise related to cloud architectures, deployment, and operations.","Java, Cassandra, Scala, Postgres, MongoDB, Azure, Apache, Python, Kubernetes, AWS"
Data Architect,Tredence Inc.,8-14 Years,,"Chennai, India",Login to check your skill match score,"Overall professional experience of the candidate should be atleast 8 years with a maximum experience upto 14 years.\nThe candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.\nCandidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.\nRole Description:\nDeveloping Modern Data Warehouse solutions using Databricks and Azure Stack\nAbility to provide solutions that are forward-thinking in data engineering and analytics space\nCollaborate with DW/BI leads to understand new ETL pipeline development requirements.\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop data model to fulfill reporting needs\nDrive technical discussion with client architect and team members\nOrchestrate the data pipelines in scheduler via Airflow Skills and Qualifications:\nBachelor's and/or master's degree in computer science or equivalent experience.\nGood understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture\nShould have hands-on experience in SQL, Python and Spark (PySpark)\nExperience in building ETL / data warehouse transformation processes\nProficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot\nDatabricks Certified Data Engineer Associate/Professional Certification (Desirable).\nComfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects\nShould have experience working in Agile methodology\nStrong verbal and written communication skills.\nStrong analytical and problem-solving skills with a high attention to detail.\nMandatory Skills\nAzure Databricks, Pyspark, Azure Data Factory, Azure Data Lake.","Azure Stack, Spark, Sql, Databricks, Pl Sql, Azure Data Factory, RDBMS, Pyspark, Data Quality, Unix Shell Scripting, Data Governance, Python, Azure Data Lake, Data Integration"
Data Architect - RWD and Analytics Products,Norstella,Fresher,,India,Login to check your skill match score,"Description\n\nAbout Norstella\n\nAt Norstella, our mission is simple: to help our clients bring life-saving therapies to market quickerand help patients in need.\n\nFounded in 2022, but with history going back to 1939, Norstella unites best-in-class brands to help clients navigate the complexities at each step of the drug development life cycle and get the right treatments to the right patients at the right time.\n\nEach Organization (Citeline, Evaluate, MMIT, Panalgo, The Dedham Group) Delivers Must-have Answers For Critical Strategic And Commercial Decision-making. Together, Via Our Market-leading Brands, We Help Our Clients\n\nCiteline accelerate the drug development cycle\nEvaluate bring the right drugs to market\nMMIT identify barrier to patient access\nPanalgo turn data into insight faster\nThe Dedham Group think strategically for specialty therapeutics\n\nBy combining the efforts of each organization under Norstella, we can offer an even wider breadth of expertise, cutting-edge data solutions and expert advisory services alongside advanced technologies such as real-world data, machine learning and predictive analytics. As one of the largest global pharma intelligence solution providers, Norstella has a footprint across the globe with teams of experts delivering world class solutions in the USA, UK, The Netherlands, Japan, China and India.\n\nJob Description\n\nWe are seeking a Technical Product Solutions Product Management Director to lead the design and implementation of scalable real-world data (RWD) solutions architecture. This role sits within the Product team but maintains strong collaboration with Engineering to ensure technical feasibility and execution. The ideal candidate has expertise in healthcare data, claims, EHR, lab and other types of RWD and is skilled in translating business needs into scalable, high-impact data products.\n\nThis role will be instrumental in shaping data-driven products, optimizing data architectures, and ensuring the integration of real-world data assets into enterprise solutions that support life sciences, healthcare, and payer analytics.\n\nResponsibilities\n\nDefine and drive the requirements for RWD data products.\nCollaborate with leadership, product managers, customers, and data scientists to identify high-value use cases.\nTranslate business and regulatory requirements into scalable and performant data models and solutions.\nDevelop architectures to support payer claims, labs, ehr-sourced insight generation and analytics.\nPartner with healthcare providers, payers, and life sciences companies to enhance data interoperability.\nWork closely with Engineering to design and implement responsive analytics layer and data architectures.\nProvide technical guidance on ETL pipelines, data normalization, and integration with third-party RWD sources.\nArchitect solutions to aggregate, standardize, and analyze EHR and molecular data, ensuring compliance with healthcare regulations (HIPAA, GDPR).\nDefine best practices for claims data ingestion, quality control, and data transformations.\nDevelop frameworks for processing structured and unstructured EHR data, leveraging NLP and data harmonization techniques.\nEnsure compliance with HIPAA, GDPR, and regulatory frameworks for healthcare data products.\nDefine and implement data governance strategies to maintain high data integrity and lineage tracking.\n\nRequirements\n\n\nDeep understanding of payer data, claims lifecycle, EHR, labs and real-world data applications.\nAbility to translate business needs into technical solutions and drive execution.\nStrong understanding of data product lifecycle and product management principles.\nExperience working with cross-functional teams, including Product, Engineering, Clinical, Business and Customer Success.\nExcellent communication skills to engage with both technical and non-technical stakeholders.\nExpertise in RWD and payer data structures (claims, EMR/EHR, registry data, prescription data, etc.).\nProficiency in SQL and NoSQL databases (PostgreSQL, Snowflake, MongoDB, etc.).\nStrong knowledge of ETL processes and data pipeline orchestration.\nExperience with big data processing (Spark, Databricks, Hadoop).\nUnderstanding of payer and provider data models used in healthcare analytics.\nStrong presentation and documentation skills to articulate solutions effectively.\nExperience working with payer organizations, PBMs, life sciences, and health plans.\nExperience with OMOP, FHIR, HL7, and other healthcare data standards.\nKnowledge of data governance, metadata management, and lineage tracking tools.\nExperience in pharmaceutical RWE studies and market access analytics.\nFamiliarity with BI tools (Tableau, Power BI, Looker).\nUnderstanding of data mesh and federated data architectures.\n\nBenefits\n\n\nHealth Insurance\nProvident Fund\nLife Insurance\nReimbursement of Certification Expenses\nGratuity\n24x7 Health Desk\n\nOur guiding principles for success at Norstella\n\n01: Bold, Passionate, Mission-First\n\nWe have a lofty mission to Smooth Access to Life Saving Therapies and we will get there by being bold and passionate about the mission and our clients. Our clients and the mission in what we are trying to accomplish must be in the forefront of our minds in everything we do.\n\n02: Integrity, Truth, Reality\n\nWe make promises that we can keep, and goals that push us to new heights. Our integrity offers us the opportunity to learn and improve by being honest about what works and what doesn't. By being true to the data and producing realistic metrics, we are able to create plans and resources to achieve our goals.\n\n03: Kindness, Empathy, Grace\n\nWe will empathize with everyone's situation, provide positive and constructive feedback with kindness, and accept opportunities for improvement with grace and gratitude. We use this principle across the organization to collaborate and build lines of open communication.\n\n04: Resilience, Mettle, Perseverance\n\nWe will persevere even in difficult and challenging situations. Our ability to recover from missteps and failures in a positive way will help us to be successful in our mission.\n\n05: Humility, Gratitude, Learning\n\nWe will be true learners by showing humility and gratitude in our work. We recognize that the smartest person in the room is the one who is always listening, learning, and willing to shift their thinking.\n\nNorstella is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people's differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual's abilities, skills, performance and behavior and our business requirements. Norstella operates a zero tolerance policy to any form of discrimination, abuse or harassment.\n\nSometimes the best opportunities are hidden by self-doubt. We disqualify ourselves before we have the opportunity to be considered. Regardless of where you came from, how you identify, or the path that led you here- you are welcome. If you read this job description and feel passion and excitement, we're just as excited about you.","data architectures, FHIR, snowflake, ETL processes, federated data architectures, OMOP, NoSQL databases, Looker, data models, healthcare data, lineage tracking, Claims, data mesh, big data processing, data normalization, Metadata Management, PostgreSQL, Tableau, Ehr, Hadoop, Power Bi, Bi Tools, Sql, Hl7, Spark, Data Governance, Databricks, MongoDB"
Data Architect,Ideas2IT Technologies,Fresher,,"Chennai, India",Login to check your skill match score,"Why Choose Ideas2IT\n\nIdeas2IT has all the good attributes of a product startup and a services company. Since we launch our products, you will have ample opportunities to learn and contribute. However, single-product companies stagnate in the technologies they use. In our multiple product initiatives and customer-facing projects, you will have the opportunity to work on various technologies.\n\nAGI is going to change the world. Big companies like Microsoft are betting heavily on this (see here and here). We are following suit. As a Data Engineer, exclusively focus on engineering data pipelines for complex products\n\nWhat's in it for you\n\nA robust distributed platform to manage a self-healing swarm of bots onunreliable network / compute\nLarge-scale Cloud-Native applications\nDocument Comprehension Engine leveraging RNN and other latest OCR techniques\nCompletely data-driven low-code platform\nYou will leverage cutting-edge technologies like Blockchain, IoT, and Data Science as you work on projects for leading Silicon Valley startups.\nYour role does not start or end with just Java development; you will enjoy the freedom to share your suggestions on the choice of tech stacks across the length of the project\nIf there is a certain technology you would like to explore, you can do your Technical PoCs\nWork in a culture that values capability over experience and continuous learning as a core tenet\n\nHere's what you'll bring\n\nProficiency in SQL and experience with database technologies (e.g., MySQL, PostgreSQL, SQL Server).Experience in any one of the cloud environments AWS, Azure\nExperience with data modeling, data warehousing, and building ETL pipelines.\nExperience building large-scale data pipelines and data-centric applications using any distributed storage platform\nExperience in data processing tools like Pandas, pyspark.\nExperience in cloud services like S3, Lambda, SQS, Redshift, Azure Data Factory, ADLS, Function Apps, etc.\nExpertise in one or more high-level languages (Python/Scala)\nAbility to handle large-scale structured and unstructured data from internal and third-party sources\nAbility to collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision-making across the organization\nExperience with data visualization tools like PowerBI, Tableau\nExperience in containerization technologies like Docker , Kubernetes\n\nAbout Us\n\nIdeas2IT stands at the intersection of Technology, Business, and Product Engineering, offering high-caliber Product Development services. Initially conceived as a CTO consulting firm, we've evolved into thought leaders in cutting-edge technologies such as Generative AI, assisting our clients in embracing innovation.\n\nOur forte lies in applying technology to address business needs, demonstrated by our track record of developing AI-driven solutions for industry giants like Facebook, Bloomberg, Siemens, Roche, and others. Harnessing our product-centric approach, we've incubated several AI-based startupsincluding Pipecandy, Element5, IdeaRx, and Carefi. inthat have flourished into successful ventures backed by venture capital.\n\nWith fourteen years of remarkable growth behind us, we're steadfast in pursuing ambitious objectives.\n\nP.S. We're all about diversity, and our doors are wide open to everyone. Join us in celebrating the awesomeness of differences!","Function Apps, data-centric applications, data visualization tools, ETL pipelines, large-scale data pipelines, ADLS, S3, PostgreSQL, Data Warehousing, Pyspark, Tableau, Data Modeling, Lambda, Docker, MySQL, Python, AWS, Scala, SQL Server, Redshift, Sql, Azure Data Factory, Pandas, Powerbi, Sqs, Azure, Kubernetes"
MS Azure Data Architect,3Pillar,8-10 Years,,India,Login to check your skill match score,"We are seeking a highly experienced Data Architect with a strong background in designing scalable data solutions and leading data engineering teams. The ideal candidate will have deep expertise in Microsoft Azure, ETL processes, and modern data architecture principles. This role involves close collaboration with stakeholders, engineering teams, and business units to design and implement robust data pipelines and architectures.\n\nAssessments of existing data components, Performing POCs, Consulting to the stakeholders\nProposing end to end solutions to an enterprise's data specific business problems, and taking care of data collection, extraction, integration, cleansing, enriching and data visualization\nAbility to design large data platforms to enable Data Engineers, Analysts & scientists\nStrong exposure to different Data architectures, data lake & data warehouse\nDesign and implement end-to-end data architecture solutions on Azure cloud platform.\nLead the design and development of scalable ETL/ELT pipelines using tools such as Azure Data Factory (ADF).\nArchitect data lakes using Azure Data Lake Storage (ADLS) and integrate with Azure Synapse Analytics for enterprise-scale analytics.\nCollaborate with business analysts, data scientists, and engineers to understand data needs and deliver high-performing solutions.\nDefine data models, metadata standards, data quality rules, and security protocols.\nDefine tools & technologies to develop automated data pipelines, write ETL processes, develop dashboard & report and create insights\nContinually reassess current state for alignment with architecture goals, best practices and business needs\nDB modeling, deciding best data storage, creating data flow diagrams, maintaining related documentation\nTaking care of performance, reliability, reusability, resilience, scalability, security, privacy & data governance while designing a data architecture\nApply or recommend best practices in architecture, coding, API integration, CI/CD pipelines\nCoordinate with data scientists, analysts, and other stakeholders for data-related needs\nHelp the Data Science & Analytics Practice grow by mentoring junior Practice members, leading initiatives, leading Data Practice Offerings\nProvide thought leadership by representing the Practice / Organization on internal / external platforms\n\nQualificatons:\n\n8+ years of experience in data architecture, data engineering, or related roles.\nTranslate business requirements into data requests, reports and dashboards.\nStrong Database & modeling concepts with exposure to SQL & NoSQL Databases\nExpertise in designing and writing ETL processes in Python/PySpark\nStrong data architecture patterns & principles, ability to design secure & scalable data lakes, data warehouse, data hubs, and other event-driven architectures\nProven expertise in Microsoft Azure data services, especially ADF, ADLS, Synapse Analytics.\nStrong hands-on experience in designing and building ETL/ELT pipelines.\nProficiency in data modeling, SQL, and performance tuning.\nDemonstrated leadership experience, with the ability to manage and mentor technical teams.\nExcellent communication and stakeholder management skills.\nProficiency in data visualization tools like Tableau, Power BI or similar to create meaningful insights.\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n\nGood to have:\n\nAzure certifications (e.g., Azure Data Engineer Associate, Azure Solutions Architect).\nExperience with modern data platforms, data governance frameworks, and real-time data processing tools.\n\nBenefits:\n\nImagine a flexible work environment whether it's the office, your home, or a blend of both. From interviews to onboarding, we embody a remote-first approach.\nYou will be part of a global team, learning from top talent around the world and across cultures, speaking English everyday. Our global workforce enables our team to leverage global resources to accomplish our work in efficient and effective teams.\nWe're big on your well-being as a company, we spend a whole trimester in our annual cycle focused on wellbeing. Whether it is taking advantage of fitness offerings, mental health plans (country-dependent), or simply leveraging generous time off, we want all of our team members operating at their best.\nOur professional services model enables us to accelerate career growth and development opportunities - across projects, offerings, and industries.\nWe are an equal opportunity employer. It goes without saying that we live by values like Intrinsic Dignity and Open Collaboration to create cutting-edge technology AND reinforce our commitment to diversity - globally and locally.\nJoin us and be a part of a global tech community! Check out our Linkedin site and Careers page to learn more about what it's like to be part of our #oneteam!","NoSQL Databases, ETL processes, Azure Synapse Analytics, Pyspark, Data Modeling, Microsoft Azure, Data Architecture, Sql, Python"
Data Architect / Lead,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Position: FTE, Remote, India\n\nAvailability - Immediate -1 week\n\nCandidate proficiency level: Senior(10+)\n\nResponsibilities:\n\nDesign, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.\n\nEnhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.\n\nImplement best practices for data management, storage and security to ensure data integrity and compliance with regulations.\n\nOwn the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.\n\nParticipate in code reviews to ensure code quality and share knowledge.\n\nLead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.\n\nDefine and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.\n\nMentor junior members of the team, providing guidance and support in their professional development.\n\nCollaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration\n\nA little more about you:\n\nBachelor's degree or higher in Computer Science, Engineering, or a related field.\n\n10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.\n\nProficient in SQL and Python, with the ability to translate complexity into efficient code.\n\nExperience with data workflow development and management tools (dbt, Airflow).\n\nSolid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.\n\nStrong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.\n\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\n\nExperience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Sql, AWS, Python, Azure, Gcp"
Data Architect - Enterprise Architecture,Viasat,Fresher,,"Chennai, India",Login to check your skill match score,"About Us\n\nOne team. Global challenges. Infinite opportunities. At Viasat, we're on a mission to deliver connections with the capacity to change the world. For more than 35 years, Viasat has helped shape how consumers, businesses, governments and militaries around the globe communicate. We're looking for people who think big, act fearlessly, and create an inclusive environment that drives positive impact to join our team.\n\nWhat You'll Do\n\nThe Enterprise Architecture team is focused on providing solutions to enable an effective software engineering workforce that can scale to the business needs. This includes exploring how the business needs map to the application portfolio, business processes, APIs, and data elements across the organization. As a member of this team, you will build up a vast knowledge in software development, cloud application engineering, automation, and container orchestration. Our ideal candidate values communication, learning, adaptability, creativity, and ingenuity. They also enjoy working on challenging technical issues and use creative, innovative techniques to develop and automate solutions. This team is focused on providing our executive and business leadership with visibility into how the software organization is functioning and what opportunities lie to transform the business. In this position you will be an integral part of the Enterprise Architecture team and make meaningful impacts in our journey towards digital transformation.\n\nThe day-to-day\n\nA Strong Data-Model and High-Quality Data are pre-requisites to provide better insights and enable solid data-driven decision making. This is also key to take advantage of various technological advances in Artificial Intelligence and Machine Learning. Your responsibilities will involve build out of data models for various aspects of our enterprise in conjunction with domain experts. Examples include but are not limited to Network, Capacity, Finance, Business Support Systems etc. Responsibilities also include working with software product teams to improve data quality across the organization.\n\nWhat You'll Need\n\nBachelor's degree or higher in Computer Science & Applications, Computer Science and Computer & Systems Engineering, Computer Science & Engineering, Computer Science & Mathematics, Computer Science & Network Security and Math & Computer Science, and/or a related field\nSolid understanding of Data Architecture and Data Engineering principles\nExperience building out data models\nExperience performing data analysis and presenting data in easy to comprehend manner.\nExperience in working with Relational Databases, NoSQL, Large Scale Data technologies (Kafka, Big Query, Snowflake etc)\nExperience with digital transformation across multiple cloud platforms like AWS and GCP.\nExperience in modernizing data platforms especially in GCP is highly preferred.\nPartner with members of Data Platform team and others to build out Data Catalog and map to the data model\nDetail Oriented to ensure that the catalog represents quality data\nSolid communication skills and ability to work on a distributed team\nTenacity to remain focused on the mission and overcome obstacles\nAbility to perform hands-on work with development teams and guide them to building necessary data models.\nExperience setting up governance structure and changing the organization culture by influence\n\nWhat Will Help You On The Job\n\n\nExperience with Cloud Technologies: AWS, GCP, and/or Azure, etc.\nExpertise in GCP data services like Cloud Pub/Sub, Dataproc, Dataflow, BigQuery, and related technologies preferred.\nExperience with Airflow, DBT and SQL.\nExperience with Open-source software like Logstash, ELK stack, Telegraf, Prometheus and OpenTelemetry is a plus.\nPassionate to deliver solutions that improve developer experience and promote API-first principles and microservices architecture.\nExperience with Enterprise Architecture and related principles\n\nEEO Statement\n\n\nViasat is proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, ancestry, physical or mental disability, medical condition, marital status, genetics, age, or veteran status or any other applicable legally protected status or characteristic. If you would like to request an accommodation on the basis of disability for completing this on-line application, please click here.","Large Scale Data technologies, Big Query, Airflow, Relational Databases, OpenTelemetry, snowflake, dbt, Telegraf, Cloud Pub Sub, Data Analysis, data engineering, Enterprise Architecture, Prometheus, Kafka, Elk Stack, Nosql, Data Architecture, AWS, Logstash, Dataproc, Sql, Cloud Technologies, Gcp, DataFlow"
Staff Specialist IT - PLM Data Architect,Infineon Technologies,8-10 Years,,"Ahmedabad, India",Semiconductor Manufacturing,"As a PLM Data Architect, you will be responsible for designing, implementing, and maintaining the data architecture of our Product Lifecycle Management (PLM) system. You will work closely with cross-functional teams to ensure data consistency, integrity, and quality across the entire product lifecycle. If you have a strong background in data architecture, PLM systems, and a passion for data-driven decision-making, we encourage you to apply for this exciting opportunity.\n\nJob Description\n\nIn your new role you will:\n\nDesign and Create the framework for managing the organization's enterprise data architecture.\nFocus on identifying and using the right tools and technologies, technical methodologies, technical guardrails and guidelines ,integration with broader technical environment, etc.\nDesign and implement a scalable and flexible data architecture for the PLM system, ensuring data consistency, integrity, and quality across the entire product lifecycle.\nDevelop and maintain data models, data flows, and data governance policies to ensure data accuracy, completeness, and compliance with industry standards and regulations.\nCollaborate with cross-functional teams, including engineering, manufacturing, and quality, to ensure data requirements are met and data is properly integrated across systems.\nDevelop and maintain data interfaces, APIs, and data migration strategies to ensure seamless data exchange between PLM and other systems.\nEnsure data security, access controls, and auditing mechanisms are in place to protect sensitive product data.\nDevelop and maintain data analytics and reporting capabilities to support business decision-making and product development.\n\nYour Profile\n\nYou are best equipped for this task if you have:\n\nBachelor's or Master's degree (Computer Science or Related) with 8+years of relevant experience.\nProven experience as Data Architecture or in a similar role in an R&D environment.\nExperience in implementing data management, data integration and reporting technologies.\nGood Knowledge of data governance, data quality, and data security best practices.\nKnowledge in enterprise systems like PLM, ERP, MD systems.\nProficiency in data modelling and design.\nExperience working with Data Platforms, Data Catalogues, API Management and Event Driven Architecture.\nKnowledge of programming languages Python or Java , NoSQL databases, data visualization, data virtualization.\nSolid understanding of cloud services, architectures, and storage solutions.\nExcellent problem-solving and communication skills.\n\n#WeAreIn for driving decarbonization and digitalization.\n\nAs a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.\n\nAre you in\n\nWe are on a journey to create the best Infineon for everyone.\n\nThis means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicants experience and skills.\n\nPlease let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.\n\nClick here for more information about Diversity & Inclusion at Infineon.","Storage Solutions, Data virtualization, Cloud services architectures, reporting technologies, PLM systems, NoSQL databases, Data Platforms, Event Driven Architecture, Data security best practices, Data Catalogues, Java, Api Management, Data Quality, Data Governance, Data Visualization, Data Management, Data Integration, Data Architecture, Data Modelling, Python"
Senior Manager_Data Architect-Tech Ops,PwC Acceleration Centers in India,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"Summary about Organization A career in our Advisory Acceleration Center is the natural extension of PwC's leading global delivery capabilities. The team consists of highly skilled resources that can assist in the areas of helping clients transform their business by adopting technology using bespoke strategy, operating model, processes and planning. You'll be at the forefront of helping organizations around the globe adopt innovative technology solutions that optimize business processes or enable scalable technology. Our team helps organizations transform their IT infrastructure, modernize applications and data management to help shape the future of business. An essential and strategic part of Advisory's multi-sourced, multi-geography Global Delivery Model, the Acceleration Centers are a dynamic, rapidly growing component of our business. The teams out of these Centers have achieved remarkable results in process quality and delivery capability, resulting in a loyal customer base and a reputation for excellence. .\n\nJob Description\n\nSenior Data Architect with experience in design, build, and optimization of complex data landscapes and legacy modernization projects. The ideal candidate will have deep expertise in database management, data modeling, cloud data solutions, and ETL (Extract, Transform, Load) processes. This role requires a strong leader capable of guiding data teams and driving the design and implementation of scalable data architectures.\n\nKey areas of expertise include\n\nDesign and implement scalable and efficient data architectures to support business needs.\nDevelop data models (conceptual, logical, and physical) that align with organizational goals.\nLead the database design and optimization efforts for structured and unstructured data.\nEstablish ETL pipelines and data integration strategies for seamless data flow.\nDefine data governance policies, including data quality, security, privacy, and compliance.\nWork closely with engineering, analytics, and business teams to understand requirements and deliver data solutions.\nOversee cloud-based data solutions (AWS, Azure, GCP) and modern data warehouses (Snowflake, BigQuery, Redshift).\nEnsure high availability, disaster recovery, and backup strategies for critical databases.\nEvaluate and implement emerging data technologies, tools, and frameworks to improve efficiency.\nConduct data audits, performance tuning, and troubleshooting to maintain optimal performance\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\n13+ years of experience in data modeling, including conceptual, logical, and physical data design.\n5 8 years of experience in cloud data lake platforms such as AWS Lake Formation, Delta Lake, Snowflake or Google Big Query.\nProven experience with NoSQL databases and data modeling techniques for non-relational data.\nExperience with data warehousing concepts, ETL/ELT processes, and big data frameworks (e.g., Hadoop, Spark).\nHands-on experience delivering complex, multi-module projects in diverse technology ecosystems.\nStrong understanding of data governance, data security, and compliance best practices.\nProficiency with data modeling tools (e.g., ER/Studio, ERwin, PowerDesigner).\nExcellent leadership and communication skills, with a proven ability to manage teams and collaborate with stakeholders.\n\nPreferred Skills\n\nExperience with modern data architectures, such as data fabric or data mesh.\nKnowledge of graph databases and modeling for technologies like Neo4j.\nProficiency with programming languages like Python, Scala, or Java.\nUnderstanding of CI/CD pipelines and DevOps practices in data engineering.","data fabric, DevOps practices, NoSQL databases, cloud data solutions, snowflake, data mesh, Java, BigQuery, Hadoop, Data Security, Scala, Data Modeling, Redshift, Database Management, Gcp, Spark, Data Governance, Data Warehousing Concepts, Azure, Python, AWS"
Data Architect - Manager,PwC Acceleration Centers in India,5-9 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions. Those in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.\n\nEnhancing your leadership style, you motivate, develop and inspire others to deliver quality. You are responsible for coaching, leveraging team member's unique strengths, and managing performance to deliver on client expectations. With your growing knowledge of how business works, you play an important role in identifying opportunities that contribute to the success of our Firm. You are expected to lead with integrity and authenticity, articulating our purpose and values in a meaningful way. You embrace technology and innovation to enhance your delivery and encourage others to do the same.\n\nSkills\n\nExamples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:\n\nAnalyse and identify the linkages and interactions between the component parts of an entire system.\nTake ownership of projects, ensuring their successful planning, budgeting, execution, and completion.\nPartner with team leadership to ensure collective ownership of quality, timelines, and deliverables.\nDevelop skills outside your comfort zone, and encourage others to do the same.\nEffectively mentor others.\nUse the review of work as an opportunity to deepen the expertise of team members.\nAddress conflicts or issues, engaging in difficult conversations with clients, team members and other stakeholders, escalating where appropriate.\nUphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.\n\nData Modeler\n\nJob Summary\n\nLooking for candidates with a strong background in data modeling, metadata management, and data system optimization. You will be responsible for analyzing business needs, developing longterm data models, and ensuring the efficiency and consistency of our data systems.\n\nKey Responsibilities\n\nAnalyze and translate business needs into long term solution data models.\nEvaluate existing data systems and recommend improvements.\nDefine rules to translate and transform data across data models.\nWork with the development team to create conceptual data models and data flows.\nDevelop best practices for data coding to ensure consistency within the system.\nReview modifications of existing systems for cross compatibility.\nImplement data strategies and develop physical data models.\nUpdate and optimize local and metadata models.\nUtilize canonical data modeling techniques to enhance data system efficiency.\nEvaluate implemented data systems for variances, discrepancies, and efficiency.\nTroubleshoot and optimize data systems to ensure optimal performance.\n\nRequired Qualifications\n\nBachelor's degree in Engineering or a related field.\n5 to 9 years of experience in data modeling or a related field.\n4+ years of hands-on experience with dimensional and relational data modeling.\nExpert knowledge of metadata management and related tools.\nProficiency with data modeling tools such as Erwin, Power Designer, or Lucid.\nKnowledge of transactional databases and data warehouses.\n\nDesired Skills And Competencies\n\nAdvanced troubleshooting skills.\nExcellent communication and presentation skills.\nStrong interpersonal skills to collaborate effectively with various teams.","Data system optimization, Canonical data modeling techniques, Transactional databases, Metadata Management, relational data modeling, data warehouses, Data Modeling"
Data Center Architect,Mindsprint,Fresher,,"Chennai, India",Login to check your skill match score,"Job Title: Datacentre Architect\nLocation: Chennai, Tamil Nadu\nCompany: Mindsprint\nAbout Mindsprint: Mindsprint is at the forefront of innovation, driving transformative changes in the IT landscape. As we embark on a significant journey of de-merging our IT infrastructure and application landscape, we are seeking a highly skilled and experienced De-Merger as a Service Architect to lead this critical initiative.\nJob Summary: Data Centre Architect will be responsible for designing and implementing the de-merger strategy for Mindsprint's IT infrastructure and application landscape. This role requires a deep understanding of IT architecture, project management, and the ability to work collaboratively with various stakeholders to ensure a seamless transition.\nKey Responsibilities:\nDevelop and execute the de-merger strategy for Mindsprint's IT infrastructure and application landscape.\nAssess current IT systems and applications to identify dependencies and potential risks associated with the de-merger.\nDesign and implement solutions to separate IT systems and applications while ensuring minimal disruption to business operations.\nCollaborate with cross-functional teams, including IT, business units, and external partners, to ensure alignment and successful execution of the de-merger plan.\nProvide technical leadership and guidance throughout the de-merger process, ensuring best practices and industry standards are followed.\nDevelop detailed project plans, timelines, and budgets for the de-merger initiative.\nMonitor progress, identify issues, and implement corrective actions as needed to keep the project on track.\nEnsure compliance with all relevant regulations and standards during the de-merger process.\nCommunicate effectively with stakeholders at all levels, providing regular updates on project status and addressing any concerns.\nQualifications:\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\nProven experience in IT architecture, with a focus on de-mergers, mergers, or large-scale IT transformations.\nStrong project management skills, with experience leading complex IT projects.\nExcellent problem-solving and analytical skills.\nAbility to work effectively in a fast-paced, dynamic environment.\nStrong communication and interpersonal skills, with the ability to build relationships and influence stakeholders.\nKnowledge of relevant regulations and standards related to IT de-mergers.\nPreferred Skills:\nExperience as a Data Center Architect, Network Architect, and Cloud Architect.\nFamiliarity with IT security best practices and compliance requirements.\nCertification in project management (e.g., PMP, PRINCE2) or IT architecture (e.g., TOGAF).","de-mergers, project management, Pmp, IT transformations, compliance requirements, Prince2, Togaf, it security best practices, IT architecture"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\n10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.\nWorking with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.\nAnalyzing requirements: Performing requirement analysis and creating architectural models.\nIdentifying issues: Identifying operational issues and recommending strategies to resolve them.\nCommunicating with business users: Communicating technical solutions to business users and addressing their questions.\nValidating solutions: Ensuring solutions align with corporate standards and compliance requirements.\nDeveloping technical specifications: Creating technical design specifications for solutions and systems engineers.\nAssessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.\n\nData engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Databases, Apis, Web Scraping, Python"
Data Architect / Lead,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Position: FTE, Remote, India\n\nAvailability - Immediate -1 week\n\nCandidate proficiency level: Senior(10+)\n\nResponsibilities:\n\nDesign, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.\n\nEnhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.\n\nImplement best practices for data management, storage and security to ensure data integrity and compliance with regulations.\n\nOwn the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.\n\nParticipate in code reviews to ensure code quality and share knowledge.\n\nLead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.\n\nDefine and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.\n\nMentor junior members of the team, providing guidance and support in their professional development.\n\nCollaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration\n\nA little more about you:\n\nBachelor's degree or higher in Computer Science, Engineering, or a related field.\n\n10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.\n\nProficient in SQL and Python, with the ability to translate complexity into efficient code.\n\nExperience with data workflow development and management tools (dbt, Airflow).\n\nSolid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.\n\nStrong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.\n\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\n\nExperience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Sql, AWS, Python, Azure, Gcp"
Data Architect,Jio,12-15 Years,,"Mumbai, India",Login to check your skill match score,"Company Overview\nJio Platforms Limited is the driving force behind India's leading telecom operator Jio, serving 400M+ customers. We offer digital apps & services, end-to-end 5G solutions, AI/ML platforms, and cloud-native OSS/BSS solutions.\nJob Overview\nExperienced Data Architect role at Jio Platforms Limited, Mumbai, Bangalore India. Full-Time position with more than 10 years of experience. Join a dynamic company leading India's telecom market with 400M+ customers and innovative digital solutions for B2C and B2B sectors.\nQualifications and Skills\nExpertise in Hadoop, Hive, Apache Spark, Python, Kafka, Cloudera\nExperience with On-premise and Microsoft Azure environments\nStrong data management and data governance skills\nExcellent problem-solving and analytical abilities\nEffective communication and collaboration skills\nBachelors or masters degree in computer science, Information Technology or related field of study. MCA preferable.\n12 - 15 years of overall experience.\nCertification MS Azure AZ-304, AZ-303,\n6+ years of large-scale software development or application engineering with recent coding experience in two or more of the following: Java, JavaScript, Node.js, .NET, Python, MSSQL.\n4+ years of experience as a technical specialist in customer-facing roles.\nExperience architecting highly available systems that utilize load balancing, horizontal scalability and high availability.\nGood exposure to Agile software development and DevOps practices such as Infrastructure as Code (IaC), Continuous Integration and automated deployment Continuous Integration (CI) tools (e.g. Jenkins).\nStrong, in-depth and demonstrable hands-on experience with the following technologies: Microsoft Azure and its relevant build, deployment, automation, networking and security technologies in cloud and hybrid environments.\nExposure to Agile development methodologies and deployment strategies.\nStrong practical application development experience on Linux and Windows-based systems.\nExcellent knowledge of cloud computing technologies and current computing trends.\nExperience working directly with customers, partners or third-party developers.\nEffective communication skills (written and verbal) to properly articulate complicated cloud reports to management and other IT development partners.\nPositive attitude and a strong commitment to delivering quality work.\nRoles and Responsibilities\nDesign and implement data architecture solutions to meet business needs.\nDevelop data models, database design, and data migration strategies.\nCollaborate with cross-functional teams to ensure data integration and data quality.\nImplement data security and compliance measures.\nOptimize data infrastructure and performance for scalable solutions.\nArchitect, design, and develop Products on the Azure platform.\nDesign and develop solutions for Data Platforms ranging from Batch Data management to real-time data feeds.\nLeverage new technology paradigms (e.g., serverless, containers, microservices, Api Management, Data Storage).\nDevelop solutions for the cloud and for Azure storage.\nDesign identity & security and data platform solutions.\nDesign Azure infrastructure strategy.\nWork closely with Business Analysts, Product Managers, Data Managers and other team members to ensure successful production of application software.\nWork closely with IT security to monitor the company's cloud privacy.\nRespond to technical issues in a professional and timely manner.\nOffer guidance in infrastructure movement techniques including bulk application transfers into the cloud.\nIdentify the top cloud architecture solution patterns to successfully meet the strategic needs of the company.\nLocation: - Mumbai, Bangalore","Java, Hadoop, .NET, Apache Spark, Node.js, Kafka, Windows, Mssql, Hive, Javascript, Linux, Cloudera, Microsoft Azure, Python"
Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.\n\nRole\n\nDesign, implement and lead Data Architecture, Data Quality, Data Governance\nDefining data modeling standards and foundational best practices\nDevelop and evangelize data quality standards and practices\nEstablish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data\nDrive the successful adoption of organizational data utilization and self-serviced data platforms\nCreate and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset\nDevelop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data\nDesign data schemes, object models, and flow diagrams to structure, store, process, and integrate data\nProvide architectural assessments, strategies, and roadmaps for data management\nApply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms\nImplement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD\nTranslate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models\nDefine templates and processes for the design and analysis of data models, data flows, and integration\nLead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms\n\nQualifications\n\nB.S. or M.S. in Computer Science, or equivalent degree\n10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting\n7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more\nExtensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse\nHighly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools\nProven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker\nKnowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with hands-on experience in Amazon Web Services (AWS)\nStrong verbal and written communications skills are a must and should work effectively across internal and external organizations and virtual teams\nDemonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies\nStrong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem\nDeep knowledge of data structures and algorithms\nExperience working in large teams using CI/CD and agile methodologies","Airflow, Data Lake Technologies, CI CD, Hive Catalog, Big Data platforms, ML and Data Science platforms, Delta Lake, S3, Kafka, Tableau, Redshift, Sql, Data Quality, Hive, Docker, Spark, Data Architecture, Databricks, Data Governance, Kubernetes, Python, Etl, Aws S3"
Senior Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.\nRole:\nDesign, implement and lead Data Architecture, Data Quality, Data Governance across\nDefining data modeling standards and foundational best practices\nDevelop and evangelize data quality standards and practices\nEstablish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data.\nDrive the successful adoption of organizational data utilization and self-serviced data platforms.\nCreate and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset\nDevelop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data.\nDesign data schemes, object models, and flow diagrams to structure, store, process, and integrate data\nProvide architectural assessments, strategies, and roadmaps for data management.\nApply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms.\nImplement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD\nTranslate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models.\nDefine templates and processes for the design and analysis of data models, data flows, and integration.\nLead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms\nMandatory Qualifications:\nQualifications: B.S. or M.S. in Computer Science, or equivalent degree\n10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting.\n7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more extensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse.\nHighly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools.\nProven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker\nKnowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with Amazon Web Services (AWS)\nStrong verbal and written communications skills are a must and work effectively across internal and external organizations and virtual teams.\nDemonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies.\nStrong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem\nDeep knowledge of data structures and algorithms.\nExperience in working in large teams using CI/CD and agile methodologies.","Airflow, Delta Lake, CI CD, Data Lake Technologies, , Parquet, Big Data platforms, Hive Catalog, ML and Data Science platforms, Docker, Sql, Databricks, Tableau, Kafka, Avro, Etl, Hive, S3, Aws S3, Data Quality, Python, Kubernetes, Data Governance, Spark, Redshift"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Chennai, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata\n\nwork Mode- Hybrid\n\nRoles And Responsibilities\n\nMandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization\n\nSolution Architect for Data modelling Understanding of Enterprise datasets Sales,\n\nProcurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle\n\netc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building\n\nData lake foundation, Maintenance etc)\n\nCollaborate with the product/business teams, understand related business processes and\n\ndocument business requirements and then write high level specifications/requirements for DEs\n\nDevelop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout\n\nfor right grain of data either in True source systems or in Data WHs and build reusable data\n\nmodels in intermediary layers before creating physical consumable views from data mart\n\nUnderstand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data\n\nGovernance, Data Quality frameworks, Data Observality and the candidate should be:\n\nFamiliar with DevOps process\nKnowing how to check existing tables, data dictionary, table structures\nExperienced with normalizing tables\nHaving good understanding of Landing, Bronze, Silver and Gold layers and concepts\nFamiliar with Agile techniques\nCreate business process map, user journey map and data flow integration diagrams; Understand\n\nIntegration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of\n\nmodels\n\nStakeholder management with data engineering, product owners, central data modelling team,\n\ndata governance & stewards, Scrum master, project team and sponsor.\n\nAbility to handle large implementation program with multiple projects spanning over an year.\n\nSkills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Data projects, Agile techniques, DataOps, Data Observability, Data Lake, Sql, Data Modeling, Cloud Architecture, Devops, Data Modeler, FTP, Data Warehouse, Data Quality, Data Architect, Azure, Data Governance, Sftp"
Cloud Data Architect,owow,8-16 Years,,"Bengaluru, India",Login to check your skill match score,"Technical Skills:\nSkillsets we are looking for:\n8 to 16 years of working experience in data engineering.\n7+ years exp in PySprak\n7+ years exp in AWS Glue\n7+ years exp in AWS Redshift\n7+ years exp in in AWS CI/CD pipeline like codebuild, codecommit, codedeploy and codepipeline\nStrong proficiency in AWS services such as S3, EC2, EMR, SNS, Lambda, StepFunctions\nExperience implementing automated testing platforms like PyTest\nStrong proficiency in Python, Hadoop, Spark and or PySpark is required\nSkill of writing clean, readable, commented and easily maintainable code\nUnderstanding of fundamental design principles for building a scalable solution\nSkill for writing reusable libraries\nProficiency in understanding code versioning tools such as Git, SVN, TFS etc.,\nInterpersonal skills:\nExcellent communication and collaboration skills.\nAbility as part of a team.\nStrong problem-solving and analytical skills.\nMust have worked with US customers and should have provided at least 3-4 hours overlap with Pacific Time (PT)\nBonus points:\nCertifications in cloud platforms\nShow more Show less","Hadoop, Aws Redshift, Pyspark, Spark, AWS Glue, Python"
Enterprise Data Architect - Ramboll Tech,Ramboll,5-7 Years,,"Chennai, India",Login to check your skill match score,"Company Description\n\nAbout Ramboll\n\nFounded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.\n\nEquality, diversity, and inclusion are at the heart of what we do\n\nWe believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.\n\nJob Description\n\nWe're passionate about using data to drive sustainability. You too Join us as an Enterprise Data Architect at Ramboll Tech, and shape our architecture to be data centric.\n\nWe are looking for a highly skilled and experienced Data Architect to join our team. You will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth. This role is vital to ensuring the seamless integration, management and deployment of data and analytics solutions that support our business needs. You will identify, analyse and proactively recommend how information assets drive business outcomes, to share consistent data throughout Ramboll.\n\nYou will be a part of our exciting digital transformation journey and play an active role in turning our data ambitions into concrete actions to deliver optimisations and revenue growth targets!\n\nWhat You Will Do\n\nYou will join Technology & Data Architecture in Ramboll Tech. You will be working across Ramboll, with focus on enterprise data layer, in collaboration with Domain Enterprise Architects, Data Strategy and Data Platform teams. You will partner with Innovation and Digital Transformation Directorswho drive digitalisation, innovation, and scaling of digital solutions & products across their respective business domains.\n\nAs our Enterprise Data Architect, you will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth.\n\nYour focus will be on delivering value by shaping data strategies, roadmaps, and solutions that directly address the challenges and opportunities of our business areas. This role requires expertise in modern data management and analysis technologies, alongside a deep understanding of corporate data management best practices.\n\nAs we operate in Architecture, Engineering and Consulting (AEC), you can expect a considerable focus on Building Information Modelling (BIM). We are looking for a candidate eager to engage in applying data-centric principles to BIM data to leverage generating valuable business insights.\n\nWhere you will make an impact:\n\nBusiness needs: Translate complex business requirements into robust, scalable, and secure data architectures that are aligned with enterprise-wide data strategy. We want to unlock new revenue streams and optimise operational efficiency, by leveraging our Enterprise Data Architecture to transform our data into valuable business assets, and implement Al-enabled solutions.\nDesign and implement modern data architectures: Lead the design and implementation of scalable, robust and secure data systems using modern data stack technologies. Ensure architecture is aligned with business objectives.\nImprove and ensure the overall quality of data, including ease of access and use of data assets. Define information and data strategies as a part of an overall business and technology strategy. Design and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.\nSupport data platform teams: Work with data platform teams to design and implement operating models that support efficient data processing, integrations, reporting and AI model deployment and execution.\nData modelling: Design and develop data models that support business processes, analytics and reporting requirements. Ensure that data models are optimised for performance and scalability.\nTechnology roadmaps: Actively engage with technology providers to understand critical, data relevant, technology roadmaps. Explore innovative solutions and evaluate their alignment with business goals and enterprise-wide data strategy\n\nKey responsibilities:\n\nWork with solution architects, developers and engineers to operationalise Data Strategy and build robust and scalable technology stacks and platforms for D&A solutions\nPartner with our Domain Enterprise Architects to understand business objectives. Develop and manage cloud-based data solutions on platforms such as Microsoft Azure, GCP or AWS. Optimise cloud infrastructure for performance, cost and scalability.\nDesign and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.\nWork closely with cross-functional teams, including business stakeholders, data scientists, data engineer and other SME colleagues in Ramboll Tech, to understand data requirements and deliver solutions that meet business needs.\nMaintaining repositories for representing the data elements including the entities, relationships and attributes, the information flows, and business glossary\nProvide mentorship and guidance to junior data engineers and other team members. Foster a culture of continuous learning and improvement within the team\nContinuously evaluate and recommend new tools and technologies that can improve the efficiency and effectiveness of data engineering processing\nCoach and mentor other architects, product teams and business stakeholders to instil architectural thinking, with focus on business, data/information and solution architecture.\nEffectively provide guiding principles, standard and minimal viable architectures, technology governance model informed by business strategy and corporate governance.\n\nQualifications\n\n\nEducation:\n\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\n\nExperience:\n\n5+ years of professional experience in data architecture, with a strong focus on cloud architecture, data integration and data modelling.\nDeep understanding of the modern data stack and its components.\nProven experience with cloud platforms such as Microsoft Azure, GCP or AWS.\nPrevious engagement in establishing data strategy and data governance.\nExperience in leading data modelling, database design, data warehousing, master data management, data governance practices. Strong skills in data modelling, ETL processes and data integration, having worked with data platform teams.\nExperience in securing and protection of sensitive information\nExperience in AEC industry is great to have. An ideal candidate will have a strong understanding of Building Information Modelling (BIM) principles and applications. Applied data-centric principles to BIM data, enabling integration with enterprise systems (e.g., improved project management, cost control, and facilities management). Developed data pipelines to extract, transform, and load BIM data for analysis and reporting. Explored opportunities to leverage BIM data for generating valuable insights, predictive and prescriptive maintenance\n\nSkills:\n\nExceptional analytical and problem-solving skills with the ability to design innovative solutions to complex data challenges.\nData Modelling (Conceptual, Logical, Physical), Data Warehousing, Data Lakes, Data Mesh, Data Governance, Metadata Management, Master Data Management.\nFamiliarity with the DAMA Data Management Framework (DAMA-DMBOK) and its application in managing data as a strategic asset\nUnderstanding of BIM principles, data structures (IFC), BIM software APIs, and BIM data integration with enterprise systems. (Great to have skill)\nAdvanced knowledge of data modelling tools, data lakes, SQL, and pipeline design\nExcellent communication and interpersonal skills, with the ability to convey technical concepts to non-technical stakeholders\nA strategic mindset with the ability to navigate and influence within a matrixed organization\nProven ability to lead projects and mentor junior team members\nRelevant certifications in cloud technologies are a plus, such as: Microsoft Certified Azure Solutions Architect Expert, or Data Engineering certifications, Snowflake certifications, Databricks certifications, Kafka certifications\n\nWe encourage you to apply even if your profile does not meet all the requirements for the role. We are looking for a diverse range of experiences, skills, and interests to enrich our team.\n\nAdditional Information\n\n\nWhat defines us: CURIOSITY, OPTIMISM, AMBITION, EMPATHY\n\n\nOur team at Ramboll Tech is currently on a steep growth trajectory while maintaining a strong team culture.\n\nWe are curious about other people and their motivations; about new business models and technologies; about each other and the future.\nWe are optimistic, focusing on solutions rather than problems; we plan for success and are willing to take calculated risks instead of playing it safe.\nWe are ambitious, setting our own standards higher than others expectations, and we celebrate each other's successes.\nWe are empathetic, taking ourselves, others, and each other seriously without prejudgment, and we help each other and our clients, colleagues, and the world.\n\nHow We Work As a Team\n\n\nOur team culture is crucial to us; that's why we take time daily to exchange ideas and discuss our work priorities. We support each other when facing challenges and foster a strong team spirit. We aim to learn and grow continuously from one another. We value diversity, and although we're not perfect, we regularly engage in open discussions about how we can improve in this area.\n\nOur current hybrid work approach focuses on adapting to different needs, including increased flexibility that works best for our global team and individuals, with as much autonomy as possible.\n\nWho Is Ramboll\n\n\nRamboll is a global architecture, engineering, and consultancy firm. We believe sustainable change's aim is to create a livable world where people thrive in healthy nature. Our strength is our employees, and our history is rooted in a clear vision of how a responsible company should act. Openness and curiosity are cornerstones of our corporate culture, fostering an inclusive mindset that seeks new, diverse, and innovative perspectives. We respect and welcome all forms of diversity and focus on creating an inclusive environment where everyone can thrive and reach their full potential.\n\nWhat Does Ramboll Tech Do\n\n\nRamboll Tech / GBA Innovation and Digital Transformation accelerates innovation and digital transformation for the entire Ramboll Group/ GBA and directs all AI initiatives within the company. We digitally enable and co-create with Ramboll's world-class experts to deliver solutions that drive sustainable change for our clients and society. This includes collaborating with Global Business Areas and Enabling Functions at Ramboll on their digital journey, developing proprietary AI and digital products for Ramboll and our clients, following our product life cycle. Ramboll Tech also works on larger technology projects within Ramboll and provides world-class Technology Operations. Ramboll Tech currently has over 300 employees globally, with strongholds across Nordics, Germany, the USA, and India. We are looking to quickly expand in key areas across Europe and the globe.\n\nImportant Information\n\nWe don't require a cover letterjust send us your current CV through our application tool, and we're eager to get to know you better in a conversation.\nDo you have any questions Feel free to contact Head of Technology & Data Architecture: Elvira Janas ([HIDDEN TEXT]). Thanks","pipeline design, Data Lakes, ETL processes, BIM principles and applications, Master Data Management, Data Modelling, Data Integration, Data Warehousing, Data Governance, Sql"
Senior Lead - Data Architect,KK Wind Solutions,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"An Azure Data Architect is a strategic role in the modern data-driven enterprise. Responsible for designing and implementing effective database solutions and models to store and retrieve company data, the Azure Data Architect ensures that enterprise data is efficiently stored, managed, and available to use in decision-making across the organization.\nResponsibilities:\nDesign and implement data architecture solutions on the Azure platform to support company data initiatives\nDevelop and optimize data models for transactional and analytical systems, ensuring they align with business objectives\nBuild and manage ETL/ELT workflows using tools such as Azure Data Factory, Databricks to ingest, transform, and integrate data across various sources\nArchitect and implement Azure Synapse Analytics or other data warehousing solutions for business intelligence and reporting\nImplement data governance practices to ensure data quality, consistency, and security\nMonitor and optimize the performance of Azure-based data solutions to ensure high availability and responsiveness\nWork closely with business stakeholders to understand their data needs and translate them into technical requirements\ns\nOur Requirement:\nMinimum of 5 years of experience in data architecture or a related field\nStrong knowledge of Azure data services with hands-on experience, including Azure SQL Database and Azure Data Lake\nExpertise in data integration tools and techniques, such as Azure Data Factory and Azure Databricks\nProficiency in programming and scripting languages, such as SQL, Python, and PowerShell\nRelevant certifications in Azure, such as Microsoft Certified: Azure Solutions Architect","Azure SQL Database, Azure Data Factory, PowerShell, Azure Data Lake, Azure Databricks, Sql, Python"
Data Architect,Datacrew.ai,8-10 Years,,"Chennai, India",Login to check your skill match score,"We are looking for a highly skilled Data Architect with a strong background in designing, implementing, and managing modern data architectures. The ideal candidate should have deep expertise in data modeling, data integration, data warehousing, and cloud data platforms, along with hands-on experience in SQL, Python, and data engineering tools. This role will be responsible for defining and implementing scalable data solutions, ensuring high performance, security, and data governance compliance.\nLocation: Chennai, Tamil Nadu\nMode: WFO\nExperience: 8+ yrs\nImmediate Joiners are only preferred\nKey Responsibilities:\nDesign and implement end-to-end data architectures, including OLTP, OLAP, data lakes, and data warehouses.\nDevelop and maintain data models, metadata management, and data cataloging frameworks.\nDefine data integration strategies using ETL/ELT pipelines for structured and unstructured data.\nWork closely with data engineers and business teams to ensure data availability, consistency, and quality across all platforms.\nEstablish and enforce data governance policies and security standards.\nOptimize query performance and data storage for efficiency in processing large datasets.\nEvaluate and implement modern cloud-based data solutions (AWS, Azure, GCP) as per business requirements.\nCollaborate with data scientists, analysts, and software engineers to enable AI/ML and advanced analytics use cases.\nSupport real-time data processing and streaming architectures for high-velocity data environments.\nTroubleshoot data architecture issues and provide solutions to enhance data workflows.\nRequired Skills & Qualifications:\nTechnical Skills:\nStrong proficiency in SQL (T-SQL, PL/SQL, or PostgreSQL) for complex queries and performance tuning.\nHands-on experience in Python for data engineering, automation, and analytics tasks.\nExpertise in data modeling (conceptual, logical, physical) and database design.\nExperience with data warehousing solutions (Snowflake, Redshift, BigQuery, Synapse, etc.).\nSolid understanding of ETL/ELT tools (dbt, Talend, Apache NiFi, Data Factory, Glue, Airflow).\nExperience with big data processing frameworks (Spark, Databricks, Hadoop).\nProficiency in cloud data platforms (AWS, Azure, GCP) and serverless architectures.\nKnowledge of real-time data streaming tools (Kafka, Kinesis, Pulsar).\nExperience with NoSQL databases (MongoDB, Cassandra) and graph databases is a plus.\nStrong understanding of data security, access control, and compliance regulations (GDPR, HIPAA, NDMO in Saudi Arabia, etc.).\nSoft Skills:\nExcellent problem-solving and analytical skills.\nStrong communication skills with the ability to translate technical concepts to business stakeholders.\nAbility to lead and mentor data engineers and collaborate across teams.\nStrong documentation and architectural planning skills.\nAbility to manage multiple priorities and work in a fast-paced environment.\nEducation & Experience:\nBachelor's/Master's degree in Computer Science, Information Systems, Data Engineering, or a related field.\n8+ years of experience in data architecture, database design, and data engineering.\nHands-on experience in designing and implementing enterprise-scale data solutions.\nCertifications in cloud platforms (AWS Certified Data Analytics, Azure Data Engineer, GCP Professional Data Engineer) are a plus.\nPreferred Qualifications (Nice to Have):\nExperience with DataOps and MLOps frameworks.\nHands-on experience in metadata management and data lineage tools (Collibra, Alation).\nExposure to graph databases and semantic web technologies.\nPrior experience in data mesh and decentralized data architectures","Pulsar, Gdpr, Hadoop, Data Modeling, Cassandra, Hipaa, Kafka, Sql, ELT, Kinesis, Gcp, Spark, graph databases, Data Warehousing, Databricks, Data Security, MongoDB, Azure, Python, AWS, Etl"
"Data Architect (Financial Crime), Director",NatWest Group,Fresher,,"Bengaluru, India",Login to check your skill match score,"Join us as a Data Architect (Financial Crime)\n\nFor someone with a background in defining business and application architectures and roadmaps for complex enterprises, this is an excellent opportunity to join our business. You'll be defining the intentional architecture foryour assigned scope in order to ensure that the current architecture being delivered by engineers best supports the enterprise and its long term strategy\nYou'll provide advisory support and embed governance to ensure projects align to our simplification strategy and comply with data standards while promoting and supporting effective data modelling, metadata management and alignment to enterprise data model and data controls\nWith valuable exposure, you'll be building and leveraging relationships with colleagues across the bank to ensure commercially focused decisions and to create long term value for the bank\nWe're offering this role at director level\n\nWhat you'll do\n\nAs a Data Architect, you'll be defining and communicating the current, resultant and target state data architecture for your assigned scope. You'll advise domain and project teams to ensure alignment with data architecture standards, data principles, data controls, target data architectures, data patterns and the banks simplification strategy.\n\nWe'll look to you to influence the development of business strategies at an organisational level, identifying transformational opportunities for Financial Crime associated with both new and existing data solutions.\n\nAs Well As This, You'll Be\n\nTranslating architecture roadmaps into packages of work that allow frequent incremental delivery of value to be included in product backlog\nDefining, creating and maintaining architecture models, roadmaps, standards and outcomes, using architecture strategies to ensure alignment to adjacent and higher level model\nCollaborate and support data management SMEs and data stewards ensuring metadata management aligns with enterprise data model\nConduct data design reviews to ensure solutions meet data standards, data principles, data controls, data patterns and target data architectures\nSeeking out and utilising continuous feedback, fostering adaptive design and engineering practices to drive the collaboration of programmes and teams around a common technical vision\n\nThe skills you'll need\n\nTo succeed in this role, you'll need expert knowledge of application architecture, and at least one business, data or infrastructure architecture with working knowledge of the remaining disciplines. You'll have excellent communication skills with the ability to clearly communicate complex technical concepts to colleagues, up to senior leadership level, along with a good understanding of Agile methodologies with experience of working in an Agile team.\n\nYou'll Also Demonstrate\n\nGood collaboration and stakeholder management skills\nExperience of developing, syndicating and communicating architectures, designs and proposals for action\nAn understanding of industry architecture frameworks, such as TOGAF and ArchiMate\nExperience in data modelling methodologies (3NF, Dimensional, Data Vault, Star Schema, etc). Knowledge of data related regulations such as GDPR, CCPA,PCI DSS, BCBS 239\nExperience of working with business solution vendors, technology vendors and products within the market\nA background in systems development change life cycles, best practices and approaches\nKnowledge of hardware, software, application and systems engineering","BCBS 239, Systems engineering, archimate, CCPA, data modelling methodologies, Pci Dss, Gdpr, Togaf, Agile Methodologies, Application Architecture, Infrastructure Architecture"
Data Architect,Impetus,10-12 Years,,"Indore, India",Login to check your skill match score,"Qualifications\nDegree Graduates/Postgraduate in CSE or related field\nlooking for candidates with hands on experience in Big Data and Cloud Technologies.\n10+ Years of experience Expertise in designing and developing applications using Big Data and Cloud technologies Must Have\nExpertise and hands-on experience* on Spark, and Hadoop echo system components Must Have\nExpertise and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have\nGood knowledge of Shell script & Java/Python Must Have\nGood knowledge of migration projects on Hadoop Good to Have\nGood Knowledge of one of the Workflow engines like Oozie, Autosys Good to Have\nGood knowledge of Agile Development Good to Have\nPassionate about exploring new technologies Must Have\nAutomation approach - Good to Have\nResponsibilities\nDefine Data Warehouse modernization approach and strategy for the customer\nAlign the customer on the overall approach and solution\nDesign systems for meeting performance SLA\nResolve technical queries and issues for team\nWork with the team to establish an end-to-end migration approach for one use case so that the team can replicate the same for other iterations","Java, Hadoop, Big Data, Autosys, Cloud Technologies, Gcp, Spark, Shell script, Oozie, Azure, Python, AWS"
Head of Data Architect,Azilen Technologies,10-12 Years,,"Ahmedabad, India",Login to check your skill match score,"Job Purpose:\nWe are seeking an experienced and visionary Head of Data to lead our data strategy, architecture, and engineering initiatives. This role requires a strong technical leader with hands-on expertise in data architecture, data engineering, and enterprise data solutions. The ideal candidate will have deep experience in designing and implementing large-scale data platforms, data warehousing, and big data solutions while also demonstrating the ability to build and manage high-performing data teams.\nWho you are:\n10+ years of experience in data architecture, data engineering, or related roles, with at least 3 years in a leadership capacity.\nStrong hands-on expertise in big data technologies (Hadoop, Spark, Kafka, Flink, etc.).\nDeep knowledge of data warehousing solutions (Databricks, Snowflake, Redshift, BigQuery, Synapse, etc.).\nExperience with cloud platforms (AWS, Azure, GCP) and hybrid data architectures.\nProven ability to design and implement ETL/ELT processes and data pipeline automation.\nStrong background in SQL, NoSQL, and Graph Databases.\nKnowledge of data governance, security, and compliance frameworks.\nExposure to AI/ML-driven data solutions and MLOps practices is a plus.\nLeadership & Client Management\nDemonstrated experience in managing data teams and driving data strategy.\nProven track record of leading enterprise-level data projects from ideation to execution.\nStrong stakeholder management, with the ability to engage with C-level executives and technical teams.\nExcellent problem-solving, analytical, and communication skills.\nAbility to handle ambiguity and drive solutions in fast-paced environments.\nPreferred Qualifications:\nMaster's or Bachelor's degree in Computer Science, Data Engineering, or a related field.\nIndustry certifications (AWS Certified Data Analytics, Google Professional Data Engineer, Snowflake Architect, etc.).\nExperience in data mesh, data fabric, or real-time analytics is a plus.\nWhat will excite us :\nStrategic Leadership & Architecture\nDefine and execute the overall data strategy, ensuring alignment with business goals and client needs.\nArchitect and implement scalable enterprise data solutions, including but not limited to big data, data warehousing, and real-time data streaming solutions.\nLead data governance, security, and compliance efforts to ensure data integrity and adherence to regulatory requirements.\nDrive modernization efforts, including cloud migration, data lake implementations, and AI-driven analytics platforms.\nServe as a trusted advisor to clients, understanding their business challenges and translating them into scalable data solutions.\nLead client workshops, technical deep-dives, and solutioning sessions, ensuring innovative and tailored recommendations.\nIndependently engage with mid-size to enterprise clients to assess data maturity, define roadmaps, and execute digital transformation initiatives.\nTechnical Expertise & Hands-on Execution\nProvide hands-on expertise in designing and implementing data platforms using cloud and on-premise technologies (AWS, Azure, GCP, Hadoop, Spark, Snowflake, Databricks, etc.).\nEstablish best practices for ETL/ELT processes, data pipeline automation, and data lake architectures.\nOversee data modeling, database optimization, and performance tuning for high-scale environments.\nLead the integration of AI/ML capabilities into data platforms to drive actionable insights.\nHire, mentor, and manage a high-performing team of data engineers, architects, and analysts.\nFoster a culture of innovation, continuous learning, and collaboration within the data team.\nDefine team goals, measure performance, and drive efficiency improvements.\nWhat will excite you:\nOpportunity to lead and shape the Data & AI practice in a fast-growing technology organization.\nWork on cutting-edge data solutions for enterprise clients across diverse industries.\nCollaborative and innovative culture with a focus on continuous learning.\nCompetitive compensation, benefits, and career growth opportunities.\nJob Location:\nAhmedabad - WFO","Flink, MLOps practices, data pipeline automation, snowflake, cloud platforms, Ai, ML-driven data solutions, Synapse, security and compliance frameworks, Kafka, ELT, Nosql, AWS, BigQuery, Hadoop, Redshift, Sql, Gcp, Spark, Data Governance, graph databases, Databricks, Azure, Etl"
Data Architect,McCain Foods,8-10 Years,,"Delhi, India",Login to check your skill match score,"Position Title: Data Architect\n\nPosition Type: Regular - Full-Time\n\nPosition Location: New Delhi\n\nGrade: Grade 05\n\nRequisition ID: 34658\n\nJob Purpose\n\nReporting to the Data Architect Lead, Global Data Architect will take a lead role in creating the enterprise data model for McCain Foods, bringing together data assets across agriculture, manufacturing, supply chain and commercial. This data model will be the foundation for our analytics program that seeks to bring together McCain's industry-leading operational data sets, with 3rd party data sets, to drive world-class analytics. Working with a diverse team of data governance experts, data integration architects, data engineers and our analytics team including data scientist, you will play a key role in creating a conceptual, logical and physical data model that underpins the Global Digital & Data team's activities. .\n\nJob Responsibilities\n\nDevelop an understanding of McCain's key data assets and work with data governance team to document key data sets in our enterprise data catalog\nWork with business stakeholders to build a conceptual business model by understanding the business end to end process, challenges, and future business plans.\nCollaborate with application architects to bring in the analytics point of view when designing end user applications.\nDevelop Logical data model based on business model and align with business teams\nWork with technical teams to build physical data model, data lineage and keep all relevant documentations\nDevelop a process to manage to all models and appropriate controls\nWith a use-case driven approach, enhance and expand enterprise data model based on legacy on-premises analytics products, and new cloud data products including advanced analytics models\nDesign key enterprise conformed dimensions and ensure understanding across data engineering teams (including third parties); keep data catalog and wiki tools current\nPrimary point of contact for new Digital and IT programs, to ensure alignment to enterprise data model\nBe a clear player in shaping McCain's cloud migration strategy, enabling advanced analytics and world-leading Business Intelligence analytics\nWork in close collaboration with data engineers ensuring data modeling best practices are followed\n\nMeasures Of Success\n\nDemonstrated history of driving change in a large, global organization\nA true passion for well-structured and well-governed data; you know and can explain to others the real business risk of too many mapping tables\nYou live for a well-designed and well-structured conformed dimension table\nFocus on use-case driven prioritization; you are comfortable pushing business teams for requirements that connect to business value and also able to challenge requirements that will not achieve the business goals\nDeveloping data models that are not just elegant, but truly optimized for analytics, both advanced analytics use cases and dashboarding / BI tools\nA coaching mindset wherever you go, including with the business, data engineers and other architects\nA infectious enthusiasm for learning: about our business, deepening your technical knowledge and meeting our teams\nHave a get things done attitude. Roll up the sleeves when necessary; work with and through others as needed\n\nKey Qualification & Experiences\n\nData Design and Governance\nAt least 5 years of experience with data modeling to support business process\nAbility to design complex data models to connect and internal and external data\nNice to have: Ability profile the data for data quality requirements\nAt least 8 years of experience with requirement analysis; experience working with business stakeholders on data design\nExperience on working with real-time data.\nNice to have: experience with Data Catalog tools\nAbility to draft accurate documentation that supports the project management effort and coding\n\nTechnical skills\nAt least 5 years of experience designing and working in Data Warehouse solutions building data models; preference for having S4 hana knowledge.\nAt least 2 years of experience in visualization tools preferably Power BI or similar tools.e\nAt least 2 years designing and working in Cloud Data Warehouse solutions; preference for Azure Databricks, Azure Synapse or earlier Microsoft solutions\nExperience Visio, Power Designer, or similar data modeling tools\nNice to have: Experience in data profiling tools informatica, Collibra or similar data quality tools\nNice to have: Working experience on MDx\nExperience in working in Azure cloud environment or similar cloud environment\nMust have : Ability to develop queries in SQL for assessing , manipulating, and accessing data stored in relational databases , hands on experience in PySpark, Python\nNice to have: Ability to understand and work with unstructured data\nNice to have at least 1 successful enterprise-wide cloud migration being the data architect or data modeler. - mainly focused on building data models.\nNice to have: Experience on working with Manufacturing /Digital Manufacturing.\nNice to have: experience designing enterprise data models for analytics, specifically in a PowerBI environment\nNice to have: experience with machine learning model design (Python preferred)\nBehaviors and Attitudes\nComfortable working with ambiguity and defining a way forward.\nExperience challenging current ways of working\nA documented history of successfully driving projects to completion\nExcellent interpersonal skills\nAttention to the details.\nGood interpersonal and communication skills\nComfortable leading others through change\n\nMcCain Foods is an equal opportunity employer. We see value in ensuring we have a diverse, antiracist, inclusive, merit-based, and equitable workplace. As a global family-owned company we are proud to reflect the diverse communities around the world in which we live and work. We recognize that diversity drives our creativity, resilience, and success and makes our business stronger.\n\nMcCain is an accessible employer. If you require an accommodation throughout the recruitment process (including alternate formats of materials or accessible meeting rooms), please let us know and we will work with you to meet your needs.\n\nYour privacy is important to us. By submitting personal data or information to us, you agree this will be handled in accordance with the Global Employee Privacy Policy\n\nJob Family: Information Technology\n\nDivision: Global Digital Technology\n\nDepartment: Data Architect\n\nLocation(s): IN - India : Haryana : Gurgaon\n\nCompany: McCain Foods(India) P Ltd","Machine Learning Model Design, MDx, Data Catalog Tools, Data Profiling Tools, Data Warehouse Solutions, Data Modeling, Collibra, Power Bi, Pyspark, S4 Hana, Azure Databricks, Informatica, Sql, Cloud Migration, Azure Synapse, Data Governance, Python"
Data Architect,ACL Digital,10-12 Years,,"Pune, India",Login to check your skill match score,"Exp: 10+ yrs\nLocation: Balewadi, Pune\nNotice Period: Immediate to 15 Days\nResponsibilities\nStrong understanding of Data Architecture and models and experience leading data driven projects.\nSolid expertise with strong opinions on Data Modelling paradigms such as Kimball, Inmon, Data Marts, Data Vault, Medallion etc.\nStrong experience with Cloud Based data strategies and big data technologies AWS Preferred. Ability to create backend services in Python that enables the data pipelines is required.\nDemonstrated experience on designing data platforms on AWS for batch and stream processing pipelines.\nHands-on experience using AWS Managed and other big data services such as EMR, Glue, S3, Kinesis, DynamoDB, ECS is a must.\nStrong understanding of working of Apache Spark is a must.\nStrong understanding of various Data Lake/Lakehouse storage formats such as Delta, Iceberg, Hudi.\nExperience designing data lakehouse with Medallion architecture is desirable.\nSolid experience in designing data pipelines for ETL with expert knowledge on ingestion, transformation, and data quality is a must.\nHands-on experience in SQL is a must.\nExpertise designing ETL pipelines combining Python + SQL is required.\nUnderstanding of data manipulation libraries in python like Pandas, Polars, DuckDB is desired.\nExperience in designing the Data visualization with different tools such as Tableau and PowerBI is desirable.\nWorking knowledge of other Data Platforms on Azure, Databricks, Snowflake is desirable but not must.","data manipulation libraries, snowflake, DuckDB, Glue, Lakehouse, Polars, data pipelines, Delta, Hudi, Iceberg, Cloud Based data strategies, Big Data Technologies, Databricks, Sql, Emr, Dynamodb, Data Lake, Tableau, Pandas, Etl, ECS, S3, Data Architecture, Apache Spark, Kinesis, AWS, Powerbi, Python, Azure, Data Modelling"
Data Architect,CDK Global,10-12 Years,,"Pune, India",Login to check your skill match score,"Position Scope\n\nA Senior Data Engineer with Architect level experience and exposure\nMay lead project teams or project phases of larger scope\nWorks independently with minimal guidance and direction\nImpacts a range of customer, operational, project or service activities within own team and related work teams\nContributes to the development of concepts, methods, and techniques\nModerate impact on the functional/business unit\n\nFunctional Knowledge\n\nRequires in-depth knowledge of principles, concepts within own function/specialty and basic knowledge of other related areas.\nApplies broader knowledge of industry standards/practices to assignments\n\nProblem Solving & Critical Thinking\n\nSolves variety of problems of moderately complex or unusual within own area\nApplies independent judgement to develop creative and practical solutions based on the analysis of multiple factors\nAnticipates and identifies problems and issues\n\nLeadership\n\nGuided by area goals and objectives\nMay provide technical direction to others around the completion of short-term work goals\n\nCollaboration\n\nTrains and guides others in work area on technical skills\nNetworks with senior colleagues in own area of expertise\n\nEducation & Experience\n\nBachelor's degree in computer science, Engineering, or related field with at least 10-12 years of experience or a Masters degree; OR in lieu of a Bachelor's degree, at least 12-14 years of experience\nGood Experience with Looker, Snowflake, Postgres, Azure, AWS, Terraform, Kafka, SQL Server, CI/CD, CDC\nStrong in Data warehousing using SQL Server and strong TSQL\nUnderstanding of utilizing Agile software development methodologies\nDeep knowledge of at least one programming language along with ability to execute on complex programming tasks.\nAbility to document, track and monitor a problem/issue to a timely resolution\nKnowledge of operating systems\nCollaborative problem-solving ability and self-motivated\nStrong verbal and written communication skills along with prioritization of duties\n\nAt CDK, we believe inclusion and diversity are essential in inspiring meaningful connections to our people, customers and communities. We are open, curious and encourage different views, so that everyone can be their best selves and make an impact.\n\nCDK is an Equal Opportunity Employer committed to creating an inclusive workforce where everyone is valued. Qualified applicants will receive consideration for employment without regard to race, color, creed, ancestry, national origin, gender, sexual orientation, gender identity, gender expression, marital status, creed or religion, age, disability (including pregnancy), results of genetic testing, service in the military, veteran status or any other category protected by law.\n\nApplicants for employment in the US must be authorized to work in the US. CDK may offer employer visa sponsorship to applicants.","cdc, snowflake, Looker, CI CD, Terraform, Postgres, SQL Server, Kafka, Tsql, Azure, AWS"
Data Architect,NewVision Software,Fresher,,"Pune, India",Login to check your skill match score,"Primary Skills:\nData Engineering using Azure stack - Data Handling, Data Modeling, Data Integration, Data Governance\nAzure Data Lake Analytics,\nAzure Synapse Analytics Engineering,\nAzure Data Factory,\nAzure Databricks,\nAzure Dataflows,\nPower BI(Expert in DAX)\nScala or Python,\nPyspark, Hadoop, Spark, Hive, Kafka, Sqoop\nT-SQL,\nNoSQL,\nCertified Azure Solution Architect\nSecondary Skills:\nSource code control systems such as GIT, AzureDevops\nDevOps Basics.\nKey Responsibilities:\n1. Designing Solutions related to data to handle data silos of our customers.\n2. Creating Data Models as part of solutions to suffice customer's needs while following best practices.\n3. Handling Data Integration with structured and Unstructured sources of data.\n4. Implementing Data Governance and security on top of the existing DWH to enhance reliability.\n5. Monitoring the existing data flows and maintaining them thereby helping to resolve bugs.\n6. Collaborating with a team whenever needed and providing business intelligence solutions.\n7. Actively participating in PI Planning and assisting the team during Sprints.\n8. Work as part of a team to develop Cloud Data and Analytics solutions.","Power BI Expert in DAX, Certified Azure Solution Architect, DevOps Basics, AzureDevops, Scala or Python, Azure Synapse Analytics Engineering, Azure Data Lake Analytics, Azure Dataflows, Pyspark, T-sql, Hadoop, Kafka, Azure Databricks, Nosql, Git, Azure Data Factory, Hive, Sqoop, Spark"
Salesforce Data Cloud Architect,NTT DATA North America,Fresher,,"Hyderabad, India",Login to check your skill match score,"NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.\n\nWe are currently seeking a Salesforce Data Cloud Architect to join our team in Hyderabad, Telangana, India.\n\nSalesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.\nData Modeling: Strong experience in designing and implementing data models.\nData Integration: Experience with data integration tools and techniques.\nData Quality: Understanding of data quality concepts and practices.\nData Governance: Knowledge of data governance principles and practices.\nSQL: Proficiency in SQL for data querying and manipulation.\nProblem-Solving: Strong analytical and problem-solving skills.\nCommunication: Excellent communication and collaboration skills.\n\n#Salesforce\n\nAbout NTT DATA\n\nNTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com\n\nNTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Salesforce Data Cloud, Data Quality, Data Modeling, Data Governance, Sql, Data Integration"
Sr. Data Architect - Snowflake,Arting Digital,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect - Snowflake\n\nExperience: 12+ years\n\nBudget: Up to 38 LPA\n\nNotice Period: Immediate to 30 days\n\nLocation: Trivandrum, Bangalore, Chennai\n\nEducation: Bachelor's or Master's degree in Computer Science, Information Systems, or a related field\n\nSkill Set: Snowflake experience, Data Architecture experience, ETL process experience, Large Data migration solutions experience , data modelling, schema design DBT , Python/Java/Scala, SQL, ETL process , cloud data warehousing concept , data integration , AWS, Azure, and GCP , CDC , DataOps methodologies , cloud platforms/Snow-flak , data visualisation tools (e.g., Tableau, Power BI , data security and compliance standard\n\nJob Description:\n\nWe are seeking an experienced Senior Data Architect Snowflake to lead and design scalable, high-performance data solutions. The ideal candidate should have extensive expertise in data architecture, large-scale data migration, ETL processes, and cloud-based data warehousing. You will play a key role in designing optimized data models, ensuring efficient data integration, and implementing best practices for Snowflake and other cloud platforms.\n\nKey Responsibilities:\n\nArchitect, design, and implement scalable Snowflake-based data solutions.\nDevelop data models, schema designs, and ETL pipelines to support business requirements.\nLead large-scale data migration projects while ensuring performance optimization.\nImplement DataOps methodologies for efficient data management and automation.\nWork with AWS, Azure, and GCP to deploy cloud-based data architectures.\nEnsure data security and compliance with industry standards.\nOptimize CDC (Change Data Capture) processes for real-time data updates.\nUtilize DBT, Python, Java, or Scala to enhance data transformation and integration.\nDesign and implement data visualisation solutions using tools like Tableau and Power BI.\n\nRequired Skills & Expertise:\n\nStrong expertise in Snowflake and cloud data warehousing concepts.\nHands-on experience with ETL processes, data modelling, and schema design.\nProficiency in SQL, DBT, Python, Java, or Scala for data transformation and automation.\nExperience in data integration and large-scale data migration solutions.\nKnowledge of CDC (Change Data Capture) methodologies.\nFamiliarity with DataOps practices and modern data engineering workflows.\nExposure to AWS, Azure, and GCP cloud platforms.\nStrong understanding of data security, governance, and compliance.\nExperience with data visualisation tools such as Tableau or Power BI.","cdc, Data security and compliance, DataOps methodologies, snowflake, dbt, ETL processes, Cloud data warehousing, Large Data migration solutions, Java, Scala, Schema Design, Sql, Gcp, Data Integration, Data Architecture, Data Modelling, Azure, Python, AWS"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nBe part of a team that is transforming BCG into a bionic company! We are building a centralized Business Intelligence & Analytics function that will simplify and automate information deliveryproviding advanced insights and analysis to support decision making. To date, the team has launched and operationalized several global scale products and dashboards, enhancing how our leaders engage in information to manage the business. The next wave of digital reporting is underway which will help to unlock further value for BCG functions and leadership with best-in-class business intelligence and analytics.\n\nThe Global IT Data Architect Sr. Manager works as an integral part of an Agile team delivering scalable digital reporting solutions for global stakeholders. This role will help design optimal solutions to support the digital product portfolio. Your responsibilities include: -\n\nDefine data architecture for solutions BI&A reporting.\nEvaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.\nCollaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing PoCs to ensure those standards are implemented\nProvide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.\nDevelop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.\nCreate and maintain conceptual / logical data models to identify key business entities and visual relationships.\nWork with business and IT teams to understand data requirements.\nMaintain a data dictionary consisting of table and column definitions.\nReview data models with both technical and business audiences.\nPartner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.\nLead to design / build new models to efficiently deliver the financial results to senior management.\n\nWhat You'll Bring\n\nBachelor's degree or equivalent combination of education and experience.\nBachelor's degree in information science, data management, computer science or related field preferred.\n12+ years IT experience, including 8+ years in ELT/Data Integration for BI.\nExperience of working on large transformational program with end-to-end ownership of implementation.\nHands-on implementation experience with Data Warehousing including design, modelling, testing, security, administration and optimization.\nExpertise in cloud databases like Snowflake/RedShift, data catalogue, MDM etc.\nExpertise in writing SQL and database procedures.\nProficient in Data Modelling- Conceptual, logical, and Physical modelling\nProficient in documenting all the architecture related work performed.\nExperience in data storage, ETL/ELT and data analytics tools and technologies e.g., Snowflake, DBT, Tableau, Power BI, etc\nExperienced in Data Warehousing design/development and BI/ Analytical systems\nExperience working projects using Agile methodologies\nStrong hands-on experience with data and analytics data architecture, solution design, and engineering experience\nExperience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake\nExperience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs)\nExcellent written, oral communication and presentation skills to present architecture, features, and solution recommendations\nExperience in all aspects of Agile SDLC, and end to end participation in a project lifecycle\nExperience in Python, Data frames is a plus\nExperience within GenAI space is a plus\nReporting tool experience Tableau, PowerBI, SAP BO is a plus\n\nWho You'll Work With\n\nWorking closely with the rest of the Business Intelligence delivery teams, the product owner, Analyst, and Engineers. Architect will also work with external developers, business contacts across BCG functions, DBA's, and Infrastructure teams.\n\nAdditional info\n\nYOU'RE GOOD AT\n\nThis Position Will Involve Daily Collaboration With The Architect And Other Development Teams, Vendors And Stakeholders Throughout Agile Design, Plus The Development, Implementation And Operations Of Both Infrastructure And Business Mappings. The Successful Candidate Will Demonstrate\n\nStrong analytical abilities and creative problem solving\nAbility to work independently with general direction and flexibility in a fast-paced environment\nGood organization and excellent communication skills across cultures\nIntegrity and a positive attitude, especially while handling stressful situations\nWork with project stakeholders (technical as well as end users) to understand business requirements and implement database solutions for diverse problems\nResearch viable technical and/or non-technical solutions, evaluate new technology and advocate, influence, and build consensus for innovations that satisfy business needs\nDesign, document & train the team on the overall processes and process flows for the Data architecture.\nResolve technical challenges in critical situations that require immediate resolution.\nDevelop relationships with external stakeholders to maintain awareness of data and security issues and trends.\nReview work from other tech team members and provide feedback for growth.\nImplement Data Architecture and Data security policies that align with governance objectives and regulatory requirements\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","Sql, Data Architecture, Data Warehousing, Agile Methodologies"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata\n\nwork Mode- Hybrid\n\nRoles And Responsibilities\n\nMandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization\n\nSolution Architect for Data modelling Understanding of Enterprise datasets Sales,\n\nProcurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle\n\netc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building\n\nData lake foundation, Maintenance etc)\n\nCollaborate with the product/business teams, understand related business processes and\n\ndocument business requirements and then write high level specifications/requirements for DEs\n\nDevelop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout\n\nfor right grain of data either in True source systems or in Data WHs and build reusable data\n\nmodels in intermediary layers before creating physical consumable views from data mart\n\nUnderstand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data\n\nGovernance, Data Quality frameworks, Data Observality and the candidate should be:\n\nFamiliar with DevOps process\nKnowing how to check existing tables, data dictionary, table structures\nExperienced with normalizing tables\nHaving good understanding of Landing, Bronze, Silver and Gold layers and concepts\nFamiliar with Agile techniques\nCreate business process map, user journey map and data flow integration diagrams; Understand\n\nIntegration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of\n\nmodels\n\nStakeholder management with data engineering, product owners, central data modelling team,\n\ndata governance & stewards, Scrum master, project team and sponsor.\n\nAbility to handle large implementation program with multiple projects spanning over an year.\n\nSkills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Agile techniques, Integration through API, DataOps, Data observability, Data Modeling, Sql, Cloud Architecture, Devops, Dimensional Modeling, SAP, FTP, Data Modeler, Data Quality, Oracle, Data Governance, Data Architect, Azure, Sftp, Data Lake, Webservices"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nDefine and design future state data architecture for HR reporting, forecasting and analysis products.\nPartner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.\nEngage with line of business, operations, and project partners to gather process improvements.\nLead to design / build new models to efficiently deliver the financial results to senior management.\nEvaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.\nCollaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing POCs to ensure those standards are implemented.\nProvide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.\nDevelop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.\nCreate and maintain conceptual / logical data models to identify key business entities and visual relationships.\nWork with business and IT teams to understand data requirements.\nMaintain a data dictionary consisting of table and column definitions.\nReview data models with both technical and business audience\n\nWhat You'll Bring\n\nEssential Education\n\nMinimum of a Bachelor's degree in Computer science, Engineering or a similar field\nAdditional Certification in Data Management or cloud data platforms like Snowflake preferred\n\nEssential Experience & Job Requirements\n\n12+ years of IT experience with major focus on data warehouse/database related projects\nExpertise in cloud databases like Snowflake, Redshift etc.\nExpertise in Data Warehousing Architecture; BI/Analytical systems; Data cataloguing; MDM etc\nProficient in Conceptual, Logical, and Physical Data Modelling\nProficient in documenting all the architecture related work performed.\nProficient in data storage, ETL/ELT and data analytics tools like AWS Glue, DBT/Talend, FiveTran, APIs, Tableau, Power BI, Alteryx etc\nExperience in building Data Solutions to support Comp Benchmarking, Pay Transparency / Pay Equity and Total Rewards use cases preferred.\nExperience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake a plus\nExperience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs) a plus\nExcellent written, oral communication and presentation skills to present architecture, features, and solution recommendations is a must\n\nAdditional info\n\nYOU'RE GOOD AT\n\nDesign, document & train the team on the overall processes and process flows for the Data architecture.\nResolve technical challenges in critical situations that require immediate resolution.\nDevelop relationships with external stakeholders to maintain awareness of data and security issues and trends.\nReview work from other tech team members and provide feedback for growth.\nImplement Data security policies that align with governance objectives and regulatory requirements.\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","Cloud Databases, Cloud Big Data Technologies, FiveTran, Physical Data Modelling, Data Analytics Tools, dbt, MDM, Meta Scrum, Data Cataloguing, Kanban, Data Warehousing Architecture, BI Analytical Systems, Alteryx, AWS Glue, Tableau, ELT, Data Architecture, Scrum, Talend, AWS, Apis, Power Bi, Agile Methodologies, Gcp, Azure, Etl"
Data Architect,Arcadis,7-9 Years,,"Noida, India",Login to check your skill match score,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.\n\nWe are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world's most complex challenges and deliver more impact together.\n\nIndividual Accountabilities\n\nCollaboration\n\nCollaborates with domain architects in the DSS, OEA, EUS, and HaN towers and if appropriate, the respective business stakeholders in architecting data solutions for their data service needs.\nCollaborates with the Data Engineering and Data Software Engineering teams to effectively communicate the data architecture to be implemented. Contributes to prototype or proof of concept efforts.\nCollaborates with InfoSec organization to understand corporate security policies and how they apply to data solutions.\nCollaborates with the Legal and Data Privacy organization to understand the latest policies so they may be incorporated into every data architecture solution.\nSuggest architecture design with Ontologies, MDM team.\n\nTechnical Skills & Design\n\nSignificant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS and data warehouses.\nDeep understanding of modern data services in leading cloud environments, and able to select and assemble data services with maximum cost efficiency while meeting business requirements of speed, continuity, and data integrity.\nCreates data architecture artifacts such as architecture diagrams, data models, design documents, etc.\nGuides domain architect on the value of a modern data and analytics platform.\nResearch, design, test, and evaluate new technologies, platforms and third-party products.\nWorking experience with Azure Cloud, Data Mesh, MS Fabric, Ontologies, MDM, IoT, BI solution and AI would be greater assets.\nExpert troubleshoot skills and experience.\n\nLeadership\n\nMentors aspiring data architects typically operating in data engineering and software engineering roles.\n\nKey Shared Accountabilities\n\nLeads medium to large data services projects.\nProvides technical partnership to product owners\nShared stewardship, with domains architects, of the Arcadis data ecosystem.\nActively participates in Arcadis Tech Architect community.\n\nKey Profile Requirements\n\nMinimum of 7 years of experience in designing and implementing modern solutions as part of variety of data ingestion and transformation pipelines\nMinimum of 5 years of experience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working in large scale development and cloud environment.\n\nWhy Arcadis\n\nWe can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It's why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.\n\nYou'll do meaningful work, and no matter what role, you'll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.\n\nTogether, we can create a lasting legacy.\n\nJoin Arcadis. Create a Legacy.\n\nOur Commitment to Equality, Diversity, Inclusion & Belonging\n\nWe want you to be able to bring your best self to work every day, which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people.","modern data services, structured and unstructured data, Ai, ontologies, data architecture artifacts, architecture diagrams, cloud environments, data models, MDM, MS Fabric, Data Mesh, Iot, RDBMS, Azure Cloud, data warehouses"
Data Architect,Aumni Techworks,8-10 Years,,"Pune, India",Login to check your skill match score,"Position Summary:\n\nWe are looking for a highly skilled and experienced Data Engineer who focus on leading the development, and implementation of our Data Warehouse/Lakehouse solution, ensuring it serves as the foundation for scalable, high-performance analytics.\n\nResponsibilities:\n\nLakehouse Design & Implementation:\n\nLead the end-to-end development and deployment of a scalable and secure Lakehouse architecture.\nDefine best practices for data ingestion, storage, transformation, and processing using modern cloud technologies.\nArchitect data pipelines using ETL/ELT frameworks to support structured, semi-structured, and unstructured data.\nOptimize data modeling strategies to meet the analytical and performance needs of stakeholders.\nEvaluate and select appropriate cloud technologies, frameworks, and architectures.\n\nRequirement:\n\nExperience:\n\n8+ years of experience in data engineering, with a proven track record of implementing large-scale data solutions.\nExtensive experience with cloud platforms (AWS, GCP, or Azure), specifically in data warehouse/lakehouse implementations.\nExpertise in modern data architectures with tools like Databricks, Snowflake, or BigQuery.\nStrong background in SQL, Python, and distributed computing frameworks (Spark, Dataflow, etc.).\nIn-depth knowledge of data modeling principles (e.g., Star Schema, Snowflake Schema).\nExperience in enabling AI tools to consume data from the Lakehouse.\n\nAbout Aumni Techworks:\n\nEstablished in 2016, Aumni Techworks partners with its multinational clients to incubate and operate remote teams in India using the AumniBOT model. With a team of 250 and growing, our mission is to provide a quality alternative to project-based outsourcing.\n\nBenefits of working at Aumni Techworks:\n\nWork within a product team on cutting edge tech with one of the best pay packages.\nNo politics, no bench, voice your opinion, flat hierarchy, and global exposure\nWork environment to re-live our fun college days (awarded as Best culture by Pune Mirror)\nRecharge frequently with Friday socials, dance classes, theme parties and monsoon picnic.\nBreakout spaces at the office Gym, Pool, TT, Foosball and Carrom\nHealth focused Insurance coverage and get in shape with AumniFit (Do not miss our 4 PM plank!)","Snowflake Schema, Data ingestion, snowflake, ELT frameworks, Lakehouse architecture, BigQuery, Data Modeling, Cloud Technologies, Sql, Gcp, Spark, Databricks, DataFlow, Azure, Star Schema, Python, AWS"
Data Architect,LSEG (London Stock Exchange Group),12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Data Architect Corporate Engineering\n\nCompany Profile\n\nLSEG (London Stock Exchange Group) is a world-leading financial markets infrastructure and data business. We are dedicated, open-access partners with a commitment to excellence in delivering services across Data & Analytics, Capital Markets, and Post Trade.\n\nBacked by three hundred years of experience, innovative technologies, and a team of over 23,000 people in 70 countries, our purpose is driving financial stability, empowering economies, and enabling customers to create sustainable growth.\n\nRole Profile\n\nIn this role, you'll be joining our CRM, External Digital and Marketing team within Data and Integrations Team, Corporate Engineering (CE) as a Data Architect. This team works on data and integration requests by guiding customers on data migration cleansing, data quality techniques. This role impacts all divisional users of CRM Technology and downstream systems reliant on Customer Data 7k users across Capital Markets, Post Trade and Data & Analytics.\n\nThe data and integrations team do this by:\n\nConducting data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion/AWS Glue).\nBuilding Enterprise Data Architecture or Data Lake for the Migration projects\nDefining the data quality of the sources and lists the data quality metrics of the source systems.\nWorking on detailed scoping and requirements working with business customers, SMEs, Technology Partner organizations and internal CRM Technology and Corporate Technology teams.\nBuilding and maintaining Customer Master and Product Master.\nContinuously review operating metrics and data to find opportunities to improve.\n\nTech Profile/Essential Skills\n\n12 years of technical experience.\n7 years of experience on data migration and reporting using ETL and Reporting Tools\n5 years of experience on ETL/Database Development\n2 years of experience on Salesforce Data Migration projects\nExpert level skill on Informatica IICS or Matillion or AWS Glue or equivalent ETL tool which covers from extracting the source to building the complex mappings.\nProficient in the use or extract of data from Salesforce.\nMust have knowledge on the Salesforce Data modelling.\nMust have Experience on Snowflake Storage and Database.\nExpert level coding knowledge on Python to do ETL.\nAble to translate business requirements into data solutions.\nUnderstanding of Salesforce concepts.\nSnowflake development.\nExperience with providing technical solutions and supporting documentation.\n\nPreferred Skills And Experience\n\nExperience of Tableau and/or Power BI reporting preferred for management reporting.\nMust have experience of working with Cloud Native based applications.\nUnderstanding of the SDLC and agile delivery methodology.\nExperience working with databases and data, performing data cleanup, and/or data manipulation and migration to and from Salesforce.com.\nShould have experience with Enterprise Architect or Erwin Data modeler\nAbility to handle own work and multitask to meet tight deadlines without losing sight of priorities under minimum supervision.\nHighly motivated, self-directed individual with a positive & pro-active demeanor to work.\nCustomer and service focused, with determination to meet their needs and expectations.\nBe driven and committed to the goals and objectives of the team and organization.\n\nEducation and Professional Skills\n\nProfessional qualification or equivalent.\nBS/MS degree in Computer Science, Software Engineering or STEM degree (Desirable).\nCurious about new technologies and tools, creative thinking and initiative taking.\nAgile related certifications preferable.\n\nDetailed Responsibilities\n\nProficient in data discovery, data migration, data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion Preferable).\nAnalyses the data quality of the sources and lists the data quality metrics of the source systems.\nDomain expert in MDM and Customer data.\nLead the Single Customer View and Customer 360 implementations.\nLead the data migration strategies for large scale programs.\nData mining to uncover patterns, anomalies, and correlations in large data sets.\nData management to efficiently and cost-effectively collect, store, and use data.\nCoding languages like Python and Java to develop applications for data analysis.\nMachine learning to build scalable systems for handling Big Data Systems.\nStructured query language (SQL) to manipulate data.\nData modelling tools like Erwin or Visio to visualize metadata and database schema.\nCreating and implementing data management processes and procedures\nResearching data acquisition opportunities.\nDeveloping application programming interfaces (APIs) to retrieve data.\nDevelops and improves data governance and business data processes within the Technology and business organizations and understands client requirements, specifying and analyzing these to a sufficient level of detail to ensure transparency of definition and ability for technical teams to translate to a technical solution design.\nWorks with developers, architects, and solution designers to translate sophisticated business requirements and provides feedback on technical solutions proposed.\nResponsible for building a relationship with partners, collaborators and impacted users.\nDemonstrates proposed solutions and seeks and addresses feedback.\nProactively identifies, recommends, and implements improvements to the process as it relates to assigned projects.\nFlexible in approach, adapting plans and strategies to help handle risks around ambiguity.\nStrategic problem solver with strong intuition for business and well-versed in current technological trends and business concepts.\n\nLSEG Benefits\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate based on anyone's race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nLSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.\n\nOur purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.\n\nWorking with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.\n\nLSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.\n\nWe are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.\n\nPlease take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.\n\nIf you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","Erwin Data modeler, snowflake, Salesforce Data Migration, IICS, Matillion, Sql, AWS Glue, Tableau, Power Bi, Etl, Enterprise Architect, Python, Informatica"
Data Architect,McCain Foods,8-10 Years,,"Delhi, India",Login to check your skill match score,"Position Title: Data Architect\n\nPosition Type: Regular - Full-Time\n\nPosition Location: New Delhi\n\nGrade: Grade 05\n\nRequisition ID: 34658\n\nJob Purpose\n\nReporting to the Data Architect Lead, Global Data Architect will take a lead role in creating the enterprise data model for McCain Foods, bringing together data assets across agriculture, manufacturing, supply chain and commercial. This data model will be the foundation for our analytics program that seeks to bring together McCain's industry-leading operational data sets, with 3rd party data sets, to drive world-class analytics. Working with a diverse team of data governance experts, data integration architects, data engineers and our analytics team including data scientist, you will play a key role in creating a conceptual, logical and physical data model that underpins the Global Digital & Data team's activities. .\n\nJob Responsibilities\n\nDevelop an understanding of McCain's key data assets and work with data governance team to document key data sets in our enterprise data catalog\nWork with business stakeholders to build a conceptual business model by understanding the business end to end process, challenges, and future business plans.\nCollaborate with application architects to bring in the analytics point of view when designing end user applications.\nDevelop Logical data model based on business model and align with business teams\nWork with technical teams to build physical data model, data lineage and keep all relevant documentations\nDevelop a process to manage to all models and appropriate controls\nWith a use-case driven approach, enhance and expand enterprise data model based on legacy on-premises analytics products, and new cloud data products including advanced analytics models\nDesign key enterprise conformed dimensions and ensure understanding across data engineering teams (including third parties); keep data catalog and wiki tools current\nPrimary point of contact for new Digital and IT programs, to ensure alignment to enterprise data model\nBe a clear player in shaping McCain's cloud migration strategy, enabling advanced analytics and world-leading Business Intelligence analytics\nWork in close collaboration with data engineers ensuring data modeling best practices are followed\n\nMeasures Of Success\n\nDemonstrated history of driving change in a large, global organization\nA true passion for well-structured and well-governed data; you know and can explain to others the real business risk of too many mapping tables\nYou live for a well-designed and well-structured conformed dimension table\nFocus on use-case driven prioritization; you are comfortable pushing business teams for requirements that connect to business value and also able to challenge requirements that will not achieve the business goals\nDeveloping data models that are not just elegant, but truly optimized for analytics, both advanced analytics use cases and dashboarding / BI tools\nA coaching mindset wherever you go, including with the business, data engineers and other architects\nA infectious enthusiasm for learning: about our business, deepening your technical knowledge and meeting our teams\nHave a get things done attitude. Roll up the sleeves when necessary; work with and through others as needed\n\nKey Qualification & Experiences\n\nData Design and Governance\nAt least 5 years of experience with data modeling to support business process\nAbility to design complex data models to connect and internal and external data\nNice to have: Ability profile the data for data quality requirements\nAt least 8 years of experience with requirement analysis; experience working with business stakeholders on data design\nExperience on working with real-time data.\nNice to have: experience with Data Catalog tools\nAbility to draft accurate documentation that supports the project management effort and coding\n\nTechnical skills\nAt least 5 years of experience designing and working in Data Warehouse solutions building data models; preference for having S4 hana knowledge.\nAt least 2 years of experience in visualization tools preferably Power BI or similar tools.e\nAt least 2 years designing and working in Cloud Data Warehouse solutions; preference for Azure Databricks, Azure Synapse or earlier Microsoft solutions\nExperience Visio, Power Designer, or similar data modeling tools\nNice to have: Experience in data profiling tools informatica, Collibra or similar data quality tools\nNice to have: Working experience on MDx\nExperience in working in Azure cloud environment or similar cloud environment\nMust have : Ability to develop queries in SQL for assessing , manipulating, and accessing data stored in relational databases , hands on experience in PySpark, Python\nNice to have: Ability to understand and work with unstructured data\nNice to have at least 1 successful enterprise-wide cloud migration being the data architect or data modeler. - mainly focused on building data models.\nNice to have: Experience on working with Manufacturing /Digital Manufacturing.\nNice to have: experience designing enterprise data models for analytics, specifically in a PowerBI environment\nNice to have: experience with machine learning model design (Python preferred)\nBehaviors and Attitudes\nComfortable working with ambiguity and defining a way forward.\nExperience challenging current ways of working\nA documented history of successfully driving projects to completion\nExcellent interpersonal skills\nAttention to the details.\nGood interpersonal and communication skills\nComfortable leading others through change\n\nMcCain Foods is an equal opportunity employer. We see value in ensuring we have a diverse, antiracist, inclusive, merit-based, and equitable workplace. As a global family-owned company we are proud to reflect the diverse communities around the world in which we live and work. We recognize that diversity drives our creativity, resilience, and success and makes our business stronger.\n\nMcCain is an accessible employer. If you require an accommodation throughout the recruitment process (including alternate formats of materials or accessible meeting rooms), please let us know and we will work with you to meet your needs.\n\nYour privacy is important to us. By submitting personal data or information to us, you agree this will be handled in accordance with the Global Employee Privacy Policy\n\nJob Family: Information Technology\n\nDivision: Global Digital Technology\n\nDepartment: Data Architect\n\nLocation(s): IN - India : Haryana : Gurgaon\n\nCompany: McCain Foods(India) P Ltd","Machine Learning Model Design, MDx, Data Catalog Tools, Data Profiling Tools, Data Warehouse Solutions, Data Modeling, Collibra, Power Bi, Pyspark, S4 Hana, Azure Databricks, Informatica, Sql, Cloud Migration, Azure Synapse, Data Governance, Python"
Senior Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.\nRole:\nDesign, implement and lead Data Architecture, Data Quality, Data Governance across\nDefining data modeling standards and foundational best practices\nDevelop and evangelize data quality standards and practices\nEstablish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data.\nDrive the successful adoption of organizational data utilization and self-serviced data platforms.\nCreate and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset\nDevelop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data.\nDesign data schemes, object models, and flow diagrams to structure, store, process, and integrate data\nProvide architectural assessments, strategies, and roadmaps for data management.\nApply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms.\nImplement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD\nTranslate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models.\nDefine templates and processes for the design and analysis of data models, data flows, and integration.\nLead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms\nMandatory Qualifications:\nQualifications: B.S. or M.S. in Computer Science, or equivalent degree\n10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting.\n7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more extensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse.\nHighly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools.\nProven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker\nKnowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with Amazon Web Services (AWS)\nStrong verbal and written communications skills are a must and work effectively across internal and external organizations and virtual teams.\nDemonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies.\nStrong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem\nDeep knowledge of data structures and algorithms.\nExperience in working in large teams using CI/CD and agile methodologies.","Airflow, Delta Lake, CI CD, Data Lake Technologies, , Parquet, Big Data platforms, Hive Catalog, ML and Data Science platforms, Docker, Sql, Databricks, Tableau, Kafka, Avro, Etl, Hive, S3, Aws S3, Data Quality, Python, Kubernetes, Data Governance, Spark, Redshift"
Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.\n\nRole\n\nDesign, implement and lead Data Architecture, Data Quality, Data Governance\nDefining data modeling standards and foundational best practices\nDevelop and evangelize data quality standards and practices\nEstablish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data\nDrive the successful adoption of organizational data utilization and self-serviced data platforms\nCreate and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset\nDevelop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data\nDesign data schemes, object models, and flow diagrams to structure, store, process, and integrate data\nProvide architectural assessments, strategies, and roadmaps for data management\nApply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms\nImplement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD\nTranslate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models\nDefine templates and processes for the design and analysis of data models, data flows, and integration\nLead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms\n\nQualifications\n\nB.S. or M.S. in Computer Science, or equivalent degree\n10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting\n7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more\nExtensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse\nHighly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools\nProven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker\nKnowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with hands-on experience in Amazon Web Services (AWS)\nStrong verbal and written communications skills are a must and should work effectively across internal and external organizations and virtual teams\nDemonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies\nStrong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem\nDeep knowledge of data structures and algorithms\nExperience working in large teams using CI/CD and agile methodologies","Airflow, Data Lake Technologies, CI CD, Hive Catalog, Big Data platforms, ML and Data Science platforms, Delta Lake, S3, Kafka, Tableau, Redshift, Sql, Data Quality, Hive, Docker, Spark, Data Architecture, Databricks, Data Governance, Kubernetes, Python, Etl, Aws S3"
Data Center Architect,Mindsprint,Fresher,,"Chennai, India",Login to check your skill match score,"Job Title: Datacentre Architect\nLocation: Chennai, Tamil Nadu\nCompany: Mindsprint\nAbout Mindsprint: Mindsprint is at the forefront of innovation, driving transformative changes in the IT landscape. As we embark on a significant journey of de-merging our IT infrastructure and application landscape, we are seeking a highly skilled and experienced De-Merger as a Service Architect to lead this critical initiative.\nJob Summary: Data Centre Architect will be responsible for designing and implementing the de-merger strategy for Mindsprint's IT infrastructure and application landscape. This role requires a deep understanding of IT architecture, project management, and the ability to work collaboratively with various stakeholders to ensure a seamless transition.\nKey Responsibilities:\nDevelop and execute the de-merger strategy for Mindsprint's IT infrastructure and application landscape.\nAssess current IT systems and applications to identify dependencies and potential risks associated with the de-merger.\nDesign and implement solutions to separate IT systems and applications while ensuring minimal disruption to business operations.\nCollaborate with cross-functional teams, including IT, business units, and external partners, to ensure alignment and successful execution of the de-merger plan.\nProvide technical leadership and guidance throughout the de-merger process, ensuring best practices and industry standards are followed.\nDevelop detailed project plans, timelines, and budgets for the de-merger initiative.\nMonitor progress, identify issues, and implement corrective actions as needed to keep the project on track.\nEnsure compliance with all relevant regulations and standards during the de-merger process.\nCommunicate effectively with stakeholders at all levels, providing regular updates on project status and addressing any concerns.\nQualifications:\nBachelor's or Master's degree in Computer Science, Information Technology, or a related field.\nProven experience in IT architecture, with a focus on de-mergers, mergers, or large-scale IT transformations.\nStrong project management skills, with experience leading complex IT projects.\nExcellent problem-solving and analytical skills.\nAbility to work effectively in a fast-paced, dynamic environment.\nStrong communication and interpersonal skills, with the ability to build relationships and influence stakeholders.\nKnowledge of relevant regulations and standards related to IT de-mergers.\nPreferred Skills:\nExperience as a Data Center Architect, Network Architect, and Cloud Architect.\nFamiliarity with IT security best practices and compliance requirements.\nCertification in project management (e.g., PMP, PRINCE2) or IT architecture (e.g., TOGAF).","de-mergers, project management, Pmp, IT transformations, compliance requirements, Prince2, Togaf, it security best practices, IT architecture"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nBe part of a team that is transforming BCG into a bionic company! We are building a centralized Business Intelligence & Analytics function that will simplify and automate information deliveryproviding advanced insights and analysis to support decision making. To date, the team has launched and operationalized several global scale products and dashboards, enhancing how our leaders engage in information to manage the business. The next wave of digital reporting is underway which will help to unlock further value for BCG functions and leadership with best-in-class business intelligence and analytics.\n\nThe Global IT Data Architect Sr. Manager works as an integral part of an Agile team delivering scalable digital reporting solutions for global stakeholders. This role will help design optimal solutions to support the digital product portfolio. Your responsibilities include: -\n\nDefine data architecture for solutions BI&A reporting.\nEvaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.\nCollaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing PoCs to ensure those standards are implemented\nProvide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.\nDevelop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.\nCreate and maintain conceptual / logical data models to identify key business entities and visual relationships.\nWork with business and IT teams to understand data requirements.\nMaintain a data dictionary consisting of table and column definitions.\nReview data models with both technical and business audiences.\nPartner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.\nLead to design / build new models to efficiently deliver the financial results to senior management.\n\nWhat You'll Bring\n\nBachelor's degree or equivalent combination of education and experience.\nBachelor's degree in information science, data management, computer science or related field preferred.\n12+ years IT experience, including 8+ years in ELT/Data Integration for BI.\nExperience of working on large transformational program with end-to-end ownership of implementation.\nHands-on implementation experience with Data Warehousing including design, modelling, testing, security, administration and optimization.\nExpertise in cloud databases like Snowflake/RedShift, data catalogue, MDM etc.\nExpertise in writing SQL and database procedures.\nProficient in Data Modelling- Conceptual, logical, and Physical modelling\nProficient in documenting all the architecture related work performed.\nExperience in data storage, ETL/ELT and data analytics tools and technologies e.g., Snowflake, DBT, Tableau, Power BI, etc\nExperienced in Data Warehousing design/development and BI/ Analytical systems\nExperience working projects using Agile methodologies\nStrong hands-on experience with data and analytics data architecture, solution design, and engineering experience\nExperience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake\nExperience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs)\nExcellent written, oral communication and presentation skills to present architecture, features, and solution recommendations\nExperience in all aspects of Agile SDLC, and end to end participation in a project lifecycle\nExperience in Python, Data frames is a plus\nExperience within GenAI space is a plus\nReporting tool experience Tableau, PowerBI, SAP BO is a plus\n\nWho You'll Work With\n\nWorking closely with the rest of the Business Intelligence delivery teams, the product owner, Analyst, and Engineers. Architect will also work with external developers, business contacts across BCG functions, DBA's, and Infrastructure teams.\n\nAdditional info\n\nYOU'RE GOOD AT\n\nThis Position Will Involve Daily Collaboration With The Architect And Other Development Teams, Vendors And Stakeholders Throughout Agile Design, Plus The Development, Implementation And Operations Of Both Infrastructure And Business Mappings. The Successful Candidate Will Demonstrate\n\nStrong analytical abilities and creative problem solving\nAbility to work independently with general direction and flexibility in a fast-paced environment\nGood organization and excellent communication skills across cultures\nIntegrity and a positive attitude, especially while handling stressful situations\nWork with project stakeholders (technical as well as end users) to understand business requirements and implement database solutions for diverse problems\nResearch viable technical and/or non-technical solutions, evaluate new technology and advocate, influence, and build consensus for innovations that satisfy business needs\nDesign, document & train the team on the overall processes and process flows for the Data architecture.\nResolve technical challenges in critical situations that require immediate resolution.\nDevelop relationships with external stakeholders to maintain awareness of data and security issues and trends.\nReview work from other tech team members and provide feedback for growth.\nImplement Data Architecture and Data security policies that align with governance objectives and regulatory requirements\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","Sql, Data Architecture, Data Warehousing, Agile Methodologies"
Data Architect,Arcadis,7-9 Years,,"Noida, India",Login to check your skill match score,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.\n\nWe are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world's most complex challenges and deliver more impact together.\n\nIndividual Accountabilities\n\nCollaboration\n\nCollaborates with domain architects in the DSS, OEA, EUS, and HaN towers and if appropriate, the respective business stakeholders in architecting data solutions for their data service needs.\nCollaborates with the Data Engineering and Data Software Engineering teams to effectively communicate the data architecture to be implemented. Contributes to prototype or proof of concept efforts.\nCollaborates with InfoSec organization to understand corporate security policies and how they apply to data solutions.\nCollaborates with the Legal and Data Privacy organization to understand the latest policies so they may be incorporated into every data architecture solution.\nSuggest architecture design with Ontologies, MDM team.\n\nTechnical Skills & Design\n\nSignificant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS and data warehouses.\nDeep understanding of modern data services in leading cloud environments, and able to select and assemble data services with maximum cost efficiency while meeting business requirements of speed, continuity, and data integrity.\nCreates data architecture artifacts such as architecture diagrams, data models, design documents, etc.\nGuides domain architect on the value of a modern data and analytics platform.\nResearch, design, test, and evaluate new technologies, platforms and third-party products.\nWorking experience with Azure Cloud, Data Mesh, MS Fabric, Ontologies, MDM, IoT, BI solution and AI would be greater assets.\nExpert troubleshoot skills and experience.\n\nLeadership\n\nMentors aspiring data architects typically operating in data engineering and software engineering roles.\n\nKey Shared Accountabilities\n\nLeads medium to large data services projects.\nProvides technical partnership to product owners\nShared stewardship, with domains architects, of the Arcadis data ecosystem.\nActively participates in Arcadis Tech Architect community.\n\nKey Profile Requirements\n\nMinimum of 7 years of experience in designing and implementing modern solutions as part of variety of data ingestion and transformation pipelines\nMinimum of 5 years of experience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working in large scale development and cloud environment.\n\nWhy Arcadis\n\nWe can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It's why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.\n\nYou'll do meaningful work, and no matter what role, you'll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.\n\nTogether, we can create a lasting legacy.\n\nJoin Arcadis. Create a Legacy.\n\nOur Commitment to Equality, Diversity, Inclusion & Belonging\n\nWe want you to be able to bring your best self to work every day, which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people.","modern data services, structured and unstructured data, Ai, ontologies, data architecture artifacts, architecture diagrams, cloud environments, data models, MDM, MS Fabric, Data Mesh, Iot, RDBMS, Azure Cloud, data warehouses"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nDefine and design future state data architecture for HR reporting, forecasting and analysis products.\nPartner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.\nEngage with line of business, operations, and project partners to gather process improvements.\nLead to design / build new models to efficiently deliver the financial results to senior management.\nEvaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.\nCollaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing POCs to ensure those standards are implemented.\nProvide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.\nDevelop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.\nCreate and maintain conceptual / logical data models to identify key business entities and visual relationships.\nWork with business and IT teams to understand data requirements.\nMaintain a data dictionary consisting of table and column definitions.\nReview data models with both technical and business audience\n\nWhat You'll Bring\n\nEssential Education\n\nMinimum of a Bachelor's degree in Computer science, Engineering or a similar field\nAdditional Certification in Data Management or cloud data platforms like Snowflake preferred\n\nEssential Experience & Job Requirements\n\n12+ years of IT experience with major focus on data warehouse/database related projects\nExpertise in cloud databases like Snowflake, Redshift etc.\nExpertise in Data Warehousing Architecture; BI/Analytical systems; Data cataloguing; MDM etc\nProficient in Conceptual, Logical, and Physical Data Modelling\nProficient in documenting all the architecture related work performed.\nProficient in data storage, ETL/ELT and data analytics tools like AWS Glue, DBT/Talend, FiveTran, APIs, Tableau, Power BI, Alteryx etc\nExperience in building Data Solutions to support Comp Benchmarking, Pay Transparency / Pay Equity and Total Rewards use cases preferred.\nExperience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake a plus\nExperience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs) a plus\nExcellent written, oral communication and presentation skills to present architecture, features, and solution recommendations is a must\n\nAdditional info\n\nYOU'RE GOOD AT\n\nDesign, document & train the team on the overall processes and process flows for the Data architecture.\nResolve technical challenges in critical situations that require immediate resolution.\nDevelop relationships with external stakeholders to maintain awareness of data and security issues and trends.\nReview work from other tech team members and provide feedback for growth.\nImplement Data security policies that align with governance objectives and regulatory requirements.\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify.","Cloud Databases, Cloud Big Data Technologies, FiveTran, Physical Data Modelling, Data Analytics Tools, dbt, MDM, Meta Scrum, Data Cataloguing, Kanban, Data Warehousing Architecture, BI Analytical Systems, Alteryx, AWS Glue, Tableau, ELT, Data Architecture, Scrum, Talend, AWS, Apis, Power Bi, Agile Methodologies, Gcp, Azure, Etl"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata\n\nwork Mode- Hybrid\n\nRoles And Responsibilities\n\nMandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization\n\nSolution Architect for Data modelling Understanding of Enterprise datasets Sales,\n\nProcurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle\n\netc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building\n\nData lake foundation, Maintenance etc)\n\nCollaborate with the product/business teams, understand related business processes and\n\ndocument business requirements and then write high level specifications/requirements for DEs\n\nDevelop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout\n\nfor right grain of data either in True source systems or in Data WHs and build reusable data\n\nmodels in intermediary layers before creating physical consumable views from data mart\n\nUnderstand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data\n\nGovernance, Data Quality frameworks, Data Observality and the candidate should be:\n\nFamiliar with DevOps process\nKnowing how to check existing tables, data dictionary, table structures\nExperienced with normalizing tables\nHaving good understanding of Landing, Bronze, Silver and Gold layers and concepts\nFamiliar with Agile techniques\nCreate business process map, user journey map and data flow integration diagrams; Understand\n\nIntegration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of\n\nmodels\n\nStakeholder management with data engineering, product owners, central data modelling team,\n\ndata governance & stewards, Scrum master, project team and sponsor.\n\nAbility to handle large implementation program with multiple projects spanning over an year.\n\nSkills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Agile techniques, Integration through API, DataOps, Data observability, Data Modeling, Sql, Cloud Architecture, Devops, Dimensional Modeling, SAP, FTP, Data Modeler, Data Quality, Oracle, Data Governance, Data Architect, Azure, Sftp, Data Lake, Webservices"
Technical Architect- (Data Architect),Simpplr,8-10 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Who We Are\n\nSimpplr is Modern Intranet and EX unified. Our platform unifies employee engagement, enablement, and services, leveraging state-of-the-art AI models to deliver a seamless, cohesive, and personalized employee experience for everyone - wherever and however they work.\n\nOur mission is to transform the work experience for billions of people across the world. Because we believe that when work is good, life is better.\n\nTrusted by more than 1,000+ leading brands, including DocuSign, Penske, Splunk, Nutanix, Okta, Eurostar, and SoFi, our customers are achieving measurable improvements in employee engagement, productivity, and accelerated business performance.\n\nSimpplr is headquartered in Silicon Valley, CA with offices in the UK, Canada, and India, and is backed by Sapphire Ventures, Norwest Venture Partners, Salesforce Ventures, and Tola Capital. Learn more at simpplr.com.\n\nJob Title: Technical Architect - Analytics\n\nLocation: Gurgaon Or Bangalore - Hybrid India\n\nEmployment Type: Full-Time\n\nThe opportunity\n\nWe are looking for a hands-on Technical Architect Analytics who will be responsible for designing, developing, and optimizing our data and analytics architecture. You will play a critical role in defining the data strategy, designing scalable data pipelines, and implementing best practices for real-time and batch analytics solutions. This role requires a strong technical leader who is passionate about data engineering, analytics, and driving data-driven decision-making across the organization.\n\nKey Responsibilities\n\nData Architecture & Design: Define and own the architecture for data processing, analytics, and reporting systems, ensuring scalability, reliability, and performance.\nData Engineering: Design and implement highly efficient, scalable, and reliable data pipelines for structured and unstructured data.\nBig Data & Real-Time Analytics: Architect and optimize data processing workflows for batch, real-time, and streaming analytics.\nCross-Functional Collaboration: Work closely with Product Managers, Data Scientists, Analysts, and Software Engineers to translate business requirements into scalable data architectures.\nTechnology & Best Practices: Stay ahead of industry trends, introduce modern data technologies, and drive best practices in data architecture, governance, and security.\nCode Reviews & Mentorship: Review code, enforce data engineering best practices, and mentor engineers to build a high-performance analytics team.\nData Governance & Compliance: Ensure data security, integrity, and compliance with regulations (GDPR, CCPA, etc.).\nOptimization & Performance Tuning: Identify performance bottlenecks in data pipelines and analytics workloads, optimizing for cost, speed, and efficiency.\nCloud & Infrastructure: Lead cloud-based data platform initiatives, ensuring high availability, fault tolerance, and cost optimization.\n\nWhat Makes You a Great Fit for Us\n\n\nExperience: 8+ years of experience in data architecture, analytics, and big data processing.\nProven Track Record: Experience designing and implementing end-to-end data platforms for high-scale applications.\nStrong Data Engineering Background: Expertise in ETL/ELT pipelines, data modeling, data warehousing, and stream processing.\nAnalytics & Reporting Expertise: Experience working with BI tools, data visualization, and reporting platforms.\nDeep Knowledge of Modern Data Technologies:\nBig Data & Analytics: Spark, Kafka, Hadoop, Druid, ClickHouse, Presto, Snowflake, Redshift, BigQuery.\nDatabases: PostgreSQL, MongoDB, Cassandra, ElasticSearch.\nCloud Platforms: AWS, GCP, Azure (experience with cloud data warehouses like AWS Redshift, Snowflake is a plus).\nProgramming & Scripting: Python, SQL, Java, Scala.\nMicroservices & Event-Driven Architecture: Understanding of real-time event processing architectures.\nStrategic Thinking: Ability to design and implement long-term data strategies aligned with business goals.\nProblem-Solving & Optimization: Strong analytical skills with a deep understanding of performance tuning for large-scale data systems.\nVisionary Leadership: Ability to think strategically and drive engineering excellence within the team.\nCommunication Skills: Strong interpersonal and communication skills to collaborate effectively across teams.\nAttention to Detail: An eye for detail with the ability to translate ideas into tangible, impactful outcomes.\nAgility: Comfortable managing and delivering work in a fast-paced, dynamic environment.\n\nPreferred Skills (Good To Have)\n\nHands-on experience with AWS Public Cloud.\nExperience with Machine Learning Pipelines and AI-driven analytics.\nHands-on experience with Kubernetes, Terraform, and Infrastructure-as-Code (IaC) for data platforms.\nCertifications in AWS Data Analytics, Google Professional Data Engineer, or equivalent.\nExperience with data security, encryption, and access control mechanisms.\nExperience in Event/Data Streaming platforms\nExperience in risk management and compliance frameworks\n\nSimpplr's Hub-Hybrid-Remote Model\n\n\nAt Simpplr we believe that when work is good, life is better and that belief guides all we do. Including how we approach our flexible work model. Simpplr operates with a Hub-Hybrid-Remote model. This model is role-based with exceptions and provides employees with the flexibility that many have told us they want.\n\nHub - 100% work from Simpplr office. Role requires Simpplifier to be in the office full-time.\nHybrid - Hybrid work from home and office. Role dictates the ability to work from home, plus benefit from in-person collaboration on a regular basis.\nRemote - 100% remote. Role can be done anywhere within your country of hire, as long as the requirements of the role are met.","Event-Driven Architecture, stream processing, Druid, Real-Time Analytics, snowflake, ClickHouse, Data Architecture Design, data engineering, Bi Tools, Data Modeling, Cassandra, PostgreSQL, Kafka, ELT, Microservices, Elasticsearch, Python, AWS, Java, BigQuery, Hadoop, Scala, Big Data, Redshift, Sql, Gcp, Presto, Data Visualization, Spark, Data Warehousing, MongoDB, Azure, Etl"
Azure Data Architect,Veracity Software Inc,10-12 Years,,"Pune, India",Login to check your skill match score,"Job Details:\n\nPosition: Data Architect\n\nExperience: 10-12 years\n\nWork Mode: Onsite\n\nLocation: Pune\n\nNotice Period: Immediate\n\nJob Responsibilities:\n\nThe ideal profile should have a strong foundation in data concepts, design, and strategy, with the ability to\n\nwork across diverse technologies in an agnostic manner.\n\nTransactional Database Architecture\n\nDesign and implement high-performance, reliable, and scalable transactional database architectures.\nCollaborate with cross-functional teams to understand transactional data requirements and create\n\nsolutions that ensure data consistency, integrity, and availability.\n\nOptimize database designs and recommend best practices and technology stacks.\nOversee the management of entire transactional databases, including modernization and de-\n\nduplication initiatives.\n\nData Lake Architecture\n\nDesign and implement data lakes that consolidate data from disparate sources into a unified, scalable\n\nstorage solution.\n\nArchitect and deploy cloud-based or on-premises data lake infrastructure.\nEnsure self-service capabilities across the data engineering space for the business.\nWork closely with Data Engineers, Product Owners, and Business teams.\n\nData Integration & Governance:\n\nUnderstand ingestion and orchestration strategies.\nImplement data sharing, data exchange, and assess data sensitivity and criticality to recommend\n\nappropriate designs.\n\nBasic understanding of data governance practices.\n\nInnovation\n\nEvaluate and implement new technologies, tools, and frameworks to improve data accessibility,\n\nperformance, and scalability.\n\nStay up to date with industry trends and best practices to continuously innovate and enhance the data\n\narchitecture strategy.\nShow more Show less","Data Lake Architecture, Data Integration Governance, Data accessibility, Data governance practices, Transactional Database Architecture, Scalability"
Big Data Architect,Quick Heal,15-17 Years,,"Pune, India",Login to check your skill match score,"About Quick Heal\nQuick Heal is one of the leading IT security solutions company with a global presence in 38 cities in India and 40 countries across globe. Each Quick Heal product is designed to simplify IT security management across the length and depth of devices and on multiple platforms. They are customized to suit consumers, small businesses, Government establishments and corporate houses.\nSeqrite is the enterprise arm of India's leading and only listed cybersecurity products company Quick Heal Technologies Ltd. What sets Seqrite apart is our state-of-the-art Zero Trust technology, primed and ready to take on the market. We believe in a security paradigm where trust is never assumed, but rather consistently verified. Our Zero Trust solutions suite enables organizations to secure their endpoints, data, networks, and users across geographies, providing a robust defense against modern cyber threats.\nSeqrite is also powered by state-of-the-art Seqrite Labs that continuously mines Threat Research, Real-time Detection, and Threat Intelligence.\nIn the recent successful project of our nation Chandrayaan 3, Seqrite solutions have played an important role in securing the command & control center of ISRO from Cyber Threats.\nSeqrite has a dedicated Services wing. This division specialises in delivering comprehensive cybersecurity consulting services to a diverse clientele that includes Corporates, PSUs, Government, and Law Enforcement Agencies. Seqrite has a global marquee clientele across BFSI, Pharma, Manufacturing, Government, and Mid & Large industries.\nCore Purpose:Innovate to simplify securing digital experience.\nMission:Empowering the team to solve business problems.\nVision:To be trusted by our customers in securing the digital world and aim to grow as reputable global market leader.\nWhat makes us different:\nSeqrite is one of the most successful purpose-led businesses enabling employees to thrive and unleash their potential to innovate. We invest in career development opportunities for our employees and celebrate our diverse perspectives every step of the way. We provide you an opportunity to work on new technologies. You will be surrounded by passionate and committed colleagues and work together to create a digital safe world for everyone.\nJob Description\nPosition: Big Data Technical Architect\nExperience Required: 15 plus years\nRole:\nDesigned and built robust, high-performance, micro-service-based solution for modern enterprise platform (Java, Big Data/Relational/No-SQL/OLAP)\nProvide technical thought leadership on High Level Architecture and Design, data modelling, Big Data strategy & adoption for Ingestion and Analytics Applications within the enterprise security domain\nDemonstrates careful attention to quality and accuracy\nAbility to convince his solutions and motivate the team\nResearch, design, develop and document cutting-edge generative AI and ML algorithms to address real-world challenges\nExtensive working knowledge of various AWS Services including serverless, data streaming, big data, security related services, cloud formation, CDK\nResponsibilities:\nLarge scale software integration experience\nGood to have: Experience with Agile process methodology, CI/CD automation, TDD, Terraform\nResearch, design, develop and document cutting-edge generative AI and ML algorithms to address real-world challenges\nKey Deliverables:\nStrong understanding of development, architecture multi-threading,\nGood understanding of algorithms and data structure to implement Real-time inline data processing\nGood knowledge of Window/Linux at a systems level\nStrong analytical and troubleshooting skills using debuggers.\nKnowledge of various unit testing, performance testing frameworks\nRequired Skills:\nHands-on development experience\nBig Data, Java, Golang, Spring MVC, Microservices, AWS,\nKubernetes, SQL/NoSQL, OLAP Databases ( ElasticSearch/ClickHouse, MySQL/PostgreSQL, MongoDB/Cassandra DB)\nDesired Traits:\nKnowledge of the following areas would be good to have:\nGood understanding of cybersecurity domain will be added advantage\nThe candidate should think about the issues/solutions analytically and effectively. He should have zeal to learn new languages and technologies, stay updated on new technological trends and how those can be applied within the organization.\nThank you for your consideration to become Quick Heal Family Member","ClickHouse, OLAP Databases, Cassandra DB, Java, Golang, PostgreSQL, Big Data, Spring MVC, Sql, Microservices, Elasticsearch, Nosql, MySQL, MongoDB, Kubernetes, AWS"
"Data Architect, Director",NatWest Group,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Our people work differently depending on their jobs and needs. From hybrid working to flexible hours, we have plenty of options that help our people to thrive.\n\nThis role is based in India and as such all normal working days must be carried out in India.\n\nJob Description\n\nJoin us as a Data Architect\n\nFor someone with a background in defining business and application architectures and roadmaps for complex enterprises, this is an excellent opportunity to join our business. You'll be defining the intentional architecture for your assigned scope in order to ensure that the current architecture being delivered by engineers best supports the enterprise and its long term strategy\nYou'll provide advisory support and governance to ensure projects align to simplification strategy and comply with data standards while promoting and supporting effective data modelling, metadata management and alignment to enterprise data model and data controls\nWith valuable exposure, you'll be building and leveraging relationships with colleagues across the bank to ensure commercially focused decisions and to create long term value for the bank\nWe're offering this role at director level\n\nWhat you'll do\n\nAs a Data Architect, you'll be defining and communicating the current, resultant and target state architecture for your assigned scope. You'll advise domain and project teams to ensure alignment with data architecture standards, data principles, data controls, target data architectures, data patterns and the banks simplification strategy\n\nWe'll look to you to influence the development of business strategies at an organisational level, identifying transformational opportunities for our businesses and technology areas associated with both new and existing data solutions.\n\nAs well as this, you'll be:\n\nTranslating architecture roadmaps into packages of work that allow frequent incremental delivery of value to be included in product backlog\nDefining, creating and maintaining architecture models, roadmaps, standards and outcomes, using architecture strategies to ensure alignment to adjacent and higher level model\nCollaborate and support data management SMEs and data stewards ensuring metadata management aligns with enterprise data model\nConduct data design reviews to ensure solutions meet data standards, data principles, data controls, data patterns and target data architectures\nSeeking out and utilising continuous feedback, fostering adaptive design and engineering practices to drive the collaboration of programmes and teams around a common technical vision\n\nThe skills you'll need\n\nTo succeed in this role, you'll need expert knowledge of application architecture, and at least one business, data or infrastructure architecture with working knowledge of the remaining disciplines. You'll have excellent communication skills with the ability to clearly communicate complex technical concepts to colleagues, up to senior leadership level, along with a good understanding of Agile methodologies with experience of working in an Agile team.\n\nYou'll also demonstrate:\n\nGood collaboration and stakeholder management skills\nExperience of developing, syndicating and communicating architectures, designs and proposals for action\nAn understanding of industry architecture frameworks, such as TOGAF and ArchiMate\nExperience in data modelling methodologies (3NF, Dimensional, Data Vault, Star Schema, etc). Knowledge of data related regulations such as GDPR, CCPA,PCI DSS, BCBS 239\nExperience of working with business solution vendors, technology vendors and products within the market\nA background in systems development change life cycles, best practices and approaches\nKnowledge of hardware, software, application and systems engineering","BCBS 239, archimate, CCPA, systems development change life cycles, hardware software application and systems engineering, data modelling methodologies, Pci Dss, Gdpr, Agile Methodologies, Application Architecture, Infrastructure Architecture, Togaf"
Sr. Data Architect - Snowflake,Arting Digital,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect - Snowflake\n\nExperience: 12+ years\n\nBudget: Up to 38 LPA\n\nNotice Period: Immediate to 30 days\n\nLocation: Trivandrum, Bangalore, Chennai\n\nEducation: Bachelor's or Master's degree in Computer Science, Information Systems, or a related field\n\nSkill Set: Snowflake experience, Data Architecture experience, ETL process experience, Large Data migration solutions experience , data modelling, schema design DBT , Python/Java/Scala, SQL, ETL process , cloud data warehousing concept , data integration , AWS, Azure, and GCP , CDC , DataOps methodologies , cloud platforms/Snow-flak , data visualisation tools (e.g., Tableau, Power BI , data security and compliance standard\n\nJob Description:\n\nWe are seeking an experienced Senior Data Architect Snowflake to lead and design scalable, high-performance data solutions. The ideal candidate should have extensive expertise in data architecture, large-scale data migration, ETL processes, and cloud-based data warehousing. You will play a key role in designing optimized data models, ensuring efficient data integration, and implementing best practices for Snowflake and other cloud platforms.\n\nKey Responsibilities:\n\nArchitect, design, and implement scalable Snowflake-based data solutions.\nDevelop data models, schema designs, and ETL pipelines to support business requirements.\nLead large-scale data migration projects while ensuring performance optimization.\nImplement DataOps methodologies for efficient data management and automation.\nWork with AWS, Azure, and GCP to deploy cloud-based data architectures.\nEnsure data security and compliance with industry standards.\nOptimize CDC (Change Data Capture) processes for real-time data updates.\nUtilize DBT, Python, Java, or Scala to enhance data transformation and integration.\nDesign and implement data visualisation solutions using tools like Tableau and Power BI.\n\nRequired Skills & Expertise:\n\nStrong expertise in Snowflake and cloud data warehousing concepts.\nHands-on experience with ETL processes, data modelling, and schema design.\nProficiency in SQL, DBT, Python, Java, or Scala for data transformation and automation.\nExperience in data integration and large-scale data migration solutions.\nKnowledge of CDC (Change Data Capture) methodologies.\nFamiliarity with DataOps practices and modern data engineering workflows.\nExposure to AWS, Azure, and GCP cloud platforms.\nStrong understanding of data security, governance, and compliance.\nExperience with data visualisation tools such as Tableau or Power BI.","cdc, Data security and compliance, DataOps methodologies, snowflake, dbt, ETL processes, Cloud data warehousing, Large Data migration solutions, Java, Scala, Schema Design, Sql, Gcp, Data Integration, Data Architecture, Data Modelling, Azure, Python, AWS"
Data Center Architect,Coforge,12-14 Years,,"Delhi, India",Login to check your skill match score,"Designation Data Centre IT networking & architecture expert\nExperience 12 Years and above\nJob Location New Delhi\nUnderstanding the Project Requirements, Technical Specifications & Scope of work\nExperience in Data Centre Planning, Designing and Implementing at least 2 large data centres including Network, Compute, & Storage.\nHands On Experience with (Multi/Cross Platform Products/solutions)\nHands-on experience in Architecture, Design, Deployment, managing the High Availability\nSolution for networking & security products.\nDevelopment, Review/Rework of HLD, LLD, SoPs, ATP document, DC-DR Rack Layouts for Racking and Stacking\nDesign, Deploy, and Test Disaster Recovery for the products/Solutions at DC and DR.\nShould have prior experience in data centre designing for high-density RACKS\nDesign and implement power distribution systems, optimise power usage efficiency and ensure redundancy to minimise downtime risks.\nArchitect network infrastructure for Client data centre environments, including switches, routers, firewalls and other security & utility solutions.\nImplement high-speed interconnects and design network topologies to support scalable and resilient connectivity.\nDevelop rack layouts and configurations to maximise space utilization and airflow management, ensuring the Facilitation of RU Space for the smooth integration of additional planned security solutions (such as Antiapt Solutions, HIDS/HIPS, ZTA, etc), and take care of Intelligent cabling for these futuristic requirements.\nDesign fault-tolerant architectures to ensure high availability and minimise service disruptions.\nArchitect Networking, utility solutions tailored to meet performance, capacity, and data protection requirements.\nOptimise compute resources through virtualisation and containerization technologies.\nExperience in the integration of different IT Infra solutions.\nSupport in VAPT\nDesirable: Scripting hands-on / knowledge of PowerShell/Bash/Perl/Automation of migration process.\nMandatory skills: Data Centre IT networking & architecture expert, Date C , Server Deployment\nNote: This position doesn't require expertise/experience in MEP&FP (Mechanical, Electrical, Plumbing, and Fire Protection) but should understand the concepts used for design.","Data Centre Planning, Networking Utility Solutions, Rack Layouts and Configurations, Fault-tolerant Architectures, High-speed Interconnects, Power Distribution Systems, Containerization Technologies, Perl, PowerShell, Bash, Network Topologies"
Data Center Architect,ITC Infotech,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Hi, Please find the detailed JD for the Datacenter Practice Lead role, if interested please send your profile to [HIDDEN TEXT]\nThe Role:\nAs a DataCenter Practice Lead, you will own, lead and focus on skills and competency building for the Compute technologies for the practice of the organization, as your primary role and also support ongoing Delivery programs and manage the PnL for the ame as a secondary role.\nYou are expected to stay current with present and emerging database technologies/trends utilizing new features when applicable to the various environments. Automate reoccurring complex workflows to free up time for higher value work.\nThe work you'll do:\nTechnology support and expertise to the team on the ground on issues and problems faced in the current Delivery, if any.\nFacilitate a Cross functional interaction with product managers, domain experts, engineers and consultants during the solution build and design phases of new and existing deals and RFPs/RFIs/RFQs.\nAssist and work with the teams in support process enhancements and technology transformation themes which will enhance the model of operation and bring in value to the end customers.\nBe the escalation point for DataCentre and Compute related delivery issues and assist the team either by driving technical solutions and know-how on solving technical issues, or assist in building a bridge with the OEMs to help the team resolve issues on the ground.\nBuild competency across Data Centre technologies and create a centre of excellence to be able to act as a first line of defence during critical incidents and transformation initiatives.\nBuild and assist in creating templates and practice standardizations across the DB practice\nInterface with customer to understand delivery concerns, receive feedback on support quality and derive models/solutions to improve the operating model to be implemented within the accounts.\nWork with various OEMs and partners to evaluate tools and productivity solutions in the market to keep abreast of the trend and consult with the customer to see what is best fit for their environments, in order to up-sell and cross-sell services\nEffective Team Member\nLead cross functional team capacity planning exercises related to environment and applications ensuring vendor best practices.\nCollaborate and consult with users, system administrators, and systems programmers to overcome significant operational and/or technical issues and problems.\nUtilize strong interpersonal skills in dealing effectively with diverse skill sets and personalities and work effectively as a team player.\nLeadership\nMentor and teach less experienced team members preparing them for more responsibility while focusing on optimization and best practices.\nLead presentations to clients, upper management, and peers as it pertains to database technology roadmap, architecture, engineering, and provisioning.\nManage the completion of database administrator work, meeting deadlines, and providing deliverables to the customers.\nServe as the subject matter expert of all database related workflows. May perform additional duties related to agile project management.\nWork directly with Project Managers to update project plans and communicate project status.\nParticipate in operating model activities related to product and service ownership.\nMeet and communicate with stakeholders, document project definition, and provide direction and leadership in project estimates and sequencing.\nBuild, edit, and maintain team backlog; prioritize and document objectives for project sprints.\nThe qualifications you need:\nYou should have that rare combinationa sharp technical brain and a bent for business.\nAt least 15+ years of experience with Data Centre technologies such as Vmware, Citrix, Netapp, Storage, Network and Database knowledge\nBe able to build and contribute to solutioning and technical reviews of complex operational issues and problems.\nHave technical knowledge and have had hands on exposure to multi-technology towers\nExpertise to at least one of the following domains: business intelligence, data engineering, business analytics or a similar field.\nStrong Solution Designing skills with capability to understanding of business processes\nStrong service management exposure and on ITSM processes\nAbility to communicate effectively and build a good rapport with team members and with Customers\nSoft skills: -\nPassionate about technology, solving web-focused problems, and how good data analysis can impact strategy\nStrong analytical and quantitative reasoning\nExtremely organized\nDetail-oriented\nProblem-solving skills\nTime management skills\nComfort and effectiveness in translating between people needs and database output\nStrong oral and written communication skills\nInterpersonal skills\nPeople management and teaching skills\nService and mission- driven.","Business intelligence, Business analytics, ITSM processes, Citrix, Database Knowledge, Solution Designing, Netapp, data engineering, storage network, Vmware"
Architect- data Platform,Myntra,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"About Team\nMyntra's Engineering team builds the technology platform that empowers our customers shopping experience and enables the smooth flow of products from suppliers to our customers doorsteps. We work on areas such as building massive-scale web-applications, engaging user-interfaces, big-data analytics, mobile apps, workflow systems, inventory-management etc. We are a small technology team where each individual has a huge impact. You will have the opportunity to be part of a rapidly growing organization and gain exposure to all the parts of a comprehensive ecommerce platform.\nMyntra's cloud based big data platform is highly scalable and processes over 6 billion events per day. Over the last one year, we are on a journey to product-ize data platform and offer Data ingestion, streaming, processing and visualization as self-serve offerings for Myntra's data consumers. We use the best-of-breed open source components as starting points to build out these capabilities. The team has built a Big Data Platform to ingest data from a variety of data sources, standardize metrics and build data & analytics products .\nData and ML Platform engineering employs new-age technologies such as Distributed Computing constructs, Real Time model predictions, Deep Learning, Accelerated Compute (GPU); scalable feature stores Cassandra, MySQL, Elastic Search, Solr, Aerospike; scalable programming constructs in, Python and ML Frameworks (TensorFlow, Pytorch, etc).\nRoles and Responsibilities\nDrive the data architecture, data modelling, design, and implementation of data applications using standard open source big data tech stack, Data Warehouse / MPP databases and distributed systems. Gather business and functional requirements from external and/or internal users, and translate requirements into technical specifications to build robust, scalable, supportable solutions. Participate and drive the full development lifecycle.\nBuild the Standards and best practices around a Common Data Model and Architecture, Data Governance, Data Quality and Security for multiple business areas across Myntra. Collaborate with platform, product and other engineering and business teams to evangelise those Standards for adoption across the org.\nMentor data engineers at various levels of seniority by doing their design and code reviews, providing constructive and timely feedback on code quality, design issues, technology choices with performance and scalability being critical drivers. Manage resources on multiple technical projects and ensure schedules, milestones, and priorities are compatible with technology and business goals.\nSetting up best practices to help the team achieve the above and constantly thinking about improving the technology use are your responsibilities. Driving the adoption of these best practices around coding, design, quality, performance in your team.Stay abreast of the technology industry, market trends in the field of data architecture and development.\nDemonstrates understanding of data lifecycle (data modelling, processing, data quality, data evolution) and underlying tech stacks (Hadoop, Spark, MPP). Drives setting data architecture standards encompassing complete data life cycle (ingestion, modelling, processing, consumption, change management, quality, anomaly detection).\nChallenge the status quo and propose innovative ways to process, model, consume data when it comes to tech stack choices or design principles.\nImplementation of long term technology vision for your team.\nActive participant in technology forums; represent Myntra in external forums.\nQualifications & Experience\n12 - 15 years of experience in software development\n5+ years of development and / or DBA experience in Relational Database Management Systems[RDBMS] (MySql, SQLServer, etc.)\n8+ years of hands-on experience in implementation and performance tuning MPP databases (Microsoft SQL DW, AWS Redshift, Teradata, Vertica, etc.)\nExperience designing database environments, analyzing production deployments, and making recommendations to optimize performance\nProblem solving skills for complex & large scale data applications problems.\nTechnical Breadth - Exposure to a wide variety of problem spaces, technologies in data e.g. real-time and batch data processing, options in commercial vs open source tech stack.\nHands-on experience with Enterprise Data Warehouse and Big data storage and computation frameworks like OLAP Systems, MPP (SQL DW, Redshift, Oracle RAC, Teradata, Druid), Hadoop Compute (MR, Spark, Flink, Hive). Awareness of pitfalls & use cases for a large variety of solutions. Ability to drive capacity planning, performance optimization and large-scale system integrations.\nExpertise in designing, implementing, and operating stable, scalable, solutions to flow data from production systems into analytical data platforms (big data tech stack + MPP) and into end-user facing applications for both real-time and batch use cases.\nData modelling skills (relational, multi-dimensional) and proficiency in one of the programming languages preferably Java, Scala or Python.\nDrive design and development of automated monitoring, alerting, self healing (restartability / graceful failures) features while building the consumption pipelines.\nMentoring skills - Be the technical mentor to your team.\nB. Tech. or higher in Computer Science or equivalent required.","Flink, MPP databases, Druid, Teradata, Real Time model predictions, OLAP Systems, Accelerated Compute, Aws Redshift, Cassandra, Gpu, Tensorflow, MySQL, Aerospike, Python, Solr, Hadoop, Big Data, Deep Learning, Hive, Distributed Computing, Pytorch, Spark, Vertica, Elastic Search"
Data Scientist Frontend Architect,Ericsson,Fresher,,"Bengaluru, India",Login to check your skill match score,"About this opportunity:\n\nTeam is now looking for an experienced Front-End Architect and Front -End developer with strong technical expertise to design and lead the development of scalable, high-performance front-end applications of AI/ML. The ideal candidate should possess a deep understanding of modern front-end technologies, a passion for UX, and experience in integrating various third-party libraries and APIs. The role requires excellent problem-solving skills and the ability to collaborate with cross-functional teams, including back-end developers, DevOps, , UI/UX designers and AI Knowledge.\n\nWhat you will do:\n\nArchitect, design, and implement front-end solutions using Angular (12+), TypeScript, JavaScript, and related frameworks.\n\nDevelop and maintain scalable and reusable front-end components, ensuring a seamless user experience.\n\nIntegrate CSS3, HTML5, and frameworks like Bootstrap or Angular Material to enhance UI functionality and aesthetics.\n\nWork with RxJS, MomentJS, UnderscoreJS, ReactJS, NodeJS, or other third-party libraries as needed to meet project requirements.\n\nDesign testable and maintainable code, incorporating Jasmine and Karma for testing when appropriate.\n\nCollaborate with the DevOps team to manage builds and CI/CD pipelines using Jira, GitLab, and other tools.\n\nFacilitate seamless communication between front-end and back-end teams using REST, JSON, and SOAP for API integration.\n\nLeverage cloud-native development practices and contribute to architecture discussions.\n\nCommunicate effectively with a diverse set of technical audiences to convey complex concepts.\n\nWhat you will Bring:\n\nAngular (12+), TypeScript, JavaScript expertise.\n\nProficiency in RxJS, CSS3, HTML5, and Bootstrap/Angular Material.\n\nExperience with third-party library integration such as MomentJS, UnderscoreJS, ReactJS, NodeJS, or similar.\n\nFamiliarity with REST, JSON, and SOAP integration.\n\nSolid understanding of cloud-native development and CI/CD tools, especially Jira and GitLab.\n\nWhy join Ericsson\n\nAt Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.\n\nWhat happens once you apply\n\nClick Here to find all you need to know about what our typical hiring process looks like.\n\nEncouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.\n\nPrimary country and city: India (IN) || Bangalore\n\nReq ID: 766743","MomentJS, Underscorejs, Nodejs, Jasmine, Soap, Json, Jira, Css3, Typescript, REST, Javascript, Angular 12, Html5, Reactjs, Rxjs, Bootstrap, Gitlab, Angular Material, Karma"
Data Integration Architect,Alstom Transportation,10-14 Years,,Bengaluru,Transportation,"RESPONSIBILITIES:\nTechnical -\nHands-on-experience architecting and delivering solutions related to enterprise integration, APIs, service-oriented architecture, and technology modernizations\n3-4 years hands-on experience with the design, and implementation of integrations in the area of Dell Boomi\nUnderstanding the Business requirements and Functional requirement Documents and Design a Technical Solution as per the needs\nPerson should be good with Master Data Management, Migration and Governance best practices\nExtensive data quality and data migration experience including proficiency in data warehousing, data analysis and conversion planning for data migration activities\nLead and build data migration objects as needed for conversions of data from different sources\nShould have architected integration solutions using Dell Boomi for cloud, hybrid and on-premise integration landscapes\nAbility to build and architect a high performing, highly available, highly scale Boomi Molecule Infrastructure\nIn depth understanding of enterprise integration patterns and prowess to apply them in the customers IT landscape\nAssists project teams during system design to promote the efficient re-use of IT assets Advises project team during system development to assure compliance with architectural principles, guidelines and standards\nAdept in building the Boomi processes with Error handling and email alerts logging best practices\nShould be proficient in using Enterprise level and Database connectors\nExtensive data quality and data migration experience including proficiency in data warehousing, data analysis and conversion planning for data migration activities\nExcellent understanding on REST with in-depth understanding on how Boomi processes can expose consume services using the different http methods, URI and Media type\nUnderstand Atom, Molecule, Atmosphere Configuration and Management, Platform Monitoring, Performance Optimization Suggestions, Platform Extension, User Permissions Control Skills.\nKnowledge on API governance and skills like caching, DB management and data warehousing\nShould have hands on experience in configuring AS2, https, SFTP involving different authentication methods\nThorough knowledge on process deployment, applying extensions, setting up schedules, Web Services user management process filtering and process reporting\nShould be expert with XML and JSON activities like creation, mapping and migrations\nPerson should have worked on integration on SAP, SuccessFactors, Sharepoint, cloud-based apps, Web applications and engineering application\nSupport and resolve issues related to data integration deliveries or platform\nProject Management\nPerson should deliver Data Integration projects using data integration platform\nManage partner deliveries by setting up governance of their deliveries\nUnderstand project priorities, timelines, budget, and deliverables and the need to proactively push yourself and others to achieve project goals\nManagerial:\nPerson is individual contributor and operationally managing small technical team\nQualifications & Skills:\n10+ years of experience in the area of enterprise integrations\nMinimum 3-4 years of experience with Dell boomi\nShould have working experience with database like sql server, Data warehousing\nHands on experience on REST, SOAP, XML, JSON, SFTP, EDI\nShould have worked on integration of multiple technologies like SAP, Web, cloud based apps.","Caching, Sharepoint, Db Management, SAP, Successfactors, Data Management"
"Manager, Data Science, Solution Architect",Mondelez,10-14 Years,,Mumbai,Food and Beverage,"You will work closely with the enterprise architecture team to chart technology roadmaps, standards, best practices and guiding principles, providing your subject matter expertise and technical capabilities to oversee specific applications in architecture forums and when participating in workshops for business function requirements\nIn collaboration with the enterprise architecture and solution teams, you will help evaluate specific applications with a goal to bring in new capabilities based on the strategic roadmap\nYou will also deliver seamless integration with the existing ecosystem and support project teams in implementing these new technologies, offer input regarding decisions on key technology purchases to align IT investments with business strategies while reducing risk and participate in technology product evaluation processes and Architecture Review Board governance for project solutions\nWhat you will bring\nA desire to drive your future and accelerate your career. You will bring experience and knowledge in:\nDefining and driving successful solution architecture strategy and standards\nComponents of holistic enterprise architecture\nTeamwork, facilitation and negotiation\nPrioritizing and introducing new data sources and tools to drive digital innovation\nNew information technologies and their application\nProblem solving, analysis and communication\nGovernance, security, application life-cycle management and data privacy\nPurpose of Role\nThe Solution Architect will provide end-to-end solution architecture guidance for data science initiatives. A successful candidate will be able to handle multiple projects at a time and drive the right technical architecture decisions for specific business problems. The candidate will also execute PoC/PoVs for emerging AI/ML technologies, support the strategic roadmap and define reusable patterns from which to govern project designs.\nMain Responsibilities: -\nDetermine which technical architectures are appropriate for which models and solutions.\nRecommend what technologies and associated configurations are best to solve business problems.\nDefine and document data science architectural patterns.\nEnsure project compliance with architecture guidelines and processes.\nProvide guidance and support to development teams during the implementation process.\nDevelop and implement processes to execute AI/ML workloads.\nConfigure and optimize the AI/ML systems for performance and scalability.\nStay up to date on the latest AI/ML features and best practices.\nIntegrating SAP BW with SAP S/4HANA and other data sources.\nReview and sign off on high-level architecture designs.\nCareer Experiences Required Role Implications\nBachelor s degree in computer science or related field of study.\n10+ years of experience in a global company in data-related roles (5+ years in data science).\nStrong proficiency in Databricks and analytical application frameworks (Dash, Shiny, React).\nExperience with data engineering using common frameworks (Python, Spark, distributed SQL, NoSQL).\nExperience leading complex solution designs in a multi-cloud environment.\nExperience with a variety of analytics techniques: statistics, machine learning, optimization, and simulation.\nExperience with software engineering practices and tools (design, testing, source code management, CI/CD).\nDeep understanding of algorithms and tools for developing efficient data processing systems.","technical architectures, AI/ML technologies, SAP S/4HANA, Spark, Databricks, Python, Sap Bw"
Data Solution Architect,Atos Global It Solutions And Services Private Limited,5-8 Years,,Thane,Software,"Certified Cloud Solution Architect with 5-8 years HANDS ON work experience designing, executing, and supporting IT Cloud solutions.\nExperience on setting up data lake / data warehousing for large enterprise solution with massive data volumes.\nExperience in Docker containers, coding, with cloud native cloud agnostic tools, creation consumption of high-performance APIs programs on architectural framework and guidelines with Delta Lake, Data Lake for Data warehousing solutions to create and nurture highly available systems on Kubernetes.\nStructural YAML with Parameterization and Dynamic Configurations\nExperience in Databricks, Data Factory pipelines Automation tooling systems.\nHaving capabilities in API Management , API modeling, scripting/coding languages and experience with relational databases.\nWork with internal external teams to design a full-stack solution using software that fully integrates with customer s existing cloud infrastructure from data flow, security, DevOps, GitHub and troubleshoot performance and functional issues.\nDevelop and organize cloud systems and work closely with IT security to monitor the company s cloud privacy.\nGood Interpersonal skills and Communication skills: require working in a cross-functional team, as well as with various stakeholders with minimal supervision. So, it is vital that they can establish rapport and manage relationship with the people they have on their project team.\nLearn more about us\nAt Atos, we embrace diversity as the ultimate engine of ingenuity for our clients, and we constantly strive to create a culture where people feel supported and encouraged. Read more about our commitment here .\nWhether it is fighting climate change, promoting digital inclusion, or ensuring trust in data management tech for good sits at the core of our identity. With numerous global recognitions for our ESG practices, we are committed to building a better future for all by harnessing the power of technology. Learn more here","Data Solution Architect, Api"
Software Architect (Data Engineering OR RoR,Velotio Technologies,5-11 Years,,Pune,Software,"Job description\nAbout Velotio:\nVelotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.\nWe are looking for an experiencedSoftware Architectwith deep expertise inData Engineeringand proficiency inRuby on Rails (RoR). The ideal candidate will lead architectural decisions, optimize data pipelines, and ensure scalable, efficient, and high-performance data processing solutions. WhileData Engineering skillsare the primary focus,RoR experiencewould be a valuable addition\n7+ years of experience in software development, with a focus on data engineering and cloud architectures.\nStrong experience with Snowflake, DBT, and Data Pipeline design.\nExpertise in Kafka, Argo, Kubernetes, and ML Flow.","Snow Flake, Argo, Kafka, Kubernetes"
Senior Enterprise Architect - Data,Bread Financial,10-15 Years,,Bengaluru,Financial Services,"Sr Enterprise Architect designs and implements enterprise-wide IT solutions to align technology initiatives with business goals. Serve as a strategic advisor to leadership, ensuring architecture principles are followed.\nEssential Job Functions\nDevelop and maintain enterprise architecture frameworks and standards. -\nOversee enterprise data strategies, designing platforms for advanced analytics, AI/ML, and regulatory adherence\nEvaluate new technologies and identify opportunities for system enhancements.\nCollaborate with stakeholders to analyze business requirements and translate them into technical solutions.\nEnsure technology initiatives align with the organizations overall architectural vision.\nConduct system audits and maintain documentation for architecture solutions.\nMinimum Qualifications\nBachelor s Degree or equivalent education and / or experience in computer Science, Engineering, Mathematics or related field.\n10+ years in Information Technology\nPreferred Qualifications\nRelevant certifications, such as TOGAF or AWS Solutions Architect\nSkills\nApplication Development\nBusiness Alignment\nBusiness Process Modeling\nBusiness Case Development\nCode Inspection\nCloud Architectures\nEnterprise Architecture Framework\nIT Architecture\nIT Roadmap\nSolution Architecture\nReports To : Manager and above\nDirect Reports : 0\nWork Environment\nNormal office environment, hybrid.","Architecture, Business process modeling, Solution architecture, Enterprise Architecture, Application Development"
Data & Analytics Architect,Flexera,15-17 Years,,India,Login to check your skill match score,"Flexera saves customers billions of dollars in wasted technology spend. A pioneer in Hybrid ITAM and FinOps, Flexera provides award-winning, data-oriented SaaS solutions for technology value optimization (TVO), enabling IT, finance, procurement and cloud teams to gain deep insights into cost optimization, compliance and risks for each business service. Flexera One solutions are built on a set of definitive customer, supplier and industry data, powered by our Technology Intelligence Platform, that enables organizations to visualize their Enterprise Technology Blueprint in hybrid environmentsfrom on-premises to SaaS to containers to cloud.\n\nWe're transforming the software industry. We're Flexera. With more than 50,000 customers across the world, we're achieving that goal. But we know we can't do any of that without our team. Ready to help us re-imagine the industry during a time of substantial growth and ambitious plans Come and see why we're consistently recognized by Gartner, Forrester and IDC as a category leader in the marketplace. Learn more at flexera.com\n\nAt Flexera, we're on a mission to empower global enterprises by transforming IT insights into decisive actions. We are looking for an architect/principal engineer with deep expertise in analytics, including data modeling, semantic modeling, to take our world-class reference data in Technopedia to the next level, through deeper insights, richer features and more data sets. The ideal candidate will have a strong track record of providing technical leadership, deep technical expertise and delivering big data solutions through data lakehouse and large scale-processing technologies.\n\nAs an architect/principal engineer, you will be a key player in architecting, designing, developing, and maintaining our common ontology and data models, transformations, and analytics capabilities for our industry-leading reference data. You will collaborate closely with other architects and guide, influence and help engineering teams to ensure seamless integration with other components of our system.\n\nWhat you'll do:\n\nArchitect, design and develop our big data platform, processing pipelines and content reference data.\nDrive innovation, enabling capabilities to enhance speed to innovation for our analytics, AI/ML, and product development teams.\nDefine data models and semantic models for our wide range of data sets to provide insights and optimized user experiences.\nProvide guidance, influence and help engineering teams to deliver deep insights from our large and broad data sets.\nContinuously enhance your technical skills and mentor other engineers.\nPromote a high-standard engineering culture and operational excellence within the team.\nCollaborate with product and design teams to ensure the platform meets user needs.\nDefine project requirements, technical artifacts, and designs, driving consensus across Product Management, architects, and engineering teams.\nBalance priorities between new feature development, architectural enhancements, and technical debt reduction for a sustainable and scalable platform.\nChampion continuous improvement in product quality, security, and performance standards within the development team.\n\nYou'll be expected to have:\n\nBachelor's or higher degree in Computer Science, Software Engineering, or related field\nMinimum 15 years relevant experience in software development, including 8+ years of expertise in data modeling, semantic modeling, and data visualization.\nStrong expertise in other big data technologies, such as metadata catalogs, data lineage, and orchestration.\nStrong technical expertise in distributed systems, big data, and cloud computing.\nStrong problem-solving skills and ability to troubleshoot complex issues.\nGood understanding of streaming technologies, including Kafka, with experience in defining message schemas and data models.\nOutstanding communication skills and emotional intelligence to collaborate effectively with teams across the organization.\nExcellent written and verbal communication skills.\nAbility to work effectively both independently and in a collaborative team environment.\nPrior experience mentoring and providing technical guidance to junior engineers.\n\nAt Flexera, we foster a fun and engaged hybrid working environment where collaboration and innovation thrive. We value diversity and encourage applicants from underrepresented groups in technology to apply.\n\nJoin our team to not only contribute to a world-class global product but also to grow in your career. At Flexera, we encourage continuous learning and provide opportunities for professional development.\n\nFlexera is proud to be an equal opportunity employer. Qualified applicants will be considered for open roles regardless of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by local/national laws, policies and/or regulations.\n\nFlexera understands the value that results from employing a diverse, equitable, and inclusive workforce. We recognize that equity necessitates acknowledging past exclusion and that inclusion requires intentional effort. Our DEI (Diversity, Equity, and Inclusion) council is the driving force behind our commitment to championing policies and practices that foster a welcoming environment for all.\n\nWe encourage candidates requiring accommodations to please let us know by emailing [HIDDEN TEXT].","streaming technologies, semantic modeling, metadata catalogs, Distributed Systems, Big Data Technologies, Data Modeling, Data Lineage, Orchestration, Cloud Computing, Data Visualization, Kafka"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Chennai, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune\n\nExperience: 7-16 Years\n\nWork Mode: Hybrid\n\nMandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)\n\nJob Description\n\nWe are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.\n\nKey Responsibilities\n\nDesign, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.\nCollaborate with data analysts, data architects, and business stakeholders to align data models with business needs.\nLeverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.\nManage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.\nContribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.\nApply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.\nStay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.\nDevelop and maintain data models using data modeling tools such as ER/Studio and Hackolade.\nDrive the adoption of best practices and standards for data modeling within the organization.\n\nSkills And Qualifications\n\nMinimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.\nExpertise in Azure and Databricks for building data solutions.\nProficiency in ER/Studio, Hackolade, and other data modeling tools.\nStrong understanding of data modeling principles and techniques (e.g., ERD, UML).\nExperience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).\nSolid understanding of data warehousing, ETL processes, and data integration.\nFamiliarity with big data technologies such as Hadoop and Spark is an advantage.\nIndustry Knowledge: A background in supply chain is preferred but not mandatory.\nExcellent analytical and problem-solving skills.\nStrong communication skills, with the ability to interact with both technical and non-technical stakeholders.\nAbility to work well in a collaborative, fast-paced environment.\n\nEducation\n\nB.Tech in any branch or specialization\n\nSkills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
Data Bricks Architect,Gramener,7-9 Years,,"Chennai, India",Login to check your skill match score,"Data bricks Architect\n\nWork Location: Hyderabad/Bangalore/Chennai/Noida/Mumbai\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\nYou will engage in diverse impactful customer technical Big Data projects, including developing reference architectures, how-to guides, and minimally viable products (MVPs).\nLead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading big data and AI applications.\nProvide architectural guidance that fosters the adoption of Databricks across business-facing functions.\nCollaborate with platform engineering teams to effectively implement Databrick services within our infrastructure.\nEvaluate and assess new features and enhancements on the Databricks platform to ensure we leverage the latest capabilities.\nDesigned and implemented various integration patterns involving Databricks and third-party tools such as Collibra and data quality solutions.\nUtilize your AWS administration and architecture expertise to optimize cloud resources and ensure seamless integration with Databricks.\nLeverage your hands-on experience with Databricks Unity Catalog to implement robust data governance and lineage capabilities.\nAdvocate for and implement CI/CD practices to streamline the deployment of Databricks solutions.\nContribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.\nUnderstand and articulate the analytics capabilities of Databricks, enabling teams to derive actionable insights from their data.\n\nSkills And Qualifications\n\n7+ years of experience with Big Data technologies, including Apache Spark, cloud-native data lakes, and data mesh platforms, in a technical architecture or consulting role.\n5+ years of independent experience in Big Data architecture.\nProficiency in Python coding and familiarity with data engineering best practices.\nExtensive experience working with AWS cloud platforms, including a solid understanding of AWS services and architecture.\nSubstantial documentation and whiteboarding skills to effectively communicate complex ideas.\nIn-depth knowledge of the latest services offered by Databricks, with the ability to evaluate and integrate these services into our platform.\nProven experience implementing solutions using Databricks Unity Catalog, focusing on data governance and lineage tracking.\nDemonstrated expertise in migrating from Databricks classic platform to Lakehouse architecture, utilizing Delta file format and/or Delta Live Tables.\nA collaborative mindset with the ability to work effectively across teams and functions.\n\nAbout Us\n\nWe consult and deliver solutions to organizations where data is the core of decision-making. We undertake strategic data consulting for organizations, laying out the roadmap for data-driven decision-making. This helps organizations convert data into a strategic differentiator. Through a host of our products, solutions, and Service Offerings, we analyze and visualize large amounts of data.\n\nTo learn more about us, visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Databricks Unity Catalog, Data Mesh, Apache Spark, Databricks, Python, AWS"
"Data Analytics Architect (Looker), Contract",66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees\n\n66degrees is a leading Google Cloud Premier Partner. We believe that engineering takes heart. Focusing exclusively on Google Cloud, we help our clients achieve the most innovative and disruptive transformations in their industries.\n\n66degrees is seeking a senior contractor- Data Analytics Architect (Looker) to engage on a 8 weeks of remote assignment for approximately 40 hours per week with potential to extend. Interested candidates should have the following required skills in Golang.\n\nPlease note: ***Candidates must be available to join by first week of May, 2025***\n\nResponsibilities\n\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nWork with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.\nAdvise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.\nUtilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.\nWrite clean, efficient, and scalable code (LookML, Python as applicable)\nConduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.\nStay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.\nCollaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.\nProvide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.\n\nExcellent comprehension of Looker and its implementation, with additional analytics platform experience a major plus - especially MicroStrategy.\n\nExperience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.\nStrong proficiency with SQL required.\nDemonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.\nExcellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.\nSolid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.\nStrong problem-solving skills and the ability to translate business requirements into technical solutions.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nGoogle Cloud certifications and any analytics platform certifications are a plus.\nA desire to stay ahead of the curve and continuously learn new technologies and techniques.\n\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, LookML, Golang, Data Modeling, Google Cloud, Sql, Python"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\n10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.\nWorking with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.\nAnalyzing requirements: Performing requirement analysis and creating architectural models.\nIdentifying issues: Identifying operational issues and recommending strategies to resolve them.\nCommunicating with business users: Communicating technical solutions to business users and addressing their questions.\nValidating solutions: Ensuring solutions align with corporate standards and compliance requirements.\nDeveloping technical specifications: Creating technical design specifications for solutions and systems engineers.\nAssessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.\n\nData engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Databases, Python, Web Scraping, Apis"
Data Bricks Architect,Gramener,7-9 Years,,"Chennai, India",Login to check your skill match score,"Data bricks Architect\n\nWork Location: Hyderabad/Bangalore/Chennai/Noida/Mumbai\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\nYou will engage in diverse impactful customer technical Big Data projects, including developing reference architectures, how-to guides, and minimally viable products (MVPs).\nLead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading big data and AI applications.\nProvide architectural guidance that fosters the adoption of Databricks across business-facing functions.\nCollaborate with platform engineering teams to effectively implement Databrick services within our infrastructure.\nEvaluate and assess new features and enhancements on the Databricks platform to ensure we leverage the latest capabilities.\nDesigned and implemented various integration patterns involving Databricks and third-party tools such as Collibra and data quality solutions.\nUtilize your AWS administration and architecture expertise to optimize cloud resources and ensure seamless integration with Databricks.\nLeverage your hands-on experience with Databricks Unity Catalog to implement robust data governance and lineage capabilities.\nAdvocate for and implement CI/CD practices to streamline the deployment of Databricks solutions.\nContribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.\nUnderstand and articulate the analytics capabilities of Databricks, enabling teams to derive actionable insights from their data.\n\nSkills And Qualifications\n\n7+ years of experience with Big Data technologies, including Apache Spark, cloud-native data lakes, and data mesh platforms, in a technical architecture or consulting role.\n5+ years of independent experience in Big Data architecture.\nProficiency in Python coding and familiarity with data engineering best practices.\nExtensive experience working with AWS cloud platforms, including a solid understanding of AWS services and architecture.\nSubstantial documentation and whiteboarding skills to effectively communicate complex ideas.\nIn-depth knowledge of the latest services offered by Databricks, with the ability to evaluate and integrate these services into our platform.\nProven experience implementing solutions using Databricks Unity Catalog, focusing on data governance and lineage tracking.\nDemonstrated expertise in migrating from Databricks classic platform to Lakehouse architecture, utilizing Delta file format and/or Delta Live Tables.\nA collaborative mindset with the ability to work effectively across teams and functions.\n\nAbout Us\n\nWe consult and deliver solutions to organizations where data is the core of decision-making. We undertake strategic data consulting for organizations, laying out the roadmap for data-driven decision-making. This helps organizations convert data into a strategic differentiator. Through a host of our products, solutions, and Service Offerings, we analyze and visualize large amounts of data.\n\nTo learn more about us, visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Databricks Unity Catalog, data mesh architecture, Apache Spark, Data Governance, Databricks, Python, Big Data Technologies, AWS"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\n10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.\nWorking with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.\nAnalyzing requirements: Performing requirement analysis and creating architectural models.\nIdentifying issues: Identifying operational issues and recommending strategies to resolve them.\nCommunicating with business users: Communicating technical solutions to business users and addressing their questions.\nValidating solutions: Ensuring solutions align with corporate standards and compliance requirements.\nDeveloping technical specifications: Creating technical design specifications for solutions and systems engineers.\nAssessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.\n\nData engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Databases, Python, Web Scraping, Apis"
Data & Analytics Architect,AuxoAI,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nAs a Data & Analytics Architect, you will lead key data initiatives, including cloud transformation, data governance, and AI projects. You'll define cloud architectures, guide data science teams in model development, and ensure alignment with data architecture principles across complex solutions. Additionally, you will create and govern architectural blueprints, ensuring standards are met and promoting best practices for data integration and consumption.\n\nResponsibilities\n\nPlay a key role in driving a number of data and analytics initiatives including cloud data transformation, data governance, data quality, data standards, CRM, MDM, Generative AI and data science.\n\nDefine cloud reference architectures to promote reusable patterns and promote best practices for data integration and consumption.\n\nGuide the data science team in implementing data models and analytics models.\n\nServe as a data science architect delivering technology and architecture services to the data science community.\n\nIn addition, you will also guide application development teams in the data design of complex solutions, in a large data eco-system, and ensure that teams are in alignment with the data architecture principles, standards, strategies, and target states.\n\nCreate, maintain, and govern architectural views and blueprints depicting the Business and IT landscape in its current, transitional, and future state.\n\nDefine and maintain standards for artifacts containing architectural content within the operating model.\n\nRequirements\n\nStrong cloud data architecture knowledge (preference for Microsoft Azure)\n\n8-10+ years of experience in data architecture, with proven experience in cloud data transformation, MDM, data governance, and data science capabilities.\n\nDesign reusable data architecture and best practices to support batch/streaming ingestion, efficient batch, real-time, and near real-time integration/ETL, integrating quality rules, and structuring data for analytic consumption by end uses.\n\nAbility to lead software evaluations including RFP development, capabilities assessment, formal scoring models, and delivery of executive presentations supporting a final recommendation.\n\nWell versed in the Data domains (Data Warehousing, Data Governance, MDM, Data Quality, Data Standards, Data Catalog, Analytics, BI, Operational Data Store, Metadata, Unstructured Data, non-traditional data and multi-media, ETL, ESB).\n\nExperience with cloud data technologies such as Azure data factory, Azure Data Fabric, Azure storage, Azure data lake storage, Azure data bricks, Azure AD, Azure ML etc.\n\nExperience with big data technologies such as Cloudera, Spark, Sqoop, Hive, HDFS, Flume, Storm, and Kafka.","Azure Data Lake Storage, HDFS, Azure Data Fabric, Azure Ad, Flume, Azure Storage, Cloudera, Azure Data Factory, Storm, Kafka, Sqoop, Hive, Microsoft Azure, Azure ML, Spark"
Data Platform Architect,Gramener,Fresher,,"Chennai, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore/Chennai\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career path, steady growth prospects with great scope to innovate. Our goal is to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\nYou will engage in a diverse range of impactful customer technical Data platform projects, Including the development.\nLead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading.\nCollaborate with platform engineering teams to effectively implement Data Brick services within our infrastructure.\nLeverage your hands-on experience with Data Bricks Unity Catalog to implement robust data governance and lineage capabilities.\nAdvocate for and implement CI/CD practices to streamline the deployment of Data bricks solutions.\nContribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.\nUnderstand and articulate the analytics capabilities of the Data platform, enabling teams to derive actionable insights from their data.\n\nSkills And Qualifications\n\nCloud and Architectures:\n\nAzure Architecture and Platform: Expertise in Azure Data Lake, AI/ML model hosting, Key Vault, Event Hub, Logic Apps, and other Azure cloud services.\nDatabricks Development: Strong integration with Azure, workflow orchestration, and governance.\nData Engineering & Architecture: Hands-on experience with scalable ETL/ELT pipelines, Delta Lake, and enterprise data management.\n\nCoding And Implementation\n\nSoftware Engineering: Modular design, CI/CD, version control, and best coding and design practices.\nPython and PySpark: Experience in building reusable packages and components for both technical and business users, including data validation, lineage modules, and accelerators for data processing.\n\nProcess And Compliance\n\nEnterprise Process Understanding: Experience with large-scale workflows, structured environments, and compliance frameworks.\nGovernance and Security: Knowledge of data lineage, GxP, HIPAA, GDP compliance, and regulatory requirements.\nPharma, MedTech, Life Sciences (Good to Have): Understanding industry-specific data, regulatory constraints, and security considerations.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data acts as the core of decision making. We undertake strategic data consulting for organizations in laying out the roadmap for data-driven decision making. It helps organizations to convert data into a strategic differentiator. Through a host of our product and Solutions, and Service Offerings, we analyze and visualize large amounts of data.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Logic Apps, Key Vault, Databricks Development, Event Hub, CI CD, AI ML model hosting, enterprise data management, GDP compliance, Azure Architecture, ETL ELT pipelines, Delta Lake, Data Validation, enterprise process understanding, Security, Governance, Data Lineage, Version Control, Pyspark, Software Engineering, Azure Data Lake, Python"
CFIN P2D & Data Solution Architect,ABB,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN P2D & Data Solution Architect\n\nAt ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.\n\nWrite the next chapter of your ABB story.\n\nThis position reports to\n\nP2D & Data Solution Lead\n\nYour role and responsibilities\n\nWe are seeking a skilled P2D & Data Solution architect to play a key role in configuring, implementing, and supporting P2D/ Data processes within the Central Finance (CFIN) system. The P2D & Data Solution architect will collaborate closely with business stakeholders, process owners, and technical teams to ensure seamless integration of P2D functions within the CFIN framework. This role requires deep functional knowledge of CO processes, as well as the ability to translate business requirements into system configurations that optimize efficiency, accuracy, and business performance.\n\nThis position requires close coordination with Deployment team, Functional architects and external vendors, to maintain and evolve the P2D & Data architecture, ensuring it meets business needs and complies with ABB's standards.\n\nThe work model for the role is:\n\nThis role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.\n\nYou will be mainly accountable for:\n\nP2D Process Configuration: Configure and maintain P2D processes such as include Contribution margin reporting, COPA characteristics derivation and compliance with local and global P2D regulations within the Central Finance (CFIN) system, ensuring alignment with business needs and industry best practices.\nData replication: Oversee the configuration, implementation, and ongoing support of Data related topics within the CFIN system, ensuring processes are optimized and aligned with organizational goals. Expertise in Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data\nRequirements Gathering & Analysis: Collaborate with business stakeholders and process owners to thoroughly understand their requirements, document functional specifications, and translate these into system configurations.\nSystem Integration: Facilitate seamless integration of P2D & Data processes within the CFIN system, as well as with other enterprise systems (Local ERP, etc.), ensuring smooth data flow and automation of processes across platforms. User Support & Training: Provide ongoing functional support to end-users, addressing issues, offering solutions, and conducting training to ensure optimal utilization of the system and full understanding of P2D processes.\nTesting & Validation: Participate in testing and validation activities for new configurations, enhancements, and fixes, ensuring they meet business and functional requirements. Process Optimization: Regularly monitor and evaluate P2D & Data processes to identify opportunities for automation, efficiency improvements, and best practice implementation within the CFIN system.\nDocumentation & Compliance: Develop and maintain comprehensive documentation for P2D system configurations, process flows, and integration points, ensuring compliance with internal standards and regulatory guidelines. Collaboration with Technical Teams: Work closely with IS architects, developers, and technical teams to ensure that functional requirements are correctly implemented and aligned with system design specifications.\nTroubleshooting & Issue Resolution: Provide expert troubleshooting support for P2D -related system issues, working collaboratively with cross-functional teams to resolve any challenges promptly. Continuous Improvement: Stay informed about the latest industry trends, best practices, and system updates to continuously enhance the efficiency and effectiveness of P2D processes within CFIN.\nProject and New Demand Management: Take ownership of configuring new demands or changes in system functionality, ensuring proper alignment with system design documentation and business requirements. Data Management: Oversee the management and accuracy of all data related topics within O2C/ P2P/ TRE / R2R / TAX within the system, ensuring data integrity, consistency, and compliance with business rules across processes.\n\nQualifications for the role\n\nEducation: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in CO - SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.\nProven experience (5+ years) in FICO processes, with a strong background in system configuration and implementation within SAP or similar ERP environments. Experience in configuring CO processes in SAP or similar ERP systems, with a solid understanding of integration points and data flows across systems.\nFamiliarity with Central Finance (CFIN) and integration with other finance-related modules. Experience with requirements gathering, business analysis, and documentation of functional specifications.\nHigh level understanding of local ERP Contribution margin Reporting, COPA characteristics Derivation, COGS split, Price Difference Split, Profit Center, Cost Center, WBS Element, Order master Data mapping\nStrong analytical skills and attention to detail. Excellent communication skills, with the ability to collaborate effectively with cross-functional teams, stakeholders, and technical teams.\nA strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.\nExperience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.\n\nMore about us\n\nABB Finance is a trusted partner to the business and a world-class team who delivers forward-looking insights that drive sustainable long-term results and operates with the highest standards.\n\nWe value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory\n\nIt has come to our attention that the name of ABB is being used for asking candidates to make payments for job\n\nopportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made\n\navailable on our career portal for all fitting the criteria to apply.\n\nABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions.\n\nFor current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Error resolution, AIF monitoring, Currency Values mismatch, Process Optimization, Troubleshooting, Continuous Improvement, Documentation compliance, SLT based filtering, Testing and validation, CO processes, PC G L CC Map, P2D Data processes, Mdg, System integration, Sap Ecc, Data Management, Data Replication, Requirements Gathering, User Support"
Cloud Data & AI Architect,myCloudDoor,3-5 Years,,"Kolkata, India",Login to check your skill match score,"Description\n\nDo you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!\n\nWho we are\n\nmyCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.\n\nTasks\n\nThe Selected Person Will Do The Following Tasks:\n\nDefinition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...\nMaintenance of data solutions, failure analysis and solution proposal.\nCommunication with customers: proposal solutions, technical trainings...\n\nThe profile\n\nWe are looking for a person who fit the following requirements:\n\n+3 years of experience years of experience in a similar role.\nExperience in Azure projects\nReal experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...\nReal Experience in AI\nExperience in presales and proposals\n\nWhat we offer you\n\nCareer Path\nRemote working\nTraining: Internal and technical certifications\n\nThink you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.\n\n>\n\nDo you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!\n\nWho we are\n\nmyCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.\n\nTasks\n\nThe Selected Person Will Do The Following Tasks:\n\nDefinition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...\nMaintenance of data solutions, failure analysis and solution proposal.\nCommunication with customers: proposal solutions, technical trainings...\n\nThe profile\n\nWe are looking for a person who fit the following requirements:\n\n+3 years of experience years of experience in a similar role.\nExperience in Azure projects\nReal experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...\nReal Experience in AI\nExperience in presales and proposals\n\nWhat we offer you\n\nCareer Path\nRemote working\nTraining: Internal and technical certifications\n\nThink you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.","Presales, Ai, Data Lakes, Azure, Data Architecture, Data Warehouse"
Salesforce Data Cloud Architect,VAYUZ Technologies,Fresher,,"Bengaluru, India",Login to check your skill match score,"JOB DESCRIPTION\nJob Locations: Bangalore, Hyderabad, Chennai, Noida, Pune, Gurgaon, Mumbai\nRole Expectations:\n1) Salesforce Data Cloud Expertise\nLeverage in-depth knowledge of Salesforce Data Cloud to implement and manage scalable solutions.\nStay current with platform updates and recommend best practices for optimization and innovation.\n2) Data Modeling\nDesign and develop efficient and scalable data models that align with business requirements.\nCollaborate with stakeholders to translate data needs into logical and physical data models.\n3) Data Integration\nLead the integration of external and internal data sources using tools like MuleSoft, Informatica, or native Salesforce connectors.\nEnsure seamless data flow across systems, maintaining integrity and performance.\n4) Data Quality\nEstablish and enforce data quality standards, including validation, cleansing, and enrichment processes.\nMonitor and improve data accuracy, consistency, and reliability within the Salesforce Data Cloud.\n5) Data Governance:\nImplement data governance frameworks, ensuring compliance with organizational policies and industry regulations.\nDefine data ownership, access controls, and stewardship roles.\n6) SQL Proficiency\nWrite and optimize SQL queries to extract, manipulate, and analyze data from multiple sources.\nSupport reporting, dashboard development, and data validation tasks using SQL.\n7) Problem Solving and Analysis\nAnalyze complex data challenges, identify root causes, and implement effective solutions.\nCollaborate across teams to troubleshoot data-related issues and deliver actionable insights\nQualifications:\nSalesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.\nData Modeling: Strong experience in designing and implementing data models.\nData Integration: Experience with data integration tools and techniques.\nData Quality: Understanding of data quality concepts and practices.\nData Governance: Knowledge of data governance principles and practices.\nSQL: Proficiency in SQL for data querying and manipulation.\nProblem-Solving: Strong analytical and problem-solving skills.\nCommunication: Excellent communication and collaboration skills.","Salesforce Data Cloud, Data Quality, Data Modeling, Mulesoft, Informatica, Data Governance, Sql, Data Integration"
Data Domain Architect Assoc - Conversational AI Annotation,JPMorganChase,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description\n\nJoin us for an exciting opportunity to leverage your advanced data annotation skills in the financial industry and contribute to cutting-edge machine learning models.\n\nAs a Data Domain Architect Associate within the Consumer & Community Banking team, you will conduct ML assisted data labeling to evaluate and train machine learning models. You will apply your technical knowledge and problem-solving skills across multiple applications, supporting data quality and summarization to enable Operations Management to achieve strategic objectives while ensuring compliance with all controls, policies, and procedures.\n\nJob Responsibilities\n\nBuild & Annotate banking domain text data for LLM/Gen AI models using various data labeling tools, taxonomy, and guidelines.\nAnalyze structured and unstructured text data, identify labels through context and disambiguation, and annotate with the correct label.\nUnderstand the nuances of language used in the financial industry and stay updated with the business aspects of products.\nValidate business results from a model perspective and provide feedback for model improvement using formulated metricies.\nConduct retrospective data analyses and contribute to clarifying business definitions and concepts.\nProvide feedback and suggestions for tool improvements to enhance efficiency and accuracy.\nWork closely with stakeholders, including machine learning engineers, data scientists, data engineers, and product managers across Chase's lines of businesses.\n\nRequired Qualifications, Capabilities, And Skills\n\nBachelor's or Master's degree in Statistics, Engineering, Computer Science, Information Technology, Finance, or a related field.\nThree years of hands-on experience working with data as a data analyst (conversational AI), data annotator, or data architect.\nBasic understanding of LLM/Gen AI with expertise in text data labeling processes and quality control.\nAnalytical and problem-solving skills, along with good project management skills (self-driven, well-organized, ability to meet tight deadlines).\nExperience using Python script, GIT version control, at least one annotation tool.\nIntermediate experience with Microsoft Office suite.\nExperience in conversational AI/ Chat bots training data needs.\n\nPreferred Qualifications, Capabilities, And Skills\n\nUnderstanding of LLM and Gen AI concepts.\nExcellent verbal and written communication skills to clearly present analytical findings and business recommendations to global stakeholders.\nFamiliarity or experience with Data Analytics and Visualization.\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We're proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions all while ranking first in customer satisfaction.\n\nThe CCB Data & Analytics team responsibly leverages data across Chase to build competitive advantages for the businesses while providing value and protection for customers. The team encompasses a variety of disciplines from data governance and strategy to reporting, data science and machine learning. We have a strong partnership with Technology, which provides cutting edge data and analytics infrastructure. The team powers Chase with insights to create the best customer and business outcomes.","data annotation, conversational AI, Microsoft Office Suite, GIT version control, text data labeling, LLM Gen AI, Python"
Data & Analytics Architect,Flexera,15-17 Years,,India,Login to check your skill match score,"Flexera saves customers billions of dollars in wasted technology spend. A pioneer in Hybrid ITAM and FinOps, Flexera provides award-winning, data-oriented SaaS solutions for technology value optimization (TVO), enabling IT, finance, procurement and cloud teams to gain deep insights into cost optimization, compliance and risks for each business service. Flexera One solutions are built on a set of definitive customer, supplier and industry data, powered by our Technology Intelligence Platform, that enables organizations to visualize their Enterprise Technology Blueprint in hybrid environmentsfrom on-premises to SaaS to containers to cloud.\n\nWe're transforming the software industry. We're Flexera. With more than 50,000 customers across the world, we're achieving that goal. But we know we can't do any of that without our team. Ready to help us re-imagine the industry during a time of substantial growth and ambitious plans Come and see why we're consistently recognized by Gartner, Forrester and IDC as a category leader in the marketplace. Learn more at flexera.com\n\nAt Flexera, we're on a mission to empower global enterprises by transforming IT insights into decisive actions. We are looking for an architect/principal engineer with deep expertise in analytics, including data modeling, semantic modeling, to take our world-class reference data in Technopedia to the next level, through deeper insights, richer features and more data sets. The ideal candidate will have a strong track record of providing technical leadership, deep technical expertise and delivering big data solutions through data lakehouse and large scale-processing technologies.\n\nAs an architect/principal engineer, you will be a key player in architecting, designing, developing, and maintaining our common ontology and data models, transformations, and analytics capabilities for our industry-leading reference data. You will collaborate closely with other architects and guide, influence and help engineering teams to ensure seamless integration with other components of our system.\n\nWhat you'll do:\n\nArchitect, design and develop our big data platform, processing pipelines and content reference data.\nDrive innovation, enabling capabilities to enhance speed to innovation for our analytics, AI/ML, and product development teams.\nDefine data models and semantic models for our wide range of data sets to provide insights and optimized user experiences.\nProvide guidance, influence and help engineering teams to deliver deep insights from our large and broad data sets.\nContinuously enhance your technical skills and mentor other engineers.\nPromote a high-standard engineering culture and operational excellence within the team.\nCollaborate with product and design teams to ensure the platform meets user needs.\nDefine project requirements, technical artifacts, and designs, driving consensus across Product Management, architects, and engineering teams.\nBalance priorities between new feature development, architectural enhancements, and technical debt reduction for a sustainable and scalable platform.\nChampion continuous improvement in product quality, security, and performance standards within the development team.\n\nYou'll be expected to have:\n\nBachelor's or higher degree in Computer Science, Software Engineering, or related field\nMinimum 15 years relevant experience in software development, including 8+ years of expertise in data modeling, semantic modeling, and data visualization.\nStrong expertise in other big data technologies, such as metadata catalogs, data lineage, and orchestration.\nStrong technical expertise in distributed systems, big data, and cloud computing.\nStrong problem-solving skills and ability to troubleshoot complex issues.\nGood understanding of streaming technologies, including Kafka, with experience in defining message schemas and data models.\nOutstanding communication skills and emotional intelligence to collaborate effectively with teams across the organization.\nExcellent written and verbal communication skills.\nAbility to work effectively both independently and in a collaborative team environment.\nPrior experience mentoring and providing technical guidance to junior engineers.\n\nAt Flexera, we foster a fun and engaged hybrid working environment where collaboration and innovation thrive. We value diversity and encourage applicants from underrepresented groups in technology to apply.\n\nJoin our team to not only contribute to a world-class global product but also to grow in your career. At Flexera, we encourage continuous learning and provide opportunities for professional development.\n\nFlexera is proud to be an equal opportunity employer. Qualified applicants will be considered for open roles regardless of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by local/national laws, policies and/or regulations.\n\nFlexera understands the value that results from employing a diverse, equitable, and inclusive workforce. We recognize that equity necessitates acknowledging past exclusion and that inclusion requires intentional effort. Our DEI (Diversity, Equity, and Inclusion) council is the driving force behind our commitment to championing policies and practices that foster a welcoming environment for all.\n\nWe encourage candidates requiring accommodations to please let us know by emailing [HIDDEN TEXT].","streaming technologies, semantic modeling, metadata catalogs, Orchestration, Data Lineage, Distributed Systems, Big Data Technologies, Kafka, Data Visualization, Data Modeling, Cloud Computing"
Lead Data Integration Architect,Hitachi Digital,10-12 Years,,India,Login to check your skill match score,"Our Company\n\nWe're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.\n\nOur group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.\n\nImagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to fit every requirement your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.\n\nPreferred job location: Bengaluru, Hyderabad, Pune, New Delhi or Remote\n\nThe team\n\nHitachi Digital is a leader in digital transformation, leveraging advanced AI and data technologies to drive innovation and efficiency across various operational companies (OpCos) and departments. We are seeking a highly experienced Lead Data Integration Architect to join our dynamic team and contribute to the development of robust data integration solutions.\n\nThe role\n\nLead the design, development, and implementation of data integration solutions using SnapLogic, MuleSoft, or Pentaho.\n\nDevelop and optimize data integration workflows and pipelines.\n\nCollaborate with cross-functional teams to integrate data solutions into existing systems and workflows.\n\nImplement and integrate VectorAI and Agent Workspace for Google Gemini into data solutions.\n\nConduct research and stay updated on the latest advancements in data integration technologies.\n\nTroubleshoot and resolve complex issues related to data integration systems and applications.\n\nDocument development processes, methodologies, and best practices.\n\nMentor junior developers and participate in code reviews, providing constructive feedback to team members.\n\nProvide strategic direction and leadership in data integration and technology adoption.\n\nWhat You'll Bring\n\nBachelor's or Master's degree in Computer Science, Data Engineering, or a related field.\n\n10+ years of experience in data integration, preferably in the Banking or Finance industry.\n\nExtensive experience with SnapLogic, MuleSoft, or Pentaho (at least one is a must).\n\nExperience with Talend and Alation is a plus.\n\nStrong programming skills in languages such as Python, Java, or SQL.\n\nTechnical proficiency in data integration tools and platforms.\n\nKnowledge of cloud platforms, particularly Google Cloud Platform (GCP).\n\nExperience with VectorAI and Agent Workspace for Google Gemini.\n\nComprehensive knowledge of financial products, regulatory reporting, credit risk, and counterparty risk.\n\nPrior strategy consulting experience with a focus on change management and program delivery preferred.\n\nExcellent problem-solving skills and the ability to work independently and as part of a team.\n\nStrong communication skills and the ability to convey complex technical concepts to non-technical stakeholders.\n\nProven leadership skills and experience in guiding development projects from conception to deployment.\n\nPreferred Qualifications:\n\nFamiliarity with data engineering tools and techniques.\n\nPrevious experience in a similar role within a tech-driven company.\n\nAbout Us\n\nWe're a global, 1000-stong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.\n\nChampioning diversity, equity, and inclusion\n\nDiversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.\n\nHow We Look After You\n\nWe help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.\n\nWe're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.","Alation, VectorAI, Agent Workspace for Google Gemini, Pentaho, Java, Google Cloud Platform, Mulesoft, Snaplogic, Talend, Sql, Python"
Data Engineering Architect,MindBrain,8-10 Years,,India,Login to check your skill match score,"Job Title: Data Engineering Architect\nLocation: Remote\nDuration: 6 Months-12 months\nStart Date: Within 2 Weeks\nClient Interview Required: Yes\nJob Summary\nWe are seeking a highly skilled Data Engineering Architect to design and build a scalable, secure, and analytics-ready HR data platform. This is a strategic and technical leadership role at the core of our product team, enabling data-driven decision-making and delivering a secure, multi-tenant, cloud-based architecture for HR data domains such as Employees, Jobs, Payroll, and Performance Management.\nKey Responsibilities1. Data Modeling\nDesign and maintain scalable, efficient, and HR-centric data models (Employees, Jobs, Payroll, Performance).\nImplement Slowly Changing Dimensions (SCD) for historical tracking (e.g., job changes, salary revisions).\nEnsure data schemas are analytics- and reporting-ready for downstream consumption.\nCollaborate with product and analytics teams to understand and implement data requirements.\n2. ETL/ELT Development\nBuild and maintain robust ETL/ELT pipelines using modern tools such as dbt, Apache Airflow, Fivetran, or Informatica.\nIngest and harmonize data from various internal and third-party HR systems.\nImplement data quality checks, consistency validation, and reconciliation processes.\nAutomate and monitor data pipelines for performance and reliability.\n3. Cloud Data Warehouse Architecture\nArchitect and manage scalable data solutions on Snowflake, Google BigQuery, or Amazon Redshift.\nOptimize cloud data warehouse infrastructure for performance, cost-efficiency, and storage scalability.\nImplement data partitioning, clustering, and performance tuning strategies.\n4. Data Security & Compliance\nApply robust Role-Based Access Control (RBAC) and tenant-level isolation for secure data access.\nEnsure PII masking, secure data handling, and compliance with GDPR, CCPA, and other data regulations.\nMaintain data lineage, audit trails, and metadata management for governance.\n5. Multi-Tenant SaaS Enablement\nArchitect systems to support multi-tenant SaaS environments with secure data segregation.\nDevelop strategies for tenant-level metadata, access control, and data isolation.\nSupport tenant-specific configurations and onboarding in the data architecture.\n6. Analytics & Business Intelligence (BI) Enablement\nDesign materialized views, summary tables, and data marts for downstream analytics.\nEnable seamless integration with Looker, Power BI, and Tableau for BI reporting.\nWork closely with analytics and business teams to build dashboards and ensure timely insights.\nRequired Skills & Experience\n8+ years of experience in Data Engineering, Data Warehousing, or similar technical fields.\nProven expertise in SQL and hands-on experience with cloud data warehouses:\nSnowflake, Google BigQuery, or Amazon Redshift.\nStrong working knowledge of modern ETL/ELT frameworks:\ndbt, Apache Airflow, Fivetran, Informatica.\nDeep understanding of HR data models including:\nEmployees, Payroll, Job Changes, Performance Management.\nStrong experience in data security, PII protection, and compliance (GDPR/CCPA).\nExperience building and managing multi-tenant data architectures in SaaS platforms.\nSolid understanding of data governance, metadata management, and auditing frameworks.\nProficiency in data pipeline automation, monitoring, and CI/CD practices for data workflows.\nExcellent problem-solving ability and attention to detail.\nStrong verbal and written communication and documentation skills.\nPreferred Qualifications\nExperience working in a SaaS or multi-tenant environment.\nFamiliarity with DevOps practices and tools for data infrastructure:\nCI/CD, Terraform, GitHub Actions, etc.\nExperience with HR tech platforms:\nWorkday, SAP SuccessFactors, Oracle HCM, BambooHR.\nCertifications in cloud technologies (any of the below is a plus):\nSnowflake SnowPro, AWS Certified Data Analytics, Google Cloud Professional Data Engineer.","PII protection, snowflake, dbt, Google BigQuery, Fivetran, CCPA, multi-tenant data architectures, auditing frameworks, data pipeline automation, CI CD practices, Informatica, Sql, Apache Airflow, Gdpr, Data Governance, Metadata Management, Amazon Redshift, Data Security"
Data Domain Architect Associate,JPMorganChase,4-6 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nYou are a highly motivated individual with strong skills in advanced analytics, problem-solving, influencing, interpersonal communication, and collaboration, then you have found the right team. Join us as an Automation and Analytics Developer within the Conduct, Compliance, and Operational Risk (CCOR) domain. In this role, you will develop and maintain a diverse range of analytical tools, reports, and dashboards to uncover insights and manage risks effectively. You will work closely with Compliance and Operational Risk Managers, as well as key partners across various functions, including Data Science, Technology, and Business colleagues.\n\nAs a Developer in our CCOR Data Analytics team, your responsibilities will include promoting and identifying opportunities for operational reengineering, and leading automation initiatives to streamline processes. You will have the chance to shape and improve our risk and control processes using data transformation tools such as Alteryx, Python, and SQL, while applying your knowledge of data science principles.\n\nJob Responsibilities\n\nUnderstands and develops automation solutions for the compliance and operational risk managers; builds analytical tools, reports, and dashboards to optimize and reduce manual processes.\nIdentifies and leads automation initiatives to streamline processes.\nCreates reporting, interprets results, and conveys information in a concise, straightforward, and professional manner for all levels of operational staff from supervisors to senior-level management.\nComprehends data requirements and accurately addresses related data quality edits efficiently.\nSpearheads projects and tasks by ensuring timely completion and articulates any issues and risks to management.\nEnsures the integrity of data through automated extraction, translation, processing, analysis, and reporting.\n\nRequired Qualifications, Skills, And Capabilities\n\nTool Sets Strong knowledge of Alteryx, Tableau, Python or UIPath. Proficiency in Tableau is essential for creating interactive and insightful dashboards.\nAnalytical Independent, logical problem solver with the ability to synthesize data, identify trends, and project outcomes. Strong understanding of data science concepts and methodologies.\nTechnical Proficiency in PowerPoint and Excel; knowledge of databases and API connectivity.\nFast Paced Multi-Tasker Ability to organize and prioritize multiple projects and responsibilities with accuracy, attention to detail, and limited supervision with very short turnaround times. Demonstrates the ability to react quickly and decisively in high-stress situations.\nInterpersonal Strong written and verbal communication skills with the ability to influence and work collaboratively with diverse/cross-functional teams. Develop and maintain effective relationships with a wide range of stakeholders.\nPresentation Ability to create presentations for all levels of management and effectively report with an executive presence. Experience creating complex reporting with compelling key messages.\nRisk & Controls Ability to work on Audit, Compliance, Risk, Control and Regulatory requirements in accordance with established procedures. Demonstrates accountability for work processes and the associated risks and controls. Demonstrates the ability to raise issues to relevant stakeholders or management with respect to the control environment.\nProject Management Ability to lead an initiative, prioritize work, and meet deadlines, escalating any issues to management.\n\nPreferred Qualifications, Skills, And Capabilities\n\nAlteryx/Tableau/Python certification\nBachelor's Degree, preferably majored in Computer Science, Statistics, Math, Business Administration, Finance, or Economics\nAt least 4 years of experience in a related field.\nExperience in data science projects or coursework, with a focus on data analysis and predictive modeling\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Powerpoint, Alteryx, Tableau, Uipath, Excel, Sql, Python"
Data Modeller - Architect-10+ Years-Immediate,Cortex Consultants LLC,10-14 Years,,"Pune, India",Login to check your skill match score,"Job Title: Data Modeller - Architect\n\nExperience: 10 to 14 Years\n\nLocation: Chennai, Hyderabad, Bangalore, Pune, Delhi\n\nJob Type: Permanent Role\n\nNotice Period: Immediate Joiners Only\n\nJob Description\n\nWe are looking for an experienced Data Modeller - Architect with 10 to 14 years of expertise in designing and architecting data models for large-scale enterprise systems. The ideal candidate will have in-depth experience in data architecture, data modeling, and building scalable solutions. This is a permanent role with immediate joiner requirements across multiple locations.\n\nKey Responsibilities\n\nDesign and architect data models for complex data environments and large datasets\nDevelop high-level data architecture solutions and manage data integration across systems\nCollaborate with stakeholders to understand business requirements and translate them into effective data models\nLead and mentor teams in the development and implementation of data modeling strategies\nOptimize database performance, ensuring data integrity and efficiency\nDefine and enforce best practices for data management and modeling across the organization\nWork closely with IT, data engineering, and analytics teams to align data architecture with business goals\nEnsure the scalability, security, and performance of data solutions\n\nRequired Skills & Qualifications\n\n10 to 14 years of experience in data modeling and architecture\nStrong expertise in relational and non-relational databases, data warehousing, and cloud data platforms\nProficiency in designing data models for complex, high-volume systems\nHands-on experience with SQL, NoSQL, and big data technologies\nProven track record of leading data architecture initiatives and cross-functional teams\nExperience in data integration, ETL processes, and data governance\nImmediate joiners only\n\nSkills: non-relational databases,etl,nosql,data technologies,data architecture,analytics solutions,software development,sql,cloud platforms,relational databases,modeling,cloud data platforms,etl processes,big data technologies,data integration,data governance,data modeling,data management,data,data warehousing","Analytics Solutions, Relational Databases, ETL processes, cloud data platforms, non-relational databases, Data Management, Data Architecture, Data Warehousing, Big Data Technologies, Data Modeling, Sql, Nosql, Data Governance, Data Integration"
Data Modeller - Architect-10+ Years-Immediate,Cortex Consultants LLC,10-14 Years,,"Delhi, India",Login to check your skill match score,"Job Title: Data Modeller - Architect\n\nExperience: 10 to 14 Years\n\nLocation: Chennai, Hyderabad, Bangalore, Pune, Delhi\n\nJob Type: Permanent Role\n\nNotice Period: Immediate Joiners Only\n\nJob Description\n\nWe are looking for an experienced Data Modeller - Architect with 10 to 14 years of expertise in designing and architecting data models for large-scale enterprise systems. The ideal candidate will have in-depth experience in data architecture, data modeling, and building scalable solutions. This is a permanent role with immediate joiner requirements across multiple locations.\n\nKey Responsibilities\n\nDesign and architect data models for complex data environments and large datasets\nDevelop high-level data architecture solutions and manage data integration across systems\nCollaborate with stakeholders to understand business requirements and translate them into effective data models\nLead and mentor teams in the development and implementation of data modeling strategies\nOptimize database performance, ensuring data integrity and efficiency\nDefine and enforce best practices for data management and modeling across the organization\nWork closely with IT, data engineering, and analytics teams to align data architecture with business goals\nEnsure the scalability, security, and performance of data solutions\n\nRequired Skills & Qualifications\n\n10 to 14 years of experience in data modeling and architecture\nStrong expertise in relational and non-relational databases, data warehousing, and cloud data platforms\nProficiency in designing data models for complex, high-volume systems\nHands-on experience with SQL, NoSQL, and big data technologies\nProven track record of leading data architecture initiatives and cross-functional teams\nExperience in data integration, ETL processes, and data governance\nImmediate joiners only\n\nSkills: non-relational databases,etl,nosql,data technologies,data architecture,analytics solutions,software development,sql,cloud platforms,relational databases,modeling,cloud data platforms,etl processes,big data technologies,data integration,data governance,data modeling,data management,data,data warehousing","Analytics Solutions, Relational Databases, ETL processes, cloud data platforms, non-relational databases, Data Management, Data Architecture, Data Warehousing, Big Data Technologies, Data Modeling, Sql, Nosql, Data Governance, Data Integration"
Data Science Architect,Birdeye,7-10 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Job Type\n\nFull-time\n\nDescription\n\nWhy Birdeye\n\nBirdeye is the highest-rated reputation, social media, and customer experience platform for local businesses and brands. Over 150,000 businesses use Birdeye's AI-powered platform to effortlessly manage online reputation, connect with prospects through social media and digital channels, and gain customer experience insights to grow sales and thrive.\n\nAt Birdeye, innovation isn't just a goal it's our driving force. Our commitment to pushing boundaries and redefining industry standards has earned us accolades as one of the foremost providers of AI, Reputation Management, Social Media, and Customer Experience software by G2.\n\nFounded in 2012 and headquartered in Palo Alto, Birdeye is led by a team of industry experts and innovators from Google, Amazon, Salesforce, and Yahoo. Birdeye is backed by the who's who of Silicon Valley - Salesforce founder Marc Benioff, Yahoo co-founder Jerry Yang, Trinity Ventures, World Innovation Lab, and Accel-KKR.\n\nRoles & Responsibilities:\n\nDesign and implement scalable and robust ML infrastructure to support end-to-end machine learning workflows.\nDevelop and maintain CI/CD pipelines for ML models, ensuring smooth deployment and monitoring in production environments.\nCollaborate with data scientists and software engineers to streamline the model development lifecycle, from experimentation to deployment and monitoring.\nImplement best practices for version control, testing, and validation of ML models.\nEnsure high availability and reliability of ML systems, including performance monitoring and troubleshooting.\nDevelop automation tools to facilitate data processing, model training, and deployment.\nStay up-to-date with the latest advancements in MLOps and integrate new technologies and practices as needed.\nMentor junior team members and provide guidance on MLOps best practices.\n\nRequirements\n\n\nBachelor's/Master's degree in Computer Science, Engineering, or a related technical field with 7-10 years of experience.\nExperience in designing and implementing ML infrastructure and MLOps pipelines.\nProficiency in cloud platforms such as AWS, Azure, or GCP.\nStrong experience with containerization and orchestration tools like Docker and Kubernetes.\nExperience with CI/CD tools such as Jenkins, GitLab CI, or CircleCI.\nSolid programming skills in Python and familiarity with other programming languages such as Java or Scala.\nUnderstanding of ML model lifecycle management, including versioning, monitoring, and retraining.\nExperience with infrastructure-as-code tools like Terraform or CloudFormation.\nFamiliarity with data engineering tools and frameworks, such as Apache Spark, Hadoop, or Kafka.\nKnowledge of security best practices for ML systems and data privacy regulations.\nExcellent problem-solving skills and the ability to work in a fast-paced, collaborative environment.\nExperience with ML frameworks such as TensorFlow, PyTorch, or Scikit-learn.\nKnowledge of data visualization tools and techniques.\nUnderstanding of A/B testing and experimental design.\nStrong analytical and troubleshooting skills.\nExcellent communication and documentation skills.\nExperience with monitoring and logging tools like Prometheus, Grafana, or ELK stack.\nKnowledge of serverless architecture and functions-as-a-service (e.g., AWS Lambda).\nFamiliarity with ethical considerations in AI and machine learning.\nProven ability to mentor and train team members on MLOps practices.\n\nWhy You'll Join Us:\n\n\nAt Birdeye, we are relentless innovators driven by a singular goal: to lead our category with unparalleled excellence. We don't just set goals we surpass them. We're a team of doers who roll up our sleeves and get the job done, delivering on our promises with unwavering dedication.\n\nWorking here means embracing a culture of action and accountability, where every person is empowered to make an impact. We don't just talk about making a difference we make it happen.","CircleCI, ML infrastructure, Scikit-learn, MLOps pipelines, GitLab CI, Prometheus, Elk Stack, Kafka, Grafana, Tensorflow, Pytorch, Docker, Terraform, Python, AWS, Java, Aws Lambda, Hadoop, Cloudformation, Scala, Apache Spark, Jenkins, Gcp, Azure, Kubernetes"
Data Domain Architect Associate,JPMorganChase,4-6 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nYou are a highly motivated individual with strong skills in advanced analytics, problem-solving, influencing, interpersonal communication, and collaboration, then you have found the right team. Join us as an Automation and Analytics Developer within the Conduct, Compliance, and Operational Risk (CCOR) domain. In this role, you will develop and maintain a diverse range of analytical tools, reports, and dashboards to uncover insights and manage risks effectively. You will work closely with Compliance and Operational Risk Managers, as well as key partners across various functions, including Data Science, Technology, and Business colleagues.\n\nAs a Developer in our CCOR Data Analytics team, your responsibilities will include promoting and identifying opportunities for operational reengineering, and leading automation initiatives to streamline processes. You will have the chance to shape and improve our risk and control processes using data transformation tools such as Alteryx, Python, and SQL, while applying your knowledge of data science principles.\n\nJob Responsibilities\n\nUnderstands and develops automation solutions for the compliance and operational risk managers; builds analytical tools, reports, and dashboards to optimize and reduce manual processes.\nIdentifies and leads automation initiatives to streamline processes.\nCreates reporting, interprets results, and conveys information in a concise, straightforward, and professional manner for all levels of operational staff from supervisors to senior-level management.\nComprehends data requirements and accurately addresses related data quality edits efficiently.\nSpearheads projects and tasks by ensuring timely completion and articulates any issues and risks to management.\nEnsures the integrity of data through automated extraction, translation, processing, analysis, and reporting.\n\nRequired Qualifications, Skills, And Capabilities\n\nTool Sets Strong knowledge of Alteryx, Tableau, Python or UIPath. Proficiency in Tableau is essential for creating interactive and insightful dashboards.\nAnalytical Independent, logical problem solver with the ability to synthesize data, identify trends, and project outcomes. Strong understanding of data science concepts and methodologies.\nTechnical Proficiency in PowerPoint and Excel; knowledge of databases and API connectivity.\nFast Paced Multi-Tasker Ability to organize and prioritize multiple projects and responsibilities with accuracy, attention to detail, and limited supervision with very short turnaround times. Demonstrates the ability to react quickly and decisively in high-stress situations.\nInterpersonal Strong written and verbal communication skills with the ability to influence and work collaboratively with diverse/cross-functional teams. Develop and maintain effective relationships with a wide range of stakeholders.\nPresentation Ability to create presentations for all levels of management and effectively report with an executive presence. Experience creating complex reporting with compelling key messages.\nRisk & Controls Ability to work on Audit, Compliance, Risk, Control and Regulatory requirements in accordance with established procedures. Demonstrates accountability for work processes and the associated risks and controls. Demonstrates the ability to raise issues to relevant stakeholders or management with respect to the control environment.\nProject Management Ability to lead an initiative, prioritize work, and meet deadlines, escalating any issues to management.\n\nPreferred Qualifications, Skills, And Capabilities\n\nAlteryx/Tableau/Python certification\nBachelor's Degree, preferably majored in Computer Science, Statistics, Math, Business Administration, Finance, or Economics\nAt least 4 years of experience in a related field.\nExperience in data science projects or coursework, with a focus on data analysis and predictive modeling\n\nABOUT US\n\nJPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.\n\nWe recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.\n\nAbout The Team\n\nOur professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Powerpoint, Alteryx, Tableau, Uipath, Excel, Sql, Python"
Data & Analytics Architect,AuxoAI,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nAs a Data & Analytics Architect, you will lead key data initiatives, including cloud transformation, data governance, and AI projects. You'll define cloud architectures, guide data science teams in model development, and ensure alignment with data architecture principles across complex solutions. Additionally, you will create and govern architectural blueprints, ensuring standards are met and promoting best practices for data integration and consumption.\n\nResponsibilities\n\nPlay a key role in driving a number of data and analytics initiatives including cloud data transformation, data governance, data quality, data standards, CRM, MDM, Generative AI and data science.\n\nDefine cloud reference architectures to promote reusable patterns and promote best practices for data integration and consumption.\n\nGuide the data science team in implementing data models and analytics models.\n\nServe as a data science architect delivering technology and architecture services to the data science community.\n\nIn addition, you will also guide application development teams in the data design of complex solutions, in a large data eco-system, and ensure that teams are in alignment with the data architecture principles, standards, strategies, and target states.\n\nCreate, maintain, and govern architectural views and blueprints depicting the Business and IT landscape in its current, transitional, and future state.\n\nDefine and maintain standards for artifacts containing architectural content within the operating model.\n\nRequirements\n\nStrong cloud data architecture knowledge (preference for Microsoft Azure)\n\n8-10+ years of experience in data architecture, with proven experience in cloud data transformation, MDM, data governance, and data science capabilities.\n\nDesign reusable data architecture and best practices to support batch/streaming ingestion, efficient batch, real-time, and near real-time integration/ETL, integrating quality rules, and structuring data for analytic consumption by end uses.\n\nAbility to lead software evaluations including RFP development, capabilities assessment, formal scoring models, and delivery of executive presentations supporting a final recommendation.\n\nWell versed in the Data domains (Data Warehousing, Data Governance, MDM, Data Quality, Data Standards, Data Catalog, Analytics, BI, Operational Data Store, Metadata, Unstructured Data, non-traditional data and multi-media, ETL, ESB).\n\nExperience with cloud data technologies such as Azure data factory, Azure Data Fabric, Azure storage, Azure data lake storage, Azure data bricks, Azure AD, Azure ML etc.\n\nExperience with big data technologies such as Cloudera, Spark, Sqoop, Hive, HDFS, Flume, Storm, and Kafka.","Azure Data Lake Storage, HDFS, Azure Data Fabric, Azure Ad, Flume, Azure Storage, Cloudera, Azure Data Factory, Storm, Kafka, Sqoop, Hive, Microsoft Azure, Azure ML, Spark"
CFIN P2D & Data Solution Architect,ABB,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN P2D & Data Solution Architect\n\nAt ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.\n\nWrite the next chapter of your ABB story.\n\nThis position reports to\n\nP2D & Data Solution Lead\n\nYour role and responsibilities\n\nWe are seeking a skilled P2D & Data Solution architect to play a key role in configuring, implementing, and supporting P2D/ Data processes within the Central Finance (CFIN) system. The P2D & Data Solution architect will collaborate closely with business stakeholders, process owners, and technical teams to ensure seamless integration of P2D functions within the CFIN framework. This role requires deep functional knowledge of CO processes, as well as the ability to translate business requirements into system configurations that optimize efficiency, accuracy, and business performance.\n\nThis position requires close coordination with Deployment team, Functional architects and external vendors, to maintain and evolve the P2D & Data architecture, ensuring it meets business needs and complies with ABB's standards.\n\nThe work model for the role is:\n\nThis role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.\n\nYou will be mainly accountable for:\n\nP2D Process Configuration: Configure and maintain P2D processes such as include Contribution margin reporting, COPA characteristics derivation and compliance with local and global P2D regulations within the Central Finance (CFIN) system, ensuring alignment with business needs and industry best practices.\nData replication: Oversee the configuration, implementation, and ongoing support of Data related topics within the CFIN system, ensuring processes are optimized and aligned with organizational goals. Expertise in Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data\nRequirements Gathering & Analysis: Collaborate with business stakeholders and process owners to thoroughly understand their requirements, document functional specifications, and translate these into system configurations.\nSystem Integration: Facilitate seamless integration of P2D & Data processes within the CFIN system, as well as with other enterprise systems (Local ERP, etc.), ensuring smooth data flow and automation of processes across platforms. User Support & Training: Provide ongoing functional support to end-users, addressing issues, offering solutions, and conducting training to ensure optimal utilization of the system and full understanding of P2D processes.\nTesting & Validation: Participate in testing and validation activities for new configurations, enhancements, and fixes, ensuring they meet business and functional requirements. Process Optimization: Regularly monitor and evaluate P2D & Data processes to identify opportunities for automation, efficiency improvements, and best practice implementation within the CFIN system.\nDocumentation & Compliance: Develop and maintain comprehensive documentation for P2D system configurations, process flows, and integration points, ensuring compliance with internal standards and regulatory guidelines. Collaboration with Technical Teams: Work closely with IS architects, developers, and technical teams to ensure that functional requirements are correctly implemented and aligned with system design specifications.\nTroubleshooting & Issue Resolution: Provide expert troubleshooting support for P2D -related system issues, working collaboratively with cross-functional teams to resolve any challenges promptly. Continuous Improvement: Stay informed about the latest industry trends, best practices, and system updates to continuously enhance the efficiency and effectiveness of P2D processes within CFIN.\nProject and New Demand Management: Take ownership of configuring new demands or changes in system functionality, ensuring proper alignment with system design documentation and business requirements. Data Management: Oversee the management and accuracy of all data related topics within O2C/ P2P/ TRE / R2R / TAX within the system, ensuring data integrity, consistency, and compliance with business rules across processes.\n\nQualifications for the role\n\nEducation: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in CO - SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.\nProven experience (5+ years) in FICO processes, with a strong background in system configuration and implementation within SAP or similar ERP environments. Experience in configuring CO processes in SAP or similar ERP systems, with a solid understanding of integration points and data flows across systems.\nFamiliarity with Central Finance (CFIN) and integration with other finance-related modules. Experience with requirements gathering, business analysis, and documentation of functional specifications.\nHigh level understanding of local ERP Contribution margin Reporting, COPA characteristics Derivation, COGS split, Price Difference Split, Profit Center, Cost Center, WBS Element, Order master Data mapping\nStrong analytical skills and attention to detail. Excellent communication skills, with the ability to collaborate effectively with cross-functional teams, stakeholders, and technical teams.\nA strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.\nExperience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.\n\nMore about us\n\nABB Finance is a trusted partner to the business and a world-class team who delivers forward-looking insights that drive sustainable long-term results and operates with the highest standards.\n\nWe value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory\n\nIt has come to our attention that the name of ABB is being used for asking candidates to make payments for job\n\nopportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made\n\navailable on our career portal for all fitting the criteria to apply.\n\nABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions.\n\nFor current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Error resolution, AIF monitoring, Currency Values mismatch, Process Optimization, Troubleshooting, Continuous Improvement, Documentation compliance, SLT based filtering, Testing and validation, CO processes, PC G L CC Map, P2D Data processes, Mdg, System integration, Sap Ecc, Data Management, Data Replication, Requirements Gathering, User Support"
Data Platform Architect,Gramener,Fresher,,"Chennai, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore/Chennai\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career path, steady growth prospects with great scope to innovate. Our goal is to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\nYou will engage in a diverse range of impactful customer technical Data platform projects, Including the development.\nLead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading.\nCollaborate with platform engineering teams to effectively implement Data Brick services within our infrastructure.\nLeverage your hands-on experience with Data Bricks Unity Catalog to implement robust data governance and lineage capabilities.\nAdvocate for and implement CI/CD practices to streamline the deployment of Data bricks solutions.\nContribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.\nUnderstand and articulate the analytics capabilities of the Data platform, enabling teams to derive actionable insights from their data.\n\nSkills And Qualifications\n\nCloud and Architectures:\n\nAzure Architecture and Platform: Expertise in Azure Data Lake, AI/ML model hosting, Key Vault, Event Hub, Logic Apps, and other Azure cloud services.\nDatabricks Development: Strong integration with Azure, workflow orchestration, and governance.\nData Engineering & Architecture: Hands-on experience with scalable ETL/ELT pipelines, Delta Lake, and enterprise data management.\n\nCoding And Implementation\n\nSoftware Engineering: Modular design, CI/CD, version control, and best coding and design practices.\nPython and PySpark: Experience in building reusable packages and components for both technical and business users, including data validation, lineage modules, and accelerators for data processing.\n\nProcess And Compliance\n\nEnterprise Process Understanding: Experience with large-scale workflows, structured environments, and compliance frameworks.\nGovernance and Security: Knowledge of data lineage, GxP, HIPAA, GDP compliance, and regulatory requirements.\nPharma, MedTech, Life Sciences (Good to Have): Understanding industry-specific data, regulatory constraints, and security considerations.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data acts as the core of decision making. We undertake strategic data consulting for organizations in laying out the roadmap for data-driven decision making. It helps organizations to convert data into a strategic differentiator. Through a host of our product and Solutions, and Service Offerings, we analyze and visualize large amounts of data.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Logic Apps, Key Vault, Databricks Development, Event Hub, CI CD, AI ML model hosting, enterprise data management, GDP compliance, Azure Architecture, ETL ELT pipelines, Delta Lake, Data Validation, enterprise process understanding, Security, Governance, Data Lineage, Version Control, Pyspark, Software Engineering, Azure Data Lake, Python"
Cloud Data & AI Architect,myCloudDoor,3-5 Years,,"Kolkata, India",Login to check your skill match score,"Description\n\nDo you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!\n\nWho we are\n\nmyCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.\n\nTasks\n\nThe Selected Person Will Do The Following Tasks:\n\nDefinition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...\nMaintenance of data solutions, failure analysis and solution proposal.\nCommunication with customers: proposal solutions, technical trainings...\n\nThe profile\n\nWe are looking for a person who fit the following requirements:\n\n+3 years of experience years of experience in a similar role.\nExperience in Azure projects\nReal experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...\nReal Experience in AI\nExperience in presales and proposals\n\nWhat we offer you\n\nCareer Path\nRemote working\nTraining: Internal and technical certifications\n\nThink you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.\n\n>\n\nDo you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!\n\nWho we are\n\nmyCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.\n\nTasks\n\nThe Selected Person Will Do The Following Tasks:\n\nDefinition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...\nMaintenance of data solutions, failure analysis and solution proposal.\nCommunication with customers: proposal solutions, technical trainings...\n\nThe profile\n\nWe are looking for a person who fit the following requirements:\n\n+3 years of experience years of experience in a similar role.\nExperience in Azure projects\nReal experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...\nReal Experience in AI\nExperience in presales and proposals\n\nWhat we offer you\n\nCareer Path\nRemote working\nTraining: Internal and technical certifications\n\nThink you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.","Presales, Ai, Data Lakes, Azure, Data Architecture, Data Warehouse"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Chennai, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune\n\nExperience: 7-16 Years\n\nWork Mode: Hybrid\n\nMandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)\n\nJob Description\n\nWe are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.\n\nKey Responsibilities\n\nDesign, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.\nCollaborate with data analysts, data architects, and business stakeholders to align data models with business needs.\nLeverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.\nManage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.\nContribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.\nApply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.\nStay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.\nDevelop and maintain data models using data modeling tools such as ER/Studio and Hackolade.\nDrive the adoption of best practices and standards for data modeling within the organization.\n\nSkills And Qualifications\n\nMinimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.\nExpertise in Azure and Databricks for building data solutions.\nProficiency in ER/Studio, Hackolade, and other data modeling tools.\nStrong understanding of data modeling principles and techniques (e.g., ERD, UML).\nExperience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).\nSolid understanding of data warehousing, ETL processes, and data integration.\nFamiliarity with big data technologies such as Hadoop and Spark is an advantage.\nIndustry Knowledge: A background in supply chain is preferred but not mandatory.\nExcellent analytical and problem-solving skills.\nStrong communication skills, with the ability to interact with both technical and non-technical stakeholders.\nAbility to work well in a collaborative, fast-paced environment.\n\nEducation\n\nB.Tech in any branch or specialization\n\nSkills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Hyderabad, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune\n\nExperience: 7-16 Years\n\nWork Mode: Hybrid\n\nMandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)\n\nJob Description\n\nWe are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.\n\nKey Responsibilities\n\nDesign, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.\nCollaborate with data analysts, data architects, and business stakeholders to align data models with business needs.\nLeverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.\nManage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.\nContribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.\nApply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.\nStay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.\nDevelop and maintain data models using data modeling tools such as ER/Studio and Hackolade.\nDrive the adoption of best practices and standards for data modeling within the organization.\n\nSkills And Qualifications\n\nMinimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.\nExpertise in Azure and Databricks for building data solutions.\nProficiency in ER/Studio, Hackolade, and other data modeling tools.\nStrong understanding of data modeling principles and techniques (e.g., ERD, UML).\nExperience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).\nSolid understanding of data warehousing, ETL processes, and data integration.\nFamiliarity with big data technologies such as Hadoop and Spark is an advantage.\nIndustry Knowledge: A background in supply chain is preferred but not mandatory.\nExcellent analytical and problem-solving skills.\nStrong communication skills, with the ability to interact with both technical and non-technical stakeholders.\nAbility to work well in a collaborative, fast-paced environment.\n\nEducation\n\nB.Tech in any branch or specialization\n\nSkills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune\n\nExperience: 7-16 Years\n\nWork Mode: Hybrid\n\nMandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)\n\nJob Description\n\nWe are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.\n\nKey Responsibilities\n\nDesign, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.\nCollaborate with data analysts, data architects, and business stakeholders to align data models with business needs.\nLeverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.\nManage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.\nContribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.\nApply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.\nStay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.\nDevelop and maintain data models using data modeling tools such as ER/Studio and Hackolade.\nDrive the adoption of best practices and standards for data modeling within the organization.\n\nSkills And Qualifications\n\nMinimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.\nExpertise in Azure and Databricks for building data solutions.\nProficiency in ER/Studio, Hackolade, and other data modeling tools.\nStrong understanding of data modeling principles and techniques (e.g., ERD, UML).\nExperience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).\nSolid understanding of data warehousing, ETL processes, and data integration.\nFamiliarity with big data technologies such as Hadoop and Spark is an advantage.\nIndustry Knowledge: A background in supply chain is preferred but not mandatory.\nExcellent analytical and problem-solving skills.\nStrong communication skills, with the ability to interact with both technical and non-technical stakeholders.\nAbility to work well in a collaborative, fast-paced environment.\n\nEducation\n\nB.Tech in any branch or specialization\n\nSkills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
"Data Analytics Architect (Looker), Contract",66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees\n\n66degrees is a leading Google Cloud Premier Partner. We believe that engineering takes heart. Focusing exclusively on Google Cloud, we help our clients achieve the most innovative and disruptive transformations in their industries.\n\n66degrees is seeking a senior contractor- Data Analytics Architect (Looker) to engage on a 8 weeks of remote assignment for approximately 40 hours per week with potential to extend. Interested candidates should have the following required skills in Golang.\n\nPlease note: ***Candidates must be available to join by first week of May, 2025***\n\nResponsibilities\n\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nWork with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.\nAdvise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.\nUtilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.\nWrite clean, efficient, and scalable code (LookML, Python as applicable)\nConduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.\nStay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.\nCollaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.\nProvide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.\n\nQualifications\n\nBachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.\n\nExcellent comprehension of Looker and its implementation, with additional analytics platform experience a major plus - especially MicroStrategy.\n\nExperience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.\nStrong proficiency with SQL required.\nDemonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.\nExcellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.\nSolid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.\nStrong problem-solving skills and the ability to translate business requirements into technical solutions.\nExcellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nGoogle Cloud certifications and any analytics platform certifications are a plus.\nA desire to stay ahead of the curve and continuously learn new technologies and techniques.\n\n66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, LookML, Golang, Data Modeling, Google Cloud, Sql, Python"
Data & AI Architect,Cognologix Technologies,12-14 Years,,"Pune, India",Login to check your skill match score,"You Will Work On\n\nWe are looking for self driven Data Professional to be a key member of our Data Practice and help our customers to solve their critical data challenges. Expected to play a lead role in delivering data solutions, engineering assets and processes to support the modernization, transformation requirements. The ideal candidate is passionate about defining & delivering cutting edge solutions to support the changing business needs and technology landscape.\n\nWhat You Will Do\n\nLead analysis, architecture, design and development of medium scale data and analytics solutions\nDesign & deliver architectural solutions for key customer projects across the Data & AI practice within the organization\nApply enterprise vision to all portfolio projects, understanding and communicating different architectural strategies that are consistent with business strategies\nContribute in Technical, Architectural presentation & discussions with existing, new customers through presales to delivery phases\nParticipate in strengthening engineering practices and developing the future state data architecture frameworks, patterns, standards, guidelines, and principles.\nResearch and maintain knowledge in emerging technologies and solutions to solve business problems\n\nWhat You Bring\n\nBachelors or equivalent degree in Engineering, Computer Science or related technical field\n12+ years of relevant experience in Data & Analytics technology stack\n5+ years knowledge of Big Data technologies & Cloud Tech stack, AWS or GCP preferred\nApplied Experience and Expertise in Data Architecture aspects ( Data Management, Data Governance, Data Models Valult, Dimensional etc)\nExperience in delivering end-to-end data platform solutions, from initiation phase to delivery, including supervision of developers/engineers.\nDemonstrate the ability to create high quality technical artifacts such as Technical Architecture document, detailed design documents etc\nAwareness of diverse tech stack in Data space and latest industry trends\nExcellent Analytical and Problem Solving Skills, High attention to details\nHigh impact communication skills, Effective interpersonal and collaboration skills\nExperience in Agile Methodologies and DevOps aspects\nMust be willing to both architect solutions & get deep into the weeds of delivering solutions\n\nAdvantage Cognologix\n\nA higher degree of autonomy, startup culture & small teams.\nOpportunities to become an expert in emerging technologies.\nRemote working options for the right maturity level.\nCompetitive salary & family benefits.\nPerformance-based career advancement.\n\nAbout Cognologix\n\nCognologix helps companies disrupt by reimagining their business models and innovate like a Startup. We are at the forefront of digital disruption and take a business-first approach to help meet our client's strategic goals.\n\nWe are a Data focused organization helping our clients to deliver their next generation of products in the most efficient, modern and cloud-native way.\n\nMinimum Experience\n\n12\n\nTop Skill\n\nData Architecture\n\nSubmit Your Application\n\nYou have successfully applied\n\nYou have errors in applying\n\nApply With Resume *\n\nFirst Name*\n\nMiddle Name\n\nLast Name*\n\nEmail*\n\nMobile\n\nPhone\n\nSocial Network and Web Links\n\nProvide us with links to see some of your work (Git/ Dribble/ Behance/ Pinterest/ Blog/ Medium)\n\nEmployer\n\nEducation","data models, Data Management, Big Data Technologies, Data Architecture, Data Governance, Devops, Agile Methodologies"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore\n\nWhat Gramener offers you\n\nGramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.\n\nRoles and Responsibilities\n\n10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.\nWorking with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.\nAnalyzing requirements: Performing requirement analysis and creating architectural models.\nIdentifying issues: Identifying operational issues and recommending strategies to resolve them.\nCommunicating with business users: Communicating technical solutions to business users and addressing their questions.\nValidating solutions: Ensuring solutions align with corporate standards and compliance requirements.\nDeveloping technical specifications: Creating technical design specifications for solutions and systems engineers.\nAssessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.\n\nData engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.\n\nAbout Us\n\nWe help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.\n\nTo know more about us visit Gramener Website and Gramener Blog.\n\nApply for this role Apply for this Role","Databases, Python, Web Scraping, Apis"
Data Engineering Architect,Quest Global,15-25 Years,,"Hyderabad, India",Login to check your skill match score,"Considering Only Immediate Joinee & Candidates willing for Hyderabad Office based role\nStrong hands-on experience with Data Engineering, AWS, Java, Kubernetes, Microservices, React. Apple Technology Stack will be a plus\nExperience : 15 - 25 Years\nJob Requirements\nQuest Global is seeking a highly skilled, hands-on Solution Architect to drive innovation and deliver impactful, scalable solutions tailored to our customers needs. In this pivotal role, you will develop proof of concepts, design and lead solution implementations that enhance our offerings. After successful POC completion, you will mentor and guide engineers in developing a production-ready solution.\nKey Responsibilities:\nOverall responsibility to build architectures (new or rearchitect existing prods)\nBuild POCs to justify new Enterprise architectures (Cloud) including but not limited to AI/ Gen AI/ Data pipe enhancements, performance improvement, adoption of new technologies\nCollaborate with client architects, Prod Managers and Engineering leaders to align with the product vision and help build corresponding response from Quest Global along with the VBU and project teams\nHands on Technical\nSkill Level Expectation from Candidate\nData Engineering (Big Data, Spark, ETL pipelines, and Airflow)\nHave built solutions and gone live with that\nMust have built high scale, high performance solutions which are live in production\nHave built solutions and gone live with that\nCloud - AWS (No Azure), Hybrid solutions (On Prem/ Cloud combos)\nHave built solutions and gone live with that\nJava + Microservices\nCan work independently\nRest APIs\nCan work independently\nAI/ Machine Learning (On Premise/ Cloud)\nCan work independently\nDockers/ Kubernates (Only consumer and NOT the CI/CD set up view)\nCan work independently\nPython Programming (in context of AI/ML)\nCan work independently\nBuilding / fintuning of LLM Models\nTech know how with limited hands on (done POCs)\nUsage of LLM models\nCan work independently","Airflow, Ai, ETL pipelines, data engineering, Java, Machine Learning, Dockers, Rest Apis, Big Data, Microservices, React, Cloud, Spark, Kubernetes, Python, AWS"
"Assistant Vice President, Lead Solutions Architect- Data Engineering",Genpact,Fresher,,"Gurugram, Gurugram",IT/Computers - Hardware & Networking,"Ready to build the future with AI\n\n\n\nAt Genpact, we don&rsquot just keep up with technology&mdashwe set the pace. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.\n\n\n\nIf you thrive in a fast-moving, innovation-driven environment, love building and deploying cutting-edge AI solutions, and want to push the boundaries of what&rsquos possible, this is your moment.\n\n\n\nGenpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.\n\n\n\n\n\n\n\n\n\nInviting applications for the role of Assistant Vice President, Lead Solutions Architect- Data Engineering!\n\n\n\nThe Head of Solutioning will lead the solutioning for Genpact, working on scalable solutions for business opportunities. This leadership role requires a blend of technical expertise, business acumen, and creative problem-solving skills to conceptualize and deliver cutting-edge solutions aligned with market trends and organizational goals.\n\n\n\nResponsibilities\n\n\n\n\n\nSupport the sales team by providing subject matter expertise and solutioning inputs for RFPs, proposals, and client presentations.\n\n\n\n\n\n\n\nActively participate in the sales cycle, helping to close deals by demonstrating the value of the proposed solutions.\n\n\n\n\n\n\n\nWork closely with internal and external stakeholders to understand their needs and translate them into actionable solution strategies.\n\n\n\n\n\n\n\nFoster a culture of continuous innovation and experimentation to drive differentiation in the market.\n\n\n\n\n\n\n\nDefine governance frameworks and risk management strategies for new solutions, ensuring that they meet quality, security, and compliance standards.\n\n\n\n\n\n\n\nLead and mentor a high-performance team of solution architects, engineers, and business strategists.\n\n\n\n\n\nQualifications we seek in you!\n\n\n\nMinimum Qualifications / Skills\n\n\n\n\n\nShould have experience in Multimillion complex data engineering and needs right from the get to go.\n\n\n\n\n\n\n\nBachelor&rsquos degree in Business Administration, Engineering, Computer Science, or a related field (Master&rsquos or MBA preferred).\n\n\n\n\n\n\n\nRelevant years in IT services with strong background in solutioning leadership role.\n\n\n\n\n\n\n\n\n\n\nPreferred Qualifications/ Skills\n\n\n\n\n\nProven track record of creating and delivering new business concepts, products, or solutions that have contributed to revenue growth.\n\n\n\n\n\n\n\nStrong understanding of business strategy, P&L management, and market dynamics.\n\n\n\n\n\n\n\nAbility to translate business goals into actionable, scalable solutions.\n\n\n\n\n\n\n\nExceptional leadership skills with the ability to drive change, inspire teams, and influence stakeholders.\n\n\n\n\n\n\n\nStrong problem-solving skills and the ability to manage ambiguity.\n\n\n\n\n\nPreferred Certifications:\n\n\n\n. PMP or similar project management certification.\n\n\n\n. TOGAF or enterprise architecture certification.\n\n\n\n. Agile or SAFe certifications (e.g., Certified Scrum Master, SAFe Program Consultant).\n\n\n\n\n\n\n\n\nWhy join Genpact\n\n\n\n\n\nLead AI-first transformation - Build and scale AI solutions that redefine industries\n\n\n\n\n\n\n\nMake an impact - Drive change for global enterprises and solve business challenges that matter\n\n\n\n\n\n\n\nAccelerate your career&mdashGain hands-on experience, world-class training, mentorship, and AI certifications to advance your skills\n\n\n\n\n\n\n\nGrow with the best - Learn from top engineers, data scientists, and AI experts in a dynamic, fast-moving workplace\n\n\n\n\n\n\n\nCommitted to ethical AI - Work in an environment where governance, transparency, and security are at the core of everything we build\n\n\n\n\n\n\n\nThrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress\n\n\n\n\n\nCome join the 140,000+ coders, tech shapers, and growth makers at Genpact and take your career in the only direction that matters: Up.\n\n\n\nLet&rsquos build tomorrow together.\n\n\n\n\n\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Solution Architect - Data,PreludeSys,7-9 Years,,"Chennai, India",Login to check your skill match score,"As a Solution Architectyou will:\nDefine and evolve the company's AI vision and execution plan.\nEnsure alignment of AI initiatives with business goals and measurable outcomes.\nLead the design, development, and deployment of AI models.\nWork across the full AI lifecyclefrom problem scoping and data exploration to model development and production integration.\nStay actively engaged with machine learning, deep learning, NLP, and generative AI.\nExperiment with and implement cutting-edge models like LLMs (e.g., GPT, BERT).\nPartner with internal teams and external clients to gather requirements, provide architectural guidance, and ensure seamless integration of AI solutions into existing systems.\nMentor junior team members and promote a culture of continuous learning and excellence.\nAdvocate for responsible AI practices, ensuring models are ethical, explainable, and scalable.\nSkills Required:\n7+ years in AI/ML solution architecture, data science, or enterprise AI product development.\nProficiency in Python and hands-on experience with ML frameworks like TensorFlow, PyTorch, Scikit-learn, and Hugging Face.\nSolid understanding of algorithms, model design, evaluation, and deployment workflows.\nExperience working with transformer-based models and NLP pipelines using tools like GPT, BERT, or similar.\nKnowledge of model lifecycle tools (e.g., MLflow, Kubeflow), containerization (Docker), and API frameworks (FastAPI, Flask).\nProven experience deploying AI solutions on Azure, AWS, or GCP, utilizing native AI/ML services.\nMaster's degree in Data Science, AI, Machine Learning, or a related technical field (PhD preferred).\nAzure AI Engineer Associate or equivalent certifications preferred.\nResearch publications, patents, or contributions to open-source AI/ML projects will be an added advantage.","Hugging Face, Scikit-learn, MLflow, GPT, Kubeflow, BERT, Tensorflow, Gcp, Pytorch, Docker, Flask, FastAPI, Azure, Python, AWS"
Senior Enterprise Architect - Data,Bread Financial,10-12 Years,,India,Login to check your skill match score,"Every career journey is personal. That's why we empower you with the tools and support to create your own success story.\n\nBe challenged. Be heard. Be valued. Be you ... be here.\n\nJob Summary\n\nSr Enterprise Architect designs and implements enterprise-wide IT solutions to align technology initiatives with business goals. Serve as a strategic advisor to leadership, ensuring architecture principles are followed.\n\nEssential Job Functions\n\nDevelop and maintain enterprise architecture frameworks and standards. -\nOversee enterprise data strategies, designing platforms for advanced analytics, AI/ML, and regulatory adherence\nEvaluate new technologies and identify opportunities for system enhancements.\nCollaborate with stakeholders to analyze business requirements and translate them into technical solutions.\nEnsure technology initiatives align with the organization's overall architectural vision.\nConduct system audits and maintain documentation for architecture solutions.\n\nMinimum Qualifications\n\n\nBachelor's Degree or equivalent education and / or experience in computer Science, Engineering, Mathematics or related field.\n10+ years in Information Technology\n\nPreferred Qualifications\n\n\nRelevant certifications, such as TOGAF or AWS Solutions Architect\n\nSkills\n\n\nApplication Development\nBusiness Alignment\nBusiness Process Modeling\nBusiness Case Development\nCode Inspection\nCloud Architectures\nEnterprise Architecture Framework\nIT Architecture\nIT Roadmap\nSolution Architecture\n\nReports To: Manager and above\n\nDirect Reports: 0\n\nWork Environment\n\nNormal office environment, hybrid.\n\nOther Duties\n\n\nThis job description is illustrative of the types of duties typically performed by this job. It is not intended to be an exhaustive listing of each and every essential function of the job. Because job content may change from time to time, the Company reserves the right to add and/or delete essential functions from this job at any time.\n\nAbout Bread Financial\n\nAt Bread Financial, you'll have the opportunity to grow your career, give back to your community, and be part of our award-winning culture. We've been consistently recognized as a best place to work nationally and in many markets and we're proud to promote an environment where you feel appreciated, accepted, valued, and fulfilledboth personally and professionally. Bread Financial supports the overall wellness of our associates with a diverse suite of benefits and offers boundless opportunities for career development and non-traditional career progression.\n\nBread Financial (NYSE: BFH) is a tech-forward financial services company that provides simple, personalized payment, lending, and saving solutions to millions of U.S consumers. Our payment solutions, including Bread Financial general purpose credit cards and savings products, empower our customers and their passions for a better life. Additionally, we deliver growth for some of the most recognized brands in travel & entertainment, health & beauty, jewelry and specialty apparel through our private label and co-brand credit cards and pay-over-time products providing choice and value to our shared customers.\n\nTo learn more about Bread Financial, our global associates and our sustainability commitments, visit breadfinancial.com or follow us on Instagram and LinkedIn.\n\nAll job offers are contingent upon successful completion of credit and background checks.\nBread Financial is an Equal Opportunity Employer.\n\nJob Family\n\n\nInformation Technology\n\nJob Type\n\nRegular","IT Roadmap, business case development, Cloud Architectures, Code Inspection, Business Process Modeling, Enterprise Architecture Framework, business alignment, Application Development, IT Architecture, Solution Architecture"
Architect (Data Engineering),Amgen Inc,10-15 Years,,Hyderabad,Biotechnology,"Roles & Responsibilities:\nDesign and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have Skills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nWell versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nGood-to-Have Skills:\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred","Apache Airflow, Apache Spark, Cloud Devops, Sql, Aws"
Azure Data Lead or Architect,Coforge,8-12 Years,,Noida,Information Technology,"Experience Required:- 8 to 12 Years\nAre you a seasonedAzure Data Architect or Leadwith 8+ years of experience, passionate about building robust and scalable data solutions on Azure We're looking for a talented individual to join our team in Greater Noida and drive our data strategy forward!\nAbout the Role:\nAs a Senior Data Architect, you'll be instrumental in designing and implementing cutting-edge data architecture frameworks on Azure, ensuring our data infrastructure aligns with business objectives and technical requirements. You'll play a key role in defining reference architectures, optimizing data flow, and improving cost efficiency.\nJob Description\nResponsibilities:\n* Design and build data architecture frameworks leveraging Azure services (Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure Data Lake Storage, Azure SQL Database, etc.).\n* Define and implement reference architectures and data flow for future infrastructure development.\n* Conduct cost analysis and optimize existing data infrastructure for efficient data handling.\n* Develop and implement an organization-wide data strategy aligned with business processes.\n* Hands-on development with SQL, ETL processes, Python/PySpark.\n* Collaborate with network and infrastructure teams to ensure seamless integration.\n* Implement data modeling best practices (dimensional modeling, data vault).\n* Ensure data security and compliance using Azure security tools (Azure Active Directory, Azure Key Vault).\n* Work with BI tools (specify tools, example Power BI) to deliver data insights.\n* Implement data governance and data quality processes.\n* Utilize version control tools (specify tools, example Git).\n* Work with Infrastructure as Code (specify tools, example Terraform, ARM templates).\n* Work within an Agile environment (specify agile method, example Scrum).\n* Effectively communicate with stakeholders at all levels.\nRequirements:-\n* 8+ years of experience in Data Warehousing and Azure Cloud technologies.\n* Strong hands-on experience with SQL, ETL, Python/PySpark.\n* Proven expertise in designing and implementing data architectures on Azure.\n* Exposure to Azure DevOps and Business Intelligence.\n* Solid understanding of data governance, data security, and compliance.\n* Excellent communication and collaboration skills.\n* Ability to work effectively in a UK shift (1 PM IST to 9:30 PM IST).\n* Ability to work in a hybrid environment, with 3 days/week in office.\n* Location: Greater Noida.\nKey Improvements:\n* Specific Azure Services: Added examples of relevant Azure services.\n* Data Modeling: Explicitly mentioned data modeling.\n* Cloud Security: Added examples of Azure security tools.\n* BI Tools: added a placeholder to add specific tools.\n* Version control and IaC: added placeholders to specify the used tools.\n* Agile: added a placeholder to specify the agile methodology.\n* Clear Call to Action: Included a To Apply section.\n* Relevant Hashtags: Added relevant hashtags for increased visibility.\n* Data Quality: Added Data quality to the responsibilities.\n* Clear location and work schedule","Azure Data, Pyspark, Sql, Python, Etl, Azure Devops, Data Architecture"
Data and Analytics Architect,Wipro Limited,4-8 Years,,Chennai,Software,"The purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.\nDo\nDefine and Develop Data Architecture that aids organization and clients in new/ existing deals\nPartnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation\nAssess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy\nCreate data strategy and road maps for the Reference Data Architecture as required by the clients\nEngage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request\nEnsure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise\nDevelop, communicate, support and monitor compliance with Data Modelling standards\nOversee and monitor all frameworks to manage data across organization\nProvide insights for database storage and platform for ease of use and least manual work\nCollaborate with vendors to ensure integrity, objectives and system configuration\nCollaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization\nPresenting data repository, objects, source systems along with data scenarios for the front end and back end usage\nDefine high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes\nKnowledge of all the Data service provider platforms and ensure end to end view.\nOversight all the data standards/ reference/ papers for proper governance\nPromote, guard and guide the organization towards common semantics and the proper use of metadata\nCollecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control\nProvide solution of RFPs received from clients and ensure overall implementation assurance\nDevelop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives\nAnalyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data\nProvide technical leadership to the implementation of custom solutions through thoughtful use of modern technology\nDefine and understand current issues and problems and identify improvements\nEvaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout\nUnderstand the root cause problem in integrating business and product units\nValidate the solution/ prototype from technology, cost structure and customer differentiation point of view\nCollaborating with sales and delivery leadership teams to identify future needs and requirements\nTracks industry and application trends and relates these to planning current and future IT needs\nBuilding enterprise technology environment for data architecture management\nDevelop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes\nEvaluate all the implemented systems to determine their viability in terms of cost effectiveness\nCollect all the structural and non-structural data from different places integrate all the data in one database form\nWork through every stage of data processing: analysing, creating, physical data model designs, solutions and reports\nBuild the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices\nImplement the best security practices across all the data bases based on the accessibility and technology\nStrong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)\nDemonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration\nEnable Delivery Teams by providing optimal delivery solutions/ frameworks\nBuild and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor\nDefine database physical structure, functional capabilities, security, back-up and recovery specifications\nDevelops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nMonitor system capabilities and performance by performing tests and configurations\nIntegrate new solutions and troubleshoot previously occurred errors\nManages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\nIdentify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nEnsure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\nRecommend tools for reuse, automation for improved productivity and reduced cycle times\nHelp the support and integration team for better efficiency and client experience for ease of use by using AI methods.\nDevelops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nEnsures architecture principles and standards are consistently applied to all the projects\nEnsure optimal Client Engagement\nSupport pre-sales team while presenting the entire solution design and its principles to the client\nNegotiate, manage and coordinate with the client teams to ensure all requirements are met\nDemonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor.","Data and Analytics Architect, Data Modelling, Frameworks"
Systems and Product Architect (Data Cloud),Creditsafe Technology,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"ABOUT THE TEAM\nJoin our dynamic team of expert engineers at Creditsafe, where we are revolutionizing the data ecosystem through strategic innovation and cutting-edge architectural modernization. As a System & Product Architect, you will lead the transformation of our systems and product architecture on AWS, managing billions of data objects with daily increments exceeding 25 million. Your expertise will be pivotal in ensuring high availability, data integrity, and outstanding performance, powering our APIs and file delivery systems to deliver seamless data experiences to our global clients. Be at the forefront of data innovation and make an impact on a global scale.\nABOUT THE ROLE\nThis role places you at the center of Creditsafe's transformation journey. You will define architectural standards, design patterns, and technical roadmaps that guide our shift to a modern cloud infrastructure. Collaborating with technologies such as Python, Linux, Airflow, AWS DynamoDB, S3, Glue, Athena, Redshift, Lambda, API Gateway, Terraform, and CI/CD pipelines, you will ensure our platform is scalable, resilient, and ready for the future.\nKEY DUTIES AND RESPONSIBILITIES\nDrive the technical vision, architecture, and design principles for system replatforming and migration.\nDesign scalable, distributed architecture patterns that optimize for throughput, resilience, and maintainability.\nCreate and maintain system architecture documentation, including diagrams, data flows, and design decisions.\nEstablish governance frameworks for technical debt management and architectural compliance.\nDesign event-driven architectures for distributed data processing using AWS technologies.\nWork with team to support & build APIs capable of supporting 1000+ transactions per second.\nMentor engineers on architectural best practices and system design principles.\nPartner with security teams to ensure architectures meet compliance requirements.\nContribute to technical roadmap aligned with company's vision & Product roadmap.\nSKILLS AND QUALIFICATIONS\n8+ years of software engineering experience, with at least 4 years in system architecture.\nProven track record in large-scale replatforming and system modernization initiatives.\nCloud-native architecture expertise, particularly with AWS services (Redshift, S3, DynamoDB, Lambda, API Gateway).\nSolid understanding of data platforms, ETL/ELT pipelines, and data warehousing.\nExperience with serverless architectures, microservices, and event-driven design patterns.\nStrong technical skills with Python, Terraform and modern DevOps practices.\nExperience designing high-throughput, low-latency API solutions.\nDemonstrated technical leadership and mentoring abilities.\nClear communication skills, with the ability to translate complex technical concepts.\nStrategic thinker, love white-boarding, and keen on mentoring engineers.\nDesirable:\nExperience with AI and machine learning architecture patterns.\nAWS Solution Architect Pro certification.","Airflow, Glue, Athena, S3, Dynamodb, Redshift, Lambda, Terraform, Linux, Python, AWS, Api Gateway"
Chief Architect - Data & AI,Orion Innovation,Fresher,,India,Login to check your skill match score,"Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.\n\nWe are seeking a dynamic and experienced leader for our Data Architecture and Data Science Practice. This role will be instrumental in shaping our organization's data strategy, driving innovation through advanced analytics, and ensuring robust data architecture to support our business objectives.\n\nResponsibilities\n\nStrategic Leadership: Develop and implement data strategies that align with organizational goals and objectives. Drive innovation and efficiency through the effective use of data.\nTeam Management: Lead and mentor a team of data architects, data engineers, and data scientists. Provide guidance and support to foster professional growth and collaboration within the team.\nData Architecture: Designing and maintaining scalable and efficient solutions to ensure data integrity, availability, and security across an organization's infrastructure. This includes translating business requirements into logical and physical data models, ensuring data is transformed correctly from source to target systems through detailed mapping, and converting business models into a comprehensive data platform. Data integration combines data from various sources into a unified view using ETL/ELT processes, data pipelines, and APIs, centralizing storage in data lakes and warehouses. An audit framework tracks and monitors data activities to ensure compliance and transparency, while scheduling and monitoring tools ensure data processes run smoothly and on time. Adhering to Service Level Agreements (SLAs) involves defining SLAs, tracking key metrics, managing incidents efficiently, and providing regular reports to stakeholders. This holistic approach supports business needs, ensures data quality, and maintains operational efficiency.\nAdvanced Analytics: Oversee the development and implementation of advanced analytics techniques, including machine learning, predictive modeling, and optimization algorithms. Drive the adoption of best practices and methodologies in data science.\nStakeholder Collaboration: Collaborate with stakeholders across the organization to understand business requirements and priorities. Translate business needs into data initiatives and deliver actionable insights to drive decision-making.\nTechnology Evaluation: Stay updated on emerging technologies and trends in data management and analytics. Evaluate new tools, platforms, and methodologies to enhance the organization's data capabilities.\nGovernance and Compliance: Establish and enforce data governance policies and procedures to ensure regulatory compliance, data privacy, and security. Implement best practices for data quality management and data lineage tracking.\nPerformance Monitoring: Define key performance indicators (KPIs) to measure the effectiveness of data practices. Monitor performance metrics, analyze trends, and identify areas for improvement.\n\nQualifications\n\n\nBachelor's or Master's degree in Computer Science, Data Science, Information Systems, or a related field.\nStrong leadership experience in data architecture, data engineering, or data science.\nIn-depth knowledge of data architecture principles, data modeling techniques, and database technologies.\nProficiency in programming languages such as Python, R, SQL, etc.\nStrong communication skills with the ability to translate technical concepts into business terms.\nExperience working in a fast-paced environment and managing multiple priorities effectively\n\nOrion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.\n\nCandidate Privacy Policy\n\nOrion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, Orion, we Or us) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (Notice) Explains\n\nWhat information we collect during our application and recruitment process and why we collect it;\nHow we handle that information; and\nHow to access and update that information.\n\nYour use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.","Data lakes, R, Data architecture principles, Data pipelines, Data modeling techniques, Apis, Optimization Algorithms, Sql, ELT, Database Technologies, Machine Learning, Predictive Modeling, Python, Etl"
Senior Principal Consultant-?SFDC Data Cloud Technical?Architect,Genpact,Fresher,,Noida,IT/Computers - Hardware & Networking,"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose - the relentless pursuit of a world that works better for people - we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.\n\nInviting applications for the role of Senior Principal Consultant- SFDC Data Cloud Technical Architect\nA Salesforce Data Cloud Tech Architect is responsible for designing and implementing data solutions within the Salesforce ecosystem, leveraging Salesforce Data Cloud and related technologies. Here's an overview of the role:\nResponsibilities:\n. Design scalable and efficient data architectures using Salesforce Data Cloud.\n. Collaborate with stakeholders to understand business requirements and translate them into technical solutions.\n. Ensure data security, compliance, and governance within the Salesforce platform.\n. Lead the integration of Salesforce Data Cloud with other systems and tools.\n. Optimize data solutions for performance, scalability, and cost-efficiency\n. Provide technical guidance and mentorship to development teams.\n. Stay updated with Salesforce advancements and best practices.\n\n\nQualifications we seek in you!\nMinimum qualifications:\n. B.E or B.Tech or MCA\n. Expertise in Salesforce Data Cloud and related tools.\n. Strong knowledge of data modeling, ETL processes, and data integration.\n. Proficiency in programming languages like Apex, Java, or Python\n. Familiarity with Salesforce APIs, Lightning, and Visualforce.\n. Excellent problem-solving and communication skills.\n\nPreferred qualifications\n. Salesforce PD II, Sales Cloud, Service Cloud and Marketing Cloud Certifications etc...\n\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation. Get to know us at genpact.com and on LinkedIn, X, YouTube, and Facebook.\nFurthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Solution Architect -Data Engineering and Analytics,Eaton,10-12 Years,,"Pune, India",Login to check your skill match score,"Job Summary\n\nAs a Solutions Architect, the candidate will be responsible for understanding requirements and building solution architectures for the Data Engineering and Advanced Analytics Capability. The role will require a mix of technical knowledge and finance domain functional knowledge while the functional knowledge is not necessarily a must have.The candidate will apply best practices to create data architectures that are secure, scalable, cost-effective, efficient, reusable, and resilient. The candidate will participate in technical discussions, present their architectures to stakeholders for feedback, and incorporate their input. The candidate will evaluate, recommend, and integrate SaaS applications to meet business needs, and provide architectures for integrating existing Eaton applications or developing new ones with a cloud-first mindset. The candidate will offer design oversight and guidance during project execution, ensuring solutions align with strategic business and IT goals.\n\nAs a hands-on technical leader, the candidate will also drive Snowflake architecture. The candidate will collaborate with both technical teams and business stakeholders, providing insights on best practices and guiding data-driven decision-making. This role demands expertise in Snowflake's advanced features and cloud platforms, along with a passion for mentoring junior engineers.\n\nJob Responsibilities\n\nCollaborate with data engineers, system architects, and product owners to implement and support Eaton's data mesh strategy, ensuring scalability, supportability, and reusability of data products.\nLead the design and development of data products and solutions that meet business needs and align with the overall data strategy, creating complex enterprise datasets adhering to technology and data protection standards.\nDeliver strategic infrastructure and data pipelines for optimal data extraction, transformation, and loading, documenting solutions with architecture diagrams, dataflows, code comments, data lineage, entity relationship diagrams, and metadata.\nDesign, engineer, and orchestrate scalable, supportable, and reusable datasets, managing non-functional requirements, technical specifications, and compliance.\nAssess technical capabilities across Value Streams to select and align technical solutions following enterprise guardrails, executing proof of concepts (POCs) where applicable.\nOversee enterprise solutions for various data technology patterns and platforms, collaborating with senior business stakeholders, functional analysts, and data scientists to deliver robust data solutions aligned with quality measures.\nSupport continuous integration and continuous delivery, maintaining architectural runways for products within a Value Chain, and implement data governance frameworks and tools to ensure data quality, privacy, and compliance.\nDevelop and support advanced data solutions and tools, leveraging advanced data visualization tools like Power BI to enhance data insights, and manage data sourcing and consumption integration patterns from Eaton's data platform, Snowflake.\nAccountable for end-to-end delivery of source data acquisition, complex transformation and orchestration pipelines, and front-end visualization.\nStrong communication and presentation skills, leading collaboration with business stakeholders to deliver rapid, incremental business value/outcomes.\nLead and participate in the planning, definition, development, and high-level design of solutions and architectural alternatives.\nParticipate in solution planning, incremental planning, product demos, and inspect and adapt events.\nPlan and develop the architectural runway for products that support desired business outcomes.\nProvide technical oversight and encourage security, quality, and automation.\nSupport the team with a techno-functional approach as needed.\n\nQualifications\n\n\nBE in Computer Science, Electrical, Electronics/ Any other equivalent Degree\nEducation level required: 10 years\nExperience or knowledge of Snowflake, including administration/architecture.\nExpertise in complex SQL, Python scripting, and performance tuning.\nUnderstanding of Snowflake data engineering practices and dimensional modeling for performance and scalability.\nExperience with data security, access controls, and setting up security frameworks and governance (e.g., SOX).\n\nTechnical Skills\n\nAdvanced SQL skills for building queries and resource monitors in Snowflake.\nProficiency in automating Snowflake admin tasks and handling concepts like RBAC controls, virtual warehouses, resource monitors, SQL performance tuning, zero-copy clone, and time travel.\nExperience in re-clustering data in Snowflake and understanding micro-partitions.\nExcellent analysis, documentation, communication, presentation, and interpersonal skills.\nAbility to work under pressure, meet deadlines, and manage, mentor, and coach a team of analysts.\nStrong analytical skills for complex problem-solving and understanding business problems.\nExperience in data engineering, data visualization, and creating interactive analytics solutions using Power BI and Python.\nExtensive experience with cloud platforms like Azure and cloud-based data storage and processing technologies.\nExpertise in dimensional and transactional data modeling using OLTP, OLAP, NoSQL, and Big Data technologies.\nFamiliarity with data frameworks and storage platforms like Cloudera, Databricks, Dataiku, Snowflake, dbt, Coalesce, and data mesh.\nExperience developing and supporting data pipelines, including code, orchestration, quality, and observability.\nExpert-level programming ability in multiple data manipulation languages (Python, Spark, SQL, PL-SQL).\nIntermediate experience with DevOps, CI/CD principles, and tools, including Azure Data Factory.\nExperience with data governance frameworks and tools to ensure data quality, privacy, and compliance.\nSolid understanding of cybersecurity concepts such as encryption, hashing, and certificates.\nStrong analytical skills to evaluate data, reconcile conflicts, and abstract information.\nContinual learning of new modules, ETL tools, and programming techniques.\nAwareness of new technologies relevant to the environment.\nEstablished as a key data leader at the enterprise level.\n\n]]>","Cloud Platforms, snowflake, Data Mesh, data engineering, Etl Tools, Power Bi, Dimensional Modeling, Sql, Devops, Data Visualization, Data Governance, Azure, Python"
Sr. Data Migration Architect,Birlasoft Limited,10-15 Years,,"Hyderabad, Bengaluru, Mumbai",Software,"1.About the Job - The candidate should require mandatory Sr. Data Migration Architect Experience.\n2.Job Title Sr. Data Migration Architect\n3.Location- Noida, Mumbai, Pune, Bangalore, Hyderabad, Chennai\n4.Educational Background - UG. -B. Tech /B. E in any specialization\nPG. -MCA/MSC/MTech in Computers\n5.Key Responsibilities -\nHands-on experience as Data Migration Architect with PLM Enterprise Systems, mainly Windchill and Thing Worx.\nHas strong experience as a Solution Architect as well for Windchill and Thing Worx applications.\nStrong Experience in UDI and other Medical Devices aspects.\nStrong Experience in Medical Devices Industry with deep insights into regulatory, change control, compliances, and other nuggets.\nHas done windchill migrations on cloud as a target system.\nWhile being hands-on for Migration on Windchill PLM, the person also can help with data migration strategies; also show and tell the customer on best practices and strategies.\nStrong hands-on with data migration and has handled large business transformation programs.\nHas worked directly with onsite and offshore teams from execution standpoint.\nShould be expert in Windchill Migration using Windchill Bulk Migrator (WBM) - at least have executed 3-4 Windchill migration project using WBM.\nShould be expert in WBM tool execution (Extraction, Transformation & Loading).\n6.Skills Required-\nExperience in data migration including CAD Data migration.\nExperience in at least one non-Windchill to Windchill data migration.\nShould have good understanding of Windchill Architecture, database etc.\nShould have good understanding of Windchill object models, relationships, content.\nShould have experience on working with Customer for Migration Requirements Gathering, Source Data Analysis and Data Mapping.\nScripting Knowledge on Database - Oracle/SQL Server with large data set analysis.","windchill 9.1, Windchill, Data Migration"
Data Engineering Architect,Cybage Software Private Limited,10-15 Years,,Pune,Software,"This role involves leveraging advanced data technologies to drive business outcomes in the media and advertising sector, focusing on scalability and performance optimization. Successful candidates will be adept at collaborating across teams to implement comprehensive data strategies and ensure robust, secure data management.\nTechnical and Professional Requirements\nCandidate should have good experience into the following tech stack:\nBig Query / Big Data / Hadoop / Snowflake\nProgramming / Scripting Languages like Python / Java / Go\nHands on experience in defining and implementing various Machine Learning models for different business needs.\nHands on experience in handling various performance and data scalability problems.\nKnowledge in Advertising Domain and varied experience in working on different data sets such as Campaign Performance, Ad Performance, Attribution, Audience Data etc.\nJob Responsibilities:\nDesigning and implementing an overall data strategy as per business requirements. The strategy includes data model designs, database development standards, implementation and management of data warehouses and data analytics systems.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Big Query, Big Data, Hadoop, Snowflake.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.","Data Engineering Architect, Hadoop, Python, Java"
Data Engineering Architect (ATC),Virtusa,10-13 Years,,Hyderabad,IT Management,"Job description\nExperience 10 years of experience in data engineering, ETL processes, and cloud based data solutions.\nAWS Expertise\nHands-on experience with Amazon S3 for data storage, management, and retrieval.\nProficient in AWS Athena for serverless querying and data analysis.\nExperience with AWS Glue for ETL jobs and data cataloging.\nFamiliarity with AWS Data Pipeline or similar orchestration tools.\nProgramming: Proficiency in SQL, Python, or other data engineering programming languages.\nData Engineering Tools: Familiarity with tools like Apache Spark, Apache Airflow, or similar data orchestration and processing frameworks.\nDatabase Management: Experience with relational databases (MySQL, PostgreSQL) and NoSQL databases DynamoDB, Redshift.\nBig Data Experience with large-scale data processing and optimization for big data workloads.\nCloud Technologies: Knowledge of cloud infrastructure, networking, and security best practices.\nAnalytical Skills\nStrong problem-solving skills and the ability to work with complex datasets.\nCommunication: Excellent communication skills and ability to work collaboratively across teams.\nDesign, build, and maintain scalable and efficient data pipelines using AWS technologies (S3, Athena, Glue, Lambda, etc.\nArchitect and optimize data storage solutions on Amazon S3, ensuring cost effectiveness and efficient retrieval of large datasets.\nDevelop automated data workflows using AWS Data Pipeline or similar orchestration tools for seamless data movement and processing.\nMonitor and troubleshoot data pipeline performance, ensuring high availability, reliability, and security\nStay up to date with emerging technologies and trends in cloud data engineering and AWS services.","Analytics - Kinesis, S3, Aws Lambda, Platform, Pyspark, Apache Kafka, Redshift, Python"
Data Engineering Architect (ATC),Virtusa,10-12 Years,,Hyderabad,IT Management,"Job description\nExperience 10 years of experience in data engineering, ETL processes, and cloud based data solutions.\nAWS Expertise\nHands-on experience with Amazon S3 for data storage, management, and retrieval.\nProficient in AWS Athena for serverless querying and data analysis.\nExperience with AWS Glue for ETL jobs and data cataloging.\nFamiliarity with AWS Data Pipeline or similar orchestration tools.\nProgramming: Proficiency in SQL, Python, or other data engineering programming languages.\nData Engineering Tools: Familiarity with tools like Apache Spark, Apache Airflow, or similar data orchestration and processing frameworks.\nDatabase Management: Experience with relational databases (MySQL, PostgreSQL) and NoSQL databases DynamoDB, Redshift.\nBig Data Experience with large-scale data processing and optimization for big data workloads.\nCloud Technologies: Knowledge of cloud infrastructure, networking, and security best practices.\nAnalytical Skills\nStrong problem-solving skills and the ability to work with complex datasets.\nCommunication: Excellent communication skills and ability to work collaboratively across teams.\nDesign, build, and maintain scalable and efficient data pipelines using AWS technologies (S3, Athena, Glue, Lambda, etc.\nArchitect and optimize data storage solutions on Amazon S3, ensuring cost effectiveness and efficient retrieval of large datasets.\nDevelop automated data workflows using AWS Data Pipeline or similar orchestration tools for seamless data movement and processing.\nMonitor and troubleshoot data pipeline performance, ensuring high availability, reliability, and security\nStay up to date with emerging technologies and trends in cloud data engineering and AWS services.","Analytics - Kinesis, S3, Aws Lambda, Platform, Pyspark, Apache Kafka, Redshift, Python"
Data Engineering Architect (ATC),Virtusa,10-13 Years,,Hyderabad,IT Management,"Job description\nExperience 10 years of experience in data engineering, ETL processes, and cloud based data solutions.\nAWS Expertise\nHands-on experience with Amazon S3 for data storage, management, and retrieval.\nProficient in AWS Athena for serverless querying and data analysis.\nExperience with AWS Glue for ETL jobs and data cataloging.\nFamiliarity with AWS Data Pipeline or similar orchestration tools.\nProgramming: Proficiency in SQL, Python, or other data engineering programming languages.\nData Engineering Tools: Familiarity with tools like Apache Spark, Apache Airflow, or similar data orchestration and processing frameworks.\nDatabase Management: Experience with relational databases (MySQL, PostgreSQL) and NoSQL databases DynamoDB, Redshift.\nBig Data Experience with large-scale data processing and optimization for big data workloads.\nCloud Technologies: Knowledge of cloud infrastructure, networking, and security best practices.\nAnalytical Skills\nStrong problem-solving skills and the ability to work with complex datasets.\nCommunication: Excellent communication skills and ability to work collaboratively across teams.\nDesign, build, and maintain scalable and efficient data pipelines using AWS technologies (S3, Athena, Glue, Lambda, etc.\nArchitect and optimize data storage solutions on Amazon S3, ensuring cost effectiveness and efficient retrieval of large datasets.\nDevelop automated data workflows using AWS Data Pipeline or similar orchestration tools for seamless data movement and processing.\nMonitor and troubleshoot data pipeline performance, ensuring high availability, reliability, and security\nStay up to date with emerging technologies and trends in cloud data engineering and AWS services.","Analytics - Kinesis, S3, Aws Lambda, Platform, Pyspark, Apache Kafka, Redshift, Python"
Sr. Data Migration Architect,Birlasoft Limited,10-15 Years,,"Hyderabad, Bengaluru, Mumbai",Software,"1.About the Job - The candidate should require mandatory Sr. Data Migration Architect Experience.\n2.Job Title Sr. Data Migration Architect\n3.Location- Noida, Mumbai, Pune, Bangalore, Hyderabad, Chennai\n4.Educational Background - UG. -B. Tech /B. E in any specialization\nPG. -MCA/MSC/MTech in Computers\n5.Key Responsibilities -\nHands-on experience as Data Migration Architect with PLM Enterprise Systems, mainly Windchill and Thing Worx.\nHas strong experience as a Solution Architect as well for Windchill and Thing Worx applications.\nStrong Experience in UDI and other Medical Devices aspects.\nStrong Experience in Medical Devices Industry with deep insights into regulatory, change control, compliances, and other nuggets.\nHas done windchill migrations on cloud as a target system.\nWhile being hands-on for Migration on Windchill PLM, the person also can help with data migration strategies; also show and tell the customer on best practices and strategies.\nStrong hands-on with data migration and has handled large business transformation programs.\nHas worked directly with onsite and offshore teams from execution standpoint.\nShould be expert in Windchill Migration using Windchill Bulk Migrator (WBM) - at least have executed 3-4 Windchill migration project using WBM.\nShould be expert in WBM tool execution (Extraction, Transformation & Loading).\n6.Skills Required-\nExperience in data migration including CAD Data migration.\nExperience in at least one non-Windchill to Windchill data migration.\nShould have good understanding of Windchill Architecture, database etc.\nShould have good understanding of Windchill object models, relationships, content.\nShould have experience on working with Customer for Migration Requirements Gathering, Source Data Analysis and Data Mapping.\nScripting Knowledge on Database - Oracle/SQL Server with large data set analysis.","windchill 9.1, Windchill, Data Migration"
Data Platform Architect,DIAGEO India,2-6 Years,,Bengaluru,Food and Beverage,Preferred\nExperience in Databricks Lakehouse architecture with experience using Azure Databricks.\nExperience working on File formats (Parquet/ORC/AVRO/Delta/Hudi etc.)\nExposure to using CI/CD tools like Azure DevOps\nExperience and knowledge on Azure data offerings,"Delta, orc, platform architecture, Azure Databricks, Avro, Azure Devops"
EY - GDS Consulting - AI - DATA - GCP Architect - Manager,Ernst and young LLP,8-13 Years,,Bengaluru,Consulting,"At EY, you'll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture, and technology to become the best version of you. And we're counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.\nEY GDS Data and Analytics (D&A) GCP Data Engineer - Manager\nWe're looking for candidates with strong technology and data understanding in the data engineering space, having proven delivery capability. This is a fantastic opportunity to be part of a leading firm as well as a part of a growing Data and Analytics team.\nThe ideal candidate will have deep expertise in cloud architecture, security, networking, and automation, with hands-on experience in large-scale migrations.\nYour key responsibilities\nCloud Strategy & Architecture:\nDesign and implement scalable, secure, and cost-effective GCP architectures for a multi-cloud environment.\nMigration Planning:\nDevelop and execute migration strategies for applications, data, and infrastructure from on-premise or other cloud platforms (AWS/Azure) to GCP.\nInfrastructure as Code (IaC):\nUtilize Terraform, Ansible, or other IaC tools for automated provisioning and management of cloud resources.\nSecurity & Compliance:\nEnsure cloud environments adhere to industry security best practices, compliance standards (e.g., ISO, SOC, HIPAA), and Google Cloud security frameworks.\nCI/CD & DevOps Integration:\nWork with DevOps teams to integrate CI/CD pipelines using Azure DevOps, GitHub Actions, or Jenkins for cloud deployments.\nNetworking & Hybrid Cloud:\nDesign and implement hybrid and multi-cloud networking solutions, including VPNs, interconnects, and service mesh (Anthos, Istio).\nPerformance & Cost Optimization:\nMonitor, optimize, and provide recommendations for cloud resource utilization, cost efficiency, and performance enhancements.\nStakeholder Collaboration:\nWork closely with business, security, and engineering teams to align cloud solutions with organizational goals.\nIncident Management & Troubleshooting:\nProvide technical leadership for incident resolution, root cause analysis, and continuous improvement in cloud operations.\nSkills and attributes for success\n8 to 13 years of Hands-on experience in the field of data warehousing, ETL.\nStrong hands-on experience with GCP services (Compute Engine, GKE, Cloud Functions, Big Query, IAM, Cloud Armor, etc.).\nFamiliarity with AWS and/or Azure services and cross-cloud integrations.\nProficiency in Terraform, Ansible, or other IaC tools.\nExperience with containerization (Docker, Kubernetes) and microservices architecture.\nStrong networking skills, including VPC design, Cloud Interconnect, and hybrid cloud solutions.\nUnderstanding of security best practices, encryption, and identity management in a multi-cloud setup.\nExperience in Migration from on-prem to GCP or hybrid cloud architectures.\nExperience with Anthos, Istio, or service mesh technologies.\nStrong scripting skills in Python, Bash, or Go for automation.\nTo qualify for the role, you must have\nBe a computer science graduate or equivalent with 8 to 13 years of industry experience.\nHave working experience in an Agile-based delivery methodology (Preferable).\nFlexible and proactive/self-motivated working style with strong personal ownership of problem resolution.\nGood analytical skills and enjoys solving complex technical problems.\nProficiency in Software Development Best Practices.\nGood debugging and optimization skills.\nGood communicator (written and verbal formal and informal).\nClient management skills are good to have.\nWhat working at EY offers\nAt EY, we're dedicated to helping our clients, from start-ups to Fortune 500 companies and the work we do with them is as varied as they are.\nYou get to work with inspiring and meaningful projects. Our focus is education and coaching alongside practical experience to ensure your personal development. We value our employees and you will be able to control your own development with an individual progression plan. You will quickly grow into a responsible role with challenging and stimulating assignments. Moreover, you will be part of an interdisciplinary environment that emphasizes high quality and knowledge exchange. Plus, we offer:\nSupport, coaching, and feedback from some of the most engaging colleagues around.\nOpportunities to develop new skills and progress your career.\nThe freedom and flexibility to handle your role in a way that's right for you.\nEY | Building a better working world\nEY exists to build a better working world, helping to create long-term value for clients, people, and society and build trust in the capital markets.\nEnabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform, and operate.\nWorking across assurance, consulting, law, strategy, tax, and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.","Infrastructure as Code (IaC), Cloud Strategy & Architecture, Cloud"
GDS Consulting - AI and DATA - Azure Architect - Manager,Ernst and young LLP,10-12 Years,,Cochin / Kochi / Ernakulam,Consulting,"EY GDS Data and Analytics (D&A) Cloud Architect\nAs part of our EY-GDS D&A (Data and Analytics) team, we help our clients solve complex business challenges with the help of data and technology. We dive deep into data to extract the greatest value and discover opportunities in key business and functions like Banking, Insurance, Manufacturing, Healthcare, Retail, Manufacturing and Auto, Supply Chain, and Finance.\nThe opportunity\nWe're looking for Managers (GTM +Cloud/ Big Data Architects) with strong technology and data understanding having proven delivery capability in delivery and pre sales. This is a fantastic opportunity to be part of a leading firm as well as a part of a growing Data and Analytics team.\nYour key responsibilities\nHave proven experience in driving Analytics GTM/Pre-Sales by collaborating with senior stakeholder/s in the client and partner organization in BCM, WAM, Insurance. Activities will include pipeline building, RFP responses, creating new solutions and offerings, conducting workshops as well as managing in flight projects focused on cloud and big data.\nNeed to work with client in converting business problems/challenges to technical solutions considering security, performance, scalability etc. [ 10- 15 years]\nNeed to understand current & Future state enterprise architecture.\nNeed to contribute in various technical streams during implementation of the project.\nProvide product and design level technical best practices\nInteract with senior client technology leaders, understand their business goals, create, architect, propose, develop and deliver technology solutions\nDefine and develop client specific best practices around data management within a Hadoop environment or cloud environment\nRecommend design alternatives for data ingestion, processing and provisioning layers\nDesign and develop data ingestion programs to process large data sets in Batch mode using HIVE, Pig and Sqoop, Spark\nDevelop data ingestion programs to ingest real-time data from LIVE sources using Apache Kafka, Spark Streaming and related technologies\nSkills and attributes for success\nArchitect in designing highly scalable solutions Azure, AWS and GCP.\nStrong understanding & familiarity with all Azure/AWS/GCP /Bigdata Ecosystem components\nStrong understanding of underlying Azure/AWS/GCP Architectural concepts and distributed computing paradigms\nHands-on programming experience in Apache Spark using Python/Scala and Spark Streaming\nHands on experience with major components like cloud ETLs,Spark, Databricks\nExperience working with NoSQL in at least one of the data stores - HBase, Cassandra, MongoDB\nKnowledge of Spark and Kafka integration with multiple Spark jobs to consume messages from multiple Kafka partitions\nSolid understanding of ETL methodologies in a multi-tiered stack, integrating with Big Data systems like Cloudera and Databricks.\nStrong understanding of underlying Hadoop Architectural concepts and distributed computing paradigms\nExperience working with NoSQL in at least one of the data stores - HBase, Cassandra, MongoDB\nGood knowledge in apache Kafka & Apache Flume\nExperience in Enterprise grade solution implementations.\nExperience in performance bench marking enterprise applications\nExperience in Data security [on the move, at rest]\nStrong UNIX operating system concepts and shell scripting knowledge\nTo qualify for the role, you must have\nFlexible and proactive/self-motivated working style with strong personal ownership of problem resolution.\nExcellent communicator (written and verbal formal and informal).\nAbility to multi-task under pressure and work independently with minimal supervision.\nStrong verbal and written communication skills.\nMust be a team player and enjoy working in a cooperative and collaborative team environment.\nAdaptable to new technologies and standards.\nParticipate in all aspects of Big Data solution delivery life cycle including analysis, design, development, testing, production deployment, and support\nResponsible for the evaluation of technical risks and map out mitigation strategies\nExperience in Data security[on the move, at rest]\nExperience in performance bench marking enterprise applications\nWorking knowledge in any of the cloud platform, AWS or Azure or GCP\nExcellent business communication, Consulting, Quality process skills\nExcellent Consulting Skills\nExcellence in leading Solution Architecture, Design, Buildand Execute for leading clients in Banking, Wealth Asset Management, or Insurance domain.\nMinimum 7 years hand-on experience in one or more of the above areas.\nMinimum 10 years industry experience\nIdeally, you'll also have\nStrong project management skills\nClient management skills\nSolutioning skills\nWhat we look for\nPeople with technical experience and enthusiasm to learn new things in this fast-moving environment","Python/Scala, Hadoop architecture, Spark Streaming, Apache Spark, Apache Kafka"
Architect - Data Gov. & Stewardship,Pepsico india,4-7 Years,,Hyderabad,Food and Beverage,"We are seeking a Data Governance & Stewardship Business Analysts to join our growing team. In this role, you will be responsible for creating, developing, and implementing training content and frameworks to support data governance and stewardship initiatives. This individual will play a key role in introducing new technologies, managing the training playbook, and maintaining a centralized training repository. You will also help develop a consistent and effective training delivery model that empowers teams with the knowledge and tools necessary for successful data governance practices.\nResponsibilities\nDevelop comprehensive training content covering all aspects of data governance, data stewardship, and relevant Informatica technologies.,Design engaging and interactive materials, including presentations, e-learning modules, handbooks, and guides, that cater to different levels of users (e.g., technical and non-technical).,Create content tailored to various aspects of data governance, including data quality management, data stewardship processes, metadata management, and compliance.,Continuously update the training playbook to reflect new content, processes, tools, and user feedback.,Develop and manage a centralized training playbook that provides a structured approach to data governance training, including best practices, workflows, and step-by-step guides.,Work closely with business and technical stakeholders to identify knowledge gaps and training needs related to data governance and stewardship.\nQualifications\nUndergraduate in Management Information System, Data Analytics, or Ai DG Certifications","Mis, Architect, Stewardship, Data Analytics, Data Governance"
MDM Architect- Data Governance,Fractal Analytics,8-11 Years,,"Noida, Mumbai, Pune",Consulting,"Responsibilities\nYou will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management\nLead the end-to-end design, architecture, and implementation of MDM solutions\nDefine and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.\nCollaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.\nDesign data models and hierarchies for Customer, Product, Vendor, and other master domains.\nDevelop and operationalize Data Governance frameworks aligned to business and compliance needs.\nEnable data stewardship workflows, match & merge rules, and exception management.\nIntegrate MDM systems with upstream and downstream applications across the enterprise.\nLead workshops and training sessions on MDM and Data Governance for client teams.\nSupport RFPs, proposals, and client presentations with MDM/DG expertise\nAssess, validate and implement MDM architecture foron-prem, cloud and hybridenvironments with expertise in MDM solutions likeOracle MDM solutions (ERP, CDH, CDM),SAP MDG(S4/Hana, ERP), Informatica IDMCand other modern MDM solutions.\nDevelop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.\nCollaborate with stakeholders to define and implementMDM strategies, standards, and operating models.\nDevelop and enforceend-to-end master data lifecycle processesincluding data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.\nCollaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.\nSupport leadership in designing thought leadership, publish POV s/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.\nGood to Have\nHands-on implementation experience with on-premMDMandERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG,SAP S4/HANA, Reltio, Stiboor hybrid deployments with other modern MDM, ERP and CRM platforms.\nStrong knowledge ofreference data managementanddata stewardship workflows.\nExperience withETL/ELT toolsand integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).\nFamiliarity withdata governance tools(e.g., Collibra, Alation, Informatica CDGC, etc.) andDQ tools(e.g., Informatica IDQ, CDQ, etc.).\nBasic understanding of data governance best practices, data quality management, data privacy regulations\nFamiliarity with Agentic AI, machine learning, and analytics technologies\nKnowledge of SQL, PL/SQl, Python, or any other database\nExcellent communication and interpersonal skillsto collaborate effectively with clients and internal teams.","CDQ, Alation, S4hana, Informatica Idq, Collibra, PL/SQl, Sql, Python"
MDM Architect- Data Governance,Fractal Analytics,8-11 Years,,"Gurugram, Bengaluru, Chennai",Consulting,"Responsibilities\nYou will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management\nLead the end-to-end design, architecture, and implementation of MDM solutions\nDefine and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.\nCollaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.\nDesign data models and hierarchies for Customer, Product, Vendor, and other master domains.\nDevelop and operationalize Data Governance frameworks aligned to business and compliance needs.\nEnable data stewardship workflows, match & merge rules, and exception management.\nIntegrate MDM systems with upstream and downstream applications across the enterprise.\nLead workshops and training sessions on MDM and Data Governance for client teams.\nSupport RFPs, proposals, and client presentations with MDM/DG expertise\nAssess, validate and implement MDM architecture foron-prem, cloud and hybridenvironments with expertise in MDM solutions likeOracle MDM solutions (ERP, CDH, CDM),SAP MDG(S4/Hana, ERP), Informatica IDMCand other modern MDM solutions.\nDevelop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.\nCollaborate with stakeholders to define and implementMDM strategies, standards, and operating models.\nDevelop and enforceend-to-end master data lifecycle processesincluding data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.\nCollaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.\nSupport leadership in designing thought leadership, publish POV s/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.\nGood to Have\nHands-on implementation experience with on-premMDMandERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG,SAP S4/HANA, Reltio, Stiboor hybrid deployments with other modern MDM, ERP and CRM platforms.\nStrong knowledge ofreference data managementanddata stewardship workflows.\nExperience withETL/ELT toolsand integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).\nFamiliarity withdata governance tools(e.g., Collibra, Alation, Informatica CDGC, etc.) andDQ tools(e.g., Informatica IDQ, CDQ, etc.).\nBasic understanding of data governance best practices, data quality management, data privacy regulations\nFamiliarity with Agentic AI, machine learning, and analytics technologies\nKnowledge of SQL, PL/SQl, Python, or any other database\nExcellent communication and interpersonal skillsto collaborate effectively with clients and internal teams.","CDQ, Alation, S4hana, Informatica Idq, Collibra, PL/SQl, Sql, Python"
Architect - Data Engineer,Pepsico india,10-15 Years,,Hyderabad,Food and Beverage,"Responsibilities:Qualifications\nLead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.\nActively contribute to code development in projects and services.\nManage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\nDevelop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\nImplement best practices around systems integration, security, performance, and data management.\nCollaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\nDevelop and optimize procedures to transition data into production.\nDefine and manage SLAs for data products and operational processes.\nPrototype and build scalable solutions for data engineering and analytics.\nResearch and apply state-of-the-art methodologies in data and Platform engineering.\nCreate and maintain technical documentation for knowledge sharing.\nDevelop reusable packages and libraries to enhance development efficiency.\nQualifications:\nBachelor's degree in Computer Science, MIS, Business Management, or related field\n10 + years experience in Information Technology\n4 + years of Azure, AWS and Cloud technologies\nExperience in data platform engineering, with a focus on cloud transformation and modernization.\nStrong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).\nProficiency in SQL, Python, and Spark for data engineering tasks.\nHands-on experience building and scaling data pipelines in cloud environments.\nExperience with CI/CD pipeline management in Azure DevOps (ADO).\nUnderstanding of data governance, security, and compliance best practices.\nExperience working in an Agile development environment.\nPrior experience in migrating applications from legacy platforms to the cloud.\nKnowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\nFamiliarity with Kafka, Event Hubs, or other real-time data streaming solutions.\nExperience with lagacy RDBMS (Oracl, DB2, Teradata)\nBackground in supporting data science models in production.","Sql, Python, Spark, Databricks, Azure Devops, Data Architect, Kafka"
Network Architect - Data Center,Mind Pool Technologies,10-17 Years,INR 63.5 - 70 LPA,United Arab Emirates,Login to check your skill match score,"Expertise in Datacenter network design and implementation on SDN technologies e.g. Cisco ACI and NSX solutions\nExperience in designing integrated SDN solution to integrate with orchestration tools\nExpertise in planning and designing DC transformation and migration methods\nDesign, Solution, Build and Deployment Experience on below Advanced technologies\nHierarchical/Extended Datacenter networks\nDR solutions (Logical DC networks)\nCisco ACI\nVmware NSX\nLoad balancing (F5, Citrix NetScaler, Cisco ACE)\nWan optimization (Riverbed and Cisco)\nDNS/DHCP/IPAM (InfoBlock)\nProxy solutions\nExpertise in preparing high quality HLD and LLD design documents",Cisco Aci
"Architect , Data Science [GEN AI, LLM, NLP, Conversational AI]",Birdeye,Fresher,,India,Login to check your skill match score,"Birdeye is looking for a highly experienced Data Science Architect to design, build, and scale our AI/ML infrastructure. This high-impact, high-visibility role will be at the forefront of architecting next-generation AI solutions, optimizing ML pipelines, and deploying state-of-the-art models into production.\nYou will work closely with data scientists, ML engineers, and software developers to bridge the gap between AI research and real-world applications. If you are passionate about MLOps, cloud-native AI architectures, and building scalable machine learning solutions, this role is for you!\nKey Responsibilities\nAI/ML Architecture & Infrastructure Development\nDesign and build highly scalable, production-ready ML architectures for real-time and batch AI processing.\nDevelop end-to-end ML workflows, including data ingestion, feature engineering, model training, deployment, and monitoring.\nArchitect and optimize distributed AI computing pipelines for large-scale applications.\nImplement fault-tolerant, high-availability ML infrastructure for mission-critical systems.\nML Model Deployment & Lifecycle Management\nDevelop and maintain CI/CD pipelines for ML models, ensuring automated deployment, version control, and rollback mechanisms.\nImplement model monitoring, logging, and drift detection to maintain performance in production.\nAutomate model retraining, hyperparameter tuning, and A/B testing using scalable MLOps frameworks.\nEnsure secure AI deployments, enforcing role-based access control, encryption, and compliance.\nData Engineering & Processing\nDesign and maintain scalable data pipelines for AI models, ensuring efficient ETL and real-time data streaming.\nWork with big data frameworks (Apache Spark, Hadoop, Kafka) to process petabyte-scale datasets.\nImplement feature stores and data versioning for reproducible AI experiments.\nPerformance Optimization & Reliability\nContinuously optimize AI/ML infrastructure for latency, scalability, and cost-efficiency.\nImplement observability & monitoring using Prometheus, Grafana, ELK Stack, and other tools.\nDevelop automated failover & self-healing mechanisms to ensure ML system reliability.\nCollaboration & Leadership\nWork cross-functionally with data scientists, software engineers, and DevOps teams to streamline AI model deployment.\nAdvocate MLOps best practices, enabling teams to rapidly prototype and deploy AI solutions.\nMentor junior engineers and data scientists on AI engineering and operationalization.\nStay ahead of AI/ML industry trends, evaluating new technologies to enhance AI/ML capabilities at Birdeye.","Model monitoring, Big data frameworks, Drift detection, Real-time data streaming, Performance optimization, Observability monitoring, Cloud-native AI architectures, Scalable machine learning solutions, CI CD pipelines, AI ML Architecture, Data pipelines, Data versioning, Hyperparameter tuning, Feature stores, Hadoop, Prometheus, Apache Spark, Kafka, Grafana, Elk Stack, MLops, Etl, Logging"
Solution Architect Data Engineering,Techs to Suit Inc,14-17 Years,,"Hyderabad, India",Login to check your skill match score,"Job Location - Hyderabad, Gurgaon, Ahmedabad, Indore\nExp- 14 to 17 Years\nJoining Time- Max 30 days\nWork from Office, All Days\nMax Salary 45 Lakhs Fixed\nJob Summary:\nAs a Solution Architect, you will collaborate with our sales, presales and COE teams to provide technical expertise and support throughout the new business acquisition process. You will play a crucial role in understanding customer requirements, presenting our solutions, and demonstrating the value of our products.\nYou thrive in high-pressure environments, maintaining a positive outlook and understanding that career growth is a journey that requires making strategic choices. You possess good communication skills, both written and verbal, enabling you to convey complex technical concepts clearly and effectively. You are a team player, customer-focused, self-motivated, responsible individual who can work under pressure with a positive attitude. You must have experience in managing and handling RFPs/ RFIs, client demos and presentations, and converting opportunities into winning bids. You possess a strong work ethic, positive attitude, and enthusiasm to embrace new challenges. You can multi-task and prioritize (good time management skills), willing to display and learn. You should be able to work independently with less or no supervision. You should be process-oriented, have a methodical approach and demonstrate a quality-first approach.\nAbility to convert client's business challenges/ priorities into winning proposal/ bid through excellence in technical solution will be the key performance indicator for this role.\nWhat you'll do\nArchitecture & Design: Develop high-level architecture designs for scalable, secure, and robust solutions.\nTechnology Evaluation: Select appropriate technologies, frameworks, and platforms for business needs.\nCloud & Infrastructure: Design cloud-native, hybrid, or on-premises solutions using AWS, Azure, or GCP.\nIntegration: Ensure seamless integration between various enterprise applications, APIs, and third-party services.\nDesign and develop scalable, secure, and performant data architectures on Microsoft Azure and/or new generation analytics platform like MS Fabric.\nTranslate business needs into technical solutions by designing secure, scalable, and performant data architectures on cloud platforms.\nSelect and recommend appropriate Data services (e.g. Fabric, Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, Power BI etc) to meet specific data storage, processing, and analytics needs.\nDevelop and recommend data models that optimize data access and querying. Design and implement data pipelines for efficient data extraction, transformation, and loading (ETL/ELT) processes.\nAbility to understand Conceptual/Logical/Physical Data Modelling.\nChoose and implement appropriate data storage, processing, and analytics services based on specific data needs (e.g., data lakes, data warehouses, data pipelines).\nUnderstand and recommend data governance practices, including data lineage tracking, access control, and data quality monitoring.\nWhat you will Bring\n10+ years of working in data analytics and AI technologies from consulting, implementation and design perspectives\nCertifications in data engineering, analytics, cloud, AI will be a certain advantage\nBachelor's in engineering/ technology or an MCA from a reputed college is a must\nPrior experience of working as a solution architect during presales cycle will be an advantage","Architecture Design, Data Governance"
Enterprise Architect - Data & AI,Emids,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Role\nThe Enterprise Data Architect proactively leads and supports software developers, database architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects\nThis role will be responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams in alignment with architectural governance and standards\nThe strategy includes data model designs, database development standards, implementation and management of data warehouses and data analytics systems\nIdentifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy\nCoordinating and collaborating with cross-functional teams, stakeholders, and vendors for the smooth functioning of the enterprise data system\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution\nAbility to analyze and optimize cloud costs.\nKey Responsibilities\nStrategic Data Leadership: Define and drive the data architecture and AI strategy in alignment with organizational goals, focusing on data-driven decision-making and improving operational efficiency.\nArchitecture Design: Lead the design and implementation of scalable and efficient data architectures that support advanced analytics, reporting, and machine learning applications.\nIntegration Solutions: Oversee the development of data integration strategies, enabling seamless data flow across various healthcare systems, including RCMs, EHRs, Claims, Policy Admin and Billing systems, and analytics platforms.\nData Governance & Compliance: Establish and maintain data governance frameworks, ensuring compliance with regulations such as HIPAA, FedRAMP, and HITRUST while promoting data integrity, security, and quality.\nCollaboration with Stakeholders: Partner with executive leadership, enterprise architects, data scientists, analysts, and IT teams to understand data needs, provide architectural guidance, and ensure successful project execution.\nEmerging Technology Assessment: Evaluate and recommend emerging technologies and tools that can enhance healthcare data architecture and analytics capabilities.\nPerformance Metrics: Establish key performance indicators (KPIs) and metrics to evaluate product success and drive continuous improvement based on user feedback and data analysis.\nDocumentation and Standards: Develop comprehensive documentation of data architecture standards, best practices, and processes for internal teams.\nMentorship and Team Development: Mentor and guide data architects and engineers, fostering a culture of continuous learning and innovation within the data team.\nQualifications\n15+ years of experience in data architecture, with a minimum of 3+ years in the healthcare domain.\nMust have 5+ years of experience in any AWS/Azure/GCP cloud technology.\nExpertise in data engineering frameworks and tools\nMust have 5+ years of experience in Databricks, with the ability to architect, implement, and scale workloads.","data engineering frameworks and tools, AWS, Databricks, Azure, Gcp"
IT Office Business Solutions (Azure Data & AI Solutions Architect) - req32808,The World Bank,7-9 Years,,"Chennai, India",Login to check your skill match score,"IT Office Business Solutions (Azure Data & AI Solutions Architect)\n\nJob #: req32808\n\nOrganization: World Bank\n\nSector: Information Technology\n\nGrade: GF\n\nTerm Duration: 4 years 0 months\n\nRecruitment Type: Local Recruitment\n\nLocation: Chennai,India\n\nRequired Language(s): English\n\nPreferred Language(s)\n\nClosing Date: 5/4/2025 (MM/DD/YYYY) at 11:59pm UTC\n\nDescription\n\nDo you want to build a career that is truly worthwhile Working at the World Bank Group provides a unique opportunity for you to help our clients solve their greatest development challenges. The World Bank Group is one of the largest sources of funding and knowledge for developing countries; a unique global partnership of five institutions dedicated to ending extreme poverty, increasing shared prosperity and promoting sustainable development. With 189 member countries and more than 130 offices worldwide, we work with public and private sector partners, investing in groundbreaking projects and using data, research, and technology to develop solutions to the most urgent global challenges. For more information, visit www.worldbank.org\n\nITS Vice Presidency Context\n\nThe Information and Technology Solutions (ITS) Vice Presidential Unit (VPU) enables the World Bank Group to achieve its mission of ending extreme poverty and boost shared prosperity on a livable planet by delivering transformative information and technologies to its staff working in over 150+ locations. For more information on ITS, see this video:https://www.youtube.com/watchreload=9&v=VTFGffa1Y7w\n\nITS shapes its strategy in response to changing business priorities and leverages new technologies to achieve three high-level business outcomes: business enablement, by providing Bank Group units with innovative digital tools and technologies to transform how they deliver value for their clients; empowerment & effectiveness, by ensuring that all Bank Group staff are connected, able to find information, and productive to accelerate the delivery of development solutions globally; and resilience, by equipping the Bank Group to provide risk-based cybersecurity and robust data protection for a global network and a growing cloud platform. The ITS mission is to leverage information and technology as a force multiplier to accelerate, deepen, and sustain development impact. ITS is on an Agile transformation journey, reshaping all aspects of the operating model to increase and accelerate value creation.\n\nImplementation of the strategy is guided by three core principles. The first is to deliver solutions for business partners that are customer-centric, innovative, and transformative. The second is to provide the Bank Group with value for money with selective and standard technologies. The third principle is to excel at the basics by providing a high performing, robust, and resilient IT environment for the organization.\n\nWBG Finance (ITSFI) is responsible for providing high quality, streamlined information and technology solutions for the World Bank's Financial services, which include Corporate Finance, Risk Management, Controls, Treasury, Loans, Accounting, and Concessional finance (handling donor contributions from inception to the point of final disbursement, including IDA, Financial Intermediary Funds and Trust Funds). ITSFI is additionally responsible for building its IT services using a shared platform that provides scale, leverage, reliability, and control while at the same time improving responsiveness to emerging business needs. The ITSFI team is accountable for the implementation of the ITS Strategy supporting WBG core finance business processes.\n\nAs a unit within the ITSFI department, ITSFT is responsible for providing core accounting solutions for the World Bank Group staff that are in around 185 countries across the world. The major areas include applications in the Accounts Payable, Accounts Receivable, General Ledger and Asset accounting business processes to support the operations of World Bank Group Finance & Accounting, Treasury and Development Finance Vice presidencies.\n\nJob Opportunity\n\nITSFT is looking for an experienced Business Solutions Architect to lead the design and implementation of advanced business solutions on Azure, focusing on application engineering, data modeling, and secure cloud engineering practices. The role demands extensive expertise in the financial domain, particularly in building tools and frameworks for AI and data science applications on Azure, with exclusive experience in developing generative AI applications that meet WBG security, compliance and performance standards.\n\nRoles & Responsibilities\n\nArchitect, design and implement highly scalable, available, and fault tolerant systems in the Cloud (Azure) and on-prem that will tightly integrate with SAP S4 HANA, Data lakes and other enterprise systems.\nLead the technical engagement for defining and documenting application architectures aligned with enterprise standards, guidelines, and best practices.\nEnsure solution architectures for enterprise solutions drive efficiencies, improve performance and reliability, increase innovation, and reduce costs. These solution architectures could be cloud-native, on-premises or leverage a hybrid cloud model.\nEnsure solutions leverage industry standard enterprise integration technologies to integrate disparate systems on-premises, in the cloud and across cloud providers.\nOversight of the contractual resources that support existing financial applications on SAP and cloud.\nBuild, maintain key architectural artifacts and ensure alignment with architectural guidelines, best practices, and blueprints\nLead and/or take part in deep-dive education and design exercises along with technical discussions to create enterprise grade solutions.\nDefine transition approaches and roadmaps for migrating legacy systems\nProvide strategic direction in AI and data architecture, guiding the organization towards innovative and effective solutions.\nDevelop and deliver compelling presentations and demos to achieve buy-in from relevant stakeholders and to communicate the same to implementation teams\nEngage with project teams to provide technical guidance while ensuring alignment with defined architectural blueprints.\nDrive continuous improvement and innovation in AI and data science practices, ensuring the organization remains at the forefront of technological advancements.\n\nSelection Criteria\n\nMaster's degree with 5 years relevant experience or bachelor's degree with a minimum of 7 years of relevant experience\nMinimum 7 years of experience as a Data and AI Architect, with a proven track record in application engineering, data modeling, and AI solutions on Azure.\nProficiency in Azure, Power BI, semantic modeling, and financial reporting. Extensive experience in building tools and frameworks for AI and data science applications.\nStrong background in the financial domain\nGood to have extensive knowledge in SAP, including SAP HANA migration\nDemonstrated ability to lead and manage teams, providing strategic direction and fostering a collaborative environment.\nGood knowledge of information management and information technology including system design, planning, and project delivery management.\nDemonstrated analytical ability and problem-solving skills in translating business requirements into technical requirements/business systems.\nDemonstrated ability to quickly resolve technical problems in complex and time sensitive production systems.\nAbility to express thoughts and ideas effectively in oral and written communications.\nAbility to act sensitively in multicultural environments and build effective working relations with internal and external partners.\nHighest ethical standards.\nMicrosoft Certified Azure Solutions Architect Expert, Microsoft Certified Azure AI Engineer\n\nIn addition to the above mandatory requirements, the following are highly desired-\n\nWBG experience and knowledge of WBG specific custom processes and applications.\nExperience in Dremio and SAP.\n\nWorld Bank Group Core Competencies\n\nDeliver Results for Clients\n\nProactively addresses clients stated and unstated needs : Adds value by constantly looking for a better way to get more impactful results; sets challenging stretch goals for oneself. Immerses oneself in client experiences and perspective by asking probing questions to understand unmet needs. Demonstrates accountability for achieving results that have a development impact and financial, environmental and social sustainability. Identifies and proposes solutions to mitigate and manage risks.\n\nCollaborate Within Teams and Across Boundaries\n\nCollaborates across boundaries, gives own perspective and willingly receives diverse perspectives : Appropriately involves others in decision making and communicates with key stakeholders. Approaches conflicts as common problems to be solved. Actively seeks and considers diverse ideas and approaches displaying a sense of mutuality and respect. Integrates WBG perspective into work\n\nLead and Innovate\n\nDevelops innovative solutions : Contributes new insights to understand situations and develops solutions to resolve complex problems. Adapts as circumstances require and manages impact of own behavior on others in context of WBG's values and mission. Identifies and pursues innovative approaches to resolve issues\n\nCreate, Apply and Share Knowledge\n\nApplies knowledge across WBG to strengthen solutions for internal and/or external clients : Leverages department's expertise and body of knowledge across WBG to strengthen internal and/or external client solutions. Seeks to learn from more experienced staff to deepen or strengthen their professional knowledge and helps others to learn. Builds personal and professional networks inside and outside the department unit\n\nMake Smart Decisions\n\nInterprets a wide range of information and pushes to move forward : Seeks diversity of information and inputs, researches possible solutions, and generates recommended options. Identifies and understands risks and proposes recommendations. Based on risk analysis makes decisions in a timely manner within own area of responsibility, considering the interests and concerns of stakeholders.\n\nWorld Bank Group Core Competencies\n\nThe World Bank Group offers comprehensive benefits, including a retirement plan; medical, life and disability insurance; and paid leave, including parental leave, as well as reasonable accommodations for individuals with disabilities.\n\nWe are proud to be an equal opportunity and inclusive employer with a dedicated and committed workforce, and do not discriminate based on gender, gender identity, religion, race, ethnicity, sexual orientation, or disability.\n\nLearn more about working at the World Bank and IFC , including our values and inspiring stories.","Azure Power BI, Dremio, Application Engineering, AI solutions, Sap S4 Hana, Data Modeling, Azure"
Architect - Data Center,Bechtel Corporation,6-8 Years,,India,Login to check your skill match score,"Requisition ID: 282943\n\nRelocation Authorized: National - Family\nTelework Type: Full-Time Office/Project\nWork Location: Various Bechtel Project Locations\n\nExtraordinary Teams Building Inspiring Projects\n\nSince 1898, we have helped customers complete more than 25,000 projects in 160 countries on all seven continents that have created jobs, grown economies, improved the resiliency of the world's infrastructure, increased access to energy, resources, and vital services, and made the world a safer, cleaner place.\n\nDifferentiated by the quality of our people and our relentless drive to deliver the most successful outcomes, we align our capabilities to our customers objectives to create a lasting positive impact. We serve the Infrastructure; Nuclear, Security & Environmental; Energy; Mining & Metals, and the Manufacturing and Technology markets. Our services span from initial planning and investment, through start-up and operations.\n\nCore to Bechtel is our Vision, Values and Commitments . They are what we believe, what customers can expect, and how we deliver. Learn more about our extraordinary teams building inspiring projects in our Impact Report .\n\nBechtel India is a global operation that supports execution of projects and services around the world. Working seamlessly with business line home offices, project sites, customer organizations and suppliers, our teams have delivered more than 125 projects since our inception in 1994.\n\nOur offices in Gurgaon, Vadodara and Chennai will grow significantly and sustainably with exciting career opportunities for both professionals and young graduates who are passionate about creating a cleaner, greener, and safer world; building transformational infrastructure; making decarbonization a reality; and protecting people and the environment.\n\nJob Summary\n\nArchitect with more than 6 years of experience in Manufacturing and Infrastructure projects (with relevant experience in Data center) projects in a design office of repute. A candidate who has completed at least 1 data center project shall be given more weightage. Should have exposure to design office practices and familiarity with computer aided design and 3D modeling tools like Revit.\n\nMajor Responsibilities\n\nShall perform / supervise calculations, prepare material requisitions, service requisitions, code analysis, standard details, technical specifications and quantity take-offs and provide input for design drawings during all phases of projects.\nShall check / review the drawings prepared by the designers.\nShall assist in the development of basic layout drawings.\nShall lead conceptual studies and inter-disciplinary reviews\nShall create perspectives and presentation of design to client.\nProvide technical training.\nPerform feasibility studies for site development, building configuration, climate studies.\n\nEducation And Experience Requirements\n\nMinimum 5-year degree in Architecture from an accredited college or university.\nCandidate with Master's degree is desirable.\nProfessional license from a recognized licensing board and/or LEED certification shall be of added advantage.\nExperience of making Architectural presentation shall be an added advantage.\nLevel I: 6 - 8 years of relevant work experience\nLevel II: 8 - 10 years of relevant work experience\n\nRequired Knowledge And Skills\n\nKnowledge of architectural techniques and design principles with a basic knowledge of the types of data center, data hall layout, mechanical systems, electrical systems, and security systems needed to design a data center.\nKnowledge of precedents and latest trends in architectural engineering, the principles and practices of related technical areas.\nKnowledge of Engineering Procedures and design guides.\nThorough knowledge of the roles played by other engineering disciplines on projects.\nKnowledge of regulatory Indian and International codes and standards, industry practices and design criteria pertinent to architectural engineering design, including fire life safety codes.\nKnowledge of constructability and applicable standards and codes\nProficiency in the use of Revit, Navisworks and exposure to BIM is essential.\nProficient in selection of material from constructability and total installed cost perspective.\nSkill in oral and written communication.\nShould be proficient in using MS office tools.\nProved ability of managing a team of architects and designers will be an added advantage.\nPrevious experience of Data center design including administration building, process/ non process buildings and other ancillary facilities like maintenance building, guard house, substation, electrical buildings, pump house, etc.\nKnowledge of master planning and fire life safety design guidelines\nGood knowledge of faade design and material selection (including interior & exterior finishes)\nExperience in working on EPC projects shall be an added advantage.\n\nTotal Rewards/Benefits\n\nFor decades, Bechtel has worked to inspire the next generation of employees and beyond! Because our teams face some of the world's toughest challenges, we offer robust benefits to ensure our people thrive. Whether it is advancing careers, delivering programs to enhance our culture, or providing time to recharge, Bechtel has the benefits to build a legacy of sustainable growth. Learn more at Bechtel Total Rewards\n\nDiverse Teams Build The Extraordinary\n\nAs a global company, Bechtel has long been home to a vibrant multitude of nationalities, cultures, ethnicities, and life experiences. This diversity has made us a more trusted partner, more effective problem solvers and innovators, and a more attractive destination for leading talent.\n\nWe are committed to being a company where every colleague feels that they belong-where colleagues feel part of One Team, respected and rewarded for what they bring, supported in pursuing their goals, invested in our values and purpose, and treated equitably. Click here to learn more about the people who power our legacy.\n\nBechtel is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity and expression, age, national origin, disability, citizenship status (except as authorized by law), protected veteran status, genetic information, and any other characteristic protected by federal, state or local law. Applicants with a disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to [HIDDEN TEXT]","Bim, Revit, Computer Aided Design, Ms Office Tools, Navisworks, 3D modeling tools"
Senior Solution Architect - Data Governance Experience-Collibra,Cortex Consultants LLC,7-9 Years,,"Chennai, India",Login to check your skill match score,"Solution Architect\n\nExp-15+\n\nLocation-Multiple\n\nImmediate to 15days\n\nSkill-data gov,colibra,architect\n\nOverview: The Senior Solution Architect - Data Governance Experience-Collibra plays a critical role in shaping our organization's data governance framework. This position requires an individual who has extensive experience with Collibra and a strong understanding of data governance principles. The architect will be pivotal in designing and implementing data governance solutions that enhance data quality, compliance, and usability across the enterprise. By leading initiatives that foster a culture of data stewardship, this role ensures that our data assets are effectively managed and leveraged to drive business outcomes. The Senior Solution Architect will collaborate closely with various stakeholders, including IT, compliance, and business units, to ensure alignment with the organization's strategic objectives. This position not only focuses on technical capabilities but also emphasizes the importance of change management and user adoption to ensure long-term success in our data governance journey.\n\nLead the architecture and implementation of Collibra to support data governance initiatives.\nDevelop a comprehensive data governance framework that aligns with organizational goals.\nEngage with stakeholders to gather requirements and translate them into actionable solutions.\nEnsure compliance with data regulations and industry standards during implementation.\nDesign and maintain data models that enhance data discoverability and usability.\nCollaborate with data stewards and business units to promote a culture of data stewardship.\nConduct assessments of existing data management practices and recommend improvements.\nProvide training and support for users to maximize the effectiveness of Collibra.\nFacilitate workshops and meetings to drive consensus around data governance practices.\nMonitor and report on data governance metrics to measure the success of initiatives.\nStay current with industry trends and advancements in data governance technology.\nSupport change management efforts to drive user adoption of data governance solutions.\nDevelop and implement data stewardship policies and procedures.\nWork closely with IT to ensure seamless integration of Collibra with existing systems.\nServe as a subject matter expert for data governance best practices within the organization.\n\nRequired Qualifications\n\nBachelor's degree in Computer Science, Information Technology, or a related field.\nAt least 7 years of experience in data governance, data management, or a related area.\nProven experience as a Solution Architect, with a focus on data governance solutions.\nExtensive hands-on experience with Collibra, including implementation and configuration.\nStrong understanding of data governance principles, frameworks, and best practices.\nExperience working with data models, metadata management, and data quality assessments.\nKnowledge of regulatory compliance requirements such as GDPR, CCPA, or HIPAA.\nAbility to engage and communicate effectively with stakeholders at all levels.\nStrong analytical and problem-solving skills to address data-related challenges.\nProficient in project management methodologies, with experience leading cross-functional teams.\nExperience in change management processes to facilitate user adoption.\nStrong presentation skills with the ability to convey complex concepts to non-technical audiences.\nCertification in data governance or relevant frameworks is a plus.\nExperience with cloud-based data platforms and integration tools.\nAbility to work in a fast-paced, dynamic environment with competing priorities.\n\nSkills: regulatory compliance,project management,data stewardship,stakeholder engagement,data governance,metadata management,data quality assessments,data management,change management,collibra,data models,analytical thinking,solution architecture","data quality assessments, data models, project management, data stewardship, Regulatory Compliance, Data Management, Metadata Management, Collibra, Data Governance, change management"
Technical Architect - Data Analytics - Vice President,State Street,14-16 Years,,"Bengaluru, India",Login to check your skill match score,"Role Description\n\nWe are looking for an experienced Data Lead to drive our data strategy, architecture, and analytics initiatives. You will be responsible for managing data infrastructure, ensuring data quality, and enabling data-driven decision-making for Digital platform & products. This role requires a strong technical background, leadership skills, and a passion for leveraging data to create business impact.\n\nData Strategy & Governance\n\nDefine and implement data management strategies to align with business objectives.\nEstablish data governance policies and ensure compliance with industry standards (GDPR, CCPA, etc.).\nDrive best practices for data security, privacy, and integrity.\n\nData Engineering & Architecture\n\n\nDesign and oversee scalable data pipelines, ETL processes, and data warehousing solutions.\nWork with engineering teams to optimize database performance and storage.\nEnsure the reliability and efficiency of data infrastructure, both on-premise and in the cloud.\n\nData Analytics & Insights\n\n\nLead data analytics initiatives to provide insights that drive business decisions.\nCollaborate with cross-functional teams to develop dashboards and reports.\nPromote data-driven decision-making culture across the organization.\n\nLeadership & Collaboration\n\n\nManage and mentor a team of data engineers, analysts, and scientists.\nPartner with product, marketing, finance, and operations teams to address data challenges.\nStay updated on the latest trends and technologies in data management and analytics.\n\nCore/Must Have Skills\n\n\nBachelor's or Master's degree in Computer Science, Information Technology, Data Science, or a related field.\n\n14+ years of experience in data engineering, analytics, or a similar role.\n\nStrong expertise in SQL, Python, and data pipeline tools (Airflow, dbt, etc.).\n\nWorking experience on data technologies like Snowflake, DataBricks, Airflow, Kafka, Apache Airflow, Teradata, Hadoop, spark, Amazon redshift, etc\n\nKnowledge of data modeling, data governance, and data integration techniques.\n\nStrong problem-solving skills and ability to work in a fast-paced environment\n\nExcellent leadership, communication, and stakeholder management skills.\n\nGood To Have Skills\n\nExperience with BI tools (Tableau, Power BI, Looker) and ML frameworks is a plus.\n\nWork Schedule\n\nOn-Premise\n\nKeywords (If any)\n\nWhy this role is important to us\n\nOur technology function, Global Technology Services (GTS), is vital to State Street and is the key enabler for our business to deliver data and insights to our clients. We're driving the company's digital transformation and expanding business capabilities using industry best practices and advanced technologies such as cloud, artificial intelligence and robotics process automation.\n\nWe offer a collaborative environment where technology skills and innovation are valued in a global organization. We're looking for top technical talent to join our team and deliver creative technology solutions that help us become an end-to-end, next-generation financial services company.\n\nJoin us if you want to grow your technical skills, solve real problems and make your mark on our industry.\n\nAbout State Street\n\nWhat we do. State Street is one of the largest custodian banks, asset managers and asset intelligence companies in the world. From technology to product innovation, we're making our mark on the financial services industry. For more than two centuries, we've been helping our clients safeguard and steward the investments of millions of people. We provide investment servicing, data & analytics, investment research & trading and investment management to institutional clients.\n\nWork, Live and Grow. We make all efforts to create a great work environment. Our benefits packages are competitive and comprehensive. Details vary by location, but you may expect generous medical care, insurance and savings plans, among other perks. You'll have access to flexible Work Programs to help you match your needs. And our wealth of development programs and educational support will help you reach your full potential.\n\nInclusion, Diversity and Social Responsibility. We truly believe our employees diverse backgrounds, experiences and perspectives are a powerful contributor to creating an inclusive environment where everyone can thrive and reach their maximum potential while adding value to both our organization and our clients. We warmly welcome candidates of diverse origin, background, ability, age, sexual orientation, gender identity and personality. Another fundamental value at State Street is active engagement with our communities around the world, both as a partner and a leader. You will have tools to help balance your professional and personal life, paid volunteer days, matching gift programs and access to employee networks that help you stay connected to what matters to you.\n\nState Street is an equal opportunity and affirmative action employer.\n\nDiscover more at StateStreet.com/careers\n\nState Street's Speak Up Line\n\nJob ID: R-772025","Airflow, data integration techniques, Looker, Teradata, snowflake, dbt, Hadoop, Power Bi, Kafka, Tableau, Data Modeling, Sql, DataBricks, Amazon Redshift, Spark, Data Governance, Python"
Technical Architect - Data Analytics - Vice President,State Street Corporation,14-16 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Role Description\nWe are looking for an experiencedData Leadto drive our data strategy, architecture, and analytics initiatives. You will be responsible for managing data infrastructure, ensuring data quality, and enabling data-driven decision-making for Digital platform & products. This role requires a strong technical background, leadership skills, and a passion for leveraging data to create business impact.\nData Strategy & Governance\nDefine and implement data management strategies to align with business objectives.\nEstablish data governance policies and ensure compliance with industry standards (GDPR, CCPA, etc.).\nDrive best practices for data security, privacy, and integrity.\nData Engineering & Architecture\nDesign and oversee scalable data pipelines, ETL processes, and data warehousing solutions.\nWork with engineering teams to optimize database performance and storage.\nEnsure the reliability and efficiency of data infrastructure, both on-premise and in the cloud.\nData Analytics & Insights\nLead data analytics initiatives to provide insights that drive business decisions.\nCollaborate with cross-functional teams to develop dashboards and reports.\nPromote data-driven decision-making culture across the organization.\nLeadership & Collaboration\nManage and mentor a team of data engineers, analysts, and scientists.\nPartner with product, marketing, finance, and operations teams to address data challenges.\nStay updated on the latest trends and technologies in data management and analytics.\nCore/Must have skills\nBachelor's or Master's degree in Computer Science, Information Technology, Data Science, or a related field.\n14+ years of experience in data engineering, analytics, or a similar role.\nStrong expertise in SQL, Python, and data pipeline tools (Airflow, dbt, etc.).\nWorking experience on data technologies like Snowflake, DataBricks, Airflow, Kafka, Apache Airflow, Teradata, Hadoop, spark, Amazon redshift, etc\nKnowledge of data modeling, data governance, and data integration techniques.\nStrong problem-solving skills and ability to work in a fast-paced environment\nExcellent leadership, communication, and stakeholder management skills.\nGood to have skills\nExperience with BI tools (Tableau, Power BI, Looker) and ML frameworks is a plus.\nWork Schedule\nOn-Premise\nKeywords (If any)\nWhy this role is important to us\nOur technology function, Global Technology Services (GTS), is vital to State Street and is the key enabler for our business to deliver data and insights to our clients. We're driving the company's digital transformation and expanding business capabilities using industry best practices and advanced technologies such as cloud, artificial intelligence and robotics process automation.\nWe offer a collaborative environment where technology skills and innovation are valued in a global organization. We're looking for top technical talent to join our team and deliver creative technology solutions that help us become an end-to-end, next-generation financial services company.\nJoin us if you want to grow your technical skills, solve real problems and make your mark on our industry.\nAbout State Street\nWhat we do. State Street is one of the largest custodian banks, asset managers and asset intelligence companies in the world. From technology to product innovation, we're making our mark on the financial services industry. For more than two centuries, we've been helping our clients safeguard and steward the investments of millions of people. We provide investment servicing, data & analytics, investment research & trading and investment management to institutional clients.\nWork, Live and Grow. We make all efforts to create a great work environment. Our benefits packages are competitive and comprehensive. Details vary by location, but you may expect generous medical care, insurance and savings plans, among other perks. You'll have access to flexible Work Programs to help you match your needs. And our wealth of development programs and educational support will help you reach your full potential.\nInclusion, Diversity and Social Responsibility. We truly believe our employees diverse backgrounds, experiences and perspectives are a powerful contributor to creating an inclusive environment where everyone can thrive and reach their maximum potential while adding value to both our organization and our clients. We warmly welcome candidates of diverse origin, background, ability, age, sexual orientation, gender identity and personality. Another fundamental value at State Street is active engagement with our communities around the world, both as a partner and a leader. You will have tools to help balance your professional and personal life, paid volunteer days, matching gift programs and access to employee networks that help you stay connected to what matters to you.\nState Street is an equal opportunity and affirmative action employer.\nDiscover more at StateStreet.com/careers","snowflake, Airflow, data integration techniques, dbt, Looker, Teradata, Spark, Sql, Apache Airflow, Data Modeling, Data Governance, Hadoop, Tableau, Kafka, Power Bi, Python, Bi Tools, Amazon Redshift, DataBricks"
IT Architect (Data Engineer),Medtronic,4-6 Years,,"Pune, India",Login to check your skill match score,"At Medtronic you can begin a life-long career of exploration and innovation, while helping champion healthcare access and equity for all. You'll lead with purpose, breaking down barriers to innovation in a more connected, compassionate world.\n\nA Day in the Life\n\nWe're a mission-driven leader in medical technology and solutions with a legacy of integrity and innovation, join our new Minimed India Hub as Digital Engineer. We are working to improve how healthcare addresses the needs of more people, in more ways and in more places around the world. As a PySpark Data Engineer, you will be responsible for designing, developing, and maintaining data pipelines using PySpark. You will work closely with data scientists, analysts, and other stakeholders to ensure the efficient processing and analysis of large datasets, while handling complex transformations and aggregations.\n\nResponsibilities may include the following and other duties may be assigned:\n\nDesign, develop, and maintain scalable and efficient ETL pipelines using PySpark.\nWork with structured and unstructured data from various sources.\nOptimize and tune PySpark applications for performance and scalability.\nCollaborate with data scientists and analysts to understand data requirements, review Business Requirement documents and deliver high-quality datasets.\nImplement data quality checks and ensure data integrity.\nMonitor and troubleshoot data pipeline issues and ensure timely resolution.\nDocument technical specifications and maintain comprehensive documentation for data pipelines.\nStay up to date with the latest trends and technologies in big data and distributed computing.\n\nRequired Knowledge and Experience:\n\n\nBachelor's degree in computer science, Engineering, or a related field.\n4-5 years of experience in data engineering, with a focus on PySpark.\nProficiency in Python and Spark, with strong coding and debugging skills.\nStrong knowledge of SQL and experience with relational databases (e.g., PostgreSQL, MySQL, SQL Server).\nHands-on experience with cloud platforms such as AWS, Azure, or Google Cloud Platform (GCP).\nExperience with data warehousing solutions like Redshift, Snowflake, Databricks or Google BigQuery.\nFamiliarity with data lake architectures and data storage solutions.\nExperience with big data technologies such as Hadoop, Hive, and Kafka.\nExcellent problem-solving skills and the ability to troubleshoot complex issues.\nStrong communication and collaboration skills, with the ability to work effectively in a team environment.\n\nPreferred Skills:\n\n\nExperience with Databricks.\nExperience with orchestration tools like Apache Airflow or AWS Step Functions.\nKnowledge of machine learning workflows and experience working with data scientists.\nUnderstanding of data security and governance best practices.\nFamiliarity with streaming data platforms and real-time data processing.\nKnowledge of CI/CD pipelines and version control systems (e.g., Git).\n\nPhysical Job Requirements\n\n\nThe above statements are intended to describe the general nature and level of work being performed by employees assigned to this position, but they are not an exhaustive list of all the required responsibilities and skills of this position.\n\nBenefits & Compensation\n\nMedtronic offers a competitive Salary and flexible Benefits Package\n\nA commitment to our employees lives at the core of our values. We recognize their contributions. They share in the success they help to create. We offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage.\n\nThis position is eligible for a short-term incentive called the Medtronic Incentive Plan (MIP).\n\nAbout Medtronic\n\nWe lead global healthcare technology and boldly attack the most challenging health problems facing humanity by searching out and finding solutions.\n\nOur Mission to alleviate pain, restore health, and extend life unites a global team of 95,000+ passionate people.\n\nWe are engineers at heart putting ambitious ideas to work to generate real solutions for real people. From the R&D lab, to the factory floor, to the conference room, every one of us experiments, creates, builds, improves and solves. We have the talent, diverse perspectives, and guts to engineer the extraordinary.\n\nLearn more about our business, mission, and our commitment to diversity here","snowflake, Google BigQuery, AWS Step Functions, Git, Databricks, Sql, SQL Server, Apache Airflow, Hadoop, Pyspark, Kafka, AWS, MySQL, Hive, Python, Azure, PostgreSQL, Spark, Google Cloud Platform, Redshift"
Solutions Architect -Data/AI,Gruve,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Solutions Architect- AI\nLocation: India- Pune/Bangalore\nAbout the Company:\nGruve is an innovative software services startup dedicated to transforming enterprises to AI powerhouses. We specialize in cybersecurity, customer experience, cloud infrastructure, and advanced technologies such as Large Language Models (LLMs). Our mission is to assist our customers in their business strategies utilizing their data to make more intelligent decisions. As a well-funded early-stage startup, Gruve offers a dynamic environment with strong customer and partner networks.\nWhy Gruve:\nAt Gruve, we foster a culture of innovation, collaboration, and continuous learning. We are committed to building a diverse and inclusive workplace where everyone can thrive and contribute their best work. If you're passionate about technology and eager to make an impact, we'd love to hear from you.\nGruve is an equal opportunity employer. We welcome applicants from all backgrounds and thank all who apply; however, only those selected for an interview will be contacted.\nPosition summary:\nWe are seeking an experienced AI Solutions Architect to join our AI team. In this role, you will manage relationships with enterprise customers, gather and translate business requirements into AI solutions, and lead the design and implementation of LLM products. You will be the technical thought leader, guiding customers from prototype to production. This is an exciting opportunity with hands-on experience in building and deploying LLM-powered solutions.\nKey Roles & Responsibilities:\nEngage directly with enterprise customers to gather business and technical requirements in data and AI.\nDesign and implement scalable AI solutions leveraging LLMs.\nPartner with cross-functional teams, including business stakeholders, data engineers, and software engineers, to deploy AI solutions.\nBuild hands-on prototypes and guide customers from proof-of-concept to production, ensuring timely delivery and business impact.\nDevelop best practices for AI solution delivery, including security and compliance measures for safe AI implementations.\nDrive customer success by providing ongoing technical support and thought leadership in AI post-deployment.\nBasic Qualifications:\n8+ years of hands-on experience in software architecture.\nExperience working as a Data Scientist or Analytics Consultant.\nExpert in cloud platforms such as AWS or Azure.\nExperience integrating AI solutions with enterprise systems.\nExcellent communication skills and can engage with both technical teams and executive stakeholders effectively.\nCan lead multiple concurrent engagements.\nHave experience working with programming languages like Python.\nPreferred Qualifications\nMaster's degree in computer science, Data Science, or a related field.\nExperience with cloud-native architectures, CI/CD pipelines, and containerization (e.g., Docker, Kubernetes).\nRelevant certifications in AI/ML technologies or cloud platforms (e.g., AWS, Azure, Google Cloud).\nExperience with enterprise platforms such as Salesforce.","AWS, Kubernetes, Python, Azure, Docker"
Senior Solution Architect - Data Governance Experience-Collibra,Cortex Consultants LLC,7-9 Years,,"Pune, India",Login to check your skill match score,"Solution Architect\n\nExp-15+\n\nLocation-Multiple\n\nImmediate to 15days\n\nSkill-data gov,colibra,architect\n\nOverview: The Senior Solution Architect - Data Governance Experience-Collibra plays a critical role in shaping our organization's data governance framework. This position requires an individual who has extensive experience with Collibra and a strong understanding of data governance principles. The architect will be pivotal in designing and implementing data governance solutions that enhance data quality, compliance, and usability across the enterprise. By leading initiatives that foster a culture of data stewardship, this role ensures that our data assets are effectively managed and leveraged to drive business outcomes. The Senior Solution Architect will collaborate closely with various stakeholders, including IT, compliance, and business units, to ensure alignment with the organization's strategic objectives. This position not only focuses on technical capabilities but also emphasizes the importance of change management and user adoption to ensure long-term success in our data governance journey.\n\nLead the architecture and implementation of Collibra to support data governance initiatives.\nDevelop a comprehensive data governance framework that aligns with organizational goals.\nEngage with stakeholders to gather requirements and translate them into actionable solutions.\nEnsure compliance with data regulations and industry standards during implementation.\nDesign and maintain data models that enhance data discoverability and usability.\nCollaborate with data stewards and business units to promote a culture of data stewardship.\nConduct assessments of existing data management practices and recommend improvements.\nProvide training and support for users to maximize the effectiveness of Collibra.\nFacilitate workshops and meetings to drive consensus around data governance practices.\nMonitor and report on data governance metrics to measure the success of initiatives.\nStay current with industry trends and advancements in data governance technology.\nSupport change management efforts to drive user adoption of data governance solutions.\nDevelop and implement data stewardship policies and procedures.\nWork closely with IT to ensure seamless integration of Collibra with existing systems.\nServe as a subject matter expert for data governance best practices within the organization.\n\nRequired Qualifications\n\nBachelor's degree in Computer Science, Information Technology, or a related field.\nAt least 7 years of experience in data governance, data management, or a related area.\nProven experience as a Solution Architect, with a focus on data governance solutions.\nExtensive hands-on experience with Collibra, including implementation and configuration.\nStrong understanding of data governance principles, frameworks, and best practices.\nExperience working with data models, metadata management, and data quality assessments.\nKnowledge of regulatory compliance requirements such as GDPR, CCPA, or HIPAA.\nAbility to engage and communicate effectively with stakeholders at all levels.\nStrong analytical and problem-solving skills to address data-related challenges.\nProficient in project management methodologies, with experience leading cross-functional teams.\nExperience in change management processes to facilitate user adoption.\nStrong presentation skills with the ability to convey complex concepts to non-technical audiences.\nCertification in data governance or relevant frameworks is a plus.\nExperience with cloud-based data platforms and integration tools.\nAbility to work in a fast-paced, dynamic environment with competing priorities.\n\nSkills: regulatory compliance,project management,data stewardship,stakeholder engagement,data governance,metadata management,data quality assessments,data management,change management,collibra,data models,analytical thinking,solution architecture","data quality assessments, data models, project management, data stewardship, Regulatory Compliance, Data Management, Metadata Management, Collibra, Data Governance, change management"
Data & BI Solution Architect (Remote),Codvo.ai,10-12 Years,,"Pune, India",Login to check your skill match score,"Company Overview:\n\nAt Codvo, software and people transformations go together. We are a global empathy-led technology services company with a core DNA of product innovation and mature software engineering. We uphold the values of Respect, Fairness, Growth, Agility, and Inclusiveness in everything we do.\n\nJob Description:\n\nWe are looking for a Data & BI Solution Architect to lead data analytics initiatives in the retail domain. The candidate should be skilled in data modeling, ETL, visualization, and big data technologies.\n\nKey Responsibilities\n\nDesign and implement scalable data & BI architectures for retail analytics use cases.\nLead the development of data pipelines, ETL/ELT processes, and data integration across cloud platforms.\nDefine and enforce data governance, security, and compliance frameworks.\nCollaborate with business stakeholders to build insightful dashboards and reports.\nEnsure high-performance data delivery using modern big data and cloud-native technologies.\n\nRequired Skills & Experience\n\n\n10+ years of hands-on experience in data engineering, business intelligence, and cloud analytics.\nStrong proficiency in SQL, Python, and Apache Spark.\nProven experience with ETL tools like Informatica, Talend, or AWS Glue.\nExpertise in BI tools such as Power BI, Tableau, and Looker.\nSolid understanding and experience with cloud data platforms Snowflake, Amazon Redshift, or Google BigQuery.\nExperience in retail or e-commerce analytics is a strong plus.\nExp-10+ Years Shift Timing- 2:30 PM -11:30 PM IST Location- remote","snowflake, Google BigQuery, Looker, Power Bi, Amazon Redshift, Apache Spark, Tableau, Python, Sql"
Sr Architect - Data Center,Bechtel Corporation,12-15 Years,,India,Login to check your skill match score,"Requisition ID: 282944\n\nRelocation Authorized: National - Family\nTelework Type: Full-Time Office/Project\nWork Location: Various Bechtel Project Locations\n\nExtraordinary Teams Building Inspiring Projects\n\nSince 1898, we have helped customers complete more than 25,000 projects in 160 countries on all seven continents that have created jobs, grown economies, improved the resiliency of the world's infrastructure, increased access to energy, resources, and vital services, and made the world a safer, cleaner place.\n\nDifferentiated by the quality of our people and our relentless drive to deliver the most successful outcomes, we align our capabilities to our customers objectives to create a lasting positive impact. We serve the Infrastructure; Nuclear, Security & Environmental; Energy; Mining & Metals, and the Manufacturing and Technology markets. Our services span from initial planning and investment, through start-up and operations.\n\nCore to Bechtel is our Vision, Values and Commitments . They are what we believe, what customers can expect, and how we deliver. Learn more about our extraordinary teams building inspiring projects in our Impact Report .\n\nBechtel India is a global operation that supports execution of projects and services around the world. Working seamlessly with business line home offices, project sites, customer organizations and suppliers, our teams have delivered more than 125 projects since our inception in 1994.\n\nOur offices in Gurgaon, Vadodara and Chennai will grow significantly and sustainably with exciting career opportunities for both professionals and young graduates who are passionate about creating a cleaner, greener, and safer world; building transformational infrastructure; making decarbonization a reality; and protecting people and the environment.\n\nJob Summary\n\nArchitect with more than 12 years of experience in Manufacturing and Infrastructure projects (with relevant experience in Data center) projects in a design office of repute. A candidate who has completed at least 1 data center project shall be given more weightage. Should have exposure to design office practices and familiarity with computer aided design and 3D modeling tools like Revit.\n\nMajor Responsibilities\n\nShall perform / supervise calculations, prepare material requisitions, service requisitions, code analysis, standard details, technical specifications and quantity take-offs and provide input for design drawings during all phases of projects.\nShall check / review the drawings prepared by the designers.\nShall assist in the development of basic layout drawings.\nShall lead conceptual studies and inter-disciplinary reviews\nShall create perspectives and presentation of design to client.\nProvide technical training.\nPerform feasibility studies for site development, building configuration, climate studies.\n\nEducation And Experience Requirements\n\nMinimum 5-year degree in Architecture from an accredited college or university.\nPreference will be given to the candidate with Master's degree.\nProfessional license from a recognized licensing board and/or LEED certification shall be of added advantage.\nExperience of making Architectural presentation shall be an added advantage.\nLevel I: 12 -15 years of relevant work experience\nLevel II: 16 - 20 years of relevant work experience\n\nRequired Knowledge And Skills\n\nKnowledge of architectural techniques and design principles with a basic knowledge of the types of data center, data hall layout, mechanical systems, electrical systems, and security systems needed to design a data center.\nKnowledge of precedents and latest trends in architectural engineering, the principles and practices of related technical areas.\nKnowledge of Engineering Procedures and design guides.\nThorough knowledge of the roles played by other engineering disciplines on projects.\nKnowledge of regulatory Indian and International codes and standards, industry practices and design criteria pertinent to architectural engineering design, including fire life safety codes.\nKnowledge of constructability and applicable standards and codes\nProficiency in the use of Revit, Navisworks and exposure to BIM is essential.\nProficient in selection of material from constructability and total installed cost perspective.\nSkill in oral and written communication.\nShould be proficient in using MS office tools.\nProved ability of managing a team of architects and designers will be an added advantage.\nPrevious experience of Data center design including administration building, process/ non process buildings and other ancillary facilities like maintenance building, guard house, substation, electrical buildings, pump house, etc.\nKnowledge of master planning and fire life safety design guidelines\nGood knowledge of faade design and material selection (including interior & exterior finishes)\nExperience in working on EPC projects shall be an added advantage.\n\nTotal Rewards/Benefits\n\nFor decades, Bechtel has worked to inspire the next generation of employees and beyond! Because our teams face some of the world's toughest challenges, we offer robust benefits to ensure our people thrive. Whether it is advancing careers, delivering programs to enhance our culture, or providing time to recharge, Bechtel has the benefits to build a legacy of sustainable growth. Learn more at Bechtel Total Rewards\n\nDiverse Teams Build The Extraordinary\n\nAs a global company, Bechtel has long been home to a vibrant multitude of nationalities, cultures, ethnicities, and life experiences. This diversity has made us a more trusted partner, more effective problem solvers and innovators, and a more attractive destination for leading talent.\n\nWe are committed to being a company where every colleague feels that they belong-where colleagues feel part of One Team, respected and rewarded for what they bring, supported in pursuing their goals, invested in our values and purpose, and treated equitably. Click here to learn more about the people who power our legacy.\n\nBechtel is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity and expression, age, national origin, disability, citizenship status (except as authorized by law), protected veteran status, genetic information, and any other characteristic protected by federal, state or local law. Applicants with a disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to [HIDDEN TEXT]","Ms Office Tools, Revit, Computer Aided Design, Bim, Navisworks, 3D modeling tools"
"Senior Manager, Data and Analytics Architect",MSD,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nSenior Specialist, Data and Analytics Architect\n\nTHE OPPORTUNITY\n\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130-year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nLead an Organization driven by digital technology and data-backed approaches that supports a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be the leaders who have a passion for using data, analytics, and insights to drive decision-making, which will allow us to tackle some of the world's greatest health threats.\n\nOur Technology centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. AN integral part of our company IT operating model, Tech centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\n\nA focused group of leaders in each tech center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\n\nRole Overview\n\nWe are seeking a talented and motivated Technical Architect to join our Data and Analytics Strategy & Architecture team. Reporting to the Lead Architect, this mid-level Technical Architect role is critical in shaping the technical foundation of our cross-product architecture. The ideal candidate will focus on reference architecture, driving proofs of concept (POCs) and points of view (POVs), staying updated on industry trends, solving technical architecture issues, and enabling a robust data observability framework. The role will also emphasize enterprise data marketplaces and data catalogs to ensure data accessibility, governance, and usability.\n\nThis position will also focus on creating a customer-centric development environment that is resilient and easily adoptable by various user personas. The outcome of the cross-product integration will be improved efficiency and productivity through accelerated provisioning times and a seamless user experience, eliminating the need for interacting with multiple platforms and teams.\n\nWhat Will You Do In The Role\n\nCollaborate with product line teams to design and implement cohesive architecture solutions that enable cross-product integration, spanning ingestion, governance, analytics, and visualization.\nDevelop, maintain, and advocate for reusable reference architectures that align with organizational goals and industry standards.\nLead technical POCs and POVs to evaluate new technologies, tools, and methodologies, providing actionable recommendations.\nDiagnose and resolve complex technical architecture issues, ensuring stability, scalability, and performance across platforms.\nImplement and maintain frameworks to monitor data quality, lineage, and reliability across data pipelines.\nContribute to the design and implementation of an enterprise data marketplace to facilitate self-service data discovery, analytics, and consumption.\nOversee and extend the use of Collibra or similar tools to enhance metadata management, data governance, and cataloging across the enterprise.\nMonitor emerging industry trends in data and analytics (e.g., AI/ML, data engineering, cloud platforms) and identify opportunities to incorporate them into our ecosystem.\nWork closely with data engineers, data scientists, and other architects to ensure alignment with the enterprise architecture strategy.\nCreate and maintain technical documentation, including architecture diagrams, decision records, and POC/POV results.\n\nWhat Should You Have\n\nStrong experience with Databricks, Dataiku, Starburst and related data engineering/analytics platforms.\nProficiency in AWS cloud platforms and AWS Data and Analytics technologies\nKnowledge of modern data architecture patterns like data Lakehouse, data mesh, or data fabric.\nHands-on experience with Collibra or similar data catalog tools for metadata management and governance.\nFamiliarity with data observability tools and frameworks to monitor data quality and reliability.\nExperience contributing to or implementing enterprise data marketplaces, including facilitating self-service data access and analytics.\nExposure to designing and implementing scalable, distributed architectures.\nProven experience in diagnosing and resolving technical issues in complex systems.\nPassion for exploring and implementing innovative tools and technologies in data and analytics.\n35 years of total experience in data engineering, analytics, or architecture roles.\nHands-on experience with developing ETL pipelines with DBT, Matillion and Airflow.\nExperience with data modeling, and data visualization tools (e.g., ThoughtSpot, Power BI).\nStrong communication and collaboration skills.\nAbility to work in a fast-paced, cross-functional environment.\nFocus on continuous learning and professional growth.\n\n\nPreferred Skills\n\nCertification in Databricks, Dataiku, or a major cloud platform.\nExperience with orchestration tools like Airflow or Prefect.\nUnderstanding of AI/ML workflows and platforms.\nExposure to frameworks like Apache Spark or Kubernetes.\n\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation\n\nWho We Are\n\nWe are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\n\nWhat We Look For\n\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are intellectually curious, join usand start making your impact today.\n\nCurrent Employees apply HERE\n\nCurrent Contingent Workers apply HERE\n\nSearch Firm Representatives Please Read Carefully\n\nMerck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.\n\nEmployee Status\n\nRegular\n\nRelocation\n\nVISA Sponsorship\n\nTravel Requirements\n\nFlexible Work Arrangements\n\nHybrid\n\nShift\n\nValid Driving License\n\nHazardous Material(s)\n\nJob Posting End Date\n\n06/2/2025\n\nA job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.\n\nRequisition ID R345601","Dataiku, Starburst, AWS cloud platforms, Data observability tools, AWS Data and Analytics technologies, Collibra, Data Modeling, Databricks"
"Senior Manager, Data and Analytics Architect",MSD,5-8 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nR3\n\nSenior Manager Data and Analytics Architect\n\nThe Opportunity\n\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the world's greatest health threats.\n\nOur Technology Centre's focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company's IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\n\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\n\nRole Overview\n\nWe are seeking a highly motivated and hands-on Data & Analytics Architect to join our Strategy & Architecture team within CDNA. This mid-level role will play a critical part in designing scalable, reusable, and secure data and analytics solutions across the enterprise. You will work under the guidance of a senior architect and be directly involved in the implementation of architectural patterns, reference solutions, and technical best practices.\n\nThis is a highly technical role, ideal for someone who enjoys problem-solving, building frameworks, and working in a fast-paced, collaborative environment.\n\nWhat Will You Do In This Role\n\nPartner with senior architects to define and implement modern data architecture patterns and reusable frameworks.\nDesign and develop reference implementations for ingestion, transformation, governance, and analytics using tools such as Databricks (must-have), Informatica, AWS Glue, S3, Redshift, and DBT.\nContribute to the development of a consistent and governed semantic layer, ensuring alignment in business logic, definitions, and metrics across the enterprise.\nWork closely with product line teams to ensure architectural compliance, scalability, and interoperability.\nBuild and optimize batch and real-time data pipelines, applying best practices in data modeling, transformation, and metadata management.\nContribute to architecture governance processes, participate in design reviews, and document architectural decisions.\nSupport mentoring of junior engineers and help foster a strong technical culture within the India-based team.\n\nWhat Should You Have\n\nBachelor's degree in information technology, Computer Science or any Technology stream.\n58 years of experience in data architecture, data engineering, or analytics solution delivery.\nProven hands-on experience with Databricks (must), Informatica, AWS data ecosystem (S3, Glue, Redshift, etc.), and DBT.\nSolid understanding of semantic layer design, including canonical data models and standardized metric logic for enterprise reporting and analytics.\nProficient in SQL, Python, or Scala.\nStrong grasp of data modeling techniques (relational, dimensional, NoSQL), ETL/ELT design, and streaming data frameworks.\nKnowledge of data governance, data security, lineage, and compliance best practices.\nStrong collaboration and communication skills across global and distributed teams.\nExperience with Dataiku or similar data science/analytics platforms is a plus.\nExposure to AI/ML and GenAI use cases is advantageous.\nBackground in pharmaceutical, healthcare, or life sciences industries is preferred.\nFamiliarity with API design, data services, and event-driven architecture is beneficial.\n\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\n\nWho We Are\n\nWe are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\n\nWhat We Look For\n\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join usand start making your impact today.\n\n#HYDIT2025\n\nCurrent Employees apply HERE\n\nCurrent Contingent Workers apply HERE\n\nSearch Firm Representatives Please Read Carefully\n\nMerck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.\n\nEmployee Status\n\nRegular\n\nRelocation\n\nVISA Sponsorship\n\nTravel Requirements\n\nFlexible Work Arrangements\n\nHybrid\n\nShift\n\nValid Driving License\n\nHazardous Material(s)\n\nJob Posting End Date\n\n05/7/2025\n\nA job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.\n\nRequisition ID R341138","Dataiku, dbt, S3, Scala, AWS Glue, Databricks, Informatica, Redshift, Sql, Python"
"Senior Manager, Data and Analytics Architect",MSD,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nSenior Specialist, Data and Analytics Architect\n\nTHE OPPORTUNITY\n\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130-year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nLead an Organization driven by digital technology and data-backed approaches that supports a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be the leaders who have a passion for using data, analytics, and insights to drive decision-making, which will allow us to tackle some of the world's greatest health threats.\n\nOur Technology centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. AN integral part of the our company IT operating model, Tech centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\n\nA focused group of leaders in each tech center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\n\nRole Overview\n\nWe are seeking a talented and motivated Technical Architect to join our Data and Analytics Strategy & Architecture team. Reporting to the Lead Architect, this mid-level Technical Architect role is critical in shaping the technical foundation of our cross-product architecture. The ideal candidate will focus on reference architecture, driving proofs of concept (POCs) and points of view (POVs), staying updated on industry trends, solving technical architecture issues, and enabling a robust data observability framework. The role will also emphasize enterprise data marketplaces and data catalogs to ensure data accessibility, governance, and usability.\n\nThis position will also focus on creating a customer-centric development environment that is resilient and easily adoptable by various user personas. The outcome of the cross-product integration will be improved efficiency and productivity through accelerated provisioning times and a seamless user experience, eliminating the need for interacting with multiple platforms and teams.\n\nWhat Will You Do In The Role\n\nCollaborate with product line teams to design and implement cohesive architecture solutions that enable cross-product integration, spanning ingestion, governance, analytics, and visualization.\nDevelop, maintain, and advocate for reusable reference architectures that align with organizational goals and industry standards.\nLead technical POCs and POVs to evaluate new technologies, tools, and methodologies, providing actionable recommendations.\nDiagnose and resolve complex technical architecture issues, ensuring stability, scalability, and performance across platforms.\nImplement and maintain frameworks to monitor data quality, lineage, and reliability across data pipelines.\nContribute to the design and implementation of an enterprise data marketplace to facilitate self-service data discovery, analytics, and consumption.\nOversee and extend the use of Collibra or similar tools to enhance metadata management, data governance, and cataloging across the enterprise.\nMonitor emerging industry trends in data and analytics (e.g., AI/ML, data engineering, cloud platforms) and identify opportunities to incorporate them into our ecosystem.\nWork closely with data engineers, data scientists, and other architects to ensure alignment with the enterprise architecture strategy.\nCreate and maintain technical documentation, including architecture diagrams, decision records, and POC/POV results.\n\nWhat Should You Have\n\nStrong experience with Databricks, Dataiku, Starburst and related data engineering/analytics platforms.\nProficiency in AWS cloud platforms and AWS Data and Analytics technologies\nKnowledge of modern data architecture patterns like data Lakehouse, data mesh, or data fabric.\nHands-on experience with Collibra or similar data catalog tools for metadata management and governance.\nFamiliarity with data observability tools and frameworks to monitor data quality and reliability.\nExperience contributing to or implementing enterprise data marketplaces, including facilitating self-service data access and analytics.\nExposure to designing and implementing scalable, distributed architectures.\nProven experience in diagnosing and resolving technical issues in complex systems.\nPassion for exploring and implementing innovative tools and technologies in data and analytics.\n35 years of total experience in data engineering, analytics, or architecture roles.\nHands-on experience with developing ETL pipelines with DBT, Matillion and Airflow.\nExperience with data modeling, and data visualization tools (e.g., ThoughtSpot, Power BI).\nStrong communication and collaboration skills.\nAbility to work in a fast-paced, cross-functional environment.\nFocus on continuous learning and professional growth.\n\n\nPreferred Skills\n\nCertification in Databricks, Dataiku, or a major cloud platform.\nExperience with orchestration tools like Airflow or Prefect.\nUnderstanding of AI/ML workflows and platforms.\nExposure to frameworks like Apache Spark or Kubernetes.\n\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation\n\nWho We Are\n\nWe are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\n\nWhat We Look For\n\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are intellectually curious, join usand start making your impact today.\n\nCurrent Employees apply HERE\n\nCurrent Contingent Workers apply HERE\n\nSearch Firm Representatives Please Read Carefully\n\nMerck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.\n\nEmployee Status\n\nRegular\n\nRelocation\n\nVISA Sponsorship\n\nTravel Requirements\n\nFlexible Work Arrangements\n\nHybrid\n\nShift\n\nValid Driving License\n\nHazardous Material(s)\n\nJob Posting End Date\n\n06/2/2025\n\nA job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.\n\nRequisition ID R345600","Dataiku, Starburst, AWS cloud platforms, Data observability tools, AWS Data and Analytics technologies, Collibra, Data Modeling, Databricks"
"Senior Manager, Data and Analytics Architect",MSD,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description\n\nSenior Specialist, Data and Analytics Architect\n\nTHE OPPORTUNITY\n\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130-year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nLead an Organization driven by digital technology and data-backed approaches that supports a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be the leaders who have a passion for using data, analytics, and insights to drive decision-making, which will allow us to tackle some of the world's greatest health threats.\n\nOur Technology centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. AN integral part of the our company IT operating model, Tech centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\n\nA focused group of leaders in each tech center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\n\nRole Overview\n\nWe are seeking a talented and motivated Technical Architect to join our Data and Analytics Strategy & Architecture team. Reporting to the Lead Architect, this mid-level Technical Architect role is critical in shaping the technical foundation of our cross-product architecture. The ideal candidate will focus on reference architecture, driving proofs of concept (POCs) and points of view (POVs), staying updated on industry trends, solving technical architecture issues, and enabling a robust data observability framework. The role will also emphasize enterprise data marketplaces and data catalogs to ensure data accessibility, governance, and usability.\n\nThis position will also focus on creating a customer-centric development environment that is resilient and easily adoptable by various user personas. The outcome of the cross-product integration will be improved efficiency and productivity through accelerated provisioning times and a seamless user experience, eliminating the need for interacting with multiple platforms and teams.\n\nWhat Will You Do In The Role\n\nCollaborate with product line teams to design and implement cohesive architecture solutions that enable cross-product integration, spanning ingestion, governance, analytics, and visualization.\nDevelop, maintain, and advocate for reusable reference architectures that align with organizational goals and industry standards.\nLead technical POCs and POVs to evaluate new technologies, tools, and methodologies, providing actionable recommendations.\nDiagnose and resolve complex technical architecture issues, ensuring stability, scalability, and performance across platforms.\nImplement and maintain frameworks to monitor data quality, lineage, and reliability across data pipelines.\nContribute to the design and implementation of an enterprise data marketplace to facilitate self-service data discovery, analytics, and consumption.\nOversee and extend the use of Collibra or similar tools to enhance metadata management, data governance, and cataloging across the enterprise.\nMonitor emerging industry trends in data and analytics (e.g., AI/ML, data engineering, cloud platforms) and identify opportunities to incorporate them into our ecosystem.\nWork closely with data engineers, data scientists, and other architects to ensure alignment with the enterprise architecture strategy.\nCreate and maintain technical documentation, including architecture diagrams, decision records, and POC/POV results.\n\nWhat Should You Have\n\nStrong experience with Databricks, Dataiku, Starburst and related data engineering/analytics platforms.\nProficiency in AWS cloud platforms and AWS Data and Analytics technologies\nKnowledge of modern data architecture patterns like data Lakehouse, data mesh, or data fabric.\nHands-on experience with Collibra or similar data catalog tools for metadata management and governance.\nFamiliarity with data observability tools and frameworks to monitor data quality and reliability.\nExperience contributing to or implementing enterprise data marketplaces, including facilitating self-service data access and analytics.\nExposure to designing and implementing scalable, distributed architectures.\nProven experience in diagnosing and resolving technical issues in complex systems.\nPassion for exploring and implementing innovative tools and technologies in data and analytics.\n35 years of total experience in data engineering, analytics, or architecture roles.\nHands-on experience with developing ETL pipelines with DBT, Matillion and Airflow.\nExperience with data modeling, and data visualization tools (e.g., ThoughtSpot, Power BI).\nStrong communication and collaboration skills.\nAbility to work in a fast-paced, cross-functional environment.\nFocus on continuous learning and professional growth.\n\n\nPreferred Skills\n\nCertification in Databricks, Dataiku, or a major cloud platform.\nExperience with orchestration tools like Airflow or Prefect.\nUnderstanding of AI/ML workflows and platforms.\nExposure to frameworks like Apache Spark or Kubernetes.\n\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation\n\nWho We Are\n\nWe are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\n\nWhat We Look For\n\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are intellectually curious, join usand start making your impact today.\n\nCurrent Employees apply HERE\n\nCurrent Contingent Workers apply HERE\n\nSearch Firm Representatives Please Read Carefully\n\nMerck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.\n\nEmployee Status\n\nRegular\n\nRelocation\n\nVISA Sponsorship\n\nTravel Requirements\n\nFlexible Work Arrangements\n\nHybrid\n\nShift\n\nValid Driving License\n\nHazardous Material(s)\n\nJob Posting End Date\n\n06/2/2025\n\nA job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.\n\nRequisition ID R345600","Dataiku, Starburst, AWS cloud platforms, Data observability tools, AWS Data and Analytics technologies, Collibra, Data Modeling, Databricks"
MDM Architect- Data Governance,Fractal,Fresher,,"Bengaluru, India",Login to check your skill match score,"Responsibilities\nYou will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management\nLead the end-to-end design, architecture, and implementation of MDM solutions\nDefine and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.\nCollaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.\nDesign data models and hierarchies for Customer, Product, Vendor, and other master domains.\nDevelop and operationalize Data Governance frameworks aligned to business and compliance needs.\nEnable data stewardship workflows, match & merge rules, and exception management.\nIntegrate MDM systems with upstream and downstream applications across the enterprise.\nLead workshops and training sessions on MDM and Data Governance for client teams.\nSupport RFPs, proposals, and client presentations with MDM/DG expertise\nAssess, validate and implement MDM architecture for on-prem, cloud and hybrid environments with expertise in MDM solutions like Oracle MDM solutions (ERP, CDH, CDM), SAP MDG(S4/Hana, ERP), Informatica IDMC and other modern MDM solutions.\nDevelop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.\nCollaborate with stakeholders to define and implement MDM strategies, standards, and operating models.\nDevelop and enforce end-to-end master data lifecycle processes including data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.\nCollaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.\nSupport leadership in designing thought leadership, publish POV's/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.\nGood to Have\nHands-on implementation experience with on-prem MDM and ERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG, SAP S4/HANA, Reltio, Stibo or hybrid deployments with other modern MDM, ERP and CRM platforms.\nStrong knowledge of reference data management and data stewardship workflows.\nExperience with ETL/ELT tools and integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).\nFamiliarity with data governance tools (e.g., Collibra, Alation, Informatica CDGC, etc.) and DQ tools (e.g., Informatica IDQ, CDQ, etc.).\nBasic understanding of data governance best practices, data quality management, data privacy regulations\nFamiliarity with Agentic AI, machine learning, and analytics technologies\nKnowledge of SQL, PL/SQl, Python, or any other database\nExcellent communication and interpersonal skills to collaborate effectively with clients and internal teams.","S4 Hana ERP, Informatica CDI, Oracle MDM solutions, Informatica IDMC, Metadata Management, Pl Sql, Oracle Goldengate, Sql, Data Quality, Sap Mdg, Mulesoft, Python"
"Principal Solution Architect - Data Management, ETL, Data Integration (Presales)",Qlik,8-10 Years,,"Noida, India",Login to check your skill match score,"Description\n\nWhat makes us Qlik\n\nA Gartner Magic Quadrant Leader for 15 years in a row, Qlik transforms complex data landscapes into actionable insights, driving strategic business outcomes. Serving over 40,000 global customers, our portfolio leverages pervasive data quality and advanced AI/ML capabilities that lead to better decisions, faster.\n\nWe excel in integration and governance solutions that work with diverse data sources, and our real-time analytics uncover hidden patterns, empowering teams to address complex challenges and seize new opportunities.\n\nThe Senior Solution Architect Role\n\nAre you ready to take the lead in shaping the future of data and analytics As a Senior Solution Architect, you'll be the go-to technical expert, guiding some of the largest customers and partners in the India region. You'll be at the forefront of demonstrating how cutting-edge data integration and analytics solutions can drive real business transformation. Collaborating closely with a dynamic Presales team in a flexible, agile environment, you'll have the opportunity to showcase your expertise while working with Sales, Marketing, R&D, Product, Consulting, and Customer Success teams. If you're looking for a role that is engaging, fast-paced, and full of opportunities to make an impact, this is it.\n\nWhat makes this role interesting\n\nEngage with high-profile customers and partners: Lead technical discussions and showcase innovative solutions to help organizations unlock the true power of their data.\nDrive business success with cutting-edge technology: Leverage Qlik's next-generation data analytics and data integration platform to solve complex business challenges.\nBe at the forefront of industry trends: Stay ahead of the game by keeping up with the latest advancements in data analytics, as well as the competitive landscape.\nCollaborate with cross-functional teams: Work closely with internal teams and experts across Sales, Marketing, R&D, and Customer Success to build compelling solutions that resonate with customers.\nFlexibility and agility: Thrive in an environment that values adaptability, innovation, and dynamic thinking.\n\nHere's How You'll Be Making An Impact\n\nOwn the technical sales cycle: Become a trusted advisor by guiding customers through technical evaluations, ensuring a seamless journey from exploration to adoption.\nShowcase innovation through tailored solutions: Deliver compelling presentations and custom demonstrations that address real customer needs and business challenges.\nProve value through successful Proof-of-Concepts: Help customers experience the true power of Qlik's platform by leading impactful proof-of-concept engagements.\nSupport business development efforts: Play a key role in driving regional revenue growth by supporting strategic sales initiatives and expanding Qlik's presence in the market.\nPosition solutions for long-term success: Communicate effectively with stakeholders at all levels, from technical teams to senior leadership, ensuring alignment on the value and impact of Qlik's solutions.\n\nWe're Looking For a Teammate With\n\nAt least 8 years of experience in a presales and/or consulting capacity\nStrong Knowledge of Data Integration (ETL), Data Quality (DQ), Data Governance, iPaaS (APIs, micro services, Application Integration) and Big Data\nGood to have working knowledge in Talend ETL tools\nExcellent communication skills to the business as well as technical audience\nHighly driven with strong interpersonal skills\nTrack record of developing relationships at technical, commercial, and executive levels throughout large enterprises\nAbility to work independently and manage multiple complex opportunities.\nGood experience in BI & analytics\nFamiliarity with cloud solutions like SaaS, PaaS and iPaaS\nGood web development background (SQL, RDBMS, Java, HTML5, NET) is a plus.\nGood understanding of Machine Learning tools and its usage such as Python/R\n\nTravel Requirements\n\nWillingness and ability to travel approximately 25 - 50%\nAbility to travel internationally, if required\n\nThe location for this role is:\n\nIndia Delhi\n\nIf you're passionate about helping businesses harness the full potential of their data and want to be part of a team that values expertise, innovation, and collaboration, this is your opportunity to make a real difference. Apply today!\n\nMore About Qlik And Who We Are\n\nFind out more about life at Qlik on social: Instagram, LinkedIn, YouTube, and X/Twitter, and to see all other opportunities to join us and our values, check out our Careers Page.\n\nWhat else do we offer\n\nGenuine career progression pathways and mentoring programs\nCulture of innovation, technology, collaboration, and openness\nFlexible, diverse, and international work environment\n\nGiving back is a huge part of our culture. Alongside an extra change the world day plus another for personal development, we also highly encourage participation in our Corporate Responsibility Employee Programs\n\nIf you need assistance applying for a role due to a disability, please submit your request via email to accessibilityta @ qlik.com. Any information you provide will be treated according to Qlik's Recruitment Privacy Notice. Qlik may only respond to emails related to accommodation requests.\n\nQlik is not accepting unsolicited assistance from search firms for this employment opportunity. Please, no phone calls or emails. All resumes submitted by search firms to any employee at Qlik via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Qlik. No fee will be paid in the event the candidate is hired by Qlik as a result of the referral or through other means.","Talend ETL tools, R, Html5, Sql, ipaas, Java, Application Integration, RDBMS, Machine Learning, Etl, Data Integration, Data Quality, Python, Data Governance, Apis, Big Data, Web Development"
"Manager, Data Science, Solution Architect",Cadbury,10-12 Years,,"Mumbai, India",Login to check your skill match score,"Job Description\nAre You Ready to Make It Happen at Mondelz International\nJoin our Mission to Lead the Future of Snacking. Make It Uniquely Yours.\nYou work with business and IT stakeholders to support a future-state vision in terms of requirements, principles and models in a specific technology, process or function.\nHow you will contribute\nYou will work closely with the enterprise architecture team to chart technology roadmaps, standards, best practices and guiding principles, providing your subject matter expertise and technical capabilities to oversee specific applications in architecture forums and when participating in workshops for business function requirements. In collaboration with the enterprise architecture and solution teams, you will help evaluate specific applications with a goal to bring in new capabilities based on the strategic roadmap. You will also deliver seamless integration with the existing ecosystem and support project teams in implementing these new technologies, offer input regarding decisions on key technology purchases to align IT investments with business strategies while reducing risk and participate in technology product evaluation processes and Architecture Review Board governance for project solutions.\nWhat you will bring\nA desire to drive your future and accelerate your career. You will bring experience and knowledge in:\nDefining and driving successful solution architecture strategy and standards\nComponents of holistic enterprise architecture\nTeamwork, facilitation and negotiation\nPrioritizing and introducing new data sources and tools to drive digital innovation\nNew information technologies and their application\nProblem solving, analysis and communication\nGovernance, security, application life-cycle management and data privacy\nPurpose of Role\nThe Solution Architect will provide end-to-end solution architecture guidance for data science initiatives. A successful candidate will be able to handle multiple projects at a time and drive the right technical architecture decisions for specific business problems. The candidate will also execute PoC/PoVs for emerging AI/ML technologies, support the strategic roadmap and define reusable patterns from which to govern project designs.\nMain Responsibilities: -\nDetermine which technical architectures are appropriate for which models and solutions.\nRecommend what technologies and associated configurations are best to solve business problems.\nDefine and document data science architectural patterns.\nEnsure project compliance with architecture guidelines and processes.\nProvide guidance and support to development teams during the implementation process.\nDevelop and implement processes to execute AI/ML workloads.\nConfigure and optimize the AI/ML systems for performance and scalability.\nStay up to date on the latest AI/ML features and best practices.\nIntegrating SAP BW with SAP S/4HANA and other data sources.\nReview and sign off on high-level architecture designs.\nCareer Experiences Required & Role Implications\nBachelor's degree in computer science or related field of study.\n10+ years of experience in a global company in data-related roles (5+ years in data science).\nStrong proficiency in Databricks and analytical application frameworks (Dash, Shiny, React).\nExperience with data engineering using common frameworks (Python, Spark, distributed SQL, NoSQL).\nExperience leading complex solution designs in a multi-cloud environment.\nExperience with a variety of analytics techniques: statistics, machine learning, optimization, and simulation.\nExperience with software engineering practices and tools (design, testing, source code management, CI/CD).\nDeep understanding of algorithms and tools for developing efficient data processing systems.\nWithin Country Relocation support available and for candidates voluntarily moving internationally some minimal support is offered through our Volunteer International Transfer Policy\nBusiness Unit Summary\nAt Mondelz International, our purpose is to empower people to snack right by offering the right snack, for the right moment, made the right way. That means delivering a broad range of delicious, high-quality snacks that nourish life's moments, made with sustainable ingredients and packaging that consumers can feel good about.\nWe have a rich portfolio of strong brands globally and locally including many household names such as , and biscuits , and chocolate candy and gum. We are proud to hold the top position globally in biscuits, chocolate and candy and the second top position in gum.\nOur 80,000 makers and bakers are located in more than80countries and we sell our products in over150countries around the world. Our people are energized for growth and critical to us living our purpose and values. We are a diverse community that can make things happen-and happen fast.\nMondelz International is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation or preference, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.\nJob Type\nRegular\nTechnology Architecture\nTechnology & Digital","distributed SQL, AI ML technologies, Design, analytics techniques, Dash, CI CD, Statistics, Optimization, Shiny, source code management, Simulation, Databricks, Testing, React, Machine Learning, software engineering practices, Nosql, Python, Spark"
Data & Technical Architect,BNP Paribas India Solutions Private Limited,10-15 Years,,Mumbai,Banking,"Position Purpose\nThe CE&P AML IT team is in charge of AML Monitoring tools for CIB and all regions. AML Monitoring tools are mainly used by Financial Security Compliance and CIB ITO LoD1.\nThe purpose of the role is to seek a highly skilled and experienced AML Functional, Technical, and Data Architect with expertise in Actimize models and rules. The candidate will have a strong background in developing and optimizing database models on the AML Data Lake architecture, specifically focusing on Actimize. They will be responsible for designing, implementing, and maintaining data architecture solutions that effectively support our AML and compliance activities, with a specific emphasis on Actimize models and rules. Working closely with stakeholders, including AML analysts, data scientists, and IT teams, the candidate will ensure that the data architecture solutions align with business requirements and adhere to relevant regulations and policies, while also optimizing Actimize models and rules for enhanced detection and prevention of financial crimes. Key responsibilities will include analyzing system requirements, devising data migration strategies, and ensuring the efficient and secure storage of company information, with a focus on Actimize models and rules.\nResponsibilities\nDirect Responsibilities\nCollaborate with stakeholders to understand AML functional requirements and translate them into Actimize solution design and architecture and Data requirements and translate them into data architecture solutions that support AML and compliance activities.Design and implement Technical and data architecture solutions on the AML Products andData Lake architecture, ensuring scalability, performance, and data integrity.Able to work independently with the Program Manager to understand business requirements and translate them to technical solutions in the applicationConfigure Actimize modules and components to meet specific AML use cases and workflows.Integrate Actimize with other systems and data sources to ensure seamless data flow and information exchange.Develop and optimize database models to effectively store and retrieve AML-related data, considering data volume, complexity, and reporting needs.Establish and enforce data quality and data governance processes to ensure the accuracy, completeness, and consistency of AML data.Implement data security and access control processes to protect sensitive AML information and ensure compliance with security standards and privacy regulations.Evaluate and propose the integration of new technologies and innovative solutions to enhance AML data management processes, such as advanced analytics, machine learning, or automation.\nContributing Responsibilities\nTechnical & Behavioral Competencies\nMinimum of 10 years of experience as a Functional, Technical, and Data Architect, with a strong focus on AML and compliance, demonstrating a deep understanding of industry best practices and regulatory requirements.Extensive expertise in Actimize technical and functional architecture, with a proven track record of successfully implementing Actimize models and rules that align with specific line of business needs.Able to work independently with the Program Manager to understand business requirements and translate them to technical solutions in the application Demonstrated proficiency in developing and optimizing Actimize functional models and rules, as well as designing and optimizing database models on the AML Data Lake architecture.Strong experience in Data-Warehouse architectural design, providing efficient and effective solutions in the Compliance AML data domains.In-depth knowledge of AML and compliance regulations and policies, ensuring compliance with industry standards and legal requirements.Exceptional analytical and problem-solving skills, with the ability to identify and address complex issues related to AML and compliance architecture.Excellent communication and interpersonal skills, enabling effective collaboration with stakeholders at all levels of the organization.Ability to work both independently and as part of a team, demonstrating strong teamwork and collaboration skills.Strong project management skills, with the ability to effectively plan, prioritize, and execute projects within defined timelines and budgets.Good experience in technical analysis of n-tier applications with multiple integrations using object oriented, APIs & Microservices approaches.Very good understanding of principle behind various DevSecOps practices and working experience of industry standard toolsExperience with Agile methodology is a plus, showcasing adaptability and flexibility in project delivery.Good knowledge on front-end technologies preferably Angular.Knowledge on Software methodology practice Agile Methodology & SCRUM practices\nBusiness Skills\nIT / Business relation (Expert)\nCompliance Financial Security (Proficient)\nIT Skills: database\nTransversal Skills\nAbility to manage a project (Expert)\nAnalytical ability (expert)\nAbility to understand, explain and support change (Expert)\nBehaviors Skills\nAbility to Deliver/Results driven(Expert)\nAbility to collaborate (Expert)\nAdaptability (Expert)\nPersonal Impact/Ability to influence (Proficient)\nooResilience (Proficient)\nSpecific Qualifications(if required)\nSkills Referential\nBehaviouralSkills:(Please select up to 4 skills)\nAbility to deliver / Results driven\nAdaptability\nPersonal Impact / Ability to influence\nResilience\nTransversal Skills:(Please select up to 5 skills)\nAbility to manage a project\nAnalytical Ability\nAbility to understand, explain and support change\nAbility to set up relevant performance indicators\nChoose an item.\nEducation Level:\nBachelor Degree or equivalent\nExperience Level\nAt least 10 years\nOther/Specific Qualifications(if required)",Data Architect
Data Solution Architect,Purview India Consulting And Services Llp,10-20 Years,INR 20 - 30 LPA,"Hyderabad, Bengaluru","Information Technology, Information Services","In this role, you will:\nArchitect scalable and secure data solutions as required.\nTake on general solution architecture tasks and deliverables as part of the wider Solution Architecture team where required.\nDevelop logical and physical data models to meet business requirements, contributing to conceptual models where required.\nCollaborate with business analysts and application teams to gather and understand data requirements.\nCreate clear and effective data models, solutions and architectures, ensuring alignment with the overall data architecture and project goals.\nEnsure that the data architecture and models support real-time, near-real-time and batch data processing where needed.\nEnsure smooth data integration and flow across various platforms and systems, including integration of data sources into data refineries for analysis and reporting.\nProvide training and support to other teams in understanding, using and implementing data models and architecture.\nEnsure data models maintain high standards of data integrity, accuracy, and consistency.\nDevelop and implement best practices for data modelling, including naming conventions, documentation, and version control.\nCreate and maintain comprehensive documentation, including Solution Design Documents, entity-relationship diagrams and data dictionaries.\nEnsure that architectures and models are reviewed and approved at the appropriate forums.\nRequirements\nTo be successful in this role, you should meet the following requirements:\nDemonstrable experience in solution architecture, data modelling and data architecture.\nDemonstrable experience preparing and presenting architecture governance artefacts to design boards.\nExperience providing feedback and technical knowledge to facilitate peer review of architectures.\nUsing architecture patterns to accelerate decisions and design.\nProficiency in data modelling tools such as ER Studio, ERWIN or Visual Paradigm.\nStrong understanding of database systems, including relational (SQL) and non-relational (NoSQL) databases.\nFamiliarity with ETL processes and data integration tools.\nExperience with cloud-based (GCP, AWS, Azure), on-prem and hybrid data architectures and solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and documentation abilities.\nAbility to collaborate with technical and non-technical teams.\nIn addition to the above, the following background experience would be highly beneficial:\nConsultancy / customer facing experience within a software vendor or enterprise scale services environment.\nFinancial Services domain knowledge with practical experience of mobilizing technology to meet the market dynamics effecting the financial services domain (Risk / Cost / Customer).\nA track record of successful refactoring / tech-debt remediation projects.\nExposure or experience of big data and big data technologies.\nHigh Level & Holistic Capabilities sought:\nExperience in digital and broader architectural transformation, applying solutions from experience, decomposing monolithic solutions and applying methods to manage technical debt.\nExperience working with enterprise architectures, translating those into solution and data architectures.\nAdherence to architecture governance practices and procedures.\nExperience, knowledge and understanding of enterprise level software development.\nAbility to participate in technical design and oversee all technology related issues of cloud native product development, promoting API reuse and managing down technical debt across the enterprise.","Data Modelling, Data Architecture, cloud, ELT"
Data Platform Architect & Data Engineer + AWS,GlobalLogic Inc,15-18 Years,,Pune,Software,"Skills Requirements: Experience 14+\nJob responsibilities\nAWS solution architect with experience in handling large databases( SQL, NoSQL Mongo, Redis), data warehouses, data lakes.\nWork with teams to understand application requirements and provide guidance on data modeling and database design solutioning .\nArchitect multi-database solutions, integrating different databases and data processing systems.\nStrong Communication, Problem solving skills and ability to work with client directly\nUnderstand and align with company-wide data architecture strategies and best practices\nEnsure data security, integrity, and compliance with Penske Standards ( work with the Data Engineering and Data Services teams)\nImplement and maintain DB clusters, including replica sets and sharded clusters, ensuring data distribution and fault tolerance\nwork on database capacity planning and performance as per our application requirements\nConduct regular architecture reviews if needed\nDevelop, validate, and maintain disaster recovery and high availability strategies for the application SLAs\nTranslate technical concepts and patterns into business benefits for (Penske) management\nAdvise team on architectures, patterns, and strategies for making the best use of MongoDB\nProactively seek opportunities to support and mentor Penske team members and share standard methodologies.\nExposure to Designing High-Performance DB design with Highly Scalable, Available, and RPO/RTO Needs\nExperience in Sharding/Partition Strategy etc.\nExposure to design and review the DB collections etc\nWhat we offer\nCulture of caring.At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you'll experience an inclusive culture of acceptance and belonging, where you'll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders.\nLearning and development.We are committed to your continuous learning and development. You'll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally.\nInteresting & meaningful work.GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you'll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what's possible and bring new solutions to market. In the process, you'll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today.\nBalance and flexibility.We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way!\nHigh-trust organization.We are a high-trust organization where integrity is key.By joining GlobalLogic, you're placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do.","Data Analyst, data engineering, AWS"
Data Visualization Architect – Power BI,Ifintalent Global Private Limited,5-6 Years,,Hyderabad,Information Services,"Job Summary:\nWe're looking for a talented and passionate Data Visualization Lead Engineer, who is responsible for\nsetup, maintenance and troubleshooting of Reporting Infrastructure built using Power BI Web\nServices, Power BI Desktop and Mobile application. We are looking for a senior resource with close\nto 5 6 years of hands-on experience in Power BI Administration who can exhibit exemplary prowess\nin delivering end to end Reporting Solution and can take the accountability of running and\nmaintaining reporting operations. Also, we are focusing on as much automation possible to work with\na lean team, focusing more on quality than quantity. Applicant must have outstanding communication\nskill as he/she would be needed to attend a lot of meeting with stakeholders and run regular stand ups.\nEssential Responsibilities\nShould exhibit expert skills in Power BI Administration\nAzure Analysis Service Setup\nExpertise with Power BI Desktop\nTroubleshooting connectivity\nExpertise in Power Automate and Power Query\nShould be able to integrate Power BI with IM Tools like Microsoft Teams, Slack Channels\nExtensive experience in Power BI DAX\nExpertise in setting up and troubleshooting Data Gateways, On Premise and Cloud\nConnectors\nExtensive experience in Power BI integration to Snowflake and SQL Server\nExtensive experience in migrating of reports from Tableau Server to Power BI Web services\nExperience in migrating reports from Qlick Sense to Power BI\nShould have had experience in managing User Licenses, Capacity Licenses etc.\nExperience with Semantic Modelling Tools like AtScale etc. a big plus\nShould be able to work with Client Side IT Teams to troubleshoot connectivity and\nauthentication issues.\nFast learner\nExcellent communication skills and ability to articulate via verbal and written\ncommunications\nAdvance working knowledge of Microsoft Excel\nEducational Qualification/Experience\nBachelor's/Master's degree in Engineering/Computer Science/Mathematics/Operational\nresearch/Statistics.\nAt least 5 6 years of experience of having Power BI Infrastructure\nExpertise in delivering end to end reporting solutions",Power Bi
Data Engineer Architect,ACL Digital,10-12 Years,,"Pune, India",Login to check your skill match score,"Position: Data Engineer Architect/Data Architect\nMinimum of 10 years of experience in data engineering or a related field, with a proven track record of leading large-scale data projects.\nExtensive experience in designing and implementing data architectures, data pipelines, and ETL processes.\nPrior experience in a leadership or management role within a data engineering team.\nTechnical Skills:\nExpertise in data engineering tools and technologies such as SQL, Python, Spark, Hadoop, and cloud platforms (e.g., AWS, Azure, GCP).\nStrong knowledge of data modeling, data warehousing, and data integration techniques.\nExperience with big data technologies and frameworks is highly desirable.\nLeadership & Communication:\nExcellent leadership, mentoring, and team-building skills.\nStrong strategic thinking and problem-solving abilities.\nExceptional communication and interpersonal skills, with the ability to interact effectively with senior leadership and other stakeholders.\nEducation:\nBachelor's degree in Computer Science, Engineering, Data Science, or a related field. A Master's degree or relevant certifications is preferred.\nWhy Join Us:\nCompetitive salary and benefits package.\nOpportunity to lead a transformative data engineering practice within a dynamic and innovative organization.\nCollaborative work environment with a focus on professional growth and development.\nAccess to cutting-edge technologies and resources.\nKey Responsibilities:\nStrategic Leadership:\nDevelop and implement a strategic vision for the data engineering practice aligned with the company's goals and objectives.\nDrive the adoption of best practices in data engineering and ensure alignment with industry trends and emerging technologies.\nCollaborate with senior leadership and other departments to integrate data engineering solutions into broader business strategies.\nTeam Management:\nLead, mentor, and grow a high-performing team of data engineers, fostering a culture of innovation, collaboration, and continuous improvement.\nOversee the recruitment, development, and performance management of data engineering talent.\nData Architecture & Engineering:\nDesign and implement scalable data architectures, ensuring data integration, quality, and security.\nOversee the development and optimization of data pipelines, ETL processes, and data storage solutions.\nEnsure the effective use of data engineering tools, technologies, and frameworks.\nProject Management:\nManage and prioritize multiple data engineering projects, ensuring timely and successful delivery.\nCoordinate with project managers, data scientists, and other stakeholders to align project goals and deliverables.\nGovernance & Compliance:\nEstablish and enforce data governance policies and practices to ensure data integrity, security, and compliance with relevant regulations.\nMonitor data quality metrics and implement strategies to address data issues.\nInnovation & Continuous Improvement:\nStay abreast of industry trends, emerging technologies, and best practices in data engineering.\nDrive continuous improvement initiatives to enhance data engineering processes and methodologies.","data integration techniques, Spark, Sql, AWS, Big Data Technologies, Data Modeling, Data Warehousing, Python, Azure, Gcp, Hadoop"
Big Data / Cloud Architect (Jan2025),Atgeir Solutions,10-12 Years,,"Pune, India",Login to check your skill match score,"Minimum Experience: 10 Years\n\nWe are looking for a Big Data / Cloud Architect to become part of Atgeir's Advanced Data Analytics team. The desired candidate is a Professional with proven track record of working on Big Data and Cloud Platforms.\n\nRequired Skills\n\nWork closely with customers, understand customer requirements and render those as architectural models that will operate at large scale and high performance, and advise customers on how to run these architectural models on traditional Data Platforms (Hadoop Based) as well as Modern Data Platforms (Cloud Based).\nWork alongside customers to build data management platforms using Open Source Technologies as well as Cloud Native services\nExtract best-practice knowledge, reference architectures, and patterns from these engagements for sharing with Advanced Analytics Centre of Excellence (CoE) team at Atgeir.\nHighly technical and analytical with 10 or more years of ETL and analytics systems development and deployment experience\nStrong verbal and written communications skills are a must, as well as the ability to work effectively across internal and external organizations and virtual teams.\nAbility to think understand complex business requirements and render them as prototype systems with quick turnaround time.\nImplementation and tuning experience in the Big Data Ecosystem, (such as Hadoop, Spark, Presto, Hive), Database (such as Oracle, MySQL, PostgreSQL, MS SQL Server) and Data Warehouses (such as Redshift, Teradata, etc.)\nKnowledge of foundation infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with one of the clouds (GCP / Azure / AWS) and/or data cloud platforms (Databricks / Snowflake)\nProven hands-on experience with at least one programming language among (Python, Java. Go, Scala)\nWillingness to work hands-on on the projects\nAbility to lead and guide large teams\nArchitect level certification on one of the clouds will be an added advantage.","Go, Teradata, Cloud Platforms, snowflake, Java, Hadoop, Ms Sql Server, PostgreSQL, Scala, Big Data, Redshift, Hive, Gcp, Presto, MySQL, Spark, Databricks, Azure, Oracle, Python, AWS"
Master Data Management Architect,CONMED Corporation,5-7 Years,,India,Login to check your skill match score,"Master Data Management Architect\nCONMED is a global medical technology company that specializes in the development and manufacturing of surgical devices and equipment. With a mission to empower healthcare professionals to deliver exceptional patient care, CONMED is dedicated to innovation, quality, and excellence in all aspects of our operations.\nRole Overview:\nThe Master Data Management (MDM) Architect provides technical and administrative support for Master Data Management, focusing on improving data quality and aligning data governance to support data transformation. The role ensures consistent and accurate master data across the enterprise for better decision-making and operational efficiency. The ideal candidate is self-driven and adaptable, meticulously attentive, and brings relevant experience completing major data transformations.\nThis is a remote opportunity for people living in India.\nResponsibilities:\nMaster Data Management\nLead MDM projects involving data cleansing, standardization, and migration to support digital transformation initiatives.\nCollaborate with cross-functional teams across the organization to understand data needs, gather requirements, and ensure alignment with business transformation objectives.\nEstablish data quality metrics, monitor data quality issues, and implement corrective actions to maintain high data integrity throughout the transformation process.\nMonitor and evaluate the effectiveness of MDM processes, identify opportunities for further optimization to support evolving business needs.\nManage on-premise application and administration of Syniti and/or Informatica MDM platform\nDevelop and enforce data governance policies.\nTrain staff on data management protocols.\nQualifications:\nBachelor's degree in Information Technology, Computer Science, Business Administration, or related field.\nExperience with Syniti and/or Informatica MDM platform and SAP knowledge preferred. S4HANA.\n5+ years of proven experience in managing MDM projects and implementing MDM solutions across large organizations.\nStrong understanding of data governance principles, data quality best practices, and data cleansing techniques.\nStrong analytical and problem-solving skills to identify data quality issues and develop solutions.\nExcellent communication and stakeholder management skills to collaborate with cross-functional teams.\nSelf-driven and adaptable.\nPreferred:\n3-5 years of experience supporting ERP transformations\nKey Competencies:\nTechnical Expertise: Experience with MDM projects, data governance principles, data quality best practices, and data cleansing techniques.\nAnalytical Skills: Strong problem-solving abilities to identify and address data quality issues.\nCommunication Skills: Excellent ability to collaborate with cross-functional teams and manage stakeholders.\nProject Management: Proven experience in managing MDM projects and implementing solutions across large organizations.\nAdaptability: Self-driven and adaptable to evolving business needs.\nPlatform Knowledge: Preferred experience with the Syniti platform and SAP.","Master Data Management, Data cleansing techniques, Data governance principles, Informatica MDM platform, Data quality best practices"
Lead Data Engineer- Java/Big Data/SQL/Architect-14+yrs,Visa,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Company Description\n\nVisa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose to uplift everyone, everywhere by being the best way to pay and be paid.\n\nMake an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.\n\nJob Description\n\nThe Universal Data Catalog (UDC) / Visa Data Catalog (VDC) is a comprehensive metadata management platform designed to provide a centralized repository for all data-related information across the organization. The platform enables efficient data discovery, governance, and utilization by offering detailed metadata definitions for tables, columns, and other data assets. It supports various business units by improving data accessibility, enhancing data quality, and facilitating compliance with data governance policies. Our implementation leverages the open-source Datahub project, ensuring a robust, flexible, and scalable solution.\n\nKey Responsibilities\n\nDevelop and maintain the Enterprise Data Catalog / Visa Data Catalog platform, ensuring it meets the organization's evolving needs.\nImplement and manage metadata ingestion processes to ensure the catalog is up-to-date with the latest data definitions and business context.\nCollaborate with data stewards, data owners, and other stakeholders to enrich metadata with business definitions, data lineage, and usage context.\nEnhance the catalog's search and discovery capabilities to provide users with intuitive and efficient access to data assets.\nIntegrate the data catalog with other enterprise systems and tools to support data governance, data quality, and analytics initiatives.\nMonitor and optimize the performance of the data catalog to ensure scalability and reliability.\nProvide training and support to users on how to effectively leverage the data catalog for their data-related needs.\nActively collaborate with the open-source community to contribute to and leverage the latest advancements in the Datahub project.\nAnalyze industry best practices and keep the catalog functionality up-to-date with feature sets provided by the market, while focusing on Visa's scalability requirements.\nChampion the adoption of open infrastructure solutions that are fit for purpose while keeping technology relevant.\nSpend 70% of the time writing code in different languages, frameworks, and technology stacks.\n\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\nQualifications\n\nBasic Qualification\n\n12 to 15 yrs+ of experience in architecture design and development of large-\n\nscale data management platforms and data application with simple solutions\n\nMust have extensive hands-on coding and designing skills on\nJava/Python for backend\nMVC (model-view-controller) for end-to-end development\nSQL/NoSQL technology. Familiar with Databases like Oracle, DB2, SQL Server, etc.\nWeb Services (REST/ SOAP/gRPC)\nReact/Angular for front-end (UI front-end nice to have)\nExpertise in design and management of complex data structures and data\n\nprocesses\n\nExpertise in efficiently leveraging the power of distributed big data systems,\n\nincluding but not limited to Hadoop Hive, Spark, Kafka streaming, etc.\n\nDeep knowledge and hands on experience on big data and cloud computing\n\ntechnologies.\n\nStrong service architecture and development experience with high\n\nperformance and scalability\n\nTechnical background in data with deep understanding of issues in multiple\n\nareas such as data acquisition, ingestion and processing, data management,\n\ndistributed processing, and high availability is required.\n\nStrong on driving for results and self-motivated, strong learning mindset, with\n\ngood understanding of related advanced/new technology. Keep up with the\n\ntechnology development in the related areas in the industry, which could be\n\nleveraged to enhance current architectures and build durable new ones.\n\nBachelor's degree in Computer Science or related technical discipline required.\n\nAdvanced degree is a plus.\n\nStrong leadership and team player.\nStrong skills on mentoring/growing junior people\nPayment industry experience is a plus.\n\nPreferred Qualification\n\nExperience with ETL / ELT tools / applications\nExperience with Apache NiFi and Apache Spark for processing large data sets\nExperience on Elastic Search\nKnowledge on Data Catalog tools\nExperience in building Data Pipeline development tools\nExperience with Data Governance and Data Quality tools\n\nAdditional Information\n\nVisa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","GRPC, Data Catalog tools, Data Quality tools, Kafka, ELT, Angular, Apache Nifi, Nosql, React, Oracle, Python, Java, Hadoop, SQL Server, Soap, Sql, REST, Hive, DB2, Spark, Elastic Search, Mvc, Web Services, Etl"
Data & AI Architect,ennVee India,12-15 Years,,"Chennai, India",Login to check your skill match score,"Back to Career Portal\n\nData & AI Architect\n\n12 - 15 years of experience\nChennai\n\nWe're seeking an experienced Data & AI Architect to lead the design and implementation of large-scale data architectures on cloud platforms. The ideal candidate will have a strong background in data warehousing, big data, data analytics, machine learning, AI, and cloud data engineering.\n\nKey Responsibilities\n\nCollaborate with stakeholders to understand business requirements and develop solutions.\nDesign and implement scalable data architectures on cloud platforms.\nDevelop and maintain data models, data warehousing concepts, and data governance.\nImplement data ingestion, pipeline, preparation, and orchestration solutions.\nApply machine learning algorithms and data analytics techniques.\nDesign and implement AI/ML models using popular frameworks.\nEnsure data security, scalability, maintainability, and reliability.\nDevelop and maintain technical documentation.\n\nRequirements\n\n12-15 + years of experience in enterprise data architecture and implementation.\nStrong expertise in cloud architecture.\nExperience with AI/ML frameworks and libraries.\nExperience with big data, data warehousing, and data analytics technologies.\nProficient in Python, Java, or similar programming languages.\nExperience with data visualization tools (Power BI, Tableau, etc.,).\nStrong understanding of data governance and security.\nKnowledge of NoSQL databases (Cosmos DB, MongoDB, Cassandra, etc.,).\nStrong understanding of data governance and security.","data visualization tools, NoSQL databases, AI ML frameworks and libraries, MongoDB, Cosmos DB, Java, Data Governance, Cloud Architecture, Tableau, Power Bi, Data Analytics, Cassandra, Data Warehousing, Python, Data Security"
Data Lead/Architect,DevOn,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"DevOn we are a leading provider of innovative technology solutions with a focus on data-driven decision-making, cloud computing, and advanced analytics. Our dynamic team is passionate about solving complex business problems through innovative technology and were looking for a skilled and motivated Data Engineer Lead to join us.\nRole Overview:\nAs a Data Engineer Lead, you will be responsible for leading the design, development, and maintenance of data pipelines and ETL workflows, leveraging modern cloud technologies. You will work closely with cross-functional teams to ensure data availability, reliability, and scalability, enabling data-driven decision-making across the organization. This role requires a deep understanding of Python, PySpark, AWS Glue, RedShift, SQL, Jenkins, Bitbucket, EKS, and Airflow.\nKey Responsibilities:\nLead the design and implementation of scalable data pipelines and ETL workflows in a cloud environment (primarily AWS).\nBuild and manage data ingestion, transformation, and storage frameworks using AWS Glue, PySpark, and RedShift.\nArchitect and optimize complex SQL queries for large datasets and ensure data integrity across systems.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver high-quality data solutions.\nAutomate the end-to-end data pipeline process using Jenkins and Bitbucket, ensuring efficient CI/CD practices.\nOptimize and manage data orchestration using Apache Airflow.\nProvide technical leadership and mentorship to junior team members, ensuring best practices for data engineering are followed.\nWork with AWS services such as RedShift, S3, Lambda, and EKS for deployment and management of data solutions.\nTroubleshoot and resolve complex data pipeline issues, ensuring minimal downtime and high availability.\nParticipate in architecture and design reviews, providing input on technical solutions and improvements.\nContinuously evaluate new tools and technologies to improve the efficiency and scalability of our data infrastructure.\nRequired Skills and Qualifications:\n5+ years of professional experience in Data Engineering, with a proven track record of building scalable data pipelines and ETL workflows.\nStrong expertise in Python for data processing and scripting.\nHands-on experience with PySpark for large-scale data processing.\nIn-depth knowledge of AWS Glue, RedShift, S3, and other AWS services.\nAdvanced proficiency in SQL for data manipulation and optimization.\nExperience with Jenkins and Bitbucket for CI/CD automation.\nFamiliarity with EKS (Elastic Kubernetes Service) for containerized deployment of data applications.\nExperience with Apache Airflow for data orchestration and workflow automation.\nStrong problem-solving skills and the ability to debug complex issues in data workflows.\nExcellent communication skills, with the ability to collaborate with cross-functional teams and explain complex technical concepts in a clear manner.\nAbility to work in an Agile development environment, managing multiple priorities and delivering on tight deadlines.\nPreferred Qualifications:\nExperience with additional AWS services (e.g., Lambda, Redshift Spectrum, Athena).\nFamiliarity with Docker and container orchestration technologies like Kubernetes.\nKnowledge of data modeling and data warehousing concepts.\nBachelors or Master&aposs degree in Computer Science, Engineering, or a related field.\nShow more Show less","EKS, Apache Airflow, Jenkins, Bitbucket, Pyspark, AWS Glue, Redshift, Sql, Python"
Data Warehouse Architect,Anthology Inc,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Description\n\nData Warehouse Architect\n\nBangalore, India\n\nThe Opportunity:\n\nAnthology delivers education and technology solutions so that students can reach their full potential and learning institutions thrive. Our mission is to empower educators and institutions with meaningful innovation that's simple and intelligent, inspiring student success and institutional growth.\n\nThe Power of Together is built on having a diverse and inclusive workforce. We are committed to making diversity, inclusion, and belonging a foundational part of our hiring practices and who we are as a company.\n\nFor more information about Anthology and our career opportunities, please visit www.anthology.com.\n\nAs a Data Warehouse Architect, you will act as the primary advocate of data modeling methodologies and data processing best practices to develop a clear technical vision for our Global Customer Success (GCS) data warehouse. You will be responsible for the leadership of overarching data architecture vision and coordination of all data warehouse efforts that affect data, with a focus on data moving throughout the GCS systems. This will be accomplished by mapping and documenting current data sources used throughout GCS, creating a data dictionary which captures the business meaning of the data used, and working with various teams to eliminate unnecessary steps and modify interfaces.\n\nPrimary responsibilities will include:\n\nDocumenting the flow and movement of data through Student Success including:\nThe frequency of movement\nThe source and destination of each step in the flow\nAny transformation of data during the flow\nAny aggregation and calculations of data in the flow\nWorking with business owners and application designers to identify and model integrative views and determine the quality-of-service requirements data currency, availability, response times and data volumes\nDefining technical standards and guidelines for how to use architected databases, the technologies to be used for various purposes like data extraction, transformation and integration, and models of entities, objects, and processes. These guidelines will encourage the use of existing data stores where applicable as well as address security and quality\nEnsuring that standards are up to date by investigating emerging technologies and new releases. This means working in conjunction with technology architects (where they exist). Participation in proof-of-concept projects and other projects that are early users of new technologies is required from time to time\nCommunicating the data architecture to the development community, IT operations, IT management at all levels, and business owners\nFocusing on data quality by educating the business on its importance and involving and facilitating the work of business constituents on improvement programs including the assignment of data stewards which are a critical component\n\nThe Candidate:\n\nRequired skills/qualifications:\n\nBachelor's degree (Computer Science, Mathematics, Statistics, Industrial Engineering) or similar work experience\nAt least 10 years of experience in a directly related area, during which both professional and management capabilities have been clearly demonstrated\nExpert in database technologies, such as SQL server database, Snowflake, etc. as well as data modeling, both logical and physical\nExtensive experience working with enterprise applications like, Salesforce, Oracle Fusion and/or Peoplesoft\nProficiency in unstructured data such as YouTube, Facebook, Twitter, or LinkedIn\nExpertise in establishing master data management\nMastery of multidimensional data modeling\nExtremely strong analytical and problem-solving skills\nOutstanding oral, written, and visual presentation skills\nSubstantial negotiating experience\nAbility to thrive when working across internal functional areas in ambiguous situations\nFluency in written and spoken English\n\nPreferred skills/qualifications:\n\nAdvanced degree in Applied Mathematics, Business Analytics, Statistics, Machine Learning, Computer Science, or related field\n\nThis job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required. Nothing in this job description restricts management's right to assign or reassign duties and responsibilities at any time.\n\nAnthology is an equal employment opportunity/affirmative action employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, gender identity/expression, protected military/veteran status, or any other legally protected factor.","Salesforce, snowflake, multidimensional data modeling, master data management, Peoplesoft, Oracle Fusion, Data Modeling, unstructured data, SQL server"
Data Model Architect,Prodapt,Fresher,,"Chennai, India",Login to check your skill match score,"Overview\n\nProdapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.\n\nResponsibilities\n\nDeliverables\n\nDesign & Document - Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)\nDesign & Document Build Interface Speciation for Data Integration.\n\nActivities\n\n\nData Architecture and Modeling:\n\nDesign and maintain conceptual, logical, and physical data models\nEnsure scalability and adaptability of data models for future organizational needs.\n\nData Model P-S-R catalogs in the existing Catalogs,SOM,COM systems\n\nCMDB Design and Management:\n\nArchitect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.\nDefine data governance standards and enforce data consistency across the CMDB.\n\nDesign data integrations between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).\n\nRequirements\n\nGood Communication skills.\n\nBachelors Degree.","CMDB Design, Data Architecture, Data Governance, Data Integration"
Data Warehouse Architect,Capital Numbers,5-7 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"We are seeking a highly skilled Database/Data Warehouse Architect with a strong background in Data Vault 2.0 methodology to join our team. The ideal candidate will have a minimum of 5 years of experience in database architecture, data modeling, and data integration, with proven expertise in designing and implementing enterprise-scale data warehouses.\nKey Responsibilities:\nDesign and implement scalable and efficient Data Warehouse solutions using Data Vault 2.0 methodology.\nDevelop and maintain data models, including hubs, links, and satellites, ensuring data consistency and quality.\nCollaborate with business stakeholders, data engineers, and analysts to define data integration and ETL frameworks.\nLead the architecture and implementation of end-to-end Data Vault 2.0 solutions.\nOptimize data storage, retrieval, and performance using best practices in data architecture.\nWork with Snowflake (preferred) or other cloud-based data platforms to develop and deploy data solutions.\nEnsure compliance with data governance and security policies.\nRequired Qualifications:\n5+ years of experience as a Database/Data Warehouse Architect.\nProven expertise in Data Vault 2.0 methodology and hands-on experience with at least one end-to-end Data Vault 2.0 project.\nStrong understanding of data modeling, data integration, and ETL frameworks.\nExperience with Snowflake or similar cloud-based data platforms (preferred).\nSolid knowledge of SQL, data pipelines, and performance optimization techniques.\nExcellent problem-solving skills and the ability to work in a fast-paced environment.\nStrong communication and collaboration skills to work with cross-functional teams.\nPreferred Qualifications:\nExperience with cloud-based data solutions (AWS, Azure, or GCP).\nKnowledge of Python, Scala, or other scripting languages for data processing.\nFamiliarity with BI tools and reporting frameworks","snowflake, reporting frameworks, ETL frameworks, data pipelines, performance optimization, Data Vault 2.0 methodology, cloud-based data solutions, Data Integration, Sql, Data Modeling, AWS, Bi Tools, Python, Azure, Gcp, Scala"
Data Modeller - Architect-10+ Years-Immediate,Cortex Consultants LLC,10-14 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Data Modeller - Architect\n\nExperience: 10 to 14 Years\n\nLocation: Chennai, Hyderabad, Bangalore, Pune, Delhi\n\nJob Type: Permanent Role\n\nNotice Period: Immediate Joiners Only\n\nJob Description\n\nWe are looking for an experienced Data Modeller - Architect with 10 to 14 years of expertise in designing and architecting data models for large-scale enterprise systems. The ideal candidate will have in-depth experience in data architecture, data modeling, and building scalable solutions. This is a permanent role with immediate joiner requirements across multiple locations.\n\nKey Responsibilities\n\nDesign and architect data models for complex data environments and large datasets\nDevelop high-level data architecture solutions and manage data integration across systems\nCollaborate with stakeholders to understand business requirements and translate them into effective data models\nLead and mentor teams in the development and implementation of data modeling strategies\nOptimize database performance, ensuring data integrity and efficiency\nDefine and enforce best practices for data management and modeling across the organization\nWork closely with IT, data engineering, and analytics teams to align data architecture with business goals\nEnsure the scalability, security, and performance of data solutions\n\nRequired Skills & Qualifications\n\n10 to 14 years of experience in data modeling and architecture\nStrong expertise in relational and non-relational databases, data warehousing, and cloud data platforms\nProficiency in designing data models for complex, high-volume systems\nHands-on experience with SQL, NoSQL, and big data technologies\nProven track record of leading data architecture initiatives and cross-functional teams\nExperience in data integration, ETL processes, and data governance\nImmediate joiners only\n\nSkills: non-relational databases,etl,nosql,data technologies,data architecture,analytics solutions,software development,sql,cloud platforms,relational databases,modeling,cloud data platforms,etl processes,big data technologies,data integration,data governance,data modeling,data management,data,data warehousing","Relational Databases, ETL processes, non-relational databases, cloud data platforms, Data Warehousing, Data Architecture, Big Data Technologies, Data Modeling, Sql, Nosql, Data Governance, Data Integration"
Data / Solution Architect,Cynosure Corporate Solutions,Fresher,,"Chennai, India",Login to check your skill match score,"Responsibilities\nAnalyzing existing data sources\nExpert understanding of data models and various 1 to many, many to many and other patterns, normalization and denormalization patterns and purposes\nProfile data sources to reverse engineer data model and relationships between tables, identify key fields, and infer meaning of attributes\nMeet with system owners to tie observations of data patterns with business processes and use cases that lead to those patterns\nConduct root cause analysis (RCA) to identify underlying issues and drive effective solutions.\nPerform frequency distribution analysis to identify patterns and trends in the data.Architecting data strategy for Future State\nProfile the data sources to identify anomalies, inconsistencies, and data quality issues.\nTake ownership of the data, ensuring accuracy, completeness, and reliability.\nDevelop target data model that provides optimal long term functional opportunities\nDesign the right change data capture, audit strategy\nIdentify where reference tables are necessary\nDesign mapping tables / helper tables to support configurable ETL\nRequirements:\nGeneral Attributes\nAdvanced sql skills, familiarity with a variety of DB technologies (oracle, sql server, etc)\nComfortable with AWS environment and databricks\nExperience with ETL\nDemonstrate curiosity and a relentless pursuit of understanding complex datasets.\nEngage with various stakeholders, including clinicians, researchers, data scientists, and IT professionals, to gather requirements and ensure alignment.\nCommunicate effectively with stakeholders at all levels, translating technical concepts into clear and actionable insights.\nLead deep data analysis initiatives to extract meaningful insights and drive data-driven decision-making\nCollaborate with cross-functional teams to develop and implement data models and solutions that meet business objectives","change data capture, Mapping tables, Databricks, Sql, AWS, Etl"
Data Model Architect,Prodapt,Fresher,,"Chennai, India",Login to check your skill match score,"Overview\n\nProdapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.\n\nResponsibilities\n\nDeliverables\n\nDesign & Document - Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)\nDesign & Document Build Interface Speciation for Data Integration.\n\nActivities\n\n\nData Architecture and Modeling:\n\nDesign and maintain conceptual, logical, and physical data models\nEnsure scalability and adaptability of data models for future organizational needs.\n\nData Model P-S-R catalogs in the existing Catalogs,SOM,COM systems\n\nCMDB Design and Management:\n\nArchitect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.\nDefine data governance standards and enforce data consistency across the CMDB.\n\nDesign data integrations between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).\n\nRequirements\n\nGood Communication skills.\n\nBachelors Degree.","CMDB Design, Data Architecture, Data Governance, Data Integration"
Success Architect - Data Cloud,Salesforce,7-12 Years,,Hyderabad,Software,"Job description\nAt Salesforce, we are dedicated to fostering a diverse and inclusive workplace where individuals from all backgrounds are welcomed and valued. We believe that the unique perspectives and skills of diverse candidates greatly contribute to the success of our teams. As a Data Cloud Success Architect, you will play a crucial role in driving successful outcomes for our strategic customers by leveraging your technical expertise in data and analytics. You should have a keen interest in the emergence of AI and the role of Data in it s success.\nResponsibilities:\nBe a trusted Data Cloud subject-matter expert for the broader Success Architect organization, including how Data Cloud relates to the success of AI.\nEngage with our Signature and Strategic customers to evaluate and recommend optimization strategies for technical architecture, dev/ops, performance, and solution design specific to Data Cloud.\nIdentify and evaluate capability gaps for standard product capability or identify creative Architect solutions through customization.\nFacilitate and influence Executive stakeholders while aligning technology strategy to business value and ROI\nRun playbooks aligned with our Success Architect engagement catalog, tailored to the unique needs and opportunities of Data Cloud customers.\nBuild strong relationships with both internal and external business partners, contributing to broader goals and growth.\nDrive thought leadership through mentoring and knowledge sharing\nImpact of the Role:\nAs a Data Cloud Success Architect, you will have a significant impact on our customers success and the growth of our organization. Your expertise and guidance will directly influence the technical architecture, performance optimization, and solution design strategies for our strategic customers, ensuring their success in leveraging Data Clouds powerful data capabilities. By driving customer satisfaction and delivering exceptional value, you will contribute to the overall growth and reputation of Salesforce as a leader in the industry.\nCollaboration and Teamwork:\nCollaboration is at the core of our success, and as a Data Cloud Success Architect, you will have the opportunity to work closely with diverse teams of professionals. You will collaborate with customers, colleagues, and partners to evaluate technical architecture, optimize performance, and design effective solutions. By fostering strong relationships and working collaboratively, you will contribute to the collective success of our teams and the achievement of our goals.\nBasic Requirements:\nMinimum 2 development/project implementation lifecycles of Salesforce Data Cloud/CDP\nMinimum 8 years proven experience in enterprise consulting, including implementing enterprise software solutions in the Analytics/CDP spaces.\nDemonstrated ability to analyze, design, and optimize business processes focusing on data integration architecture, with a focus on guiding customers through migration to and optimization of Data Cloud\nDeep understanding of data modeling, integration architectures, and data governance best practices.\nExcellent communication skills, and ability to work collaboratively with cross-functional teams from Developer to Executive\nAbility to facilitate discussions and translate technical conceptssolutions into tangible business value and ROI for customers\nAbility to demonstrate basic understanding and stay up-to-date with emerging data-related and AI technologies\nProactive and self-starting attitude with the ability to manage tasks independently while collaborating remotelywith customers and colleagues\nValues the importance of Data Ethics and Privacy by ensuring that customer solutions adhere to relevant regulations and best practices in data security and privacy.\nPreferred Requirements:\nExperience in aconsulting implementationPartner for Data Cloud/CDP\nExperience with large data such as Snowflake, Databricks, AWS, Google Cloud Storage/Big Query, Azure administration or architecture\nExperience with Customer Data platforms such as Segment, Tealium, Adobe Experience Platform, Amperity, Treasure Data, Twilio Segment, Realtio, etc\nExperience implementing Salesforce Clouds, Multi cloud scenarios - Sales, Service, Industries, Marketing or Commerce\nFamiliar with at least one of the Salesforce Architecture Domains: Integration, Access and Identity Management, Data Architecture, Sharing and Visibility, Declarative and APEX Development\nExperience in programming languages such as Python, Java, .Net, SQL","Java, .NET, Python, Sql"
Data and Analytics Architect - L1,Wipro Limited,Fresher,,Delhi,IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\nJob Description\nRole Purpose\nThe purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.\nDo\n1. Define and Develop Data Architecture that aids organization and clients in new/ existing deals\na. Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation\nb. Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy\nc. Create data strategy and road maps for the Reference Data Architecture as required by the clients\nd. Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request\ne. Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise\nf. Develop, communicate, support and monitor compliance with Data Modelling standards\ng. Oversee and monitor all frameworks to manage data across organization\nh. Provide insights for database storage and platform for ease of use and least manual work\ni. Collaborate with vendors to ensure integrity, objectives and system configuration\nj. Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization\nk. Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage\nl. Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes\nm. Knowledge of all the Data service provider platforms and ensure end to end view.\nn. Oversight all the data standards/ reference/ papers for proper governance\no. Promote, guard and guide the organization towards common semantics and the proper use of metadata\np. Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control\nq. Provide solution of RFPs received from clients and ensure overall implementation assurance\ni. Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives\nii. Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data\niii. Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology\niv. Define and understand current issues and problems and identify improvements\nv. Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout\nvi. Understand the root cause problem in integrating business and product units\nvii. Validate the solution/ prototype from technology, cost structure and customer differentiation point of view\nviii. Collaborating with sales and delivery leadership teams to identify future needs and requirements\nix. Tracks industry and application trends and relates these to planning current and future IT needs\n2. Building enterprise technology environment for data architecture management\na. Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes\nb. Evaluate all the implemented systems to determine their viability in terms of cost effectiveness\nc. Collect all the structural and non-structural data from different places integrate all the data in one database form\nd. Work through every stage of data processing: analysing, creating, physical data model designs, solutions and reports\ne. Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices\nf. Implement the best security practices across all the data bases based on the accessibility and technology\ng. Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)\nh. Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration\n3. Enable Delivery Teams by providing optimal delivery solutions/ frameworks\na. Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor\nb. Define database physical structure, functional capabilities, security, back-up and recovery specifications\nc. Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results\nd. Monitor system capabilities and performance by performing tests and configurations\ne. Integrate new solutions and troubleshoot previously occurred errors\nf. Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards\ng. Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects\nh. Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams\ni. Recommend tools for reuse, automation for improved productivity and reduced cycle times\nj. Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.\nk. Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams\nl. Ensures architecture principles and standards are consistently applied to all the projects\nm. Ensure optimal Client Engagement\ni. Support pre-sales team while presenting the entire solution design and its principles to the client\nii. Negotiate, manage and coordinate with the client teams to ensure all requirements are met\niii. Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",
IN-Manager_Azure and Databricks Architect_Data & Analytics_Advisory_Bangalore,PwC India,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service\n\nAdvisory\n\nIndustry/Sector\n\nNot Applicable\n\nSpecialism\n\nData, Analytics & AI\n\nManagement Level\n\nManager\n\nJob Description & Summary\n\nAt PwC, our people in business application consulting specialise in consulting services for a variety of business applications, helping clients optimise operational efficiency. These individuals analyse client needs, implement software solutions, and provide training and support for seamless integration and utilisation of business applications, enabling clients to achieve their strategic objectives.\n\nAs a business application consulting generalist at PwC, you will provide consulting services for a wide range of business applications. You will leverage a broad understanding of various software solutions to assist clients in optimising operational efficiency through analysis, implementation, training, and support.\n\nWhy PWC\n\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.\n\nAt PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n\nJob Description & Summary We are seeking an experienced Azure and Databricks Architect with 7 to 10 years of experience specializing in data services, data architecture, and data platforms. The ideal candidate will have a strong background in designing and implementing scalable data solutions on Azure, with a focus on Databricks.\n\nResponsibilities\n\nArchitect and design end-to-end data solutions on Azure, with a focus on Databricks.\nLead data architecture initiatives, ensuring alignment with best practices and business objectives.\nCollaborate with stakeholders to define data strategies, architectures, and roadmaps.\nMigrate and transform data from Oracle to Azure Data Lake.\nEnsure data solutions are secure, reliable, and scalable.\nProvide technical leadership and mentorship to junior team members.\n\nMandatory Skill Sets\n\n\nExtensive experience with Azure Data Services, including Azure Data Factory, Azure SQL Data Warehouse, and Synapse Analytics.\nDeep expertise in Databricks, including Spark, Delta Lake.\nStrong understanding of data architecture principles and best practices.\nProven track record of leading large-scale data projects and initiatives.\nHands-on experience with Oracle to Azure Data Lake migrations.\nExcellent communication and collaboration skills.\n\nPreferred Skill Sets\n\n\nAzure Solutions Architect Expert certification Databricks Certification.\nDatabricks Certification.\n\nYears Of Experience Required\n\n\n7 to 10 years\n\nEducation Qualification\n\nGraduate Engineer or Management Graduate\n\nEducation (if blank, degree and/or field of study not specified)\n\nDegrees/Field of Study required: Bachelor Degree\n\nDegrees/Field Of Study Preferred\n\nCertifications (if blank, certifications not specified)\n\nRequired Skills\n\nMicrosoft Azure\n\nOptional Skills\n\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment + 21 more\n\nDesired Languages (If blank, desired languages not specified)\n\nTravel Requirements\n\nNot Specified\n\nAvailable for Work Visa Sponsorship\n\nNo\n\nGovernment Clearance Required\n\nNo\n\nJob Posting End Date","data architecture principles, Azure SQL Data Warehouse, Synapse Analytics, Delta Lake, Azure Data Services, Oracle to Azure Data Lake migrations, Azure Data Factory, Spark, Databricks"
Data and Analytics - Architect,Schneider Electric,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"Company: Schneider Electric\nDepartment: Supply Chain Analytics\nJob Role: Solution Architect\nLocation: Bangalore\nExperience & Education: Around 8-12 years of experience\nSUMMARY OF JOB:\n1. Design data model or Expand existing data models with new features based on business feedback\n2. Collaborate with the Global and Regional based data integration team to acquire expanded data from source systems\n3. Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality\n4. Acquiring data from primary or secondary data sources and maintaining databases\n5. Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality\n6. Work with the larger Growth Technology Systems team to help develop and uphold data governance policies and procedures to ensure standardized data naming, establish consistent data definitions and monitor overall data quality for assigned data entities\n7. Filter and clean data by reviewing reports and performance indicators to locate and correct code problems\n8. Analyze and improve performance on existing data models including query optimization, DAX optimization, and processing optimization (partitions).\n9. Build and enhance SQL Server Analysis Services data models\n10. Interpreting data, analyzing results using statistical techniques\n11. Develop or assist in the development of SSRS, Excel, Tableau and Power BI reports\n12. Develop required process documentation and adhere to security compliance\nQualifications\nPrimary Skills:\n1. Good business analytical skills\n2. Deep understanding of dimensional modeling, OLTP and OLAP database designs and implementation strategies\n3. Data Warehousing concepts, architecture & schemas\n4. Familiarity with ETL tools such as Microsoft SSIS/DTS, Data Stage, Informatica Power Center, Informatica Cloud\n5. Experience in providing solutions for BI & DW environment\n6. Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)\n7. Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc)\n8. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\n9. Good communication skills both written and verbal\n10. Experience in Manufacturing sector\n11. Familiarity with Agile methodologies such as Scrum\nPreferred Knowledge:\n1. Exposure to data visualization/BI\n2. AWS\nESSENTIAL FUNCTIONS:\n1. DW solutioning\n2. Understanding business requirements, source the data, data model design\n3. Assess Data Quality, Data Integration, Migration & Modelling\n4. Handle end to end development\n5. Frequent client interaction\nSchedule: Full-timeReq: 00974M","OLTP and OLAP database designs, Data Warehousing Concepts, Xml, Javascript, Dax, Dimensional Modeling, Sql Server Analysis Services"
Associate Architect-Data Engineer & AI,Fission Computer Labs Private Limited,10-13 Years,,Hyderabad,Software,"Roles and Responsibilities\nAssociate Architect- Data Engineer is responsible for overseeing the design, development, and management of data infrastructure and pipelines within an organization. This role involves a mix of technical leadership, project management, and collaboration with other teams to ensure the efficient collection, storage, processing, and analysis of large datasets. The Lead Data Engineer typically manages a team of data engineers, architects, and analysts, ensuring that data workflows are scalable, reliable, and meet the business's requirements.\nResponsibilities:\nLead the design, development, and maintenance of data pipelines and ETL processes\narchitect and implement scalable data solutions using Databricks and AWS.\nOptimize data storage and retrieval systems using Rockset, Clickhouse, and CrateDB.\nDevelop and maintain data APIs using FastAPI.\nOrchestrate and automate data workflows using Airflow.\nCollaborate with data scientists and analysts to support their data needs.\nEnsure data quality, security, and compliance across all data systems.\nMentor junior data engineers and promote best practices in data engineering.\nEvaluate and implement new data technologies to improve the data infrastructure.\nParticipate in cross-functional projects and provide technical leadership.\nManage and optimize data storage solutions using AWS S3, implementing best practices for data lakes and data warehouses.\nImplement and manage Databricks Unity Catalog for centralized data governance and access control across the organization.\nQualifications Required\nBachelor's or Master's degree in Computer Science, Engineering, or related field\n10+ years of experience in data engineering, with at least 6 years in a lead role\nStrong proficiency in Python, PySpark, and SQL\nExtensive experience with Databricks and AWS cloud services\nHands-on experience with Airflow for workflow orchestration\nFamiliarity with FastAPI for building high-performance APIs\nExperience with columnar databases like Rockset, Clickhouse, and CrateDB\nSolid understanding of data modeling, data warehousing, and ETL processes\nExperience with version control systems (e.g., Git) and CI/CD pipelines\nExcellent problem-solving skills and ability to work in a fast-paced environment\nStrong communication skills and ability to work effectively in cross-functional teams\nKnowledge of data governance, security, and compliance best practices\nProficiency in designing and implementing data lake architectures using AWS S3\nExperience with Databricks Unity Catalog or similar data governance and metadata\nmanagement tools\nSkills and Experience Required\nPreferred Qualifications:\nExperience with real-time data processing and streaming technologies\nFamiliarity with machine learning workflows and MLOps\nCertifications in Databricks, AWS\nExperience implementing data mesh or data fabric architectures\nKnowledge of data lineage and metadata management best practices\nTech Stack\nDatabricks, Python, PySpark, SQL, Airflow, FastAPI, AWS (S3, IAM, ECR, Lambda), Rockset, Clickhouse, CrateDB\nWhy you'll love working with us:\nOpportunity to work on business challenges from top global clientele with high impact.\nVast opportunities for self-development, including online university access and sponsored certifications.\nSponsored Tech Talks, industry events & seminars to foster innovation and learning.\nGenerous benefits package including health insurance, retirement benefits, flexible work hours, and more.\nSupportive work environment with forums to explore passions beyond work.\nThis role presents an exciting opportunity for a motivated individual to contribute to the development of cutting-edge solutions while advancing their career in a dynamic and collaborative environment.","Data Analysis, Machine Learning, Cloud Services, Databricks, Python, Pyspark, Data Pipeline"
"Senior Solutions Acceleration Architect, Data",Google Inc,10-15 Years,,"Gurugram, Bengaluru, Mumbai",Software,"Minimum qualifications:\nBachelor's degree in Computer Science, a related technical field, or equivalent practical experience.\n10 years of experience in cloud computing, with a focus on data architecture, data analytics, and data engineering.\nExperience with modern application development and DevOps practices, including CI/CD, containerization (Docker, Kubernetes), and infrastructure-as-code.\nExperience in programming/scripting languages such as Python, with developing and deploying data solutions.\nExperience in architecting and developing software or infrastructure for scalable, distributed systems.\nExperience with database technologies (SQL, NoSQL), streaming data, and data warehousing solutions.\nPreferred qualifications:\n10 years of experience in cloud computing, with a focus on data architecture, data analytics, and data engineering, in a customer-facing or consulting role.\nExperience in understanding customer requirements, breaking them down into actionable components, and designing technical architectures to meet those needs.\nExperience managing stakeholder expectations and building consensus around complex technical projects.\nAbility to analyze complex business problems and develop innovative technical solutions leveraging GenAI and data.\nAbility to build rapid prototypes and proof-of-concepts to demonstrate innovative solutions.\nAbility to communicate complex technical concepts effectively to both technical and non-technical audiences.\nResponsibilities:\nLead the design, development, and iterative refinement of data-centric and AI-powered solutions on Google Cloud Platform (GCP), showcasing the potential of data and AI to address specific business needs.\nCollaborate with customers to understand their business challenges, technical requirements, and objectives. Build strong, trusted relationships with customers and stakeholders.\nMentor and guide junior team members, fostering a culture of innovation and continuous learning.\nEstablish and promote innovative best practices and methodologies for data-driven solutions, actively contributing to industry thought leadership through publications, presentations, and community engagement.","Data Analytics, Application Development, data engineering, Data Architecture, Devops, Cloud Computing"
Solution Architect (Digital/Data practice),Reflections Info Systems,6-11 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Chennai",IT Management,"We are seeking a highly experiencedSolution Architect - Digitalto join our team. This role is pivotal in designing and implementing innovative digital solutions that meet both business objectives and customer needs.\nThe ideal candidate will possess a deep understanding of modern technologies, includingmicroservices architecture, mobile development (Android/iOS), data BI, backend systems, infrastructure, and DevOps.\nResponsibilities include:\n1. Architecture Design\nLead the design of end-to-end digital solutions that align with business objectives.\nDevelop and maintain architectural blueprints, design patterns, and best practices for digital transformation.\nEnsure solutions are scalable, secure, and meet performance standards, with a strong focus on microservices architecture (preferably in Python).\n2. Technical Leadership\nProvide guidance and leadership to development teams throughout the project lifecycle.\nCollaborate with business leaders, developers, and other architects to ensure alignment with business goals.\nStay updated on emerging technologies, trends, and best practices in digital architecture, mobile development, and DevOps.\n3. Mobile and Backend Development\nDesign and implement solutions involving mobile native technologies forAndroid and iOS platforms.\nEnsure smooth integration betweenfront-end and back-end systems, optimizing for performance, security, and scalability.\n4. Data, BI, and 3rd Party Integrations\nLead the integration ofdata analyticsandbusiness intelligence (BI)solutions within the digital architecture.\nManage 3rd party integrations to enhance digital functionality.\nUtilize BI tools to provide actionable insights for decision-making.\n5. UI/UX and Customer Experience\nCollaborate withUI/UX designersto create intuitive, seamless user experiences.\nEnsure digital solutions align with customer expectations, enhancing engagement and satisfaction.\n6. Infrastructure and DevOps\nOversee the design and management of infrastructure components to ensure they are robust, scalable, and secure.\nImplementDevOpspractices for efficient development, deployment, and maintenance.\n7. Enterprise-Grade Software and Team Collaboration\nBring experience from working withenterprise-grade softwareto ensure the reliability and performance of solutions.\nFoster a strong, collaborative team environment, promoting effective communication and knowledge sharing.\nAct as a thought leader and trusted advisor, building strong relationships with customers and stakeholders.\n8. Project Incubation and Delivery\nSupportproject incubation, helping to prepare teams for successful project execution.\nReview dashboards and reporting to ensure alignment with key deliverables and progress tracking.\nAssist in addressing customer escalations and build customer confidence.\n9. Productivity and Reusability\nCreate reusableassets, templates, and toolsto drive productivity improvements.\nMonetize in-house developed accelerators and support key project implementations.\n10. Mentoring and Training\nConductskills gap assessmentsand provide technology mentoring for team members.\nDeliver training on key technologies and architectural best practices.\nPrimary Skills :\nBachelor s degreein computer science, Information Technology, Engineering, or a related field.\nMinimum ofX yearsof experience insolution architecture, with a focus ondigital technologies.\nStrong expertise inmicroservices architecture(preferably Python) andmobile native Android/iOS development.\nProficiency indata and BI tools,3rd party integrations,UI/UX design, andbackend technologies.\nSolid understanding ofinfrastructure designandDevOps practices.\nExperience withenterprise-grade software and products.\nExcellent problem-solvingand decision-making skills, able to work under pressure.\nStrong communicationand interpersonal skills, able to convey complex technical concepts to non-technical stakeholders.\nAbility to buildstrong relationshipswith team members and customers, serving as a trusted advisor.\nHelp with project incubation and delivery preparedness\nCreate knowledge assets\nProject progress reviews-Help the accounts deliver successfully by engaging in complex projects or projects in Red/Amber status\nProvide solutioning for complex design requirements and problems\nReview sufficiency of dashboards and status reporting\nAlign tech leads for key deliverables reviews\nHelp with addressing customer escalations and improve customer confidence\nAssist AMs with account mining\nOwn up productivity improvements by creating re-usable assetstemplates, in-house developed tools/accelerators, monetize the accelerators\nSupport key project implementations\nSkills gap assessment\nImpart trainings on key technologies\nTechnology mentoring\nKey Competencies\nAnalytical Thinking:Ability to analyze complex problems and design appropriate solutions.\nStrategic Vision:Understands the long-term objectives and aligns digital solutions accordingly.\nCollaboration:Strong team player, able to work effectively with cross-functional teams.\nInnovation:Stays ahead of technological trends and integrates innovative solutions into architecture designs.\nAdaptability:Quickly adapts to evolving business needs and emerging technologies.","Business intelligence, Analytical, Ios Development, Android, Front End, Python"
Presales Solutions Architect - Modern Data Center,AHEAD,5-10 Years,,Gurugram,Information Technology,"The AHEAD Modern Datacenter Specialist Solutions Engineer (SSE) will be focused on Datacenter technologies, includingstorage (block & file), compute, virtualization, and data protection. The SSE is considered a subject matter expert in these areas with responsibility for selling and designing complex solutions. The SSE is also considered an organizational thought leader for their specialty area.\nResponsibilities:\nActive participation in the AHEAD Modern Datacenter SSE team, advisory and delivery teams.\nWorking with the AHEAD partner management team and directly with selected strategic vendors to maintain technical and sales relationships.\nBe a thought leader within the Solutions Engineering organization, responsible for designing, architecting, and at times consulting for AHEAD clients.\nBuild skills, relationships, and industry credibility across many segments to drive product, services, and consulting sales.\nSupport the sales teams during engagements by providing advice and solutions or proposals optimized to meet customer technical and business requirements.\nDevelop relationships with other Modern Datacenter SSE team members, leaders, and partners in support of sales objectives.\nStrategize and execute technical sales calls.\nComplete required pre-sales documentation (configurations, pricing, services, presentations, justifications, etc.) quickly and accurately.\nQualify sales opportunities in terms of customer technical requirements, decision-making process, and funding.\nPresent and market the design and value of proposed AHEAD solutions and business justification to customers, prospects, and AHEAD management.\nParticipate in the mentorship of entry-level team members.\nPossess strong, detailed product/technology/industry knowledge.\nKnowledge of job-associated software and applications.\nQualifications:\nExpertise in Storage (Block & File), Compute, and Virtualization/VMware Cloud Foundation (VCF).\nData Protection experience with Dell, Rubrik, Commvault, etc.\nFamiliarity with Cisco UCS, Dell MX, PowerScale, PowerFlex and hyperconverged compute platforms (VxRail, Nutanix, Azure Stack HCI, OpenShift-V).\nUnderstanding of Datacenter automation tools like Ansible.\nBasic knowledge of cloud platforms like AWS and Azure.\nVMware and Virtualization technology expertise.\nVendor experience with Dell, NetApp, Vast Data, Cisco, etc.\nProfessional communication, presentation, analytical, and problem-solving skills\nExperience working as a technical lead in a pre-sales or sales campaign.\nAbility to work under critical conditions and influence others to achieve results.\nStrong interpersonal skills with excellent presentation skills.\nMust be independent, self-motivated, a self-starter, and possess a good working\nattitude.\nAble to work well within a team and partner environment. Customer-focused and results-driven.\nOther Skills:\nInnovation Mindset: Ability to identify and leverage emerging Datacenter technologies to drive innovative solutions that meet evolving client needs.\nAutomation Expertise: Advanced understanding of automation frameworks and scripting to optimize Datacenter operations and reduce manual effort.\nProblem Solving:Strong analytical skills with a focus on troubleshooting complex Datacenter environments, including storage, compute, and networking issues.\nConsultative Approach:Ability to engage with clients as a trusted advisor, guiding them through the decision-making process with clear, strategic recommendations.\nCollaboration Skills:Proven track record of working effectively across teams, including sales, engineering, and vendor partners, to deliver cohesive solutions.\nContinuous Learning: Commitment to staying updated on the latest industry trends, certifications, and best practices related to Datacenter technologies.\nSecurity Awareness:Knowledge of Datacenter security best practices, ensuring that proposed solutions adhere to compliance and regulatory standards.","Compute, VCF, VMware, Storage, Ansible, Data Protection"
Solution Architect (Digital/Data practice),Reflections Info Systems,10-14 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Chennai",IT Management,"We are seeking a highly experiencedSolution Architect - Digitalto join our team. This role is pivotal in designing and implementing innovative digital solutions that meet both business objectives and customer needs.\nThe ideal candidate will possess a deep understanding of modern technologies, includingmicroservices architecture, mobile development (Android/iOS), data & BI, backend systems, infrastructure, and DevOps.\nResponsibilities include:\n1. Architecture Design\nLead the design of end-to-end digital solutions that align with business objectives.\nDevelop and maintain architectural blueprints, design patterns, and best practices for digital transformation.\nEnsure solutions are scalable, secure, and meet performance standards, with a strong focus on microservices architecture (preferably in Python).\n2. Technical Leadership\nProvide guidance and leadership to development teams throughout the project lifecycle.\nCollaborate with business leaders, developers, and other architects to ensure alignment with business goals.\nStay updated on emerging technologies, trends, and best practices in digital architecture, mobile development, and DevOps.\n3. Mobile and Backend Development\nDesign and implement solutions involving mobile native technologies forAndroid and iOS platforms.\nEnsure smooth integration betweenfront-end and back-end systems, optimizing for performance, security, and scalability.\n4. Data, BI, and 3rd Party Integrations\nLead the integration ofdata analyticsandbusiness intelligence (BI)solutions within the digital architecture.\nManage 3rd party integrations to enhance digital functionality.\nUtilize BI tools to provide actionable insights for decision-making.\n5. UI/UX and Customer Experience\nCollaborate withUI/UX designersto create intuitive, seamless user experiences.\nEnsure digital solutions align with customer expectations, enhancing engagement and satisfaction.\n6. Infrastructure and DevOps\nOversee the design and management of infrastructure components to ensure they are robust, scalable, and secure.\nImplementDevOpspractices for efficient development, deployment, and maintenance.\n7. Enterprise-Grade Software and Team Collaboration\nBring experience from working withenterprise-grade softwareto ensure the reliability and performance of solutions.\nFoster a strong, collaborative team environment, promoting effective communication and knowledge sharing.\nAct as a thought leader and trusted advisor, building strong relationships with customers and stakeholders.\n8. Project Incubation and Delivery\nSupportproject incubation, helping to prepare teams for successful project execution.\nReview dashboards and reporting to ensure alignment with key deliverables and progress tracking.\nAssist in addressing customer escalations and build customer confidence.\n9. Productivity and Reusability\nCreate reusableassets, templates, and toolsto drive productivity improvements.\nMonetize in-house developed accelerators and support key project implementations.\n10. Mentoring and Training\nConductskills gap assessmentsand provide technology mentoring for team members.\nDeliver training on key technologies and architectural best practices.\nPrimary Skills :\nBachelor s degreein computer science, Information Technology, Engineering, or a related field.\nMinimum of 10yearsof experience insolution architecture, with a focus ondigital technologies.\nStrong expertise inmicroservices architecture(preferably Python) andmobile native Android/iOS development.\nProficiency indata and BI tools,3rd party integrations,UI/UX design, andbackend technologies.\nSolid understanding ofinfrastructure designandDevOps practices.\nExperience withenterprise-grade software and products.\nExcellent problem-solvingand decision-making skills, able to work under pressure.\nStrong communicationand interpersonal skills, able to convey complex technical concepts to non-technical stakeholders.\nAbility to buildstrong relationshipswith team members and customers, serving as a trusted advisor.\nHelp with project incubation and delivery preparedness\nCreate knowledge assets\nProject progress reviews-Help the accounts deliver successfully by engaging in complex projects or projects in Red/Amber status\nProvide solutioning for complex design requirements and problems\nReview sufficiency of dashboards and status reporting\nAlign tech leads for . This is to notify jobseekers that some fraudsters are promising jobs with Reflections Info Systems for a fee. Please note that no payment is ever sought for jobs in Reflections. We contact our candidates only through our official website or LinkedIn and all employment related mails are sent through the official HR email id. for any clarification/ alerts on this subject.","Business intelligence, Analytical, Ios Development, Android, Front End, Python"
Presales Solutions Architect - Modern Data Center,AHEAD,6-10 Years,,Gurugram,Information Technology,"Modern Datacenter Specialist Solutions Engineer (SSE) Datacenter Technologies\nThe AHEAD Modern Datacenter Specialist Solutions Engineer (SSE) will be focused on Datacenter technologies, including storage (block & file), compute, virtualization, and data protection. The SSE is considered a subject matter expert in these areas with responsibility for selling and designing complex solutions. The SSE is also considered an organizational thought leader for their specialty area.\nResponsibilities\nActive participation in the AHEAD Modern Datacenter SSE team, advisory and delivery teams\nWorking with the AHEAD partner management team and directly with selected strategic vendors to maintain technical and sales relationships\nBe a thought leader within the Solutions Engineering organization, responsible for designing, architecting, and at times consulting for AHEAD clients\nBuild skills, relationships, and industry credibility across many segments to drive product, services, and consulting sales\nSupport the sales teams during engagements by providing advice and solutions or proposals optimized to meet customer technical and business requirements\nDevelop relationships with other Modern Datacenter SSE team members, leaders, and partners in support of sales objectives\nStrategize and execute technical sales calls\nComplete required pre-sales documentation (configurations, pricing, services, presentations, justifications, etc.) quickly and accurately\nQualify sales opportunities in terms of customer technical requirements, decision-making process, and funding\nPresent and market the design and value of proposed AHEAD solutions and business justification to customers, prospects, and AHEAD management\nParticipate in the mentorship of entry-level team members\nPossess strong, detailed product/technology/industry knowledge\nKnowledge of job-associated software and applications\nQualifications\nExpertise in Storage (Block & File), Compute, and Virtualization/VMware Cloud Foundation (VCF)\nData Protection experience with Dell, Rubrik, Commvault, etc.\nFamiliarity with Cisco UCS, Dell MX, PowerScale, PowerFlex and hyperconverged compute platforms (VxRail, Nutanix, Azure Stack HCI, OpenShift-V)\nUnderstanding of Datacenter automation tools like Ansible\nBasic knowledge of cloud platforms like AWS and Azure\nVMware and Virtualization technology expertise\nVendor experience with Dell, NetApp, Vast Data, Cisco, etc.\nProfessional communication, presentation, analytical, and problem-solving skills\nExperience working as a technical lead in a pre-sales or sales campaign\nAbility to work under critical conditions and influence others to achieve results\nStrong interpersonal skills with excellent presentation skills\nMust be independent, self-motivated, a self-starter, and possess a good working attitude\nAble to work well within a team and partner environment\nCustomer-focused and results-driven\nOther Skills\nInnovation Mindset: Ability to identify and leverage emerging Datacenter technologies to drive innovative solutions that meet evolving client needs\nAutomation Expertise: Advanced understanding of automation frameworks and scripting to optimize Datacenter operations and reduce manual effort\nProblem Solving: Strong analytical skills with a focus on troubleshooting complex Datacenter environments, including storage, compute, and networking issues\nConsultative Approach: Ability to engage with clients as a trusted advisor, guiding them through the decision-making process with clear, strategic recommendations\nCollaboration Skills\nProven track record of working effectively across teams, including sales, engineering, and vendor partners, to deliver cohesive solutions\nContinuous Learning: Commitment to staying updated on the latest industry trends, certifications, and best practices related to Datacenter technologies\nSecurity Awareness: Knowledge of Datacenter security best practices, ensuring that proposed solutions adhere to compliance and regulatory standards","Compute, vmware, virtualization, Storage, Dell, Ansible"
Data Engineer - Solution Architect,IOMETE,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"About Us\nIOMETE is the self-hosted data lakehouse platform for the age of AI and is pioneering a new approach to data management for complex enterprise data infrastructure environments with large data sets. While SaaS solutions may offer convenience for smaller organizations, larger enterprises often face one-size-fits-all-rigidity, vendor lock-in, data leaks and runaway costs. IOMETE provides a better alternative. No data ever leaves the customer's security boundary, providing enterprises full control over their data at significantly lower costs. Ideal for highly regulated industries like Financial Services, Healthcare, Government and Technology. Built with leading-edge technology like Apache Spark and Apache Iceberg,\nJob Summary\nAt IOMETE, we're seeking a skilled Data Engineer Solution Architect to join our team and work closely with the support organization of one of our Fortune 50 customers. While you'll be on IOMETE's payroll, you will operate day-to-day as an integrated part of the customer's internal teamusing their systems, tools, and processes to provide exceptional support and drive user success.\nIn this role, you'll work directly with the customer's user organizationresolving support tickets, creating user-facing documentation, and delivering training to ensure smooth adoption and usage. You'll act as a technical expert and advisor, helping the customer navigate complex data workflows and get the most out of IOMETE's platform.\nYou will also collaborate closely with IOMETE's product and engineering teamsbringing back insights from the field to help prioritize bug fixes, shape feature development, and improve the overall customer experience.\nThis is a high-impact role that combines deep technical expertise with direct customer interactionideal for someone who enjoys working at the intersection of engineering and user success.\nQualifications\nStrong proficiency in PySpark for distributed data processing and transformation.\nSolid experience with SQL for querying and managing large datasets.\nHands-on experience with at least one modern data warehouse platform such as Snowflake, Databricks, or BigQuery.\nProficient in Python programming for data manipulation, automation, and building ETL pipelines.\nProven experience in designing, developing, and maintaining robust ETL (Extract, Transform, Load) workflows.\nFamiliarity with data modeling, data integration techniques, and performance optimization.\nAbility to work with large-scale structured and unstructured data.\nExperience with version control systems (e.g., Git) and CI/CD practices.\nKnowledge of workflow orchestration tools (e.g., Airflow, Prefect, Dagster) is a plus.\nStrong problem-solving and analytical skills with attention to detail.\nExcellent communication skills and ability to collaborate with cross-functional teams.\nRequirements\nLocation: For this role we are seeking candidates that are located in India only, preferably in the Bangalore area.\nMinimum 5 years of relevant experience.\nWhat We Offer\nExciting projects and challenges.\nOpportunity to work with cutting-edge technology.\nCollaborative and innovative work environment.\nCompetitive compensation and stock options.\nThis is a contracting role.\nCompensation\nMonthly compensation ranging from $4,000 to $6,000, commensurate with experience and qualifications.","workflow orchestration tools, Airflow, data integration techniques, Prefect, snowflake, Dagster, performance optimization, BigQuery, Pyspark, Data Modeling, Sql, Git, Version Control Systems, Databricks, Python, Etl"
Senior Solution Architect - Data Governance Experience-Collibra,Cortex Consultants LLC,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Solution Architect\n\nExp-15+\n\nLocation-Multiple\n\nImmediate to 15days\n\nSkill-data gov,colibra,architect\n\nOverview: The Senior Solution Architect - Data Governance Experience-Collibra plays a critical role in shaping our organization's data governance framework. This position requires an individual who has extensive experience with Collibra and a strong understanding of data governance principles. The architect will be pivotal in designing and implementing data governance solutions that enhance data quality, compliance, and usability across the enterprise. By leading initiatives that foster a culture of data stewardship, this role ensures that our data assets are effectively managed and leveraged to drive business outcomes. The Senior Solution Architect will collaborate closely with various stakeholders, including IT, compliance, and business units, to ensure alignment with the organization's strategic objectives. This position not only focuses on technical capabilities but also emphasizes the importance of change management and user adoption to ensure long-term success in our data governance journey.\n\nLead the architecture and implementation of Collibra to support data governance initiatives.\nDevelop a comprehensive data governance framework that aligns with organizational goals.\nEngage with stakeholders to gather requirements and translate them into actionable solutions.\nEnsure compliance with data regulations and industry standards during implementation.\nDesign and maintain data models that enhance data discoverability and usability.\nCollaborate with data stewards and business units to promote a culture of data stewardship.\nConduct assessments of existing data management practices and recommend improvements.\nProvide training and support for users to maximize the effectiveness of Collibra.\nFacilitate workshops and meetings to drive consensus around data governance practices.\nMonitor and report on data governance metrics to measure the success of initiatives.\nStay current with industry trends and advancements in data governance technology.\nSupport change management efforts to drive user adoption of data governance solutions.\nDevelop and implement data stewardship policies and procedures.\nWork closely with IT to ensure seamless integration of Collibra with existing systems.\nServe as a subject matter expert for data governance best practices within the organization.\n\nRequired Qualifications\n\nBachelor's degree in Computer Science, Information Technology, or a related field.\nAt least 7 years of experience in data governance, data management, or a related area.\nProven experience as a Solution Architect, with a focus on data governance solutions.\nExtensive hands-on experience with Collibra, including implementation and configuration.\nStrong understanding of data governance principles, frameworks, and best practices.\nExperience working with data models, metadata management, and data quality assessments.\nKnowledge of regulatory compliance requirements such as GDPR, CCPA, or HIPAA.\nAbility to engage and communicate effectively with stakeholders at all levels.\nStrong analytical and problem-solving skills to address data-related challenges.\nProficient in project management methodologies, with experience leading cross-functional teams.\nExperience in change management processes to facilitate user adoption.\nStrong presentation skills with the ability to convey complex concepts to non-technical audiences.\nCertification in data governance or relevant frameworks is a plus.\nExperience with cloud-based data platforms and integration tools.\nAbility to work in a fast-paced, dynamic environment with competing priorities.\n\nSkills: regulatory compliance,project management,data stewardship,stakeholder engagement,data governance,metadata management,data quality assessments,data management,change management,collibra,data models,analytical thinking,solution architecture","data quality assessments, Regulatory Compliance, data stewardship, Analytical Thinking, project management, Data Governance, Metadata Management, Collibra, solution architecture, Data Management, change management"
Principal Professional Services Architect (Data Loss Prevention),Zscaler,10-12 Years,,India,Login to check your skill match score,"About Zscaler\n\nServing thousands of enterprise customers around the world including 40% of Fortune 500 companies, Zscaler (NASDAQ: ZS) was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. As the operator of the world's largest security cloud, Zscaler accelerates digital transformation so enterprises can be more agile, efficient, resilient, and secure. The pioneering, AI-powered Zscaler Zero Trust Exchange platform, which is found in our SASE and SSE offerings, protects thousands of enterprise customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.\n\nNamed a Best Workplace in Technology by Fortune and others, Zscaler fosters an inclusive and supportive culture that is home to some of the brightest minds in the industry. If you thrive in an environment that is fast-paced and collaborative, and you are passionate about building and innovating for the greater good, come make your next move with Zscaler.\n\nAt Zscaler, our Customer Success Organization is a global, customer-focused team dedicated to delivering high-impact experiences and identifying innovative solutions. We leverage valuable data and research to provide expert, hands-on support starting from the implementation phase and beyond, ensuring customers achieve their goals and leverage our technology to its fullest potential. Together, we create a customer-centric culture that fosters success, adoption, and continuous growth.\n\nResponsibilities\n\nWe're looking for an experienced Principal Professional Services Architect, Data Loss Prevention to join our Professional Services team. Reporting to the Senior Manager, Professional Services, you'll be responsible for:\n\nBuilding long-term, trusted advisor relationships with customers to deliver tailored Data Protection solutions\nDesigning and executing solutions using Zscaler's suite of products, including DLP, CASB, Shadow IT, and SaaS integrations\nAdvocating customer needs with cross-functional teams to influence product development\nProducing documentation, best practices, and demos to enable organizational growth\n\nWhat We're Looking for (Minimum Qualifications)\n\n\n10+ years of professional experience in network security, professional services, or DevOps\nMinimum Bachelor's degree in Computer Science, Computer/Electrical Engineering, or related field\nMust be proficient in Data Loss Prevention (DLP), CASB, and cloud security solutions\nProven ability with scripting languages, REST APIs, and Linux/Windows systems\n\nWhat Will Make you Stand Out (Preferred Qualifications)\n\n\nCertifications such as CISSP, CISM, or CCSP\nAdvanced expertise in network security architecture and SaaS integration\nRelevant industry experience at leading networking security companies\n\nAt Zscaler, we believe that diversity drives innovation, productivity, and success. We are looking for individuals from all backgrounds and identities to join our team and contribute to our mission to make doing business seamless and secure. We are guided by these principles as we create a representative and impactful team, and a culture where everyone belongs. For more information on our commitments to Diversity, Equity, Inclusion, and Belonging, visit the Corporate Responsibility page of our website.\n\nBenefits\n\nOur Benefits program is one of the most important ways we support our employees. Zscaler proudly offers comprehensive and inclusive benefits to meet the diverse needs of our employees and their families throughout their life stages, including:\n\nVarious health plans\nTime off plans for vacation and sick time\nParental leave options\nRetirement options\nEducation reimbursement\nIn-office perks, and more!\n\nBy applying for this role, you adhere to applicable laws, regulations, and Zscaler policies, including those related to security and privacy standards and guidelines.\n\nZscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws.\n\nSee more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link.\n\nPay Transparency\n\nZscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, click here .\n\nZscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support.","CASB, Cloud Security Solutions, Windows Systems, Linux, Scripting Languages, Rest Apis"
Data Engineer(Azure/Architect),VidPro Consultancy Services,8-12 Years,,"Chennai, India",Login to check your skill match score,"Experience: 8-12 years\n\nLocation: Bangalore, Chennai, Delhi, Pune, Kolkata\n\nWork Type : Full Time\n\nWork Mode : Hybrid\n\nMandatory Skills: Python/ PySpark / Data Engineer/Azure Databricks/Azure Data Factory/ETL/SQL/Architect\n\nPrimary Roles And Responsibilities\n\nDeveloping Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack\nAbility to provide solutions that are forward-thinking in data engineering and analytics space\nCollaborate with DW/BI leads to understand new ETL pipeline development requirements.\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop data model to fulfill reporting needs\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussion with client architect and team members\nOrchestrate the data pipelines in scheduler via Airflow\n\nSkills And Qualifications\n\nBachelor's and/or master's degree in computer science or equivalent experience.\nMust have total 6+ yrs. of IT experience and 3+ years experience in Data warehouse/ETL projects.\nDeep understanding of Star and Snowflake dimensional modelling.\nStrong knowledge of Data Management principles\nGood understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture\nShould have hands-on experience in SQL, Python and Spark (PySpark)\nCandidate must have experience in AWS/ Azure stack\nDesirable to have ETL with batch and streaming (Kinesis).\nExperience in building ETL / data warehouse transformation processes\nExperience with Apache Kafka for use with streaming data / event-based data\nExperience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)\nExperience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)\nExperience working with structured and unstructured data including imaging & geospatial data.\nExperience working in a Dev/Ops environment with tools such as Terraform, Circle CI, GIT.\nProficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot\nDatabricks Certified Data Engineer Associate/Professional Certification (Desirable).\nComfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects\nShould have experience working in Agile methodology\nStrong verbal and written communication skills.\nStrong analytical and problem-solving skills with a high attention to detail.\n\nSkills: etl,sql,projects,cassandra,mongodb,unix shell scripting,azure synapses,azure data factory,data engineering,azure datafactory,data lakes,azure,data management,hadoop,rdbms,pipelines,neo4j,circle ci,azure databricks,aws,git,data engineer,data,apache kafka,pyspark,terraform,skills,python,data warehouse,architects,pl/sql","Circle CI, Hadoop, Cassandra, Pyspark, Azure Databricks, Sql, Azure Data Factory, Terraform, Neo4j, Unix Shell Scripting, Apache Kafka, MongoDB, Python, AWS, Etl"
Associate Architect Data Historian,Merck Group,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Work Your Magic with us!\n\nReady to explore, break barriers, and discover more We know you've got big plans so do we! Our colleagues across the globe love innovating with science and technology to enrich people's lives with our solutions in Healthcare, Life Science, and Electronics. Together, we dream big and are passionate about caring for our rich mix of people, customers, patients, and planet. That's why we are always looking for curious minds that see themselves imagining the unimaginable with us.\n\nYour role\n\nIn your role, you manage the technical implementation of the Merck AVEVA PI (former OSI PI) data historian programs.\n\nYou actively prepare architecture schemas and data flow diagrams, and you explain and defend the selected approach in respective discussions in front of infrastructure teams and architectural boards. You contribute to selection of implementation partners, and you actively manage external implementation teams and their delivery. You actively contribute to the optimization of the operating model for running programs and services. You provide implementation effort, cost estimation, resource planning and everything required from a work-package owner of a larger program. You act as first point of contact for technical implementation questions raised by our business stakeholders, as well as architectural guide to the implementation teams. Internally, you align and coordinate your activities with the service delivery managers and support them with the required compliance documentation. You manage the project to service hand-over, organize knowledge transfer and define training for the local IT service team members.\n\nWho You Are\n\nYou have 10 + years of experience in responsible role in IT project delivery for global companies. You are technical expert for AVEVA PI (OSI PI) and you have a very good technical understanding of the surrounding technologies, such as SCADA, OPCs, Kepware, OEE and others. During your career you acted as functional team lead and technical instructor for an implementation team. You know how it feels to face technical issues in systems which operate 24/7 and you have resolved them in responsible position. You are assertive, being able to define and implement technical standards within a group of developers. You worked in the Healthcare industry, where you got used to comply to GxP guidelines and regulations. You strategically plan the future and express upcoming operative demands in front of your manager. Last not least, you are looking for a long-term engagement and an office-based local working place in a dynamic team with the perspective of growth and development.\n\nWhat we offer: We are curious minds that come from a broad range of backgrounds, perspectives, and life experiences. We celebrate all dimensions of diversity and believe that it drives excellence and innovation, strengthening our ability to lead in science and technology. We are committed to creating access and opportunities for all to develop and grow at your own pace. Join us in building a culture of inclusion and belonging that impacts millions and empowers everyone to work their magic and champion human progress!\n\nApply now and become a part of our diverse team!","Oee, AVEVA PI, OPCs, Kepware, SCADA, Osi Pi"
Senior Architect - Data Integration Technology,WSAudiology,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"Driven by the passion to improve quality of people's lives, WS Audiology continues to grow as market leader in the hearing aid industry. With our commitment to increase penetration in an underserved hearing care market, we want to accelerate our business transformation in order to reach more people, more effectively.\n\nWe are looking for a Data Integration Technology Architect for our newly created stream for IPASS. This role leads technically & Functionally our strategic project for data integration between applications using a Platform as a Service.\n\nWhat you will do\n\nLead Technically & Functionally our strategic project for data integration between appplication using a Platform as a Service.\n\nCollaborate with external vendors and developers, partner with internal stakeholders to create the best possible data product.\n\nAnalyze business requirements and prepare specifications for product development.\n\nCreate and design architecture and developer framework package for rollout.\n\nShare Knowledge and create awareness around the new platform\n\nCollaboration with Global IT functions on requirements and adoption of IPAAS & Management of external consultants.\n\nWhat you bring\n\n7+ years of relevant experience in relational DBs: SQL Server, Oracle, Mysql, NoSQL(Mongo DB etc)\nMulesoft or Workato experience preferred\nExtensive knowledge on API driven Development\nExtensive Cloud Knowledge (Azure, AWS , Google)\nExtensive Knowledge on Streaming Applications such as Kafka / Flink\nExtensive knowledge for Event-Driven Architecture vs API (REST, SOAP) for microservices.\nKnowledge on DevOps & CI/CD Topics such as Containerization (Docker, Kubernetes), Jenkinks / Gitlab for CI/CD and versioning control.\nKnowledge on data encryption, network protocols and general ACL for cloud integration.\nUniversity degree in Computer Science, Information Technology or similar streams.\n\nWho we are\n\nAt WS Audiology, we provide innovative hearing aids and hearing health services.\n\nTogether with our 12,000 colleagues in 130 countries, we invite you to help unlock human potential by bringing back hearing for millions of people around the world.\n\nWith us, you will become part of a truly global company where we care for one another, welcome diversity and celebrate our successes.\n\nSounds wonderful We can't wait to hear from you.\n\nWS Audiology is an equal-opportunity employer and committed to creating an inclusive employee experience for all. Regardless of race, color, religion, national origin, age, sex, gender, gender identity, gender expression, sexual orientation, marital status, medical condition, ancestry, disability, military or veteran status we firmly believe that our work is at its best when everyone feels free to be their most authentic self.","Event-Driven Architecture, Cloud Knowledge, relational DBs, Flink, Streaming Applications, CI CD, API driven Development, data encryption, API REST, Google, containerization, general ACL, Kafka, Microservices, Nosql, Docker, MySQL, Oracle, AWS, SQL Server, Mulesoft, Soap, Network Protocols, Workato, Devops, Gitlab, Azure, Kubernetes"
Architect - Data Center,Bechtel Corporation,6-8 Years,,India,Login to check your skill match score,"Requisition ID: 282943\n\nRelocation Authorized: National - Family\nTelework Type: Full-Time Office/Project\nWork Location: Various Bechtel Project Locations\n\nExtraordinary Teams Building Inspiring Projects\n\nSince 1898, we have helped customers complete more than 25,000 projects in 160 countries on all seven continents that have created jobs, grown economies, improved the resiliency of the world's infrastructure, increased access to energy, resources, and vital services, and made the world a safer, cleaner place.\n\nDifferentiated by the quality of our people and our relentless drive to deliver the most successful outcomes, we align our capabilities to our customers objectives to create a lasting positive impact. We serve the Infrastructure; Nuclear, Security & Environmental; Energy; Mining & Metals, and the Manufacturing and Technology markets. Our services span from initial planning and investment, through start-up and operations.\n\nCore to Bechtel is our Vision, Values and Commitments . They are what we believe, what customers can expect, and how we deliver. Learn more about our extraordinary teams building inspiring projects in our Impact Report .\n\nBechtel India is a global operation that supports execution of projects and services around the world. Working seamlessly with business line home offices, project sites, customer organizations and suppliers, our teams have delivered more than 125 projects since our inception in 1994.\n\nOur offices in Gurgaon, Vadodara and Chennai will grow significantly and sustainably with exciting career opportunities for both professionals and young graduates who are passionate about creating a cleaner, greener, and safer world; building transformational infrastructure; making decarbonization a reality; and protecting people and the environment.\n\nJob Summary\n\nArchitect with more than 6 years of experience in Manufacturing and Infrastructure projects (with relevant experience in Data center) projects in a design office of repute. A candidate who has completed at least 1 data center project shall be given more weightage. Should have exposure to design office practices and familiarity with computer aided design and 3D modeling tools like Revit.\n\nMajor Responsibilities\n\nShall perform / supervise calculations, prepare material requisitions, service requisitions, code analysis, standard details, technical specifications and quantity take-offs and provide input for design drawings during all phases of projects.\nShall check / review the drawings prepared by the designers.\nShall assist in the development of basic layout drawings.\nShall lead conceptual studies and inter-disciplinary reviews\nShall create perspectives and presentation of design to client.\nProvide technical training.\nPerform feasibility studies for site development, building configuration, climate studies.\n\nEducation And Experience Requirements\n\nMinimum 5-year degree in Architecture from an accredited college or university.\nCandidate with Master's degree is desirable.\nProfessional license from a recognized licensing board and/or LEED certification shall be of added advantage.\nExperience of making Architectural presentation shall be an added advantage.\nLevel I: 6 - 8 years of relevant work experience\nLevel II: 8 - 10 years of relevant work experience\n\nRequired Knowledge And Skills\n\nKnowledge of architectural techniques and design principles with a basic knowledge of the types of data center, data hall layout, mechanical systems, electrical systems, and security systems needed to design a data center.\nKnowledge of precedents and latest trends in architectural engineering, the principles and practices of related technical areas.\nKnowledge of Engineering Procedures and design guides.\nThorough knowledge of the roles played by other engineering disciplines on projects.\nKnowledge of regulatory Indian and International codes and standards, industry practices and design criteria pertinent to architectural engineering design, including fire life safety codes.\nKnowledge of constructability and applicable standards and codes\nProficiency in the use of Revit, Navisworks and exposure to BIM is essential.\nProficient in selection of material from constructability and total installed cost perspective.\nSkill in oral and written communication.\nShould be proficient in using MS office tools.\nProved ability of managing a team of architects and designers will be an added advantage.\nPrevious experience of Data center design including administration building, process/ non process buildings and other ancillary facilities like maintenance building, guard house, substation, electrical buildings, pump house, etc.\nKnowledge of master planning and fire life safety design guidelines\nGood knowledge of faade design and material selection (including interior & exterior finishes)\nExperience in working on EPC projects shall be an added advantage.\n\nTotal Rewards/Benefits\n\nFor decades, Bechtel has worked to inspire the next generation of employees and beyond! Because our teams face some of the world's toughest challenges, we offer robust benefits to ensure our people thrive. Whether it is advancing careers, delivering programs to enhance our culture, or providing time to recharge, Bechtel has the benefits to build a legacy of sustainable growth. Learn more at Bechtel Total Rewards\n\nDiverse Teams Build The Extraordinary\n\nAs a global company, Bechtel has long been home to a vibrant multitude of nationalities, cultures, ethnicities, and life experiences. This diversity has made us a more trusted partner, more effective problem solvers and innovators, and a more attractive destination for leading talent.\n\nWe are committed to being a company where every colleague feels that they belong-where colleagues feel part of One Team, respected and rewarded for what they bring, supported in pursuing their goals, invested in our values and purpose, and treated equitably. Click here to learn more about the people who power our legacy.\n\nBechtel is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity and expression, age, national origin, disability, citizenship status (except as authorized by law), protected veteran status, genetic information, and any other characteristic protected by federal, state or local law. Applicants with a disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to [HIDDEN TEXT]","Fire life safety codes, Bim, Security Systems, Mechanical Systems, Material Selection, Navisworks, Architectural techniques, Design Principles, Revit, Electrical Systems, data center design, Ms Office"
MDM Architect- Data Governance,Fractal,Fresher,,"Bengaluru, India",Login to check your skill match score,"Responsibilities\nYou will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management\nLead the end-to-end design, architecture, and implementation of MDM solutions\nDefine and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.\nCollaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.\nDesign data models and hierarchies for Customer, Product, Vendor, and other master domains.\nDevelop and operationalize Data Governance frameworks aligned to business and compliance needs.\nEnable data stewardship workflows, match & merge rules, and exception management.\nIntegrate MDM systems with upstream and downstream applications across the enterprise.\nLead workshops and training sessions on MDM and Data Governance for client teams.\nSupport RFPs, proposals, and client presentations with MDM/DG expertise\nAssess, validate and implement MDM architecture for on-prem, cloud and hybrid environments with expertise in MDM solutions like Oracle MDM solutions (ERP, CDH, CDM), SAP MDG(S4/Hana, ERP), Informatica IDMC and other modern MDM solutions.\nDevelop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.\nCollaborate with stakeholders to define and implement MDM strategies, standards, and operating models.\nDevelop and enforce end-to-end master data lifecycle processes including data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.\nCollaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.\nSupport leadership in designing thought leadership, publish POV's/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.\nGood to Have\nHands-on implementation experience with on-prem MDM and ERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG, SAP S4/HANA, Reltio, Stibo or hybrid deployments with other modern MDM, ERP and CRM platforms.\nStrong knowledge of reference data management and data stewardship workflows.\nExperience with ETL/ELT tools and integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).\nFamiliarity with data governance tools (e.g., Collibra, Alation, Informatica CDGC, etc.) and DQ tools (e.g., Informatica IDQ, CDQ, etc.).\nBasic understanding of data governance best practices, data quality management, data privacy regulations\nFamiliarity with Agentic AI, machine learning, and analytics technologies\nKnowledge of SQL, PL/SQl, Python, or any other database\nExcellent communication and interpersonal skills to collaborate effectively with clients and internal teams.","ELT tools, Informatica CDI, S4 Hana ERP, Data Governance frameworks, Oracle MDM solutions, Alation, Informatica IDMC, Data stewardship workflows, Informatica CDGC, Metadata Management, Collibra, Pl Sql, Oracle Goldengate, Sql, Data Quality, Informatica Idq, Sap Mdg, Mulesoft, Python"
Chief Architect - Data & AI,Orion Innovation,Fresher,,India,Login to check your skill match score,"Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.\n\nWe are seeking a dynamic and experienced leader for our Data Architecture and Data Science Practice. This role will be instrumental in shaping our organization's data strategy, driving innovation through advanced analytics, and ensuring robust data architecture to support our business objectives.\n\nResponsibilities\n\nStrategic Leadership: Develop and implement data strategies that align with organizational goals and objectives. Drive innovation and efficiency through the effective use of data.\nTeam Management: Lead and mentor a team of data architects, data engineers, and data scientists. Provide guidance and support to foster professional growth and collaboration within the team.\nData Architecture: Designing and maintaining scalable and efficient solutions to ensure data integrity, availability, and security across an organization's infrastructure. This includes translating business requirements into logical and physical data models, ensuring data is transformed correctly from source to target systems through detailed mapping, and converting business models into a comprehensive data platform. Data integration combines data from various sources into a unified view using ETL/ELT processes, data pipelines, and APIs, centralizing storage in data lakes and warehouses. An audit framework tracks and monitors data activities to ensure compliance and transparency, while scheduling and monitoring tools ensure data processes run smoothly and on time. Adhering to Service Level Agreements (SLAs) involves defining SLAs, tracking key metrics, managing incidents efficiently, and providing regular reports to stakeholders. This holistic approach supports business needs, ensures data quality, and maintains operational efficiency.\nAdvanced Analytics: Oversee the development and implementation of advanced analytics techniques, including machine learning, predictive modeling, and optimization algorithms. Drive the adoption of best practices and methodologies in data science.\nStakeholder Collaboration: Collaborate with stakeholders across the organization to understand business requirements and priorities. Translate business needs into data initiatives and deliver actionable insights to drive decision-making.\nTechnology Evaluation: Stay updated on emerging technologies and trends in data management and analytics. Evaluate new tools, platforms, and methodologies to enhance the organization's data capabilities.\nGovernance and Compliance: Establish and enforce data governance policies and procedures to ensure regulatory compliance, data privacy, and security. Implement best practices for data quality management and data lineage tracking.\nPerformance Monitoring: Define key performance indicators (KPIs) to measure the effectiveness of data practices. Monitor performance metrics, analyze trends, and identify areas for improvement.\n\nQualifications\n\n\nBachelor's or Master's degree in Computer Science, Data Science, Information Systems, or a related field.\nStrong leadership experience in data architecture, data engineering, or data science.\nIn-depth knowledge of data architecture principles, data modeling techniques, and database technologies.\nProficiency in programming languages such as Python, R, SQL, etc.\nStrong communication skills with the ability to translate technical concepts into business terms.\nExperience working in a fast-paced environment and managing multiple priorities effectively\n\nOrion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.\n\nCandidate Privacy Policy\n\nOrion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, Orion, we Or us) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (Notice) Explains\n\nWhat information we collect during our application and recruitment process and why we collect it;\nHow we handle that information; and\nHow to access and update that information.\n\nYour use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.","Data lakes, R, Data architecture principles, Data pipelines, Data modeling techniques, Apis, Optimization Algorithms, Sql, ELT, Database Technologies, Machine Learning, Predictive Modeling, Python, Etl"
Software Architect (Data Engineering),Velotio Technologies,7-9 Years,,"Pune, India",Login to check your skill match score,"About Velotio:\n\nVelotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.\n\nWe are looking for an experienced Software Architect with deep expertise in Data Engineering and proficiency in Ruby on Rails (RoR). The ideal candidate will lead architectural decisions, optimize data pipelines, and ensure scalable, efficient, and high-performance data processing solutions. While Data Engineering skills are the primary focus, RoR experience would be a valuable addition.\n\nRequirements\n\n7+ years of experience in software development, with a focus on data engineering and cloud architectures\nStrong experience with Snowflake, DBT, and Data Pipeline design\nExpertise in Kafka, Argo, Kubernetes, and ML Flow\nDeep understanding of AWS services, particularly Lambdas, Step Functions, Serverless, and SageMaker\nSolid Python development skills for data engineering and backend services\nExperience with Terraform for infrastructure automation\nStrong architectural experience in designing scalable, fault-tolerant, and distributed systems\n\nNice-to-Have (RoR Focused) Skills:\n\n\nExperience with Ruby on Rails (RoR) for API development and backend services\nFamiliarity with Sidekiq, MySQL, Redis, and performance tuning in a RoR environment\nUnderstanding of CI/CD pipelines, DevOps best practices, and microservices architecture\n\nResponsibilities :\n\n\nDesign and implement scalable, high-performance data architectures in cloud environments (AWS preferred)\nDrive best practices for data pipeline design, integration, and orchestration using technologies like DBT, Kafka, Argo, and Kubernetes\nDefine and implement data governance, security, and compliance best practices\nProvide technical guidance on integrating MLFlow, SageMaker, and other AI/ML solutions into data pipelines\nBuild and maintain data pipelines using Snowflake, DBT, Kafka, and AWS serverless services (Lambdas, Step Functions)\nEnsure optimal performance, scalability, and cost-effectiveness of data workflows\nUtilize Terraform for infrastructure as code to manage cloud environments efficiently\nWork with Kubernetes for deploying, scaling, and managing applications\nDevelop and optimize data-driven applications leveraging Python\nCollaborate with software teams to integrate data pipelines into RoR applications\nImplement and optimize background job processing using Sidekiq, Redis, and MySQL\nEnsure scalability, reliability, and maintainability of services\n\nBenefits\n\n\nOur Culture:\n\nWe have an autonomous and empowered work culture encouraging individuals to take ownership and grow quickly\nFlat hierarchy with fast decision making and a startup-oriented get things done culture\nA strong, fun & positive environment with regular celebrations of our success. We pride ourselves in creating an inclusive, diverse & authentic environment\n\nAt Velotio, we embrace diversity. Inclusion is a priority for us, and we are eager to foster an environment where everyone feels valued. We welcome applications regardless of ethnicity or cultural background, age, gender, nationality, religion, disability or sexual orientation.","Serverless, snowflake, ML Flow, SageMaker, Step Functions, DevOps best practices, Sidekiq, dbt, microservices architecture, Data Pipeline design, Lambdas, Argo, Aws Services, data engineering, Kafka, Redis, MySQL, Python, Kubernetes, Terraform"
Solutions Architect - Data Governance,Informatica,7-9 Years,,"Bengaluru, India",IT/Computers - Software,"Build Your Career at Informatica\nWe seek innovative thinkers who believe in the power of data to drive meaningful change. At Informatica, we welcome adventurous, work-from-anywhere minds eager to tackle the world's most complex challenges. Our employees are empowered to push their bold ideas forward, and we are united by a shared passion for using data to do the extraordinary for each other and the world.\nUse a sales CRM to manage the sales pipeline and record information on prospects.Solutions Architect - Data Governance\nWe're looking for an Solutions Architect candidate with experience in IDMC Cloud Data Governance and Catalogue (CDGC and Metadata Command Centre), Cloud Marketplace, Cloud Data Quality and Profiling, Claire GPT, to join our team in Bangalore / Hybrid work schedule.\nYou will report to the Senior Manager, Customer Success Architect.\nYou will work in Pre-sales or Post-sales consulting role in Enterprise software solutions. Customer Success Management (CSM) organisation focusing on our Cloud-First Cloud-Native and Data 4.0 strategy.\nTechnology You'll Use\nIDMC Cloud Data Governance and Catalog (CDGC and Metadata Command Centre), Cloud Marketplace, Cloud Data Quality and Profiling, Claire GPT\nYour Role Responsibilities Here's What You'll Do\nYou would provide architecture and design, use cases solution, and solution implementation advice.\nYou would also work with our Professional Services team and have a seamless handoff for broader service engagements. Partner with Product/Engineering Teams to understand the best recommendations to design a solution OR provide comprehensive feedback to them to better align our product to customer needs.\nDeliver compelling architectural blueprints, best practices, expert sessions, and scoped implementations to influence the strategic direction of customer adoption and lead customers through solution design for our SAAS products.\nCollaborate with Customer Support and Engineering/Product Management.\nManage customer relationships, engaging them in value-added activities.\nExperience using a CRM software system.\nWhat We'd Like to See\nPromote the value proposition in presentations and demos with prospects and customers.\nCompose and publish external facing whitepapers, artefacts, case studies, architectural blueprints, blog posts, and articles for technical/industry publications.\nRole Essentials\nBachelor's degree in computer engineering/Technology\n7+ years of work experience.\nPerks & Benefits\nComprehensive health, vision, and wellness benefits (Paid parental leave, adoption benefits, life insurance, disability insurance and 401k plan or international pension/retirement plans\nFlexible time-off policy and hybrid working practices\nEquity opportunities and an employee stock purchase program (ESPP)\nComprehensive Mental Health and Employee Assistance Program (EAP) benefit\nOur DATA values are our north star and we are passionate about building and delivering solutions that accelerate data innovations. At Informatica, our employees are our greatest competitive advantage. So, if your experience aligns but doesn't exactly match every qualification, apply anyway. You may be exactly who we need to fuel our future with innovative ideas and a thriving culture.\nInformatica (NYSE: INFA), a leader in enterprise AI-powered cloud data management, brings data and AI to life by empowering businesses to realize the transformative power of their most critical assets. We pioneered the Informatica Intelligent Data Management Cloud that manages data across any multi-cloud, hybrid system, democratizing data to advance business strategies. Customers in approximately 100 countries and more than 80 of the Fortune 100 rely on Informatica. . Connect with , , and . Informatica. Where data and AI come to life.","Profiling, Metadata Command Centre, crm software, Claire GPT, Cloud Marketplace, Catalogue CDGC, Cloud Data Quality, IDMC Cloud"
Solutions Architect - Data Integration,Informatica,7-9 Years,,"Bengaluru, India",IT/Computers - Software,"Build Your Career at Informatica\nWe seek innovative thinkers who believe in the power of data to drive meaningful change. At Informatica, we welcome adventurous, work-from-anywhere minds eager to tackle the world's most complex challenges. Our employees are empowered to push their bold ideas forward, and we are united by a shared passion for using data to do the extraordinary for each other and the world.\nSolutions Architect - Data Integration\nWe're looking for an Solutions Architect candidate with experience in Cloud Data integration, Cloud Application Integration, Informatica PowerCenter to join our team in Bangalore., to join our team in Bangalore / Hybrid work schedule.\nYou will report to the Manager, Customer Success Architect.\nYou will work in Pre-sales or Post-sales consulting role in Enterprise software solutions. Customer Success Management (CSM) organization focusing on our Cloud-First Cloud-Native and Data 4.0 strategy.\nTechnology You'll Use\nCloud Data integration, Cloud Application Integration, Informatica PowerCenter\nYour Role Responsibilities Here's What You'll Do\nYou would provide architecture and design, use cases solution, and solution implementation advice.\nYou would also work with our Professional Services team and have a seamless handoff for broader service engagements. Partner with Product/Engineering Teams to understand the best recommendations to design a solution OR provide comprehensive feedback to them to better align our product to customer needs.\nDeliver compelling architectural blueprints, best practices, expert sessions, and scoped implementations to influence the strategic direction of customer adoption and lead customers through solution design for our SAAS products.\nCollaborate with Customer Support and Engineering/Product Management.\nManage customer relationships, engaging them in value-added activities.\nExperience using a CRM software system.\nUse a sales CRM to manage the sales pipeline and record information on prospects.\nWhat We'd Like to See\nPromote the value proposition in presentations and demos with prospects and customers.\nCompose and publish external facing whitepapers, artefacts, case studies, architectural blueprints, blog posts, and articles for technical/industry publications.\nRole Essentials\nBachelor's degree in computer engineering/Technology\n7+ years of work experience.\nPerks & Benefits\nComprehensive health, vision, and wellness benefits (Paid parental leave, adoption benefits, life insurance, disability insurance and 401k plan or international pension/retirement plans\nFlexible time-off policy and hybrid working practices\nEquity opportunities and an employee stock purchase program (ESPP)\nComprehensive Mental Health and Employee Assistance Program (EAP) benefit\nOur DATA values are our north star and we are passionate about building and delivering solutions that accelerate data innovations. At Informatica, our employees are our greatest competitive advantage. So, if your experience aligns but doesn't exactly match every qualification, apply anyway. You may be exactly who we need to fuel our future with innovative ideas and a thriving culture.\nInformatica (NYSE: INFA), a leader in enterprise AI-powered cloud data management, brings data and AI to life by empowering businesses to realize the transformative power of their most critical assets. We pioneered the Informatica Intelligent Data Management Cloud that manages data across any multi-cloud, hybrid system, democratizing data to advance business strategies. Customers in approximately 100 countries and more than 80 of the Fortune 100 rely on Informatica. . Connect with , , and . Informatica. Where data and AI come to life.","Cloud Application Integration, crm software, Cloud Data integration, Informatica Powercenter"
"Senior Manager, Principal Solution Architect - Clinical Data Ecosystem",Bristol Myers Squibb,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Working with Us\n\nChallenging. Meaningful. Life-changing. Those aren't words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You'll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more careers.bms.com/working-with-us .\n\nPosition Summary\n\nAt BMS, digital innovation and Information Technology are central to our vision of transforming patients lives through science. To accelerate our ability to serve patients around the world, we must unleash the power of technology. We are committed to being at the forefront of transforming the way medicine is made and delivered by harnessing the power of computer and data science, artificial intelligence, and other technologies to promote scientific discovery, faster decision making, and enhanced patient care.\n\nPrincipal Solution Architect - Clinical Data Ecosystem will develop solution architectures for Clinical Data Ecosystem Drug Development business processes utilizing and other related technologies to help enable BMS to deliver medicines to patients faster. The incumbent will work closely with IT product teams including business stakeholders in Hyderabad and globally to analyze business and technology requirements and define, prototype, and scale automation solutions. As a Principal Solution Architect for Clinical Data Ecosystem IT based out of our BMS Hyderabad you are part of the Drug Development IT team that delivers, platform, data and analytics capabilities for GDD Global Biostatistics and Data Sciences, Clinical Data Management (Clinical Analytics, Site Selection, Feasibility, Real World Evidence). This position will require the incumbent to demonstrate effective teamwork, collaboration, and communication across IT and business stakeholders.\n\nIf you want an exciting and rewarding career that is meaningful, consider joining our diverse team!\n\nKey Responsibilities\n\nWork with Drug Development teams to identify opportunities for automation and recommend the appropriate technology solutions that meet their requirements\nDevelop and maintain architecture diagrams, technical specifications, and other documentation to ensure that technology solutions are scalable, secure, and maintainable\nDeveloping data architecture strategy Working with stakeholders to define the overall strategy for the organization's data architecture. This includes defining data structures, metadata, data models, and data integration processes.\nPartner with data scientists to develop GenAI and LLM-powered applications (Chat with Data), leveraging techniques like AI Agents, Agentic architectures, RAG, fine-tuning, and vector embeddings to deliver high-quality data discovery & consumption features.\nDeep understanding of the GenAI Tech Stack, including its architecture, components, and capabilities.\nDevelop and Maintain CI/CD workflows and tools for efficient code and infrastructure deployment.\nDefining data product strategy Working with stakeholders to define the overall strategy for the organization's data products. This involves understanding business goals, identifying data sources, and determining the appropriate technology stack.\nDesigning data products Creating conceptual, logical, and physical data models for the organization's data products. This includes defining data structures, schema, and data processing pipelines. Designing data systems Developing and implementing data systems that support the organization's business needs. This may involve working with various technologies, including data warehouses, data lakes, data marts, and data APIs.\nData modeling Developing logical and physical data models that reflect the organization's data needs. This includes defining data elements, data relationships, data flows, and data transformation rules.\nData governance Developing and implementing data governance policies and procedures to ensure that data is accurate, consistent, and secure. This includes defining data quality standards, data security protocols, and data privacy policies.\nCollaborating with stakeholders Working closely with various stakeholders, including data scientists, software developers, business analysts, and project managers, to ensure that data is being used effectively and efficiently across the organization.\nStaying up-to-date with industry trends Keeping up-to-date with the latest trends and advancements in data architecture and technology, and applying this knowledge to enhance the organization's data systems and processes.\nOptimize data storage and retrieval to ensure efficient performance and scalability\nCollaborate with data analysts and data scientists to understand their data needs and ensure that the data infrastructure supports their requirements\nImplement and maintain security protocols to protect sensitive data\nClosely partner with the Enterprise Data and Analytics Platform team, other functional data pods and Data Community lead to shape and adopt data and technology strategy.\nServes as the Subject Matter Expert on GDD Data & Analytics Solutions and build domain knowledge of the GDD specific area\nHas End to End ownership mindset in driving initiatives through completion\nComfortable working in a fast-paced environment with minimal oversight\nMentors other team members effectively to unlock full potential\nPrior experience working in an Agile/Product based environment\nProvides strategic feedback to vendors on service delivery and balances workload with vendor teams\n\nQualifications & Experience\n\n10-12 years of hands-on experience working on implementing, architecting and operating data capabilities and cutting-edge data solutions, preferably in a cloud environment. Breadth of experience in technology capabilities that span the full life cycle of data management including data lakehouses, master/reference data management, data quality and analytics/AI ML is needed.\nBachelor's degree in Computer Science, Information Technology, Life Sciences, or a related field. Advanced degree preferred.\nTake Accountability and Ownership delivering business outcomes within time and budgets.\nAt least 7 years data engineering experience in designing and building complex data solutions\nHands-on experience developing and delivering data, ETL solutions with some of the technologies like AWS data services (Glue, Redshift, Athena, lakeformation, etc.), BI is a plus\nCreate and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.\nIn-depth expertise with data security, storage solutions, database virtualization and replication as well as other complex database/data technical tools and solutions.\nIdentify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nStrong programming skills in languages such as Python, R, PySpark etc.\nExperience with SQL and database technologies such as MySQL, PostgreSQL etc.\nExperience with cloud-based data technologies such as AWS, Azure, or Google Cloud Platform\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills Functional knowledge in regulated environments or prior experience in Lifesciences Research and Development domain is a plus\nExperience and expertise in establishing agile and product-oriented teams that work effectively with teams in US and other global BMS site.\nInitiates challenging opportunities that build strong capabilities for self and team\nDemonstrates a focus on improving processes, structures, and knowledge within the team. Leads in analyzing current states, deliver strong recommendations in understanding complexity in the environment, and the ability to execute to bring complex solutions to completion.\n\nIf you come across a role that intrigues you but doesn't perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\n\nWith a single vision as inspiring as Transforming patients lives through science , every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\n\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role\n\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to [HIDDEN TEXT] . Visit careers.bms.com/ eeo -accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information https //careers.bms.com/california-residents/\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.","GenAI, data lakes, R, AWS data services, LLM-powered applications, data APIs, Data Security, Pyspark, PostgreSQL, Data Architecture, Data Modeling, Sql, data engineering, Etl Solutions, MySQL, Data Governance, data warehouses, Data Integration, Python"
Data Analytics Lead (Associate Architect),Toolify Private Limited,15-20 Years,,Bengaluru,"Recruiting, Staffing Agency","Data Analytics Lead (Associate Architect)\nLocation: Bengaluru, India\nExperience 15+ Years\nJoining- Immediate to 15 Days\nSummary\nToolify is seeking a visionary Data Analytics Lead to guide and inspire our data delivery team supporting a capital markets client specializing in private markets and alternative investment strategies. This role requires a blend of technical expertise, leadership acumen, and domain experience in capital markets and private markets.\nKey Responsibilities\nTeam Leadership & Delivery Excellence:\nLead a cross-functional team comprising data architects, analysts, business SMEs, and technologists to deliver high-impact data analytics solutions.\nDefine and enforce best practices for efficient, scalable, and high-quality delivery.\nInspire a culture of collaboration, accountability, and continuous improvement within the team.\nStrategic Data Leadership:\nDevelop and execute a data strategy aligned with client business objectives, ensuring seamless integration of analytics into decision-making processes.\nCollaborate with stakeholders to translate business needs into actionable data solutions, influencing strategic decisions.\nTechnical and Architectural Expertise:\nArchitect and oversee data platforms, including SQL Server, Snowflake, and Power BI, ensuring optimal performance, scalability, and governance.\nLead initiatives in Data Architecture, Data Modeling, and Data Warehouse (DWH) development, tailored to alternative investment strategies.\nEvaluate emerging technologies, such as big data and advanced analytics tools, and recommend their integration into client solutions.\nChampion data quality, integrity, and security, aligning with compliance standards in private equity and alternative investments.\nPerformance & Metrics:\nDefine and monitor KPIs to measure team performance and project success, ensuring timely delivery and measurable impact.\nCollaborate with stakeholders to refine reporting, dashboarding, and visualization for decision support.\nGovernance & Compliance:\nEstablish robust data governance frameworks in partnership with client stakeholders.\nEnsure adherence to regulatory requirements impacting private markets investments, including fund accounting and compliance\nWhat's on offer\nCompetitive and above-market salary.\nFlexible hybrid work schedule with tools for both office and remote productivity.\nHands-on exposure to cutting-edge technology and global financial markets.\nOpportunity to collaborate directly with international teams in New York and London.\nCandidate Profile\nExperience:\n15+ years of progressive experience in program or project management within the capital markets and financial services sectors.\nDemonstrated expertise in SQL Server, Snowflake, Power BI, ETL processes, and Azure Cloud Data Platforms.\nHands-on experience with big data technologies and modern data architecture.\nProven track record in delivering projects emphasizing data quality, integrity, and accuracy.\nDeep understanding of private markets, including areas such as private equity, private credit, CLOs, compliance, and regulations governing alternative investments.\nLeadership & Collaboration:\nExceptional problem-solving skills and decision-making abilities in high-pressure, dynamic environments.\nExperience leading multi-disciplinary teams to deliver large-scale data initiatives.\nStrong client engagement and communication skills, fostering alignment and trust with stakeholders.\nPreferred Certifications:\nRelevant certifications (e.g., CFA, Snowflake Certified Architect, or Microsoft Power BI Certified).\nEducation\nBachelor's degree in computer science, IT, Finance, Economics, or a related discipline. Advanced degrees (MBA, MS in Computer Science, or related fields) preferred.","Architect, Snowflake Certified Architect, snowflake, Azure Cloud Data Platforms, CFA, Power Bi, SQL Server"
Architect - Enterprise Data Operations,Pepsico india,8-16 Years,,Hyderabad,Food and Beverage,"Responsibilities\nComplete conceptual, logical and physical data models for any supported platform, including SQL Data Warehouse, EMR, Spark, DataBricks, Snowflake, Azure Synapse or other Cloud data warehousing technologies.\nGoverns data design/modeling documentation of metadata (business definitions of entities and attributes) and constructions database objects, for baseline and investment funded projects, as assigned.\nProvides and/or supports data analysis, requirements gathering, solution development, and design reviews for enhancements to, or new, applications/reporting.\nSupports assigned project contractors (both on- & off-shore), orienting new contractors to standards, best practices, and tools.\nContributes to project cost estimates, working with senior members of team to evaluate the size and complexity of the changes or new development.\nEnsure physical and logical data models are designed with an extensible philosophy to support future, unknown use cases with minimal rework.\nDevelop a deep understanding of the business domain and enterprise technology inventory to craft a solution roadmap that achieves business objectives, maximizes reuse.\nPartner with IT, data engineering and other teams to ensure the enterprise data model incorporates key dimensions needed for the proper management: business and financial policies, security, local-market regulatory rules, consumer privacy by design principles (PII management) and all linked across fundamental identity foundations.\nDrive collaborative reviews of design, code, data, security features implementation performed by data engineers to drive data product development.\nAssist with data planning, sourcing, collection, profiling, and transformation.\nCreate Source To Target Mappings for ETL and BI developers.\nShow expertise for data at all levels: low-latency, relational, and unstructured data stores; analytical and data lakes; data str/cleansing.\nPartner with the Data Governance team to standardize their classification of unstructured data into standard structures for data discovery and action by business customers and stakeholders.\nSupport data lineage and mapping of source system data to canonical data stores for research, analysis and productization.\n\nQualifications\n8+ years of overall technology experience that includes at least 4+ years of data modeling and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n4+ years of experience developing enterprise data models.\nExperience in building solutions in the retail or in the supply chain space.\nExpertise in data modeling tools (ER/Studio, Erwin, IDM/ARDM models).\nExperience with integration of multi cloud services (Azure) with on-premises technologies.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse, Teradata or SnowFlake.\nExperience with version control systems like Github and deployment & CI tools.\nExperience with Azure Data Factory, Databricks and Azure Machine learning is a plus.\nExperience of metadata management, data lineage, and data glossaries is a plus.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nFamiliarity with business intelligence tools (such as PowerBI).","snowflake, Data Operations, Data Warehousing, Data Analytics, Azure Databricks, Azure Machine Learning"
Architect - Fieldglass Data Analytics,Pepsico india,4-6 Years,,Hyderabad,Food and Beverage,"Responsibilities:\nLead team in the expansion new module of Fieldglass to roll out to all countries ensuring that all suppliers enroll and meet the expectations of program requirements\nLead the team to on time deployment of program expansion and meet project deadlines\nOversee the overall change management plan for effective communication to IT leadership and stakeholders globall\nResponsible for leading Fieldglass implementations and maintain program excellence in all countries\nLead the managed service provider supporting PepsiCo IT's external labor management program is following Pepsico Policies, Procedures, Contractual commitments and other requirements\nReport all results to Program lead and develop remediate plans\nLead the development and oversight of quarterly audits on the program to ensure policy and procedure adherence set forth in the program manual and ensure effective remediation plans are implemented where needed\nMonitor, evaluate and report on all relevant service and supplier performance metrics including contract SLAs and KPIs and remediation plans where needed\nWorking with IT leadership and key PepsiCo stakeholders on program feedback to continuously improve the user experience\nDevelop governance structure needed for overall strategic goal of supplier consolidation and configuration changes to ensure global process consistency\nEscalation point for all program related issues to support analyst\nLead and develop automation on existing processes for efficiency and cost savings\nManage all configuration changes globally to meet stakeholder needs\nDotted line manager experience with country analysts\nEnsure MSP is meeting contractual commitments and cost savings goals and remediate when needed\nReporting, Analytics & Insights\nDevelop and execute productivity initiative to meet team's goals\nLead development and design of analytics dashboards used to present data to executive committees and it leadership\nQualifications\nQualifications\nBachelor's Degree wit h9- 11 years of IT experience within or interacting with IT or Master's Degree with\n4-6 years demonstrated experience working with complex global commercial IT contracts and outsourcing constructs and agreements\n4-6 years demonstrated experience providing support to global Information Technology groups and organizations\n4-6 years demonstrated experience with supplier and/or client relationship management; including supplier performance and governance responsibilities\nAbility to interact with key stakeholders and communicate persuasively in a multi-functional environment\nAbility to deliver credible insights through work products and communications\nAbility to organize and prioritize work and meet deadlines through excellent time management and strong organizational and problem-solving skills\nAbility to work independently with little direction\nProfessional image and adherence to standards consistent with company policies and procedures\nExcellent analytical skills and attention to detail\nExcellent MS Office skills, in particular Excel, Word, and PowerPoint\nAbility to take instructions readily and to formulate work plans that will provide the best results to achieve the intended goals","fieldglass, Ms Office Skills, Msp, Reporting, Data Analytics"
Architect - Enterprise Data Operations,Pepsico india,12-18 Years,,Gurugram,Food and Beverage,"Responsibilities\nResponsibilities:Qualifications\nIndependently complete conceptual, logical and physical data models for any supported platform, including SQL Data Warehouse, EMR, Spark, Data Bricks, Snowflake, Azure Synapse or other Cloud data warehousing technologies.\nGoverns data design/modeling documentation of metadata (business definitions of entities and attributes) and constructions database objects, for baseline and investment funded projects, as assigned.\nProvides and/or supports data analysis, requirements gathering, solution development, and design reviews for enhancements to, or new, applications/reporting.\nSupports assigned project contractors (both on- & off-shore), orienting new contractors to standards, best practices, and tools.\nAdvocates existing Enterprise Data Design standards; assists in establishing and documenting new standards.\nContributes to project cost estimates, working with senior members of team to evaluate the size and complexity of the changes or new development.\nEnsure physical and logical data models are designed with an extensible philosophy to support future, unknown use cases with minimal rework.\nDevelop a deep understanding of the business domain and enterprise technology inventory to craft a solution roadmap that achieves business objectives, maximizes reuse.\nPartner with IT, data engineering and other teams to ensure the enterprise data model incorporates key dimensions needed for the proper management: business and financial policies, security, local-market regulatory rules, consumer privacy by design principles (PII management) and all linked across fundamental identity foundations.\nDrive collaborative reviews of design, code, data, security features implementation performed by data engineers to drive data product development.\nAssist with data planning, sourcing, collection, profiling, and transformation.\nCreate Source To Target Mappings for ETL and BI developers.\nShow expertise for data at all levels: low-latency, relational, and unstructured data stores; analytical and data lakes; data streaming (consumption/production), data in-transit.\nDevelop reusable data models based on cloud-centric, code-first approaches to data management and cleansing.\nPartner with the data science team to standardize their classification of unstructured data into standard structures for data discovery and action by business customers and stakeholders.\nSupport data lineage and mapping of source system data to canonical data stores for research, analysis and productization.\nQualifications:\n12+ years of overall technology experience that includes at least 6+ years of data modelling and systems architecture.\n6+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n6+ years of experience developing enterprise data models.\n6+ years in cloud data engineering experience in at least one cloud (Azure, AWS, GCP).\n6+ years of experience with building solutions in the retail or in the supply chain space.\nExpertise in data modelling tools (ER/Studio, Erwin, IDM/ARDM models).\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience scaling and managing a team of 5+ data modelers\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience with at least one MPP database technology such as Redshift, Synapse, Teradata, or Snowflake.\nExperience with version control systems like GitHub and deployment & CI tools.\nExperience with Azure Data Factory, Databricks and Azure Machine learning is a plus.\nExperience of metadata management, data lineage, and data glossaries is a plus.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nFamiliarity with business intelligence tools (such as PowerBI).\nSkills, Abilities, Knowledge\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring, hiring and scaling data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals\nAbility to lead others without direct authority in a matrixed environment.\nDifferentiating Competencies Required\nAbility to work with virtual teams (remote work locations); lead team of technical resources (employees and contractors) based in multiple locations across geographies\nLead technical discussions, driving clarity of complex issues/requirements to build robust solutions\nStrong communication skills to meet with business, understand sometimes ambiguous, needs, and translate to clear, aligned requirements\nAble to work independently with business partners to understand requirements quickly, perform analysis and lead the design review sessions.\nHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.\nPlaces the user in the center of decision making.\nTeams up and collaborates for speed, agility, and innovation.\nExperience with and embraces agile methodologies.\nStrong negotiation and decision-making skill.\nExperience managing and working with globally distributed teams.","enterprise data management, Data Modeling, Data Lake, System Architectecture, Data Warehousing, Data Analytics, Azure Cloud Services"
Architect - Enterprise Data Operations,Pepsico india,12-18 Years,,Gurugram,Food and Beverage,"Responsibilities\nResponsibilities:Qualifications\nIndependently complete conceptual, logical and physical data models for any supported platform, including SQL Data Warehouse, EMR, Spark, Data Bricks, Snowflake, Azure Synapse or other Cloud data warehousing technologies.\nGoverns data design/modeling documentation of metadata (business definitions of entities and attributes) and constructions database objects, for baseline and investment funded projects, as assigned.\nProvides and/or supports data analysis, requirements gathering, solution development, and design reviews for enhancements to, or new, applications/reporting.\nSupports assigned project contractors (both on- & off-shore), orienting new contractors to standards, best practices, and tools.\nAdvocates existing Enterprise Data Design standards; assists in establishing and documenting new standards.\nContributes to project cost estimates, working with senior members of team to evaluate the size and complexity of the changes or new development.\nEnsure physical and logical data models are designed with an extensible philosophy to support future, unknown use cases with minimal rework.\nDevelop a deep understanding of the business domain and enterprise technology inventory to craft a solution roadmap that achieves business objectives, maximizes reuse.\nPartner with IT, data engineering and other teams to ensure the enterprise data model incorporates key dimensions needed for the proper management: business and financial policies, security, local-market regulatory rules, consumer privacy by design principles (PII management) and all linked across fundamental identity foundations.\nDrive collaborative reviews of design, code, data, security features implementation performed by data engineers to drive data product development.\nAssist with data planning, sourcing, collection, profiling, and transformation.\nCreate Source To Target Mappings for ETL and BI developers.\nShow expertise for data at all levels: low-latency, relational, and unstructured data stores; analytical and data lakes; data streaming (consumption/production), data in-transit.\nDevelop reusable data models based on cloud-centric, code-first approaches to data management and cleansing.\nPartner with the data science team to standardize their classification of unstructured data into standard structures for data discovery and action by business customers and stakeholders.\nSupport data lineage and mapping of source system data to canonical data stores for research, analysis and productization.\nQualifications:\n12+ years of overall technology experience that includes at least 6+ years of data modelling and systems architecture.\n6+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n6+ years of experience developing enterprise data models.\n6+ years in cloud data engineering experience in at least one cloud (Azure, AWS, GCP).\n6+ years of experience with building solutions in the retail or in the supply chain space.\nExpertise in data modelling tools (ER/Studio, Erwin, IDM/ARDM models).\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience scaling and managing a team of 5+ data modelers\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience with at least one MPP database technology such as Redshift, Synapse, Teradata, or Snowflake.\nExperience with version control systems like GitHub and deployment & CI tools.\nExperience with Azure Data Factory, Databricks and Azure Machine learning is a plus.\nExperience of metadata management, data lineage, and data glossaries is a plus.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nFamiliarity with business intelligence tools (such as PowerBI).\nSkills, Abilities, Knowledge\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring, hiring and scaling data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals\nAbility to lead others without direct authority in a matrixed environment.\nDifferentiating Competencies Required\nAbility to work with virtual teams (remote work locations); lead team of technical resources (employees and contractors) based in multiple locations across geographies\nLead technical discussions, driving clarity of complex issues/requirements to build robust solutions\nStrong communication skills to meet with business, understand sometimes ambiguous, needs, and translate to clear, aligned requirements\nAble to work independently with business partners to understand requirements quickly, perform analysis and lead the design review sessions.\nHighly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.\nPlaces the user in the center of decision making.\nTeams up and collaborates for speed, agility, and innovation.\nExperience with and embraces agile methodologies.\nStrong negotiation and decision-making skill.\nExperience managing and working with globally distributed teams.","enterprise data management, Data Modeling, Data Lake, System Architectecture, Data Warehousing, Data Analytics, Azure Cloud Services"
Architect - Enterprise Data Operations,Pepsico india,4-14 Years,,Gurugram,Food and Beverage,"Responsibilities\nActive contributor to code development in projects and services.\nCollaborate with a cross-functional team of application developers, operations engineers, architects to understand complex product requirements and translate them into automated solutions that you build.\nCollaborate with colleagues to support and improve architecture, systems, processes, standards and tools.\nLead technical discussions to ensure solutions are designed for successful deployment, security, and high availability in the cloud\nDesign, implement, and maintain server, storage, network, and security infrastructure as code.\nBuild reusable pipelines for application deployments.\nWrite and maintain code for automating the creation of scalable/resilient systems/infrastructure with a focus on immutability and containers.\nDevelop, implement, and test automated data backup and recovery, and disaster recovery procedures across multiple regions.\nWrite and maintain clear, concise documentation, runbooks and operational standards including infrastructure diagrams.\nAssist development teams in the creation and understanding of automated application configurations.\nEnsure all solutions are properly monitored and instrumented.\nTroubleshoot and resolve complex issues in development, test and production environments.\nDesign and deploy scalable, highly available, and fault tolerant distributed systems.\nContinuously identify, adopt, & refine best practices.\nQualifications\nBachelor's Degree in Cyber Security, Information Technology, Computer Science or related field or related practical experience.\n4+ years of experience in Software and/or Infrastructure, with a desired 3+ years in a relevant cloud, Kubernetes, automation development, and/or orchestration positions.\n2+ years of hands-on experience on Azure leveraging number PAAS services offered by the platform.\nRequires excellent problem solving and analytic skills to effectively address the needs of customers, including experience handling problem escalations and notifications.\nExperience working in GCP, AWS, PCF, Azure, or other cloud-based technologies.\nExperience with Terraform, Ansible, Salt or similar automation tools are a benefit as we drive towards Infrastructure as Code (IaC).\nExperience with SCM and DevOps tool suites; examples include Git, Sonar, Jenkins, Artifactory, HashiCorp Packer etc.\nExperience with containers, docker, Kubernetes, serverless functions.\nExperience with Linux (RHEL/CentOS) and Windows system administration.\nProgramming / Scripting background with knowledge of Python, PowerShell, Groovy.\nHands-on experience with Azure services (Proficiency with Azure DevOps, ARM Templates, Azure Policy, Azure CLI, Azure Rest API).\nExperience provisioning, operating, monitoring, troubleshooting and maintaining systems running in the cloud.\nMulti-year experience in application development and configuration automation.\nUnderstanding of application, server, and network security.\nUnderstanding of immutable infrastructure and infrastructure as code concepts.\nWorking knowledge of Agile/Scrum, experience leading continuous integration and continuous delivery concepts and frameworks.\nExperience in Firewall and Load Balancing technology; Palo Alto, F5, Citrix Netscaler is a plus.\nCloud Certifications (Azure Solutions Architect, DevOps Engineer, or other cloud professional certifications) is a plus","enterprise data management, Gcp, Linux, Devops, Agile Scrum, Cloud Infrastructure, Python"
Architect - Salesforce Data Cloud,Pepsico india,3-7 Years,,Hyderabad,Food and Beverage,"Responsibilities\nOversee Salesforce Data Cloud environments across development, staging, and production.\nDefine best practices for environment setup, security, and governance.\nManage data pipelines, ingestion processes, and harmonization rules for efficient data flow.\nEstablish role-based access control (RBAC) to ensure data security and compliance.\nMonitor data processing jobs, ingestion performance, and data harmonization.\nEnsure compliance with GDPR, CCPA, and other data privacy regulations\nEstablish CI/CD pipelines using tools like Azure DevOps\nImplement version control and automated deployment strategies for Data Cloud configurations\nDefine a data refresh strategy for lower environments to maintain consistency.\n\nQualifications\nMandatory Technical Skills\nExtensive experience in setting up, maintaining, and troubleshooting CI/CD pipelines for Salesforce apps.\nStrong knowledge of Azure DevOps tools and pipeline creation, with proficiency in automation scripting (primarily YAML, with additional languages as needed).\nHands-on experience with SFDX, Azure Repos, and automated release deployments for Salesforce.\nExpertise in implementing GIT branching strategies using VS Code integrated with Salesforce CLI tool.\nMandatory Skills\nProficiency in Salesforce Data Cloud architecture and best practices.\nExperience with data lake, Snowflake, or cloud-based data storage solutions.\nFamiliarity with OAuth, authentication mechanisms, and data security standards.\nSalesforce Data Cloud Consultant Certification","Salesforce, snowflake, Cloud Architecture, Data Lake, Oauth, Yaml, Git, Cloud Data"
Data Management+DWH+Ananalytics+Architect exp+Synapse+ADF+ Data Migration,Coders Brain Technology Private Limited,12-16 Years,INR 20 - 30 LPA,"Bengaluru, Pune, Noida",Login to check your skill match score,"Job Description-\nPosition/Role- Data Management+DWH+Ananalytics+Architect exp+Synapse+ADF+ Data Migration\nExperience: 12-16 yrs\nLocation Chennai/Bangalore/Mumbai/Pune/Noida\nNotice Period- Immediate- 20 days\nRate- 30 lpa\nJob Overview-\nExperience: 12 to 15 years in Data Management, Data Warehousing, and Analytics.\nData Solutions: 4 to 5 years in architecting and implementing data solutions.\nAzure Expertise: 3 years in implementing data solutions using Azure Data Factory and MS Synapse.\nAzure Functions: Understanding of Azure Functions, including their operation and migration processes.\nProgramming: Proficiency in Python scripting and exposure to Java API development.\nAzure Synapse Analytics: 1 to 2 years of experience is a plus.\nADF Integration: Experience in installing and configuring Azure Data Factory integration runtimes and linked services.\nBig Data: Hands-on experience with big data platform tool selection\nData Migration: 2 years of experience in data migrations to Azure using Data Box or Data Migration Services.\nMicrosoft Fabric: Any commercial exposure to Fabric is highly desirable.","Ananalytics, Architect, exp, Synapse, Data Migration, Data Management, Dwh, Adf"
Data Platform and Infrastructure Architect,Arabelle Solutions,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Arabelle Solutions offers a broad portfolio of turbine island technologies and services that are used in more than a third of nuclear power plants globally helping customers across the world deliver reliable power as they transition to a lower-carbon future. The Arabelle steam turbine is the most advanced of its kind and the company provides turbine island lifecycle support solutions for all nuclear reactor types - improving power output, reducing environmental footprint, and lowering operational cost. Arabelle Solutions has around 3,300 employees across 16 countries and is a subsidiary of EDF Group.\n\nAt Arabelle Solutions, we're proud to design services and solutions that are generating power not just for today and tomorrow but for generations to come. You'll work alongside passionate bright minds. We offer a broad range of opportunities for those eager to build tomorrow's world. We believe a supportive culture is key to reach common goals. Diversity and an inclusive mindset makes us and our business stronger.\n\nwww.arabellesolutions.com\n\nData Platform And Infrastructure Architect\n\nWe are looking to onboard a Senior data platform architect as part of Arabelle Solutions Data & Analytics team to lead the architecture, administration and cost management of our data platforms. This role is a combination of technical leadership and hands-on technical expertise. The data platform architect will help define the overall data platform strategy, modernization efforts and will drive scalability, maintainability, cost management & monitoring initiatives.\n\nRoles And Responsibilities\n\nPrimarily responsible for the design, development, administration, and maintenance of the data platforms\nStrictly maintain access management, to ensure the integrity, security, and compliance of our data across all initiatives.\nBuild procedures and utilities to track performance and cost for all data engineering infrastructure, perform data platform administration, and enforce cost management best practices.\nInvestigate and incorporate modern data services and technologies to boost the functionality and efficiency of our data platform, in line with our strategic objectives.\nReview and suggest enhancements to our data solutions with a focus on cost efficiency and ease of use, making our data platform scalable, accessible and beneficial to a wide range of user groups.\nWork with the Data Architect to help build a next generation data architecture that is resilient to change, maintainable, and scalable\nEnsure data platforms adheres to EU data security & privacy practices, laws and regulations\nLead data platform upgrades and also enable High Availability (HA) and Disaster Recovery (DR) capabilities for the data platforms\nManage Infrastructure and Platform Security & Vulnerability\nResearch, recommend and implement the appropriate data platform tools, software, applications, and systems to support data technology goals.\nDesign and manage data platform improvement and growth projects.\nExecute platform configuration and performance tuning.\nIdentify inefficiencies and gaps in the current data platform and leverage solutions to ensure data standards\nDiagnose and work with IT and data personnel to resolve platform access and performance issues.\nInstall and configure relevant components to ensure data platform access.\nMonitor system details within the data platform, including platform storage and execution time, and implement efficiency improvements.\nActs as a team lead or project manager, directs the activities of matrixed project team members and contractors.\n\nMinimum Qualifications Required\n\nBachelor's Degree from an accredited university in a relevant technical field including, but not limited to computer science, computer engineering, information systems, mathematics, or systems engineering.\nMinimum 8 year(s) of experience in Data Platform architecture and managing multiple data platforms.\nTechnical experience with designing, building, installing, configuring, and supporting data platforms including Talend, HVR, AWS Aurora RDS PostgreSQL & Tableau.\nGood understanding of Cloud Infrastructure and networking (AWS or Google)\nGood knowledge of applicable data privacy practices, laws and regulations.\nExceptional analytical, conceptual, and problem-solving abilities.\nStrong written and oral communication skills.\nDemonstrated leadership ability in the data realm.\nStrong presentation and interpersonal skills.\nExperience working in a team-oriented, collaborative environment.\nDemonstrated ability to interpret technical information and apply it to corporate goals.\n\nThis is an LPB (Lead Professional Band) Position.","Data Platform architecture, HVR, RDS, Networking, AWS Aurora, PostgreSQL, Tableau, Cloud Infrastructure, Talend"
Enterprise Architect - Airflow & Data Engineering,Quest Global,Fresher,,"Hyderabad, India",Login to check your skill match score,Job Requirements\nOverall Expectations\nApplication Architecture\nHigh level and Low-level design\nCode Quality responsibility\nCI/CD Best practices\nHands-on & implementation\nSpecific Areas of experience:\nIoT Experience\nScale\nData pipelines\nMQTT\nData base experience\nPartition\nSharding\nData lake\nTechnology\nJava\nPython\nKubernetes\nMicro service Architecture\nWork Experience\nOverall Expectations\nApplication Architecture\nHigh level and Low-level design\nCode Quality responsibility\nCI/CD Best practices\nHands-on & implementation\nSpecific Areas of experience:\nIoT Experience\nScale\nData pipelines\nMQTT\nData base experience\nPartition\nSharding\nData lake\nTechnology\nJava\nPython\nKubernetes\nMicro service Architecture,"Sharding, Data base experience, CI CD Best practices, Micro service Architecture, Data pipelines, Hands-on implementation, IoT Experience, SCALE, Java, Partition, Mqtt, Application Architecture, Data Lake, Python, Kubernetes"
Presales Solutions Architect - Modern Data Center,AHEAD,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"AHEAD's Specialist Solutions Engineers are the technical experts that collaborate with our AHEAD account teams to help identify, qualify, and build solutions that support our customer's Digital Business transformation.\n\nThe AHEAD Modern Datacenter Specialist Solutions Engineer (SSE) will be focused on Datacenter technologies, including storage (block & file), compute, virtualization, and data protection . The SSE is considered a subject matter expert in these areas with responsibility for selling and designing complex solutions. The SSE is also considered an organizational thought leader for their specialty area.\n\nResponsibilities\n\nActive participation in the AHEAD Modern Datacenter SSE team, advisory and delivery teams.\nWorking with the AHEAD partner management team and directly with selected strategic vendors to maintain technical and sales relationships.\nBe a thought leader within the Solutions Engineering organization, responsible for designing, architecting, and at times consulting for AHEAD clients.\nBuild skills, relationships, and industry credibility across many segments to drive product, services, and consulting sales.\nSupport the sales teams during engagements by providing advice and solutions or proposals optimized to meet customer technical and business requirements.\nDevelop relationships with other Modern Datacenter SSE team members, leaders, and partners in support of sales objectives.\nStrategize and execute technical sales calls.\nComplete required pre-sales documentation (configurations, pricing, services, presentations, justifications, etc.) quickly and accurately.\nQualify sales opportunities in terms of customer technical requirements, decision-making process, and funding.\nPresent and market the design and value of proposed AHEAD solutions and business justification to customers, prospects, and AHEAD management.\nParticipate in the mentorship of entry-level team members.\nPossess strong, detailed product/technology/industry knowledge.\nKnowledge of job-associated software and applications.\n\nQualifications\n\nExpertise in Storage (Block & File), Compute, and Virtualization/VMware Cloud Foundation (VCF).\nData Protection experience with Dell, Rubrik, Commvault, etc.\nFamiliarity with Cisco UCS, Dell MX, PowerScale, PowerFlex and hyperconverged compute platforms (VxRail, Nutanix, Azure Stack HCI, OpenShift-V).\nUnderstanding of Datacenter automation tools like Ansible.\nBasic knowledge of cloud platforms like AWS and Azure.\nVMware and Virtualization technology expertise.\nVendor experience with Dell, NetApp, Vast Data, Cisco, etc.\nProfessional communication, presentation, analytical, and problem-solving skills\nExperience working as a technical lead in a pre-sales or sales campaign.\nAbility to work under critical conditions and influence others to achieve results.\nStrong interpersonal skills with excellent presentation skills.\nMust be independent, self-motivated, a self-starter, and possess a good working\nattitude.\nAble to work well within a team and partner environment. Customer-focused and results-driven.\n\nOther Skills\n\nInnovation Mindset: Ability to identify and leverage emerging Datacenter technologies to drive innovative solutions that meet evolving client needs.\nAutomation Expertise: Advanced understanding of automation frameworks and scripting to optimize Datacenter operations and reduce manual effort.\nProblem Solving: Strong analytical skills with a focus on troubleshooting complex Datacenter environments, including storage, compute, and networking issues.\nConsultative Approach: Ability to engage with clients as a trusted advisor, guiding them through the decision-making process with clear, strategic recommendations.\nCollaboration Skills: Proven track record of working effectively across teams, including sales, engineering, and vendor partners, to deliver cohesive solutions.\nContinuous Learning: Commitment to staying updated on the latest industry trends, certifications, and best practices related to Datacenter technologies.\nSecurity Awareness: Knowledge of Datacenter security best practices, ensuring that proposed solutions adhere to compliance and regulatory standards.","Azure Stack HCI, Storage Block File Compute, powerflex, VxRail, OpenShift-V, VMware and Virtualization technology, PowerScale, Dell MX, Cisco Ucs, Nutanix"
Generative AI Architect (Big Data & Java Expertise),eProSoft,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Generative AI Architect (Big Data & Java Expertise)\nHybrid role\nHyderabad, INDIA\nWe are seeking a skilled and innovative Generative AI Architect with a strong background in Big Data and Java to lead our AI-driven initiatives. The ideal candidate will design and implement cutting-edge generative AI models and systems that leverage large datasets, while ensuring seamless integration with Java-based platforms. This role requires a deep understanding of AI/ML technologies, data architecture, and software engineering to drive the next generation of AI-powered solutions.\nKey Responsibilities:\nLead the design and architecture of generative AI models and systems that utilize large-scale data. Collaborate with data scientists, ML engineers, and product teams to create scalable and efficient AI-driven platforms.\nLeverage Big Data technologies (Hadoop, Spark, etc.) to handle massive datasets and ensure efficient data ingestion, processing, and storage for AI applications.\nArchitect, develop, and integrate AI models into Java-based enterprise applications, ensuring smooth operation within existing systems.\nWork with teams to develop generative AI models (e.g., GPT, GANs) and optimize them for performance, scalability, and business needs. Ensure models are trained on relevant datasets and continuously updated.\nCollaborate with cross-functional teams, including data engineering, software development, and business stakeholders, to drive AI initiatives. Mentor junior engineers and provide guidance on AI and Big Data architecture.\nArchitect and deploy AI systems in cloud environments (AWS, GCP, Azure) and ensure robust infrastructure for AI model deployment and scalability.\nStay updated with the latest trends and advancements in AI, Big Data, and Java technologies, applying this knowledge to enhance systems and processes.\nRequired Qualifications:\n8+ years of experience in software engineering, AI, or data architecture roles.\nProven experience in designing and deploying AI models, particularly generative AI (e.g., GPT, GANs, VAEs).\nStrong background in Big Data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.\nExtensive hands-on experience with Java, including multithreading, JVM tuning, and integration with large-scale systems.\nExperience with cloud platforms (AWS, GCP, Azure) for AI model deployment and scaling.\nExpertise in machine learning frameworks (TensorFlow, PyTorch, Keras).\nStrong understanding of data pipelines, ETL processes, and distributed systems.\nProficiency in AI algorithms and techniques (e.g., NLP, computer vision, deep learning).\nKnowledge of microservices architecture, RESTful APIs, and containerization (Docker, Kubernetes).\nBachelor's or Master's degree in Computer Science, Engineering, Data Science, or a related field.\nPlease submit your resume on LinkedIn (or) you can share your resume with [HIDDEN TEXT]","microservices architecture, NoSQL databases, Generative AI, ETL processes, Keras, Java, Tensorflow, Hadoop, Kafka, Deep Learning, AWS, Pytorch, Kubernetes, Azure, Docker, Gcp, Big Data, Computer Vision, Nlp, Spark, Restful Apis"
