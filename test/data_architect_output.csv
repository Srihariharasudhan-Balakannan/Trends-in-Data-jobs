job_title,company_name,experience,salary,location,industry,job_description,skills
Data Architect_Remote,OSI Digital,8-13 Years,,Chennai,"Information Technology, Information Services","We are seeking a highly skilled and experiencedData Architectwith strong expertise indata modelingandSnowflaketo design, develop, and optimize enterprise data architecture. The ideal candidate will play a critical role in shaping data strategy, building scalable models, and ensuring efficient data integration and governance.
Key Responsibilities:
Design and implement end-to-end data architecture using Snowflake
Develop and maintain conceptual, logical, and physical data models.
Define and enforce data architecture standards, best practices, and policies.
Collaborate with data engineers, analysts, and business stakeholders to gather requirements and design data solutions.
Optimize Snowflake performance including data partitioning, caching, and query tuning.
Create and manage data dictionaries, metadata, and lineage documentation.
Ensure data quality, consistency, and security across all data platforms.
Support data integration from various sources (cloud/on-premises) into Snowflake.
Required Skills and Experience:
8+ years of experience in data architecture, data modeling, or similar roles.
Hands-on expertise withSnowflakeincluding Snowpipe, Streams, Tasks, and Secure Data Sharing.
Strong experience withdata modeling tools(e.g., Erwin, ER/Studio, dbt).
Proficiency inSQL,ETL/ELT pipelines, anddata warehousing concepts.
Experience working with structured, semi-structured (JSON, XML), and unstructured data.
Solid understanding of data governance, data cataloging, and security frameworks.
Excellent analytical, communication, and stakeholder management skills.
Preferred Qualifications:
Experience with cloud platforms likeAWS,Azure, orGCP.
Familiarity with data lakehouse architecture and real-time data processing.
Snowflake Certification(s) or relevant cloud certifications.
Knowledge of Python or scripting for data automation is a plus.","snowflake, Data lakehouse, Data Modeling, Data Architecture, Python, Sql, Aws"
Data Architect_Remote,OSI Digital,8-13 Years,,Bengaluru,"Information Technology, Information Services","We are seeking a highly skilled and experiencedData Architectwith strong expertise indata modelingandSnowflaketo design, develop, and optimize enterprise data architecture. The ideal candidate will play a critical role in shaping data strategy, building scalable models, and ensuring efficient data integration and governance.
Key Responsibilities:
Design and implement end-to-end data architecture using Snowflake
Develop and maintain conceptual, logical, and physical data models.
Define and enforce data architecture standards, best practices, and policies.
Collaborate with data engineers, analysts, and business stakeholders to gather requirements and design data solutions.
Optimize Snowflake performance including data partitioning, caching, and query tuning.
Create and manage data dictionaries, metadata, and lineage documentation.
Ensure data quality, consistency, and security across all data platforms.
Support data integration from various sources (cloud/on-premises) into Snowflake.
Required Skills and Experience:
8+ years of experience in data architecture, data modeling, or similar roles.
Hands-on expertise withSnowflakeincluding Snowpipe, Streams, Tasks, and Secure Data Sharing.
Strong experience withdata modeling tools(e.g., Erwin, ER/Studio, dbt).
Proficiency inSQL,ETL/ELT pipelines, anddata warehousing concepts.
Experience working with structured, semi-structured (JSON, XML), and unstructured data.
Solid understanding of data governance, data cataloging, and security frameworks.
Excellent analytical, communication, and stakeholder management skills.
Preferred Qualifications:
Experience with cloud platforms likeAWS,Azure, orGCP.
Familiarity with data lakehouse architecture and real-time data processing.
Snowflake Certification(s) or relevant cloud certifications.
Knowledge of Python or scripting for data automation is a plus.","snowflake, Data lakehouse, Data Modeling, Data Architecture, Python, Sql, Aws"
Principal Data Architect- Data Modelling,Tiger Analytics,16-21 Years,,Delhi NCR,Consulting,"Role & responsibilities
Understand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.
Work as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.
Lead cross-functional teams, define data strategies, andleveragethe latest technologies in data handling.
Define and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.
Suggest best modelling approach to the client based on their requirement and target architecture.
Analyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.
Profile the Data sets to generate relevant insights.
Optimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.
Establish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.
Drives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.
Closely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.
Guide /mentor team members, and review artifacts.
Contribute to the overall data strategy and roadmaps.
Propose and execute technical assessments, proofs of concept to promote innovation in the data space.
Preferred candidate profile
Minimum 16 years of experience
Deep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.
Experience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.
Decent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server) Good understanding of Relational, Dimensional, Data Vault Modelling
Experience in implementing 2 or more data models in a database with data security and access controls.
Good experience in OLTP and OLAP systems
Excellent Data Analysis skills with demonstrable knowledge on standard datasets and sources.
Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)
Experience on one or more cloud platforms (e.g. AWS, Azure, GCP)
Understanding of DevOps processes
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python
Experience in leading the large and complex teams
Good understanding of agile methodology","Medallion Architecture, Solution Architecting, Data Modeling, Data Architecture, Data Warehousing, Rfp, Sql"
Principal Data Architect- Data Modelling,Tiger Analytics,16-21 Years,,Gurugram,Consulting,"Role & responsibilities
Understand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.
Work as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.
Lead cross-functional teams, define data strategies, andleveragethe latest technologies in data handling.
Define and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.
Suggest best modelling approach to the client based on their requirement and target architecture.
Analyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.
Profile the Data sets to generate relevant insights.
Optimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.
Establish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.
Drives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.
Closely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.
Guide /mentor team members, and review artifacts.
Contribute to the overall data strategy and roadmaps.
Propose and execute technical assessments, proofs of concept to promote innovation in the data space.
Preferred candidate profile
Minimum 16 years of experience
Deep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.
Experience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.
Decent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server) Good understanding of Relational, Dimensional, Data Vault Modelling
Experience in implementing 2 or more data models in a database with data security and access controls.
Good experience in OLTP and OLAP systems
Excellent Data Analysis skills with demonstrable knowledge on standard datasets and sources.
Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)
Experience on one or more cloud platforms (e.g. AWS, Azure, GCP)
Understanding of DevOps processes
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python
Experience in leading the large and complex teams
Good understanding of agile methodology","Medallion Architecture, Solution Architecting, Data Modeling, Data Architecture, Data Warehousing, Rfp, Sql"
Data Architect-Data Modelling,Tiger Analytics,10-15 Years,,Hyderabad,Consulting,"Role & responsibilities
As an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.
On a typical day, you might
Engage the clients & understand the business requirements to translate those into data models.
Analyze customer problems, propose solutions from a data structural perspective, and estimate and deliver proposed solutions.
Create and maintain a Logical Data Model (LDM) and Physical Data Model (PDM) by applying best practices to provide business insights.
Use the Data Modelling tool to create appropriate data models
Create and maintain the Source to Target Data Mapping document that includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.
Gather and publish Data Dictionaries.
Ideate, design, and guide the teams in building automations and accelerators
Involve in maintaining data models as well as capturing data models from existing databases and recording descriptive information.
Contribute to building data warehouse & data marts (on Cloud) while performing data profiling and quality analysis.
Use version control to maintain versions of data models.
Collaborate with Data Engineers to design and develop data extraction and integration code modules.
Partner with the data engineers & testing practitioners to strategize ingestion logic, consumption patterns & testing.
Ideate to design & develop the next-gen data platform by collaborating with cross-functional stakeholders.
Work with the client to define, establish and implement the right modelling approach as per the requirement
Help define the standards and best practices
Involve in monitoring the project progress to keep the leadership teams informed on the milestones, impediments, etc.
Coach team members, and review code artifacts.
Contribute to proposals and RFPs
Preferred candidate profile
10+ years of experience in Data space.
Decent SQL knowledge
Able to suggest modeling approaches for a given problem.
Significant experience in one or more RDBMS (Oracle, DB2, and SQL Server)
Real-time experience working in OLAP & OLTP database models (Dimensional models).
Comprehensive understanding of Star schema, Snowflake schema, and Data Vault Modelling. Also, on any ETL tool, Data Governance, and Data quality.
Eye to analyze data & comfortable with following agile methodology.
Adept understanding of any of the cloud services is preferred (Azure, AWS & GCP)
Enthuse to coach team members & collaborate with various stakeholders across the organization and take complete ownership of deliverables.
Experience in contributing to proposals and RFPs
Good experience in stakeholder management
Decent communication and experience in leading the team","Cloud Platforms, Data Modeling, Data Architecture, OLAP, Data Warehousing, Sql, Oltp"
Principal Data Architect - Data Modelling,Tiger Analytics,16-22 Years,,Bengaluru,Consulting,"Role & responsibilities:
Minimum 15 years of experience
Deep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.
Experience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.
Decent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server)
Good understanding of Relational, Dimensional, Data Vault Modelling
Experience in implementing 2 or more data models in a database with data security and access controls. Good experience in OLTP and OLAP systems
Excellent Data Analysis skills with demonstrable knowledge on standard datasets and sources. Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)
Experience on one or more cloud platforms (e.g. AWS, Azure, GCP)
Understanding of DevOps processes
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python
Experience in leading the large and complex teams
Good understanding of agile methodology
Extensive expertise in leading data transformation initiatives, driving cultural change, and promoting a data driven mindset across the organization.
Excellent Communication skills
Requirements:
Understand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.
Work as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.
Lead cross-functional teams, define data strategies, and leverage the latest technologies in data handling.
Define and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.
Suggest best modelling approach to the client based on their requirement and target architecture.
Analyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.
Profile the Data sets to generate relevant insights.
Optimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.
Establish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.
Drives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.
Closely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.
Guide /mentor team members, and review artifacts.
Contribute to the overall data strategy and roadmaps.
Propose and execute technical assessments, proofs of concept to promote innovation in the data space.","Solution Architecting, data vault, Data Modeling, Data Architecture, Dimensional Modeling, Data Warehousing, Oltp"
Principal Data Architect - Data Modelling,Tiger Analytics,16-22 Years,,Chennai,Consulting,"Role & responsibilities:
Minimum 15 years of experience
Deep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.
Experience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.
Decent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server)
Good understanding of Relational, Dimensional, Data Vault Modelling
Experience in implementing 2 or more data models in a database with data security and access controls. Good experience in OLTP and OLAP systems
Excellent Data Analysis skills with demonstrable knowledge on standard datasets and sources. Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)
Experience on one or more cloud platforms (e.g. AWS, Azure, GCP)
Understanding of DevOps processes
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python
Experience in leading the large and complex teams
Good understanding of agile methodology
Extensive expertise in leading data transformation initiatives, driving cultural change, and promoting a data driven mindset across the organization.
Excellent Communication skills
Requirements:
Understand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.
Work as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.
Lead cross-functional teams, define data strategies, and leverage the latest technologies in data handling.
Define and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.
Suggest best modelling approach to the client based on their requirement and target architecture.
Analyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.
Profile the Data sets to generate relevant insights.
Optimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.
Establish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.
Drives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.
Closely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.
Guide /mentor team members, and review artifacts.
Contribute to the overall data strategy and roadmaps.
Propose and execute technical assessments, proofs of concept to promote innovation in the data space.","Solution Architecting, data vault, Data Modeling, Data Architecture, Dimensional Modeling, Data Warehousing, Oltp"
Principal Data Architect - Data Modelling,Tiger Analytics,16-22 Years,,Hyderabad,Consulting,"Role & responsibilities:
Minimum 15 years of experience
Deep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.
Experience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.
Decent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server)
Good understanding of Relational, Dimensional, Data Vault Modelling
Experience in implementing 2 or more data models in a database with data security and access controls. Good experience in OLTP and OLAP systems
Excellent Data Analysis skills with demonstrable knowledge on standard datasets and sources. Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)
Experience on one or more cloud platforms (e.g. AWS, Azure, GCP)
Understanding of DevOps processes
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python
Experience in leading the large and complex teams
Good understanding of agile methodology
Extensive expertise in leading data transformation initiatives, driving cultural change, and promoting a data driven mindset across the organization.
Excellent Communication skills
Requirements:
Understand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.
Work as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.
Lead cross-functional teams, define data strategies, and leverage the latest technologies in data handling.
Define and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.
Suggest best modelling approach to the client based on their requirement and target architecture.
Analyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.
Profile the Data sets to generate relevant insights.
Optimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.
Establish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.
Drives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.
Closely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.
Guide /mentor team members, and review artifacts.
Contribute to the overall data strategy and roadmaps.
Propose and execute technical assessments, proofs of concept to promote innovation in the data space.","Solution Architecting, data vault, Data Modeling, Data Architecture, Dimensional Modeling, Data Warehousing, Oltp"
Sr Data Architect,Houghton Mifflin Harcourt,5-7 Years,,Pune,E-Learning,"Responsibilities
Interprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps
Designs the structure and layout of data systems, including databases, warehouses, and lakes
Selects and designs database management systems that meet the organizations needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures
Defines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms
Designs processes for the ETL process from various sources into the organizations data systems
Translates high-level business requirements into data models and appropriate metadata, test data, and data quality standards
Manages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps
Simplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company
Leads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums
Defines and manages standards, guidelines, and processes to ensure data quality
Works with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions
Evaluates and recommends emerging technologies for data management, storage, and analytics
Design, create, and implement logical and physical data models for both IT and business solutions to capture the structure, relationships, and constraints of relevant datasets
Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
Effectively collaborate and communicate with various stakeholders to understand data and business requirements and translate them into data models
Create entity-relationship diagrams (ERDs), data flow diagrams, and other visualization tools to represent data models
Collaborate with database administrators and software engineers to implement and maintain data models in databases, data warehouses, and data lakes
Develop data modeling best practices, and use these standards to identify and resolve data modeling issues and conflicts
Conduct performance tuning and optimization of data models for efficient data access and retrieval
Incorporate core data management competencies, including data governance, data security and data quality
Job Requirements
Education:
A bachelors degree in computer science, data science, engineering, or related field
Experience:
At least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives
Experience leading projects involving data warehousing, data modeling, and data analysis
Design experience in Azure Databricks, PySpark, and Power BI/Tableau
Skills:
Ability in programming languages such as Java, Python, and C/C++
Ability in data science languages/tools such as SQL, R, SAS, or Excel
Proficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)
Experience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata
Understanding of entity-relationship modeling, metadata systems, and data quality tools and techniques
Ability to think strategically and relate architectural decisions and recommendations to business needs and client culture
Ability to assess traditional and modern data architecture components based on business needs
Experience with business intelligence tools and technologies such as ETL, Power BI, and Tableau
Ability to regularly learn and adopt new technology, especially in the ML/AI realm
Strong analytical and problem-solving skills
Ability to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings
Ability to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders
Ability to guide solution design and architecture to meet business needs
Expert knowledge of data modeling concepts, methodologies, and best practices
Proficiency in data modeling tools such as Erwin or ER/Studio
Knowledge of relational databases and database design principles
Familiarity with dimensional modeling and data warehousing concepts
Strong SQL skills for data querying, manipulation, and optimization, and knowledge of other data science languages, including JavaScript, Python, and R
Ability to collaborate with cross-functional teams and stakeholders to gather requirements and align on data models
Excellent analytical and problem-solving skills to identify and resolve data modeling issues
Strong communication and documentation skills to effectively convey complex data modeling concepts to technical and business stakeholders","R, Business Analytics, Java, Data Modeling, Cloud Services, C C++, Data Architecture, Sql, Python, Big Data Technologies"
Data Architect-Data Modelling,Tiger Analytics,10-15 Years,,Bengaluru,Consulting,"Role & responsibilities
As an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.
On a typical day, you might
Engage the clients & understand the business requirements to translate those into data models.
Analyze customer problems, propose solutions from a data structural perspective, and estimate and deliver proposed solutions.
Create and maintain a Logical Data Model (LDM) and Physical Data Model (PDM) by applying best practices to provide business insights.
Use the Data Modelling tool to create appropriate data models
Create and maintain the Source to Target Data Mapping document that includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.
Gather and publish Data Dictionaries.
Ideate, design, and guide the teams in building automations and accelerators
Involve in maintaining data models as well as capturing data models from existing databases and recording descriptive information.
Contribute to building data warehouse & data marts (on Cloud) while performing data profiling and quality analysis.
Use version control to maintain versions of data models.
Collaborate with Data Engineers to design and develop data extraction and integration code modules.
Partner with the data engineers & testing practitioners to strategize ingestion logic, consumption patterns & testing.
Ideate to design & develop the next-gen data platform by collaborating with cross-functional stakeholders.
Work with the client to define, establish and implement the right modelling approach as per the requirement
Help define the standards and best practices
Involve in monitoring the project progress to keep the leadership teams informed on the milestones, impediments, etc.
Coach team members, and review code artifacts.
Contribute to proposals and RFPs
Preferred candidate profile
10+ years of experience in Data space.
Decent SQL knowledge
Able to suggest modeling approaches for a given problem.
Significant experience in one or more RDBMS (Oracle, DB2, and SQL Server)
Real-time experience working in OLAP & OLTP database models (Dimensional models).
Comprehensive understanding of Star schema, Snowflake schema, and Data Vault Modelling. Also, on any ETL tool, Data Governance, and Data quality.
Eye to analyze data & comfortable with following agile methodology.
Adept understanding of any of the cloud services is preferred (Azure, AWS & GCP)
Enthuse to coach team members & collaborate with various stakeholders across the organization and take complete ownership of deliverables.
Experience in contributing to proposals and RFPs
Good experience in stakeholder management
Decent communication and experience in leading the team","Cloud Platforms, Data Modeling, Data Architecture, OLAP, Data Warehousing, Sql, Oltp"
Data Architect-Data Modelling,Tiger Analytics,10-15 Years,,Chennai,Consulting,"Role & responsibilities
As an Architect, you will work to solve some of the most complex and captivating data management problems that would enable them as a data-driven organization; Seamlessly switch between roles of an Individual Contributor, team member, and Data Modeling Architect as demanded by each project to define, design, and deliver actionable insights.
On a typical day, you might
Engage the clients & understand the business requirements to translate those into data models.
Analyze customer problems, propose solutions from a data structural perspective, and estimate and deliver proposed solutions.
Create and maintain a Logical Data Model (LDM) and Physical Data Model (PDM) by applying best practices to provide business insights.
Use the Data Modelling tool to create appropriate data models
Create and maintain the Source to Target Data Mapping document that includes documentation of all entities, attributes, data relationships, primary and foreign key structures, allowed values, codes, business rules, glossary terms, etc.
Gather and publish Data Dictionaries.
Ideate, design, and guide the teams in building automations and accelerators
Involve in maintaining data models as well as capturing data models from existing databases and recording descriptive information.
Contribute to building data warehouse & data marts (on Cloud) while performing data profiling and quality analysis.
Use version control to maintain versions of data models.
Collaborate with Data Engineers to design and develop data extraction and integration code modules.
Partner with the data engineers & testing practitioners to strategize ingestion logic, consumption patterns & testing.
Ideate to design & develop the next-gen data platform by collaborating with cross-functional stakeholders.
Work with the client to define, establish and implement the right modelling approach as per the requirement
Help define the standards and best practices
Involve in monitoring the project progress to keep the leadership teams informed on the milestones, impediments, etc.
Coach team members, and review code artifacts.
Contribute to proposals and RFPs
Preferred candidate profile
10+ years of experience in Data space.
Decent SQL knowledge
Able to suggest modeling approaches for a given problem.
Significant experience in one or more RDBMS (Oracle, DB2, and SQL Server)
Real-time experience working in OLAP & OLTP database models (Dimensional models).
Comprehensive understanding of Star schema, Snowflake schema, and Data Vault Modelling. Also, on any ETL tool, Data Governance, and Data quality.
Eye to analyze data & comfortable with following agile methodology.
Adept understanding of any of the cloud services is preferred (Azure, AWS & GCP)
Enthuse to coach team members & collaborate with various stakeholders across the organization and take complete ownership of deliverables.
Experience in contributing to proposals and RFPs
Good experience in stakeholder management
Decent communication and experience in leading the team","Cloud Platforms, Data Modeling, Data Architecture, OLAP, Data Warehousing, Sql, Oltp"
Principal Data Architect- Data Modelling,Tiger Analytics,16-21 Years,,Pune,Consulting,"Role & responsibilities
Understand the Business Requirements and translate business requirements into conceptual, logical and physical Data models.
Work as a principal advisor on data architecture, across various data requirements, aggregation data lake data models data warehouse etc.
Lead cross-functional teams, define data strategies, andleveragethe latest technologies in data handling.
Define and govern data architecture principles, standards, and best practices to ensure consistency, scalability, and security of data assets across projects.
Suggest best modelling approach to the client based on their requirement and target architecture.
Analyze and understand the Datasets and guide the team in creating Source to Target Mapping and Data Dictionaries, capturing all relevant details.
Profile the Data sets to generate relevant insights.
Optimize the Data Models and work with the Data Engineers to define the Ingestion logic, ingestion frequency and data consumption patterns.
Establish data governance practices, including data quality, metadata management, and data lineage, to ensure data accuracy, reliability, and compliance.
Drives automation in modeling activities Collaborate with Business Stakeholders, Data Owners, Business Analysts, Architects to design and develop next generation data platform.
Closely monitor the Project progress and provide regular updates to the leadership teams on the milestones, impediments etc.
Guide /mentor team members, and review artifacts.
Contribute to the overall data strategy and roadmaps.
Propose and execute technical assessments, proofs of concept to promote innovation in the data space.
Preferred candidate profile
Minimum 16 years of experience
Deep understanding of data architecture principles, data modelling, data integration, data governance, and data management technologies.
Experience in Data strategies and developing logical and physical data models on RDBMS, NoSQL, and Cloud native databases.
Decent experience in one or more RDBMS systems (such as Oracle, DB2, SQL Server) Good understanding of Relational, Dimensional, Data Vault Modelling
Experience in implementing 2 or more data models in a database with data security and access controls.
Good experience in OLTP and OLAP systems
Excellent Data Analysis skills with demonstrable knowledge on standard datasets and sources.
Good Experience on one or more Cloud DW (e.g. Snowflake, Redshift, Synapse)
Experience on one or more cloud platforms (e.g. AWS, Azure, GCP)
Understanding of DevOps processes
Hands-on experience in one or more Data Modelling Tools
Good understanding of one or more ETL tool and data ingestion frameworks
Understanding of Data Quality and Data Governance
Good understanding of NoSQL Database and modeling techniques
Good understanding of one or more Business Domains
Understanding of Big Data ecosystem
Understanding of Industry Data Models
Hands-on experience in Python
Experience in leading the large and complex teams
Good understanding of agile methodology","Medallion Architecture, Solution Architecting, Data Modeling, Data Architecture, Data Warehousing, Rfp, Sql"
Senior Data Architect,Swiss Re,5-7 Years,,"Bengaluru, India",Consulting/Advisory Services,"Our success depends on our ability to build an inclusive culture encouraging fresh perspectives and innovative thinking. We embrace a workplace where everyone has equal opportunities to thrive and develop professionally regardless of their gender, race, ethnicity, gender identity and/or expression, sexual orientation, physical or mental ability, skillset, thought or other characteristics. In our inclusive and flexible environment everyone can bring their authentic selves to work.
About D&A Re
The Digital & Tech Re organization is at the forefront of driving the digital transformation for our reinsurance business units across P&C Re, L&H Re and Solutions. We strive to build an inspiring environment for our people to use data and technology to create a sustainable and strategic competitive advantage.
Data & Analytics Re is the data arm of Digital & Tech Re. We create innovative analytics, data science and robust data foundation capabilities to generate data-driven insights that serve the heart of Swiss Re's business. Together with our business counterparts in Property & Casualty and Life & Health, we work daily to deliver differentiating insights, elevate underwriting excellence and effectively select and manage risk pools.
Our team is composed of an international workforce based in different locations and serving a global customer basis, with a large part of our leadership located in Zurich.
About the Role
Are you an energetic Data Architect who enjoys working with teams to design and deliver exceptional data driven solutions
As a Data Architect you will be responsible for supporting the companies ambition to become a data driven commercial insurer. Working with both internal and external customers, you will need to have strong analytical and conceptual thinking, with the ability to recognize competing requirements and constraints, to balance and negotiate trade-offs.
The role is a mix of strong solution architecture awareness combined with a strong background in data modelling and canonical modelling. The role is very much focused on constantly improving the data landscape aimed at democratizing our data assets.
You thrive working in an agile, collaborative environment providing knowledge and guidance on the data architecture.
Your responsibilities include:
.High-level architecture and designs for database and data integrations leveraging Conceptual, logical and physical data models.
.End to end data analysis, from business analysis and data modelling through to data quality assurance, on a major global program in the Commercial Insurance sector in a fast-moving Agile environment.
.Complex SQL based data analysis
.Maintenance of the logical and physical data models within a CASE tool (SAP Sybase Power Designer), supporting both SDLC and Agile application development approaches.
.Collaborate with your customers, understanding their data needs, finding innovative ways to bring data into their daily process.
.Work with other members of the data team, including data architects, data analysts, data engineers, and data scientists.
.Create full transparency of data for customers to know what is available and how to use it
About you
Are you eager to disrupt the insurance industry with us and make an impact Do you wish to have your talent recognized and rewarded Then join our growing team and become part of the next wave of insurance innovation.
Your experience
.Proven track record of delivering results and impact
.Minimum of 5-7 years as a Data Architect
.Strong business acumen and a deep strategic mindset
.Requirements gathering, analysis and prioritization for business processes and data flows, persistence and integration.
.Conceptual and logical data and physical modeling (Sybase Power Designer)
.Data retrieval and manipulation using SQL
.Data architecture: design for integration of complex data flows from multiple applications
.Data warehousing design, both dimensional and 3NF
.SDLC and Agile/SCRUM software development processes
.Experience working with and understanding the needs of customers or clients (internal or external)
.Ability to understand and internalize an organization's strategy and culture, and contribute to their implementation and reinforcement through technology architecture and data designs
.A desire and openness to learning and continuous improvement, both of yourself and your team members
.Proven analytical skills and experience making decisions based on hard and soft data
.Commitment to the organization's new way of working through greater collaboration and breaking down of siloes
.English fluency is a requirement, demonstrating excellent verbal and written communication skills
.Excellent in team and organizational development by providing the highest quality of services
.Experience in information & data governance would be a plus
.Self-motivated and good communication skills
.Dedication and flexibility aptitude for fast learning
.Ideally experience in insurance/reinsurance or financial industry
Behavioural Competences
.Excellent organizational skills.
.Excellent communication and presentation skills ability to communicate on different levels of seniority.
.Team player enjoying being part of a cross-functional setup.
.Ability to perform well on time-critical endeavours and on multiple fronts at the time.
.Strong dedication to quality and client mindset.
.A passion for learning and continuous improvement, both of yourself and your team members.
We are an equal opportunity employer, and we value diversity at our company. Our aim is to live visible and invisible diversity -diversity of age, race, ethnicity, nationality, gender, gender identity, sexual orientation, religious beliefs, physical abilities, personalities and experiences - at all levels and in all functions and regions. We also collaborate in a flexible working environment, providing you with a compelling degree of autonomy to decide how, when and where to carry out your tasks.
We provide feedback to all candidates via email. If you have not heard back from us, please check your spam folder.
About Swiss Re
Swiss Re is one of the world's leading providers of reinsurance, insurance and other forms of insurance-based risk transfer, working to make the world more resilient. We anticipate and manage a wide variety of risks, from natural catastrophes and climate change to cybercrime. We cover both Property & Casualty and Life & Health. Combining experience with creative thinking and cutting-edge expertise, we create new opportunities and solutions for our clients. This is possible thanks to the collaboration of more than 14,000 employees across the world.
Our success depends on our ability to build an inclusive culture encouraging fresh perspectives and innovative thinking. We embrace a workplace where everyone has equal opportunities to thrive and develop professionally regardless of their age, gender, race, ethnicity, gender identity and/or expression, sexual orientation, physical or mental ability, skillset, thought or other characteristics. In our inclusive and flexible environment everyone can bring their authentic selves to work and their passion for sustainability.
If you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.
Keywords:
Reference Code:133976","Canonical Modelling, SAP Sybase Power Designer, Data Modelling, Agile Scrum, Data Architecture, Data Warehousing, Sql, Sdlc"
Data Architect- Snowflake,3Pillar Global,10-12 Years,,"Indi, India",Login to check your skill match score,"Embark on an exciting journey into the realm of data analytics with 3Pillar! We extend an invitation for you to join our team and gear up for a thrilling adventure. As a Snowflake Data Architect you will be at the heart of the organization and support our clients to take control of their data and get value out of it by defining a reference architecture for our customers. This means that you will work closely with business leaders and information management teams to define and implement a roadmap on data management, business intelligence or analytics solutions..
If your passion for data analytics solutions that make a real-world impact, consider this your pass to the captivating world of Data Science and Engineering!
Relevant Experience: 10+ years in Data Practice
Must have Skills: Snowflake, Data Architecture, Engineering, Governance, and Cloud services
Responsibilities
Assessments of existing data components, Performing POCs, Consulting to the stakeholders
Lead the migration to Snowflake
In a strong client-facing role, proposing, advocating, leading implementation of end to end solutions to an enterprise's data specific business problems, and taking care of data collection, extraction, integration, cleansing, enriching and data visualization.
Ability to design large data platforms to enable Data Engineers, Analysts & scientists
Strong exposure to different Data architectures, data lake & data warehouse, including migrations, rearchitect and platform modernization
Define tools & technologies to develop automated data pipelines, write ETL processes, develop dashboard & report and create insights
Continually reassess current state for alignment with architecture goals, best practices and business needs
DB modeling, deciding best data storage, creating data flow diagrams, maintaining related documentation
Taking care of performance, reliability, reusability, resilience, scalability, security, privacy & data governance while designing a data architecture
Apply or recommend best practices in architecture, coding, API integration, CI/CD pipelines
Coordinate with data scientists, analysts, and other stakeholders for data-related needs
Help the Data Science & Analytics Practice grow by mentoring junior Practice members, leading initiatives, leading Data Practice Offerings
Provide thought leadership by representing the Practice / Organization on internal / external platforms
Qualification:
Translate business requirements into data requests, reports and dashboards.
Strong Database & modeling concepts with exposure to SQL & NoSQL Databases
Strong data architecture patterns & principles, ability to design secure & scalable data lakes, data warehouse, data hubs, and other event-driven architectures
Expertise in designing and writing ETL processes.
Strong experience to Snowflake, and its components.
Knowledge of Master Data management and related tools
Strong exposure to data security and privacy regulations (GDPR, HIPAA) and best practices
Skilled in ensuring data accuracy, consistency, and quality
Experience of AWS services viz., AWS S3, Redshift, Lambda, DynamoDB, EMR, Glue, Lake formation, Athena, Quicksight, RDS, Kinesis, Managed Kafka, API Gateway, CloudWatch
AWS S3, Redshift, Lambda, DynamoDB, EMR, Glue, Lake formation, Athena, Quicksight, RDS, Kinesis, Managed Kafka, Elasticsearch and Elastic Cache, API Gateway, CloudWatch
Ability to implement data validation processes and establish data quality standards.
Experience in Linux, and scripting
Proficiency in data visualization tools like Tableau, Power BI or similar to create meaningful insights
Additional Experience Desired:
Experience working with data ingestion tools such as Fivetran, stitch, or Matillion
AWS IOT solutions
Apache NiFi, Talend, Informatica
Knowledge of GCP Data services
Exposure to AI / ML technologies","Engineering Governance, Managed Kafka, NoSQL Databases, GCP Data services, Glue, AI ML technologies, Athena, Lake formation, Snowflake Data Architecture, ETL processes, Tableau, Sql, Emr, Dynamodb, Lambda, Api Gateway, Quicksight, RDS, Power Bi, Cloud Services, Informatica, Apache Nifi, Aws S3, Kinesis, Talend, Cloudwatch, Redshift"
Data Architect,NTT Data,8-10 Years,,"Chennai, India",IT/Computers - Software,"Req ID:324664
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Architect to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).
Key Responsibilities:
Develop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.
Utilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.
Conceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.
Institute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.
Implement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.
Evaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.
Develop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.
Identify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.
Formulate and maintain data models and establish policies and procedures for functional design.
Offer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.
Stay informed about upgrades and emerging database technologies through continuous research.
Collaborate with project managers and business leaders on all projects involving enterprise data.
Document the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.
Design and implement data solutions tailored to meet customer needs and specific use cases.
Provide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.
Basic Qualifications:
8+ years of hands-on experience with various database technologies
6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.
Experience with Azure, Databricks, Snowflake
Knowledgeable on concepts of GenAI
Ability to travel at least 25%.
Preferred Skills:
Possess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.
Demonstrated expertise with certifications in Snowflake.
Valuable Big 4 Management Consulting experience or exposure to multiple industries.
Undergraduate or graduate degree preferred.
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","GenAI, snowflake, Databricks, AWS, Azure, Gcp"
Data Architect,Axtria,10-17 Years,,"Gurugram, Bengaluru, Delhi NCR",Data Integration,"To leverage expertise in data architecture and management to design, implement, and optimize a robust data warehousing platform for the pharmaceutical industry.
The goal is to ensure seamless integration of diverse data sources, maintain high standards of data quality and governance, and enable advanced analytics through the definition and management of semantic and common data layers.
Utilizing Axtria DataMAx and generative AI technologies, the aim is to accelerate business insights and support regulatory compliance, ultimately enhancing decision-making and operational efficiency.
Key Responsibilities
Data Modeling: Design logical and physical data models to ensure efficient data storage and retrieval.
ETL Processes: Develop and optimize ETL processes to accurately and efficiently move data from various sources into the data warehouse.
Infrastructure Design: Plan and implement the technical infrastructure, including hardware, software, and network components.
Data Governance: Ensure compliance with regulatory standards and implement data governance policies to maintain data quality and security.
Performance Optimization: Continuously monitor and improve the performance of the data warehouse to handle large volumes of data and complex queries.
Semantic Layer Definition: Define and manage the semantic layer architecture and technology stack to manage the lifecycle of semantic constructs including consumption into downstream systems.
Common Data Layer Management: Integrate data from multiple sources into a centralized repository, ensuring consistency and accessibility.
Deep expertise in architecting enterprise-grade software systems that are performant, scalable, resilient, and manageable. Architecting GenAI based systems is an added plus.
Advanced Analytics: Enable advanced analytics and machine learning to identify patterns in genomic data, optimize clinical trials, and personalize medication.
Generative AI: Should have worked with production-ready use case for GenAI based data.
Stakeholder Engagement: Work closely with business stakeholders to understand their data needs and translate them into technical solutions.
Cross-Functional Collaboration: Collaborate with IT, data scientists, and business analysts to ensure the data warehouse supports various analytical and operational needs.
Qualifications
Proven experience in data architecture and data warehousing, preferably in the pharmaceutical industry.
Strong knowledge of data modeling, ETL processes, and infrastructure design.
Experience with data governance and regulatory compliance in the life sciences sector.
Proficiency in using Axtria DataMAx or similar data management products.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
Preferred Skills
Familiarity with advanced analytics and machine learning techniques.
Experience in managing semantic and common data layers.
Knowledge of FDA guidelines, HIPAA regulations, and other relevant regulatory standards.
Experience with generative AI technologies and their application in data warehousing.","Gen AI, Infrastructure Design, Data Architecture, Data Warehousing, Data Modelling, Data Governance, Etl"
Data Architect,Kezan Consulting,12-16 Years,,Bengaluru,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,"Delhi, Delhi NCR",Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,Bengaluru,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,Pune,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-22 Years,,Bengaluru,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-22 Years,,Pune,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kezan Consulting,12-16 Years,,Noida,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
AWS -Senior Data Architect - R01543971,Brillio,Fresher,,"Gurugram, India",Login to check your skill match score,"Senior Data Specialist
Primary Skills
Athena, SQS, CloudWatch, Macie, Spark - Scala, Kinesis, CloudFormation, EMR, Open Search, DynamoDB, Amazon API Gateway, SCT, Redshift, DMS, Oozie
Secondary Skills
Python
Job requirements
About Brillio:
Brillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.
Brillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.
Work Location: Gurgaon/Hyderabad/Pune/Bangalore/Chennai
Job Title: AWS Data Architect/Senior Lead Data Engineer
Job Type: [Full-time
Job Summary:
We are looking for an experienced and dynamic AWS Data Architect to lead the design and implementation of data solutions in the cloud. This role will focus on leveraging AWS technologies to create scalable, reliable, and optimized data architectures that drive business insights and data-driven decision-making. As an AWS Data Architect, you will play a pivotal role in shaping the data strategy, implementing best practices, and ensuring the seamless integration of AWS-based data platforms, with a focus on services like Amazon Redshift, Aurora, and other AWS data services.
Key Responsibilities:
Data Architecture Design:
Lead the design and implementation of cloud-native data architectures on AWS, using services like Amazon Redshift, Amazon Aurora, Amazon S3, AWS Glue, and Amazon Athena.
Define data management strategies and ensure data models, structures, and systems are aligned with business goals.
Architect end-to-end data pipelines for ingesting, processing, storing, and analyzing large datasets using AWS services.
Cloud Data Infrastructure Management:
Design and implement highly available, scalable, and secure data infrastructure on AWS.
Lead the migration of on-premise data solutions to AWS cloud platforms, ensuring smooth transitions and minimizing operational impact.
Oversee performance tuning and optimization of data storage and query processing for Amazon Redshift and Aurora environments.
Data Integration and Processing:
Architect data integration solutions for combining data from various on-premise and cloud-based systems.
Design and implement ETL/ELT workflows utilizing AWS Glue, Lambda, and other relevant AWS tools to transform raw data into consumable formats.
Work with stakeholders to identify data sources and design appropriate interfaces and APIs for seamless integration into the cloud architecture.
Governance and Security:
Ensure that data architecture adheres to best practices for data security, governance, and compliance (e.g., GDPR, HIPAA).
Define and implement data privacy and access control policies using AWS Identity and Access Management (IAM), encryption, and audit logging.
Establish data lineage and metadata management strategies to ensure transparency and traceability in data workflows.
Collaboration and Leadership:
Act as a technical leader and provide guidance on cloud-based data solutions to internal teams, including Data Engineers, Data Scientists, and DevOps engineers.
Collaborate with business stakeholders and leadership to understand requirements and design data solutions that meet business needs.
Lead the development of best practices and standards for AWS-based data engineering solutions and ensure these are followed across the organization.
Innovation and Optimization:
Continuously evaluate and introduce new AWS data technologies, tools, and features to optimize performance, scalability, and cost-efficiency.
Stay up-to-date with the latest trends in cloud data architecture and AWS service updates to recommend improvements and innovations in existing solutions.
Documentation and Knowledge Sharing:
Create and maintain clear and detailed documentation for data architecture, design decisions, and system configurations.
Promote a culture of knowledge sharing by mentoring team members and providing ongoing training on AWS data technologies.","Amazon Athena, Amazon Aurora, Amazon Redshift, Amazon S3, AWS Glue, ELT, Etl"
Data Architect Sr. Advisor,NTT Data,12-15 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:323777
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Architect Sr. Advisor to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Job Duties: The Data & AI Architect is a seasoned level expert who is responsible for participating in the delivery of multi-technology consulting services to clients by providing strategies and solutions on all aspects of infrastructure and related technology components.
This role collaborates with other stakeholders on the development of the architectural approach for one or more layer of a solution. This role has the primary objective is to work on strategic projects that ensure the optimal functioning of the client's technology infrastructure.
. Key Responsibilities:
. Ability and experience to have conversations with the CEO, Business owners and CTO/CDO
. Break down intricate business challenges, devise effective solutions, and focus on client needs.
. Craft high level innovative solution approach for complex business problems
. Utilize best practices and creativity to address challenges
. Leverage market research, formulate perspectives, and communicate insights to clients
. Establish strong client relationships
. Interact at appropriate levels to ensure client satisfaction
. Knowledge and Attributes:
. Ability to focus on detail with an understanding of how it impacts the business strategically.
. Excellent client service orientation.
. Ability to work in high-pressure situations.
. Ability to establish and manage processes and practices through collaboration and the understanding of business.
. Ability to create new and repeat business for the organization.
. Ability to contribute information on relevant vertical markets
. Ability to contribute to the improvement of internal effectiveness by contributing to the improvement of current methodologies, processes and tools.
Minimum Skills Required: Academic Qualifications and Certifications:
. BE/BTech or equivalent in Information Technology and/or Business Management or a related field.
. Scaled Agile certification desirable.
. Relevant consulting and technical certifications preferred, for example TOGAF.
Required Experience: 12-15 years
. Seasoned demonstrable experience in a similar role within a large scale (preferably multi- national) technology services environment.
. Very good understanding of Data, AI, Gen AI and Agentic AI
. Must have Data Architecture and Solutioning experience. Capable of E2E Data Architecture and GenAI Solution design.
. Must be able to work on Data & AI RFP responses as Solution Architect
. 10+ years of experience in Solution Architecting of Data & Analytics, AI/ML & Gen AI Technical Architect
. Develop Cloud-native technical approach and proposal plans identifying the best practice solutions meeting the requirements for a successful proposal. Create, edit, and review documents, diagrams, and other artifacts in response to RPPs RFQs and Contribute to and participate in presentations to customers regarding proposed solutions.
. Proficient with Snowflake, Databricks, Azure, AWS, GCP cloud, Data Engineering & AI tools
. Experience with large scale consulting and program execution engagements in AI and data
. Seasoned multi-technology infrastructure design experience.
. Seasoned demonstrable level of expertise coupled with consulting and client engagement experience, demonstrating good experience in client needs assessment and change management.
. Additional Job Description
Additional Job Description
Additional Career Level Description:
Knowledge and application:
. Seasoned, experienced professional has complete knowledge and understanding of area of specialization.
. Uses evaluation, judgment, and interpretation to select right course of action.
Problem solving:
. Works on problems of diverse scope where analysis of information requires evaluation of identifiable factors.
. Resolves and assesses a wide range of issues in creative ways and suggests variations in approach.
Interaction:
. Enhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.
. Works
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","snowflake, Gen AI, Ai, Cloud-native technical approach, AI tools, Databricks, Ml, Data Architecture, data engineering, Solution Architecting, Data Analytics, AWS, Azure, Gcp"
AWS -Senior Data Architect - R01543971,Brillio,Fresher,,"Gurugram, India",Login to check your skill match score,"Senior Data Specialist
Primary Skills
Athena, SQS, CloudWatch, Macie, Spark - Scala, Kinesis, CloudFormation, EMR, Open Search, DynamoDB, Amazon API Gateway, SCT, Redshift, DMS, Oozie
Secondary Skills
Python
Job requirements
About Brillio:
Brillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.
Brillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.
Work Location: Gurgaon/Hyderabad/Pune/Bangalore/Chennai
Job Title: AWS Data Architect/Senior Lead Data Engineer
Job Type: [Full-time
Job Summary:
We are looking for an experienced and dynamic AWS Data Architect to lead the design and implementation of data solutions in the cloud. This role will focus on leveraging AWS technologies to create scalable, reliable, and optimized data architectures that drive business insights and data-driven decision-making. As an AWS Data Architect, you will play a pivotal role in shaping the data strategy, implementing best practices, and ensuring the seamless integration of AWS-based data platforms, with a focus on services like Amazon Redshift, Aurora, and other AWS data services.
Key Responsibilities:
Data Architecture Design:
Lead the design and implementation of cloud-native data architectures on AWS, using services like Amazon Redshift, Amazon Aurora, Amazon S3, AWS Glue, and Amazon Athena.
Define data management strategies and ensure data models, structures, and systems are aligned with business goals.
Architect end-to-end data pipelines for ingesting, processing, storing, and analyzing large datasets using AWS services.
Cloud Data Infrastructure Management:
Design and implement highly available, scalable, and secure data infrastructure on AWS.
Lead the migration of on-premise data solutions to AWS cloud platforms, ensuring smooth transitions and minimizing operational impact.
Oversee performance tuning and optimization of data storage and query processing for Amazon Redshift and Aurora environments.
Data Integration and Processing:
Architect data integration solutions for combining data from various on-premise and cloud-based systems.
Design and implement ETL/ELT workflows utilizing AWS Glue, Lambda, and other relevant AWS tools to transform raw data into consumable formats.
Work with stakeholders to identify data sources and design appropriate interfaces and APIs for seamless integration into the cloud architecture.
Governance and Security:
Ensure that data architecture adheres to best practices for data security, governance, and compliance (e.g., GDPR, HIPAA).
Define and implement data privacy and access control policies using AWS Identity and Access Management (IAM), encryption, and audit logging.
Establish data lineage and metadata management strategies to ensure transparency and traceability in data workflows.
Collaboration and Leadership:
Act as a technical leader and provide guidance on cloud-based data solutions to internal teams, including Data Engineers, Data Scientists, and DevOps engineers.
Collaborate with business stakeholders and leadership to understand requirements and design data solutions that meet business needs.
Lead the development of best practices and standards for AWS-based data engineering solutions and ensure these are followed across the organization.
Innovation and Optimization:
Continuously evaluate and introduce new AWS data technologies, tools, and features to optimize performance, scalability, and cost-efficiency.
Stay up-to-date with the latest trends in cloud data architecture and AWS service updates to recommend improvements and innovations in existing solutions.
Documentation and Knowledge Sharing:
Create and maintain clear and detailed documentation for data architecture, design decisions, and system configurations.
Promote a culture of knowledge sharing by mentoring team members and providing ongoing training on AWS data technologies.","Amazon Athena, Amazon Aurora, Amazon Redshift, Amazon S3, AWS Glue, ELT, Etl"
AWS -Data Architect - R01545805,Brillio,10-12 Years,,"Gurugram, India",Login to check your skill match score,"Data Architect
Primary Skills
AWS, Python, Advanced SQL
Deep expertise in S3, Redshift, Aurora, Glue and Lambda services.
Job requirements
About Brillio:
Brillio is one of the fastest growing digital technology service providers and a partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Brillio, renowned for its world-class professionals, referred to as Brillians, distinguishes itself through their capacity to seamlessly integrate cutting-edge digital and design thinking skills with an unwavering dedication to client satisfaction.
Brillio takes pride in its status as an employer of choice, consistently attracting the most exceptional and talented individuals due to its unwavering emphasis on contemporary, groundbreaking technologies, and exclusive digital projects. Brillio's relentless commitment to providing an exceptional experience to its Brillians and nurturing their full potential consistently garners them the Great Place to Work certification year after year.
Job Description:
10 years of IT experience with deep expertise in S3, Redshift, Aurora, Glue and Lambda services.
At least one instance of proven experience in developing Data platform end to end using AWS
Hands-on programming experience with Data Frames, Python, and unit testing the python as well as Glue code.
Experience in orchestrating mechanisms like Airflow, Step functions etc.
Experience working on AWS redshift is Mandatory. Must have experience writing stored procedures, understanding of Redshift data API and writing federated queries
Experience in Redshift performance tuning. Good in communication and problem solving.
Very good stakeholder communication and management.","Step Functions, Aurora, Airflow, Glue, S3, Lambda, Advanced Sql, Redshift, Python, AWS"
Data Architect,Kezan Consulting,12-16 Years,,Pune,Information Technology,"Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems.
Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP).
Strong communication skillswith the ability to translate complex data concepts into business insights.","Cloud Platforms, Data Modeling, Big Data Technologies, Data Architecture, Data Warehousing"
Data Architect,Kanini Software Solutions,10-15 Years,,"Bengaluru, Chennai",Software,"We are looking forsomeone who has a deep understanding of Big Data and Cloud architecture and isexcited to advance innovative analytics solutions. As a Data Architect, you effectively communicate ideas and concepts to peers and bring onboard the experience of leading projects that support the organization s businessobjectives and goals.
You areall set to:
Architect andimplement data ingestion, data validation, and data transformation pipelines. Collaboratewith the Product, Development, and Enterprise Data teams to design and maintainbatch and streaming integrations across a variety of data domains andplatforms.
You are someone who can:
Take ownership in building solutions and proposingarchitectural designs related to building efficient and timely data ingestionand transformation.
processes geared towards analytics workloads.
Manage code deployment to various environments.
Be proficient at positively critiquing andsuggesting improvements via code reviews
Work with stakeholders to define and develop dataingest, validation, and transform pipelines.
Troubleshoot data pipelines and resolve issues inalignment with SDLC.
Ability to diagnose and troubleshoot data issues,recognizing common data integration andtransformation patterns
Estimate, track, and communicate status of assigneditems to a diverse group of stakeholders required
You bring in:
12 years experience in the industry.
Experience and knowledge of Big Data Architectures,on cloud and on premise
Proficiency in AWS Collection Services: Kinesis,Kafka, Database Migration Service
Proficiency in AWS main Storage Service: S3, EBS,EFS
Proficiency in AWS main Compute Service: EC2,Lambda, ECS, EKS
Proven experience in: Java, Working experiencewith: AWS Athena and Glue Pyspark, EMR,DynamoDB, Redshift, Kinesis, Lambda, Apache Spark, Databricks on AWS, Snowflakeon AWS
Proficient in AWS Redshift, S3, Glue, Athena, DynamoDB
AWS Certification: AWS Certified SolutionsArchitect and/or AWS Certified Data Analytics
Working experience with Agile Methodology andKanban
We Prefer:
Strong experiencein Azure is preferred with hands-on experience in 2 or more of these skills:Azure SQL DB, Azure SQL Managed Instance, Azure Data Lake Store, Azure CosmosDB, Azure Database for PostgreSQL, Azure Database for MySQL
Your qualification is:
B.E/B.Tech/M.C.A/MSc(preferably in Computer Science/IT)
Role:Data warehouse Architect / Consultant
Industry Type:Software Product
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:DBA / Data warehousing","Agile Methodology, MySQL, Data Architect, Cosmos DB, Sql, Database, AWS, Sdlc"
Big Data Architect,World Of Opportunities For Women LLP,8-14 Years,,"Bengaluru, Chennai, Pune",Login to check your skill match score,"Big Data /Cloud Architect
Good understanding of Data Mesh Architecture
Experience with Data Vault modelling and data modeling in general
Experience in data integration and solution designs as Data architect
Understanding of multiple ETL, Reporting and CI/CD tools
As part of the Infosys delivery team, your primary role would be to provide best fit architectural solutions for one or more projects.
You would also provide technologyconsultation and assist in defining scope and sizing of work
You would implement solutions, create technology differentiation and leverage partner technologies.
Additionally, you would participate in competency development with the objective of ensuring the best-fit and high-quality technical solutions.
You would be a key contributor in creating thought leadership within the area of technology specialization and in compliance with guidelines, policies and norms of Infosys. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!
Knowledge of architectural design patterns, performance tuning, database and functional designs","Cloud Platforms, ETL Processes, Machine Learning, Nosql, Hadoop, Data Modeling, Spark, Data Warehousing, Data Governance, Sql"
AEP - Data Architect,Codilar,10-12 Years,,"Noida, Bengaluru",E-Commerce Platforms,"10+ years of strong experience with data transformation & ETL on large datasets.
5+ years of Data Modeling experience (Relational, Dimensional, Columnar, Big Data).
5+ years of complex SQL or NoSQL experience.
5+ years of experience with industry ETL tools (Informatica, Unifi).
Experience as an enterprise technical or engineer consultant.
Data and Technology:
Experience with designing customer-centric datasets (CRM, Call Center, Marketing, Offline, Point of Sale, etc.).
Knowledge of Adobe Experience Platform (AEP) is mandatory.
Advanced Data Warehouse concepts.
Experience with Big Data technologies (Hadoop, Spark, Redshift, Snowflake, Hive, Pig, etc.).
Experience with Reporting Technologies (Tableau, PowerBI).
Experience & knowledge with Adobe Experience Cloud solutions.
Experience & knowledge with Digital Analytics or Digital Marketing.
Software Development and Scripting:
Experience in professional software development.
Experience in programming languages (Python, Java, or Bash scripting).
Business and Communication:
Experience with Business Requirements definition and management, structured analysis, process design, use case documentation.
Strong verbal & written communication skills to interface with Sales teams and lead customers to successful outcomes.
Demonstrated exceptional organizational skills and ability to multi-task simultaneous different customer projects.
Must be self-managed, proactive, and customer-focused.","AEP, Data Modeling, Big Data, Sql, Python, Etl"
Azure Data Architect,iLink Digital,9-14 Years,,"Chennai, Bengaluru, Pune",Financial Services,"At least 12+ years of overall experience mainly in the field of DataWarehousing or DataAnalytics
Solid experience with Datamodeling using various modeling methodologies/Techniques like dimensional modeling, datavault, 3rdNormalization ..etc
Experience in designing common/reusable domain specific datasets for DataLakes
Experience in PowerBI Tabular models for self-service
Experience in working with Azure dataplatform tools like ADF, Synapse, databricks, Zen2 storage
Solid understanding of various Datastructures /DataFormats and the best usage for those in Big Data/Hadoop/DataLake environment (Relational vs Parquet vs ORC ..etc)
Good experience with the datain Supply Chain and Manufacturing domains.
Working experience with datafrom SAP ERP systems is preferable
Should be an expert with SQL queries and dataexploration methods.
Other technical skills include SparkSQL, Python, PySpark, Any Datamodeling tools like ERWin.","azure data platforms, Data Modeling, Power Bi, Data Warehousing, Tableau, Data Analytics"
Data Architect,Tmf Group,8-13 Years,,"Delhi, Kolkata, Mumbai",BPO,"Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.
Technical Expertise:
Strong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.
Proficiency in SQL, Python, Spark, and PowerShell.
Experience in data modeling, ETL/ELT, and data warehousing.
Understanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).
Strong problem-solving and analytical skills.
Excellent communication and stakeholder management.
Exposure to hybrid cloud environments (Azure on-premises)
Key Requirements
Knowledge of data mesh and data fabric architectures.
What s in it for you
Pathways for Career Development
Work with colleagues and clients around the world on interesting and challenging work.
We provide internal career opportunities, so you can take your career further within TMF.
Continuous development is supported through global learning opportunities from the TMF Business Academy.
Making an Impact
You ll be helping us to make the world a simpler place to do business for our clients.
Through our corporate social responsibility program, you ll also be making a difference in the communities where we work.
A Supportive Environment
Strong feedback culture to help build an engaging workplace.
Our inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.","Powershell, Data Warehousing, Sql, Python, Azure Data, Data Architecture"
Data Architect,Tmf Group,8-13 Years,,Bengaluru,BPO,"Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.
Technical Expertise:
Strong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.
Proficiency in SQL, Python, Spark, and PowerShell.
Experience in data modeling, ETL/ELT, and data warehousing.
Understanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).
Strong problem-solving and analytical skills.
Excellent communication and stakeholder management.
Exposure to hybrid cloud environments (Azure on-premises)
Key Requirements
Knowledge of data mesh and data fabric architectures.
What s in it for you
Pathways for Career Development
Work with colleagues and clients around the world on interesting and challenging work.
We provide internal career opportunities, so you can take your career further within TMF.
Continuous development is supported through global learning opportunities from the TMF Business Academy.
Making an Impact
You ll be helping us to make the world a simpler place to do business for our clients.
Through our corporate social responsibility program, you ll also be making a difference in the communities where we work.
A Supportive Environment
Strong feedback culture to help build an engaging workplace.
Our inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.","Powershell, Azure Data Bricks, Spark, Sql, Python, Azure Data Lake, Data Architecture"
Senior Data Architect-GCP,Reflections Info Systems,7-13 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Pune",IT Management,"Responsibilities include:
Integrate machine learning workflows with data pipelines and analytics tools.
Define data governance frameworks and manage data lineage.
Lead data modeling efforts to ensure consistency, accuracy, and performance across systems.
Optimize cloud infrastructure for scalability, performance, and reliability.
Mentor junior team members and ensure adherence to architectural standards.
Collaborate with DevOps teams to implement Infrastructure as Code (Terraform, Cloud Deployment Manager).
Ensure high availability and disaster recovery solutions are built into data systems.
Conduct technical reviews, audits, and performance tuning for data solutions.
Design solutions for multi-region and multi-cloud data architecture.
Stay updated on emerging technologies and trends in data engineering and GCP.
Drive innovation in data architecture, recommending new tools and services on GCP.
Primary Skills :
7+ years of experience in data architecture, with at least 3 years in GCP environments.
Expertise in BigQuery, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage, and related GCP services.
Strong experience in data warehousing, data lakes, and real-time data pipelines.
Proficiency in SQL, Python, or other data processing languages.
Experience with cloud security, data governance, and compliance frameworks.
Strong problem-solving skills and ability to architect solutions for complex data environments.
Google Cloud Certification (Professional Data Engineer, Professional Cloud Architect) preferred.
Leadership experience and ability to mentor technical teams.
Excellent communication and collaboration skills.","Analytics, Data Modeling, Sql, Python, Performance Tuning, Machine Learning, Data Architecture"
Data Architect,Reflections Info Systems,10-14 Years,,"Thiruvananthapuram / Trivandrum, Chennai, Bengaluru",IT Management,"Work as part of a team to Design and Architect Large Data Platforms, Analytics and AI solutions
Participate in the development of cloud data warehouses, data as a service, business intelligence solutions
Ability to provide solutions that are forward-thinking in data integration
Programming experience in Scala or Python, SQL
Working experience in Apache Spark is highly preferred
Familiarity with some of these AWS and Azure Services like S3, ADLS Gen2, AWS, Redshift, AWS Glue, Azure Data Factory, Azure Synapse
Certifications :
AWS/ other Cloud Services Architect Certifications
SnowPro Core Certification
Databricks Professional level Certification
Primary Skills :
Must Have: Bachelors or Masters degree in Computer Science or a related field
Must Have: 7+ years of hands-on experience Data Engineering, Building Data Pipelines, Warehouses, Architecting Data platforms, Data Models, Data Science, AI use cases etc.
Proven experience as a Data Architect or in a similar role.
Proficiency in developing and maintaining our data architecture, ensuring its scalability, security, and alignment with business objectives.
Good experience in MDM/PIM Solution Implementation
Soft Skills :
Excellent communication and interpersonal skills, with the ability to articulate ideas and discuss technical concepts with both technical and non-technical team members
Clear and effective documentation, code comments, and the ability to write technical reports or emails are essential.
Collaboration is often an integral part of software development, Data Projects. Being able to work well with others, share knowledge, and contribute positively to a team is crucial.
Strong problem-solving and analytical skills, with the ability to make sound decisions under pressure.
Efficiently managing ones time and meeting deadlines is critical in a fast-paced development environment.
Understanding the needs and expectations of end-users or clients and developing solutions that meet or exceed those expectations. . This is to notify jobseekers that some fraudsters are promising jobs with Reflections Info Systems for a fee. Please note that no payment is ever sought for jobs in Reflections. We contact our candidates only through our official website or LinkedIn and all employment related mails are sent through the official HR email id. for any clarification/ alerts on this subject.","Gcp, Database Management, Data Architect, Data Modeling, Data Analytics, Sql, Python"
Associate Data Architect,Murugappa Group,5-10 Years,,Chennai,Financial Services,"Purpose
The candidate is responsible for designing, creating, deploying, and maintaining an organization's data architecture.
To ensure that the organization's data assets are managed effectively and efficiently, and that they are used to support the organization's goals and objectives.
Responsible for ensuring that the organization's data is secure, and that appropriate data governance policies and procedures are in place to protect the organization's data assets.
Key Responsibilities
Responsibilities will include but will not be restricted to:
Responsible for designing and implementing a data architecture that supports the organization's business goals and objectives.
Developing data models, defining data standards and guidelines, and establishing processes for data integration, migration, and management.
Create and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organization's data assets.
Ensure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis.
Organization's data is secure, and that appropriate data governance policies and procedures are in place to protect the organization's data assets.
Work closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organization's data architecture is integrated and aligned with other IT systems and applications.
Stay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organization's data architecture.
Communicate with stakeholders across the organization to understand their data needs and ensure that the organization's data architecture is aligned with the organization's strategic goals and objectives.
Mandatory Skills
AWS Cloud Services
Compute EC2
Storage - S3 mandatory Other storage components
AWS Services Like IAM,S3.
Programming Python, Pyspark, Lamda.
Knowledge on ETL Glue (Mandatory), DMS.
Databases Data Bricks , RDBMS skills added advantages like Oracle, SQLServer, MPP databases like Snowflake, Redshift .
Knowledge on Data modelling and ETL process is mandatory.
Architecture Data Mesh, Medallion and EDW - Data Modelling.
Technical requirements
Bachelor's or master's degree in Computer Science or a related field.
Certificates in Database Management will be preferred.
Expertise in data modeling and design, including conceptual, logical, and physical data models, and must be able to translate business requirements into data models.
Proficient in a variety of data management technologies, including relational databases, NoSQL databases, data warehouses, and data lakes.
Expertise in ETL processes, including data extraction, transformation, and loading, and must be able to design and implement data integration processes.
Experience with data analysis and reporting tools and techniques and must be able to design and implement data analysis and reporting processes.
Familiar with industry-standard data architecture frameworks, such as TOGAF and Zachman, and must be able to apply them to the organization's data architecture.
Familiar with cloud computing technologies, including public and private clouds, and must be able to design and implement data architectures that leverage cloud computing.
Qualitative Requirements
Able to effectively communicate complex technical concepts to both technical and non-technical stakeholders.
Strong analytical and problem-solving skills.
Must be able to inspire and motivate their team to achieve organizational goal.","aws cloud services, Cloud Computing, Databases, Data Modelling, programming, Etl"
Data Architect,Tmf Group,8-12 Years,,"Hyderabad, Chennai, Pune",BPO,"Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.
Technical Expertise:
Strong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.
Proficiency in SQL, Python, Spark, and PowerShell.
Experience in data modeling, ETL/ELT, and data warehousing.
Understanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).
Strong problem-solving and analytical skills.
Excellent communication and stakeholder management.
Exposure to hybrid cloud environments (Azure on-premises)
Key Requirements
Knowledge of data mesh and data fabric architectures.
What s in it for you
Pathways for Career Development
Work with colleagues and clients around the world on interesting and challenging work.
We provide internal career opportunities, so you can take your career further within TMF.
Continuous development is supported through global learning opportunities from the TMF Business Academy.
Making an Impact
You ll be helping us to make the world a simpler place to do business for our clients.
Through our corporate social responsibility program, you ll also be making a difference in the communities where we work.
A Supportive Environment
Strong feedback culture to help build an engaging workplace.
Our inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.","Powershell, Azure Data Bricks, Data Architecture, Datawarehousing, Sql, Python"
Data Architect,Reflections Info Systems,12-20 Years,,"Chennai, Pune",IT Management,"Responsibilities include:
Lead data platforms, engineering, product ownership, and architecture teams to drive business growth through next-generation capabilities.
Foster innovation and thought leadership by adapting to industry trends, leveraging data for high-value product creation.
Collaborate with business leaders, effectively communicating the value of data-driven solutions across diverse domains.
Develop advanced data analytics, utilizing cutting-edge technology to support various business functions.
Cultivate strong partnerships with internal and external stakeholders, alongside fellow analytics professionals.
Design, establish, and maintain cost-effective data platforms and solutions while prioritizing return on investment and talent growth for high performance.
Primary Skills :
Azure , AWS, GCP cloud data engineering
Cloud-based databases (Synapse, Databricks, ,Atacama (MDM), Snowflake, Redshift)
Data integration techniques (API, stream, file) using DBT, SQL/PySpark, Python.
Implementing data products through data mesh / fabric concepts
Data modeling
ETL development
Database management
Managing teams budgets, portfolios and driving value.
Ability to define and implement prototypes and MVPs / RATs","Gcp, Database Management, Data Architect, Data Modeling, Data Analytics, Sql, Python"
Data Architect,Reflections Info Systems,12-20 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru",IT Management,"Responsibilities include:
Lead data platforms, engineering, product ownership, and architecture teams to drive business growth through next-generation capabilities.
Foster innovation and thought leadership by adapting to industry trends, leveraging data for high-value product creation.
Collaborate with business leaders, effectively communicating the value of data-driven solutions across diverse domains.
Develop advanced data analytics, utilizing cutting-edge technology to support various business functions.
Cultivate strong partnerships with internal and external stakeholders, alongside fellow analytics professionals.
Design, establish, and maintain cost-effective data platforms and solutions while prioritizing return on investment and talent growth for high performance.
Primary Skills :
Azure , AWS, GCP cloud data engineering
Cloud-based databases (Synapse, Databricks, ,Atacama (MDM), Snowflake, Redshift)
Data integration techniques (API, stream, file) using DBT, SQL/PySpark, Python.
Implementing data products through data mesh / fabric concepts
Data modeling
ETL development
Database management
Managing teams budgets, portfolios and driving value.
Ability to define and implement prototypes and MVPs / RATs","Gcp, Database Management, Data Architect, Data Modeling, Data Analytics, Sql, Python"
Data Architect,Genzeon Corporation,15-20 Years,,Hyderabad,Information Technology,"We are seeking an experienced Data Architect to join our team. The ideal candidate should have a deep understanding of data architecture principles and practices, as well as experience designing and implementing data solutions. The Data Architect will be responsible for developing and maintaining data models, designing data storage solutions, and ensuring data quality and integrity.
Responsibilities:
Design and develop scalable and efficient data solutions that meet business requirements.
Create and maintain data models and documentation.
Develop and implement data migration and transformation strategies.
Work with cross-functional teams to identify and resolve data-related issues
Collaborate with data engineers and data scientists to ensure data accuracy and consistency.
Ensure compliance with data security and privacy policies and regulations.
Stay up-to-date with emerging technologies and trends in data architecture.
Core Skills for Data Engineer:
Proficiency in Python and SQL
Experience working with Linux and AWS
Strong analytical and problem-solving skills
Ability to work independently and in a team environment
Excellent communication and interpersonal skills
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus
Experience with data warehousing, ETL, and data integration is highly desirable
Knowledge of big data technologies (e.g., Hadoop, Spark) is a plus.
Qualifications:
Bachelor's or Master's degree in computer science, information technology, or related field
Minimum of 15 years of experience in data architecture or related field
Experience designing and implementing data solutions for enterprise-level organizations
Strong knowledge of data modeling and database design principles
Knowledge of data governance and compliance requirements
Familiarity with Agile development methodologies and DevOps practices
Must have skills:
Experience working with Linux and AWS
Strong analytical and problem-solving skills
Ability to work independently and in a team environment
Excellent communication and interpersonal skills","python, linux, Data Modeling, Sql, aws, Etl"
Data Architect,Genzeon Corporation,15-20 Years,,Hyderabad,Information Technology,"We are seeking an experienced Data Architect to join our team. The ideal candidate should have a deep understanding of data architecture principles and practices, as well as experience designing and implementing data solutions. The Data Architect will be responsible for developing and maintaining data models, designing data storage solutions, and ensuring data quality and integrity.
Responsibilities:
Design and develop scalable and efficient data solutions that meet business requirements.
Create and maintain data models and documentation.
Develop and implement data migration and transformation strategies.
Work with cross-functional teams to identify and resolve data-related issues
Collaborate with data engineers and data scientists to ensure data accuracy and consistency.
Ensure compliance with data security and privacy policies and regulations.
Stay up-to-date with emerging technologies and trends in data architecture.
Core Skills for Data Engineer:
Proficiency in Python and SQL
Experience working with Linux and AWS
Strong analytical and problem-solving skills
Ability to work independently and in a team environment
Excellent communication and interpersonal skills
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus
Experience with data warehousing, ETL, and data integration is highly desirable
Knowledge of big data technologies (e.g., Hadoop, Spark) is a plus.
Qualifications:
Bachelor's or Master's degree in computer science, information technology, or related field
Minimum of 15 years of experience in data architecture or related field
Experience designing and implementing data solutions for enterprise-level organizations
Strong knowledge of data modeling and database design principles
Knowledge of data governance and compliance requirements
Familiarity with Agile development methodologies and DevOps practices
Must have skills:
Experience working with Linux and AWS
Strong analytical and problem-solving skills
Ability to work independently and in a team environment
Excellent communication and interpersonal skills","python, linux, Data Modeling, Sql, aws, Etl"
Data Architect,Purview India Consulting And Services Llp,12-22 Years,,Pune,Banking,"Responsibilities
Primarily supporting the design, development and deployment of data and analytics-led solutions
Produce application specific designs based on a pre-defined high-level global architecture/platform.
Contribute to evolution and development of the global architecture/platform.
Provide necessary governance to ensure alignment with program principles and agreed architectures, including creating and presenting material to relevant governance bodies.
Support both the business and engineering teams regarding elaboration of requirements and lower-level designs documenting key decisions and risks.
Skills
Strong data background (data storage, transformation, event processing, APIs, IAM, security)
Beneficial to have knowledge/experience of analytics solutions (ML/AI & BI) - note though that this is not a Data Scientist role.
Google Cloud Platform & Hadoop experience/skills (as this is where the solutions are being developed/deployed)
Proven strength in conceptual and logical thinking, ability to abstract information and look at the bigger picture. Proven analysis skills. Experience in managing and resolving complex issues.
Ability to articulate and present solutions at the right level for a wide audience.
Experience of working within a large, complex and geographically dispersed programme","Data Storage, Gcp, Hadoop, Data Architecture, Api"
Data Architect,Health Catalyst,9-11 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Principal Data Architect / Lead Data Platform Architect / Enterprise Data Architect / Solutions Architect Data Engineering
About Company:
The healthcare industry is the next great frontier of opportunity for software development, and Health Catalyst is one of the most dynamic and influential companies in this space. We are working on solving national-level healthcare problems, and this is your chance to improve the lives of millions of people, including your family and friends. Health Catalyst is a fast-growing company that values smart, hardworking, and humble individuals. Each product team is a small, mission-critical team focused on developing innovative tools to support Catalyst's mission to improve healthcare performance, cost, and quality.
Health Catalyst is expanding and maintains a large suite of Improvement Apps that contribute to healthcare analytics and process improvement solutions. This includes products that manage the care of health system populations, better serve patients at the point of care, reduce health system costs, and reduce clinician workload.
Job Overview
We are seeking a seasoned Principal Data Architect to design, implement, and maintain scalable data ingestion and processing systems for healthcare and clinical data pipelines. This role is ideal for someone who thrives in building real-time data platforms using Kafka, microservices, and distributed systems on AWS. You will lead architectural decisions, influence data governance practices, and support systems that process billions of patient records daily.
Key Responsibilities
Architect and lead the development of scalable, secure, and high-performance data platforms on AWS.
Own and optimize Kafka-based pipelines handling real-time and batch ingestion of clinical data (HL7v2, CCDA).
Design and manage Kafka Connect infrastructure with custom SMTs and JDBC, S3, MongoDB, and OpenSearch connectors.
Implement fault-tolerant systems using DLQs, Prometheus, Grafana, and CloudWatch for observability and alerting.
Drive deduplication strategies and stream processing (Kafka Streams, Flink) to reduce downstream load.
Collaborate with parser teams to optimize Java-based services that convert HL7 and CCDA into structured data for Kafka brokers.
Define and enforce data quality and governance best practices across the pipeline.
Provide mentorship and technical leadership to a team of engineers and data professionals.
Engage with product, compliance, and analytics teams to align the data platform with broader business goals.
Required Qualifications
9+ years of experience in software/data engineering with a focus on distributed systems.
Expertise in Kafka ecosystem (brokers, Kafka Connect, Streams, Schema Registry).
Deep knowledge of AWS services (EC2, S3, RDS, CloudWatch, IAM, MSK, etc.).
Strong Java development skills; familiarity with multithreading, concurrency, and system design patterns.
Experience with microservices, Docker, and CI/CD pipelines.
Proficiency with clinical data formats like HL7v2 and CCDA.
Experience in performance tuning large-scale data systems (billions of records/day).
Knowledge of database technologies: MySQL/PostgreSQL, MongoDB, Snowflake, Cassendra, DynamoDB
Excellent communication skills and ability to work cross-functionally.
Preferred Qualifications
Experience with Flink or Spark for streaming and batch processing.
Knowledge of FHIR, HIPAA compliance, and healthcare domain challenges.
Experience with Confluent Kafka
Background in data deduplication strategies, ETL optimizations, and distributed caching.
Familiarity with Agile and Scrum methodologies.","HL7v2, Flink, FHIR, snowflake, Java, Ccda, PostgreSQL, Dynamodb, Kafka, Hipaa, Docker, MySQL, Spark, MongoDB, AWS"
Data Architect,Technogen India Pvt. Ltd.,Fresher,,"Bengaluru, India",Login to check your skill match score,"Job Description
We are looking for an experienced Data Architect to design, implement, and oversee scalable, high-performance data architectures that align with business objectives. The ideal candidate will lead data strategy, governance, integration, and architecture, ensuring secure, cost-efficient, and well-optimized data solutions.
This role includes data warehousing, data mesh principles, ETL, BI platform integration, estimations, and review of implementation, ensuring best practices are followed for data security, governance, and performance.
Key Responsibilities
Define and implement an enterprise-wide data architecture that supports scalability, performance, and business needs.
Develop data models (conceptual, logical, and physical) for structured and unstructured data.
Architect Data Warehouses, Data Lakes, and Data Mesh-based solutions based on business requirements.
Ensure data interoperability across APIs, streaming platforms, and BI tools.
Define best practices for data ingestion, transformation, and storage.
Provide accurate effort estimations for data platform implementation, ETL pipelines, and BI integration.
Optimize cloud storage and compute costs (AWS/Azure).
Guide teams on efficient resource utilization to minimize unnecessary expenses.
Work with project managers to estimate timelines, resources, and infrastructure needs for data projects.
Review end-to-end implementation of data solutions to ensure architectural compliance.
Conduct design and code reviews for ETL processes, and BI reporting layers.
Work closely with data engineers, BI developers, and analytics teams to ensure best practices in data modeling, query optimization, and indexing strategies.
Troubleshoot scalability and performance issues, providing guidance on tuning and refactoring.
Design and oversee ETL pipelines in cloud-native solutions.
Optimize data storage, partitioning, and indexing strategies for better performance.
Implement Master Data Management (MDM) strategies to maintain a Single Source of Truth (SSOT).
Define best practices for data replication, caching, and real-time streaming.
Design and implement cloud-native data solutions in AWS/Azure.
Architect Data Warehouses, Data Lakes, and Lakehouses using AWS Glue/Snowflake/BigQuery/Azure Synapse.
Lead the adoption of Data Mesh principles for decentralized data ownership.
Integrate data pipelines with BI platforms such as Power BI, QuickSight, Tableau or ThoughtSpot.
Ensure data models are optimized for analytical queries in BI platforms.
Collaborate with BI teams to ensure self-service analytics enablement.
Optimize query performance, dashboard responsiveness, and reporting efficiency.
Establish and enforce data governance policies, standards, and best practices.
Ensure data integrity, consistency, and quality across all domains.
Oversee data lineage, metadata management, and cataloging for discoverability.
Ensure compliance with HIPAA, CCPA, and other regulatory requirements.
Implement role-based access controls (RBAC), encryption, and anonymization techniques.
Monitor and audit data access and usage for security compliance.
Ensure secure data transmission and storage with encryption techniques.
Work closely with data engineers, BI developers, business analysts, and product teams to understand data requirements.
Act as a bridge between technical and non-technical teams, ensuring alignment on data strategy.
Provide technical leadership in data-driven projects and proof-of-concepts (PoCs).
Required Skills & Experience
Must-Have Skills & Experience
Proven experience in data architecture, data modeling, and database design.
Expertise in AWS Glue/Azure Data Factory and cloud-native data services.
Strong knowledge of ETL/ELT frameworks (Informatica/Talend/SSIS/AWS Glue).
Hands-on experience with SQL, NoSQL, and cloud-native databases.
Experience with Big Data processing technologies (Aws Kinesis, Azure Synapse, Amazon Managed Streaming).
Strong understanding of BI platform integration and query performance optimization.
Knowledge of data security, encryption, and compliance frameworks (HIPAA).
Ability to estimate efforts and optimize costs for cloud-based data solutions.
Experience in data governance, metadata management, and data lineage tracking.
Strong analytical and problem-solving skills with the ability to troubleshoot performance issues.
Excellent collaboration and communication skills to work with technical and business stakeholders.
Nice-to-Have Skills
Familiarity with multi-cloud and hybrid data architectures.
Hands-on experience with Data Mesh principles and decentralized data ownership.
Certifications in AWS Certified Data Analytics, Google Professional Data Engineer, or Azure Data Engineer.","Big Data processing technologies, data lineage tracking, BI platform integration, Amazon Managed Streaming, compliance frameworks, AWS Kinesis, Data Security, Metadata Management, AWS Glue, Informatica, SSIS, Sql, Nosql, Azure Synapse, Encryption, Azure Data Factory, Data Governance, Talend"
Data Architect,NTT DATA North America,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Req ID: 324663
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Architect to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Key Responsibilities:
Develop and articulate long-term strategic goals for data architecture vision and establish data standards for enterprise systems.
Utilize various cloud technologies, including Azure, AWS, GCP, and data platforms like Databricks and Snowflake.
Conceptualize and create an end-to-end vision outlining the seamless flow of data through successive stages.
Institute processes for governing the identification, collection, and utilization of corporate metadata, ensuring accuracy and validity.
Implement methods and procedures for tracking data quality, completeness, redundancy, compliance, and continuous improvement.
Evaluate and determine governance, stewardship, and frameworks for effective data management across the enterprise.
Develop comprehensive strategies and plans for data capacity planning, data security, life cycle data management, scalability, backup, disaster recovery, business continuity, and archiving.
Identify potential areas for policy and procedure enhancements, initiating changes where required for optimal data management.
Formulate and maintain data models and establish policies and procedures for functional design.
Offer technical recommendations to senior managers and technical staff in the development and implementation of databases and documentation.
Stay informed about upgrades and emerging database technologies through continuous research.
Collaborate with project managers and business leaders on all projects involving enterprise data.
Document the data architecture and environment to ensure a current and accurate understanding of the overall data landscape.
Design and implement data solutions tailored to meet customer needs and specific use cases.
Provide thought leadership by recommending the most suitable technologies and solutions for various use cases, spanning from the application layer to infrastructure.
Basic Qualifications:
8+ years of hands-on experience with various database technologies
6+ years of experience with Cloud-based systems and Enterprise Data Architecture, driving end-to end technology solutions.
Experience with Azure, Databricks, Snowflake
Knowledgeable on concepts of GenAI
Ability to travel at least 25%.
Preferred Skills:
Possess certifications in AWS, Azure, and GCP to complement extensive hands-on experience.
Demonstrated expertise with certifications in Snowflake.
Valuable Big 4 Management Consulting experience or exposure to multiple industries.
Undergraduate or graduate degree preferred.
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com
NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","GenAI, snowflake, Gcp, Databricks, Azure, AWS"
Data Architect,Affine,Fresher,,"Chennai, India",Login to check your skill match score,"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.
Key Responsibilities:
Architect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.
Develop scalable ETL pipelines using AWS Glue, Lambda, and PySpark to support data transformation, ingestion, and migration.
Design and optimize data models following Medallion architecture, Data Mesh, and Enterprise Data Warehouse (EDW) principles.
Implement data governance, security, and compliance best practices using IAM policies, encryption, and data masking.
Work with MPP databases such as Snowflake, Redshift, or Databricks, ensuring performance tuning, indexing, and query optimization.
Collaborate with cross-functional teams, including data engineers, analysts, and business stakeholders, to design efficient data integration strategies.
Ensure high availability and reliability of data solutions by implementing monitoring, logging, and automation in AWS.
Evaluate and recommend best practices for ETL workflows, data pipelines, and cloud-based data warehousing solutions.
Troubleshoot performance bottlenecks and optimize query execution plans, indexing strategies, and data partitioning.
Required Qualifications & Skills:
Strong expertise in AWS Cloud Services: Compute (EC2), Storage (S3), and security (IAM).
Proficiency in programming languages: Python, PySpark, and AWS Lambda.
Mandatory experience in ETL tools: AWS Glue and DMS for data migration and transformation.
Expertise in MPP databases: Snowflake, Redshift, or Databricks; knowledge of RDBMS (Oracle, SQL Server) is a plus.
Deep understanding of data modeling techniques: Medallion architecture, Data Mesh, EDW principles.
Experience in designing and implementing large-scale, high-performance data solutions.
Strong analytical and problem-solving skills, with the ability to optimize data pipelines and storage solutions.
Excellent communication and collaboration skills, with experience working in agile environments.
Preferred Qualifications:
AWS Certification (AWS Certified Data Analytics, AWS Certified Solutions Architect, or equivalent).
Experience with real-time data streaming (Kafka, Kinesis, or similar).
Familiarity with Infrastructure as Code (Terraform, CloudFormation).
Understanding of data governance frameworks and compliance standards (GDPR, HIPAA, etc.","ETL processes, snowflake, Data Mesh, Medallion architecture, Monitoring, AWS Cloud Services, Dms, IAM policies, Data partitioning, Indexing, Performance Tuning, Pyspark, AWS Glue, Data Modeling, Data Governance, Encryption, Query Optimization, Python, Logging, Aws Lambda, data masking, Redshift, Automation, Databricks"
Data Architect - R01545805,Brillio,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Data Architect
Primary Skills
AWS, Python, Advanced SQL
Job requirements
Years of experience- 10+ Years
JOB role- AWS Architect
8 year of IT experiences with deep expertise in S3, Redshift, Aurora, Glue and Lambda services.
At least one instance of proven experience in developing Data platform end to end using AWS
Hands-on programming experience with Data Frames, Python, and unit testing the python as well as Glue code.
Experience in orchestrating mechanisms like Airflow, Step functions etc.
Experience working on AWS redshift is Mandatory. Must have experience writing stored procedures, understanding of Redshift data API and writing federated queries
Experience in Redshift performance tunning. Good in communication and problem solving.
Very good stakeholder communication and management.","Step Functions, Aurora, Airflow, Glue, Data Frames, S3, Lambda, Advanced Sql, Redshift, Python, AWS"
"Senior IT Architect, Data Architect, Platinion",Boston Consulting Group (BCG),7-9 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
What You'll Do
Design and implement a data architecture that supports the organization's business goals and objectives.
Develop data models, define data standards and guidelines, and establish processes for data integration, migration, and management.
Create and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organization's data assets.
Ensure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis.
Work closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organization's data architecture is integrated and aligned with other IT systems and applications.
Stay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organization's data architecture.
Communicate with stakeholders across the organization to understand their data needs and ensure that the organization's data architecture is aligned with the organization's strategic goals and objectives.
What You'll Bring
A BTech / MTech degree in Computer Science or a related field
At least 7+ years of experience in working on data architecture
Expertise in data modeling and design, including conceptual, logical, and physical data models,
and must be able to translate business requirements into data models
Proficient in a variety of data management technologies, including relational databases,
NoSQL databases, data warehouses, and data lakes
Expertise in ETL processes, including data extraction, transformation, and loading, and must
be able to design and implement data integration processes
Experience with data analysis and reporting tools and techniques and must be able to design
and implement data analysis and reporting processes
Familiar with industry-standard data architecture frameworks, such as TOGAF or Zachman,
and must be able to apply them to the organization's data architecture
Familiar with cloud computing technologies, including public and private clouds, and must be
able to design and implement data architectures that leverage cloud computing
Certificates in Database Management will be preferred
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","NoSQL databases, data lakes, Relational Databases, zachman, ETL processes, data management technologies, cloud computing technologies, data analysis and reporting tools, Data Architecture, Data Modeling, Togaf, data warehouses"
Consultant (Data Architect),Improzo,7-9 Years,,"Pune, India",Login to check your skill match score,"About Improzo
At Improzo (Improve + Zoe; meaning Life in Greek), we believe in improving life by empowering our customers. Founded by seasoned Industry leaders, we are laser focused on delivering quality-led commercial analytical solutions to our clients. Our dedicated team of experts in commercial data, technology, and operations has been evolving and learning together since our inception. Here, you won't find yourself confined to a cubicle; instead, you'll be navigating open waters, collaborating with brilliant minds to shape the future. You will work with leading Life Sciences clients, seasoned leaders and carefully chosen peers like you!
People are at the heart of our success, so we have defined our CARE values framework with a lot of effort, and we use it as our guiding light in everything we do. We CARE!
Customer-Centric: Client success is our success. Prioritize customer needs and outcomes in every action.
Adaptive: Agile and Innovative, with a growth mindset. Pursue bold and disruptive avenues that push the boundaries of possibilities.
Respect: Deep respect for our clients & colleagues. Foster a culture of collaboration and act with honesty, transparency, and ethical responsibility.
Execution: Laser focused on quality-led execution; we deliver! Strive for the highest quality in our services, solutions, and customer experiences.
About The Role
Introduction: We are seeking an experienced and highly skilled Data Architect to lead a strategic project focused on Pharma Commercial Data Management Operations. This role demands a professional with 7-9 years of experience in data architecture, data management, ETL, data transformation, and governance, with an emphasis on providing scalable and secure data solutions for the pharmaceutical sector.
The ideal candidate will bring a deep understanding of data architecture principles, experience with cloud platforms such as Snowflake, and a solid background in driving commercial data management projects. If you're passionate about leading impactful data initiatives, optimizing data workflows, and supporting the pharmaceutical industry's data needs, we invite you to apply.
Responsibilities
Key Responsibilities:
Lead Data Architecture and Strategy:
Design, develop, and implement the overall data architecture for commercial data management operations within the pharmaceutical business.
Lead the design and operations of scalable and secure data systems that meet the specific needs of the pharma commercial team, including marketing, sales, and operations.
Define and implement best practices for data architecture, ensuring alignment with business goals and technical requirements.
Develop a strategic data roadmap for efficient data management and integration across multiple platforms and systems.
Data Integration, ETL & Transformation:
Oversee the ETL (Extract, Transform, Load) processes to ensure seamless integration and transformation of data from multiple sources, including commercial, sales, marketing, and regulatory databases.
Collaborate with data engineers and developers to design efficient and automated data pipelines for processing large volumes of data.
Lead efforts to optimize data workflows and improve data transformation processes to enhance reporting and analytics capabilities.
Data Governance & Quality Assurance:
Implement and enforce data governance standards across the data management ecosystem, ensuring the consistency, accuracy, and integrity of commercial data.
Develop and maintain policies for data stewardship, data security, and compliance with industry regulations, such as HIPAA, GDPR, and other pharma-specific compliance requirements.
Work closely with business stakeholders to ensure the proper definition of master data and reference data standards.
Cloud Platform Expertise (Snowflake (critical to have), AWS, Azure):
Lead the adoption and utilization of cloud-based data platforms, particularly Snowflake, to support data warehousing, analytics, and business intelligence needs.
Collaborate with cloud infrastructure teams to ensure efficient management of data storage, compute resources, and performance optimization within cloud environments.
Stay up-to-date with the latest cloud technologies, such as Snowflake, AWS, Azure, or Google Cloud (optional)), and evaluate opportunities for incorporating them into data architectures.
Collaboration with Cross-functional Teams:
Work closely with business leaders in commercial operations, analytics, and IT teams to understand their data needs and provide strategic data solutions that enhance business operations.
Collaborate with data scientists, analysts, and business intelligence teams to ensure data is available for reporting, analysis, and decision-making.
Facilitate communication between IT, business stakeholders, and external vendors to ensure data architecture solutions align with business requirements.
Continuous Improvement & Innovation:
Drive continuous improvement efforts to optimize data pipelines, data storage, and analytics workflows.
Identify opportunities to improve data quality, streamline processes, and enhance the efficiency of data management operations.
Advocate for the adoption of new data management technologies, tools, and methodologies to improve data processing, security, and integration.
Leadership and Mentorship:
Lead and mentor a team of data engineers, analysts, and other technical resources, fostering a collaborative and innovative work environment.
Provide leadership in setting clear goals, performance metrics, and expectations for the team.
Offer guidance on data architecture best practices, ensuring all team members are aligned with the organization's data strategy.
Required Qualifications
Bachelor's degree in Computer Science, Data Science, Information Systems, or a related field.
7-9 years of experience in data architecture, data management, and data governance, with a proven track record of leading commercial data management operations projects.
Extensive experience in data integration, ETL, and data transformation processes, including familiarity with tools like Informatica, Talend, or Apache NiFi.
Strong expertise with cloud platforms, particularly Snowflake, AWS, Azure, or Google Cloud.
Strong knowledge of data governance frameworks, including data security, privacy regulations, and compliance standards in the pharmaceutical industry (e.g., HIPAA, GDPR).
Hands-on experience in designing scalable and efficient data architecture solutions to support business intelligence, analytics, and reporting needs.
Proficient in SQL and other query languages, with a solid understanding of database management and optimization techniques.
Ability to communicate technical concepts effectively to non-technical stakeholders and align data strategies with business goals.
Preferred Qualifications
Experience in the pharmaceutical or life sciences sector, particularly in commercial data management, sales, marketing, or operations.
Certification or formal training in cloud platforms (e.g., Snowflake, AWS, Azure) or data management frameworks.
Familiarity with data science methodologies, machine learning, and advanced analytics tools.
Knowledge of Agile methodologies for managing data projects.
Key Skills
Data Architecture & Design
Cloud Platforms (Snowflake critical to have)
Data Governance & Quality Assurance
ETL & Data Transformation
Data Integration & Pipelines
Pharmaceutical Data Management (Preferred)
SQL & Database Optimization
Leadership & Mentorship
Business & Technical Collaboration
Benefits
Competitive salary and benefits package.
Opportunity to work on cutting-edge tech projects, transforming the life sciences industry
Collaborative and supportive work environment.
Opportunities for professional development and growth.
Skills: snowflake,database,data governance & quality assurance,data integration & pipelines,etl & data transformation,azure,business & technical collaboration,aws,data management,analytics,sql,sql & database optimization,cloud platforms (snowflake),leadership & mentorship,cloud platforms,data architecture & design,data","Leadership, Business Technical Collaboration, Mentorship, Data Architecture Design, Data Integration Pipelines, snowflake, Database Optimization, Sql, Data Governance, Quality Assurance, Azure, AWS, Data Transformation, Etl"
Senior Analyst - Data Architect,Kraft Heinz,5-7 Years,,"Ahmedabad, India",Login to check your skill match score,"Job Description
General Information
Role Description The data architect is responsible for designing, creating, and managing an organization's data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, structured, accessible, secure, and aligned with business objectives.
Responsibilities
Interact & Influence business stakeholders to secure strong engagement and ensures that the data & analytical product delivery aligns with longer-term strategic roadmaps
Design & contribute towards the structure and layout of lake house architecture optimizing data storage, and establishing data access controls and security measures
Implement the long-term Data & Analytics strategy and deliver functional objectives
Assess requirement feasibility, translates high-level business requirements into data requirements, appropriate metadata, test data, and data quality standards
Explore Data Sources by working with Application owners to confirm datasets to be extracted
Contribute to establishing and implementing database structure, including schema design, table definitions, column specifications, and naming conventions
Design Data models for Source data products, Master data products & Insight data products.
Document Data Architecture artifacts for different Data Products and solutions and perform peer review across various functions.
Support Data Engineering and BI Engineering teams during the build phase
Review Data models development, validate and provide deployment approval
Work closely with data stewards and governance functions to continuously improve data quality and enhance the reliability of data model(s).
Simplify the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company
Leads and participates in the peer review and quality assurance of project architectural artifacts across the Data Architecture group through governance forums
Collaborate and contribute to the development and enhancement of standards, guidelines, and best practices within Data Architecture discipline.
Works with Product owners, Business stewards, Data Scientists and end users to understand data consumers needs and develop data products/ data solutions
Evaluates and recommends emerging technologies for data management, storage, and analytics
Education
A bachelor's degree in computer science, data science, engineering, or related field
Experience
At least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives
Translate business requirements and ability to guide solution design & architecture in developing Data Products
Develop scalable, high-performance, and reusable data models that can be efficiently utilized across different data initiatives and help in generating actionable insights
Work collaboratively with data stewardship and governance functions to continuously improve data quality and enhance the reliability of data models
Ability to navigate and collaborate with cross-functional teams involving data scientists, business analysts, and stakeholders
Strong Business Process and Functional understanding with an Analytical background
CPG experience with knowledge in domain specific concepts is a plus
Knowledge on Agile methodologies with experience working on tools such as Jira & Confluence
Skills
Proficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake)
Experience with database technologies such as SQL, NoSQL, Snowflake, HANA
Understanding of entity-relationship modeling, metadata systems, and data quality tools and techniques
Experience building enterprise data models (Logical, Physical, Conceptual) and data modeling tool experience a plus (ERWIN, ER/STUDIO, etc.)
Strong Business Process and SAP functional understanding with an analytics background (preferred SAP ECC/S4, BW, HANA, BI, ARIBA experience) is a plus
Expert-level SQL skills
Experience with enterprise scale data engineering orchestration frameworks/ELT tools and common data engineering Python libraries (dbt, pandas, great expectations, etc.) is a plus
Experience with business intelligence tools and technologies such as Power BI & Tableau
Strong analytical and problem-solving skills
Understanding of Data Governance principles and practices including Data Quality, Data Security and compliance
Ability to think strategically on the use of data within the Organization that support both current and future needs.
Excellent communication and interpersonal skills for stakeholder management and cross-functional collaboration.
Location(s)
Ahmedabad - Venus Stratum GCC
Kraft Heinz is an Equal Opportunity Employer Underrepresented Ethnic Minority Groups/Women/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity and other protected classes.","S4, data quality tools, snowflake, HANA BI, entity-relationship modeling, dbt, great expectations, ARIBA, Erwin, Kafka, Tableau, Hana, Sap Ecc, Nosql, Bw, Python, AWS, Power Bi, Er Studio, Sql, Pandas, Gcp, DataFlow, Azure"
Lead Data Architect,JPMorganChase,5-7 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Your goal is to become a key player among other imaginative thinkers who share a common commitment to continuous improvement and meaningful impact. Don't miss this chance to collaborate with brilliant minds and deliver premier solutions that set a new standard.
As a Lead Data Architect at JPMorgan Chase within the Risk Technology which is aligned to Enterprise Technology division, you are an integral part of a team that works to develop high-quality data architecture solutions for various software applications on modern cloud-based technologies. As a core technical contributor, you are responsible for carrying out critical data architecture solutions across multiple technical areas within various business functions in support of project goals.
Job Responsibilities
Engages technical teams and business stakeholders to discuss and propose data architecture approaches to meet current and future needs
Defines the data architecture target state of their product and drives achievement of the strategy
Participates in data architecture governance bodies
Evaluates recommendations and provides feedback for new technologies
Executes creative data architecture solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down technical problems
Develops secure, high-quality production code and reviews and debugs code written by others
Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems
Facilitates evaluation sessions with external vendors, startups, and internal teams to drive outcomes through probing of data architectural designs, technical credentials, and applicability for use within existing systems and information architecture
Leads data architecture communities of practice to drive awareness and use of modern data architecture technologies
Adds to team culture of diversity, equity, inclusion, and respect
Required Qualifications, Capabilities, And Skills
Formal training or certification in Data Architecture concepts and 5+ years of applied experience.
Hands-on practical experience delivering system design, application development, testing, and operational stability
Advanced knowledge of architecture and one or more programming languages
Proficiency in automation and continuous delivery methods
Proficiency in all aspects of the Software Development Life Cycle
Demonstrated proficiency in software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)
In-depth knowledge of the financial services industry and their IT systems
Practical cloud native experience
Advanced knowledge of one or more software, application, and architecture disciplines
Ability to evaluate current and emerging technologies to recommend the best data architecture solutions for the future state architecture
Preferred Qualifications, Capabilities, And Skills
Experience in banking / financial domain
About Us
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Operational Stability, Cloud Native Experience, Mobile, Machine Learning, Continuous Delivery, Artificial Intelligence, Automation, Software Development Life Cycle, Cloud, Application Development, Data Architecture, Programming Languages, System Design, Testing"
AWS Data Architect,Knack Consulting Services Pvt Ltd.,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Experience: 10+ years(Candidate must have expiring in doing coding on day to day basis)
UK Shift 2:00PM to 10:30PM
Mandatory skills : Python, SQL, AWS service, lambda glue, data modelling
Key Responsibilities:
Lead a team of data engineers and architects, providing technical guidance and mentorship. Develop and execute a strategic roadmap for data processing, storage, and analytics in alignment with organizational goals.
Design, implement, and maintain robust data pipelines using Python and Airflow, ensuring efficient data flow and transformation for analytical and operational purposes.
Utilize AWS services, including S3 for data storage, Glue and EMR for data processing, and orchestrate data workflows that are scalable, reliable, and secure.
Implement real-time data processing solutions using Kafka, SQS, and Event Bridge, addressing high-volume data ingestion and streaming needs.
Oversee the integration of diverse systems and data sources through AppFlow, APIs, and other integration tools, ensuring seamless data exchange and connectivity.
Lead the development of data warehousing solutions, applying best practices in data modelling to support efficient data storage, retrieval, and analysis.
Continuously monitor, optimize, and troubleshoot data pipelines and infrastructure, ensuring optimal performance and scalability.
Ensure adherence to data governance, privacy, and security policies, implementing measures to protect sensitive data and comply with regulatory requirements.
Qualifications: -
Bachelor's or Master's degree in Computer Science, Engineering, or a related field
8-10 years of experience in data engineering, with at least 3 years in a leadership role.
Proficient in Python programming and experience with Airflow for workflow management.
Strong expertise in AWS cloud services, particularly in data storage, processing, and analytics (S3, Glue, EMR, etc.).
Experience with real-time streaming technologies like Kafka, SQS, and Event Bridge.
Solid understanding of API based integrations and familiarity with integration tools such as AppFlow
Deep knowledge of data warehousing concepts.","Airflow, Event Bridge, AppFlow, Glue, S3, Data Modelling, Kafka, Emr, Sql, Lambda, Sqs, Python, AWS"
Senior Data Architect,Nuivio Ventures Inc.,Fresher,,"Chennai, India",Login to check your skill match score,"Job Summary:
The Senior Data Architect will lead the design and implementation of scalable, high-performance data architectures across both on-premise and cloud environments. This role involves advanced data modeling, SQL development, and ETL design, with a strong focus on data quality, integrity, and strategy. The architect will collaborate closely with cross-functional teams to align technical solutions with business needs, perform in-depth data analysis, and drive the development of robust data models that support long-term organizational goals.
Core Competencies:
Data Architecture & Strategy
Advanced Data Modeling (Star, Snowflake)
SQL Development (Partitioning, Stored Procedures, Recursive Queries)
Data Profiling & Root Cause Analysis
ETL Design & Implementation
Cloud & On-Premise Data Platforms (Oracle, Databricks)
Data Quality & Governance
Stakeholder Engagement
Change Data Capture & Audit Strategy
Cross-Functional Collaboration
Roles and Responsibilities:
Analyze existing data sources to understand data flow, relationships, and usage.
Design and implement scalable data models following best practices in normalization, denormalization, and dimensional modeling.
Develop and optimize complex SQL queries, including recursive SQLs and macros, to extract, transform, and analyze data.
Architect end-to-end data strategies aligned with future-state objectives, focusing on performance, scalability, and flexibility.
Reverse engineer data models through data profiling, identifying key attributes and relationships.
Collaborate with business and technical stakeholders to map data patterns to underlying processes and use cases.
Conduct root cause analysis (RCA) to troubleshoot data inconsistencies and propose effective resolutions.
Perform frequency distribution and statistical analyses to detect data trends and quality issues.
Design and implement change data capture (CDC), audit logging, and reference data strategies.
Define and create mapping tables and helper tables to support flexible and configurable ETL processes.
Own and ensure data integrity, accuracy, and completeness across platforms.
General Attributes:
Demonstrates intellectual curiosity and passion for uncovering insights in complex datasets.
Skilled communicator capable of translating technical concepts into actionable insights for diverse stakeholders.
Engages effectively with clinicians, researchers, data scientists, and IT teams to gather requirements and align objectives.
Leads deep-dive data analysis initiatives to support data-driven decision-making.
Collaborates across departments to deliver robust, business-aligned data models and solutions.
Preferred Skills
Familiarity with data visualization tools (e.g., Tableau, Power BI) is an advantage.
Experience in the Life Sciences or Pharmaceutical industry is highly desirable.","ETL Design Implementation, Data Quality Governance, Root Cause Analysis, change data capture, Data Architecture Strategy, Advanced Data Modeling, Cloud On-Premise Data Platforms, Sql Development, Data Profiling"
Lead Data Architect,JPMorganChase,5-7 Years,,"Delhi, India",Login to check your skill match score,"Job Description
We know that people want great value combined with an excellent experience from a bank they can trust, so we launched our digital bank, Chase UK, to revolutionize mobile banking with seamless journeys that our customers love. We're already trusted by millions in the US and we're quickly catching up in the UK but how we do things here is a little different. We're building the bank of the future from scratch, channeling our start-up mentality every step of the way meaning you'll have the opportunity to make a real impact.
As a Lead Data Architect - Data Scientist at JPMorgan Chase within the International Consumer Bank, you will be a part of a flat-structure organization. Your responsibilities are to deliver end-to-end cutting-edge solutions in the form of cloud-native microservices architecture applications leveraging the latest technologies and the best industry practices. You are expected to be involved in the design and architecture of the solutions while also focusing on the entire SDLC lifecycle stages.
Our Business Analytics team is at the heart of this venture, focused on getting smart ideas into the hands of our customers. We're looking for people who have a curious mindset, thrive in collaborative squads, and are passionate about new technology. By their nature, our people are also solution-oriented, commercially savvy and have a head for fintech. We work in tribes and squads that focus on specific products and projects and depending on your strengths and interests, you'll have the opportunity to move between them.
Job Responsibilities
Collaborating with business partners, research teams and domain experts to understand business problems.
Providing stakeholders with timely and accurate reporting.
Performing ad hoc analysis based on diverse data sources to give decision-makers actionable insights about the performance of the products, customer behavior and market trends.
Presenting your findings in a clear, logical, and persuasive manner, illustrating them with effective visualizations.
Collaborating with data engineers, machine learning engineers and dashboard developers to automate and optimize business processes.
Identifying unexplored opportunities to change how we do business using data.
Required Qualifications, Capabilities And Skills
Formal training or certification in Data Analysis using Python and 5+ years applied experience
Advanced SQL querying skills.
Experience in taking open ended business questions, then use big data and statistics to create analysis that can provide an answer to the questions at hand.
Experience with customer analytics such as user behavioral analysis, campaign analysis, etc.
Demonstrated ability to think beyond raw data and to understand the underlying business context and sense business opportunities hidden in data.
Ability to work in a dynamic, agile environment within a geographically distributed team.
Excellent written and verbal communication skills in English.
Preferred Qualifications, Capabilities And Skills
Distinctive problem-solving skills and impeccable business judgment.
Familiarity with machine learning.
ABOUT US
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Advanced SQL querying, customer analytics, Data Analysis using Python, Big data and statistics, Machine Learning"
Senior Data Architect,ChaiBu Group,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect
Experience Required: 510 years
Location: Bangalore
About the Role:
We're looking for a Senior Data Architect to lead the design and development of our data systems. This includes how we collect, store, move, and use data to help the business make better decisions. You'll work mainly with Azure but may also use other platforms. You'll help build strong data foundations for analytics, AI, and app integration while keeping costs and security in check.
What You'll Do:
Create and lead the overall data architecture strategy.
Guide and mentor junior data team members.
Build scalable and high-performing data solutions that fit business needs.
Design data models that are secure, reusable, and easy to access.
Create and manage data flow diagrams and data dictionaries.
Work closely with business teams to understand their data requirements and turn them into technical solutions.
Make sure data is easily accessible for data analysts, data scientists, and developers.
Maintain high standards of data quality, accuracy, and security.
Lead the design of data warehouses, data lakes, and other storage solutions.
Set and follow best practices for data governance and compliance.
Keep up with industry changes and help improve internal standards and practices.
What We're Looking For:
Bachelor's degree in Computer Science, IT, or equivalent experience.
5+ years of proven experience as a Data Architect or in a similar role.
Strong knowledge of data modeling, warehousing, and integration.
Hands-on experience with databases like SQL Server, Oracle, PostgreSQL.
Familiarity with big data tools like Hadoop, Spark, and data lakes such as Azure Data Lake.
Experience in creating ER diagrams, designing star/snowflake schemas, and building cost-effective data pipelines.
Ability to turn business goals into technical solutions.
Comfortable working with cloud platforms like Azure, AWS, or Google Cloud.
Strong understanding of data security and governance.
Excellent communication and teamwork skills.
Bonus Skills (Preferred, not mandatory):
Tools: Erwin, Azure Synapse, Azure Databricks, Azure DevOps, Power BI, Spark, Python, R.
Experience with Azure AI/ML Services, Event Hub, Stream Analytics, and scripting tools like Ansible.
Understanding of machine learning, CI/CD, container tools like Docker/Kubernetes.
Relevant certifications such as AWS Certified Solutions Architect or IBM Certified Data Architect.","snowflake schema, data pipelines, Event Hub, Azure AI ML Services, R, Stream Analytics, Hadoop, Power Bi, PostgreSQL, SQL Server, Azure Databricks, Data Modeling, Azure Synapse, Docker, Ansible, Er Diagrams, Azure Data Lake, Spark, Oracle, Azure, Python, Kubernetes, Azure DevOps"
Data Architect (Azure & Snowflake),CES,3-5 Years,,"Chennai, India",Login to check your skill match score,"CES has 26+ years of experience in delivering Software Product Development, Quality Engineering, and Digital Transformation Consulting Services to Global SMEs & Large Enterprises. CES has been delivering services to some of the leading Fortune 500 Companies including Automotive, AgTech, Bio Science, EdTech, FinTech, Manufacturing, Online Retailers, and Investment Banks. These are long-term relationships of more than 10 years and are nurtured by not only our commitment to timely delivery of quality services but also due to our investments and innovations in their technology roadmap. As an organization, we are in an exponential growth phase with a consistent focus on continuous improvement, process-oriented culture, and a true partnership mindset with our customers. We are looking for the right qualified and committed individuals to play an exceptional role as well as to support our accelerated growth.
You can learn more about us at: http://www.cesltd.com/
Job Description
Experience with Azure Synapse Analytics: Hands-on experience in designing, developing, and deploying solutions using Azure Synapse Analytics, including familiarity with its various components such as SQL pools, Spark pools, and Integration Runtimes.
Expertise in Azure Data Lake Storage: In-depth understanding of Azure Data Lake Storage, including its architecture, features, and best practices for managing a large-scale Data Lake or Lakehouse in an Azure environment.
Experience with AI Tools: Experience with AI Tools and LLMs (e.g. GitHub Copilot, Copilot, ChatGPT) in automating many of the responsibilities outlined for this role.
Knowledge of Avro and Parquet: Experience working with Avro and Parquet file formats, including data serialization, compression techniques, and schema evolution. Understanding of their advantages and use cases in a big data environment.
Healthcare: Prior experience working with data in a healthcare or clinical laboratory environment and a strong understanding of PHI, GDPR & HIPPA/HITRUST is highly desirable.
Certifications: Relevant certifications such as Azure Data Engineer Associate or Azure Synapse Analytics Developer Associate are highly desirable.
Essential Functions
Data Integration and ELT Development: Design, develop, and maintain data pipelines for ingestion, transformation, and loading of data into Azure Synapse Analytics. This includes understanding functional and non-functional requirements, performing source data analysis, data profiling, and implementing efficient ELT processes.
Azure Synapse Development: Work with Azure Synapse Analytics to build and optimize data models, SQL queries, stored procedures, and other artifacts necessary for data processing and analysis.
Data Lake File Handling: Understand the characteristics of various file formats, optimizing data storage, and implementing efficient data reading and writing mechanisms for incremental updates within Azure Synapse Analytics.
Data Governance and Security: Ensure compliance with data governance policies and implement security measures to protect sensitive data stored in Azure. This involves encryption, masking, and access control mechanisms.
Performance Optimization: Continuously optimize data pipelines and storage configurations to improve performance, scalability, and reliability. This includes identifying bottlenecks, query tuning, and leveraging Azure Synapse Analytics features for parallel processing.
Monitoring and Troubleshooting: Implement monitoring solutions to track data pipeline performance, data quality, and system health. Troubleshoot issues related to data ingestion, transformation, or storage, and provide timely resolutions.
Skills Needed to be Successful
Relational Database Experience: Proficiency with one or more of the following database platforms; e.g. Oracle, Microsoft SQL Server, PostgreSQL, MySQL/MariaDB
Proficiency in SQL: Strong SQL skills, including experience with complex SQL queries, stored procedures, and performance optimization techniques. Familiarity with T-SQL for Azure Synapse Analytics is a plus.
ELT and Data Integration Skills: Proven experience in building ELT pipelines and data integration solutions using tools like Azure Data Factory, Oracle Golden Gate, or similar platforms.
Ability to handle a variety of legacy data sources and file formats efficiently.
Data Modeling and Warehousing Concepts: Familiarity with dimensional modeling, star schemas, and data warehousing principles. Experience in designing and implementing data models for analytical workloads.
Analytical and Problem-Solving Abilities: Strong analytical skills with the ability to understand complex data requirements, troubleshoot technical issues, and propose effective solutions to meet business needs.
Communication and Collaboration: Excellent communication skills with the ability to collaborate effectively with cross-functional teams, including Data Scientists, Reporting Analysts, and DevOps professionals.","Azure Data Lake Storage, Relational Database Experience, Data Modeling and Warehousing Concepts, AI Tools, ELT and Data Integration, Parquet, Sql, Avro, Azure Synapse Analytics"
Data Architect (Databricks or Snowflake Certified ),BDO in India,11-15 Years,,"Chennai, India",Login to check your skill match score,"Job Role - Data Architect
Must : We need Databricks or Snowflake Certified
Location: Chennai (Look only for Chennai Candidates)
Work Mode: Work From Office
Yrs Of Exp: 11-15 Yrs
Notice Period: Immediate (Max 20days)
Job Description:
Mid-level to senior data architects frequently have 3+ years or more of experience.
Researching data acquisition opportunities.
Engaging with clients through presentations and demonstrations to showcase data solutions
Excellent communication skills are required to work with cross-functional teams and convert business objectives into technical solutions.
Translating business requirements into databases, data warehouses, and data streams.
Design and build scalable & metadata-driven data ingestion framework (For Batch and Streaming Datasets)
Analyzing, planning, and defining data architecture framework, including security, reference data, metadata, and master data.
Creating and implementing data management processes and proceduresto ensure data accuracy and accessibility.
Experience in building data solution in any cloud platform (Azure/AWS).
Collaborating with other teams within the organisation to devise and implement data strategies, build models, and assess shareholder needs and goals.
Prior experience in data modelling, database design, and data administration is required.
Database Expertise: Knowledge of data warehousing ideas and proficiency in various database systems (e.g., SQL, NoSQL).
Understanding on different file formats like Delta Lake, Avro, Parquet, JSON, and CSV.
Understanding on data migration projects and implementation strategies.
Knowledge of Data Governance: Understanding data governance principles, data security, and regulatory compliance.
Knowledge of programming languages such as Python, SQL and PySpark.
Experience in supporting BI and Data Science teams in consuming the data in a secure and governed manner.
Mentoring team members to improve their proficiency/efficiency in data architecture.","Parquet, data administration, snowflake, Delta Lake, Data Modelling, Csv, Pyspark, Json, Avro, Sql, Nosql, Database Design, Databricks, Azure, Python, AWS"
Senior Data Architect,Forbes Advisor,15-17 Years,,"Mumbai, India",Login to check your skill match score,"Job Description
Key Responsibilities
Strategic Data Architecture & Pipeline Leadership
Vision & Strategy:
Define and execute the long-term strategy for our data warehousing
platform using medallion architecture (Bronze, Silver, Gold layers) and
modern cloud-based solutions.
End-to-End Pipeline Oversight:
Oversee data ingestion (via Google Ads, Bing Ads, Facebook Ads, GA,
APIs, SFTP, etc.), transformation (leveraging DBT, and SQL [via
BigQuery]), and reporting, ensuring that our pipelines are robust and
scalable.
Data Modeling Best Practices:
Champion best practices in data modeling, including the effective use
of DBT packages to streamline complex transformations.
Data Quality, Governance & Attribution
Quality & Validation:
Establish and enforce rigorous data quality standards, governance
policies, and automated validation frameworks across all data streams.
Standardization & Visibility:
Collaborate with the Data Engineering, Insights and BIOps team to
standardize data definitions (including engagement metrics and
revenue attribution) and ensure consistency across all reports.
Attribution Focus:
Develop frameworks to reconcile revenue discrepancies and unify
validation across Finance, SEM, and Analytics teams.
Ensure accurate attribution of revenue and paid marketing channel
performance, working closely with SEM and Digital Experiences teams.
Monitoring & Alerting:
Implement robust monitoring and alerting systems (e.g., Slack and
email notifications) to quickly identify, diagnose, and resolve data
pipeline issues.
Team Leadership & Cross-Functional Collaboration
People & Process:
Lead, mentor, and grow a high-performing team of data warehousing
specialists, fostering a culture of accountability, innovation, and
continuous improvement.
Stakeholder Engagement:
Partner with RevOps, Analytics, SEM, Finance, and Product teams to align
the data infrastructure with business objectives.
Serve as the primary data warehouse expert in discussions around
revenue attribution and paid marketing channel performance, ensuring
that business requirements drive technical solutions.
Communication:
Translate complex technical concepts into clear business insights for
both technical and non-technical stakeholders.
Operational Excellence & Process Improvement
Deployment & QA:
Oversee deployment processes, including staging, QA, and rollback
strategies, to ensure minimal disruption during updates.
Continuous Optimization:
Regularly assess and optimize data pipelines for performance,
scalability, and reliability while reducing operational overhead.
Legacy to Cloud Transition:
Lead initiatives to transition from legacy on-premise systems to
modern cloud-based architectures for improved agility and cost
efficiency.
Innovation & Thought Leadership
Emerging Trends:
Stay abreast of emerging trends and technologies in data warehousing,
analytics, and cloud solutions.
Pilot Projects:
Propose and lead innovative projects to enhance our data capabilities,
with a particular focus on predictive and prescriptive analytics.
Executive Representation:
Represent the data warehousing function in senior leadership
discussions and strategic planning sessions
Qualifications
Education & Experience
Bachelor's or Master's degree in Computer Science, Data Science, Information
Systems, or a related field.
15+ years of experience in data engineering, warehousing, or analytics roles,
with at least 5+ years in a leadership capacity.
Proven track record in designing and implementing scalable data
warehousing solutions in cloud environments.
Technical Expertise
Deep experience with medallion architecture and modern data pipeline tools,
including DBT (and DBT packages), Databricks, SQL, and cloud-based data
platforms.
Strong understanding of ETL/ELT best practices, data modeling (logical and
physical), and large-scale data processing.
Hands-on experience with BI tools (e.g., Tableau, Looker) and familiarity with
Google Analytics, and other tracking systems.
Solid understanding of attribution models (first-touch, last-touch, multi-
touch) and experience working with paid marketing channels.
Leadership & Communication
Excellent leadership and team management skills with the ability to mentor
and inspire cross-functional teams.
Outstanding communication skills, capable of distilling complex technical
information into clear business insights.
Demonstrated ability to lead strategic initiatives, manage competing
priorities, and deliver results in a fast-paced environment.
Perks & Benefits
Flexible/Remote Working: Enjoy flexible work arrangements in a collaborative,
distributed team culture.
Competitive Compensation: Attractive salary, performance-based bonuses,
and comprehensive benefits.
Time Off: Generous paid time off, parental leave policies, and a dedicated day
off on the 3rd Friday of each month.
If you are a visionary leader with a passion for building resilient data infrastructures,
a deep understanding of revenue attribution and paid marketing channels, and a
proven ability to drive strategic business outcomes through data, we invite you to
join our Data & Analytics team and shape the future of our data warehousing
function.","dbt, medallion architecture, Looker, BigQuery, Tableau, Data Modeling, Google Analytics, Sql, Etl, ELT"
Data Architect,OneMagnify,7-9 Years,,"Chennai, India",Login to check your skill match score,"The Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.
Team: Data Platform, Application & Data
Reports to: Engineering Manager, Data Platform, Application & Data
Minimum Education: Bachelor's Degree or Equivalent Experience
Recommended Tenure: 7+ years in data platform engineering or architecture roles, including at least 3 years in a hands-on architecture role
Role Summary:
The Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.
Architecture & Design:
Translate business and technical requirements into data models and architecture specifications
Design and document data architecture artifacts, including logical/physical data models, data flow diagrams, and integration patterns
Align data models with application architecture and system interaction patterns in partnership with Solution Architects
Establish and maintain design patterns for relational, NoSQL, and streaming-based data solutions
Solution Delivery & Support:
Serve as a hands-on architecture lead during project discovery, planning, and delivery phases
Support data engineers in implementing data architecture that aligns with platform and project requirements
Validate implementation through design reviews and provide guidance throughout the development lifecycle
Contribute to platform evolution by defining and enforcing scalable, reusable architecture practices
Data Governance & Quality:
Define and uphold best practices for data modeling, data security, lineage tracking, and performance tuning
Promote consistency in metadata, naming conventions, and data access standards across environments
Support data privacy, classification, and auditability across integrated systems
Cross-Functional Collaboration:
Work closely with product managers, engineering leads, DevOps, and analytics teams to deliver scalable and future-proof data solutions
Collaborate with Solution Architects to ensure integrated delivery across application and data domains
Act as a subject matter expert on data structure, semantics, and lifecycle across key business domains
Key Competencies:
7+ years of experience in data engineering or data architecture roles, including 3+ years in a dedicated architecture capacity
Proven experience in cloud platformspreferably GCP and/or Azurewith strong familiarity with native data services
Deep understanding of data storage paradigms including relational, NoSQL, and object storage
Hands-on experience with databases such as Oracle and Postgres; Python proficiency preferred
Familiarity with modern DevOps practices including infrastructure-as-code and CI/CD for data pipelines
Strong communication skills with the ability to lead through influence across technical and non-technical audiences
Self-starter with excellent organization and prioritization skills across multiple initiatives","Data Services, Postgres, Oracle, Python"
Senior Data Architect,Axtria - Ingenious Insights,Fresher,,"Bengaluru, India",Login to check your skill match score,"POSITION: Data Architect (Individual contributor)
Data Architect + Gen AI
LOCATION: Noida, Gurugram, Pune, Bangalore
60% through out in academics
JOB OBJECTIVE: To leverage expertise in data architecture and management to design, implement, and
optimize a robust data warehousing platform for the pharmaceutical industry. The goal is to ensure
seamless integration of diverse data sources, maintain high standards of data quality and governance,
and enable advanced analytics through the definition and management of semantic and common data
layers. Utilizing Axtria's product and generative AI technologies, the aim is to accelerate business
insights and support regulatory compliance, ultimately enhancing decision-making and operational
efficiency.
Key Responsibilities:
Strong AIML, Gen AI exp
Data Modeling: Design logical and physical data models to ensure efficient data storage and
retrieval.
ETL Processes: Develop and optimize ETL processes to accurately and efficiently move data
from various sources into the data warehouse.
Infrastructure Design: Plan and implement the technical infrastructure, including hardware,
software, and network components.
Data Governance: Ensure compliance with regulatory standards and implement data
governance policies to maintain data quality and security.
Performance Optimization: Continuously monitor and improve the performance of the data
warehouse to handle large volumes of data and complex queries.
Semantic Layer Definition: Define and manage the semantic layer architecture and technology
stack to manage the lifecycle of semantic constructs including consumption into downstream
systems.
Common Data Layer Management: Integrate data from multiple sources into a centralized
repository, ensuring consistency and accessibility.
Deep expertise in architecting enterprise grade software systems that are performant,
scalable, resilient and manageable. Architecting GenAI based systems is an added plus.
Advanced Analytics: Enable advanced analytics and machine learning to identify patterns in
genomic data, optimize clinical trials, and personalize medication.
Generative AI: Should have worked with production ready usecase for GenAI based data and
Stakeholder Engagement: Work closely with business stakeholders to understand their data
needs and translate them into technical solutions.
Cross-Functional Collaboration: Collaborate with IT, data scientists, and business analysts to
ensure the data warehouse supports various analytical and operational needs.
Qualifications:
Proven experience in data architecture and data warehousing, preferably in the
pharmaceutical industry.
Strong knowledge of data modeling, ETL processes, and infrastructure design.
Experience with data governance and regulatory compliance in the life sciences sector.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
Preferred Skills:
Familiarity with advanced analytics and machine learning techniques.
Experience in managing semantic and common data layers.
Knowledge of FDA guidelines, HIPAA regulations, and other relevant regulatory standards.
Experience with generative AI technologies and their application in data warehousing
ABOUT AXTRIA
Axtria (www.axtria.com) is high growth advanced analytics and business information Management
Company based out of New Jersey with locations in AZ, GA and VA in USA and Gurgaon in India. We
have been named as one of the fastest growing companies in the US by Inc. 5000 in 2014.
Our broad portfolio of services and solutions help our clients improve their sales, marketing, supply
chain and distribution planning and operations in various industries such as Pharma, Retail, Banking and
Technology. We blend analytics, technology and consulting to help customers gain deep insights from
their customer data, create strategic advantage and drive profitable growth.
The leadership team at Axtria brings deep industry experience, expertise in sales, marketing and risk
management as well as a passion for building cutting-edge analytics and technology solutions.
Our global team is committed to delivering high quality, high-impact solutions for our clients and to
building a world-class firm with enduring value. Our unique team combines real-world business
knowledge, a depth of analytical skill and experience with the latest technologies. Those who excel at
Axtria share a number of common qualities. They are smart, humble and have the analytical toolkit to
put their intelligence to work. They are passionate about data and analytics & seek to constantly learn.","Performance Optimization, Generative AI, ETL Processes, Common Data Layer Management, Infrastructure Design, Machine Learning, Data Modeling, Advanced Analytics, Data Architecture, Data Warehousing, Data Governance"
Data Architect,MathCo,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Architect, you will design and implement scalable, cloud-native data solutions that handle petabyte-scale datasets. You will lead architecture discussions, build robust data pipelines, and work closely with cross-functional teams to deliver enterprise-grade data platforms. Your work will directly support analytics, AI/ML, and real-time data processing needs across global clients.
Key Responsibilities:
Translate complex data and analytics requirements into scalable technical architectures.
Design and implement cloud-native architectures for real-time and batch data processing.
Build and maintain large-scale data pipelines and frameworks using modern orchestration tools (e.g., Airflow, Oozie).
Define strategies for data modeling, integration, metadata management, and governance.
Optimize data systems for cost-efficiency, performance, and scalability.
Leverage cloud services (AWS, Azure, GCP) including Azure Synapse, AWS Redshift, BigQuery, etc.
Implement data governance frameworks covering quality, lineage, cataloging, and access control.
Work with modern big data technologies (e.g., Spark, Kafka, Databricks, Snowflake, Hadoop).
Collaborate with data engineers, analysts, DevOps, and business stakeholders.
Evaluate and adopt emerging technologies to improve data architecture.
Provide architectural guidance in cloud migration and modernization projects.
Lead and mentor engineering teams and provide technical thought leadership.
Required Skills and Experience:
Bachelor's or Master's in Computer Science, Engineering, or related field.
10+ years of experience in data architecture, engineering, or platform roles.
5+ years of experience with cloud data platforms (Azure, AWS, or GCP).
Proven experience building scalable enterprise data platforms (data lakes/warehouses).
Strong expertise in distributed computing, data modeling, and pipeline optimization.
Proficiency in SQL and NoSQL databases (e.g., Snowflake, SQL Server, Cosmos DB, DynamoDB).
Experience with data integration tools like Azure Data Factory, Talend, or Informatica.
Hands-on experience with real-time streaming technologies (Kafka, Kinesis, Event Hub).
Expertise in scripting/programming languages such as Python, Spark, Java, or Scala.
Deep understanding of data governance, security, and regulatory compliance (GDPR, HIPAA, CCPA).
Strong communication, presentation, and stakeholder management skills.
Ability to lead multiple projects simultaneously in an agile environment.","Airflow, Event Hub, Snowflake SQL Server, CCPA, Cloud-native architectures, snowflake, Data pipelines, Gdpr, Aws Redshift, Kafka, Hipaa, Data Governance, Informatica, Nosql, Azure Synapse, Kinesis, Oozie, Cosmos DB, Talend, Python, AWS, Java, BigQuery, Hadoop, Scala, Dynamodb, Sql, Azure Data Factory, Gcp, Spark, Databricks, Azure"
Data Architect - India,Zywave,5-7 Years,,"Itanagar, India",Login to check your skill match score,"Brief Description
As a Data Architect at Zywave, you will play a critical role in designing and implementing data solutions that support our financial and business intelligence initiative. The ideal candidate will have a strong background in data architecture, data modeling and data reporting.
Essential Functions:
Design and develop scalable and efficient data architectures to support financial and BI reporting.
Collaborate with cross-functional teams to understand data requirements and translate them into technical solutions.
Create and maintain data models, schemas, and databases to ensure data integrity and consistency.
Develop and implement ETL processes to extract, transform, and load data from various sources.
Optimize data storage and retrieval processes to enhance performance and scalability.
Ensure data security and compliance with relevant regulations and standards.
Provide technical guidance and support to data analysts and BI developers.
Stay up to date with industry trends and best practices in data architecture and management.
Factors for Success:
Bachelor's degree in Computer Science, Information Systems, or related field
5+ years experience as a Data Architect or similar role
Strong knowledge of data modeling, database design, and data management principles
Familiarity with BI tools such as Snowflake, Tableau, PowerBI, etc.
Excellent problem-solving and analytical skills
Ability to communicate complex technical concepts to non-technical stakeholders
Experience with SaaS products and understanding of SaaS business models is a plus
Strong attention to detail and ability to work independently or collaboratively
Why pick Zywave
Zywave empowers insurers and brokers to drive profitable growth and thrive in today's escalating risk landscape. Only Zywave delivers a powerful Performance Multiplier, bringing together transformative, ecosystem-wide capabilities to amplify impact across data, processes, people, and customer experiences. More than 15,000 insurers, MGAs, agencies, and brokerages trust Zywave to sharpen risk assessment, strengthen client relationships, and enhance operations. Additional information can be found at www.zywave.com.","data reporting, data integrity, snowflake, ETL processes, Data Security, Powerbi, Bi Tools, Database Design, Data Architecture, Tableau, Data Modeling"
Principal Digital Architect-Data Architect,Caterpillar Inc.,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Career Area
Technology, Digital and Data
Job Description
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Job Purpose
A Principal Digital Architect drives solutions with the core digital platform that underpins many applications that supports Cat Digital's key business domains of eCommerce, advanced condition monitoring, lead generation, service support and equipment management. The solutions will solve big data problems with over 1.4 million connected assets worldwide, advanced analytics, AI capabilities, global marketing and much more. This role is a key leader and accountable to work across the organization for both business and technical alignment to drive the required business outcomes.
Job Duties
Key areas of accountability include leading the development of Architecture Solutions for strategic Cat Digital projects and programs; developing and maintaining global technology roadmaps and application evolution plans; leading in the evaluation of new technology, information or integration standards; and developing digital strategy for a specific technical or business domain.
Responsibilities Include One Or More Of The Following
Provide oversight for architecture assessment and design for infrastructure, information or integration domains that provide core capabilities for the enterprise.
Lead Architecture design of end to end enterprise integrated systems that serves multiple business functions.
Lead the design and implementation of enterprise data model and metadata structures for complex projects.
Initiate and deliver technology evaluation and recommendations.
Develop and maintain current, planned and future state architecture blueprints.
Lead in the identification and analysis of enterprise business drivers and requirements that drive the future state architecture.
Basic Qualifications
Position requires a four-year degree from an accredited college or university in computer science, information technology, or related field or equivalent work experience.
15 or more years of a progressive career in distributed system software engineering and architecture
Strong demonstratable experience delivering product and/or enterprise architecture for enterprise scale solutions in public cloud or hybrid eco-systems
At least 5 years working experience in Cloud Technologies such as AWS & Microsoft Azure
Must have excellent communication skills and be able to deal with sensitive issues, mentor and coach and/or persuade others on new technologies, new applications, or potential solutions.
Top Candidates Will Have
Understand the data platform and build new data solutions on the existing data platform. Impact analysis needs to be performed so as not to have unknown impact in other data solutions build on the platform.
Understand the current data landscape and build new solutions on top of existing solution.
Trouble shoots and finds solutions for technical and functional issues identified in the program/project.
Evaluate, analyse, document and communicate business requirements to stakeholders.
Run Architecture meetings/discussions and document the solutions on confluence. Complete the solutions and have engineering handover.
Support the engineering team during the entire cycle of the build and deploy phases.
Report on common sources of technical, functional issues and/or questions and make recommendations to architecture team for long term solution.
Own and develop relationship with partners (customers, dealers, Technical Product Management, Architect teams), working with them to optimize and enhance the data products.
Provide guidance on the technical solutions and guidance on new product like optimal database recommendations like dynamo vs Postgre, AWS options like Kinesis and Event bridge.
Own the solutions on the Data lake (Snowflake), The solutions should be performant, secure and Cost Optimal.
Own some of the data domains in the Data Platform i.e. Any solution on the data domain should be either worked upon or reviewed by the architect.
Provide ROM (rough order of magnitude) for the solutions and data products.
Based on understanding of data domains and Business requirements create reusable data products which can be used across applications and teams.
Create/Review HLA and TA documentation with reference to a business requirement.
Improve architecture by tracking emerging technologies and evaluating their applicability to business goals and operational requirements.
Identify and solution for business critical business rules for improving the data quality in the platform.
Skills
Must Have:
AWS (EMR, Glue, S3, Fargate, SNS, SQS, Kinesis, AWS EventBridge, RDS, DynamoDB) , Snowflake, SQL, Python, ER Modelling.
Good To Have
Microservices and API knowledge
Posting Dates
May 19, 2025 - May 25, 2025
Caterpillar is an Equal Opportunity Employer.
Not ready to apply Join our Talent Community.","AWS EventBridge, Fargate, snowflake, Glue, API knowledge, ER Modelling, S3, RDS, Dynamodb, Emr, Sql, Microservices, Kinesis, Sqs, Sns, Python, AWS"
Sr. Data Architect,WTW,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Description
The Role:
Partner with other architecture resources to lead the end-to-end architecture of the health and benefits data platform using Azure services, ensuring scalability, flexibility, and reliability.
Develop broad understanding of the data lake architecture, including the impact of changes on a whole system, the onboarding of clients and the security implications of the solution.
Design a new or improve upon existing architecture including data ingestion, storage, transformation and consumption layers.
Define data models, schemas, and database structures optimized for H&B use cases including claims, census, placement, broking and finance sources.
Designing solutions for seamless integration of diverse health and benefits data sources.
Implement data governance and security best practices in compliance with industry standards and regulations using Microsoft Purview.
Evaluate data lake architecture to understand how technical decisions may impact business outcomes and suggest new solutions/technologies that better align to the Health and Benefits Data strategy.
Draw on internal and external practices to establish data lake architecture best practices and standards within the team and ensure that they are shared and understood.
Continuously develop technical knowledge and be recognised as a key resource across the global team.
Collaborate with other specialists and/or technical experts to ensure H&B Data Platform is delivering to the highest possible standards and that solutions support stakeholder needs and business requirements.
Initiate practices that will increase code quality, performance and security.
Develop recommendations for continuous improvements initiatives, applying deep subject matter knowledge to provide guidance at all levels on the potential implications of changes.
Build the team's technical expertise/capabilities/skills through the delivery of regular feedback, knowledge sharing, and coaching.
High learning adaptability, demonstrating understanding of the implications of technical issues on business requirements and / or operations.
Analyze existing data design and suggest improvements that promote performance, stability and interoperability.
Work with product management and business subject matter experts to translate business requirements into good data lake design.
Maintain the governance model on the data lake architecture through training, design reviews, code reviews, and progress reviews.
Participate in the development of Data lake Architecture and Roadmaps in support of business strategies
Communication with key stakeholders and development teams on technical solutions. Convince and present proposals by way of high-level solutions to end users and/or stakeholders.
The Requirement
Candidate must have significant experience in a technology related discipline, such as IT or Engineering with a Bachelor's/College Degree in these areas being beneficial.
Strong experience in databases, tools and methodologies
Strong skills across a broad range of database technologies including Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, Azure Data Lake Storage, and other Azure Services.
Working knowledge of Microsoft Fabric is preferred.
Data Analysis, Data Modeling, Data Integration, Data Warehousing, Database Design
Experience with database performance evaluation and remediation
Develop strategies for data acquisitions, archive recovery and implementation
Be able to design and develop Databases, Data Warehouses and Multidimensional Databases
Experience in Data Governance including Microsoft Purview, Azure Data Catalogue, Azure Data Share, and other Azure tools.
Familiarity with legal risks related to data usage and rights.
Experience in data security, including Azure Key Vault, Azure Data Encryption, Azure Data Masking, Azure Data Anonymization, and Azure Active Directory.
Ability to develop database strategies for flexible high-performance reporting and business intelligence
Experience using data modeling tools & methodology
Experience working within an Agile Scrum Development Life Cycle, across varying levels of Agile maturity
Experience working with geographically distributed scrum teams
Excellent verbal and writing skills, including the ability to research, design, and write new documentation, as well as to maintain and improve existing material
Technical Competencies
Subject Matter Expertise
Developing expertise
You strengthen your depth and/or breadth of subject matter knowledge and skills across multiple areas.
You define the expertise required in your area based on emerging technologies, industry practices. You build the team's capability accordingly.
Applying expertise
You apply subject matter knowledge and skills across multiple areas to assess the impact of complex issues and implement long-term solutions. You foster innovation using subject matter knowledge to enhance tools, practices, and processes for the team.
Solution Development
Systems thinking
You lead and foster collaboration across H&B Data Platform Technology to develop solutions to complex issues.
You apply a whole systems approach to evaluating impact, and take ownership for ensuring links between structure, people and processes are made.
Focusing on quality
You instill a quality mindset to the team and ensure the appropriate methods, processes and standards are in place for teams to deliver quality solutions. You create and deliver improvement initiatives.
Technical Communication
Simplifying complexity
You develop tools, aids and/or original content to support the delivery and/or understanding of complex information. You guide others on best practice.
Qualifications
Candidate must have significant experience in a technology related discipline, such as IT or Engineering with a Bachelor's/College Degree in these areas being beneficial.","Azure Data Encryption, Azure Key Vault, Azure Data Masking, Azure SQL Database, Azure Data Lake Storage, Azure Data Anonymization, Microsoft Purview, Azure Data Share, Azure Data Catalogue, Data Analysis, Data Modeling, Azure Databricks, Data Integration, Database Design, Azure Active Directory, Azure Data Factory, Azure Synapse Analytics, Data Warehousing"
Data Architect,Gramener,Fresher,,"Chennai, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore/Chennai/Noida/Mumbai
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career path, steady growth prospects with great scope to innovate. Our goal is to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
You'll architect and build a platform that helps thousands of developers tell data stories! Gramex, our flagship micro services-based low code applications platform, builds custom data applications at supersonic speed.
We are looking for a technical architect to build domain-specific new solutions and products leveraging the power of Gramex.
You Will
Design and implement enterprise-grade data architectures leveraging the medallion architecture (Bronze, Silver, Gold).
Develop and enforce data modelling standards, including flattened data models optimized for analytics.
Define and implement MDM strategies (Reltio), data governance frameworks (Collibra), and data classification policies.
Lead the development of data landscapes, capturing sources, flows, transformations, and consumption layers.
Collaborate with domain teams to ensure consistency across decentralized data products in a data mesh architecture.
Guide best practices for ingesting and transforming data using Fivetran, PySpark, SQL, and Delta Live Tables (DLT).
Define metadata and data quality standards across domains.
Provide architectural oversight for data platform development on Databricks (Lakehouse) and AWS ecosystem.
Skills And Qualifications
Experience in Pharma domain.
Data Modeling (dimensional, flattened, common data model, canonical, and domain-specific, entity level data understanding from business process point of view).
Proven expertise in Data Mesh or Domain-Oriented Data Architecture.
Experience with medallion/lakehouse architecture.
Ability to create data blueprints and landscape maps across complex enterprise systems.
Master Data Management (MDM) principles and tools (Reltio) (1).
Data Governance and Data Classification frameworks (1).
Strong experience with Fivetran**, PySpark, SQL, Python.
Deep understanding of Databricks (Delta Lake, Unity Catalog, Workflows, DLT) .
Experience with AWS services related to data (e.g., S3, Glue, Redshift, IAM, ).
Experience on Snowflake.
About Us
We help consult and deliver solutions to organizations where data acts as the core of decision making. We undertake strategic data consulting for organizations in laying out the roadmap for data-driven decision making. It helps organizations to convert data into a strategic differentiator. Through a host of our product and Solutions, and Service Offerings, we analyze and visualize large amounts of data.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Unity Catalog, snowflake, data classification, Data Mesh, Fivetran, Workflows DLT, Databricks Delta Lake, Domain-Oriented Data Architecture, Medallion Lakehouse Architecture, Data Modeling, Pyspark, Sql, Data Governance, Python"
Data Architect,Gramener,Fresher,,"Hyderabad, India",Login to check your skill match score,"What You'll Do:
Design and implement enterprise-grade data architectures leveraging the medallion architecture
Develop and enforce data modelling standards, including flattened data models optimized for analytics.
Define and implement MDM strategies (using Immuta), data governance
frameworks (Collibra), and data classification policies.
Lead the development of data landscapes, capturing sources, flows, transformations, and consumption layers.
Provide architectural oversight for data platform development on Databricks (Lakehouse) and AWS ecosystem.
What We're Looking For:
Data Modeling (dimensional, flattened, canonical, and domain-specific)
Master Data Management (MDM) principles and tools
Data Governance and Data Classification frameworks
Strong experience with Fivetran, Pyspark, SQL, DLT
Deep understanding of Databricks (Delta Lake, Unity Catalog, Workflows)
Experience with AWS services related to data (e.g., S3, Glue, Redshift, IAM)
Experience on Snowflake.
Experience in Pharma, MedTech, or Life Sciences domains
Familiarity with regulatory and compliance frameworks (e.g., GxP, HIPAA, GDPR)
Background in data product building
Architecture & Design:
Proven expertise in Data Mesh or Domain-Oriented Data Architecture
Experience with medallion/lake house architecture
Ability to create data blueprints and landscape maps across complex enterprise systems","Databricks Delta Lake, Domain-Oriented Data Architecture, DLT, Medallion Lake House Architecture, Unity Catalog, snowflake, data classification, Data Mesh, Fivetran, Pyspark, Data Modeling, Sql, Data Governance"
Senior Data Architect,Tredence Inc.,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"Primary Roles and Responsibilities:
Working experience in Snowflake; use of Snow SQL CLI, Snow Pipe creation of custom functions and Snowflake stored producers, schema modelling, performance tuning etc.
Expertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts
Extensive experience in DBT CLI, DBT Cloud, GitHub version control and repository knowledge, and DBT scripting to design & develop SQL processes to perform complex ELT processes and data pipeline build.
Ability to independently envision and develop innovative ETL and reporting solutions and execute them through to completion.
Triage issues to find gaps in existing pipelines and fix the issues
Analyze the data quality, align the technical design to data governance, and address all the required non-business but operational requirements during the design and build of data pipelines
Develop and maintain data pipelines using DBT
Provide advice, guidance, and best practices around Snowflake
Provide guidance on moving data across different environments in Snowflake
Create relevant documentation around database objects
Troubleshoot production support issues post-deployment and come up with solutions as required
Good Understanding and knowledge of CI/CD process and GitHub->DBT-> Snowflake integrations.
Advance SQL knowledge and hands-on experience in complex query writing using Analytical functions, Troubleshooting, problem-solving, and performance tuning of SQL queries accessing data warehouse as well as Strong knowledge of stored procedures.
Experience in Snowflake advanced concepts such as resource monitors, virtual warehouse sizing, query performance tuning, zero-copy clone, time travel and understanding how to use these features
Help joiner team members to resolve issues and technical challenges.
Drive technical discussions with client architect and team members
Good experience in developing scripts for data auditing and automating various database platform manual activities.
Understanding of the full software lifecycle and how development teams should work with DevOps to create more software faster.
Excellent communication, working in Agile Methodology/Scrum
Skills and Qualifications:
Bachelor's and/or master's degree in computer science or equivalent experience.
Must have total 13+ yrs. of IT experience and 3+ years experience in data Integration, ETL/ETL development, and database design or Datawarehouse design
Deep understanding of Star and Snowflake dimensional modelling.
Strong knowledge of Data Management principles
Working experience in Snowflake; use of Snow SQL CLI, schema modelling, performance tuning etc.
Develop and maintain data pipelines using DBT
Experience with writing complex SQL queries, especially dynamic SQL
Expertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts
Experience with performance tuning and optimization of SQL queries
Experience of working with Retail Data
Experience with data security and role-based access controls
Comfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects
Should have experience working in Agile methodology
Strong verbal and written communication skills.
Strong analytical and problem-solving skills with high attention to detail.","Snow SQL CLI, snowflake, dbt, Github, Data Modelling, Agile Methodology, Sql, Performance Tuning, Etl"
Google Cloud Data Architect - Remote Work,techolution,Fresher,,India,Login to check your skill match score,"Techolution is looking for highly skilled and innovative hands-on Google Cloud Data Architect who will help Architect & build a highly scalable and reliable platform to match our exponential growth. In this role you will be responsible for Architecting & building a solid back end infrastructure on Google Cloud which will enable data delivery in near real-time using next-gen technologies. You are a technical leader, serving as a liaison among business partners, technical resources, and project stakeholders. Also work closely with our clients, helping them to leverage the power of GCP to unlock the full potential of their data and drive their businesses forward.
Role: Google Cloud Data Architect
Location: Remote
Employment Type: Full time
Responsibilities:
Architect, Build & Deploy scalable Data Engineering & Analytics solutions in GCP.
Design and development of large scale data solutions using GCP services like DataProc, Dataflow, Cloud Bigtable, Big Query, Cloud SQL, Pub/Sub, Cloud Data Fusion, Cloud Composer, Cloud Functions, Cloud storage, Compute Engine, Looker and Cloud IAM.
Mentor and provide technical oversight and guidance to implementation teams while working in a coordinated manner to deliver and deploy the designed architecture.
Deploy the data pipeline following data governance and data security requirements, and implement it with the data quality check on Google Cloud Platform.
Create and deliver best practices recommendations for On Prem to Cloud or Cloud Native solutions in GCP.
Design and creation of data led strategies which provide clients with opportunities to leverage their data for greater insight or performance.
Leading the technical design and implementation of data solutions, ensuring that they meet our clients business requirements while adhering to best practices and industry standards.
Requirements:
Experience in Architecting and designing solutions leveraging services like Cloud Bigquery, Cloud Dataflow, Data Proc, Cloud Composer, Datafusion, Pubsub, Airflow and Cloud BigTable.
Must have handled projects using ETL / Data Pipeline and Orchestration tools Cloud composer, Cloud data fusion, Dataflow and Dataproc.
Hands-on experience with programming languages such as java/Go/Python.
Exposure with non relational databases such as Mongodb, cassandra, Dynamodb, redis etc..
Knowledge of messaging queues such as kafka and rabbitmq with strong troubleshooting skills
Certified Professional Cloud Architect & Official Google Data Engineer Certification is beneficial.
About Techolution:
Techolution is a next gen AI consulting firm on track to become one of the most admired brands in the world for AI done right. Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better human experience for the communities they serve.
At Techolution, we build custom AI solutions that produce revolutionary outcomes for enterprises worldwide. Specializing in AI Done Right, we leverage our expertise and proprietary IP to transform operations and help achieve business goals efficiently.
We are honored to have recently received the prestigious Inc 500 Best In Business award, a testament to our commitment to excellence. We were also awarded - AI Solution Provider of the Year by The AI Summit 2023, Platinum sponsor at Advantage DoD 2024 Symposium and a lot more exciting stuff! While we are big enough to be trusted by some of the greatest brands in the world, we are small enough to care about delivering meaningful ROI-generating innovation at a guaranteed price for each client that we serve.
Our thought leader, Luv Tulsidas, wrote and published a book in collaboration with Forbes, Failing Fast Secrets to succeed fast with AI. Refer here for more details on the content - https://www.luvtulsidas.com/
Let's explore further!
Uncover our unique AI accelerators with us:
1. Enterprise LLM Studio: Our no-code DIY AI studio for enterprises. Choose an LLM, connect it to your data, and create an expert-level agent in 20 minutes.
2. AppMod. AI: Modernizes ancient tech stacks quickly, achieving over 80% autonomy for major brands!
3. ComputerVision. AI: Our ComputerVision. AI Offers customizable Computer Vision and Audio AI models, plus DIY tools and a Real-Time Co-Pilot for human-AI collaboration!
4. Robotics and Edge Device Fabrication: Provides comprehensive robotics, hardware fabrication, and AI-integrated edge design services.
5. RLEF AI Platform: Our proven Reinforcement Learning with Expert Feedback (RLEF) approach bridges Lab-Grade AI to Real-World AI.
Some videos you wanna watch!
Computer Vision demo at The AI Summit New York 2023
Life at Techolution
GoogleNext 2023
Ai4 - Artificial Intelligence Conferences 2023
WaWa - Solving Food Wastage
Saving lives - Brooklyn Hospital
Innovation Done Right on Google Cloud
Techolution featured on Worldwide Business with KathyIreland
Techolution presented by ION World's Greatest
Visit us @www.techolution.com : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with technology.","Big Query, Go, Pub Sub, Cloud Composer, Cloud SQL, Cloud IAM, Cloud Data Fusion, Cloud Functions, Looker, Orchestration tools, Cloud Bigtable, Cassandra, Kafka, Cloud Storage, Data Pipeline, Python, Java, Dynamodb, Redis, Rabbitmq, Compute Engine, Dataproc, MongoDB, DataFlow, Etl"
Senior Data Architect,ChaiBu Group,5-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect
Experience Required: 510 years
Location: Bangalore
About the Role:
We're looking for a Senior Data Architect to lead the design and development of our data systems. This includes how we collect, store, move, and use data to help the business make better decisions. You'll work mainly with Azure but may also use other platforms. You'll help build strong data foundations for analytics, AI, and app integration while keeping costs and security in check.
What You'll Do:
Create and lead the overall data architecture strategy.
Guide and mentor junior data team members.
Build scalable and high-performing data solutions that fit business needs.
Design data models that are secure, reusable, and easy to access.
Create and manage data flow diagrams and data dictionaries.
Work closely with business teams to understand their data requirements and turn them into technical solutions.
Make sure data is easily accessible for data analysts, data scientists, and developers.
Maintain high standards of data quality, accuracy, and security.
Lead the design of data warehouses, data lakes, and other storage solutions.
Set and follow best practices for data governance and compliance.
Keep up with industry changes and help improve internal standards and practices.
What We're Looking For:
Bachelor's degree in Computer Science, IT, or equivalent experience.
5+ years of proven experience as a Data Architect or in a similar role.
Strong knowledge of data modeling, warehousing, and integration.
Hands-on experience with databases like SQL Server, Oracle, PostgreSQL.
Familiarity with big data tools like Hadoop, Spark, and data lakes such as Azure Data Lake.
Experience in creating ER diagrams, designing star/snowflake schemas, and building cost-effective data pipelines.
Ability to turn business goals into technical solutions.
Comfortable working with cloud platforms like Azure, AWS, or Google Cloud.
Strong understanding of data security and governance.
Excellent communication and teamwork skills.
Bonus Skills (Preferred, not mandatory):
Tools: Erwin, Azure Synapse, Azure Databricks, Azure DevOps, Power BI, Spark, Python, R.
Experience with Azure AI/ML Services, Event Hub, Stream Analytics, and scripting tools like Ansible.
Understanding of machine learning, CI/CD, container tools like Docker/Kubernetes.
Relevant certifications such as AWS Certified Solutions Architect or IBM Certified Data Architect.","snowflake schema, data pipelines, Event Hub, Azure AI ML Services, R, Stream Analytics, Hadoop, Power Bi, PostgreSQL, SQL Server, Azure Databricks, Data Modeling, Azure Synapse, Docker, Ansible, Er Diagrams, Azure Data Lake, Spark, Oracle, Azure, Python, Kubernetes, Azure DevOps"
Data Architect (Azure & Snowflake),CES,3-5 Years,,"Chennai, India",Login to check your skill match score,"CES has 26+ years of experience in delivering Software Product Development, Quality Engineering, and Digital Transformation Consulting Services to Global SMEs & Large Enterprises. CES has been delivering services to some of the leading Fortune 500 Companies including Automotive, AgTech, Bio Science, EdTech, FinTech, Manufacturing, Online Retailers, and Investment Banks. These are long-term relationships of more than 10 years and are nurtured by not only our commitment to timely delivery of quality services but also due to our investments and innovations in their technology roadmap. As an organization, we are in an exponential growth phase with a consistent focus on continuous improvement, process-oriented culture, and a true partnership mindset with our customers. We are looking for the right qualified and committed individuals to play an exceptional role as well as to support our accelerated growth.
You can learn more about us at: http://www.cesltd.com/
Job Description
Experience with Azure Synapse Analytics: Hands-on experience in designing, developing, and deploying solutions using Azure Synapse Analytics, including familiarity with its various components such as SQL pools, Spark pools, and Integration Runtimes.
Expertise in Azure Data Lake Storage: In-depth understanding of Azure Data Lake Storage, including its architecture, features, and best practices for managing a large-scale Data Lake or Lakehouse in an Azure environment.
Experience with AI Tools: Experience with AI Tools and LLMs (e.g. GitHub Copilot, Copilot, ChatGPT) in automating many of the responsibilities outlined for this role.
Knowledge of Avro and Parquet: Experience working with Avro and Parquet file formats, including data serialization, compression techniques, and schema evolution. Understanding of their advantages and use cases in a big data environment.
Healthcare: Prior experience working with data in a healthcare or clinical laboratory environment and a strong understanding of PHI, GDPR & HIPPA/HITRUST is highly desirable.
Certifications: Relevant certifications such as Azure Data Engineer Associate or Azure Synapse Analytics Developer Associate are highly desirable.
Essential Functions
Data Integration and ELT Development: Design, develop, and maintain data pipelines for ingestion, transformation, and loading of data into Azure Synapse Analytics. This includes understanding functional and non-functional requirements, performing source data analysis, data profiling, and implementing efficient ELT processes.
Azure Synapse Development: Work with Azure Synapse Analytics to build and optimize data models, SQL queries, stored procedures, and other artifacts necessary for data processing and analysis.
Data Lake File Handling: Understand the characteristics of various file formats, optimizing data storage, and implementing efficient data reading and writing mechanisms for incremental updates within Azure Synapse Analytics.
Data Governance and Security: Ensure compliance with data governance policies and implement security measures to protect sensitive data stored in Azure. This involves encryption, masking, and access control mechanisms.
Performance Optimization: Continuously optimize data pipelines and storage configurations to improve performance, scalability, and reliability. This includes identifying bottlenecks, query tuning, and leveraging Azure Synapse Analytics features for parallel processing.
Monitoring and Troubleshooting: Implement monitoring solutions to track data pipeline performance, data quality, and system health. Troubleshoot issues related to data ingestion, transformation, or storage, and provide timely resolutions.
Skills Needed to be Successful
Relational Database Experience: Proficiency with one or more of the following database platforms; e.g. Oracle, Microsoft SQL Server, PostgreSQL, MySQL/MariaDB
Proficiency in SQL: Strong SQL skills, including experience with complex SQL queries, stored procedures, and performance optimization techniques. Familiarity with T-SQL for Azure Synapse Analytics is a plus.
ELT and Data Integration Skills: Proven experience in building ELT pipelines and data integration solutions using tools like Azure Data Factory, Oracle Golden Gate, or similar platforms.
Ability to handle a variety of legacy data sources and file formats efficiently.
Data Modeling and Warehousing Concepts: Familiarity with dimensional modeling, star schemas, and data warehousing principles. Experience in designing and implementing data models for analytical workloads.
Analytical and Problem-Solving Abilities: Strong analytical skills with the ability to understand complex data requirements, troubleshoot technical issues, and propose effective solutions to meet business needs.
Communication and Collaboration: Excellent communication skills with the ability to collaborate effectively with cross-functional teams, including Data Scientists, Reporting Analysts, and DevOps professionals.","Azure Data Lake Storage, Relational Database Experience, Data Modeling and Warehousing Concepts, AI Tools, ELT and Data Integration, Parquet, Sql, Avro, Azure Synapse Analytics"
Azure Data Architect (Contract),TalenXis,5-7 Years,,India,Login to check your skill match score,", . .
- - .
.
% ()
Responsibilities:
Architect and model Business Intelligence (BI) and Analytics solutions to support data-driven decision-making.
Collaborate with stakeholders to understand business requirements and translate them into technical specifications for BI and Analytics solutions.
Develop and maintain data models, data integration processes, and data warehousing solutions to support BI and Analytics initiatives.
Implement and manage ETL (Extract, Transform, Load) processes to ensure accurate and timely data availability for BI and Analytics.
Design and implement effective cloud data models in Azure Data Lake & Azure Databricks to store and retrieve company data.
Ensure data quality and integrity across data models
Develop and enforce development standards.
Must Have Requirements:
Extensive experience in architecting and modeling BI and Analytics solutions.
Proficiency in designing and implementing ETL processes and data integration workflows.
Strong background in developing data models and data warehousing solutions to support BI and Analytics.
Hands-on expertise with Databricks for data engineering and SQL warehouse.
In-depth understanding of data warehouse's structure principles.
Expertise in SQL and database management systems (e.g., Oracle, SQL Server, PostgreSQL).
Knowledge of data visualization tools (e.g., Qlik, Power BI).
Familiarity with data warehousing solutions (e.g., Snowflake, Redshift).
Preferred Qualifications:
Experience with big data technologies (e.g., Hadoop, Spark).
Certification in data management or data architecture (e.g., CDMP, TOGAF)","Data warehousing solutions, Analytics Solutions, Data integration workflows, Azure Data Lake, Azure Databricks, Sql"
Consultant (Data Architect),Improzo,7-9 Years,,"Pune, India",Login to check your skill match score,"About Improzo
At Improzo (Improve + Zoe; meaning Life in Greek), we believe in improving life by empowering our customers. Founded by seasoned Industry leaders, we are laser focused on delivering quality-led commercial analytical solutions to our clients. Our dedicated team of experts in commercial data, technology, and operations has been evolving and learning together since our inception. Here, you won't find yourself confined to a cubicle; instead, you'll be navigating open waters, collaborating with brilliant minds to shape the future. You will work with leading Life Sciences clients, seasoned leaders and carefully chosen peers like you!
People are at the heart of our success, so we have defined our CARE values framework with a lot of effort, and we use it as our guiding light in everything we do. We CARE!
Customer-Centric: Client success is our success. Prioritize customer needs and outcomes in every action.
Adaptive: Agile and Innovative, with a growth mindset. Pursue bold and disruptive avenues that push the boundaries of possibilities.
Respect: Deep respect for our clients & colleagues. Foster a culture of collaboration and act with honesty, transparency, and ethical responsibility.
Execution: Laser focused on quality-led execution; we deliver! Strive for the highest quality in our services, solutions, and customer experiences.
About The Role
Introduction: We are seeking an experienced and highly skilled Data Architect to lead a strategic project focused on Pharma Commercial Data Management Operations. This role demands a professional with 7-9 years of experience in data architecture, data management, ETL, data transformation, and governance, with an emphasis on providing scalable and secure data solutions for the pharmaceutical sector.
The ideal candidate will bring a deep understanding of data architecture principles, experience with cloud platforms such as Snowflake, and a solid background in driving commercial data management projects. If you're passionate about leading impactful data initiatives, optimizing data workflows, and supporting the pharmaceutical industry's data needs, we invite you to apply.
Responsibilities
Key Responsibilities:
Lead Data Architecture and Strategy:
Design, develop, and implement the overall data architecture for commercial data management operations within the pharmaceutical business.
Lead the design and operations of scalable and secure data systems that meet the specific needs of the pharma commercial team, including marketing, sales, and operations.
Define and implement best practices for data architecture, ensuring alignment with business goals and technical requirements.
Develop a strategic data roadmap for efficient data management and integration across multiple platforms and systems.
Data Integration, ETL & Transformation:
Oversee the ETL (Extract, Transform, Load) processes to ensure seamless integration and transformation of data from multiple sources, including commercial, sales, marketing, and regulatory databases.
Collaborate with data engineers and developers to design efficient and automated data pipelines for processing large volumes of data.
Lead efforts to optimize data workflows and improve data transformation processes to enhance reporting and analytics capabilities.
Data Governance & Quality Assurance:
Implement and enforce data governance standards across the data management ecosystem, ensuring the consistency, accuracy, and integrity of commercial data.
Develop and maintain policies for data stewardship, data security, and compliance with industry regulations, such as HIPAA, GDPR, and other pharma-specific compliance requirements.
Work closely with business stakeholders to ensure the proper definition of master data and reference data standards.
Cloud Platform Expertise (Snowflake (critical to have), AWS, Azure):
Lead the adoption and utilization of cloud-based data platforms, particularly Snowflake, to support data warehousing, analytics, and business intelligence needs.
Collaborate with cloud infrastructure teams to ensure efficient management of data storage, compute resources, and performance optimization within cloud environments.
Stay up-to-date with the latest cloud technologies, such as Snowflake, AWS, Azure, or Google Cloud (optional)), and evaluate opportunities for incorporating them into data architectures.
Collaboration with Cross-functional Teams:
Work closely with business leaders in commercial operations, analytics, and IT teams to understand their data needs and provide strategic data solutions that enhance business operations.
Collaborate with data scientists, analysts, and business intelligence teams to ensure data is available for reporting, analysis, and decision-making.
Facilitate communication between IT, business stakeholders, and external vendors to ensure data architecture solutions align with business requirements.
Continuous Improvement & Innovation:
Drive continuous improvement efforts to optimize data pipelines, data storage, and analytics workflows.
Identify opportunities to improve data quality, streamline processes, and enhance the efficiency of data management operations.
Advocate for the adoption of new data management technologies, tools, and methodologies to improve data processing, security, and integration.
Leadership and Mentorship:
Lead and mentor a team of data engineers, analysts, and other technical resources, fostering a collaborative and innovative work environment.
Provide leadership in setting clear goals, performance metrics, and expectations for the team.
Offer guidance on data architecture best practices, ensuring all team members are aligned with the organization's data strategy.
Required Qualifications
Bachelor's degree in Computer Science, Data Science, Information Systems, or a related field.
7-9 years of experience in data architecture, data management, and data governance, with a proven track record of leading commercial data management operations projects.
Extensive experience in data integration, ETL, and data transformation processes, including familiarity with tools like Informatica, Talend, or Apache NiFi.
Strong expertise with cloud platforms, particularly Snowflake, AWS, Azure, or Google Cloud.
Strong knowledge of data governance frameworks, including data security, privacy regulations, and compliance standards in the pharmaceutical industry (e.g., HIPAA, GDPR).
Hands-on experience in designing scalable and efficient data architecture solutions to support business intelligence, analytics, and reporting needs.
Proficient in SQL and other query languages, with a solid understanding of database management and optimization techniques.
Ability to communicate technical concepts effectively to non-technical stakeholders and align data strategies with business goals.
Preferred Qualifications
Experience in the pharmaceutical or life sciences sector, particularly in commercial data management, sales, marketing, or operations.
Certification or formal training in cloud platforms (e.g., Snowflake, AWS, Azure) or data management frameworks.
Familiarity with data science methodologies, machine learning, and advanced analytics tools.
Knowledge of Agile methodologies for managing data projects.
Key Skills
Data Architecture & Design
Cloud Platforms (Snowflake critical to have)
Data Governance & Quality Assurance
ETL & Data Transformation
Data Integration & Pipelines
Pharmaceutical Data Management (Preferred)
SQL & Database Optimization
Leadership & Mentorship
Business & Technical Collaboration
Benefits
Competitive salary and benefits package.
Opportunity to work on cutting-edge tech projects, transforming the life sciences industry
Collaborative and supportive work environment.
Opportunities for professional development and growth.
Skills: snowflake,database,data governance & quality assurance,data integration & pipelines,etl & data transformation,azure,business & technical collaboration,aws,data management,analytics,sql,sql & database optimization,cloud platforms (snowflake),leadership & mentorship,cloud platforms,data architecture & design,data","Leadership, Business Technical Collaboration, Mentorship, Data Architecture Design, Data Integration Pipelines, snowflake, Database Optimization, Sql, Data Governance, Quality Assurance, Azure, AWS, Data Transformation, Etl"
Enterprise Data Architect - Data & Analytics Delivery Practice Lead,"Aezion, Inc",10-12 Years,,"Bengaluru, India",Login to check your skill match score,"About the Role:
We are seeking a highly skilled and results-driven Data & Analytics Delivery Practice Lead to lead our end-to-end data engineering, Big Data, Streaming and business intelligence (BI) solutions. This is a key leadership role for someone who thrives on building scalable, cloud-agnostic data solutions and driving measurable impact through data analytics.
You will be responsible for managing a high-performing data team of 20+ members across onsite and offshore locations, primarily working on any Big Data, streaming Platforms Databricks, Snowflake, Redshift, Spark, Kafka, SQL Server, PostgreSQL on any Cloud ( AWS/Azure/GCP). Your expertise in Python, cloud infrastructure, and data architecture will be instrumental in creating intelligent and efficient data pipelines and products.
In addition to delivery excellence, you will serve as the primary customer contact for ongoing engagements and play an active role in pre-sales effortsparticipating in prospect calls and leading proposal creation with an exceptional success rate.
Key Responsibilities:
Team Leadership and Development:
Lead, mentor, and develop a high-performing global team of 20+ data professionals across onshore and offshore locations.
Foster a culture of accountability, innovation, and continuous learning within the data team.
Establish career development plans and performance metrics to drive individual and team growth that aligns with organizational KPIs.
Align team structure and skill sets with evolving business and technology needs.
Delivery & Customer Focus:
Serve as the primary point of contact for customers, ensuring successful delivery of data and analytics solutions.
Translate complex customer requirements into scalable, high-impact data solutions.
Maintain a strong focus on stakeholder engagement and satisfaction, ensuring delivery meets or exceeds expectations.
Drive pre-sales activities, including client discovery calls and proposal development, maintaining close to 100% win rate.
Technology & Market Adaptation:
Architect and implement scalable, cloud-agnostic data solutions using technologies such as Big Data, streaming Platforms Databricks, Snowflake, Redshift, Spark, Kafka, SQL Server, PostgreSQL on any Cloud ( AWS/Azure/GCP)
Stay ahead of technology trends in data engineering, cloud computing, and big data platforms to ensure competitive offerings.
Evaluate and adopt emerging tools and frameworks that enhance productivity, performance, and innovation.
Cross-Functional Collaboration with Global Teams:
Collaborate closely with platform engineering, BI, and AI/ML teams to deliver integrated, end-to-end data solutions.
Act as a bridge between technical and non-technical stakeholders, translating business needs into technical execution.
Promote knowledge sharing and alignment across functional and geographical team boundaries.
Process Improvement and Reporting:
Define and continuously refine best practices in Technology frameworks and operational processes.
Drive improvements in data quality, performance, and governance across the data lifecycle.
Track team utilization and proactively forecast upcoming bench time, ensuring maximum billing efficiency and planning for resource allocation.
Achieve and maintain 100% billing utilization by aligning team capacity with project demands.
Generate and share regular utilization and billing reports with the leadership team to inform resourcing and financial strategies.
Lead initiatives to automate and streamline data processes, reducing manual effort and increasing delivery speed.
Required Skills and Qualifications:
10+ years of experience in Data Engineering and Analytics, with at least 35 years in a leadership role.
Proven experience managing global teams, both onsite and offshore.
Strong hands-on technical background in Big Data, streaming Platforms, Databricks, Snowflake, Redshift, Spark, Kafka, SQL Server, PostgreSQL on any Cloud ( AWS/Azure/GCP), and modern BI tools.
Demonstrated ability to design and scale data pipelines and architectures in large, complex environments.
Excellent soft skills including leadership, client communication, and stakeholder management.
A successful track record of pre-sales engagement and proposal development with high closure rates.
Experience working across cross-functional teams including platform and AI/ML engineering.
Strong problem-solving mindset with the ability to think strategically and execute tactically.
Preferred Qualifications:
Master's or Bachelor's degree in Computer Science, Data Science, Engineering, or related field.
Experience working in consulting or services-based environments.
Familiarity with governance, data security, and compliance best practices.
Exposure to modern data stack tools like dbt, Airflow, and Kafka.","Big Data streaming Platforms, snowflake, PostgreSQL, Data Architecture, SQL Server, Kafka, Redshift, Gcp, Spark, Databricks, Azure, Python, AWS"
Senior Data Architect,Axtria - Ingenious Insights,Fresher,,"Bengaluru, India",Login to check your skill match score,"POSITION: Data Architect (Individual contributor)
Data Architect + Gen AI
LOCATION: Noida, Gurugram, Pune, Bangalore
60% through out in academics
JOB OBJECTIVE: To leverage expertise in data architecture and management to design, implement, and
optimize a robust data warehousing platform for the pharmaceutical industry. The goal is to ensure
seamless integration of diverse data sources, maintain high standards of data quality and governance,
and enable advanced analytics through the definition and management of semantic and common data
layers. Utilizing Axtria's product and generative AI technologies, the aim is to accelerate business
insights and support regulatory compliance, ultimately enhancing decision-making and operational
efficiency.
Key Responsibilities:
Strong AIML, Gen AI exp
Data Modeling: Design logical and physical data models to ensure efficient data storage and
retrieval.
ETL Processes: Develop and optimize ETL processes to accurately and efficiently move data
from various sources into the data warehouse.
Infrastructure Design: Plan and implement the technical infrastructure, including hardware,
software, and network components.
Data Governance: Ensure compliance with regulatory standards and implement data
governance policies to maintain data quality and security.
Performance Optimization: Continuously monitor and improve the performance of the data
warehouse to handle large volumes of data and complex queries.
Semantic Layer Definition: Define and manage the semantic layer architecture and technology
stack to manage the lifecycle of semantic constructs including consumption into downstream
systems.
Common Data Layer Management: Integrate data from multiple sources into a centralized
repository, ensuring consistency and accessibility.
Deep expertise in architecting enterprise grade software systems that are performant,
scalable, resilient and manageable. Architecting GenAI based systems is an added plus.
Advanced Analytics: Enable advanced analytics and machine learning to identify patterns in
genomic data, optimize clinical trials, and personalize medication.
Generative AI: Should have worked with production ready usecase for GenAI based data and
Stakeholder Engagement: Work closely with business stakeholders to understand their data
needs and translate them into technical solutions.
Cross-Functional Collaboration: Collaborate with IT, data scientists, and business analysts to
ensure the data warehouse supports various analytical and operational needs.
Qualifications:
Proven experience in data architecture and data warehousing, preferably in the
pharmaceutical industry.
Strong knowledge of data modeling, ETL processes, and infrastructure design.
Experience with data governance and regulatory compliance in the life sciences sector.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
Preferred Skills:
Familiarity with advanced analytics and machine learning techniques.
Experience in managing semantic and common data layers.
Knowledge of FDA guidelines, HIPAA regulations, and other relevant regulatory standards.
Experience with generative AI technologies and their application in data warehousing
ABOUT AXTRIA
Axtria (www.axtria.com) is high growth advanced analytics and business information Management
Company based out of New Jersey with locations in AZ, GA and VA in USA and Gurgaon in India. We
have been named as one of the fastest growing companies in the US by Inc. 5000 in 2014.
Our broad portfolio of services and solutions help our clients improve their sales, marketing, supply
chain and distribution planning and operations in various industries such as Pharma, Retail, Banking and
Technology. We blend analytics, technology and consulting to help customers gain deep insights from
their customer data, create strategic advantage and drive profitable growth.
The leadership team at Axtria brings deep industry experience, expertise in sales, marketing and risk
management as well as a passion for building cutting-edge analytics and technology solutions.
Our global team is committed to delivering high quality, high-impact solutions for our clients and to
building a world-class firm with enduring value. Our unique team combines real-world business
knowledge, a depth of analytical skill and experience with the latest technologies. Those who excel at
Axtria share a number of common qualities. They are smart, humble and have the analytical toolkit to
put their intelligence to work. They are passionate about data and analytics & seek to constantly learn.","Performance Optimization, Generative AI, ETL Processes, Common Data Layer Management, Infrastructure Design, Machine Learning, Data Modeling, Advanced Analytics, Data Architecture, Data Warehousing, Data Governance"
Senior Data Architect,Tredence Inc.,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"Primary Roles and Responsibilities:
Working experience in Snowflake; use of Snow SQL CLI, Snow Pipe creation of custom functions and Snowflake stored producers, schema modelling, performance tuning etc.
Expertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts
Extensive experience in DBT CLI, DBT Cloud, GitHub version control and repository knowledge, and DBT scripting to design & develop SQL processes to perform complex ELT processes and data pipeline build.
Ability to independently envision and develop innovative ETL and reporting solutions and execute them through to completion.
Triage issues to find gaps in existing pipelines and fix the issues
Analyze the data quality, align the technical design to data governance, and address all the required non-business but operational requirements during the design and build of data pipelines
Develop and maintain data pipelines using DBT
Provide advice, guidance, and best practices around Snowflake
Provide guidance on moving data across different environments in Snowflake
Create relevant documentation around database objects
Troubleshoot production support issues post-deployment and come up with solutions as required
Good Understanding and knowledge of CI/CD process and GitHub->DBT-> Snowflake integrations.
Advance SQL knowledge and hands-on experience in complex query writing using Analytical functions, Troubleshooting, problem-solving, and performance tuning of SQL queries accessing data warehouse as well as Strong knowledge of stored procedures.
Experience in Snowflake advanced concepts such as resource monitors, virtual warehouse sizing, query performance tuning, zero-copy clone, time travel and understanding how to use these features
Help joiner team members to resolve issues and technical challenges.
Drive technical discussions with client architect and team members
Good experience in developing scripts for data auditing and automating various database platform manual activities.
Understanding of the full software lifecycle and how development teams should work with DevOps to create more software faster.
Excellent communication, working in Agile Methodology/Scrum
Skills and Qualifications:
Bachelor's and/or master's degree in computer science or equivalent experience.
Must have total 13+ yrs. of IT experience and 3+ years experience in data Integration, ETL/ETL development, and database design or Datawarehouse design
Deep understanding of Star and Snowflake dimensional modelling.
Strong knowledge of Data Management principles
Working experience in Snowflake; use of Snow SQL CLI, schema modelling, performance tuning etc.
Develop and maintain data pipelines using DBT
Experience with writing complex SQL queries, especially dynamic SQL
Expertise in Snowflake data modelling, ELT using Snowflake SQL, Snowflake Task Orchestration implementing complex stored Procedures and standard DWH and ETL concepts
Experience with performance tuning and optimization of SQL queries
Experience of working with Retail Data
Experience with data security and role-based access controls
Comfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects
Should have experience working in Agile methodology
Strong verbal and written communication skills.
Strong analytical and problem-solving skills with high attention to detail.","Snow SQL CLI, snowflake, dbt, Github, Data Modelling, Agile Methodology, Sql, Performance Tuning, Etl"
Solution / Data Architect (Databricks),Koantek,9-11 Years,,"Mumbai, India",Login to check your skill match score,"Location: Mumbai
Work mode: Hybrid
Must have skills : Databricks, (Hands on Python, SQL,Pyspark with any cloud platform)
Job Summary:
The Databricks AWS/Azure/GCP Architect at Koantek builds secure, highly scalable big data solutions to
achieve tangible, data-driven outcomes all the while keeping simplicity and operational effectiveness in
mind. This role collaborates with teammates, product teams, and cross-functional project teams to lead
the adoption and integration of the Databricks Lakehouse Platform into the enterprise ecosystem and
AWS/Azure/GCP architecture. This role is responsible for implementing securely architected big data
solutions that are operationally reliable, performant, and deliver on strategic initiatives.
Requirements:
Expert-level knowledge of data frameworks, data lakes and open-source projects such as Apache
Spark, MLflow, and Delta Lake
Expert-level hands-on coding experience in Spark/Scala,Python or Pyspar
In depth understanding of Spark Architecture including Spark Core, Spark SQL, Data Frames,
Spark Streaming, RDD caching, Spark MLib
IoT/event-driven/microservices in the cloud- Experience with private and public cloud
architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using
AWS/Azure/GCP services
9+ years in consulting experience with minimum 7+ Years of experience in data engineering,
data platform and analytics,
Projects delivered with hands-on experience in development on databricks
knowledge of any one cloud platform (AWS or Azure or GCP)
Deep experience with distributed computing with spark with knowledge of spark runtime
internals
Familiarity with CI/CD for production deployments
Familiarity with optimization for performance and scalabilityCompleted data engineering
professional certification and required classes","Spark MLib, MLflow, Data Frames, CI CD, Delta Lake, RDD caching, Pyspark, Spark SQL, Microservices, Iot, Sql, Spark Core, Spark Streaming, Gcp, Databricks, Azure, Python, AWS"
Principal Digital Architect-Data Architect,Caterpillar Inc.,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Career Area
Technology, Digital and Data
Job Description
Your Work Shapes the World at Caterpillar Inc.
When you join Caterpillar, you're joining a global team who cares not just about the work we do but also about each other. We are the makers, problem solvers, and future world builders who are creating stronger, more sustainable communities. We don't just talk about progress and innovation here we make it happen, with our customers, where we work and live. Together, we are building a better world, so we can all enjoy living in it.
Job Purpose
A Principal Digital Architect drives solutions with the core digital platform that underpins many applications that supports Cat Digital's key business domains of eCommerce, advanced condition monitoring, lead generation, service support and equipment management. The solutions will solve big data problems with over 1.4 million connected assets worldwide, advanced analytics, AI capabilities, global marketing and much more. This role is a key leader and accountable to work across the organization for both business and technical alignment to drive the required business outcomes.
Job Duties
Key areas of accountability include leading the development of Architecture Solutions for strategic Cat Digital projects and programs; developing and maintaining global technology roadmaps and application evolution plans; leading in the evaluation of new technology, information or integration standards; and developing digital strategy for a specific technical or business domain.
Responsibilities Include One Or More Of The Following
Provide oversight for architecture assessment and design for infrastructure, information or integration domains that provide core capabilities for the enterprise.
Lead Architecture design of end to end enterprise integrated systems that serves multiple business functions.
Lead the design and implementation of enterprise data model and metadata structures for complex projects.
Initiate and deliver technology evaluation and recommendations.
Develop and maintain current, planned and future state architecture blueprints.
Lead in the identification and analysis of enterprise business drivers and requirements that drive the future state architecture.
Basic Qualifications
Position requires a four-year degree from an accredited college or university in computer science, information technology, or related field or equivalent work experience.
15 or more years of a progressive career in distributed system software engineering and architecture
Strong demonstratable experience delivering product and/or enterprise architecture for enterprise scale solutions in public cloud or hybrid eco-systems
At least 5 years working experience in Cloud Technologies such as AWS & Microsoft Azure
Must have excellent communication skills and be able to deal with sensitive issues, mentor and coach and/or persuade others on new technologies, new applications, or potential solutions.
Top Candidates Will Have
Understand the data platform and build new data solutions on the existing data platform. Impact analysis needs to be performed so as not to have unknown impact in other data solutions build on the platform.
Understand the current data landscape and build new solutions on top of existing solution.
Trouble shoots and finds solutions for technical and functional issues identified in the program/project.
Evaluate, analyse, document and communicate business requirements to stakeholders.
Run Architecture meetings/discussions and document the solutions on confluence. Complete the solutions and have engineering handover.
Support the engineering team during the entire cycle of the build and deploy phases.
Report on common sources of technical, functional issues and/or questions and make recommendations to architecture team for long term solution.
Own and develop relationship with partners (customers, dealers, Technical Product Management, Architect teams), working with them to optimize and enhance the data products.
Provide guidance on the technical solutions and guidance on new product like optimal database recommendations like dynamo vs Postgre, AWS options like Kinesis and Event bridge.
Own the solutions on the Data lake (Snowflake), The solutions should be performant, secure and Cost Optimal.
Own some of the data domains in the Data Platform i.e. Any solution on the data domain should be either worked upon or reviewed by the architect.
Provide ROM (rough order of magnitude) for the solutions and data products.
Based on understanding of data domains and Business requirements create reusable data products which can be used across applications and teams.
Create/Review HLA and TA documentation with reference to a business requirement.
Improve architecture by tracking emerging technologies and evaluating their applicability to business goals and operational requirements.
Identify and solution for business critical business rules for improving the data quality in the platform.
Skills
Must Have:
AWS (EMR, Glue, S3, Fargate, SNS, SQS, Kinesis, AWS EventBridge, RDS, DynamoDB) , Snowflake, SQL, Python, ER Modelling.
Good To Have
Microservices and API knowledge
Posting Dates
May 19, 2025 - May 25, 2025
Caterpillar is an Equal Opportunity Employer.
Not ready to apply Join our Talent Community.","AWS EventBridge, Fargate, snowflake, Glue, API knowledge, ER Modelling, S3, RDS, Dynamodb, Emr, Sql, Microservices, Kinesis, Sqs, Sns, Python, AWS"
Senior GCP Data Architect,Miracle Software Systems India Private Limited,Fresher,,India,IT/Computers - Software,"Responsibilities
Architect and deliver end-to-end data solutions on Google Cloud Platform, aligning with enterprise data strategy and business objectives.
Should possess strong expertise in core GCP services, including integration, orchestration, and analytics tools.
Collaborate directly with US-based stakeholders to gather requirements, define architecture, and ensure successful delivery.
Lead and manage offshore development teams, providing technical direction and ensuring adherence to best practices.
Drive data governance, security, and compliance throughout the data lifecycle in cloud-based environments.
Conduct performance tuning, optimization, and troubleshooting complex data workflows and systems.
Communicate complex technical concepts to cross-functional teams, executives, and client stakeholders.
Continuously evaluate and adopt emerging GCP capabilities to enhance architectural performance and innovation.
Develop scalable and maintainable cloud-native data pipelines and solutions tailored to business needs.
Additional experience with other cloud platforms, such as AWS or Azure, is advantageous.","Looker, BigQuery, DataFlow"
Data Architect,Innova solutinos,14-20 Years,,"Hyderabad, Chennai",Information Technology,"Responsibilities:
Design and implement enterprise data models, ensuring data integrity, consistency, and scalability.
Analyse business needs and translate them into technical requirements for data storage, processing, and access.
In-memory Cache: Optimizes query performance by storing frequently accessed data in memory.
Query Engine: Processes and executes complex data queries efficiently.
Business Rules Engine (BRE): Enforces data access control and compliance with business rules.
Select and implement appropriate data management technologies, including databases, data warehouses.
Collaborate with data engineers, developers, and analysts to ensure seamless integration of data across various systems.
Monitor and optimize data infrastructure performance, identifying and resolving bottlenecks.
Stay up-to-date on emerging data technologies and trends, recommending and implementing solutions.
Document data architecture and processes for clear communication and knowledge sharing, including the integration.
Qualifications:
Proven experience in designing and implementing enterprise data models.
Expertise in SQL and relational databases (e.g., Oracle, MySQL, PostgreSQL).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) is mandatory.
Working experience with ETL tools and data ingestion leveraging any real-time solutions (e.g., Kafka, streaming) is required
Strong understanding of data warehousing concepts and technologies.
Familiarity with data governance principles and best practices.
Excellent communication, collaboration, and problem-solving skills.
Ability to work independently and as part of a team.
Strong analytical and critical thinking skills.
Experience with data visualization & UI Development is a plus.
Bachelors degree in computer science, Information Technology, or a related fiel
Role:DBA / Data warehousing - Other
Industry Type:IT Services & Consulting
Department:Engineering - Software & QA
Employment Type:Full Time, Permanent
Role Category:DBA / Data warehousing
Education
UG:Graduation Not Required","Data Modeling, Data Architecture, Etl"
Data Architect,Citiustech Healthcare Technology Private Limited,5-10 Years,,"Mumbai City, Bengaluru, Mumbai","Information Technology, Information Services","We are looking for a data architect with below skillset
Hands on skills withImage data (preferably DICOM) parsing
Architect and implement data warehousing solutions using AWS (Redshift, S3, Lambda)
Develop and maintain ETL pipelines using Informatica PowerCenter or AWS Glue
Collaborate with cross-functional teams to identify and prioritize data requirements
Analyze complex data sets using SQL, Python, and Java and or/Apex languageto inform business decisions
Implement data visualization tools (Tableau, Power BI) for stakeholder reporting
Ensure data integrity, security, and compliance with HIPAA, GDPR, and CCPA
Requirements:> 5 years of experience in data management with focus in image metadata/bigdata
Strong expertise in:
PACS& RIS, SYNAPSE/XNAT, systems
AWS (Redshift, S3, Lambda, Glue)
Data governance, quality, and security
ETL pipelines (Informatica PowerCenter or AWS Glue)
Data warehousing and visualization","Aws, Etl"
Data Architect - Precision Medicine Team,Amgen Inc,6-11 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities:
Architect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.
Leverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation
Support development planning by breaking down features into work that aligns with the architectural direction runway
Participate hands-on in pilots and proofs-of-concept for new patterns
Create robust documentation of architectural direction, patterns, and standards
Present and train engineers and cross-team collaborators on architecture strategy and patterns
Collaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.
Design robust data models, and processing layers, that support both analytical processing and operational reporting needs.
Develop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.
Ensure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.
Provide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.
Develop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.
Serve as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.
Collaborate with stakeholders to define data requirements, architecture specifications, and project goals.
Continuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.
Basic Qualifications and Experience:
Masters degree with 6 to 8 years of experience in data management and data solution architecture
Bachelors degree with 8 to 10 years of experience in in data management and data solution architecture
Diploma and 10 to 12 years of experience in in data management and data solution architecture
Functional Skills:
Must-Have Skills:
Minimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.
Minimum of 7 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.
Hands-on experience with Databricks, including data engineering, optimization, and analytics workloads.
Deep understanding of Power BI, including model design, DAX, and Power Query.
Proven experience designing and implementing data mastering solutions and data governance frameworks.
Expertise in cloud platforms (AWS), data lakes, and data warehouses.
Strong knowledge of ETL processes, data pipelines, and integration technologies.
Strong communication and collaboration skills to work with cross-functional teams and senior leadership.
Ability to assess business needs and design solutions that align with organizational goals.
Exceptional hands-on capabilities with data profiling, data transformation, data mastering
Success in mentoring and training team members
Good-to-Have Skills:
Experience in developing differentiated and deliverable solutions
Experience with human data, ideally human healthcare data
Familiarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management
Professional Certifications(please mention if the certification is preferred or mandatory for the role):
ITIL Foundation or other relevant certifications (preferred)
SAFe Agile Practitioner (6.0)
Microsoft Certified: Data Analyst Associate (Power BI) or related certification.
Databricks Certified Professional or similar certification.
Soft Skills:
Excellent analytical and troubleshooting skills
Deep intellectual curiosity
Highest degree of initiative and self-motivation
Strong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics
Confidence technical leader
Ability to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones
Ability to manage multiple priorities successfully
Team-oriented, with a focus on achieving team goals
Strong problem solving, analytical skills;
Ability to learn quickly and retain and synthesize complex information from diverse sources.","healthcare data, analytical, Laboratory, Troubleshooting, Data Warehouse, Etl Process"
Data Architect,Grid Dynamics,6-8 Years,,Pune,Information Technology,"Responsibilities:
Trusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.
Pre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.
Technology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing
assets and solutions strategy across multiple industries.
Engineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.
Business Development & Partnership:
Manage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.
Requirements:
Extensive practical experience in Big Data engineering, data governance, and cloud data
platforms.
Strong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.
Strong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.
Deep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.
Experience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.
Managed key client relations worldwide and advised global technology and business leaders on innovation and data strategy.
Experience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,Grid Dynamics,6-8 Years,,Bengaluru,Information Technology,"Responsibilities:
Trusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.
Pre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.
Technology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing
assets and solutions strategy across multiple industries.
Engineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.
Business Development & Partnership:
Manage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.
Requirements:
Extensive practical experience in Big Data engineering, data governance, and cloud data
platforms.
Strong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.
Strong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.
Deep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.
Experience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.
Managed key client relations worldwide and advised global technology and business leaders on innovation and data strategy.
Experience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,Grid Dynamics,6-8 Years,,Chennai,Information Technology,"Responsibilities:
Trusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.
Pre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.
Technology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing
assets and solutions strategy across multiple industries.
Engineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.
Business Development & Partnership:
Manage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.
Requirements:
Extensive practical experience in Big Data engineering, data governance, and cloud data
platforms.
Strong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.
Strong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.
Deep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.
Experience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.
Managed key client relations worldwide and advised global technology and business leaders on innovation and data strategy.
Experience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,INNOVA SOLUTIONS,14-20 Years,,"Hyderabad, Chennai",IT Management,"Data Architect:
Design and implement enterprise data models, ensuring data integrity, consistency, and scalability
Responsibilities:
Design and implement enterprise data models, ensuring data integrity, consistency, and scalability.
Analyse business needs and translate them into technical requirements for data storage, processing, and access.
In-memory Cache: Optimizes query performance by storing frequently accessed data in memory.
Query Engine: Processes and executes complex data queries efficiently.
Business Rules Engine (BRE): Enforces data access control and compliance with business rules.
Select and implement appropriate data management technologies, including databases, data warehouses.
Collaborate with data engineers, developers, and analysts to ensure seamless integration of data across various systems.
Monitor and optimize data infrastructure performance, identifying and resolving bottlenecks.
Stay up-to-date on emerging data technologies and trends, recommending and implementing solutions.
Document data architecture and processes for clear communication and knowledge sharing, including the integration.
Qualifications:
Proven experience in designing and implementing enterprise data models.
Expertise in SQL and relational databases (e.g., Oracle, MySQL, PostgreSQL).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) is mandatory.
Working experience with ETL tools and data ingestion leveraging any real-time solutions (e.g., Kafka, streaming) is required
Strong understanding of data warehousing concepts and technologies.
Familiarity with data governance principles and best practices.
Excellent communication, collaboration, and problem-solving skills.
Ability to work independently and as part of a team.
Strong analytical and critical thinking skills.
Experience with data visualization & UI Development is a plus.
Bachelors degree in computer science, Information Technology, or a related fiel","Data Modeling, Mysql, Data Architecture, Etl, Oracle, Aws"
Data Architect,RARR Technologies,10-18 Years,,"Hyderabad, Noida",Software,"Develop and Implement, Strong indata modellingand pipeline design Experience with metadata driven frameworks and governance practices Strong analytical skills to identify and reduce redundancies Knowledge of Snowflake and Medallion Architecture Objective
Optimize Data Pipeline Performance and Reliability Expertise in data pipeline optimization and performance tuning Experience with Indexing and efficient orchestration techniques Ability to identify and implement cost-saving measures Knowledge of monitoring tools and processes Objective
Enhance data modelling and Reusability Strong Communication and training skills Experience in data modeling and reusable asset creation Able to identify and train Subject matter experts Proficiency in gathering and analyzing Stakeholder Feedback Objective
Strengthen Devops Practices and Documentation Knowledge of version control and release processes Experience of DevOps process and CI/CD pipelines Ability to establish and maintain data asset frameworks Strong Documentation skills Objective
Lead and Develop the Data Engineering Team Leadership and team management skills Experience in conducting performance reviews and skill development plans Ability to establish and lead a center of Excellence(CoE) Proficiency in using tasking and estimation tools like Jira and DevOps
Data Modeler, Data Architect, Snowfake","snowflake, Metadata Governance, Team Leadership, Devops, Data Modeling, Data Pipeline"
Data Architect,Whisk Software Private Limited,6-13 Years,,"Gurugram, Hyderabad, Pune",Software,"Develop architectural strategies for data modelling, design and implementation to meet stated requirements for metadata management, master data management, Data warehouses, ETL and ELT.
Analyzing business requirements, designing scalable/ robust data models, documenting conceptual, logical physical data model design, helping developers in development/ creating DB structures and supporting developers throughout the project life cycle.
Lead and Mentor Data Engineers: This role will be responsible for leading and developing a team of data engineers focused on the growth in the teams skills and ability to execute as a team using DevOps and Data Ops principles.
Investigate new technologies, data modelling methods and information management systems to determine which ones should be incorporated onto data architectures, and develop implementation timelines and milestones.
Recognizes and resolves conflicts between models, ensuring that information and data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions).
Participates in the design of the information architecture: supports projects, reviews information elements including models, glossary, flows, data usage.
Provides guidance to the team in achieving the project goals/milestones.
Works independently within broad guidelines and policies, with guidance in only the most complex situations.
Contribute as an expert to multiple delivery teams, defining best practices, building reusable design components, capability building, aligning industry trends and actively engaging with wider data communities.
Skills and Experience
Graduate or post graduate in Computer science/Electronics/Software engineering.
6+ years of relevant experience in Data modelling for DW analytics applications (OLAP) / Database related technologies.
Expert data modelling skills (i.e. conceptual, logical and physical model design, experience with Enterprise Data Warehouses and Data Marts).
Solid understanding of cloud database technologies and services (eg..GCP, Redshift, Aurora, DynamoDB, etc)
Experience in working with data governance, data quality, and data security teams.
Experienced in knowledge-driven data processing techniques like data curation, representation, standardization, normalization and any other type of processing that prepares the data for integration, persistence, analysis, exchange/share and so forth.
Experience in handling very large DBs and large data volumes
Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, data replication/CDC, message-oriented data movement and upcoming data ingestion and integration technologies such as stream data integration and data virtualization.
Strong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production.
Knowledge with popular data discovery, analytics, and BI software tools like MicroStrategy, Tableau, Power BI and others for semantic-layer-based data discovery.
Ability to lead and mentor teams for effective delivery
Crisp and effective executive communication skills, including significant experience presenting cross-functionally and across all levels.","Computer Science, Information Management, metadata, Microstrategy, Analytics, virtualization, Data Quality, Olap, Sql, Python"
Data Architect,Grid Dynamics,6-8 Years,,Mumbai,Information Technology,"Responsibilities:
Trusted Advisor:Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.
Pre-Sales and Consulting:Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.
Technology Strategy and R&D:Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing
assets and solutions strategy across multiple industries.
Engineering:Working with Grid Dynamics s delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.
Business Development & Partnership:
Manage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.
Requirements:
Extensive practical experience in Big Data engineering, data governance, and cloud data
platforms.
Strong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.
Strong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.
Deep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.
Experience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.
Managed key client relations worldwide and advised global technology and business leaders on innovation and data strategy.
Experience with Big 4 consulting is a plus.","Consulting, Analytical, Enterprise Architecture, Cassandra, Data Management"
Data Architect,Siemens,8-12 Years,,Bengaluru,Manufacturing,"Job description
As a Data Architect, you are required to:
Design & develop technical solutions which combine disparate information to create meaningful insights for business, using Big-data architectures
Build and analyze large, structured and unstructured databases based on scalable cloud infrastructures
Develop prototypes and proof of concepts using multiple data-sources and big-data technologies
Process, manage, extract and cleanse data to apply Data Analytics in a meaningful way
Design and develop scalable end-to-end data pipelines for batch and stream processing
Regularly scan the Data Analytics landscape to stay up to date with latest technologies, techniques, tools and methods in this field
Stay curious and enthusiastic about using related technologies to solve problems and enthuse others to see the benefit in business domain
Qualification:
Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Engineering / Analytics is desirable.
Experience level:
Minimum 8 years in software development with at least 2 - 3 years hands-on experience in the area of Big-data / Data Engineering.
Desired Knowledge & Experience:
Data Engineer -Big Data Developer
Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming
Knowing Spark internals: Catalyst/Tungsten/Photon
Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader
IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot
Test: pytest, Great Expectations
CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing
Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction
Languages: Python/Functional Programming (FP)
SQL: TSQL/Spark SQL/HiveQL
Storage: Data Lake and Big Data Storage Design
Additionally it is helpful to know basics of:
Data Pipelines: ADF/Synapse Pipelines/Oozie/Airflow
Languages: Scala, Java
NoSQL: Cosmos, Mongo, Cassandra
Cubes: SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model
SQL Server: TSQL, Stored Procedures
Hadoop: HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka
Data Catalog: Azure Purview, Apache Atlas, Informatica
Big Data Architect
Expert: in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer
Mentor: mentors/educates Developers in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer
Architecture Styles: Lakehouse, Lambda, Kappa, Delta, Data Lake, Data Mesh, Data Fabric, Data Warehouses (e.g. Data Vault)
Application Architecture: Microservices, NoSql, Kubernetes, Cloud-native
Experience: Many years of experience with all kinds of technology in the evolution of data platforms (Data Warehouse -> Hadoop -> Big Data -> Cloud -> Data Mesh)
Certification: Architect certification (e.g. Siemens Certified Software Architect or iSAQB CPSA)
Required Soft-skills & Other Capabilities:
Excellent communication skills, in order to explain your work to people who don't understand the mechanics behind data analysis
Great attention to detail and the ability to solve complex business problems
Drive and the resilience to try new ideas, if the first ones don't work
Good planning and organizational skills
Collaborative approach to sharing ideas and finding solutions
Ability to work independently and also in a global team environment.","Github Copilot, Git, Sql, Azure Devops, Java"
AWS Data Architect,RARR Technologies,10-15 Years,,Hyderabad,Software,"Design, develop, and implement scalable, secure, and high-performance data architectures on AWS.
Lead the architecture and development of large-scale data platforms utilizing AWS services such as S3, Redshift, RDS, Lambda, Glue, EMR, Athena, and others.
Develop and optimize data pipelines and workflows using Python and AWS services.
Design and implement data lakes and data warehouses for efficient storage and querying of large datasets.
Collaborate with business stakeholders and cross-functional teams to gather requirements and translate them into technical solutions.
Ensure high-quality data availability, integrity, and security across platforms.
Automate manual processes, build frameworks, and implement CI/CD pipelines for data workflows.
Monitor and optimize the performance of data systems, databases, and processing pipelines.
Develop and maintain documentation for data models, processes, and infrastructure.
Provide mentorship and leadership to junior developers and data engineers.
Stay up-to-date with emerging AWS technologies, Python libraries, and industry best practices","Lambda, Data Architecture, Data Warehousing, Python, Aws"
Data Architect - R&D Data Catalyst Team,Amgen Inc,4-6 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities
Architect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.
Leverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation
Support development planning by break ing down features into work that aligns with the architectural direction runway
Participate hands-on in pilots and proofs-of-concept for new pattern
Create robust documentation of architectural direction, patterns, and standards
Present and train engineers and cross-team collaborators on architecture strategy and patterns
Collaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.
Design robust data models , and processing layers, that support both analytical processing and operational reporting needs.
Develop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.
ensure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.
Provide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.
Develop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.
Serve as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.
Collaborate with stakeholders to define data requirements, architecture specifications, and project goals.
Continuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions
Basic Qualifications and Experience
Master's degree with 4 to 6 years of experience in data management and data architecture O
Bachelor's degree with 6 to 8 years of experience in data management and data architectur
Functional
Skills:
Must-Have Skills
Minimum of 3 years of hands-on experience with BI solutions (Preferably Power BI or Business Objects) including report development, dashboard creation, and optimization.
Minimum of 7 years of hands-on experience building change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management
Hands-on experience with Databricks, including data engineering, optimization, and analytics workloads.
Deep understanding of Power BI, including model design , DAX, and Power Query.
Proven experience designing and implementing data mastering solutions and data governance frameworks.
Expertise in cloud platforms ( AWS ), data lakes, and data warehouses.
Strong knowledge of ETL processes, data pipelines, and integration technologies
Strong communication and collaboration skills to work with cross-functional teams and senior leadership.
Ability to assess business needs and design solutions that align with organizational goals.
Exceptional h ands - on capabilities with data profiling, data transformation, data mastering
Success in mentoring and training team members
Good-to-Have
Skills:
Experience in developing differentiated and deliverable solutions
Experience with human data, ideally human healthcare data
Familiarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management
Professional Certifications (please mention if the certification is preferred or mandatory for the role)
ITIL Foundation or other relevant certifications (preferred)
SAFe Agile Practitioner (6.0
Microsoft CertifiedData Analyst Associate (Power BI) or related certification.
Databricks Certified Professional or similar certification.
Soft
Skills:
Excellent analytical and troubleshooting skills
Deep intellectual curiosity
High est degree of initiative and self-motivation
Strong verbal and written communication skills , including presentation to varied audiences of complex technical/business topics
Confidence technical leader
Ability to work effectively with global, virtual teams , specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones
ability to manage multiple priorities successfully
Team-oriented, with a focus on achieving team goals
Strong problem solving, analytical skills;
Ability to learn quickly and retain and synthesize complex information from diverse sources","Optimization, Etl Process, Dax, aws, Power Query, Powerbi"
Master Data Management Data Architect,Amgen Inc,3-6 Years,,Hyderabad,Pharmaceutical,"Deliver outstanding self-service and automation experiences for our global workforce
Create ServiceNow catalog items, workflows, and cross-platform API integrations
Create and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.
Create and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.
Create and maintain data integrations between ServiceNow and other systems
Develop system integrations and process automation
Participate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives
Collaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions
Design, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner
Participate in problem analysis, code review, and system design
Remain current on new technology and apply innovation to improve functionality
Collaborate closely with stakeholders and team members to configure, improve and maintain current applications
Work directly with users to resolve support issues within product team responsibilities
Monitor health, performance and usage of developed solutions
Basic Qualifications:
Master's degree and 1 to 3 years of experience in computer science, IT, or related field OR
Bachelor's degree and 3 to 5 years of experience in computer science, IT, or related field OR
Diploma and 7 to 9 years of experience in computer science, IT, or related fiel
Required Skills & Qualifications:
6+ years of deep hands-on experiencewith ServiceNow administration and development in two or more productsITSM, ITBM, ITOM, GRC, HRSD, or Security Operations
ServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;
Strong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform
Experience creating and managing Scoped Applications
Workflow automation and integration development using REST, SOAP, or MID servers
Scripting skills in Python, Bash, or other programming languages
Working in an Agile (SAFe, Scrum, and Kanban) environment
Preferred Qualifications:
Good-to-Have
Skills:
Experience with other configuration management tools (e.g., Puppet, Chef).
Experience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)
Experience with Terraform & CloudFormation for AWS infrastructure automation
Knowledge of AWS Lambda and event-driven architectures.
Exposure to multi-cloud environments (Azure, GCP)
Experience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)
Professional Certifications (preferred):
Service Now Certified System Administrator
Service Now Certified Application Developer
Service Now Certified Technical Architect
Soft Skills:
Strong analytical and problem-solving skills.
Ability to work effectively with global, virtual teams
Effective communication and collaboration with cross-functional teams.
Ability to work in a fast-paced environment.","Rest Api, Servicenow, Itsm, Javascript, Agile, Automation"
Junior Data Architect,Terralogic Software Solutions,2-7 Years,,Bengaluru,Information Technology,"Junior Data ArchitectLocation :Bangalore
Overview: Role:
We are hiring a Junior Data Architect to support solution design and data architecture tasks, with exposure to Azure cloud services. This role will support senior architects and collaborate with BI and data
engineering teams to design scalable, secure, and cost-effective data solutions.
Minimum Requirements:
2+ years of experience in data engineering, BI, or solution design roles.
Working knowledge of Azure Data Services including Azure Data Factory, Azure SQL, and Power BI.
Understanding of data modeling concepts (star schema, snowflake, normalized/denormalized models).
Familiarity with data governance and security concepts.
Good communication and documentation skills.
Ability to assist with architecture diagrams, metadata documentation, and best practice
recommendations.
Nice to Have:
Exposure to Synapse Analytics, Azure Data Lake, or Azure Purview.
Knowledge of infrastructure components like VNets, Private Endpoints, and Managed Identity.
Willingness to grow into a senior architect or cloud data strategist role.","data architecture tasks, Azure Data Services, Azure Data Factory, Power Bi Desktop, Azure Sql"
Master Data Management Data Architect,Amgen Inc,7-11 Years,,Hyderabad,Biotechnology,"Job description
What you will do
Let's do this. Let's change the world. In this vital role you will be responsible for designing, building, maintaining, analyzing, and interpreting data deliver actionable insights that drive business decisions. This role involves working with large datasets, developing reports, supporting and driving data governance initiatives, and visualizing data to ensure data is accessible, reliable, and efficiently managed. The ideal candidate has deep technical skills and provides administration support for Master Data Management (MDM) and Data Quality platform, including solution architecture, inbound/outbound data integration (ETL), Data Quality (DQ), and maintenance/tuning of match rules.
Roles & Responsibilities:
Design, develop, and maintain data solutions for data generation, collection, and processing
Collaborate and communicate with MDM Developers, Data Architects, Product teams, Business SMEs, and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions
Identify and resolve complex data-related challenges
Adhere to standard processes for coding, testing, and designing reusable code/component
Participate in sprint planning meetings and provide estimations on technical implementation
As a SME, work with the team on MDM related product installation, configuration, customization and optimization
Responsible for the understanding, documentation, maintenance, and additional creation of master data related data-models (conceptual, logical, and physical) and database structures
Review technical model specifications and participate in data quality testing
Collaborate with Data Quality & Governance Analyst and Data Governance Organization to monitor and preserve the master data quality
Create and maintain system specific master data data-dictionaries for domains in scope
Architect MDM Solutions, including data modeling and data source integrations from proof-of-concept through development and delivery
Develop the architectural design for Master Data Management domain development, base object integration to other systems and general solutions as related to Master Data Management
Develop and deliver solutions individually or as part of a development team
Approves code reviews and technical work
Maintains compliance with change control, SDLC and development standards
Contribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions
Collaborate with multi-functional teams to understand data requirements and design solutions that meet business needs
Implement data security and privacy measures to protect sensitive data
Leverage cloud platforms (AWS preferred) to build scalable and efficient data solutions
What we expect of you
We are all different, yet we all use our unique contributions to serve patients.
Basic Qualifications:
Master's degree and 1 to 3 years of Computer Science, IT or related field experience OR
Bachelor's degree and 3 to 5 years of Computer Science, IT or related field experience OR
Diploma and 7 to 9 years of Computer Science, IT or related field experience.
Preferred Qualifications:
Expertise in architecting and designing Master Data Management (MDM) solutions.
Practical experience with AWS Cloud, Databricks, Apache Spark, workflow orchestration, and optimizing big data processing performance.
Familiarity with enterprise source systems and consumer systems for master and reference data, such as CRM, ERP, and Data Warehouse/Business Intelligence.
At least 2 to 3 years of experience as an MDM developer using Informatica MDM or Reltio MDM, along with strong proficiency in SQL.
Good-to-Have
Skills:
Experience with ETL tools such as Apache Spark, and various Python packages related to data processing, machine learning model development.
Good understanding of data modeling, data warehousing, and data integration concepts.
Experience with development using Python, React JS, cloud data platforms.
Certified Data Engineer / Data Analyst (preferred on Databricks or cloud environments).
Soft Skills:
Excellent critical-thinking and problem-solving skills
Good communication and collaboration skills
Demonstrated awareness of how to function in a team setting
Team-oriented, with a focus on achieving team goals
Strong presentation and public speaking skills.","MDM developer, Reltio MDM, Informatica Mdm, Apache Spark, Etl Tools, Python"
Data Architect - Precision Medicine Team,Amgen Inc,6-11 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities:
Architect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.
Leverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation
Support development planning by breaking down features into work that aligns with the architectural direction runway
Participate hands-on in pilots and proofs-of-concept for new patterns
Create robust documentation of architectural direction, patterns, and standards
Present and train engineers and cross-team collaborators on architecture strategy and patterns
Collaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.
Design robust data models, and processing layers, that support both analytical processing and operational reporting needs.
Develop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.
Ensure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.
Provide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.
Develop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.
Serve as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.
Collaborate with stakeholders to define data requirements, architecture specifications, and project goals.
Continuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.
Basic Qualifications and Experience:
Masters degree with 6 to 8 years of experience in data management and data solution architecture
Bachelors degree with 8 to 10 years of experience in in data management and data solution architecture
Diploma and 10 to 12 years of experience in in data management and data solution architecture
Functional Skills:
Must-Have Skills:
Minimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.
Minimum of 7 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.
Hands-on experience with Databricks, including data engineering, optimization, and analytics workloads.
Deep understanding of Power BI, including model design, DAX, and Power Query.
Proven experience designing and implementing data mastering solutions and data governance frameworks.
Expertise in cloud platforms (AWS), data lakes, and data warehouses.
Strong knowledge of ETL processes, data pipelines, and integration technologies.
Strong communication and collaboration skills to work with cross-functional teams and senior leadership.
Ability to assess business needs and design solutions that align with organizational goals.
Exceptional hands-on capabilities with data profiling, data transformation, data mastering
Success in mentoring and training team members
Good-to-Have Skills:
Experience in developing differentiated and deliverable solutions
Experience with human data, ideally human healthcare data
Familiarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management
Professional Certifications(please mention if the certification is preferred or mandatory for the role):
ITIL Foundation or other relevant certifications (preferred)
SAFe Agile Practitioner (6.0)
Microsoft Certified: Data Analyst Associate (Power BI) or related certification.
Databricks Certified Professional or similar certification.
Soft Skills:
Excellent analytical and troubleshooting skills
Deep intellectual curiosity
Highest degree of initiative and self-motivation
Strong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics
Confidence technical leader
Ability to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones
Ability to manage multiple priorities successfully
Team-oriented, with a focus on achieving team goals
Strong problem solving, analytical skills;
Ability to learn quickly and retain and synthesize complex information from diverse sources.","healthcare data, analytical, Laboratory, Troubleshooting, Data Warehouse, Etl Process"
Azure Data Architect,RARR Technologies,7-12 Years,,Mumbai,Software,"A leading software consultancy company in Mumbai is seeking a highly skilled Azure Data Architect. The ideal candidate should have extensive experience in data architecture, with a strong focus on transforming legacy systems to Azure Graph DB. The role requires excellent communication skills, teamwork, and the ability to mentor and support other team members.
Responsibilities:
Data Architecture Design:Design and implement robust data architecture solutions on Azure, ensuring scalability, performance, and security.
Legacy System Transformation:Lead the transformation of legacy systems to modern Azure Graph DB, ensuring minimal disruption and maximum efficiency.
Data Migration:Plan and execute data migration strategies, ensuring data integrity and consistency throughout the process & remapping source to target model.
Azure Data Services:Utilize Azure data services such as Azure Cosmos DB (Graph API) to build and manage graph database solutions.
Data Modeling:Develop and maintain graph data models to support business requirements and optimize data storage and retrieval.
Collaboration:Work closely with stakeholders, including business analysts, developers, and operations teams, to ensure data solutions meet business needs.
Framework Utilization:Prioritize the use of out-of-the-box framework features over custom solutions to enhance efficiency and maintainability.
Required Qualifications:
Minimum 7 years of experience in data architecture and database management.
Specialist in Azure data services with proven experience in transforming legacy systems to Azure Graph DB.
Extensive experience with data migration strategies and tools.
Proficient in data modeling and database design, specifically for graph databases.
Experience with Azure Cosmos DB (Graph API) and other graph database technologies.
Excellent communication skills and a strong team player.
Proven experience in mentoring and leading data architecture teams.
Technical Skills:
Azure Cosmos DB (Graph API)
Data Modeling for Graph Databases
Data Migration Tools and Strategies
Scripting (PowerShell, Bash, Python)
Good to Have Skills:
Knowledge of data governance and security best practices
Knowledge of Virtuoso Graph Database & SPARQL Protocol and RDF Query Language.","Powershell And Bash, Graph Db, Graph Database, Azure Cosmos DB, Data Migration, Azure, Python"
Data Architect - R&D Data Catalyst Team,Amgen Inc,4-6 Years,,Hyderabad,Pharmaceutical,"Roles & Responsibilities
Architect scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.
Leverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation
Support development planning by break ing down features into work that aligns with the architectural direction runway
Participate hands-on in pilots and proofs-of-concept for new pattern
Create robust documentation of architectural direction, patterns, and standards
Present and train engineers and cross-team collaborators on architecture strategy and patterns
Collaborate with data engineers to build and optimize ETL pipelines, ensuring efficient data ingestion and processing from multiple sources.
Design robust data models , and processing layers, that support both analytical processing and operational reporting needs.
Develop and implement best practices for data governance, security, and compliance within Databricks and Power BI environments.
ensure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.
Provide thought leadership and strategic guidance on data architecture, advanced analytics, and data mastering best practices.
Develop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.
Serve as a subject matter expert on Power BI and Databricks, providing technical leadership and mentoring to other teams.
Collaborate with stakeholders to define data requirements, architecture specifications, and project goals.
Continuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions
Basic Qualifications and Experience
Master's degree with 4 to 6 years of experience in data management and data architecture O
Bachelor's degree with 6 to 8 years of experience in data management and data architectur
Functional
Skills:
Must-Have Skills
Minimum of 3 years of hands-on experience with BI solutions (Preferably Power BI or Business Objects) including report development, dashboard creation, and optimization.
Minimum of 7 years of hands-on experience building change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management
Hands-on experience with Databricks, including data engineering, optimization, and analytics workloads.
Deep understanding of Power BI, including model design , DAX, and Power Query.
Proven experience designing and implementing data mastering solutions and data governance frameworks.
Expertise in cloud platforms ( AWS ), data lakes, and data warehouses.
Strong knowledge of ETL processes, data pipelines, and integration technologies
Strong communication and collaboration skills to work with cross-functional teams and senior leadership.
Ability to assess business needs and design solutions that align with organizational goals.
Exceptional h ands - on capabilities with data profiling, data transformation, data mastering
Success in mentoring and training team members
Good-to-Have
Skills:
Experience in developing differentiated and deliverable solutions
Experience with human data, ideally human healthcare data
Familiarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management
Professional Certifications (please mention if the certification is preferred or mandatory for the role)
ITIL Foundation or other relevant certifications (preferred)
SAFe Agile Practitioner (6.0
Microsoft CertifiedData Analyst Associate (Power BI) or related certification.
Databricks Certified Professional or similar certification.
Soft
Skills:
Excellent analytical and troubleshooting skills
Deep intellectual curiosity
High est degree of initiative and self-motivation
Strong verbal and written communication skills , including presentation to varied audiences of complex technical/business topics
Confidence technical leader
Ability to work effectively with global, virtual teams , specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones
ability to manage multiple priorities successfully
Team-oriented, with a focus on achieving team goals
Strong problem solving, analytical skills;
Ability to learn quickly and retain and synthesize complex information from diverse sources","Optimization, Etl Process, Dax, aws, Power Query, Powerbi"
Master Data Management Data Architect,Amgen Inc,3-6 Years,,Hyderabad,Pharmaceutical,"Deliver outstanding self-service and automation experiences for our global workforce
Create ServiceNow catalog items, workflows, and cross-platform API integrations
Create and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.
Create and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.
Create and maintain data integrations between ServiceNow and other systems
Develop system integrations and process automation
Participate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives
Collaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions
Design, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner
Participate in problem analysis, code review, and system design
Remain current on new technology and apply innovation to improve functionality
Collaborate closely with stakeholders and team members to configure, improve and maintain current applications
Work directly with users to resolve support issues within product team responsibilities
Monitor health, performance and usage of developed solutions
Basic Qualifications:
Master's degree and 1 to 3 years of experience in computer science, IT, or related field OR
Bachelor's degree and 3 to 5 years of experience in computer science, IT, or related field OR
Diploma and 7 to 9 years of experience in computer science, IT, or related fiel
Required Skills & Qualifications:
6+ years of deep hands-on experiencewith ServiceNow administration and development in two or more productsITSM, ITBM, ITOM, GRC, HRSD, or Security Operations
ServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;
Strong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform
Experience creating and managing Scoped Applications
Workflow automation and integration development using REST, SOAP, or MID servers
Scripting skills in Python, Bash, or other programming languages
Working in an Agile (SAFe, Scrum, and Kanban) environment
Preferred Qualifications:
Good-to-Have
Skills:
Experience with other configuration management tools (e.g., Puppet, Chef).
Experience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)
Experience with Terraform & CloudFormation for AWS infrastructure automation
Knowledge of AWS Lambda and event-driven architectures.
Exposure to multi-cloud environments (Azure, GCP)
Experience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)
Professional Certifications (preferred):
Service Now Certified System Administrator
Service Now Certified Application Developer
Service Now Certified Technical Architect
Soft Skills:
Strong analytical and problem-solving skills.
Ability to work effectively with global, virtual teams
Effective communication and collaboration with cross-functional teams.
Ability to work in a fast-paced environment.","Rest Api, Servicenow, Itsm, Javascript, Agile, Automation"
AWS Big data Architect,Birlasoft Limited,15-20 Years,,Bengaluru,Software Engineering,"Key Responsibilities
Design and architect end to end solutions on AWS and create the HLD and LLD documents
Experience in real-time Data Ingestion and Processing
Hands-on experience in AWS S3, Kafka, Glue, PySpark, Python, Redshift, AWS Cloud best practices and optimization
Experience with integration of different data sources with Data Lake is required
Experience in creating data lakes for Reporting, AI and Machine Learning
Experience of data modelling and data architecture concepts
Good in Creating Technical Specifications and Data Flow document
To be able to clearly articulate pros and cons of various technologies and platforms
Experience in create the Technical Specification Design.
Skills Required
15 years of IT experience with 4+ years of AWS Big Data Services Architecture experience
Client facing experience.
In-depth knowledge of domain Industry and business environment
Analytical and problem-solving capabilities","AWS Big Data Services Architecture, AWS Big Data, AWS"
Sr IT Architect- Data Architect IICS,Honeywell,8-15 Years,,Bengaluru,Consumer Electronics,"Role Responsibilities:
Design and implement data architecture integrating Salesforce with other platforms.
Develop and maintain ETL processes using Informatica tools.
Ensure data integrity, security, and compliance with governance standards.
Collaborate with stakeholders to define and meet business data requirements.
Job Requirements:
Graduate with 7+ years of experience in data architecture and integration.
Hands-on expertise in Informatica and Salesforce APIs.
Strong data modeling and cloud ETL knowledge.
Certifications in Informatica and Salesforce preferred.","Salesforce Integration, Data Architecture, Informatica, Data Governance, Etl Development"
Data Architect,Motivity Labs Inc,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Roles and Responsibilities
10+ years of relevant work experience, including previous experience leading Data related projects in the field of Reporting and Analytics.
Design, build & maintain scalable data lake and data warehouse in cloud (GCP)
Expertise in gathering business requirements, analysing business needs, defining the BI/DW architecture to support and help deliver technical solutions to complex business and technical requirements Creating solution prototype and participating in technology selection. Perform POC and technical presentations Architect, develop and test scalable data warehouses and data pipelines architecture in Cloud Technologies (GCP)
Experience in SQL and No SQL DBMS like MS SQL Server, MySQL, PostgreSQL, DynamoDB, Cassandra, MongoDB.
Design and develop scalable ETL processes, including error handling. Expert in Query and program languages MS SQL Server, T-SQL, PostgreSQL, MY SQL, Python, R.
Preparing data structures for advanced analytics and self-service reporting using MS SQL, SSIS, SSRS
Write scripts for stored procedures, database snapshots backups and data archiving. Experience with any of these cloud-based technologies: o PowerBI/Tableau, Azure Data Factory, Azure Synapse, Azure Data Lake o AWS RedShift, Glue, Athena, AWS Quicksight o Google Cloud Platform Good to have:
Agile development environment pairing DevOps with CI/CD pipelines
AI/ML background Interview Rounds: 2 Technical Rounds followed by HR Round.","MS SQL SSIS, R, Glue, No SQL DBMS, Athena, AWS Quicksight, T-sql, Ssrs, Ms Sql Server, Cassandra, PostgreSQL, Dynamodb, Tableau, Sql, Azure Synapse, Azure Data Factory, Gcp, Powerbi, MySQL, Azure Data Lake, Aws Redshift, MongoDB, Python"
Data Architect,OneMagnify,7-9 Years,,"Chennai, India",Login to check your skill match score,"The Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.
Team: Data Platform, Application & Data
Reports to: Engineering Manager, Data Platform, Application & Data
Minimum Education: Bachelor's Degree or Equivalent Experience
Recommended Tenure: 7+ years in data platform engineering or architecture roles, including at least 3 years in a hands-on architecture role
Role Summary:
The Data Architect is responsible for designing and managing robust, scalable data solutions that ensure data accuracy, accessibility, and security to support both internal and client-facing needs. As part of the Data Platform team, this role collaborates closely with Solution Architects to align data architecture with broader application design, and partners with data engineers to implement and optimize solutions that power analytics, integrations, and real-time processing.
Architecture & Design:
Translate business and technical requirements into data models and architecture specifications
Design and document data architecture artifacts, including logical/physical data models, data flow diagrams, and integration patterns
Align data models with application architecture and system interaction patterns in partnership with Solution Architects
Establish and maintain design patterns for relational, NoSQL, and streaming-based data solutions
Solution Delivery & Support:
Serve as a hands-on architecture lead during project discovery, planning, and delivery phases
Support data engineers in implementing data architecture that aligns with platform and project requirements
Validate implementation through design reviews and provide guidance throughout the development lifecycle
Contribute to platform evolution by defining and enforcing scalable, reusable architecture practices
Data Governance & Quality:
Define and uphold best practices for data modeling, data security, lineage tracking, and performance tuning
Promote consistency in metadata, naming conventions, and data access standards across environments
Support data privacy, classification, and auditability across integrated systems
Cross-Functional Collaboration:
Work closely with product managers, engineering leads, DevOps, and analytics teams to deliver scalable and future-proof data solutions
Collaborate with Solution Architects to ensure integrated delivery across application and data domains
Act as a subject matter expert on data structure, semantics, and lifecycle across key business domains
Key Competencies:
7+ years of experience in data engineering or data architecture roles, including 3+ years in a dedicated architecture capacity
Proven experience in cloud platformspreferably GCP and/or Azurewith strong familiarity with native data services
Deep understanding of data storage paradigms including relational, NoSQL, and object storage
Hands-on experience with databases such as Oracle and Postgres; Python proficiency preferred
Familiarity with modern DevOps practices including infrastructure-as-code and CI/CD for data pipelines
Strong communication skills with the ability to lead through influence across technical and non-technical audiences
Self-starter with excellent organization and prioritization skills across multiple initiatives","Postgres, Oracle, Python, Data Services"
Data Architect,Blackbaud,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"About The Role
We are seeking a highly skilled and experienced Data Architect to join our team. The ideal candidate will have at least 12 years of experience in software & data engineering and analytics and a proven track record of designing and implementing complex data solutions. You will be expected to design, create, deploy, and manage Blackbaud's data architecture. This role has considerable technical influence within the Data Platform, Data Engineering teams, and the Data Intelligence Center of Excellence atBlackbaud. Thisindividual acts as an evangelist for proper data strategy with other teams at Blackbaud and assists with the technical direction, specifically with data, of other projects.
What You'll Be Doing
Develop and direct the strategy for all aspects of Blackbaud's Data and Analytics platforms, products and services
Set, communicate and facilitate technical directionmore broadly for the AI Center of Excellence and collaboratively beyond the Center of Excellence
Design and develop breakthrough products, services or technological advancements in the Data Intelligence space that expand our business
Work alongside product management to craft technical solutions to solve customer business problems.
Own the technical data governance practices and ensures data sovereignty, privacy, security and regulatory compliance.
Continuously challenging the status quo of how things have been done in the past.
Build data access strategy to securely democratize data and enable research, modelling, machine learning and artificial intelligence work.
Help define the tools and pipeline patterns our engineers and data engineers use to transform data and support our analytics practice
Work in a cross-functional team to translate business needs into data architecture solutions.
Ensure data solutions are built for performance, scalability, and reliability.
Mentor junior data architects and team members.
Keep current on technology: distributed computing, big data concepts and architecture.
Promote internally how data within Blackbaud can help change the world.
What We Want You To Have
10+ years of experience in data and advanced analytics
At least 8 years of experience working on data technologies in Azure/AWS
Experience building modern products and infrastructure
Experience working with .Net/Java and Microservice Architecture
Expertise in SQL and Python
Expertise in SQL Server, Azure Data Services, and other Microsoft data technologies.
Expertise in Databricks, Microsoft Fabric
Strong understanding of data modeling, data warehousing, data lakes, data mesh and data products.
Experience with machine learning
Excellent communication and leadership skills.
Able to work flexible hours as required by business priorities
Ability to deliver software that meets consistent standards of quality, security and operability.
Stay up to date on everything Blackbaud, follow us on Linkedin, X, Instagram, Facebook and YouTube
Blackbaud is a digital-first company which embraces a flexible remote or hybrid work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!
Blackbaud is proud to be an equal opportunity employer and is committed to maintaining an inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.
R0012317","Data Lakes, Analytics, Data Mesh, Microsoft Fabric, Azure Data Services, Java, data engineering, Machine Learning, Data Modeling, .NET, SQL Server, Microservice Architecture, Sql, Databricks, Data Warehousing, Azure, Python, AWS"
Data Architect,Evoke HR Solutions Pvt. Ltd.,10-13 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title : GCP Data architect
Location : Pune,Benguluru (Local)
Exp : 10 to 13 Years
Minimum of 5 years of experience in a Data Architect role, supporting warehouse and Cloud data platforms/environments.
Experience with common GCP services such as BigQuery, Dataflow, GCS, Service Accounts, cloud function
Extremely strong in BigQuery design, development
Extensive knowledge and implementation experience in data management, governance, and security frameworks.
Proven experience in creating high-level and detailed data architecture and design documentation.
Strong aptitude for business analysis to understand domain data requirements.
Proficiency in Data Modelling using any Modelling tool for Conceptual, Logical, and Physical models is preferred
Hands-on experience with architecting end-to-end data solutions for both batch and real-time designs.
Ability to collaborate effectively with clients, developers, and architecture teams to implement enterprise-level data solutions.
Familiarity with Data Fabric and Data Mesh architecture is a plus.
Excellent verbal and written communication skills.","Service Accounts, GCS, Data Fabric, Data Mesh, BigQuery, Data Modelling, Gcp, Data Management, DataFlow"
Data Architect,BNP Paribas,7-9 Years,,"Mumbai, India",Login to check your skill match score,"About BNP Paribas India Solutions
Established in 2005, BNP Paribas India Solutions is a wholly owned subsidiary of BNP Paribas SA, European Union's leading bank with an international reach. With delivery centers located in Bengaluru, Chennai and Mumbai, we are a 24x7 global delivery center. India Solutions services three business lines: Corporate and Institutional Banking, Investment Solutions and Retail Banking for BNP Paribas across the Group. Driving innovation and growth, we are harnessing the potential of over 10000 employees, to provide support and develop best-in-class solutions.
About BNP Paribas Group
BNP Paribas is the European Union's leading bank and key player in international banking. It operates in 65 countries and has nearly 185,000 employees, including more than 145,000 in Europe. The Group has key positions in its three main fields of activity: Commercial, Personal Banking & Services for the Group's commercial & personal banking and several specialised businesses including BNP Paribas Personal Finance and Arval; Investment & Protection Services for savings, investment, and protection solutions; and Corporate & Institutional Banking, focused on corporate and institutional clients. Based on its strong diversified and integrated model, the Group helps all its clients (individuals, community associations, entrepreneurs, SMEs, corporates and institutional clients) to realize their projects through solutions spanning financing, investment, savings and protection insurance. In Europe, BNP Paribas has four domestic markets: Belgium, France, Italy, and Luxembourg. The Group is rolling out its integrated commercial & personal banking model across several Mediterranean countries, Turkey, and Eastern Europe. As a key player in international banking, the Group has leading platforms and business lines in Europe, a strong presence in the Americas as well as a solid and fast-growing business in Asia-Pacific. BNP Paribas has implemented a Corporate Social Responsibility approach in all its activities, enabling it to contribute to the construction of a sustainable future, while ensuring the Group's performance and stability
Commitment to Diversity and Inclusion
At BNP Paribas, we passionately embrace diversity and are committed to fostering an inclusive workplace where all employees are valued, respected and can bring their authentic selves to work. We prohibit Discrimination and Harassment of any kind and our policies promote equal employment opportunity for all employees and applicants, irrespective of, but not limited to their gender, gender identity, sex, sexual orientation, ethnicity, race, colour, national origin, age, religion, social status, mental or physical disabilities, veteran status etc. As a global Bank, we truly believe that inclusion and diversity of our teams is key to our success in serving our clients and the communities we operate in.
Position Purpose
The Data Architect is to support the work for ensuring that systems are designed, upgraded, managed, de-commissioned and archived in compliance with data policy across the full data life cycle. This includes complying with the data strategy and undertaking the design of data models and supporting the management of metadata. The Data Architect mission will integrate a focus on GDPR law, with the contribution to the privacy impact assessment and Record of Process & Activities relating to personal Data.
The scope is CIB EMEA and CIB ASIA
Responsibilities
Direct Responsibilities
Engage with key business stakeholders to assist with establishing fundamental data governance processes
Define key data quality metrics and indicators and facilitate the development and implementation of supporting standards
Help to identify and deploy enterprise data best practices such as data scoping, metadata standardization, data lineage, data deduplication, mapping and transformation and business validation
Structures the information in the Information System (any data modelling tool like Abacus), i.e. the way information is grouped, as well as the navigation methods and the terminology used within the Information Systems of the entity, as defined by the lead data architects.
Creates and manages data models (Business Flows of Personal Data with process involved) in all their forms, including conceptual models, functional database designs, message models and others in compliance with the data framework policy
Allows people to step logically through the Information System (be able to train them to use tools like Abacus)
Contribute and enrich the Data Architecture framework through the material collected during analysis, projects and IT validations Update all records in Abacus collected from stakeholder interviews/ meetings.
Skill Area
Expected
Communicating between the technical and the non-technical
Is able to communicate effectively across organisational, technical and political boundaries, understanding the context. Makes complex and technical information and language simple and accessible for non- technical audiences. Is able to advocate and communicate what a team does to create trust and authenticity, and can respond to challenge.
Able to effectively translate and accurately communicate across technical and non- technical stakeholders as well as facilitating discussions within a multidisciplinary team, with potentially difficult dynamics.
Data Modelling (Business Flows of Data in Abacus)
Produces data models and understands where to use different types of data models. Understands different tools and is able to compare between different data models.
Able to reverse engineer a data model from a live system. Understands industry recognized data modelling patterns and standards.
Understands the concepts and principles of data modelling and is able to produce, maintain and update relevant data models for specific business needs.
Data Standards (Rules defined to manage/ maintain Data)
Develops and sets data standards for an organisation.
Communicates the business benefit of data standards, championing and governing those standards across the organisation.
Develops data standards for a specific component. Analyses where data standards have been applied or breached and undertakes an impact analysis of that breach.
Metadata Management
Understands a variety of metadata management tools. Designs and maintains the appropriate metadata repositories to enable the organization to understand their data assets.
Works with metadata repositories to complete and Maintains it to ensure information remains accurate and up to date.
The objective is to manage own learning and contribute to domain knowledge building
Turning business problems into data design
Works with business and technology stakeholders to translate business problems into data designs. Creates optimal designs through iterative processes, aligning user needs with organisational objectives and system requirements.
Designs data architecture by dealing with specific business problems and aligning it to enterprise-wide standards and principles. Works within the context of well understood architecture and identifies appropriate patterns.
Contributing Responsibilities
It is expected that the data architect applies knowledge and experience of the capability, including tools and technique and adopts those that are more appropriate for the environment.
The Data Architect Needs To Have The Knowledge Of
The Functional & Application Architecture, Enterprise Architecture and Architecture rules and principles
The activities Global Market and/or Global Banking
Market meta-models, taxonomies and ontologies (such as FpML, CDM, ISO2022)
Skill Area
Expected
Data Communication
Uses the most appropriate medium to visualise data to tell compelling and actionable stories relevant for business goals.
Presents, communicates and disseminates data appropriately and with high impact.
Able to create basic visuals and presentations.
Data Governance
Understands data governance and how it works in relation to other organisational governance structures. Participates in or delivers the assurance of a service.
Understands what data governance is required and contribute to these data governance.
Data Innovation
Recognises and exploits business opportunities to ensure more efficient and effective performance of organisations. Explores new ways of conducting business and organisational processes
Aware of opportunities for innovation with new tools and uses of data
Technical & Behavioral Competencies
Able to effectively translate and accurately communicate across technical and non- technical stakeholders as well as facilitating discussions within a multidisciplinary team, with potentially difficult dynamics.
Able to create basic visuals and presentations.
Experience in working with Enterprise Tools (like Abacus, informatica, big data, collibra, etc)
Experience in working with BI Tools (Like Power BI)
Good understanding of Excel (formulas and Functions)
Specific Qualifications (if Required)
Preferred: BE/ BTech, BSc-IT, BSc-Comp, MSc-IT, MSc Comp, MCA
Skills Referential
Behavioural Skills: (Please select up to 4 skills)
Communication skills - oral & written
Ability to collaborate / Teamwork
Ability to deliver / Results driven
Creativity & Innovation / Problem solving
Transversal Skills: (Please select up to 5 skills)
Analytical Ability
Ability to understand, explain and support change
Ability to develop and adapt a process
Ability to anticipate business / strategic evolution
Choose an item.
Education Level
Bachelor Degree or equivalent
Experience Level
At least 7 years
Other/Specific Qualifications (if Required)
Experience in GDPR (General Data Protection Regulation) or in Privacy by Design would be preferred
DAMA Certified","Data Innovation, Abacus, data standards, Data Communication, Big Data, Power Bi, Excel, Metadata Management, Informatica, Data Governance, Collibra, Data Modelling"
Data Architect,Siemens Healthineers,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Architect, you are required to:
Design & develop technical solutions which combine disparate information to create meaningful insights for business, using Big-data architectures
Build and analyze large, structured and unstructured databases based on scalable cloud infrastructures
Develop prototypes and proof of concepts using multiple data-sources and big-data technologies
Process, manage, extract and cleanse data to apply Data Analytics in a meaningful way
Design and develop scalable end-to-end data pipelines for batch and stream processing
Regularly scan the Data Analytics landscape to stay up to date with latest technologies, techniques, tools and methods in this field
Stay curious and enthusiastic about using related technologies to solve problems and enthuse others to see the benefit in business domain
Qualification:
Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Engineering / Analytics is desirable.
Experience level:
Minimum 8 years in software development with at least 2 - 3 years hands-on experience in the area of Big-data / Data Engineering.
Desired Knowledge & Experience:
Data Engineer - Big Data Developer
Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming
Knowing Spark internals: Catalyst/Tungsten/Photon
Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader
IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot
Test: pytest, Great Expectations
CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing
Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction
Languages: Python/Functional Programming (FP)
SQL: TSQL/Spark SQL/HiveQL
Storage: Data Lake and Big Data Storage Design
Additionally it is helpful to know basics of:
Data Pipelines: ADF/Synapse Pipelines/Oozie/Airflow
Languages: Scala, Java
NoSQL: Cosmos, Mongo, Cassandra
Cubes: SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model
SQL Server: TSQL, Stored Procedures
Hadoop: HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka
Data Catalog: Azure Purview, Apache Atlas, Informatica
Big Data Architect
Expert: in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer
Mentor: mentors/educates Developers in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer
Architecture Styles: Lakehouse, Lambda, Kappa, Delta, Data Lake, Data Mesh, Data Fabric, Data Warehouses (e.g. Data Vault)
Application Architecture: Microservices, NoSql, Kubernetes, Cloud-native
Experience: Many years of experience with all kinds of technology in the evolution of data platforms (Data Warehouse -> Hadoop -> Big Data -> Cloud -> Data Mesh)
Certification: Architect certification (e.g. Siemens Certified Software Architect or iSAQB CPSA)
Required Soft-skills & Other Capabilities:
Excellent communication skills, in order to explain your work to people who don't understand the mechanics behind data analysis
Great attention to detail and the ability to solve complex business problems
Drive and the resilience to try new ideas, if the first ones don't work
Good planning and organizational skills
Collaborative approach to sharing ideas and finding solutions
Ability to work independently and also in a global team environment.","Big Data Storage Design, Nosql, Hadoop, Spark, Data Lake, Databricks, Sql, Python, Azure DevOps"
Celonis Data Architect,"Sky Systems, Inc. (SkySys)",Fresher,,India,Login to check your skill match score,"Job Title: Celonis Data Architect
Job Type & Location: India - Remote
Job Description:
We are seeking a meticulous and experienced Data Architect to design, develop, and implement data architectures and solutions. The ideal candidate will have a strong understanding of database management systems and data modelling techniques, as well as experience in data warehousing, ETL processes, and data governance.
Job Responsibilities:
Collaborate with business stakeholders and technical teams to understand data requirements and develop data architecture solutions.
Design, develop, and maintain data models, data dictionaries, and data flow diagrams, and implement data architecture solutions
Create and implement data warehouse strategies and structures to support reporting and analytics.
Develop and maintain data integration processes, including Extract, Transform, Load (ETL) processes.
Ensure data quality and integrity by establishing and enforcing data governance practices and data stewardship responsibilities.
Prepare and deliver presentations to business stakeholders and technical teams, explaining data architecture concepts and solutions.
Collaborate with software developers and DBAs to optimize database performance and ensure scalability and reliability.
Stay updated with industry trends and emerging technologies related to data architecture and data management.
Define and enforce data security and privacy standards and policies.
Provide guidance and mentoring to junior data professionals.
Job Requirements:
Bachelor's degree in computer science, Information Technology, or a related field.
Proven experience as a Data Architect or in a similar role.
Strong knowledge of data modelling principles and techniques.
Proficiency in database management systems such as Oracle, SQL Server, or MySQL.
Experience with data warehousing concepts and tools such as ETL processes, star schemas, and dimensional modeling.
Familiarity with data governance and data stewardship practices.
Excellent analytical and problem-solving skills.
Strong communication and presentation skills.
Ability to work well in a team environment and collaborate with stakeholders from various departments.
Familiarity with cloud-based data storage and data management platforms is a plus.
As a Data Architect, you will play a critical role in designing and implementing data architectures that support the organization's data needs. Your expertise in data modelling, data warehousing, and ETL processes will contribute to the successful management and utilization of data within the organization.
Support in technical feasibility of various source systems with Celonis EMS. Understand details of the proposed integration pattern to help in effort & timeline estimation.
Ensure that detailed designs match high level designs and are traceable to requirements in functional specification
Ensure designs produced adhere to architectural roadmap and support the development, execution and operations of solutions
Ensure that solutions meet requirements outlined in the design documentation. Ensure the overall user experience is taken into account when designing and deploying new solutions and services. Ensure that developed solutions are peer reviewed, formally documented and signed off by business
Ensure that all work is delivered to agreed time, cost and quality constraints. Initiate solution testing to ensure they meet quality standards
Establish standardized design and development processes to enable cost effective delivery. Authorize and conduct service handover and lead the go-live authorization discussions with the other work streams
Ensure that all release and deployment packages can be tracked, installed, tested, verified and backed out (if required). Ensure that release delivers the expected outcomes and value for the customers
Take accountability to ensure adherence with Security and Compliance policies and procedures within Solution Delivery scope.","ETL Processes, data stewardship, Cloud-based Data Storage, Data Modelling, MySQL, Database Management Systems, Data Architecture, SQL Server, Data Warehousing, Data Governance, Oracle"
Data Engineer / Data Architect,Soul AI,Fresher,,India,Login to check your skill match score,"About Us:
Soul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.
Role Overview:
We are seeking a Data Engineer / Data Architect who will be responsible for designing, building, and maintaining scalable data infrastructure and systems for a client. You'll play a key role in enabling efficient data flow, storage, transformation, and access across our organization or client ecosystems.
Whether you're just beginning or already an expert, we value strong technical skills, curiosity, and the ability to translate complex requirements into reliable data pipelines.
Responsibilities:
Design and implement scalable, robust, and secure data pipelines.
Build ETL/ELT frameworks to collect, clean, and transform structured and unstructured data.
Collaborate with data scientists, analysts, and backend engineers to enable seamless data access and model integration.
Maintain data integrity, schema design, lineage, and quality monitoring.
Optimize performance and ensure reliability of data workflows in production environments.
Design and manage data warehousing and lakehouse architecture.
Set up and manage infrastructure using IaC (Infrastructure as Code) when applicable.
Required Skills:
Strong programming skills in Python, SQL, and Shell scripting.
Hands-on experience with ETL tools and orchestration frameworks (e.g., Airflow, Luigi, dbt).
Proficiency in relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Redis).
Experience with big data technologies: Apache Spark, Kafka, Hive, Hadoop, etc.
Deep understanding of data modeling, schema design, and data warehousing concepts.
Proficient with cloud platforms (AWS/GCP/Azure) and services like Redshift, BigQuery, S3, Dataflow, or Databricks.
Knowledge of DevOps and CI/CD tools relevant to data infrastructure.
Nice to Have:
Experience working in real-time streaming environments.
Familiarity with containerization and Kubernetes.
Exposure to MLOps and collaboration with ML teams.
Experience with security protocols, data governance, and compliance frameworks.
Educational Qualifications:
Bachelor's or Master's in Computer Science, Data Engineering, Information Systems, or a related technical field.","Airflow, Luigi, CI CD tools, dbt, S3, PostgreSQL, Data Warehousing, Kafka, Schema Design, Data Modeling, MySQL, Shell scripting, Etl Tools, Python, AWS, BigQuery, Hadoop, Apache Spark, Redshift, Redis, Sql, Devops, Hive, Gcp, Databricks, MongoDB, DataFlow, Azure"
Data Architect,TMF Group,8-10 Years,,"Pune, India",Login to check your skill match score,"We never ask for payment as part of our selection process, and we always contact candidates via our corporate accounts and platforms. If you are approached for payment, this is likely to be fraudulent. Please check to see whether the role you are interested in is posted on our career website.
Experience: 8+ years in Data Architecture, with at least 3+ years in Azure Data solutions.
Technical Expertise:
Strong knowledge of Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL, Azure Data Lake, and Cosmos DB.
Proficiency in SQL, Python, Spark, and PowerShell.
Experience in data modeling, ETL/ELT, and data warehousing.
Understanding of DevOps, CI/CD, and Infrastructure-as-Code (Terraform, ARM templates).
Strong problem-solving and analytical skills.
Excellent communication and stakeholder management.
Exposure to hybrid cloud environments (Azure & on-premises)
Knowledge of data mesh and data fabric architectures.","data fabric architectures, data mesh, Infrastructure-as-Code, PowerShell, Data Warehousing, Azure Databricks, Data Modeling, Sql, Azure Sql, ELT, Devops, Azure Data Factory, ARM templates, Terraform, Azure Synapse Analytics, Azure Data Lake, Spark, Cosmos DB, Python, Etl"
AWS Data Architect,DataPMI Inc,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Data Architect AWS Cloud
Location: Bangalore
Client: Deloitte India
Experience: 8+ years
Job Summary:
We are looking for a highly skilled Data Architect with a strong background in AWS cloud to design and oversee enterprise data architectures. The ideal candidate will be responsible for establishing data standards, modeling strategies, and optimizing data flows and repositories on AWS cloud infrastructure.
Key Responsibilities:
Design and implement end-to-end data architecture solutions on AWS.
Define enterprise data models and frameworks aligned with business needs.
Architect scalable data pipelines and data lakes using AWS services (e.g., S3, Glue, Redshift, Lake Formation).
Lead data governance, metadata management, and data quality initiatives.
Collaborate with business and technology stakeholders to capture requirements and translate them into architectural blueprints.
Evaluate and recommend tools and platforms to support data strategy.
Provide technical leadership and mentoring to data engineering teams.
Required Skills:
Strong experience with AWS data ecosystem: S3, Redshift, Glue, Lambda, Athena, EMR, Lake Formation.
Proficiency in data modeling techniques (conceptual, logical, physical) for OLTP and OLAP.
Experience with ERwin, PowerDesigner, or similar modeling tools.
Deep understanding of data governance, security, and compliance on AWS.
Ability to communicate complex technical concepts to non-technical stakeholders.
Preferred Qualifications:
AWS Certified Solutions Architect or equivalent certification.
Experience with real-time/streaming data (e.g., Kinesis, Kafka).
Background in BI, analytics, and data warehousing","powerdesigner, data modeling techniques, Compliance, Lake Formation, Security, Glue, Athena, S3, Erwin, Emr, Redshift, Lambda, Data Governance, AWS"
"Lead IT Architect, Data Architect, Platinion",Boston Consulting Group (BCG),7-9 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
About BCG Platinion
BCG Platinion's presence spans across the globe, with offices in Asia, Europe, and South and North America. We achieve digital excellence for clients with sustained solutions to the most complex and time-sensitive challenge. We guide clients into the future to push the status quo, overcome tech limitations, and enable our clients to go further in their digital journeys than what has ever been possible in the past. At BCG Platinion, we deliver business value through the innovative use of technology at a rapid pace. We roll up our sleeves to transform business, revolutionize approaches, satisfy customers, and change the game through Architecture, Cybersecurity, Digital Transformation, Enterprise Application and Risk functions. We balance vision with a pragmatic path to change transforming strategies into leading-edge tech platforms, at scale.
What You'll Do
Design and implement a data architecture that supports the organization's business goals and objectives
Develop data models, define data standards and guidelines, and establish processes for data integration, migration, and management
Create and maintain data dictionaries, which are a comprehensive set of data definitions and metadata that provide context and understanding of the organization's data assets
Ensure that the data is accurate, consistent, and reliable across the organization. This includes establishing data quality metrics and monitoring data quality on an ongoing basis
Work closely with other IT professionals, including database administrators, data analysts, and developers, to ensure that the organization's data architecture is integrated and aligned with
other IT systems and applications
Stay up to date with new technologies and trends in data management and architecture and evaluate their potential impact on the organization's data architecture
Communicate with stakeholders across the organization to understand their data needs and ensure that the organization's data architecture is aligned with the organization's strategic
goals and objectives
What You'll Bring
A BTech / MTech degree in Computer Science or a related field
At least 7+ years of experience in working on data architecture
Expertise in data modeling and design, including conceptual, logical, and physical data models,
and must be able to translate business requirements into data models
Proficient in a variety of data management technologies, including relational databases,
NoSQL databases, data warehouses, and data lakes
Expertise in ETL processes, including data extraction, transformation, and loading, and must
be able to design and implement data integration processes
Experience with data analysis and reporting tools and techniques and must be able to design
and implement data analysis and reporting processes
Familiar with industry-standard data architecture frameworks, such as TOGAF or Zachman,
and must be able to apply them to the organization's data architecture
Familiar with cloud computing technologies, including public and private clouds, and must be
able to design and implement data architectures that leverage cloud computing
Certificates in Database Management will be preferred
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","NoSQL databases, data lakes, Relational Databases, zachman, ETL processes, data management technologies, cloud computing technologies, data analysis and reporting tools, Data Architecture, Data Modeling, Togaf, data warehouses"
GCP Data Architect,techolution,5-7 Years,,India,Login to check your skill match score,"Techolution is seeking an experienced GCP Data Architect who brings deep technical expertise in cloud-native data architecture, a strong grasp of Google Cloud Platform (GCP) services, and the ability to deliver enterprise-grade solutions. The ideal candidate will lead the end-to-end design and implementation of data pipelines and platforms, working closely with cross-functional teams to drive innovation and ensure scalable, secure, and efficient data infrastructure.
Designation: GCP Data Architect
Location: Remote
Employment Type: Full-time
Work Timings: 2 PM to 11 PM IST
Job Description:
Architect, implement, and optimize data platforms using GCP services like BigQuery, Pub/Sub, Dataflow, and Cloud Storage.
Lead the design of modern data lake/lakehouse solutions, integrating real-time and batch data pipelines.
Collaborate with business stakeholders to translate requirements into technical architectures.
Leverage tools like Cloud Composer, Apache Beam, and Informatica for ETL/ELT workflows.
Apply security best practices including IAM, VPC SC, and encryption mechanisms.
Manage infrastructure through IaC using Terraform and DevOps pipelines (Cloud Build, GitOps).
Oversee implementation of monitoring and logging tools to ensure data platform health.
Support and guide development teams with code reviews, architecture best practices, and documentation.
Engage in Agile delivery processes and contribute to process improvement.
Mandatory Skills:
5+ years in data architecture, including 3+ years on GCP.
Experience in handling data architecture projects on GCP.
Hands-on with GCP services: BigQuery, Cloud Storage, Pub/Sub, Dataflow, Dataproc.
Strong ETL/ELT development experience using Cloud Composer, Apache Beam, and Python.
Strong skills in SQL and Python; experience in JavaScript/TypeScript is a plus.
Proven experience in data lake/lakehouse and microservices architecture.
Hands-on with Terraform, CI/CD pipelines, and secure cloud deployments.
Experience with security & governance: IAM, encryption, Data Catalog.
Soft skills: excellent communication, documentation, stakeholder collaboration.
Preferred Skills:
Google Cloud Professional Data Engineer or Architect certification.
Familiarity with BI tools like Looker, Tableau, or Power BI.
Experience with Kafka, Hadoop, or Spark.
Leadership and mentorship qualities; team player attitude.
Ability to work in a fast-paced Agile environment.
About Techolution:
Techolution is a leading innovation consulting company on track to become one of the most admired brands in the world for innovation done right. Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better human
experience for the communities they serve. With that, we are now fully committed to helping our clients build the enterprise of tomorrow by making the leap from Lab Grade AI to Real World AI. In 2019, we won the prestigious Inc. 500 Fastest-Growing Companies in America award, only 4 years after its formation. In 2022, Techolution was honored with the Best-in-Business title by Inc. for Innovation Done Right. Most recently, we received the AIConics trophy for being the Top AI Solution Provider of the Year at the AI Summit in New York.
Let's give you more insights!
One of our amazing products with Artificial Intelligence:
1. https://faceopen.com/ :Our proprietary and powerful AI Powered user identification system which is built on artificial intelligence technologies such as image recognition, deep neural networks, and robotic process automation. (No more touching keys, badges or fingerprint scanners ever again!)
Some videos you wanna watch!
Life at Techolution
GoogleNext 2023
Ai4 - Artificial Intelligence Conferences 2023
WaWa - Solving Food Wastage
Saving lives - Brooklyn Hospital
Innovation Done Right on Google Cloud
Techolution featured on Worldwide Business with KathyIreland
Techolution presented by ION World's Greatest
Visit us @www.techolution.com : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with technology.","Data Catalog, Pub Sub, Cloud Composer, CI CD pipelines, GCP services, BigQuery, Dataproc, Sql, Cloud Storage, Encryption, Terraform, Iam, Apache Beam, DataFlow, Python"
Azure Data Architect,Delphi Consulting Middle East,Fresher,,India,Login to check your skill match score,"This role requires a blend of technical expertise in data architecture, ETL processes, data engineering, and data analytics, along with strong communication and leadership skills to effectively drive data and AI initiatives within the organization.
Key Responsibilities
Data Architecture & Engineering
Design and architect end-to-end data and artificial intelligence solutions that meet business requirements and align with organizational goals.
Lead the development and implementation of data architecture strategies, including data modeling, data warehousing, and data governance.
Design and implement efficient ETL processes to extract, transform, and load data from various sources into data lakes, data warehouses, and analytical platforms.
Develop and maintain data models and schemas to support data analysis, reporting, and visualization needs across the organization.
Ensure data security, compliance, and governance requirements are met throughout the data lifecycle.
Technology & Tools
Utilize Databricks for data engineering tasks such as data preparation, data transformation, and batch/stream processing.
Implement and optimize data workflows using Data Factory for orchestrating data movement and data processing tasks.
Design and implement scalable and high-performance data storage solutions using Microsoft Fabric.
Collaborate with data engineers and data scientists to design and optimize data pipelines for machine learning model development and deployment.
Leadership & Collaboration
Provide technical leadership and guidance to cross-functional teams on best practices for data architecture, ETL processes, and data analytics.
Collaborate with business stakeholders to understand requirements, define project scope, and prioritize initiatives to deliver actionable insights and drive business outcomes.
Stay updated with the latest trends and advancements in data and AI technologies and evaluate emerging tools and platforms for potential integration into existing solutions.
What You'll Bring
Strong expertise in data architecture principles and best practices.
Proficiency in ETL/ELT (Extract, Transform, Load) processes and tools.
Experience with data engineering techniques and technologies.
Deep understanding of data modeling concepts and techniques.
Hands-on experience with data warehousing solutions.
Expertise in Databricks for data engineering and analytics.
Familiarity with Azure Data Factory for data integration and orchestration.
Experience with Microsoft Fabric or Azure Databricks for scalable data storage and analytics.
Strong analytical and problem-solving skills.
Excellent communication and collaboration skills.
Ability to work effectively in cross-functional teams and influence technical decision-making.
What We Offer
At Delphi, we are dedicated to creating an environment where you can thrive, both professionally and personally. Our competitive compensation package, performance-based incentives, and health benefits are designed to ensure you're well-supported.","ETL processes, Microsoft Fabric, data engineering, Azure Data Factory, Data Modeling, Data Architecture, Data Warehousing, Databricks, Data Analytics"
GCP Data Architect,techolution,5-7 Years,,India,Login to check your skill match score,"Techolution is seeking an experienced GCP Data Architect who brings deep technical expertise in cloud-native data architecture, a strong grasp of Google Cloud Platform (GCP) services, and the ability to deliver enterprise-grade solutions. The ideal candidate will lead the end-to-end design and implementation of data pipelines and platforms, working closely with cross-functional teams to drive innovation and ensure scalable, secure, and efficient data infrastructure.
Designation: GCP Data Architect
Location: Remote
Employment Type: Full-time
Work Timings: 2 PM to 11 PM IST
Job Description:
Architect, implement, and optimize data platforms using GCP services like BigQuery, Pub/Sub, Dataflow, and Cloud Storage.
Lead the design of modern data lake/lakehouse solutions, integrating real-time and batch data pipelines.
Collaborate with business stakeholders to translate requirements into technical architectures.
Leverage tools like Cloud Composer, Apache Beam, and Informatica for ETL/ELT workflows.
Apply security best practices including IAM, VPC SC, and encryption mechanisms.
Manage infrastructure through IaC using Terraform and DevOps pipelines (Cloud Build, GitOps).
Oversee implementation of monitoring and logging tools to ensure data platform health.
Support and guide development teams with code reviews, architecture best practices, and documentation.
Engage in Agile delivery processes and contribute to process improvement.
Mandatory Skills:
5+ years in data architecture, including 3+ years on GCP.
Experience in handling data architecture projects on GCP.
Hands-on with GCP services: BigQuery, Cloud Storage, Pub/Sub, Dataflow, Dataproc.
Strong ETL/ELT development experience using Cloud Composer, Apache Beam, and Python.
Strong skills in SQL and Python; experience in JavaScript/TypeScript is a plus.
Proven experience in data lake/lakehouse and microservices architecture.
Hands-on with Terraform, CI/CD pipelines, and secure cloud deployments.
Experience with security & governance: IAM, encryption, Data Catalog.
Soft skills: excellent communication, documentation, stakeholder collaboration.
Preferred Skills:
Google Cloud Professional Data Engineer or Architect certification.
Familiarity with BI tools like Looker, Tableau, or Power BI.
Experience with Kafka, Hadoop, or Spark.
Leadership and mentorship qualities; team player attitude.
Ability to work in a fast-paced Agile environment.
About Techolution:
Techolution is a leading innovation consulting company on track to become one of the most admired brands in the world for innovation done right. Our purpose is to harness our expertise in novel technologies to deliver more profits for our enterprise clients while helping them deliver a better human
experience for the communities they serve. With that, we are now fully committed to helping our clients build the enterprise of tomorrow by making the leap from Lab Grade AI to Real World AI. In 2019, we won the prestigious Inc. 500 Fastest-Growing Companies in America award, only 4 years after its formation. In 2022, Techolution was honored with the Best-in-Business title by Inc. for Innovation Done Right. Most recently, we received the AIConics trophy for being the Top AI Solution Provider of the Year at the AI Summit in New York.
Let's give you more insights!
One of our amazing products with Artificial Intelligence:
1. https://faceopen.com/ :Our proprietary and powerful AI Powered user identification system which is built on artificial intelligence technologies such as image recognition, deep neural networks, and robotic process automation. (No more touching keys, badges or fingerprint scanners ever again!)
Some videos you wanna watch!
Life at Techolution
GoogleNext 2023
Ai4 - Artificial Intelligence Conferences 2023
WaWa - Solving Food Wastage
Saving lives - Brooklyn Hospital
Innovation Done Right on Google Cloud
Techolution featured on Worldwide Business with KathyIreland
Techolution presented by ION World's Greatest
Visit us @www.techolution.com : To know more about our revolutionary core practices and getting to know in detail about how we enrich the human experience with technology.","Data Catalog, Pub Sub, Cloud Composer, CI CD pipelines, GCP services, BigQuery, Dataproc, Sql, Cloud Storage, Encryption, Terraform, Iam, Apache Beam, DataFlow, Python"
Data Architect,Horizontal Digital,7-9 Years,,"Jaipur, India",Login to check your skill match score,"Horizontal Digital is an experience-forward consultancy. So, what does this mean We help organizations meet ever-increasing customer expectations and set the bar higher in the process. And we deliver on this promise by putting customers at the absolute center of everything we do, helping them build stronger possibilities with our clients in the process.
Our solutions are driven by strategy, creativity, and execution and powered by Sitecore, Salesforce, and other enterprise platforms. Get a deeper look at our expertise with some sample case studies.
But enough about us. Let's talk about you.
As a Data and Technical Architect for Salesforce Data Cloud at Horizontal Digital, you will be a demonstrated leader in technical and data architecture aspects of customer and partner engagements that lead to the successful delivery of Data Cloud Projects. In this key leadership role, you will play the critical role for setting customers up for success by prescriptively helping to shape and then lead the project teams within the Salesforce Data Cloud space. You will collaborate with stakeholders to define technical vision, drive solution and data architecture, and ensure seamless integration of Salesforce Data Cloud with enterprise systems, aligning technology solutions with business objectives. This role requires deep technical expertise in data architecture, integration, and advanced data strategies, enabling organizations to unlock the full potential of their customer data.
What you'll do:
Facilitate and lead technical discovery workshops to document detailed data architecture, integration requirements, and data ingestion strategies.
Synthesize complex requirements to create clear and comprehensive technical solution designs and collaborate with technical teams to document and implement them.
Assess current-state data ecosystems, contact/subscriber management, and identity resolution processes, while defining the future-state architecture and performing gap analysis across data, platform and technology.
Lead the refinement, design, and configuration of complex data models, ensuring alignment with business processes, scalability, and Salesforce Data Cloud best practices.
Collaborate with cross-functional data teams to design and implement data integration and migration strategies leveraging ETL tools, APIs, and middleware solutions.
Guide and oversee the execution of user acceptance testing (UAT), ensuring the delivery of solutions that meet client expectations and quality standards.
Serve as the primary technical point of contact for client stakeholders, providing enablement training on Salesforce Data Cloud and driving adoption across their teams.
Advocate for data governance best practices, including data privacy, quality assurance, and regulatory compliance frameworks.
Collaborate with Sales, Go-to-Market, and professional services teams in pre-sales activities, including scoping, solution estimation, and proposal development.
Contribute to internal growth by developing thought leadership, building best practices, delivering training, and mentoring teams to scale Data Cloud expertise across the organization.
Manage multiple engagements simultaneously, ensuring a balance between billable utilization and team leadership objectives.
What you bring:
7+ years of client-facing consulting/professional services experience delivering enterprise-grade data solutions.
3+ years of experience implementing Salesforce Data Cloud or equivalent Customer Data Platforms (CDPs) (e.g., Adobe AEP, Segment, Tealium, Arm Treasure Data, BlueShift).
Strong background in data architecture, data modeling, ETL/ELT pipelines, data integration, and API-driven solutions.
Certifications in Salesforce Data Cloud and a solid understanding of the Salesforce ecosystem (Sales Cloud, Service Cloud, Marketing Cloud).
Experience implementing data governance, data security, and regulatory compliance (e.g., GDPR, CCPA) frameworks.
Expertise in identity resolution, subscriber management, and harmonizing data across systems to enable a single customer view.
Demonstrated success in facilitating technical workshops, delivering solution documentation, and leading cross-functional technical teams.
Strong analytical and problem-solving skills with expertise in agile delivery methodologies and complex solution lifecycles.
Excellent written and verbal communication skills for engaging with technical and non-technical stakeholders alike.
Industry experience in one or more of the following: Financial Services, Health and Life Sciences, Manufacturing, Retail, or Hospitality.
Bachelor's degree preferred; Master's degree is a plus.
Who you are:
You are a data integration strategy and architecture enthusiast with deep expertise in integrating enterprise-scale data solutions.
You excel at simplifying and leading discussions around complex data challenges, data integration, and technical solutioning for diverse stakeholders.
You are passionate about leveraging data to transform customer experiences, create unified data ecosystems, and enable actionable insights.
You thrive in fast-paced, agile environments and demonstrate the flexibility to manage shifting priorities, ambiguity, and multiple client engagements.
You are recognized as a trusted advisor, known for your strong stakeholder relationships, empathetic communication, and accountability.
You bring a growth mindset, with a commitment to professional development, mentoring teams, and delivering excellence in every engagement.
You are optimistic and results-driven, operating with a focus on collaboration, innovation, and successful outcomes for clients.","subscriber management, identity resolution, Regulatory Compliance, API-driven solutions, middleware solutions, Salesforce Data Cloud, Data Security, Apis, Data Architecture, Data Modeling, Etl Tools, Data Governance, Data Integration"
Lead Data Architect,Encora Inc.,5-7 Years,,"Noida, India",Login to check your skill match score,"Encora is seeking a full-time Lead Data Engineer to support our manufacturing clients large scale digital transformation.
The Lead Data Engineer is responsible for ensuring the day-to-day leadership and guidance of the local, India-based, data team. This role will be the primary interface with the management team of the client and will work cross functionally with various IT functions to streamline project delivery.
Duties and Responsibilities:
Work with a distributed team with primary focus on the management of resources, team assignments, and fostering career growth
Input into the hiring of team members for the locally based team
Support compliance with best practices related to data availability and security procedures
Apply proven communication and problem-solving skills to resolve support data issues as they arise
Demonstrate skills in abstract development frameworks
Demonstrate accountability to assigned work & timelines
Demonstrated leadership skills.
Demonstrated technical competency for the role.
Demonstrated timely delivery on assigned tasks and effective communication on project status and escalation when needed.
Education and Experience:
5+ years of experience in a management role dealing with resourcing, career growth or similar activities and/or responsibilities.
12+ Years or equivalent degree + experience
Required Skills/Certifications:
Strong analytical and problem-solving skills - Ability to determine data patterns and perform root cause analysis to resolve production reporting bugs
Familiarity with the data, analytics and BI space and knowledge of Power BI
Experience working with Microsoft data integration platforms such as Azure Data Factory, SSIS, Synapse
Experience with SQL (Python experience a plus)
Familiarity with Azure DevOps for source code management a plus
Understanding of data management (e. g. permissions, security, and monitoring)
Experience working with and building strong relationships with customers and external partners
Excellent written and verbal communication skills, including logical structuring and delivering presentations
Knowledge of working with helpdesk tools or platforms like ServiceNow is a plus
Experience with cloud services (Azure) is a plus
Experience in working with an onshore client is desirable
Eagerness to learn and think critically to design improved processes
Preferred Experience with:
Estimating and Planning
Work breakdown for the team
Data pipelines
Data Transformations
Code/Unit Test
PR & Code Reviews
Performance & Load Testing
Functional & Regression Testing
Tier 2 support/consulting
Production Support
Functional ask to Technical design
Data Engineering
Agile Methodologies
CI/CD
Metadata driven frameworks.
Technical documentation
Azure Synapse Spark
Python
Complex SQL
Parquet & Delta file processing
ETL Optimizations
Azure DevOps
Location: Noida, India
Mandatory Skills:
1. Data Engineering :
2. Data Pipeline:
3. Pyspark:
4. Spark SQL:
5. Python:
6. SQL:
7. Datawarehousing:
8. Azure Synapse:
9. Enterprise level Data Architecture:
10. Logistic Domain:
11. MS Fabric:
12. Azure Databricks:
About Encora:
Encora is the preferred digital engineering and modernization partner of some of the world's leading enterprises and digital native companies. With over 9,000 experts in 47+ offices and innovation labs worldwide, Encora's technology practices include Product Engineering & Development, Cloud Services, Quality Engineering, DevSecOps, Data & Analytics, Digital Experience, Cybersecurity, and AI & LLM Engineering.
At Encora, we hire professionals based solely on their skills and qualifications, and do not discriminate based on age, disability, religion, gender, sexual orientation, socioeconomic status, or nationality.","Parquet Delta file processing, Complex SQL, Logistic Domain, CI CD, Synapse, Enterprise level Data Architecture, Metadata driven frameworks, MS Fabric, ETL Optimizations, Pyspark, data engineering, Spark SQL, Power Bi, Azure Databricks, SSIS, Sql, Azure Synapse, Azure Data Factory, Data Pipeline, Cloud Services, Datawarehousing, Python, Azure DevOps"
Senior Data Engineer/Data Architect,Spaulding Ridge,8-10 Years,,"Pune, India",Login to check your skill match score,"Spaulding Ridge is an advisory and IT implementation firm. We help global organizations get financial clarity into the complex, daily sales, and operational decisions that impact profitable revenue generations, efficient operational performance, and reliable financial management.
At Spaulding Ridge, we believe all business is personal. Core to our values is our relationships with our clients, our business partners, our team, and the global community. Our employees dedicate their time to helping our clients transform their business, from strategy through implementation and business transformation.
What You Will Do And Learn
As a Data Architect/ Manager in Data Solutions, you'll be responsible for designing, implementing, and testing proposed modern analytic solutions. Working closely with our client partners and architects, you'll develop relationships with key technical resources while delivering tangible business outcomes.
Manage the Data engineering lifecycle including research, proof of concepts, architecture, design, development, test, deployment, and maintenance
Collaborate with team members to design and implement technology that aligns with client business objectives
Build proof of concepts for a modern analytics stack supporting a variety of Cloud-based Business Systems for potential clients
Team management experience and ability to manage, mentor and develop talent of assigned junior resources
Create actionable recommendations based on identified platform, structural and/or logic problems
Communicate and demonstrate a clear understanding of client business needs, goals, and objectives
Collaborate with other architects on solution designs and recommendations.
Qualifications:
8+ years experience developing industry leading business intelligence and analytic solutions
Must have thorough knowledge of data warehouse concepts and dimensional modelling
Must have experience in writing advanced SQL
Must have at least 5+ years of hands-on experience on DBT (Data Build Tool). Mandatory to have most recent hands-on experience on DBT.
Must have experience working with DBT on one or more of the modern databases like Snowflake / Amazon Redshift / BigQuery / Databricks / etc.
Hands-on experience with Snowflake would carry higher weightage
Snowflake SnowPro Core certification would carry higher weightage
Experience working in AWS, Azure, GCP or similar cloud data platform would be an added advantage
Hands-on experience on Azure would carry higher weightage
Must have experience in setting up DBT projects
Must have experience in understanding / creating / modifying & optimizing YML files within DBT
Must have experience in implementing and managing data models using DBT, ensuring efficient and scalable data transformations
Must have experience with various materialization techniques within DBT
Must have experience in writing & executing DBT Test cases
Must have experience in setting up DBT environments
Must have experience in setting up DBT Jobs
Must have experience with writing DBT Jinja and Macros
Must have experience in creating DBT Snapshots
Must have experience in creating & managing incremental models using DBT
Must have experience with DBT Docs
Should have a good understanding of DBT Seeds
Must have experience with DBT Deployment
Must Experience with architecting data pipelines using DBT, utilizing advanced DBT features
Proficiency in version control systems and CI/CD
Must have hands-on experience configuring DBT with one or more version control systems like Azure DevOps / Github / Gitlab / etc.
Must have experience in PR approval workflow
Participate in code reviews and best practices for SQL and DBT development
Experience working with visualization tools such as Tableau, PowerBI, Looker and other similar analytic tools would be an added advantage
2+ years of Business Data Analyst experience
2+ years of experience writing Business requirements, Use cases and/or user stories, for data warehouse or data mart initiatives.
Understanding and experience on ETL/ELT is an added advantage
2+ years of consulting experience working on project-based delivery using Software Development Life Cycle (SDLC)
2+ years of years of experience with relational databases (Postgres, MySQL, SQL Server, Oracle, Teradata etc.)
2+ years of experience creating functional test cases and supporting user acceptance testing
2+ years of experience in Agile/Kanban/DevOps Delivery
Outstanding analytical, communication, and interpersonal skillsAbility to manage projects and teams against planned work
Responsible for managing the day-to-day client relationship on projects
Spaulding Ridge's Commitment to an Inclusive Workplace
When we engage the expertise, insights, and creativity of people from all walks of life, we become a better organization, we deliver superior services to clients, and we transform our communities and world for the better.
At Spaulding Ridge, we believe our team should reflect the rich diversity of society and we take seriously the responsibility to cultivate a workplace where every bandmate feels accepted, respected, and valued for who they are. We do this by creating a culture of trust and belonging, through practices and policies that support inclusion, and through our employee led Employee Resource Groups (ERGs): CRE (Cultural Race and Ethnicity), Women Elevate, PROUD and Mental Wellness Alliance.
The company is committed to offering Equal Employment Opportunity and to providing reasonable accommodation to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Spaulding Ridge and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to our VP of Human Resources, Cara Halladay ([HIDDEN TEXT]). Requests for reasonable accommodation will be considered on a case-by-case basis.
Qualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, gender, sexual orientation, gender identity, protected veteran status or disability.","snowflake, Looker, Tableau, Sql, ELT, AWS, Etl, Powerbi, Version Control Systems, Azure, Gcp"
Celonis Data Architect,"Sky Systems, Inc. (SkySys)",Fresher,,India,Login to check your skill match score,"Job Title: Celonis Data Architect
Job Type & Location: India - Remote
Job Description:
We are seeking a meticulous and experienced Data Architect to design, develop, and implement data architectures and solutions. The ideal candidate will have a strong understanding of database management systems and data modelling techniques, as well as experience in data warehousing, ETL processes, and data governance.
Job Responsibilities:
Collaborate with business stakeholders and technical teams to understand data requirements and develop data architecture solutions.
Design, develop, and maintain data models, data dictionaries, and data flow diagrams, and implement data architecture solutions
Create and implement data warehouse strategies and structures to support reporting and analytics.
Develop and maintain data integration processes, including Extract, Transform, Load (ETL) processes.
Ensure data quality and integrity by establishing and enforcing data governance practices and data stewardship responsibilities.
Prepare and deliver presentations to business stakeholders and technical teams, explaining data architecture concepts and solutions.
Collaborate with software developers and DBAs to optimize database performance and ensure scalability and reliability.
Stay updated with industry trends and emerging technologies related to data architecture and data management.
Define and enforce data security and privacy standards and policies.
Provide guidance and mentoring to junior data professionals.
Job Requirements:
Bachelor's degree in computer science, Information Technology, or a related field.
Proven experience as a Data Architect or in a similar role.
Strong knowledge of data modelling principles and techniques.
Proficiency in database management systems such as Oracle, SQL Server, or MySQL.
Experience with data warehousing concepts and tools such as ETL processes, star schemas, and dimensional modeling.
Familiarity with data governance and data stewardship practices.
Excellent analytical and problem-solving skills.
Strong communication and presentation skills.
Ability to work well in a team environment and collaborate with stakeholders from various departments.
Familiarity with cloud-based data storage and data management platforms is a plus.
As a Data Architect, you will play a critical role in designing and implementing data architectures that support the organization's data needs. Your expertise in data modelling, data warehousing, and ETL processes will contribute to the successful management and utilization of data within the organization.
Support in technical feasibility of various source systems with Celonis EMS. Understand details of the proposed integration pattern to help in effort & timeline estimation.
Ensure that detailed designs match high level designs and are traceable to requirements in functional specification
Ensure designs produced adhere to architectural roadmap and support the development, execution and operations of solutions
Ensure that solutions meet requirements outlined in the design documentation. Ensure the overall user experience is taken into account when designing and deploying new solutions and services. Ensure that developed solutions are peer reviewed, formally documented and signed off by business
Ensure that all work is delivered to agreed time, cost and quality constraints. Initiate solution testing to ensure they meet quality standards
Establish standardized design and development processes to enable cost effective delivery. Authorize and conduct service handover and lead the go-live authorization discussions with the other work streams
Ensure that all release and deployment packages can be tracked, installed, tested, verified and backed out (if required). Ensure that release delivers the expected outcomes and value for the customers
Take accountability to ensure adherence with Security and Compliance policies and procedures within Solution Delivery scope.","ETL Processes, data stewardship, Cloud-based Data Storage, Data Modelling, MySQL, Database Management Systems, Data Architecture, SQL Server, Data Warehousing, Data Governance, Oracle"
Senior Data Architect,Decision Minds,Fresher,,"Bengaluru, India",Login to check your skill match score,"Company Description
Decision Minds is a leading company in Data Cloud, Big Data, Cloud Analytics, AI/ML, and Multi-Cloud deployments. The team consists of passionate thought leaders and industry experts dedicated to revolutionizing Data Analytics, Artificial Intelligence, Cloud Computing, and Robotic Process Automation. The company's focus on utilizing technology for positive change and creating innovative solutions sets it apart in the industry.
Role Description
This is a full-time hybrid role for a Senior Data Architect at Decision Minds. The role will be based in Bengaluru with some opportunities for remote work. The Senior Data Architect will be responsible for data governance, data architecture, data modeling, ETL processes, and data warehousing on a daily basis.
Qualifications
Data Governance and Data Architecture skills
Data Modeling and ETL (Extract Transform Load) skills
Experience in Data Warehousing
Strong analytical and problem-solving skills
Excellent communication and stakeholder management abilities
Knowledge of AI/ML technologies is a plus
Bachelor's or Master's degree in Computer Science, Data Science, or related field","AI ML technologies, Data Modeling, Data Architecture, Data Warehousing, Data Governance"
CDC Data Architect,LSEG (London Stock Exchange Group),8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Design and Development: Build, enhance, and lead the company's logical, conceptual, and physical data models, demonstrating Snowflake's advanced capabilities.
Data Integration: Lead all aspects of the development of comprehensive data integration processes using ETL for various data types, including structured, semi-structured, and unstructured data, ensuring seamless integration with Snowflake.
Performance Tuning: Implement and fine-tune Snowflake features such as resource monitors, RBAC controls, scalable virtual warehouses, SQL performance tuning, zero copy clone, and time travel to optimize performance.
Data Security: Ensure robust data security and handle access controls effectively within the Snowflake environment.
Cloud Integration: Deploy cloud-based enterprise data warehouse solutions, Leverage AWS services such as S3, Glue, Athena, CloudWatch, and EMR to enhance data storage, processing, and analytics capabilities and integrate seamlessly with platforms like AWS and applying Snowflake's cloud-native architecture.
Data Governance: Uphold consistent data governance, testing, and continuous delivery practices, ensuring data integrity and compliance within Snowflake.
AI Integration: Incorporate AI and machine learning models into the data architecture, demonstrating Snowflake's capabilities to handle large-scale data processing and real-time analytics.
Snowpark/Python Development: Use Python/Snowpark for developing data pipelines, ETL processes, and automation scripts, ensuring efficient data handling and processing within the Snowflake environment
Teamwork: Serve as a data domain expert, working closely with various teams to ensure standard methodologies in data management are followed, and facilitate the integration of AI insights into business processes !
Candidate Profile / Key skills
Experience: At least 8 years in Data Engineering or Data Management Solutions, with a proven track record of improving data pipeline processes and leading initiatives.
Snowflake Expertise: A minimum of 5 years of meaningful experience with Snowflake.
Client Leadership: Skilled in leading data-centric client engagements.
Technical Proficiency: Demonstrable skills in sophisticated SQL, Unix Shell/Python scripting, performance tuning, and database optimization.
Cloud Technologies: Expertise in AWS services, including S3, EC2, Lambda, and Redshift.
Data Handling & Migration: Proficient in managing semi-structured data (JSON, XML) and using Snowflake's VARIANT attribute. Experience in migrating data from on-premises databases to Snowflake.
Automation: Skilled in crafting and developing automated data pipelines using Snowpipe and other relavant tools !
Database Experience: Hands-on experience with databases and data warehousing solutions such as Oracle, Microsoft SQL Server, AWS Redshift, or Snowflake.
Cloud Experience: Experience with AWS or Azure is a plus.
SQL Analysis: Strong SQL analysis skills and familiarity with tools like JIRA, Asana, or other relevant defect tracking tools. Experience in implementing data quality frameworks is an added advantage.
Programming Skills: Proficiency in Python, PySpark, or Snowpark is helpful. Additional expertise in Cortex or AI capabilities is a big plus.
LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.
Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.
Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.
Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.
If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","Data Handling, snowflake, Automation, Data Integration, Sql, Performance Tuning, Cloud Technologies, Unix Shell, Data Security, Data Governance, Python, AWS, Etl"
Data Engineer / Data Architect,Soul AI,Fresher,,India,Login to check your skill match score,"About Us:
Soul AI is a pioneering company founded by IIT Bombay and IIM Ahmedabad alumni, with a strong founding team from IITs, NITs, and BITS. We specialize in delivering high-quality human-curated data and AI-first scaled operations services. Based in San Francisco and Hyderabad, we are a fast-moving team on a mission to build AI for Good, driving innovation and societal impact.
Role Overview:
We are seeking a Data Engineer / Data Architect who will be responsible for designing, building, and maintaining scalable data infrastructure and systems for a client. You'll play a key role in enabling efficient data flow, storage, transformation, and access across our organization or client ecosystems.
Whether you're just beginning or already an expert, we value strong technical skills, curiosity, and the ability to translate complex requirements into reliable data pipelines.
Responsibilities:
Design and implement scalable, robust, and secure data pipelines.
Build ETL/ELT frameworks to collect, clean, and transform structured and unstructured data.
Collaborate with data scientists, analysts, and backend engineers to enable seamless data access and model integration.
Maintain data integrity, schema design, lineage, and quality monitoring.
Optimize performance and ensure reliability of data workflows in production environments.
Design and manage data warehousing and lakehouse architecture.
Set up and manage infrastructure using IaC (Infrastructure as Code) when applicable.
Required Skills:
Strong programming skills in Python, SQL, and Shell scripting.
Hands-on experience with ETL tools and orchestration frameworks (e.g., Airflow, Luigi, dbt).
Proficiency in relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Redis).
Experience with big data technologies: Apache Spark, Kafka, Hive, Hadoop, etc.
Deep understanding of data modeling, schema design, and data warehousing concepts.
Proficient with cloud platforms (AWS/GCP/Azure) and services like Redshift, BigQuery, S3, Dataflow, or Databricks.
Knowledge of DevOps and CI/CD tools relevant to data infrastructure.
Nice to Have:
Experience working in real-time streaming environments.
Familiarity with containerization and Kubernetes.
Exposure to MLOps and collaboration with ML teams.
Experience with security protocols, data governance, and compliance frameworks.
Educational Qualifications:
Bachelor's or Master's in Computer Science, Data Engineering, Information Systems, or a related technical field.","Airflow, Luigi, CI CD tools, dbt, S3, PostgreSQL, Data Warehousing, Kafka, Schema Design, Data Modeling, MySQL, Shell scripting, Etl Tools, Python, AWS, BigQuery, Hadoop, Apache Spark, Redshift, Redis, Sql, Devops, Hive, Gcp, Databricks, MongoDB, DataFlow, Azure"
Data Architect - DX1,Maruti Suzuki,5-10 Years,,Gurugram,Automotive/Automobile/Ancillaries,"Job Title: Data Architect
Experience: 5-10 Years
Job Summary:
We are looking for an experienced and highly motivated Data Architect to join our team. The ideal candidate will have a strong background in architecture design, and implementing enterprise data solutions. You will play a critical role in shaping our data infrastructure, ensuring scalability, performance, and security across data platforms.
Key Responsibilities:
.Design and implement scalable data architectures for enterprise applications.
.Develop and maintain conceptual, logical, and physical data models.
.Define data governance policies and ensure data integrity and security.
.Collaborate with stakeholders to identify data requirements and translate them into architectural solutions.
.Lead the evaluation and selection of database technologies and tools.
.Oversee data integration, data warehousing, and ETL/ELT processes.
.Optimize database performance and manage data storage solutions.
.Ensure alignment of data architecture with business and technology strategies.
Required Skills & Qualifications:
.Bachelor's or Master's degree in Computer Science, Information Systems, or related field.
.5-10 years of experience in data architecture, and database design.
.Strong knowledge of relational (e.g., SQL Server).
.Expertise in data warehousing, ETL tools (e.g., Informatica, Talend), and big data platforms (e.g., Hadoop, Spark).
.Strong understanding of data governance, security, and compliance standards.
.Experience with cloud data platforms (e.g., AWS Redshift, Azure Synapse, Google BigQuery) is a plus.
.Excellent communication and stakeholder management skills.
Preferred Certifications (optional):
.AWS Certified Data Analytics - Specialty
.Google Professional Data Engineer
.Microsoft Certified: Azure Data Engineer Associate","Google BigQuery, SQL Server, Aws Redshift, Talend, Azure Synapse, Informatica, Hadoop, Spark"
Data Architect,DHL,Fresher,,"Indore, India",Login to check your skill match score,"Your IT Future, Delivered
Solutions Architect
With a global team of 5800 IT professionals, DHL IT Services connects people and keeps the global economy running by continuously innovating and creating sustainable digital solutions. We work beyond global borders and push boundaries across all dimensions of logistics. You can leave your mark shaping the technology backbone of the biggest logistics company of the world. Our offices in Cyberjaya, Prague, and Chennai have earned #GreatPlaceToWork certification, reflecting our commitment to exceptional employee experiences.
Digitalization. Simply delivered.
At IT Services, we are passionate about Solution Architect in datawarehouse and business intelligence space. Our Customer Service Complex Data Solution team is continuously expanding. No matter your level of Solution Architect proficiency, you can always grow within our diverse environment.
#DHL #DHLITServices #GreatPlace #ppmt #Kart #cscombine
Grow together.
We strive to deliver efficient and optimized business solutions in the Area of Customer Service Complex Data Solutions for our business. You will work as Solutions Architect for existing and new applications to provide end to end Architecture expertise on wide range of technologies like Snowflake, Teradata, Power BI, Matillion, Azure Cloud and many more.
You will be our main Architect providing guidance and direction on the implementation of Analytics, Data Warehousing & Reporting products. You will ensure that the Analytics & Reporting solutions meets the required performance benchmark and adheres to standards & guidelines.
You will work with project teams to ensure Business Requirements are delivered keeping in mind the end-to-end Solution & Data Architecture. You will get to work with some of the complex data structures that will need your expertise to Data Modelling & Design. You will be involved in Optimizing the performance and resource utilization of the existing solutions.
You will guide the development team with technical expertise for ensuring business requirements are implemented as expected. This would mean you sometime have to get down to coding and provide a solution or high-level approach to achieve the requirement to give direction to the Dev Team.
As a senior member in the team, you will collaborate with business users on Requirements and ensure that the requirements are well defined before assigning for development. Lead discussion with Business during UAT Defects review.
You will be working on latest technologies like Snowflake, Matilllion, Teradata, ERWIN, Microservices, Data pipelines, Jenkins, Jira/Confluence, Splunk etc. You will get ample opportunities to grow within the organization and with focus on continuous learning will get opportunity to work & learn many different technologies.
Ready to embark on the journey Here's what we are looking for:
As a Solution Architect, having excellent skill in understanding the latest technology relation to the business knowledge of customer service experience is a huge plus. Very good knowledge of data modeling will also be an integral part of this role and experience in implementation of customer facing application. Been part of the Agile / Scrum team experience is useful. Well versed in Architecture design, software development experiences especially in Python, familiarity of development framework and also analytics and problem solving skills.
You are a business intelligence technology aficionado, therefore you have a good understanding of latest analytics skill sets and experience in implementation of MVP and POC rapid prototyping experience is good to have also in the AI space of new technology adoptions. You are able to work independently prioritize and organize your tasks under time and workload pressure. Working in a multinational environment, you can expect cross-region collaboration with teams around the globe, thus being advanced in spoken and written English will be certainly useful. Basic certification / knowledge of AWS / Azure/ Snowflake/ Teradata/ Power BI related too is a plus.
An array of benefits for you:
Hybrid work arrangements to balance in-office collaboration and home flexibility.
Annual Leave: 42 days off apart from Public / National Holidays.
Medical Insurance: Self + Spouse + 2 children. An option to opt for Voluntary
Parental Insurance (Parents / Parent -in-laws) at a nominal premium covering pre existing disease.
In House training programs: professional and technical training certifications.","Matillion, Reporting, Teradata, Analytics, snowflake, Data pipelines, Data Modelling, Power Bi, Jira, Microservices, Jenkins, Confluence, Azure Cloud, Data Warehousing, Splunk, Python"
IN_Senior Manager_ Big Data Architect _D&A_Advisory_Noida,PwC India,10-12 Years,,"Noida, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
FS X-Sector
Specialism
Data, Analytics & AI
Management Level
Senior Manager
Job Description & Summary
A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Key Responsibilities:
Provide technical leadership regarding data strategy and roadmap exercises, data architecture definition, business intelligence/data warehouse product selection, design and implementation for the enterprise.
Proven track record of success in implementations for Data Lake, Business Intelligence and DW architecture.
Hands on experience in leading large-scale global data warehousing and analytics projects. Demonstrated industry leadership in the fields of database, data warehousing or data sciences
Be accountable for creating end-to-end solution design and development approach in a big data environment including hardware and software recommendations
Should have Deep technical expertise with Big Data technologies like Hadoop, Hive, Hbase, Spark, and 2-3 years of experience of cloud technologies. Real time streaming technologies and time series with tools such as Spark, Flink, Samza etc. Caching and queueing technologies Kafka/Kinesis etc.
NoSQL understanding and use case application Cassandra, HBase, DynamoDB
Should have worked extensively in creating re-usable assets for Data Integration, transformation, auditing and validation frameworks
Knowledge of any Scripting/Programming skills Python, Java, Scala, Go
Implementation and tuning experience of data warehousing platforms, including knowledge of data warehouse schema design, query tuning and optimization, and data migration and integration. Experience of requirements for the analytics presentation layer including dashboards, reporting, and OLAP
Extensive experience in designing Data architecture, data modeling, design, development, data migration and data integration aspects of SDLC.
Participate and/or lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Should have experience designing new or enhancing existing architecture frameworks and implementing them in a cooperative and collaborative setting
Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and initiative in stressful situations
Contributed in Business Development activities.
Strong oral and written communication and interpersonal skills
Working experience on Agile & Scrum methods
Develop documentation and maintain as needed
Support projects by providing SME knowledge to project teams in the areas of Enterprise Data Management
Mandatory Skill Sets
Big Data Architect
Preferred Skill Sets
Big Data Architect
Years Of Experience Required
10+
Education Qualification
BTech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Big Data Architecture
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Intellectual Curiosity, Learning Agility, Optimism + 24 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Go, Flink, Samza, Real time streaming technologies, Data warehousing platforms, Cassandra, Kafka, Data Modeling, Nosql, Kinesis, Data Integration, Agile, Scrum, Python, Java, Data Migration, Hadoop, Scala, Dynamodb, Cloud Technologies, Data Architecture, HBase, Big Data Technologies, Hive, Spark"
Azure Data Architect,HDFC Bank,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Job Name: Azure Data Architect
Experience: 12 years and above
Location: Mumbai ,Bangalore & Gurgaon
Key Skills:
Fundamentals of DevOps, DevSecOps, CD / CI Pipeline using ADO
Good understanding of MPP Architecture, MySQL, RDS, MS-SQL DB, Oracle ,Postgres DB
ELT - Trino, Azure Data factory, Azure Databricks, PySpark, Python, Iceberg, Parquet
CDC Tool like Qlik/ Golden Gate/Dbsium/IBM CDC, Kafka/ Solace
Scripting Shell, Python, Java
5 or more years of experience of software or application development and implementation
Experience with data integration concepts
Development skill using Trino, PySpark and Databricks
Job Role
Work with Retail, Corporates, SME's and Fintech partners to implement and or co- develop cloud applications for the banks business, with particular emphasis on MS Azure, understanding competitive landscape, and prioritizing projects based on client impact
Development and Implement Data Engineering practice (Batch,CDC & Stream) by identifying the capabilities and enhancements to meet client-driven needs, leverage market opportunities, counter competitive threats, and comply with regulatory requirements.
Lead engagement with Business Analysis and core dev teams to create business requirements documentation to feed into the development process.
Study interfaces and providing consulting inputs for remediation and include changes if any interfaces","Dbsium, solace, CDC Tool, Trino, MS-SQL DB, CD CI Pipeline using ADO, Parquet, IBM CDC, MPP Architecture, Iceberg, Postgres DB, Oracle, Java, Devops, RDS, Pyspark, Kafka, Azure Databricks, MySQL, Qlik, Golden Gate, ELT, DevSecOps, Azure Data Factory, Python"
IN_Senior Manager_ Big Data Architect _D&A_Advisory_Noida,PwC India,10-12 Years,,"Noida, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
FS X-Sector
Specialism
Data, Analytics & AI
Management Level
Senior Manager
Job Description & Summary
A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Key Responsibilities:
Provide technical leadership regarding data strategy and roadmap exercises, data architecture definition, business intelligence/data warehouse product selection, design and implementation for the enterprise.
Proven track record of success in implementations for Data Lake, Business Intelligence and DW architecture.
Hands on experience in leading large-scale global data warehousing and analytics projects. Demonstrated industry leadership in the fields of database, data warehousing or data sciences
Be accountable for creating end-to-end solution design and development approach in a big data environment including hardware and software recommendations
Should have Deep technical expertise with Big Data technologies like Hadoop, Hive, Hbase, Spark, and 2-3 years of experience of cloud technologies. Real time streaming technologies and time series with tools such as Spark, Flink, Samza etc. Caching and queueing technologies Kafka/Kinesis etc.
NoSQL understanding and use case application Cassandra, HBase, DynamoDB
Should have worked extensively in creating re-usable assets for Data Integration, transformation, auditing and validation frameworks
Knowledge of any Scripting/Programming skills Python, Java, Scala, Go
Implementation and tuning experience of data warehousing platforms, including knowledge of data warehouse schema design, query tuning and optimization, and data migration and integration. Experience of requirements for the analytics presentation layer including dashboards, reporting, and OLAP
Extensive experience in designing Data architecture, data modeling, design, development, data migration and data integration aspects of SDLC.
Participate and/or lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Should have experience designing new or enhancing existing architecture frameworks and implementing them in a cooperative and collaborative setting
Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and initiative in stressful situations
Contributed in Business Development activities.
Strong oral and written communication and interpersonal skills
Working experience on Agile & Scrum methods
Develop documentation and maintain as needed
Support projects by providing SME knowledge to project teams in the areas of Enterprise Data Management
Mandatory Skill Sets
Big Data Architect
Preferred Skill Sets
Big Data Architect
Years Of Experience Required
10+
Education Qualification
BTech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Big Data Architecture
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Intellectual Curiosity, Learning Agility, Optimism + 24 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Go, Flink, Samza, Real time streaming technologies, Data warehousing platforms, Cassandra, Kafka, Data Modeling, Nosql, Kinesis, Data Integration, Agile, Scrum, Python, Java, Data Migration, Hadoop, Scala, Dynamodb, Cloud Technologies, Data Architecture, HBase, Big Data Technologies, Hive, Spark"
Data Architect,Siemens Healthineers,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Architect, you are required to:
Design & develop technical solutions which combine disparate information to create meaningful insights for business, using Big-data architectures
Build and analyze large, structured and unstructured databases based on scalable cloud infrastructures
Develop prototypes and proof of concepts using multiple data-sources and big-data technologies
Process, manage, extract and cleanse data to apply Data Analytics in a meaningful way
Design and develop scalable end-to-end data pipelines for batch and stream processing
Regularly scan the Data Analytics landscape to stay up to date with latest technologies, techniques, tools and methods in this field
Stay curious and enthusiastic about using related technologies to solve problems and enthuse others to see the benefit in business domain
Qualification:
Bachelor's or Master's in Computer Science & Engineering, or equivalent. Professional Degree in Data Engineering / Analytics is desirable.
Experience level:
Minimum 8 years in software development with at least 2 - 3 years hands-on experience in the area of Big-data / Data Engineering.
Desired Knowledge & Experience:
Data Engineer - Big Data Developer
Spark: Spark 3.x, RDD/DataFrames/SQL, Batch/Structured Streaming
Knowing Spark internals: Catalyst/Tungsten/Photon
Databricks: Workflows, SQL Warehouses/Endpoints, DLT, Pipelines, Unity, Autoloader
IDE: IntelliJ/Pycharm, Git, Azure Devops, Github Copilot
Test: pytest, Great Expectations
CI/CD Yaml Azure Pipelines, Continuous Delivery, Acceptance Testing
Big Data Design: Lakehouse/Medallion Architecture, Parquet/Delta, Partitioning, Distribution, Data Skew, Compaction
Languages: Python/Functional Programming (FP)
SQL: TSQL/Spark SQL/HiveQL
Storage: Data Lake and Big Data Storage Design
Additionally it is helpful to know basics of:
Data Pipelines: ADF/Synapse Pipelines/Oozie/Airflow
Languages: Scala, Java
NoSQL: Cosmos, Mongo, Cassandra
Cubes: SSAS (ROLAP, HOLAP, MOLAP), AAS, Tabular Model
SQL Server: TSQL, Stored Procedures
Hadoop: HDInsight/MapReduce/HDFS/YARN/Oozie/Hive/HBase/Ambari/Ranger/Atlas/Kafka
Data Catalog: Azure Purview, Apache Atlas, Informatica
Big Data Architect
Expert: in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer
Mentor: mentors/educates Developers in technologies, languages and methodologies mentioned in Data Engineer - Big Data Developer
Architecture Styles: Lakehouse, Lambda, Kappa, Delta, Data Lake, Data Mesh, Data Fabric, Data Warehouses (e.g. Data Vault)
Application Architecture: Microservices, NoSql, Kubernetes, Cloud-native
Experience: Many years of experience with all kinds of technology in the evolution of data platforms (Data Warehouse -> Hadoop -> Big Data -> Cloud -> Data Mesh)
Certification: Architect certification (e.g. Siemens Certified Software Architect or iSAQB CPSA)
Required Soft-skills & Other Capabilities:
Excellent communication skills, in order to explain your work to people who don't understand the mechanics behind data analysis
Great attention to detail and the ability to solve complex business problems
Drive and the resilience to try new ideas, if the first ones don't work
Good planning and organizational skills
Collaborative approach to sharing ideas and finding solutions
Ability to work independently and also in a global team environment.","Big Data Storage Design, Nosql, Hadoop, Spark, Data Lake, Databricks, Sql, Python, Azure DevOps"
Assistant Manager - Data Architect,KPMG India,6-8 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
About KPMG in India
KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.
KPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.
The person will work on a variety of projects in a highly collaborative, fast-paced environment. The person will be responsible for software development activities of KPMG, India. Part of the development team, he/she will work on the full life cycle of the process, develop code and unit testing. He/she will work closely with Technical Architect, Business Analyst, user interaction designers, and other software engineers to develop new product offerings and improve existing ones. Additionally, the person will ensure that all development practices are in compliance with KPMG's best practices policies and procedures. This role requires quick ramp up on new technologies whenever required.
Responsibilities
Role Fabric Data E ngineer
Location Bangalore
Experience 6 to 8 Years
Key Responsibilities :-
6+ years of experience as a Data Platform Architect, with demonstrated expertise in designing, managing, and optimizing data warehouses, lakes, and lakehouses.
Proficiency with data storage formats such as Parquet, ORC, and AVRO, as well as storage layers like Delta Lake and other transactional storage solutions
In-depth knowledge of data technologies, including Databricks, Snowflake, and similar platforms
Strong understanding of Business Intelligence (BI) tools such as Power BI, Tableau, and other analytics tools
Experience with data integration and ETL tools such as Azure Data Factory, Talend, Ab Initio, or equivalent
Proven experience with Microsoft Fabric or similar data platforms
Python + Spark is must.
Advanced SQL skills and solid expertise in database design principles
Experience in data modeling, particularly in data warehouse and lakehouse design
Knowledge of the Azure Cloud Platform, especially in relation to data warehousing and storage solutions
Background in data integration and engineering, including familiarity with data pipelines and storage optimization.
Strong grasp of data governance, data security, and compliance requirements
Problem-solving skills with a track record of resolving complex technical issues.
Excellent communication skills, capable of conveying technical concepts to both technical and non-technical stakeholders.
Ability to work independently and as part of a collaborative team.
Microsoft certifications in data-related fields are preferred.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Equal Opportunity Employer
KPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","orc, Compliance, snowflake, Azure Cloud Platform, Parquet, Data Platform Architect, Delta Lake, Microsoft Fabric, Data Modeling, Tableau, Data Integration, Database Design, Data Governance, Talend, Python, Power Bi, Avro, Sql, Azure Data Factory, Spark, Databricks, Data Warehousing, Ab Initio, Data Security"
IN-Director_Senior Data Architect_Data & Analytics_Advisory_ Bangalore,PwC India,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Director
Job Description & Summary
At PwC, our people in business application consulting specialise in consulting services for a variety of business applications, helping clients optimise operational efficiency. These individuals analyse client needs, implement software solutions, and provide training and support for seamless integration and utilisation of business applications, enabling clients to achieve their strategic objectives.
As a business application consulting generalist at PwC, you will provide consulting services for a wide range of business applications. You will leverage a broad understanding of various software solutions to assist clients in optimising operational efficiency through analysis, implementation, training, and support.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary: We are seeking an experienced Senior Data Architect to lead the design and development of our data architecture, leveraging cloud-based technologies, big data processing frameworks, and DevOps practices. The ideal candidate will have a strong background in data warehousing, data pipelines, performance optimization, and collaboration with DevOps teams.
Responsibilities
Design and implement end-to-end data pipelines using cloud-based services (AWS/ GCP/Azure) and conventional data processing frameworks.
Lead the development of data architecture, ensuring scalability, security, and performance.
Collaborate with cross-functional teams, including DevOps, to design and implement data lakes, data warehouses, and data ingestion/extraction processes.
Develop and optimize data processing workflows using PySpark, Kafka, and other big data processing frameworks.
Ensure data quality, integrity, and security across all data pipelines and architectures.
Provide technical leadership and guidance to junior team members.
Design and implement data load strategies, data partitioning, and data storage solutions.
Collaborate with stakeholders to understand business requirements and develop data solutions to meet those needs.
Work closely with DevOps team to ensure seamless integration of data pipelines with overall system architecture.
Participate in design and implementation of CI/CD pipelines for data workflows.
DevOps Requirements
Knowledge of DevOps practices and tools, such as Jenkins, GitLab CI/CD, or Apache Airflow.
Experience with containerization using Docker.
Understanding of infrastructure as code (IaC) concepts using tools like Terraform or AWS CloudFormation.
Familiarity with monitoring and logging tools, such as Prometheus, Grafana, or ELK Stack.
Requirements
12-14 years of experience for Senior Data Architect
in data architecture, data warehousing, and big data processing.
Strong expertise in cloud-based technologies (AWS/ GCP/ Azure) and data processing frameworks (PySpark, Kafka, Flink , Beam etc.).
Experience with data ingestion, data extraction, data warehousing, and data lakes.
Strong understanding of performance optimization, data partitioning, and data storage solutions.
Excellent leadership and communication skills.
Experience with NoSQL databases is a plus.
Mandatory Skill Sets
Experience with agile development methodologies.
Certification in cloud-based technologies (AWS / GCP/ Azure) or data processing frameworks.
Experience with data governance, data quality, and data security.
Preferred Skill Sets
Knowledge of AgenticAI and GenAI is added advantage
Years Of Experience Required
12 to 14 syears
Education Qualification
Graduate Engineer or Management Graduate
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Bachelor in Business Administration
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Innovation, Intellectual Curiosity, Learning Agility + 28 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Available for Work Visa Sponsorship
Government Clearance Required
Job Posting End Date","Flink, Beam, GitLab CI CD, Pyspark, Prometheus, Kafka, Grafana, Elk Stack, Apache Airflow, Jenkins, Gcp, Docker, Terraform, AWS CloudFormation, Azure, AWS"
IN_Senior Manager_ Big Data Architect _D&A_Advisory_Noida,PwC India,10-12 Years,,"Noida, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
FS X-Sector
Specialism
Data, Analytics & AI
Management Level
Senior Manager
Job Description & Summary
A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.
Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data. Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
Key Responsibilities:
Provide technical leadership regarding data strategy and roadmap exercises, data architecture definition, business intelligence/data warehouse product selection, design and implementation for the enterprise.
Proven track record of success in implementations for Data Lake, Business Intelligence and DW architecture.
Hands on experience in leading large-scale global data warehousing and analytics projects. Demonstrated industry leadership in the fields of database, data warehousing or data sciences
Be accountable for creating end-to-end solution design and development approach in a big data environment including hardware and software recommendations
Should have Deep technical expertise with Big Data technologies like Hadoop, Hive, Hbase, Spark, and 2-3 years of experience of cloud technologies. Real time streaming technologies and time series with tools such as Spark, Flink, Samza etc. Caching and queueing technologies Kafka/Kinesis etc.
NoSQL understanding and use case application Cassandra, HBase, DynamoDB
Should have worked extensively in creating re-usable assets for Data Integration, transformation, auditing and validation frameworks
Knowledge of any Scripting/Programming skills Python, Java, Scala, Go
Implementation and tuning experience of data warehousing platforms, including knowledge of data warehouse schema design, query tuning and optimization, and data migration and integration. Experience of requirements for the analytics presentation layer including dashboards, reporting, and OLAP
Extensive experience in designing Data architecture, data modeling, design, development, data migration and data integration aspects of SDLC.
Participate and/or lead in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Should have experience designing new or enhancing existing architecture frameworks and implementing them in a cooperative and collaborative setting
Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and initiative in stressful situations
Contributed in Business Development activities.
Strong oral and written communication and interpersonal skills
Working experience on Agile & Scrum methods
Develop documentation and maintain as needed
Support projects by providing SME knowledge to project teams in the areas of Enterprise Data Management
Mandatory Skill Sets
Big Data Architect
Preferred Skill Sets
Big Data Architect
Years Of Experience Required
10+
Education Qualification
BTech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor of Engineering, Master of Business Administration, Bachelor of Technology
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Big Data Architecture
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Intellectual Curiosity, Learning Agility, Optimism + 24 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","Go, Flink, Samza, Real time streaming technologies, Data warehousing platforms, Cassandra, Kafka, Data Modeling, Nosql, Kinesis, Data Integration, Agile, Scrum, Python, Java, Data Migration, Hadoop, Scala, Dynamodb, Cloud Technologies, HBase, Data Architecture, Big Data Technologies, Hive, Spark"
IN_Director_Data Architect_D&A_Advisory_Gurgaon,PwC India,15-17 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Director
Job Description & Summary
At PwC, our people in data and analytics focus on leveraging data to drive insights and make informed business decisions. They utilise advanced analytics techniques to help clients optimise their operations and achieve their strategic goals.
In business intelligence at PwC, you will focus on leveraging data and analytics to provide strategic insights and drive informed decision-making for clients. You will develop and implement innovative solutions to optimise business performance and enhance competitive advantage.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Responsibilities
About the Role:
We are seeking a highly experienced and motivated Tech Director to lead our data engineering team and spearhead the architecture, development, and implementation of our next-generation data platform, with a strong emphasis on Databricks and the Hadoop ecosystem. The ideal candidate is a proven technical leader with a deep understanding of big data technologies, distributed systems, and a passion for building scalable, high-performance data pipelines.
Key Responsibilities
Leadership & Strategy:
Hire, lead and mentor a team of data engineers, providing technical guidance and fostering a culture of innovation and collaboration.
Define the strategic vision and roadmap for the data platform, aligning with business objectives and industry best practices.
Stay abreast of emerging technologies and trends in the big data and distributed computing landscape.
Build POVs around modern Data platforms.
Guide team to build accelerators/prototypes.
Mandatory Skill Sets
Data Architect
Preferred Skill Sets
Data Architect
Years Of Experience Required
15+
Education Qualification
BE/BTech/MBA/MCA
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Master of Business Administration, Bachelor of Engineering
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Database Architecture
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Thinking, Business Case Development, Business Data Analytics, Business Intelligence and Reporting Tools (BIRT), Business Intelligence Development Studio, Coaching and Feedback, Communication, Competitive Advantage, Continuous Process Improvement, Creativity, Data Analysis and Interpretation, Data Architecture, Database Management System (DBMS), Data Collection, Data Pipeline, Data Quality, Data Science, Data Visualization, Embracing Change, Emotional Regulation, Empathy, Inclusion + 24 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Available for Work Visa Sponsorship
Government Clearance Required
Job Posting End Date","Data Analysis and Interpretation, Data Pipeline, Databricks, Data Architecture, Data Quality, Hadoop Ecosystem, Data Visualization"
Data Architect,Compunnel Technology India Private Limited,7-12 Years,,"Hyderabad, Chandigarh, Noida",Software,"Analyse current state of inventory of data sources, discover ETL activities & processes adopted, understand data quality & lineage.
Discover and analyse Metadata, Master data and Reference data
Analyse Data Access Management, self-service BI Practices in a system
Analyse data life cycle (Important points of access, acquire, transport, store, query, manage, secure, and share)
Discover current state of Databases Availability, Backup, Recovery Mechanism
Understand the system of data virtualisation and related tools like Denodo
Evaluate current state and recommend future roadmap for the organisation data lake, data warehousing and cloud storage requirements (Azure, Snowflake, AWS for semi-structured and unstructured data)
Design data pipeline architecture for data lake & data warehouse (like Azure Data Factory)
Evaluate ETL run-time and recommend suitable schema designs.
Recommend cloud-based data warehouse and analytics solution (Azure Data Factory, Synapse Analytics and Data bricks, AWS services, Snowflake)
Analyse structural requirements for new software, hardware and applications
Understand business & technical requirements to migrate data from legacy systems to new solutions
Build data models for database structures, analytics and AI applications.
Improve & elaborate system performance parameters
Evaluate & Optimize new and current database systems
Provide insight into the changing database storage and utilization requirements and offer suggestions
Integrate new systems and functions like security, performance, scalability, reliability and data recovery.
Collaborate in a data strategy that meets the industry requirements
Envision data pipelines and how data will flow through the enterprise
Evaluate current data management technologies, policies and suggest improvements
Design, document, build and implement database architectures and applications.
Develop measures that ensure data accuracy, integrity and accessibility.
Ensure that the data architecture is scalable and maintainable
Suggest suitable data architecture landscape for integration and scaling
Understand granular details of current MIS
Requirements and skills
Bachelor's/Master's Degree in Computer Engineering or degree/certification in data architecture
Proven work experience as a Data Architect, Data warehouse designer, Data Engineer, Data Scientist, Data Analyst or similar role
In-depth understanding of database structure principles
Familiarity with data virtualisation tools
Data management and reporting technologies, data visualization and structured/unstructured data management
Strong business and communication skills
Good understanding of key architecture concerns such as availability, scalability, operability and maintainability","Data Architect, Etl, AWS"
Sr. Staff Data Architect,Warner Bros. Discovery,6-10 Years,,Hyderabad,Media and Entertainment,"Roles and Responsibilities
Design and implement enterprise data architecture solutions.
Develop and maintain data models, including canonical data models, to standardize data across the organization.
Implement and manage Master Data Management (MDM) solutions to ensure data consistency and accuracy.
Develop and enforce data governance frameworks to ensure data quality, security, and compliance.
Ensure data quality by implementing best practices and tools for data validation, cleansing, and enrichment.
Collaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.
Create conceptual and logical data models and flowcharts.
Provide technical expertise and guidance on data architecture best practices and emerging technologies.
Oversee the integration of data from various sources into a unified data architecture.
Ensure the scalability and performance of data architecture solutions to support growing data volumes and complexity.
Optimize resources to ensure cost-effective and efficient data architecture solutions.
Design and implement different data architectures, including data warehouses, data lakes, and real-time data processing systems.
Leverage AI capabilities to enhance data processing, analysis, and decision-making.
Collaborate with business, management, and data scientists to align data architecture with organizational goals.
Stay up to date with the latest trends and technologies in data architecture and AI and apply them to improve our data infrastructure.
Qualifications & Experiences
Advanced degree in Computer Science, Data Science, Engineering, or a related field
1 2 + years experience with data platforms, cloud services (e.g., AWS, Azure, Google Cloud), and big data technologies
Extensive experience with AWS services , Snowflake and its ecosystem
1 0 + years experience in data architecture, data modeling, and database design
Lead the design and implementation of data architecture from scratch at the enterprise level, ensuring scalability, reliability, and performance
Proficiency in data warehousing, ETL processes, data integration, and analytics tools
Experience in Media and Entertainment industry is preferred
Strong analytical and problem-solving skills.
Excellent verbal and written communication skills.
Ability to work effectively with cross-functional teams.","Computer Science, Data Science, Data Modelling, Azure, Google Cloud, Aws, Etl Process, Data Architecture"
Data Architect,SBM Offshore,2-5 Years,,Bengaluru,Oil and Gas,"SBM Offshore created an Operations Data Architect Team under the Operations Solutions Management department to govern the data from entry to its destruction. The goal is to embrace more digitally automated functions across the entire asset lifecycle. The team is responsible to manage the process related data structuration, enrichment, consolidation, contextualization, and quality through the full operations lifecycle. The team also coordinate with business specialists to guarantee the data modeling is fit for purpose and aligned with the Smart Services ITS Data Strategy.
Among digital applications, the suite of tools has been put in place in order to leverage on the data intelligence for reporting, analysis and prediction purpose:
IFS as ERP solution covering HSSE, Finance, CMMS areas
Aveva PI suite for timeseries historian and structuration
Fieldbox (vendor) for display front end and manual entry, analytics and predictive algorithms
MS Power BI for reporting
Microsoft Azure technologies
Seeq
Data Architect with expertise in Aveva PI and PI Asset Framework. The Data Architect will be responsible for designing and maintaining data architectures, structures, timeseries analytics, creating data models, implementing data governance policies, and ensuring the efficient storage and retrieval of data.
RESPONSIBILITIES
Develop and maintain data architecture and data models for all Digital Solutions within the Operations Portfolio such as: Aveva PI and PI Asset Framework, Seeq, Cognite
Collaborate with Business stakeholders to understand their data needs and requirements, and design appropriate solutions.
Design, implement, and maintain data governance policies and procedures to ensure data quality, security, and compliance.
Develop, test and deploy and maintain simple and complex analyses
Work with the development team to ensure efficient data storage and retrieval and optimize system performance.
Create data mapping and integration strategies to ensure seamless data flow across multiple systems.
Conduct data analysis and provide insights to support decision-making and business strategy.
Stay up-to-date with the latest industry trends, technologies, and best practices related to data architecture.
JOB REQUIREMENTS
Education: Bachelors degree in Engineering, Computer Science, Information Systems, or related field
Experience: 2-5 of experience in Aveva PI, Time series
Specific competencies
Solid understanding of data modeling, database design, and data governance principles.
Proficient in SQL, database programming, and data integration technologies.
Strong analytical and problem-solving skills, with the ability to analyze complex data sets and provide insights.
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Familiarity with industry standards like CFIHOS.
Experience with cloud-based data platforms such as AWS, Azure, or Google Cloud is a plus.
Certification in Aveva PI or PI Asset Framework is highly desirable.
Experience in Seeq is desirable
Experience in MS Azure IOT framework would be a differentiator","Database Programming, Data Architect, Azure, Iot, Sql, AWS"
Data Architect,Norstella,8-10 Years,,India,Login to check your skill match score,"Description
About Norstella
At Norstella, our mission is simple: to help our clients bring life-saving therapies to market quickerand help patients in need.
Founded in 2022, but with history going back to 1939, Norstella unites best-in-class brands to help clients navigate the complexities at each step of the drug development life cycle and get the right treatments to the right patients at the right time.
Each Organization (Citeline, Evaluate, MMIT, Panalgo, The Dedham Group) Delivers Must-have Answers For Critical Strategic And Commercial Decision-making. Together, Via Our Market-leading Brands, We Help Our Clients
Citeline accelerate the drug development cycle
Evaluate bring the right drugs to market
MMIT identify barrier to patient access
Panalgo turn data into insight faster
The Dedham Group think strategically for specialty therapeutics
By combining the efforts of each organization under Norstella, we can offer an even wider breadth of expertise, cutting-edge data solutions and expert advisory services alongside advanced technologies such as real-world data, machine learning and predictive analytics.
As one of the largest global pharma intelligence solution providers, Norstella has a footprint across the globe with teams of experts delivering world class solutions in the USA, UK, The Netherlands, Japan, China and India.
Job Description
Have you wondered how life saving drugs and therapies are created, tested, marketed and
made available to patients in need Have you wondered how clinical trials are conducted at
a global scale How governments and health authorities regulate various organizations
participating in this marketplace Have you wondered how those companies and insurance
providers price a certain drug, and how a care provider determines the right treatment for
a given patient If yes, Norstella could the next step in your career.
We are looking for a Data Engineer with a strong background in cloud data warehousing,
data pipelines, and ETL development. The ideal candidate will have extensive experience
with AWS services, Python, and advanced SQL, coupled with a solid understanding of data
modeling and ETL testing. The role requires a candidate who is proactive, detail-oriented,
and capable of leading projects within a collaborative team environment.
Key Requirements
Cloud Data Architecture Design:
Design and implement scalable, high-performance data models and architectures
using cloud data warehousing concepts.
Develop and maintain data models (including Snowflake and Star Schema) for both
structured and unstructured data, ensuring optimal performance and reliability
across AWS services (e.g., S3, Redshift, Glue, and Lambda).
Data Pipeline And ETL Development
Build and manage data pipelines to ensure efficient data ingestion, processing, and
integration, utilizing tools like AWS Glue, Airflow, and Pyspark.
Implement ETL processes to transform and load data from various sources into
Snowflake, Redshift, PostgreSQL, and other platforms, ensuring data completeness
and quality.
Advanced SQL And RDBMS Management
Leverage advanced SQL (including joins, subqueries, CTEs) and RDBMS concepts to
develop and optimize complex queries, with a preference for RDS SQL Server.
Manage AWS RDS instances, specifically PostgreSQL, ensuring robust data storage
and retrieval processes.
Collaboration With Data Science Team
Work closely with Data Scientists to understand their data needs, ensuring data
availability and quality for real-world data (RWD) analysis and modeling.
Provide Python and Pyspark-based data support, troubleshooting, and performance
tuning for data science projects.
Large Data Set Management
Handle large data sets with a focus on performance optimization, including
implementing strategies for data partitioning, indexing, and caching within AWS
ecosystems.
Optimize the querying of large data sets to enhance performance and ensure
efficient data processing.
Performance Tuning And ETL Testing
Monitor and optimize data systems for performance, including query optimization,
resource management, and AWS DevOps CI/CD pipelines.
Perform ETL testing to validate data completeness and quality across various data
feeds, resolving any bottlenecks in data processing and retrieval.
Data Delivery And Governance Ownership
Take ownership of data delivery processes, ensuring data is accurate, timely, and
accessible, while establishing and maintaining robust data governance policies and
procedures.
Ensure data infrastructure is scalable, cost-effective, and aligns with industry best
practices, particularly in the life sciences/pharma domain.
Life Science Data Expertise
Apply deep knowledge of life science data and industry-specific requirements to
inform data architecture and modeling decisions, ensuring compliance with relevant
regulations and standards.
Demonstrate strong leadership and a positive attitude, embodying Norstella's
principles in collaboration and project execution.
Required Skills And Qualifications
Cloud Data Warehousing Concepts: Strong understanding of cloud data warehousing architectures and best practices.
Data Pipelines/ETL Development: Proven experience in designing and implementing data pipelines and ETL processes.
RDS Postgres: Hands-on experience with AWS RDS, specifically Postgres.
Python & Pyspark: Proficiency in Python and Pyspark for data manipulation and transformation.
AWS Services: Experience with AWS ECS, Lambda, API Gateway, S3, RDS, Glue, and Airflow.
RDBMS & Advanced SQL: Expertise in RDBMS and advanced SQL, including joins, subqueries, CTEs, and complex query writing, with a preference for RDS SQL Server.
Data Modeling: Understanding of data modeling concepts, including Snowflake and Star Schema.
ETL Testing: Experience with ETL testing, focusing on data completeness and quality.
AWS DevOps CI/CD: Experience with AWS DevOps tools and CI/CD pipelines.
Life Sciences/Pharma Domain Knowledge: Familiarity with the life sciences or pharmaceutical domain.
Soft Skills: Strong leadership attitude, aligns with Norstella principles, and exhibits a positive and collaborative work attitude.
Education: Minimum bachelor's degree in computer science and engineering or related field of study, or equivalent experience. 8+ years of experience as a Data Architect or in a similar role, with demonstrated expertise in the required skills.
The guiding principles for success at Norstella
01: Bold, Passionate, Mission-First
We have a lofty mission to Smooth Access to Life Saving Therapies and we will get there by being bold and passionate about the mission and our clients. Our clients and the mission in what we are trying to accomplish must be in the forefront of our minds in everything we do.
02: Integrity, Truth, Reality
We make promises that we can keep, and goals that push us to new heights. Our integrity offers us the opportunity to learn and improve by being honest about what works and what doesn't. By being true to the data and producing realistic metrics, we are able to create plans and resources to achieve our goals.
03: Kindness, Empathy, Grace
We will empathize with everyone's situation, provide positive and constructive feedback with kindness, and accept opportunities for improvement with grace and gratitude. We use this principle across the organization to collaborate and build lines of open communication.
04: Resilience, Mettle, Perseverance
We will persevere even in difficult and challenging situations. Our ability to recover from missteps and failures in a positive way will help us to be successful in our mission.
05: Humility, Gratitude, Learning
We will be true learners by showing humility and gratitude in our work. We recognize that the smartest person in the room is the one who is always listening, learning, and willing to shift their thinking.
Benefits
Health Insurance
Provident Fund
Life Insurance
Reimbursement of Certification Expenses
Gratuity
24x7 Health Desk
Norstella is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people's differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual's abilities, skills, performance and behavior and our business requirements. Norstella operates a zero tolerance policy to any form of discrimination, abuse or harassment.
Sometimes the best opportunities are hidden by self-doubt. We disqualify ourselves before we have the opportunity to be considered. Regardless of where you came from, how you identify, or the path that led you here- you are welcome. If you read this job description and feel passion and excitement, we're just as excited about you.","snowflake, Airflow, Aws Rds, Etl Development, Data Modeling, Aws Services, AWS Glue, Pyspark, Star Schema, Data Governance, Python, PostgreSQL, Advanced Sql"
Data Architect,Grid Dynamics,12-16 Years,,"Bengaluru, India",Login to check your skill match score,"Experience: 12-16Years(only)
Location: Bangalore/Hyderabad
A result- oriented thought-leader to drive the development of the data engineering practice
A trusted advisor and business partner to customers across verticals, and consulting team leader who establishes engineering processes and skill development.
Responsibilities:
Trusted Advisor: Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.
Pre-Sales and Consulting: Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.
Technology Strategy and R&D: Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing
assets and solutions strategy across multiple industries.
Engineering: Working with Grid Dynamics's delivery organization to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.
Business Development & Partnership:
Manage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.
Requirements:
Extensive practical experience in Big Data engineering, data governance, and cloud data
platforms.
Strong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.
Strong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.
Deep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.
Experience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.
Managed key client relations worldwide and advised global technology and business leaders on innovation and data strategy.
Experience with Big 4 consulting is a plus.
Technology skills (any of below):
Data engineering: analytical data platforms, streaming, big data, EDW, data lakes, data governance, data mesh, Spark, Kafka, Snowflake, (2 from below: AWS, GCP, Azure) Transactional databases: Redis, Cassandra, MongoDB, (2 from below: AWS, GCP, Azure) ML and MLOps: VertexAI, Sagemaker, Dataiku, Databricks, mlflow
Grid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, and advanced analytics services. Fusing technical vision with business acumen, we enable positive business outcomes for enterprise companies undergoing business transformation by solving their most pressing technical challenges. A key differentiator for Grid Dynamics is our 7+ years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization, and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India. Follow us on LinkedIn.","Big Data engineering, data aggregation, cloud-based architectures, ML platforms, data lakes, Sagemaker, snowflake, ML and MLOps, Reporting, streaming big data, Data Collection, mlflow, analytical data platforms, Dataiku, Transactional databases, data mesh, VertexAI, Cassandra, BI, Kafka, Edw, AWS, Redis, data engineering, Gcp, Spark, Data Governance, Databricks, MongoDB, Azure"
Data Architect,eInfochips (An Arrow Company),10-12 Years,,"Ahmedabad, India",Login to check your skill match score,"Role: Data Architect Data Science and Azure Cloud
Years of Experience: 10+ Years
Location: Ahmedabad
What You Will Be Doing:
Collaborate with stakeholders to translate business requirements into scalable data architecture solutions.
Design data systems that support machine learning, advanced data science, and generative AI models.
Build and maintain data pipelines, integration flows, and storage solutions for efficient data handling.
Define and uphold data governance standards ensuring data quality, compliance, and security.
Mentor junior data scientists, data engineers, and developers on building end-to-end data solutions.
Provide architectural guidance and best practices across technical teams.
Stay updated with trends in Azure Cloud, Data Science, and Generative AI to introduce relevant technologies.
Perform data profiling, detect quality issues, and recommend solutions for better data reliability.
Work with IT to optimize Azure cloud environments for data storage, retrieval, and processing.
What Are We Looking For:
Bachelor's or Master's in Computer Science, Data Science, or a related field.
Deep expertise in data modeling, statistical analysis, and machine learning.
Hands-on experience with Azure Data Factory, Azure Databricks, Azure OpenAI, and Azure ML.
Knowledge of generative AI (GANs, VAEs) is a strong plus.
Proven track record of designing scalable, enterprise-grade data architectures.
Strong grasp of data governance, management, and security principles.
Proficiency in Python, R, or Scala for data manipulation and analysis.
Familiarity with modern storage solutions data lakes, warehouses, and relational DBs.
Strong analytical thinking and problem-solving skills.
Excellent communication and cross-team collaboration skills.
Certifications in Azure, Data Science, ML, or Generative AI are highly preferred.","relational DBs, R, data lakes, Azure OpenAI, Machine Learning, Azure Databricks, Data Modeling, Python, Azure Data Factory, Statistical Analysis, Scala, Azure ML"
Data Architect,VLink Inc,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"VLink Inc Hiring: Data Architect | Bangalore | Hybrid Work Mode
We're looking for expert Data Architects with 1215 years of experience to join us immediately in Bangalore.
We are seeking experienced Data Architects to design and implement enterprise data solutions, ensuring data governance, quality, and advanced analytics capabilities. The ideal candidate will have expertise in defining data policies, managing metadata, and leading data migrations from legacy systems to Microsoft Fabric/DataBricks/Snowflake. Experience and deep knowledge about at least one of these 3 platforms is critical. Additionally, they will play a key role in identifying use cases for advanced analytics and developing machine learning models to drive business insights.
Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems. Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP). Strong communication skills with the ability to translate complex data concepts into business insights.
Preferred candidate profile
Data Modeling (Conceptual, Logical, Physical)- Minimum 5 years
Database Technologies (SQL Server, Oracle, PostgreSQL, NoSQL)- Minimum 5 years
Cloud Platforms (AWS, Azure, GCP) - Minimum 3 Years
ETL Tools (Informatica, Talend, Apache Nifi) - Minimum 3 Years
Big Data Technologies (Hadoop, Spark, Kafka) - Minimum 5 Years
Data Governance & Compliance (GDPR, HIPAA) - Minimum 3 years
Master Data Management (MDM) - Minimum 3 years
Data Warehousing (Snowflake, Redshift, BigQuery)- Minimum 3 years
API Integration & Data Pipelines - Good to have.
Performance Tuning & Optimization - Minimum 3 years
business Intelligence (Power BI, Tableau)- Minimum 3 years
Interested candidate can share their updated profile on below mentioned mail:-
[HIDDEN TEXT]
Regards As Ever
Ankit Malik","snowflake, Microsoft Fabric, Master Data Management, Data Quality Management, Oracle, Apache Nifi, Hipaa, Data Warehousing, Hadoop, Tableau, Kafka, BigQuery, Power Bi, Etl, Nosql, Informatica, PostgreSQL, SQL Server, Gdpr, Metadata Management, Machine Learning, Talend, Data Governance, Data Modeling, Redshift, Spark, DataBricks"
AI & Data Architect - Azure,CloudFronts - Microsoft Solutions Partner,5-7 Years,,"Mumbai, India",Login to check your skill match score,"AI & Data Architect - Azure
Job Summary:
AI & Data Architect will lead all aspects of AI, Business Intelligence, and Data Architecture with expertise in Azure services, including AI/ML solutions, data engineering, and analytics. The role involves designing, configuring, and managing BI services, aggregating data from multiple sources into a data warehouse, implementing AI-driven insights, and deploying AI models while adhering to best practices.
Skills, Experience & Key Responsibilities :
5+ years of experience in AI, Data Engineering, and Analytics with Azure cloud technologies.
5+ years of integration experience with the Azure Platform (Azure Data Factory, Azure Data Lake, Azure Synapse, Azure Databricks, Azure Machine Learning, Azure Cognitive Services, Logic Apps, API Management).
Hands-on experience with AI/ML models, NLP, Computer Vision, and Predictive Analytics using Azure AI services.
Architect and manage AI-driven BI solutions (portals, dashboards, standard, and ad-hoc reporting) with a focus on data management and AI-driven insights.
Experience in developing and deploying AI models into production environments.
Deep analytical experience and understanding of data modeling and big data solutions.
Strong knowledge of requirements gathering, documentation processes, and stakeholder management.
Proficiency in programming languages such as Python, R, or SQL for AI/ML model development.
Designing & developing dimensions, hierarchies & cubes with Azure Synapse & Databricks.
Provide technical direction and mentoring to a team of AI, BI, and Data Engineers working with enterprise data tools (SSRS, SSIS, SQL Server, Power BI, Azure ML, Databricks).
Ensures best practices in AI model development, testing, and deployment while overseeing, planning, and estimating project needs.
Provide technical leadership on client AI & Data projects and during the sales cycle.
Location: Mumbai
Job Type: Full Time (5days WFO)
Working Hours: 8.30 am to 5.00 pm (Monday to Friday)
Experience Range: 5+ years of experience (Relevant)
ABOUTCLOUDFRONTS:
CloudFronts is a 100% Dynamics 365 focused Microsoft Solutions Partner helping Teams & Organizations worldwide solve their Complex Business Challenges with Microsoft Cloud. Our head office and robust delivery center are based out of Mumbai, India along with branch offices in Singapore & U.S.
CloudFronts was established in 2012 by a former Microsoft CRM Solution Architect Anil Shah with a mission to help other businesses scale up their productivity and reduce their costs concurrently with Microsoft Dynamics. Since its inception, the CloudFronts team has successfully served over 500+ small and medium-sized clients all over the world such as North America, Europe, Australia, Maldives & India with diverse experiences in sectors ranging from Professional services, Finances, Pharmaceutical, Manufacturing, F&B, Retail, Logistics, Energy, Automotive and non-profits.
Our customer success stories and testimonials speak for us. We urge you to look at https://www.cloudfronts.com/dynamics-365-customer-success-stories/
Explore the power of Microsoft Dynamics at www.cloudfronts.com","Analytics, Azure Cognitive Services, R, Logic Apps, AI ML solutions, Sql, SQL Server, SSIS, Computer Vision, Azure Data Factory, Ssrs, Azure Machine Learning, Power Bi, Azure Databricks, data engineering, Python, Azure Synapse, Azure, Api Management, Azure Data Lake, Nlp, Predictive Analytics"
Data Architect,Impetus,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Qualifications
Degree Graduates/Postgraduate in CSE or related field
looking for candidates with hands on experience in Big Data and Cloud Technologies.
10+ Years of experience Expertise in designing and developing applications using Big Data and Cloud technologies Must Have
Expertise and hands-on experience* on Spark, and Hadoop echo system components Must Have
Expertise and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of Shell script & Java/Python Must Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engines like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Must Have
Automation approach - Good to Have
Responsibilities
Define Data Warehouse modernization approach and strategy for the customer
Align the customer on the overall approach and solution
Design systems for meeting performance SLA
Resolve technical queries and issues for team
Work with the team to establish an end-to-end migration approach for one use case so that the team can replicate the same for other iterations","Java, Hadoop, Big Data, Autosys, Cloud Technologies, Gcp, Spark, Shell script, Oozie, Azure, Python, AWS"
Data Architect,DATAECONOMY,15-17 Years,,"Hyderabad, India",Login to check your skill match score,"About Us
About DATAECONOMY: We are a fast-growing data & analytics company headquartered in Dublin with offices inDublin, OH, Providence, RI, and an advanced technology center in Hyderabad,India. We are clearly differentiated in the data & analytics space via our suite of solutions, accelerators, frameworks, and thought leadership.
Job Description
15+ years of professional experience in the field of Data Architecture, Design and Development of Data Lifecycle Management (DLM) and Data Governance projects
Experience should include hands-on experience in implementing a Customer Data Hub Customer Information Management system or Customer Data Mart which includes Modeling Customer Data Model in the 3NF / Dimensional Model, defining and developing integration patterns like Batch and real-time, defining & developing the physical storage layer, defining & developing the consumption pattern
Experience with Data Modeling is a MUST, experience in defining standards and data architecture best practices is a must
Experience with Data Management/Architecture Practices like Data Integration, Data Governance, Data Quality, Metadata management, Data Security, Data Encryption etc. is a must
Must have hands-on experience with developing large Data Mart or Data warehouse or Data hub involving data modelling, data ingestion, data processing and extract generation to the downstream systems
Must have experience with any ETL tool like Informatica or Data Stage or other for data processing requirements.
Experience with Identity resolution products for Name and Address standardization like Informatica IDQ / Address Doctor /IBM Quality stage is required.
Experience with any Data Quality product like Informatica IDQ, Quality Stage or Trillium will be an added advantage
Must have strong experience with writing SQL for pulling and analyzing source/data platforms
Experience with Data Science models, model validation, model tuning and management will be an added advantage.
Proactively communicate and collaborate with business stakeholders to understand the business requirements, translate them into design, and develop the respective module based on the requirements.
Must have experience working in an Agile implementation set-up with Product Backlogs, User Stories, Scrum and sprint planning etc.
Strong verbal and written communication and English language skills
Strong consulting skills and consulting experience are strongly desired.
Requirements
Developing Data Architecture and Design documents for Data Lake, Data warehouse & Data Marts corresponding to Customer Information Management Systems
Experience in Data Lifecycle Management (DLM)
Configuring Solutions and Coding any customization required for the Data platform including Data Lake, Data warehouse and Data Marts
Working with the clients to understand the requirements. Develop the required codebase for the functional needs
Develop Source to Target mapping document for all the attributes that are required to be captured for Data Lake, Data warehouse and Data Marts
Configure and develop code required for Upstream and downstream system communication in a Batch and real-time mode
Provide clarifications for any questions that the Business / SME team or any other stakeholder has on Data implementations.
Participate in system and acceptance testing along with the stakeholders
Benefits
Standard Company Benefits","Agile implementation, Data Encryption, Customer Data Hub, Data Science models, Customer Data Mart, Customer Information Management, Metadata Management, Data Modeling, Data Integration, Sql, Data Quality, Data Architecture, Data Security, Data Governance"
Data Architect,Accurate Background,12-15 Years,,"Hyderabad, India",Login to check your skill match score,"When you join Accurate Background, you're an integral part of making every hire the start of a success story. Your contributions will help us fulfill our mission of advancing the background screening experience through visibility and insights, empowering our clients to make smarter, unbiased decisions.
Accurate Background is a fast-growing organization, focused on providing employment background screenings and building trustful relationships with our clients. Accurate Background continues to exceed expectations by offering an array of innovative and cutting-edge background check and credentialing products to meet the needs of human resource, loss prevention, and security/legal professionals in employment screening and vendor certification.
Lead Data/BI Architect, where you will take charge of developing and executing the enterprise-wide data & analytics strategy. As a senior leader within the organization, you will guide a small data team across integration and BI functions, shaping the data landscape and driving the company towards a single source of truth. This role demands a highly skilled and experienced individual who can deliver a comprehensive data strategy while ensuring robust governance, architecture, and integration across the accurate enterprise.
Lead Data Architect will oversee data governance, security, integration, and data quality initiatives. You will be instrumental in defining architectural design patterns, Data standards, and best practices while leading the team in implementing scalable, optimized, and secure data solutions that support business intelligence objectives.
Responsibilities:
Should be able to understand different cloud Platforms/Architecture, preferably on AWS Platform Services for Data & Analytics
Architect end-to-end data solutions on data lakes, warehouses, and real-time analytics.
Should have Designed/Architected Medium to Large Data Warehouses
Should have created Conceptual, Logical, Physical Data Models, OLTP, OLAP/Dimensional Data Models, Data Analysis
Should be aware of Data Architecture/Design Patterns in the areas of Data Ingestion / Curation / Data Consumption / Reporting Semantic Models
Should have participated in Defining Data Strategy/Roadmaps in Data & Analytics
Should be hands on in some of the areas of the required Technology tools.
Should be aware of Data fabric, Data Ingestion Tools, Data Quality Management, Metadata Management, Data Lineage, Data Security
Should have good experience in Designing Reusable utilities
Should have good experience with Data Warehouse Migrations
Should be aware of Agile Methodologies/Data Products, leading teams technically from Design to Development to Deployment, through DevOps, DataOps
Automate data quality checks and validations to maintain high data integrity. Monitor, troubleshoot, and resolve issues across data platforms
Qualifications:
Should have at least 12-15 years of total IT experience in Software Development, with 5 years exclusively in Design & Architecting Data Warehousing projects
Should have good hands-on below tools & technologies
Must Have:
Data Lake Architecture / Data Fabric
Snowflake (Tasks, Streams, Stored Procs, Snow pipes), SQL Server
Data Modeling tools (like Erwin)
Data Warehouse Migrations
Agile Methodologies
Replication tools (like AWS DMS, Qlik Replicate)
OLAP/Dimensional Data Models
NoSQL Databases (like MongoDB)
Good to have
ETL Tools (like SSIS, dBT)
BI/Reporting tools (Power BI, Tableau)
Cloud Platforms (AWS, Microsoft Fabric)
Real-Time databases (Cassandra, DynamoDB)
Solid understanding of the Agile development process and software release processes.
Must be a self-starter who is highly organized, hands-on, and a team player.
Should be able to create Design Documents/Mapping documents (either PPT or Word document)
Should be able to communicate & Collaborate with all stakeholders (Director level, Business Units, other Architects, Product Managers, Scrum Stay updated on industry trends to continuously improve data systems and processes.
The Accurate Way:
We offer a fun, fast-paced environment, with lots of room for growth. We have an unwavering commitment to diversity, ensuring everyone has a complete sense of belonging here. To do this, we follow four guiding principles Take Ownership, Be Open, Stay Curious, Work as One core values that dictate what we stand for, and how we behave.
Take ownership.
Be accountable for your actions, your team, and the company. Accept responsibility willingly, especially when it's what's best for our customers. Give others every reason to trust you, believe in you, and count on you. Rise to every occasion with your personal best.
Be open.
Be open to new ideas. Be inclusive of people and ways of doing things. Make yourself accessible and approachable, and communicate with genuineness, transparency, honesty, and respect. Embrace differences.
Stay curious.
Stay curious even as you move forward. Tirelessly ask questions and challenge the status quo in your pursuit of new ideas, ways to solve problems, and to continually grow and improve.
Work as one.
Work together to create the best customer and workplace experience. Put our customers and employees firstbefore individual or departmental agendas. Make sure they get the help they need to succeed.
About Accurate Background:
Accurate Background's vision is to make every hire the start of a success story. As a trusted provider of employment background screening and workforce monitoring services, Accurate Background gives companies of all sizes the confidence to make smarter, unbiased hiring decisions at the speed of demand. Experience a new standard of support with a dedicated team, comprehensive technology and insight, and the most extensive coverage and search options to advance your business while keeping your brand and people safe.
Special Notice:
Accurate is aware of schemes involving fraudulent job postings/offers and/or individuals or entities claiming to be employees of Accurate. Those involved are offering fabricated employment opportunities to applicants, often asking for sensitive personal and financial information. If you believe you have been contacted by anyone misrepresenting themselves as an employee of Accurate, please contact [HIDDEN TEXT].
Please be advised that all legitimate correspondence from an Accurate employee will come from @accurate.com email accounts.
Accurate will not interview candidates via text or email. Our interviews are conducted by recruiters and leaders via the phone, Zoom/Teams or in an in-person format.
Accurate will never ask candidates to make any type of personal financial investment related to gaining employment with the Company.","snowflake, Data Lake Architecture, Data Fabric, OLAP Dimensional Data Models, Data Warehouse Migrations, Replication tools like AWS DMS Qlik Replicate, NoSQL Databases like MongoDB, BI Reporting tools Power BI Tableau, Real-Time databases Cassandra DynamoDB, Cloud Platforms AWS Microsoft Fabric, ETL Tools like SSIS dBT, Data Modeling tools like Erwin, SQL Server, Agile Methodologies"
Data Architect,eInfochips (An Arrow Company),10-12 Years,,"Ahmedabad, India",Login to check your skill match score,"Role: Data Architect Data Science and Azure Cloud
Years of Experience: 10+ Years
Location: Ahmedabad
What You Will Be Doing:
Collaborate with stakeholders to translate business requirements into scalable data architecture solutions.
Design data systems that support machine learning, advanced data science, and generative AI models.
Build and maintain data pipelines, integration flows, and storage solutions for efficient data handling.
Define and uphold data governance standards ensuring data quality, compliance, and security.
Mentor junior data scientists, data engineers, and developers on building end-to-end data solutions.
Provide architectural guidance and best practices across technical teams.
Stay updated with trends in Azure Cloud, Data Science, and Generative AI to introduce relevant technologies.
Perform data profiling, detect quality issues, and recommend solutions for better data reliability.
Work with IT to optimize Azure cloud environments for data storage, retrieval, and processing.
What Are We Looking For:
Bachelor's or Master's in Computer Science, Data Science, or a related field.
Deep expertise in data modeling, statistical analysis, and machine learning.
Hands-on experience with Azure Data Factory, Azure Databricks, Azure OpenAI, and Azure ML.
Knowledge of generative AI (GANs, VAEs) is a strong plus.
Proven track record of designing scalable, enterprise-grade data architectures.
Strong grasp of data governance, management, and security principles.
Proficiency in Python, R, or Scala for data manipulation and analysis.
Familiarity with modern storage solutions data lakes, warehouses, and relational DBs.
Strong analytical thinking and problem-solving skills.
Excellent communication and cross-team collaboration skills.
Certifications in Azure, Data Science, ML, or Generative AI are highly preferred.","relational DBs, R, data lakes, Azure OpenAI, Machine Learning, Azure Databricks, Data Modeling, Python, Azure Data Factory, Statistical Analysis, Scala, Azure ML"
Data Architect,VLink Inc,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"VLink Inc Hiring: Data Architect | Bangalore | Hybrid Work Mode
We're looking for expert Data Architects with 1215 years of experience to join us immediately in Bangalore.
We are seeking experienced Data Architects to design and implement enterprise data solutions, ensuring data governance, quality, and advanced analytics capabilities. The ideal candidate will have expertise in defining data policies, managing metadata, and leading data migrations from legacy systems to Microsoft Fabric/DataBricks/Snowflake. Experience and deep knowledge about at least one of these 3 platforms is critical. Additionally, they will play a key role in identifying use cases for advanced analytics and developing machine learning models to drive business insights.
Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems. Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP). Strong communication skills with the ability to translate complex data concepts into business insights.
Preferred candidate profile
Data Modeling (Conceptual, Logical, Physical)- Minimum 5 years
Database Technologies (SQL Server, Oracle, PostgreSQL, NoSQL)- Minimum 5 years
Cloud Platforms (AWS, Azure, GCP) - Minimum 3 Years
ETL Tools (Informatica, Talend, Apache Nifi) - Minimum 3 Years
Big Data Technologies (Hadoop, Spark, Kafka) - Minimum 5 Years
Data Governance & Compliance (GDPR, HIPAA) - Minimum 3 years
Master Data Management (MDM) - Minimum 3 years
Data Warehousing (Snowflake, Redshift, BigQuery)- Minimum 3 years
API Integration & Data Pipelines - Good to have.
Performance Tuning & Optimization - Minimum 3 years
business Intelligence (Power BI, Tableau)- Minimum 3 years
Interested candidate can share their updated profile on below mentioned mail:-
[HIDDEN TEXT]
Regards As Ever
Ankit Malik","snowflake, Microsoft Fabric, Master Data Management, Data Quality Management, Oracle, Apache Nifi, Hipaa, Data Warehousing, Hadoop, Tableau, Kafka, BigQuery, Power Bi, Etl, Nosql, Informatica, PostgreSQL, SQL Server, Gdpr, Metadata Management, Machine Learning, Talend, Data Governance, Data Modeling, Redshift, Spark, DataBricks"
GridOS Data Architect,GE Vernova,Fresher,,"Hyderabad, India",Login to check your skill match score,"Job Description Summary
As a Data Architect, you will play a pivotal role in defining and implementing common data models, API standards, and leveraging the Common Information Model (CIM) standard across a portfolio of products deployed in Critical National Infrastructure (CNI) environments globally.
GE Vernova is the leading software provider for the operations of national and regional electricity grids worldwide. Our software solutions range from supporting electricity markets, enabling grid and network planning, to real-time electricity grid operations.
In this senior technical role, you will collaborate closely with lead software architects to ensure secure, performant, and composable designs and implementations across our portfolio.
Job Description
Grid Software (a division of GE Vernova) is driving the vision of GridOS - a portfolio of software running on a common platform to meet the fast-changing needs of the energy sector and support the energy transition. Grid Software has extensive and well-established software stacks that are progressively being ported to a common microservice architecture, delivering a composable suite of applications. Simultaneously, new applications are being designed and built on the same common platform to provide innovative solutions that enable our customers to accelerate the energy transition.
Responsibilities
This role is for a senior data architect who understands the core designs, principles, and technologies of GridOS. Key responsibilities include:
Formalizing Data Models and API Standards: Lead the formalization and standardization of data models and API standards across products to ensure interoperability and efficiency.
Leveraging CIM Standards: Implement and advocate for the Common Information Model (CIM) standards to ensure consistent data representation and exchange across systems.
Architecture Reviews and Coordination: Contribute to architecture reviews across the organization as part of Architecture Review Boards (ARB) and the Architecture Decision Record (ADR) process.
Knowledge Transfer and Collaboration: Work with the Architecture SteerCo and Developer Standard Practices team to establish standard pratcise around data modeling and API design.
Documentation: Ensure that data modeling and API standards are accurately documented and maintained in collaboration with documentation teams.
Backlog Planning and Dependency Management: Work across software teams to prepare backlog planning, identify, and manage cross-team dependencies when it comes to data modeling and API requirements.
Key Knowledge Areas and Expertise
Data Architecture and Modeling: Extensive experience in designing and implementing data architectures and common data models.
API Standards: Expertise in defining and implementing API standards to ensure seamless integration and data exchange between systems.
Common Information Model (CIM): In-depth knowledge of CIM standards and their application within the energy sector.
Data Mesh and Data Fabric: Understanding of data mesh and data fabric principles, enabling software composability and data-centric design trade-offs.
Microservice Architecture: Understandig of microservice architecture and software development
Kubernetes: Understanding of Kubernetes, including software development in an orchestrated microservice architecture. This includes Kubernetes API, custom resources, API aggregation, Helm, and manifest standardization.
CI/CD and DevSecOps: Experience with CI/CD pipelines, DevSecOps practices, and GitOps, especially in secure, air-gapped environments.
Mobile Software Architecture: Knowledge of mobile software architecture for field crew operations, offline support, and near-realtime operation.
Additional Knowledge (Advantageous But Not Essential)
Energy Industry Technologies: Familiarity with key technologies specific to the energy industry, such as Supervisory Control and Data Acquisition (SCADA), Geospatial network modeling, etc.
This is a critical role within Grid Software, requiring a broad range of knowledge and strong organizational and communication skills to drive common architecture, software standards, and principles across the organization.
Additional Information
Relocation Assistance Provided: No","Data Mesh and Data Fabric, Data Architecture and Modeling, CI CD and DevSecOps, API Standards, Common Information Model CIM, Mobile Software Architecture, Microservice Architecture, Kubernetes"
Data Architect,EverExpanse,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"About Us
EverExpanse is a dynamic technology-driven organization specializing in modern web and e-commerce solutions. We pride ourselves on building scalable, high-performance applications that drive user engagement and business success. Our development team thrives on innovation and collaboration, delivering impactful digital experiences across diverse industries. About Us
Job Overview
clip0_599_14767
EverExpanse Pvt. Ltd.
Bangalore ,India
8 + Years
Full Time
Bachelor's degree in Computer Science, IT, or a related field.
Job Description
We are seeking experienced Data Architects to lead the design, governance, and implementation of large-scale enterprise data solutions. The ideal candidate will have hands-on expertise in Microsoft Fabric, Databricks, or Snowflake, and strong capabilities in data governance, metadata management, and analytics. This role requires the ability to engage with senior stakeholders, define strategic data use cases, and drive the adoption of modern data architectures and advanced analytics solutions.
Key Responsibilities
Data Governance & Management
Establish and maintain a Data Usage Hierarchy to enable structured access to enterprise data.
Define and enforce data policies, standards, and governance frameworks to maintain data consistency and compliance.
Implement Data Quality Management (DQM) practices to enhance data reliability and integrity.
Oversee Metadata Management and Master Data Management (MDM) to enable unified data integration across platforms.
Data Architecture & Migration
Lead end-to-end data migration projects from legacy systems to Microsoft Fabric, Databricks, or Snowflake.
Design scalable and high-performance data architectures that support business intelligence and real-time analytics.
Collaborate with engineering and IT teams to define and implement robust data pipelines and ETL frameworks.
Advanced Analytics & Machine Learning
Identify and prioritize advanced analytics use cases aligned with business objectives.
Design and develop machine learning models to derive actionable business insights.
Collaborate with data scientists and business teams to operationalize ML models for real-world applications.
Required Qualifications
Proven experience as a Data Architect or similar role with deep knowledge in data management, governance, and analytics.
Hands-on expertise in at least one of: Microsoft Fabric, Databricks, or Snowflake (must-have).
In-depth understanding of data governance frameworks, DQM, metadata management, and MDM.
Experience designing and implementing machine learning models and AI-powered solutions.
Proficiency with cloud platforms like Azure, AWS, or GCP and experience in data modeling and ETL development.
Strong communication skills, with the ability to engage senior-level stakeholders and translate complex data concepts into actionable insights.
Preferred Qualifications
Experience with enterprise-level data migration projects.
Background in AI/ML model development in cloud environments.
Familiarity with data visualization tools such as Power BI or Tableau.
Knowledge of modern data pipeline orchestration tools (e.g., Azure Data Factory, Apache Airflow).
Soft Skills
Strategic thinking and problem-solving abilities.
Strong stakeholder management and interpersonal communication.
Ability to work independently and as part of a cross-functional team.
Curiosity and drive to stay updated with emerging data technologies.
Why Join Us
Be at the forefront of data transformation with platforms like Microsoft Fabric, Databricks, and Snowflake.
Work in a hybrid model from our Bangalore office, offering flexibility and collaboration.
Opportunity to interact with senior stakeholders and influence enterprise data strategies.
Competitive compensation and career growth in a future-focused environment.
To Apply send your Resume to [HIDDEN TEXT]","snowflake, Microsoft Fabric, Etl Development, Databricks, Data Migration, Metadata Management, Data Governance, Machine Learning, Data Modeling"
Data Architect,Gainwell Technologies,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Summary
We're looking for a Solution Architect to join our product development team to drive a cloud based data platform architecture that will power BI and advanced analytics at Gainwell Technologies. You will have a direct impact on the design, architecture, and implementation of BI and data science use cases. The ideal candidate will have industry-leading programming skills and established knowledge of implementing, designing, deploying, and maintaining big data analytics platforms in a cloud environment.
The solution architect will use knowledge of healthcare data to influence the implementation and governance of our data architecture. Your designs will account for data movement, storage, compute and BI consumption, and will comply with security and data governance standards. You will work closely with a variety of partners within the Data and Analytics organization.
Your role in our mission
Provide thought leadership and technical direction to the data engineering team in building analytic data products
Understand and translate business requirements to data strategies that align with overall technology vision
Design, develop and enforce standards for the data storage, processing and governance across all environments
Work closely with enterprise and application architects to align the data engineering team to the overall company SDLC standards, practices, and data access patterns
Develop, document and maintain overall view of data platform architecture, data acquisition, data quality, and data retention
Provide formal and informal training for data engineers, platform engineers and ETL developers
Maintain knowledge of emerging technologies and architectures
Document and publish technical principles and standards, and mentor the data engineering team to incorporate them into their daily practices
Champion and present the technical vision to the executive team and business stakeholders
What we're looking for
Basic Qualifications
Bachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields
8+ years of overall experience in big data, database and enterprise data architecture and delivery
8+ years of programming proficiency in a subset of Python, Java, and Scala
5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks
5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms
3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred
Strong SQL skills to create/maintain DB objects, query/load required data using data governance (e.g. business glossary, data dictionary, data catalog, data quality, master data management, etc.) and visualization tools to bring data literacy to the organization.
Practical experience on workload management, monitoring, and performance tuning Apache Spark jobs
Broad knowledge of data technologies, tools, and disciplines including data modeling, dimensional models, third-normal-form structures, ETL/ELT, change data capture and slowly changing dimensions
Experience with healthcare data a big plus
Experience with Machine Learning & MLOPs is a big plus
What you should expect in this role
Client or office environment / may work remotely
Occasional evening and weekend work
Req. ID: 26672","Java, Machine Learning, Hadoop, Scala, Data Modeling, Sql, ELT, MLops, Spark, Data Governance, Databricks, Python, Etl"
Senior Data Architect,Volvo Group,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.
What You Will Do
As Senior Data Architect, your mission is to define and design secured and adaptable data environments for data management and analytics in order to provide access to high quality, consistent data in an easy and convenient manner to all authorized end users.
You will interact with various stakeholders such as Data Analyst, Solution Architects, Business process Developer, capability leads and global teams to provide them with the right architecture for their data solutions based on ITSM and Foundation data.
You will support product delivery teams, providing architecture leadership and support within the end to end life-cycle of managing data.
You will also work with data modeling and data transformation, and by that define and deploy data management and analytical services. This could include reporting and analytics, machine learning, artificial intelligence, data quality, master data management and data provisioning.
You will be part of IT Performance Management Team. This organization is gathering professionals in performance analytics using platforms such as Power BI or ServiceNow and expertise in architecture with goal of defining and supporting healthy and sustainable development of our IT ecosystem.
For this position we also expect high focus on interpersonal skills and attitude. You are passionate about people, customer value and have a strong digital era leadership, meaning that you are able to lead in pluriverse and changing environment.
Who are you
Do you dream big We do too, and we are excited to grow together.
For this position, we are looking for a person with teamwork and innovation spirit. You have proven experience as Data Architect, have already an experience in supporting a team and a good understanding and knowledge of an enterprise with a comparable volume and complexity.
You have strong experience in mentoring and coaching other Architects and you have a good knowledge of ITSM data. You are used to be the Go-to person for your team on the full scope of the delivery and strong contributor cross teams. You can demonstrate effective technical and technology trends dialog with business stakeholders and define right priorities and approach for team to prepare technical solutions that can maximize value delivered to customers. You are able to define the roadmap and the vision for the data environment in accordance with the Department strategy and support the team in taking actions aligned to this strategy. You are already part of various technology communities and cultivate continuous learning core habits.
You have critical thinking, sense making and complex problem solving with ownership mindset.
You share core values according to the Volvo Way. You appreciate diversity and work in cross-functional teams.
You actively collaborate with various stakeholders and navigate with ease in a multi-cultural environment.
You get an opportunity to work together with highly skilled colleagues in an exciting, global environment which provides opportunities to develop both professionally and personally.
Required Competencies
Extended Architect experience on ITSM Data (ServiceNow or in a similar environment)
Professional skills
Security competencies and mindset
Advanced Communication Skills including the ability to effectively communicate with technical and non-technical audiences.
Familiar with Agile environment
Good understanding of the (business) processes and requirements
Excellent analytical skills.
Interest in technology, passionate about ITSM platforms curiosity about the market
Have already successfully supported an organization in a global context with ITSM Data Architecture and managing good relationships with business partners and stakeholders.
Knowledge of Volvo environment (ITSM related) is nice to have
Strong leadership and committed to getting the job done
Familiarity and commitment to work in a global and multinational environment
Minimum 7 years as Data Architect
Relevant university degree
Complete proficiency in English is a requirement
Joining our group means becoming part of a global organization that values innovation, collaboration, and continuous improvement. We offer a dynamic and inclusive work environment where your ideas and contributions truly matter. You'll have the opportunity to work with cutting-edge technologies and develop your skills through continuous learning and professional development programs. We believe in empowering our employees to take ownership of their careers, and we support work-life balance.
If you want to make a real impact in your career, the transportation business is where you want to be. We look forward to meeting you.
We value your data privacy and therefore do not accept applications via mail.
Who We Are And What We Believe In
Our focus on Inclusion, Diversity, and Equity allows each of us the opportunity to bring our full authentic self to work and thrive by providing a safe and supportive environment, free of harassment and discrimination. We are committed to removing the barriers to entry, which is why we ask that even if you feel you may not meet every qualification on the job description, please apply and let us decide.
Applying to this job offers you the opportunity to join Volvo Group. Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group's leading brands and entities.
Group Digital & IT is the hub for digital development within Volvo Group. Imagine yourself working with cutting-edge technologies in a global team, represented in more than 30 countries. We are dedicated to leading the way of tomorrow's transport solutions, guided by a strong customer mindset and high level of curiosity, both as individuals and as a team. Here, you will thrive in your career in an environment where your voice is heard and your ideas matter.","Data Provisioning, Master Data Management, Servicenow, Machine Learning, Data Management, Itsm, Data Modeling, Artificial Intelligence, Power Bi, Data Quality, Data Architecture, Agile, Data Transformation"
Senior Data Architect,Volvo Group,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"Transport is at the core of modern society. Imagine using your expertise to shape sustainable transport and infrastructure solutions for the future If you seek to make a difference on a global scale, working with next-gen technologies and the sharpest collaborative teams, then we could be a perfect match.
What You Will Do
As Senior Data Architect, your mission is to define and design secured and adaptable data environments for data management and analytics in order to provide access to high quality, consistent data in an easy and convenient manner to all authorized end users.
You will interact with various stakeholders such as Data Analyst, Solution Architects, Business process Developer, capability leads and global teams to provide them with the right architecture for their data solutions based on ITSM and Foundation data.
You will support product delivery teams, providing architecture leadership and support within the end to end life-cycle of managing data.
You will also work with data modeling and data transformation, and by that define and deploy data management and analytical services. This could include reporting and analytics, machine learning, artificial intelligence, data quality, master data management and data provisioning.
You will be part of IT Performance Management Team. This organization is gathering professionals in performance analytics using platforms such as Power BI or ServiceNow and expertise in architecture with goal of defining and supporting healthy and sustainable development of our IT ecosystem.
For this position we also expect high focus on interpersonal skills and attitude. You are passionate about people, customer value and have a strong digital era leadership, meaning that you are able to lead in pluriverse and changing environment.
Who are you
Do you dream big We do too, and we are excited to grow together.
For this position, we are looking for a person with teamwork and innovation spirit. You have proven experience as Data Architect, have already an experience in supporting a team and a good understanding and knowledge of an enterprise with a comparable volume and complexity.
You have strong experience in mentoring and coaching other Architects and you have a good knowledge of ITSM data. You are used to be the Go-to person for your team on the full scope of the delivery and strong contributor cross teams. You can demonstrate effective technical and technology trends dialog with business stakeholders and define right priorities and approach for team to prepare technical solutions that can maximize value delivered to customers. You are able to define the roadmap and the vision for the data environment in accordance with the Department strategy and support the team in taking actions aligned to this strategy. You are already part of various technology communities and cultivate continuous learning core habits.
You have critical thinking, sense making and complex problem solving with ownership mindset.
You share core values according to the Volvo Way. You appreciate diversity and work in cross-functional teams.
You actively collaborate with various stakeholders and navigate with ease in a multi-cultural environment.
You get an opportunity to work together with highly skilled colleagues in an exciting, global environment which provides opportunities to develop both professionally and personally.
Required Competencies
Extended Architect experience on ITSM Data (ServiceNow or in a similar environment)
Professional skills
Security competencies and mindset
Advanced Communication Skills including the ability to effectively communicate with technical and non-technical audiences.
Familiar with Agile environment
Good understanding of the (business) processes and requirements
Excellent analytical skills.
Interest in technology, passionate about ITSM platforms curiosity about the market
Have already successfully supported an organization in a global context with ITSM Data Architecture and managing good relationships with business partners and stakeholders.
Knowledge of Volvo environment (ITSM related) is nice to have
Strong leadership and committed to getting the job done
Familiarity and commitment to work in a global and multinational environment
Minimum 7 years as Data Architect
Relevant university degree
Complete proficiency in English is a requirement
Joining our group means becoming part of a global organization that values innovation, collaboration, and continuous improvement. We offer a dynamic and inclusive work environment where your ideas and contributions truly matter. You'll have the opportunity to work with cutting-edge technologies and develop your skills through continuous learning and professional development programs. We believe in empowering our employees to take ownership of their careers, and we support work-life balance.
If you want to make a real impact in your career, the transportation business is where you want to be. We look forward to meeting you.
We value your data privacy and therefore do not accept applications via mail.
Who We Are And What We Believe In
Our focus on Inclusion, Diversity, and Equity allows each of us the opportunity to bring our full authentic self to work and thrive by providing a safe and supportive environment, free of harassment and discrimination. We are committed to removing the barriers to entry, which is why we ask that even if you feel you may not meet every qualification on the job description, please apply and let us decide.
Applying to this job offers you the opportunity to join Volvo Group. Every day, across the globe, our trucks, buses, engines, construction equipment, financial services, and solutions make modern life possible. We are almost 100,000 people empowered to shape the future landscape of efficient, safe and sustainable transport solutions. Fulfilling our mission creates countless career opportunities for talents with sharp minds and passion across the group's leading brands and entities.
Group Digital & IT is the hub for digital development within Volvo Group. Imagine yourself working with cutting-edge technologies in a global team, represented in more than 30 countries. We are dedicated to leading the way of tomorrow's transport solutions, guided by a strong customer mindset and high level of curiosity, both as individuals and as a team. Here, you will thrive in your career in an environment where your voice is heard and your ideas matter.","ITSM Data, Reporting and Analytics, Data Provisioning, Master Data Management, Servicenow, Machine Learning, Data Management, Data Modeling, Artificial Intelligence, Power Bi, Data Quality, Data Architecture, Agile, Data Transformation"
Data Architect,Tezo,7-12 Years,,"Hyderabad, India",Login to check your skill match score,"Tezo is a new generation Digital & AI solutions provider, with a history of creating remarkable outcomes for our customers. We bring exceptional experiences using cutting-edge analytics, data proficiency, technology, and digital excellence.
About the Role
Job Title: Data Engineering Architect
Location: Hyderabad
Employment Type: Full-time
Experience: 7 -12 years
Seeking a highly skilled and modern Data Engineering Lead/Architect to lead technical teams in architecting and delivering cutting-edge data solutions across multiple cloud platforms.
This role requires deep expertise in Azure, Snowflake, and Databricks, along with a strong background in data engineering, architecture, and analytics.
As a Architect, you will drive end-to-end data solutioning, oversee data pipeline development, and ensure scalability, performance, and security while aligning solutions with business objectives.
Responsibilities
Design and implement modern, scalable, and high-performance data architectures across Azure cloud platforms
Develop, optimize, and manage ETL/ELT pipelines, data lakes, and real-time streaming solutions using Snowflake, Databricks, and cloud-native tools.
Deploy and manage data warehousing, analytics, and Lakehouse solutions on Azure Synapse, Azure Data factory, Databricks
Collaborate with data scientists to integrate AI/ML models into data pipelines and optimize analytics workflows.
Implement data governance frameworks, compliance (GDPR, CCPA), role-based access controls, and best practices for security across multi-cloud environments.
Lead and mentor a team of data engineers, define best practices, and drive innovation in data engineering strategies.
Ensure cost-efficient and high-performance data processing, leveraging Spark, DBT, and cloud-native tools.
Collaborate with business leaders, data analysts, and engineering teams to deliver data-driven solutions aligned with business needs.","Ai, real-time streaming solutions, data lakes, snowflake, dbt, Ml, ELT, Azure Synapse, Azure Data Factory, Spark, Databricks, Azure, Etl"
Senior Data Architect,Milacron,10-12 Years,,"Coimbatore, India",Login to check your skill match score,"We are seeking a highly experienced Senior Business Intelligence (BI) Data Architect to join our team. The ideal candidate will have a minimum of 10 years of experience in data architecture, data management, and data analysis. The candidate will be responsible for designing, creating, and managing our company's business intelligence data architecture to drive business decision-making and growth.
The role reports to the head of global business intelligence, and may be based in Coimbatore, India offices or remote.
Responsibilities
Design and develop a scalable and efficient data architecture to support business intelligence needs.
Collaborate with stakeholders to understand and translate business needs into data models supporting long-term solutions.
Work with the development team to design and implement data strategies, build data flows, and develop conceptual data models.
Create logical and physical data models using best practices to ensure high data quality and reduced redundancy.
Optimize and update logical and physical data models to support new and existing projects.
Maintain data architecture standards across the organization.
Identify opportunities to optimize data flows and data management to improve overall efficiency and performance.
Provide technical guidance and support to the BI team members.
Qualifications
Bachelor's degree in Computer Science, Information Systems, or a related field. A Master's degree is preferred.
Minimum of 10 years of experience in data architecture, data management, and data analysis.
Proven experience in BI tools and technologies, data modeling, data warehousing, ETL tools, SQL, and data governance. Expertise in Azure Data Factory, Azure Data Bricks, Azure Warehouse, and PowerBI is a must.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Excellent communication skills with the ability to explain complex technical concepts to non-technical stakeholders.
Strong problem-solving skills, with a proven ability to design creative solutions and make critical decisions.
Experience in project management and team leadership. Familiarity with cloud-based data strategies and architectures. Certification in Azure Data Engineering or similar is a plus.
We offer a competitive salary and benefits package, a challenging and rewarding work environment, and the opportunity to be part of a team that is shaping the future of our company. If you meet the above qualifications and are ready to take the next step in your career, we invite you to apply today.
Who We Are
Milacron is a global leader in the manufacture, distribution and service of highly engineered and customized systems within the $27 billion plastic technology and processing industry. We are the only global company with a full-line product portfolio that includes hot runner systems, injection molding, extrusion equipment. We maintain strong market positions across these products, as well as leading positions in process control systems, mold bases and components, maintenance, repair and operating (MRO) supplies for plastic processing equipment. Our strategy is to deliver highly customized equipment, components and service to our customers throughout the lifecycle of their plastic processing technology systems. Milacron is an Operating Company of Hillenbrand.
Hillenbrand (NYSE: HI) is a global industrial company that provides highly-engineered, mission-critical processing equipment and solutions to customers in over 100 countries around the world. Our portfolio is composed of leading industrial brands that serve large, attractive end markets, including durable plastics, food, and recycling. Guided by our Purpose Shape What Matters For Tomorrow we pursue excellence, collaboration, and innovation to consistently shape solutions that best serve our associates, customers, communities, and other stakeholders. To learn more, visit: www.Hillenbrand.com.
EEO: The policy of Hillenbrand Inc. is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity/expression or veteran status. Additionally, Hillenbrand Inc. and our operating companies are committed to being an Equal Employment Opportunity (EEO) Employer and offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email us at [HIDDEN TEXT] . In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying. At Hillenbrand, everyone is welcome to apply and Shape What Matters for Tomorrow.","Data Analysis, BI tools and technologies, Azure Warehouse, Data Management, Azure Data Bricks, Data Warehousing, Data Architecture, Data Modeling, Sql, Azure Data Factory, Powerbi, Data Governance, Etl Tools"
Data Architect,66degrees,Fresher,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees
66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.
At 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.
Overview of Role
:We are seeking an experienced Data Architect to design, develop, and maintain our google cloud data architecture. The ideal candidate will have a strong background in data architecture, data engineering, and cloud technologies, with experience in managing data across google cloud platforms
.Responsibilities
:GCP Cloud Architecture: Design, implement, and manage robust, scalable, and cost-effective cloud-based data architectures on Google Cloud Platform (GCP), leveraging services like BigQuery, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage, Cloud DataProc, Cloud Run, and Cloud Composer. Experience designing cloud architectures on Oracle Cloud is a plus
.Data Modeling: Develop and maintain conceptual, logical, and physical data models to support various business needs
.Big Data Processing: Design and implement solutions for processing large datasets using technologies such as Spark and Hadoop
.Data Governance: Establish and enforce data governance policies, including data quality, security, compliance, and metadata management
.Data Pipelines: Build and optimize data pipelines for efficient data ingestion, transformation, and loading
.Performance Optimization: Monitor and tune data systems to ensure high performance and availability
.Collaboration: Work closely with data engineers, data scientists, and other stakeholders to understand data requirements and provide architectural guidance
.Innovation: Stay current with the latest technologies and trends in data architecture and cloud computing
.Qualifications
:GCP Core Services: In-depth knowledge of GCP data services, including BigQuery, Cloud Dataflow, Cloud Pub/Sub, Cloud Storage, Cloud DataProc, Cloud Run, and Cloud Composer
.Data Modeling: Expertise in data modeling techniques and best practices
.Big Data Technologies: Hands-on experience with Spark and Hadoop
.Cloud Architecture: Proven ability to design scalable, reliable, and cost-effective cloud architectures
.Data Governance: Understanding of data quality, security, compliance, and metadata management
.Programming: Proficiency in SQL, Python, and DBT (Data Build Tool)
.Problem-Solving: Strong analytical and problem-solving skills
.Communication: Excellent written and verbal communication skills
.A Bachelor's degree in Computer Science, Computer Engineering, Data or related or equivalent work experience required
.GCP Professional Data Engineer or Cloud Architect certification is a plus
.66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class
.","Cloud Dataflow, Cloud Composer, Cloud Run, Cloud DataProc, DBT Data Build Tool, BigQuery, Hadoop, Data Modeling, Sql, Cloud Storage, Spark, Python"
GCP BIG DATA ARCHITECT- Chennai,Grid Dynamics,Fresher,,"Chennai, India",Login to check your skill match score,"Details on tech stack
GCP Services: BigQuery, Cloud Dataflow, Pub/Sub, Dataproc, Cloud Storage.
Data Processing: Apache Beam (batch/stream), Apache Kafka, Cloud Dataprep.
Programming: Python, Java/Scala, SQL.
Orchestration: Apache Airflow (Cloud Composer), Terraform.
Security: IAM, Cloud Identity, Cloud Security Command Center.
Containerization: Docker, Kubernetes (GKE).
Machine Learning: Google AI Platform, TensorFlow, AutoML.
Certifications: Google Cloud Data Engineer, Cloud Architect (preferred).
Proven ability to design scalable and robust AI/ML systems in production, with a focus on high-performance and cost-effective solutions.
Strong experience with cloud platforms (Google Cloud, AWS, Azure) and cloud-native AI/ML services (e.g., Vertex AI, SageMaker).
Expertise in implementing MLOps practices, including model deployment, monitoring, retraining, and version control.
Strong leadership skills with the ability to guide teams, mentor engineers, and collaborate with cross-functional teams to meet business objectives.
Deep understanding of frameworks like TensorFlow, PyTorch, and Scikit-learn for designing, training, and deploying models.
Experience with data engineering principles, scalable pipelines, and distributed systems (e.g., Apache Kafka, Spark, Kubernetes).
Nice to have requirements to the candidate
Strong leadership and mentorship capabilities, guiding teams toward best practices and high-quality deliverables.
Excellent problem-solving skills, with a focus on designing efficient, high-performance systems.
Effective project management abilities to handle multiple initiatives and ensure timely delivery.
Strong emphasis on collaboration and teamwork, fostering a positive and productive work environment.","Cloud Dataprep, Cloud Dataflow, Google AI Platform, Pub Sub, Cloud Security Command Center, Cloud Composer, Cloud Identity, Vertex AI, GCP Services, GKE, Scikit-learn, SageMaker, AutoML, Apache Airflow, Tensorflow, Cloud Storage, Pytorch, Docker, Terraform, Python, AWS, Java, BigQuery, Scala, Dataproc, Sql, MLops, Iam, Apache Kafka, Apache Beam, Azure, Kubernetes"
Data Architect - Enterprise Architecture,Viasat,Fresher,,"Hyderabad, India",Login to check your skill match score,"About Us
One team. Global challenges. Infinite opportunities. At Viasat, we're on a mission to deliver connections with the capacity to change the world. For more than 35 years, Viasat has helped shape how consumers, businesses, governments and militaries around the globe communicate. We're looking for people who think big, act fearlessly, and create an inclusive environment that drives positive impact to join our team.
What You'll Do
The Enterprise Architecture team is focused on providing solutions to enable an effective software engineering workforce that can scale to the business needs. This includes exploring how the business needs map to the application portfolio, business processes, APIs, and data elements across the organization. As a member of this team, you will build up a vast knowledge in software development, cloud application engineering, automation, and container orchestration. Our ideal candidate values communication, learning, adaptability, creativity, and ingenuity. They also enjoy working on challenging technical issues and use creative, innovative techniques to develop and automate solutions. This team is focused on providing our executive and business leadership with visibility into how the software organization is functioning and what opportunities lie to transform the business. In this position you will be an integral part of the Enterprise Architecture team and make meaningful impacts in our journey towards digital transformation.
The day-to-day
A Strong Data-Model and High-Quality Data are pre-requisites to provide better insights and enable solid data-driven decision making. This is also key to take advantage of various technological advances in Artificial Intelligence and Machine Learning. Your responsibilities will involve build out of data models for various aspects of our enterprise in conjunction with domain experts. Examples include but are not limited to Network, Capacity, Finance, Business Support Systems etc. Responsibilities also include working with software product teams to improve data quality across the organization.
What You'll Need
Bachelor's degree or higher in Computer Science & Applications, Computer Science and Computer & Systems Engineering, Computer Science & Engineering, Computer Science & Mathematics, Computer Science & Network Security and Math & Computer Science, and/or a related field
Solid understanding of Data Architecture and Data Engineering principles
Experience building out data models
Experience performing data analysis and presenting data in easy to comprehend manner.
Experience in working with Relational Databases, NoSQL, Large Scale Data technologies (Kafka, Big Query, Snowflake etc)
Experience with digital transformation across multiple cloud platforms like AWS and GCP.
Experience in modernizing data platforms especially in GCP is highly preferred.
Partner with members of Data Platform team and others to build out Data Catalog and map to the data model
Detail Oriented to ensure that the catalog represents quality data
Solid communication skills and ability to work on a distributed team
Tenacity to remain focused on the mission and overcome obstacles
Ability to perform hands-on work with development teams and guide them to building necessary data models.
Experience setting up governance structure and changing the organization culture by influence
What Will Help You On The Job
Experience with Cloud Technologies: AWS, GCP, and/or Azure, etc.
Expertise in GCP data services like Cloud Pub/Sub, Dataproc, Dataflow, BigQuery, and related technologies preferred.
Experience with Airflow, DBT and SQL.
Experience with Open-source software like Logstash, ELK stack, Telegraf, Prometheus and OpenTelemetry is a plus.
Passionate to deliver solutions that improve developer experience and promote API-first principles and microservices architecture.
Experience with Enterprise Architecture and related principles
EEO Statement
Viasat is proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, ancestry, physical or mental disability, medical condition, marital status, genetics, age, or veteran status or any other applicable legally protected status or characteristic. If you would like to request an accommodation on the basis of disability for completing this on-line application, please click here.","Large Scale Data technologies, Big Query, Airflow, Relational Databases, OpenTelemetry, snowflake, dbt, Telegraf, Cloud Pub Sub, Data Analysis, data engineering, Enterprise Architecture, Prometheus, Kafka, Elk Stack, Nosql, Data Architecture, AWS, Logstash, Dataproc, Sql, Cloud Technologies, Gcp, DataFlow"
Senior Data Architect- Snowflake (Remote),Reflections Info Systems,12-14 Years,,India,Login to check your skill match score,"We are looking for an 12+years experienced candidates for this role and a minimum of 8-10 years of experience in data engineering, encompassing the development and scaling of data warehouse and data lake platforms.
Working hours - 8 hours , with a few hours of overlap during EST Time zone. This overlap hours is mandatory as meetings happen during this overlap hours. Working hours will be 12 PM - 9 PM
Primary Skills :
Extensive experience in designing and implementing data solutions using Snowflake. DBT,
Proficiency in data modeling, schema design, and optimization within Snowflake environments.
Strong understanding of cloud data warehousing concepts and best practices, particularly with Snowflake.
Expertise in Dimension Modeling is a must
Expertise in python/java/scala, SQL, ETL processes, and data integration techniques, with a focus on Snowflake.
Familiarity with other cloud platforms and data technologies (e.g., AWS, Azure, GCP )
Demonstrated experience in implementing data governance frameworks and DataOps practices.
Working experience in SAP environments
Familiarity with realtime streaming technologies and Change Data Capture (CDC) mechanisms.
Knowledge of data governance principles and DataOps methodologies
Proven track record of architecting and delivering complex data solutions in cloud platforms/ Snowflake.","realtime streaming technologies, snowflake, cloud data warehousing, data governance principles, data integration techniques, SAP environments, Dimension Modeling, Optimization, dbt, ETL processes, data governance frameworks, DataOps practices, AWS, Sql, Data Modeling, Schema Design, Java, Azure, python, Scala, Gcp"
Chief Technical Manager-Data Architect,NuRe FutureTech,15-17 Years,,"Noida, India",Login to check your skill match score,"Data Governance Senior Consultant Job Description
The opportunity
We're looking for Senior Consultant with expertise in Data Governance with 15 years of experience. Experience in Banking Industries and a good understanding of Core Banking systems/flow is preferred.
Key Responsibilities
Assess clients current data governance maturity and develop recommendations for improvement.
Develop data governance strategies and roadmaps aligned with clients business objectives.
Develop and implement data governance policies, procedures, and standards to ensure data quality.
Collaborate with cross-functional teams to define and document data governance requirements and document them in DG tools.
Managed metadata for data assets, ensuring accurate documentation and integrating technical metadata with business metadata.
Conduct data governance assessments and audits to identify areas for improvement.
Develop and implement data quality, metadata management, and data security policies and procedures.
Collaborate with IT teams to implement data governance technologies and tools.
Develop and manage data governance metrics and reporting frameworks.
Mandatory Requirements
15 years of relevant experience in data governance, data management, or a related field.
Strong understanding of data governance principles, practices, and technologies.
Experience with data governance tools and technologies, such as data catalog, metadata management tools, and data quality software.
Excellent communication, project management, and stakeholder management skills.
Desirable
Experience of cloud-based data governance solutions.
Knowledge of data privacy regulations, such as GDPR, CCPA, or HIPAA.
Experience with agile project management methodologies.
Strong business acumen and understanding of business operations.
Qualifications
Graduate, preference for degree in Computer Science (MCA/BS/BE) with industry recognized certifications
Strong customer service orientation ability to connect with global customers and work with Global teams.",", Data Security Policies, Metadata Management Tools, Data Quality Software, Data Catalog, Data Management, Technologies"
Lead Data Architect Snowflake & Airflow,Mogi I/O : OTT/Podcast/Short Video Apps for you,3-5 Years,,India,Login to check your skill match score,"Note: After your resume is shortlisted, our recruiting team will contact you for initial screening and interview scheduling.
Job Purpose
The Data Engineer will play a crucial role in designing, building, and maintaining modern data pipelines and infrastructure that power business intelligence, reporting, and analytics across the organization. Working with tools like Snowflake, DBT, Airflow, and Looker/Power BI, you will collaborate closely with Data Analysts and cross-functional teams to ensure timely, reliable, and scalable data delivery within a modern cloud-based environment (AWS/GCP). You will be instrumental in translating business needs into technical solutions and ensuring data quality, governance, and best practices are consistently applied.
Roles And Responsibilities
Design and develop robust ETL/ELT pipelines for business analytics and reporting needs.
Work with business analysts to understand data requirements and translate them into scalable technical implementations.
Implement and maintain scalable and automated processes for data ingestion, transformation, and delivery in a Data Mesh architecture.
Collaborate with data stakeholders to ensure optimal data feed performance, including change data capture and delta loading strategies.
Conduct exploratory data analysis to proactively identify and resolve data quality issues, and implement automated data validation tests.
Develop and manage data models and build dashboards using Looker or Power BI as needed.
Ensure adherence to data governance standards including testing, peer reviews, and coding best practices.
Mentor junior engineers and act as a subject matter expert for data engineering tools and workflows.
Provide hands-on troubleshooting and solutions to architecture and design challenges in the data ecosystem.
Continuously evaluate and improve the performance and efficiency of existing pipelines and data workflows.
Must-Have Qualifications
Minimum 3 years of hands-on experience with Snowflake or equivalent cloud data warehouse platforms.
Proficiency in SQL with proven experience in writing and tuning complex queries.
Experience with DBT for data modeling and transformation.
Familiarity with modern data stack tools such as Airflow, Fivetran, and git.
Dashboarding experience using Looker or Power BI.
Solid understanding of data ingestion methods and best practices.
Background in data modeling and data warehouse design principles.
Experience working with large-scale datasets and implementing scalable ETL processes.
Comfortable working in an agile environment (SCRUM) and collaborating with cross-functional teams.
Bachelor's degree in Engineering or related field.
Must be able to join within 30 days.
This is a fully remote position.","snowflake, Airflow, dbt, Looker, Fivetran, Power Bi, Sql, Git"
Staff Software Engineer- Data Architect,Convera,10-15 Years,,"Pune, India",Login to check your skill match score,"As a Staff Software Engineer- Data Architect with expertise in Data Modelling & Architecture along with SQL, Snowflake & AWS with Convera, you will be responsible for to oversee the development and utilization of data systems. You will be reporting to the Manager Data Architect, to join our dynamic team in the Foreign Exchange payments processing industry. The ideal candidate is responsible for defining and implementing the enterprise data architecture strategy and ensuring robust data governance across the organization. This role requires a deep understanding of business processes, technology, data management, and regulatory compliance. The successful candidate will work closely with business and IT leaders to ensure that the enterprise data architecture supports business goals, and that data governance policies and standards are adhered to across the organization. Your responsibilities will include working closely with data modelers, data engineers, analysts, cross-functional teams, and other stakeholders to ensure that our data platform meets the needs of our organization and supports our data-driven initiatives. It also includes building a new data platform, integrating data from various sources, and ensuring data availability for various application and reporting needs. Additionally, the candidate should have experience working with AI/ML technologies and collaborating with data scientists to meet their data requirements.
In your role as a Staff Data Architect, you will:
Architect and Develop Data Architecture Solutions: Design and implement scalable and efficient data architecture solutions on enterprise data platform. Lead the end-to-end architecture, design and modeling using Snowflake, Tableau and AWS.
Collaborate and Design Data Models: Partner with stakeholders and business units to understand data requirements and business goals. Lead the architecture and designing the data models, schemas, data mappings, data transformations and metadata that align with business needs, analytical requirements, and industry standards. Ability to analyze the legacy data models within legacy platforms involving SQL Server, Oracle and Stored Procedures. Possess strong hands-on SQL and Python skills.
Data Integration, Governance and Security: Collaborate with internal and external teams to design, implement, and maintain data integration solutions, ensuring high data integrity, consistency, and accuracy. Ensure data security, integrity, and compliance with regulations. Oversee the integration of data from multiple sources into a unified system.
Business Intelligence and Reporting: Develop and implement semantic models to support business intelligence and reporting needs. Design and manage data models involving dimensions, facts, metrics, and measures to ensure accurate and efficient reporting. Work closely with business analysts and BI developers to create and maintain semantic layers that facilitate self-service reporting and analytics. Ensure the semantic models align with business requirements and support key performance indicators (KPIs) and metrics.
Implementation and Troubleshooting: Oversee the implementation of data solutions from initial concept through to production. Troubleshoot and resolve complex issues to ensure data pipeline stability and high performance. Monitor and optimize the performance of data systems and processes.
Leadership and Mentorship: Provide guidance and leadership to data modelers and engineering teams, promoting a culture of continuous improvement, knowledge sharing, and technical excellence. Mentor data modelers and engineers and foster their professional growth.
AI and ML Capabilities: Stay updated with industry trends and advancements in AI and ML and integrate these technologies into the data architecture to enhance data processing and analytics capabilities. Leverage AI and ML to automate data integration, cleansing, and transformation processes, improving efficiency and accuracy. Implement AI-driven analytics and predictive modeling to provide deeper insights and support data-driven decision-making.
Innovation and Strategy: Drive technical innovation by staying abreast of industry trends and emerging technologies. Influence technical strategies and decisions to align with organizational goals and objectives. Evaluate and recommend data technologies, tools, and platforms.
Documentation and Best Practices: Develop and maintain comprehensive documentation for data architecture, pipelines, and processes. Establish and enforce best practices for data engineering and quality assurance.
A Successful Candidate For This Position Should Have:
Bachelor's degree or equivalent in Computer Science, Engineering, or a related field with proven experience as a Data Architect or in a similar role. in architecting, designing, deploying, and managing data models on cloud-based infrastructure, preferably for data platforms.
Minimum of 10-15 years of experience in enterprise data architecture, data management and data governance, or a related field.
Strong knowledge of database design, data modeling, and ETL processes. Experience in designing and implementing data models involving dimensions, facts, metrics, and measures.
Hands-on experience with Snowflake and SQL. Proficiency with data modeling tools such as ER/Studio, Erwin Data Modeler, Lucidchart, MySQL Workbench, and Oracle SQL Developer Data Modeler. Possess strong hands-on SQL and Python skills.
Strong experience with data architecture principles, including data modelling, ETL/ELT processes, and data management and hands on experience with Big Data technologies.
Familiarity with database systems such as Snowflake, SQL Server, PostgreSQL, or NoSQL databases
Ability to translate business requirements into effective semantic models that support reporting and analytics using Tableau.
Understanding of current industry trends in AI and ML, and their application in data architecture.
Knowledge of data governance and compliance standards.
Excellent problem-solving and analytical skills.
Strong communication and collaboration abilities.
Nice To Have Qualifications:
Experience with regulatory compliance related to data management (e.g., GDPR, HIPAA).
Knowledge of emerging technologies such as AI, machine learning, and data analytics.
Certifications in cloud platforms (e.g., AWS).
About Convera
Convera is the largest non-bank B2B cross-border payments company in the world. Formerly Western Union Business Solutions, we leverage decades of industry expertise and technology-led payment solutions to deliver smarter money movements to our customers helping them capture more value with every transaction. Convera serves more than 30,000 customers ranging from small business owners to enterprise treasurers to educational institutions to financial institutions to law firms to NGOs.
Our teams care deeply about the value we bring to our customers which makes Convera a rewarding place to work. This is an exciting time for our organization as we build our team with growth-minded, results-oriented people who are looking to move fast in an innovative environment.
As a truly global company with employees in over 20 countries, we are passionate about diversity; we seek and celebrate people from different backgrounds, lifestyles, and unique points of view. We want to work with the best people and ensure we foster a culture of inclusion and belonging.
We offer an abundance of competitive perks and benefits including:
Competitive salary
Opportunity to earn an annual bonus.
Great career growth and development opportunities in a global organization
A flexible approach to work
There are plenty of amazing opportunities at Convera for talented, creative problem solvers who never settle for good enough and are looking to transform Business to Business payments. Apply now if you're ready to unleash your potential.
About Convera
Convera is the largest non-bank B2B cross-border payments company in the world. Formerly Western Union Business Solutions, we leverage decades of industry expertise and technology-led payment solutions to deliver smarter money movements to our customers helping them capture more value with every transaction. Convera serves more than 30,000 customers ranging from small business owners to enterprise treasurers to educational institutions to financial institutions to law firms to NGOs.
Our teams care deeply about the value we bring to our customers which makes Convera a rewarding place to work. This is an exciting time for our organization as we build our team with growth-minded, results-oriented people who are looking to move fast in an innovative environment.
As a truly global company with employees in over 20 countries, we are passionate about diversity; we seek and celebrate people from different backgrounds, lifestyles, and unique points of view. We want to work with the best people and ensure we foster a culture of inclusion and belonging.
We offer an abundance of competitive perks and benefits including:
Competitive salary
Opportunity to earn an annual bonus.
Great career growth and development opportunities in a global organization
A flexible approach to work
There are plenty of amazing opportunities at Convera for talented, creative problem solvers who never settle for good enough and are looking to transform Business to Business payments. Apply now if you're ready to unleash your potential.","snowflake, Ai, Data Modelling Architecture, Tableau, Ml, Sql, ELT, Big Data Technologies, AWS, Etl, Data Management, Data Governance, Python, Data Security, Data Integration"
GCP Data Architect,Lingaro,10-12 Years,,India,Login to check your skill match score,"Job Title: Senior Data Architect (GCP)
Location: India (Remote)
About Lingaro:
Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.
Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.
About Data Management Competency: Focused on Data Governance and Quality Management, establishing and enforcing policies, processes, and practices to ensure the integrity, availability, and reliability of data across the organization.
Duties:
Formulate and communicate the organization's data strategy, including data quality standards, data flow, and data security measures.
Provides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements.
Define and implement data governance policies, procedures, and frameworks to ensure data integrity and compliance.
Collaborate with stakeholders to align data strategy with business goals and objectives, document current and target state in the form of business process and data journey diagrams.
Design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.
Define data standards, naming conventions, and data classification guidelines. Ensure data models are scalable, efficient, and optimized for performance.
Evaluate and select appropriate database technologies and solutions based on organizational needs and requirements.
Design and oversee the implementation of data platforms, including relational databases, NoSQL databases, data warehousing, and Big Data solutions.
Optimize database performance, ensure data security, and implement backup and recovery strategies.
Design data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, document source to target mappings.
Collaborate with IT team and data experts to identify opportunities for data acquisition.
Understand and follow data architecture patterns for various types of data systems, e.g. data lakehouse platforms, master data management systems, ML enriched data flows.
Implement data profiling and data cleansing processes to identify and resolve data quality issues.
Establish data quality standards and implement processes to measure, monitor, and improve data quality.
Facilitate discussions and workshops to gather requirements and align data initiatives with business goals, prepare data inventory documentation.
Communicate complex technical concepts effectively to both technical and non-technical stakeholders.
Stay abreast of industry trends and emerging technologies in data management, analytics, and security.
Evaluate and recommend new tools, technologies, and frameworks to enhance data architecture capabilities.
Provide guidance and support to developers and other team members on data-related topics.
Conduct knowledge sharing sessions and training programs to promote understanding and adoption of data architecture best practices.
Requirements:
Bachelor's or master's degree in computer science, Information Systems, or a related field.
10+ years experience as a Data Architect or in a similar role, ideally in a complex organizational setting.
Strong understanding of data management principles, data modeling techniques, database design and data integration flows.
Experience in developing and implementing data governance policies, procedures, and frameworks to ensure data integrity and compliance.
Familiarity with industry best practices and emerging trends in data management and governance.
Ability to design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.
Strong understanding of data modeling best-practices, e.g. data normalization, denormalization, generalization, and performance optimization techniques.
Expertise in working with various database technologies, such as relational databases (e.g., Oracle, SQL Server, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB).
Knowledge of database management systems, data warehousing, data integration technologies, and Big Data solutions.
Familiarity with cloud-based database, warehouse, and lakehouse platforms.
Experience in designing and implementing data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, understanding of data integration patterns and best practices.
Understanding of Business Intelligence and data analytics requirements, optimization of data storage and processing for reporting needs.
Excellent communication and presentation skills to effectively articulate complex technical concepts to both technical and non-technical stakeholders.
Ability to collaborate with cross-functional teams, including business analysts, developers, and data scientists, to understand requirements and drive data-related initiatives.
Strong analytical and problem-solving abilities to identify data-related issues, propose solutions, and make data-driven decisions.
Familiarity with data profiling, data cleansing, data harmonization, and data quality assessment techniques.
Knowledge of data security and privacy regulations, such as GDPR or CCPA, understanding of data encryption, access controls, and data masking techniques to ensure the security of sensitive data.
Professional certification in data management or related field would be advantageous.
Why join us:
Stable employment. On the market since 2008, 1300+ talents currently on board in 7 global sites.
100% remote.
Flexibility regarding working hours.
Full-time position
Comprehensive online onboarding program with a Buddy from day 1.
Cooperation with top-tier engineers and experts.
Unlimited access to the Udemylearning platform from day 1.
Certificate training programs. Lingarians earn 500+ technology certificates yearly.
Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.
Grow as we grow as a company. 76% of our managers are internal promotions.
A diverse, inclusive, and values-driven community.
Autonomy to choose the way you work. We trust your ideas.
Create our community together. Refer your friends to receive bonuses.
Activities to support your well-being and health.
Plenty of opportunities to donate to charities and support the environment.","Relational Databases, data quality assessment, Big Data solutions, NoSQL databases, ETL processes, Data Integration, Data Modeling, Data Governance, Database Design, Data Profiling, Data Warehousing, Data Security, Data Cleansing"
CFIN Data Architect,ABB,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN Data Architect
At ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.
Write the next chapter of your ABB story.
This position reports to
Head of Central Finance
Your role and responsibilities
We are looking for an experienced and technically proficient Data Architect to lead the design, integration, and optimization of the technical solutions within the Central Finance (CFIN) landscape. The Data architect will be responsible for ensuring that data replication and technical activities are fully aligned with business needs, effectively integrated with other enterprise applications, and supported by automated solutions to enhance operational efficiency. This role involves close collaboration with various internal teams, including Finance, IS Architecture, and external vendors, to maintain and evolve the data architecture, ensuring it meets business requirements and is fully compliant with ABB's standards.
The work model for the role is:
This role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.
You will be mainly accountable for:
Solution Design & Validation: Review and validate the design of all Data & Technical related solutions within the CFIN framework, ensuring they are aligned with business goals and technical requirements.
Ownership of Data Architecture: Define, document, and own the overall data architecture within the CFIN ecosystem, including technical components, modules, and integration with other applications.
Data Replication and automation: Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG and Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data
Integration with other processes: Collaborate with other business streams (O2C, P2P, P2D, R2R, TAX, Treasury) to ensure data standards are maintained and design comprehensive data solutions that incorporate all work streams
Maintain Solution Roadmap: Keep the target Data solution architecture up-to-date, documenting changes to the roadmap and their impact on the broader enterprise architecture. Collaboration with Stakeholders: Work closely with the CFIN solution team, IS architects, vendors, and business stakeholders (including Finance, Process, Data, and Systems Finance teams) to configure, maintain, and enhance the CFIN landscape, ensuring business continuity.
Business Process Alignment: Collaborate with Data Global Process Owners (GPOs) and business teams to define and implement robust Data solutions that align with business requirements and global best practices. Automation & Innovation: Drive the regular implementation of automation solutions within the CFIN system to streamline Data processes, reduce manual effort, and improve efficiency.
Requirements Validation: Support the validation of business and functional requirements alongside Process Owners, FPDS team, and Technical Leads, ensuring processes are allocated to the appropriate applications and technologies.
Compliance & Standards: Ensure that all Data & technical solutions and work processes are compliant with ABB's internal standards, policies, and regulatory requirements. Continuous Improvement: Maintain and enhance domain expertise in Data and related technologies, keeping abreast of industry trends and ABB standards to drive continuous improvement within the organization.
Qualifications for the role
Education: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in FICO SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.
At least 7-10 years of experience in Data Architect, SAP Architect, or a similar role, with deep knowledge of Data processes and system integration.
Advanced expertise in SAP Central Finance (CFIN), SAP S/4HANA, or other ERP systems. Proficient in data process automation tools and strategies.
Extensive experience with data migration and replication between SAP systems. In-depth knowledge of SAP Business Technology Platform (BTP), FIORI, and other related applications.
Strong understanding of real-time data replication and automation standards. Strong leadership and team management skills, with the ability to motivate and guide cross-functional teams.
Excellent collaboration skills with the ability to coordinate between different stakeholders, including business leaders, technical teams, and external partners.
A strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.
Experience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.
More about us
Finance Services is ABB's shared services organization which delivers operational and expert services in Finance, with employees based in five main hubs and front offices, finance service provides mainly Business services to ABB teams across the globe as well as supports with external customer inquiries.
We value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory
It has come to our attention that the name of ABB is being used for asking candidates to make payments for job opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made available on our career portal for all fitting the criteria to apply. ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions. For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Real-time data replication and automation standards, Data migration and replication between SAP systems, Data process automation tools, Fiori"
Lead Data Architect,Chevron,10-15 Years,,"Bengaluru, India",Login to check your skill match score,"About The Position
Lead Data architects lead the design and implementation of data collection, storage, transformation, orchestration (movement) and consumption to achieve optimum value from data. They are the technical leaders within data delivery teams. They play a key role in modeling data for optimal reuse, interoperability, security and accessibility as well as in the design of efficient ingestion and transformation pipelines. They ensure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration. And they instill trust through the employment of data quality frameworks and tools.
The data architect at Chevron predominantly works within the Azure Data Analytics Platform, but they are not limited to it. The Senior Data architect is responsible for optimizing costs for delivering data. They are also responsible for ensuring compliance to enterprise standards and are expected to contribute to the evolution of those standards resulting from changing technologies and best practices.
Key Responsibilities
Design and overseeing the entire data architecture strategy
Mentor junior data architects to ensure skill development in alignment with the team strategy
Design and implement complex scalable, high-performance data architectures that meet business requirements
Model data for optimal reuse, interoperability, security and accessibility
Develop and maintain data flow diagrams, and data dictionaries
Collaborate with stakeholders to understand data needs and translate them into technical solutions
Ensure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration
Ensure data quality, integrity, and security across all data systems
Required Qualifications
Bachelor's degree in computer science, Information Technology, or a related field (or equivalent experience)
Overall 10-15 years of experience with at least 5 years of proven experience as a Data Architect or similar role
Strong knowledge of data modeling, data warehousing, and data integration techniques
Proficiency in database management systems (e.g., SQL Server, Oracle, PostgreSQL)
Experience with big data technologies (e.g., Hadoop, Spark) and data lake solutions (e.g., Azure Data Lake, AWS Lake Formation)
Experience with big data technologies data lake solutions DBMS and cloud platforms
Experience in data modeling, ERDs, Star and/or Snowflake, and physical model design for analytics and application integration
Experience in designing data pipelines for optimal performance, resiliency, and cost efficiency
Experience translating business objectives and goals into technical architecture for data solutions
Familiarity with cloud platforms (e.g., Microsoft Azure, AWS, Google Cloud Platform)
Strong understanding of data governance and security best practices
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Track record for defining/implementing data architecture framework and governance around master data, meta data, modeling
Preferred Qualifications
Experience in Erwin, Azure Synapse, Azure Databricks, Azure DevOps, SQL, Power BI, Spark, Python, R
Ability to drive business results by building optimal cost data landscapes
Familiarity with Azure AI/ML Services, Azure Analytics: Event Hub, Azure Stream Analytics, Scripting: Ansible
Experience with machine learning and advanced analytics
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes)
Understanding of CI/CD pipelines and automated testing frameworks
Certifications such as AWS Certified Solutions Architect, IBM certified data architect or similar are a plus
Chevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm.
Chevron participates in E-Verify in certain locations as required by law.","CI CD pipelines, data integration techniques, Azure Analytics, Azure AI ML Services, Azure Stream Analytics, data pipelines, R, Event Hub, AWS Lake Formation, Hadoop, Erwin, Power Bi, Azure Databricks, Data Warehousing, PostgreSQL, Azure DevOps, SQL Server, Data Modeling, Data Governance, Ansible, AWS, Oracle, Kubernetes, Python, Azure Synapse, Docker, Azure Data Lake, Microsoft Azure, Google Cloud Platform, Spark"
Data Architect III - Workforce Technology,JPMorganChase,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Be a part of a dynamic team and excel in an environment that values diversity and creativity. Continue to sharpen your skills and ambition while pushing the industry forward.
As a Data Architect at JPMorgan Chase within the Employee Platforms, you serve as a seasoned member of a team to develop high-quality data architecture solutions for various software applications and platforms. By incorporating leading best practices and collaborating with teams of architects, you are an integral part of carrying out critical technology solutions across multiple technical areas within various business functions in support of the firm's business objectives.
In this role, you will be responsible for designing and implementing data models that support our organization's data strategy. You will work closely with Data Product Managers, Engineering teams, and Data Governance teams to ensure the delivery of high-quality data products that meet business needs and adhere to best practices.
Job Responsibilities
Executes data architecture solutions and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down problems
Collaborate with Data Product Managers to understand business requirements and translate them into data modeling specifications. Conduct interviews and workshops with stakeholders to gather detailed data requirements.
Create and maintain data dictionaries, entity-relationship diagrams, and other documentation to support data models.
Creates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems
Produces data architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development
Evaluates data architecture designs and provides feedback on recommendations
Represents their team in architectural governance bodies
Leads the data architecture team in evaluating new technologies to modernize the architecture using existing data standards and framework
Gathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of data frameworks, applications, and systems
Proactively identifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture
Contributes to data architecture communities of practice and events that explore new and emerging technologies
Required Qualifications, Capabilities, And Skills
Formal training or certification on Data Architect and 3+ years applied experience
Hands on experience in data platforms, cloud services (eg, AWS, Azure or Google Cloud) and big data technologies
Strong understanding of database management systems, data warehousing, and ETL processes.
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Knowledge of data governance principles and best practices.
Ability to evaluate current technologies to recommend ways to optimize data architecture
Hands-on practical experience in system design, application development, testing, and operational stability
Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming and database querying languages
Overall knowledge of the Software Development Life Cycle
Solid understanding of agile methodologies such as continuous integration and delivery, application resiliency, and security
Demonstrated knowledge of software applications and technical processes within a technical discipline (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)
Preferred Qualifications, Capabilities, And Skills
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud).
Familiarity with big data technologies (e.g., Hadoop, Spark).
Certification in data modeling or data architecture.
About Us
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","application resiliency, Security, ETL processes, data governance principles, Delivery, Mobile, operational stability, database querying languages, Continuous Integration, Data Architect, Data Warehousing, Big Data Technologies, Database Management Systems, Artificial Intelligence, Software Development Life Cycle, cloud, AWS, Machine Learning, Testing, Application Development, Google Cloud, System Design, Agile Methodologies, Azure"
Principal Data Architect - Data Engineering,Minfy,Fresher,,"Hyderabad, India",Login to check your skill match score,"Role & Responsibilities
We are seeking a highly skilled and experienced Architect to lead the design, implementation, and optimization Intelligent data Platform. The ideal candidate will have a strong background in cloud-based data processing systems, data warehousing, and big data technologies. They will work closely with our data engineering team to ensure that the data platform is optimized for performance, scalability, and reliability.
Collaborate with stakeholders to understand business objectives and translate them into data architecture requirements.
Design and implement data models, develop Data attribute maps, database schemas, and data integration strategies that comply with regulatory requirements and industry best practices.
Develop and maintain data governance policies and procedures to ensure the confidentiality, integrity, and availability of sensitive data.
Implement data security measures and access controls to protect against unauthorized access and mitigate potential risks.
Architect and optimize data storage and retrieval processes to meet the performance and scalability demands of banking applications.
Leverage AWS services such as Amazon Redshift, Amazon RDS, Amazon Aurora, and AWS Glue to build scalable and cost-effective data solutions.
Architect and design solutions to meet functional and non-functional requirements.
Lead the design, implementation, and optimization of Intelligent Data platform.
Develop and maintain a comprehensive understanding of data pipeline and data architecture.
Develop and maintain documentation for our Intelligent data platform including architecture diagrams, deployment guides, and operational procedures.
Provide guidance and support to our data engineering team.
Create and review architecture and solution design artifacts.
Evangelize re-use through the implementation of shared assets.
Enforce adherence to architectural standards/principles, global product-specific guidelines, usability design standards, etc.
Proactively guide engineering methodologies, standards, and leading practices.
Identify, communicate, and mitigate Risks, Assumptions, Issues, and Decisions throughout the full lifecycle.
Considers the art of the possible, compares various architectural options based on feasibility and impact, and proposes actionable plans.
Demonstrate strong analytical and technical problem-solving skills.
Ability to analyze and operate at various levels of abstraction.
Ability to balance what is strategically right with what is practically realistic.
Growing the Data Engineering business by helping customers identify opportunities to deliver improved business outcomes, designing and driving the implementation of those solutions.
Leading team in the definition of best practices & repeatable methodologies in Cloud Data Engineering, including Data Storage, ETL, Data Integration & Migration, Data Warehousing and Data Governance
Should have Technical Experience in AWS Cloud Data Engineering services and solutions.
Contributing to Sales & Pre-sales activities including proposals, pursuits, demonstrations, and proof of concept initiatives
Evangelizing the Data Engineering service offerings to both internal and external stakeholders
Development of Whitepapers, blogs, webinars and other though leadership material
Development of Go-to-Market and Service Offering definitions for Data Engineering
Expand the business within existing accounts and help clients, by building and sustaining strategic executive relationships, doubling up as their trusted business technology advisor.
Position differentiated and custom solutions to clients, based on the market trends, specific needs of the clients and the supporting business cases.
Build new Data capabilities, solutions, assets, accelerators, and team competencies.
Mandatory Skills Description
- Provide technical leadership and mentorship to junior members.
- Proven experience as a Data Architect with a deep understanding of Enterprise systems and processes.
- Strong proficiency in SQL and database technologies, with experience in designing and optimizing data models for core applications.
- Hands-on experience with AWS cloud services, particularly those relevant to data architecture such as Amazon Redshift, Amazon RDS, AWS Glue, and AWS Lambda.
- Technology Agnostic approach for multitude of data source systems and API gateways
Nice-to-Have Skills
Familiarity with compliance requirements such as GDPR, PII protection frameworks, PCI DSS.
Excellent communication and interpersonal skills, with the ability to effectively collaborate with diverse teams and stakeholders.
AWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified Data Analytics Specialty) are highly desirable.
Minimum Qualifications
Excellent technical architecture skills, enabling the creation of future-proof, complex global Platform solutions.
Excellent interpersonal communication and organizational skills are required to operate as a leading member of global, distributed teams that deliver quality services and solutions.
Ability to rapidly gain knowledge of the organizational structure of the firm to facilitate work with groups outside of the immediate technical team.
Knowledge and experience in IT methodologies and life cycles that will be used.
Familiar with solution implementation/management, service/operations management, etc.
Maintains close awareness of new and emerging technologies and their potential application for service offerings and products.
Bachelor's Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline) or equivalent work experience.
Experience in architecting and designing technical solutions for cloud-centric solutions based on industry standards using IaaS, PaaS, and SaaS capabilities.
Must have strong hands-on experience on various cloud services like Lambda, S3, Security, Monitoring, Governance & Compliance.
Must have good knowledge of Data Engineering concept and related services of cloud.
Must have good experience in Python and Spark.
Must have good experience in setting up development best practices.
Experience with claims-based authentication (SAML/OAuth/OIDC), MFA,RBAC, SSO etc.
Knowledge of cloud security controls including tenant isolation, encryption at rest, encryption in transit, key management, vulnerability assessments, application firewalls, SIEM, etc.
Experience building and supporting mission-critical technology components with DR capabilities.
Experience with multi-tier system and service design and development for large enterprises
Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
Exposure to infrastructure and application security technologies and approaches
Familiarity with requirements gathering techniques.
Preferred Qualifications
Must have experience in DevSecOps working closely on Data Engineering based project
Strong expertise in Data platform component
Delta lake
db API 2.0
SQL Endpoint Photon engine
Delta Sharing
Unity Catalog
Security management
Platform governance, Auditing & Compliance
Data Security
Proficiency in AWS services including but not limited to S3, EC2, IAM, VPC, EKS, Lambda, Glue, Private Link, KMS, CloudWatch, EMR etc.
Must know how to enable geo redundancy and DR capabilities on databricks.
Proficient in designing and implementing
Everything as a code
Infrastructure as a code
Configuration as a code
Configuration as a code
Security configuration as a code
Must have strong expertise in designing platform with strong observability and Monitoring standards.
Proficient in developing and setting best practices of various DevSecOps activities including CI/CD.
Good to have Rest API knowledge.
Good to have understanding around cost distribution.
Good to have if worked on migration project to build Unified data platform.
Good to have knowledge of DBT.
Software development full lifecycle methodologies, patterns, frameworks, libraries, and tools
Knowledge of programming and scripting languages such as JavaScript, Bash, SQL, Python, etc.
Experience in distilling complex technical challenges to actionable decisions for stakeholders and guiding project teams by building consensus and mediating compromises when necessary.
Experience coordinating the intersection of complex system dependencies and interactions
Experience in solution delivery using common methodologies especially SAFe Agile but also Waterfall, Iterative, etc.","Auditing, Compliance, Unity Catalog, SQL Endpoint Photon engine, Delta Sharing, Platform governance, Delta lake, db API 2.0, Data Security, Data Governance, Data Warehousing, Data Architecture, Sql, Security Management, DevSecOps, Data Integration, Spark, Python, Etl"
Data Architect,Gainwell Technologies,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Summary
We're looking for a Solution Architect to join our product development team to drive a cloud based data platform architecture that will power BI and advanced analytics at Gainwell Technologies. You will have a direct impact on the design, architecture, and implementation of BI and data science use cases. The ideal candidate will have industry-leading programming skills and established knowledge of implementing, designing, deploying, and maintaining big data analytics platforms in a cloud environment.
The solution architect will use knowledge of healthcare data to influence the implementation and governance of our data architecture. Your designs will account for data movement, storage, compute and BI consumption, and will comply with security and data governance standards. You will work closely with a variety of partners within the Data and Analytics organization.
Your role in our mission
Provide thought leadership and technical direction to the data engineering team in building analytic data products
Understand and translate business requirements to data strategies that align with overall technology vision
Design, develop and enforce standards for the data storage, processing and governance across all environments
Work closely with enterprise and application architects to align the data engineering team to the overall company SDLC standards, practices, and data access patterns
Develop, document and maintain overall view of data platform architecture, data acquisition, data quality, and data retention
Provide formal and informal training for data engineers, platform engineers and ETL developers
Maintain knowledge of emerging technologies and architectures
Document and publish technical principles and standards, and mentor the data engineering team to incorporate them into their daily practices
Champion and present the technical vision to the executive team and business stakeholders
What we're looking for
Basic Qualifications
Bachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields
8+ years of overall experience in big data, database and enterprise data architecture and delivery
8+ years of programming proficiency in a subset of Python, Java, and Scala
5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks
5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms
3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred
Strong SQL skills to create/maintain DB objects, query/load required data using data governance (e.g. business glossary, data dictionary, data catalog, data quality, master data management, etc.) and visualization tools to bring data literacy to the organization.
Practical experience on workload management, monitoring, and performance tuning Apache Spark jobs
Broad knowledge of data technologies, tools, and disciplines including data modeling, dimensional models, third-normal-form structures, ETL/ELT, change data capture and slowly changing dimensions
Experience with healthcare data a big plus
Experience with Machine Learning & MLOPs is a big plus
What you should expect in this role
Client or office environment / may work remotely
Occasional evening and weekend work
Req. ID: 26672","Java, Machine Learning, Hadoop, Scala, Data Modeling, Sql, ELT, MLops, Spark, Data Governance, Databricks, Python, Etl"
Data Architect - Data Warehousing,ParentPay Group - India,Fresher,,"Pune, India",Login to check your skill match score,"Department: Development
Location: Pune, India
Description
ParentPay Group is Europe's leading software product company and the UK's largest education technology business. We are on a mission to bring next-generation innovation to positively impact on the lives of millions of parents, teachers, and students every day in over 49 countries.
Our market leading products use cutting edge cloud-based technology to streamline school processes, including secure web and mobile apps that enable secure online payments for school items such as meals, trips, clubs and uniform, improve parental engagement, simplify meal management and - through our product SIMS - collect and manage a database of student information and core school operations.
ParentPay Group's new offices in Pune are a fantastic tech hub for those looking to boost their careers in software product development.
Our bright team FastTrack their career with international exposure and ways of working based on agile development best practices from globally renowned technology consultancies.
Key Responsibilities
Responsibilities: Data Architect
Creating data models that specify how data is formatted, stored, and retrieved inside an organisation. This comprises data models that are conceptual, logical, and physical.
Creating and optimising databases, including the selection of appropriate database management systems (DBMS) and the standardisation and indexing of data.
Creating and maintaining data integration processes, ETL (Extract, Transform, Load) workflows, and data pipelines to seamlessly transport data between systems.
Collaborating with business analysts, data scientists, and other stakeholders to understand data requirements and align architecture with business objectives.
Stay current with industry trends, best practices, and advancements in data management through continuous learning and professional development.
Establishing processes for monitoring and improving the quality of data within the organisation.
Implement data quality tools and practices to detect and resolve data issues.
Requirements and Skills: Data Architect
Prior experience in designing Data Warehouse, data modelling, database design, and data administration is required.
Database Expertise: Knowledge of data warehousing ideas and proficiency in various database systems (e.g., SQL).
Knowledge of data modelling tools such as Visual Paradigm is required.
Knowledge of ETL methods and technologies (for example, Azure ADF, Events).
Expertise writing complex stored procedures.
Good understanding of Data Modelling Concepts like Star Schema ,SnowFlake etc
Strong problem-solving and analytical skills are required to build effective data solutions.
Excellent communication skills are required to work with cross-functional teams and convert business objectives into technical solutions.
Knowledge of Data Governance: Understanding data governance principles, data security, and regulatory compliance.
Knowledge of programming languages such as .net can be advantageous.","data administration, Stored procedures, Database Design, .NET, Sql, Data Governance"
Data Architect,Blackbaud,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"About The Role
We are seeking a highly skilled and experienced Data Architect to join our team. The ideal candidate will have at least 12 years of experience in software & data engineering and analytics and a proven track record of designing and implementing complex data solutions. You will be expected to design, create, deploy, and manage Blackbaud's data architecture. This role has considerable technical influence within the Data Platform, Data Engineering teams, and the Data Intelligence Center of Excellence atBlackbaud. Thisindividual acts as an evangelist for proper data strategy with other teams at Blackbaud and assists with the technical direction, specifically with data, of other projects.
What You'll Be Doing
Develop and direct the strategy for all aspects of Blackbaud's Data and Analytics platforms, products and services
Set, communicate and facilitate technical directionmore broadly for the AI Center of Excellence and collaboratively beyond the Center of Excellence
Design and develop breakthrough products, services or technological advancements in the Data Intelligence space that expand our business
Work alongside product management to craft technical solutions to solve customer business problems.
Own the technical data governance practices and ensures data sovereignty, privacy, security and regulatory compliance.
Continuously challenging the status quo of how things have been done in the past.
Build data access strategy to securely democratize data and enable research, modelling, machine learning and artificial intelligence work.
Help define the tools and pipeline patterns our engineers and data engineers use to transform data and support our analytics practice
Work in a cross-functional team to translate business needs into data architecture solutions.
Ensure data solutions are built for performance, scalability, and reliability.
Mentor junior data architects and team members.
Keep current on technology: distributed computing, big data concepts and architecture.
Promote internally how data within Blackbaud can help change the world.
What We Want You To Have
10+ years of experience in data and advanced analytics
At least 8 years of experience working on data technologies in Azure/AWS
Experience building modern products and infrastructure
Experience working with .Net/Java and Microservice Architecture
Expertise in SQL and Python
Expertise in SQL Server, Azure Data Services, and other Microsoft data technologies.
Expertise in Databricks, Microsoft Fabric
Strong understanding of data modeling, data warehousing, data lakes, data mesh and data products.
Experience with machine learning
Excellent communication and leadership skills.
Able to work flexible hours as required by business priorities
Ability to deliver software that meets consistent standards of quality, security and operability.
Stay up to date on everything Blackbaud, follow us on Linkedin, X, Instagram, Facebook and YouTube
Blackbaud is a digital-first company which embraces a flexible remote or hybrid work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!
Blackbaud is proud to be an equal opportunity employer and is committed to maintaining an inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.
R0012317","Microsoft Fabric, Data Mesh, Data Lakes, Azure Data Services, Analytics, Java, Databricks, Sql, SQL Server, data products, data engineering, Data Warehousing, .NET, Machine Learning, AWS, Python, Azure, Microservice Architecture, Data Modeling"
Data Architect,McCain Foods,5-8 Years,,"Delhi, India",Login to check your skill match score,"Position Title: Data Architect
Position Type: Regular - Full-Time
Position Location: New Delhi
Grade: Grade 05
Requisition ID: 34658
Job Purpose
Reporting to the Data Architect Lead, Global Data Architect will take a lead role in creating the enterprise data model for McCain Foods, bringing together data assets across agriculture, manufacturing, supply chain and commercial. This data model will be the foundation for our analytics program that seeks to bring together McCain's industry-leading operational data sets, with 3rd party data sets, to drive world-class analytics. Working with a diverse team of data governance experts, data integration architects, data engineers and our analytics team including data scientist, you will play a key role in creating a conceptual, logical and physical data model that underpins the Global Digital & Data team's activities. .
Job Responsibilities
Develop an understanding of McCain's key data assets and work with data governance team to document key data sets in our enterprise data catalog
Work with business stakeholders to build a conceptual business model by understanding the business end to end process, challenges, and future business plans.
Collaborate with application architects to bring in the analytics point of view when designing end user applications.
Develop Logical data model based on business model and align with business teams
Work with technical teams to build physical data model, data lineage and keep all relevant documentations
Develop a process to manage to all models and appropriate controls
With a use-case driven approach, enhance and expand enterprise data model based on legacy on-premises analytics products, and new cloud data products including advanced analytics models
Design key enterprise conformed dimensions and ensure understanding across data engineering teams (including third parties); keep data catalog and wiki tools current
Primary point of contact for new Digital and IT programs, to ensure alignment to enterprise data model
Be a clear player in shaping McCain's cloud migration strategy, enabling advanced analytics and world-leading Business Intelligence analytics
Work in close collaboration with data engineers ensuring data modeling best practices are followed
Measures Of Success
Demonstrated history of driving change in a large, global organization
A true passion for well-structured and well-governed data; you know and can explain to others the real business risk of too many mapping tables
You live for a well-designed and well-structured conformed dimension table
Focus on use-case driven prioritization; you are comfortable pushing business teams for requirements that connect to business value and also able to challenge requirements that will not achieve the business goals
Developing data models that are not just elegant, but truly optimized for analytics, both advanced analytics use cases and dashboarding / BI tools
A coaching mindset wherever you go, including with the business, data engineers and other architects
A infectious enthusiasm for learning: about our business, deepening your technical knowledge and meeting our teams
Have a get things done attitude. Roll up the sleeves when necessary; work with and through others as needed
Key Qualification & Experiences
Data Design and Governance
At least 5 years of experience with data modeling to support business process
Ability to design complex data models to connect and internal and external data
Nice to have: Ability profile the data for data quality requirements
At least 8 years of experience with requirement analysis; experience working with business stakeholders on data design
Experience on working with real-time data.
Nice to have: experience with Data Catalog tools
Ability to draft accurate documentation that supports the project management effort and coding
Technical skills
At least 5 years of experience designing and working in Data Warehouse solutions building data models; preference for having S4 hana knowledge.
At least 2 years of experience in visualization tools preferably Power BI or similar tools.e
At least 2 years designing and working in Cloud Data Warehouse solutions; preference for Azure Databricks, Azure Synapse or earlier Microsoft solutions
Experience Visio, Power Designer, or similar data modeling tools
Nice to have: Experience in data profiling tools informatica, Collibra or similar data quality tools
Nice to have: Working experience on MDx
Experience in working in Azure cloud environment or similar cloud environment
Must have : Ability to develop queries in SQL for assessing , manipulating, and accessing data stored in relational databases , hands on experience in PySpark, Python
Nice to have: Ability to understand and work with unstructured data
Nice to have at least 1 successful enterprise-wide cloud migration being the data architect or data modeler. - mainly focused on building data models.
Nice to have: Experience on working with Manufacturing /Digital Manufacturing.
Nice to have: experience designing enterprise data models for analytics, specifically in a PowerBI environment
Nice to have: experience with machine learning model design (Python preferred)
Behaviors and Attitudes
Comfortable working with ambiguity and defining a way forward.
Experience challenging current ways of working
A documented history of successfully driving projects to completion
Excellent interpersonal skills
Attention to the details.
Good interpersonal and communication skills
Comfortable leading others through change
McCain Foods is an equal opportunity employer. We see value in ensuring we have a diverse, antiracist, inclusive, merit-based, and equitable workplace. As a global family-owned company we are proud to reflect the diverse communities around the world in which we live and work. We recognize that diversity drives our creativity, resilience, and success and makes our business stronger.
McCain is an accessible employer. If you require an accommodation throughout the recruitment process (including alternate formats of materials or accessible meeting rooms), please let us know and we will work with you to meet your needs.
Your privacy is important to us. By submitting personal data or information to us, you agree this will be handled in accordance with the Global Employee Privacy Policy
Job Family: Information Technology
Division: Global Digital Technology
Department: Data Architect
Location(s): IN - India : Haryana : Gurgaon
Company: McCain Foods(India) P Ltd","Visio, Data Catalog Tools, power designer, Data Profiling Tools, Data Warehouse Solutions, Data Modeling, Power Bi, Pyspark, Azure Databricks, Sql, Azure Synapse, Python"
AI & Data Architect - Azure,CloudFronts - Microsoft Solutions Partner,5-7 Years,,"Mumbai, India",Login to check your skill match score,"AI & Data Architect - Azure
Job Summary:
AI & Data Architect will lead all aspects of AI, Business Intelligence, and Data Architecture with expertise in Azure services, including AI/ML solutions, data engineering, and analytics. The role involves designing, configuring, and managing BI services, aggregating data from multiple sources into a data warehouse, implementing AI-driven insights, and deploying AI models while adhering to best practices.
Skills, Experience & Key Responsibilities :
5+ years of experience in AI, Data Engineering, and Analytics with Azure cloud technologies.
5+ years of integration experience with the Azure Platform (Azure Data Factory, Azure Data Lake, Azure Synapse, Azure Databricks, Azure Machine Learning, Azure Cognitive Services, Logic Apps, API Management).
Hands-on experience with AI/ML models, NLP, Computer Vision, and Predictive Analytics using Azure AI services.
Architect and manage AI-driven BI solutions (portals, dashboards, standard, and ad-hoc reporting) with a focus on data management and AI-driven insights.
Experience in developing and deploying AI models into production environments.
Deep analytical experience and understanding of data modeling and big data solutions.
Strong knowledge of requirements gathering, documentation processes, and stakeholder management.
Proficiency in programming languages such as Python, R, or SQL for AI/ML model development.
Designing & developing dimensions, hierarchies & cubes with Azure Synapse & Databricks.
Provide technical direction and mentoring to a team of AI, BI, and Data Engineers working with enterprise data tools (SSRS, SSIS, SQL Server, Power BI, Azure ML, Databricks).
Ensures best practices in AI model development, testing, and deployment while overseeing, planning, and estimating project needs.
Provide technical leadership on client AI & Data projects and during the sales cycle.
Location: Mumbai
Job Type: Full Time (5days WFO)
Working Hours: 8.30 am to 5.00 pm (Monday to Friday)
Experience Range: 5+ years of experience (Relevant)
ABOUTCLOUDFRONTS:
CloudFronts is a 100% Dynamics 365 focused Microsoft Solutions Partner helping Teams & Organizations worldwide solve their Complex Business Challenges with Microsoft Cloud. Our head office and robust delivery center are based out of Mumbai, India along with branch offices in Singapore & U.S.
CloudFronts was established in 2012 by a former Microsoft CRM Solution Architect Anil Shah with a mission to help other businesses scale up their productivity and reduce their costs concurrently with Microsoft Dynamics. Since its inception, the CloudFronts team has successfully served over 500+ small and medium-sized clients all over the world such as North America, Europe, Australia, Maldives & India with diverse experiences in sectors ranging from Professional services, Finances, Pharmaceutical, Manufacturing, F&B, Retail, Logistics, Energy, Automotive and non-profits.
Our customer success stories and testimonials speak for us. We urge you to look at https://www.cloudfronts.com/dynamics-365-customer-success-stories/
Explore the power of Microsoft Dynamics at www.cloudfronts.com","Analytics, Azure Cognitive Services, R, Logic Apps, AI ML solutions, Sql, SQL Server, SSIS, Computer Vision, Azure Data Factory, Ssrs, Azure Machine Learning, Power Bi, Azure Databricks, data engineering, Python, Azure Synapse, Azure, Api Management, Azure Data Lake, Nlp, Predictive Analytics"
Data Architect (Snowflake),P99SOFT,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"We are seeking a highly motivated and experienced Data Architect to join our growing team. The ideal candidate will have a strong understanding of data warehousing principles and extensive experience with Snowflake, a leading cloud-based data warehouse platform.
Responsibilities
Design, develop, and implement data warehouse solutions using Snowflake for various business needs.
Lead the migration of existing data pipelines and projects from legacy systems to Snowflake.
Develop and maintain ETL processes (Extract, Transform, Load) using DBT (Data Build Tool) and/or Informatica Power Center.
Ensure data quality through the implementation of robust data quality rules and monitoring frameworks.
Own and manage Snowflake administration activities for assigned projects.
Collaborate with business users to understand their requirements and translate them into actionable data solutions.
Mentor and guide team members on best practices for data warehousing and Snowflake utilization.
Qualifications
10+ years of experience in data warehousing and data management.
5+ years of experience with Snowflake, including a strong understanding of its architecture, capabilities, and best practices.
Expertise in DBT (Data Build Tool) or Informatica Power Center for building ETL pipelines.
Familiarity with cloud platforms such as AWS S3 and Azure.
Experience with relational databases like Oracle, SQL Server, and MySQL.
Experience with data governance and security best practices.
Excellent communication and collaboration skills.
Strong analytical and problem-solving abilities.
Ability to work independently and as part of a team.
Benefits
Competitive salary and performance-based bonuses.
Comprehensive health insurance.
Paid time off and flexible working hours.
Professional development opportunities and support for continuing education.
Udemy Learning Licenses to enhance skills and stay up-to-date with the latest technology.
P99soft is an equal-opportunity employer
P99soft is an equal-opportunity employer. At P99soft, we are committed to providing equal employment opportunities regardless of job history, disability, gender identity, religion, race, color, caste, marital/parental status, veteran status, or any other special status. We stand against the discrimination of employees and individuals and are proud to be an equitable workplace that welcomes individuals from all walks of life if they fit the designated roles and responsibilities.
Life@P99soft
At P99soft, we believe in creating a work environment that balances professional excellence with personal well-being. Our vibrant office culture is built on mutual respect, inclusivity, and a shared passion for innovation. We host regular team-building activities, workshops, and social events to foster a sense of community and collaboration among our employees. Join us and be part of a team that values your contributions and supports your growth
About Us
P99soft is a leading IT Services and IT Consulting company dedicated to making our customers business operations seamless by providing them with digital services and applications using the latest technologies. We are committed to bringing excellence to all our creations. We need passion, empathy, and being customer-focused to achieve technical excellence. We believe in transparency through open communication and work with honesty and integrity while dealing with our customers and partners.
Our Vision: To lead digital transformation for our customers, delighting them at every step, while empowering our employees and fostering success.
Our Mission: To attain our objectives within a framework of integrity, transparency, and respect for our clients, employees, partners, and the community, fostering mutual trust and sustainable growth.
Journey towards digital transformations begins here. Our talented teams help you make informed decisions, enhance operation efficiencies, create state-of-the-art visual interfaces, and design dynamic workflows to bring a better customer experience. Our engineering teams take complete ownership of assigning the right talent and architecting solutions using the latest technologies.","snowflake, DBT Data Build Tool, Informatica Power Center, MySQL, SQL Server, Azure, Oracle, Aws S3"
AWS Data Architect Lead,Thoucentric,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"About Us
Thoucentric is the Consulting arm of Xoriant, a prominent digital engineering services company with 5000+ employees. We are headquartered in Bangalore with presence across multiple locations in India, US, UK, Singapore & Australia Globally.
As the Consulting business of Xoriant, We help clients with Business Consulting, Program & Project Management, Digital Transformation, Product Management, Process & Technology Solutioning and Execution including Analytics & Emerging Tech areas cutting across functional areas such as Supply Chain, Finance & HR, Sales & Distribution across US, UK, Singapore and Australia. Our unique consulting framework allows us to focus on execution rather than pure advisory. We are working closely with marquee names in the global consumer & packaged goods (CPG) industry, new age tech and start-up ecosystem. Xoriant (Parent entity) started in 1990 and is a Sunnyvale, CA headquartered digital engineering firm with offices in the USA, Europe, and Asia. Xoriant is backed by ChrysCapital, a leading private equity firm. Our strengths are now combined with Xoriant's capabilities in AI & Data, cloud, security and operations services proven for 30 years.
We have been certified as Great Place to Work by AIM and have been ranked as 50 Best Firms for Data Scientists to Work For.
We have an experienced consulting team of over 450+ world-class business and technology consultants based across six global locations, supporting clients through their expert insights, entrepreneurial approach and focus on delivery excellence. We have also built point solutions and products through Thoucentric labs using AI/ML in the supply chain space.
Job Description
Position Overview
We are looking for an experienced AWS Data Engineer to design, develop, and maintain data solutions using core AWS services. The ideal candidate will have hands-on experience with tools like Amazon S3, Redshift, AWS Glue, and DynamoDB, and will build scalable, efficient, and secure data pipelines and architectures. The role also requires strong expertise in PySpark, SQL, and workflow orchestration tools like Airflow.
Key Responsibilities
Data Pipeline Development
Develop and manage ETL/ELT workflows using AWS Glue and PySpark to process large datasets.
Automate data workflows using Apache Airflow and other orchestration tools.
Data Storage and Management
Architect and manage data storage in Amazon S3 , ensuring performance, cost-efficiency, and security.
Create and optimize Amazon Redshift clusters for data warehousing and analytics workloads.
Design scalable NoSQL solutions using Amazon DynamoDB for real-time data needs.
Compute and Serverless
Build and deploy serverless solutions using AWS Lambda for event-driven data processing.
Configure and manage virtual machine instances using Amazon EC2 for custom data processing tasks.
Security and Monitoring
Implement fine-grained access controls with AWS IAM to ensure data security.
Set up monitoring, logging, and alerts using AWS CloudWatch for proactive system health management.
Data Integration and Transformation
Create efficient SQL queries to handle data transformations and analytics.
Integrate and process structured and unstructured data from multiple sources.
Design data models and implement them in Redshift or DynamoDB for optimized query performance.
Collaboration and Optimization
Collaborate with data scientists, analysts, and stakeholders to gather requirements and deliver solutions.
Continuously optimize data processing workflows to improve performance and reduce costs.
Requirements
Required Skills and Qualifications
Core AWS Expertise
Strong knowledge of Amazon S3 , Redshift , AWS Glue , Lambda , DynamoDB , EC2 , IAM , and CloudWatch .
Technical Proficiency
Hands-on experience with PySpark for big data processing.
Proficient in writing complex SQL queries for data manipulation and analysis.
Expertise in workflow orchestration using Apache Airflow or similar tools.
Experience
10+ years in data engineering with a focus on AWS technologies.
Experience in designing scalable, fault-tolerant data pipelines.
Familiarity with data modeling, ETL/ELT, and data warehousing principles.
Soft Skills
Strong problem-solving and analytical skills.
Excellent communication and collaboration abilities.
Ability to manage priorities in a fast-paced environment
Benefits
What a Consulting role at Thoucentric will offer you
Opportunity to define your career path and not as enforced by a manager
A great consulting environment with a chance to work with Fortune 500 companies and startups alike.
A dynamic but relaxed and supportive working environment that encourages personal development.
Be part of One Extended Family. We bond beyond work - sports, get-togethers, common interests etc. Work in a very enriching environment with Open Culture, Flat Organization and Excellent Peer Group.
Be part of the exciting Growth Story of Thoucentric!","Aws Lambda, Amazon Ec2, Amazon S3, Pyspark, Dynamodb, AWS Glue, AWS CloudWatch, Redshift, Sql, Apache Airflow, AWS IAM, AWS"
Principal Cloud Data Architect,Algonomy,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Principal Cloud Data Architect (Big Data, PySpark, Databricks stack)
Job Description
Algonomy is seeking a highly experienced Senior Cloud Data Architect specializing in Azure, Databricks, and Spark to drive digital transformation for clients by designing scalable information architectures. You have technical depth and business knowledge and can drive complex technology discussions which express the value of the these technologies, throughout the lifecycle of the engagement (sales / pre-sales, architecture development, implementation). You will understand the business use cases, research technologies, recommend solutions, define short term-tactical to long term strategic information architecture roadmap, contribute to best practices, and provide architectural guidance to project teams, ensuring high-quality technical solutions within a learning-oriented culture. Partner with customers, leveraging your technical and business acumen to drive complex technology discussions and become a trusted advisor.
Key Responsibilities
Contribute to planning activities, particularly for price/performance engineering, and participate in customer discussions for requirement analysis. You will utilize architecture frameworks (e.g., TOGAF, Zachman) to recommend solutions using various Databricks stack & other Big Data technologies, implementation approaches, and deployment options.
You will perform comparative analysis of technologies and anchor Proof of Concept (PoC) development to validate solutions and mitigate risks
Develop architecture & transition plans which meet functional and non-functional requirements, applying knowledge of multiple build tools and provide expert guidance to project teams on quality, process, and architectural frameworks to enhance technical quality
Collaborate with project teams to resolve complex technical issues.
Contribute to technology and architectural frameworks and deliver impactful presentations to customers showcasing thought leadership. You will also coordinate tasks within teams, providing feedback to ensure deliverables meet standards.
Work with Sales to develop account strategies, establish standard data architectures (Databricks Lakehouse architecture), and build/present reference architectures and demos to prospects. Your focus will be to capture technical wins by consulting on big data, data engineering, and data science projects.
Required Qualifications
Bachelor's degree or equivalent experience.
Minimum 12 years of IT experience, including 7+ years hands-on experience architecting solutions on Azure, Databricks, and Spark.
Experience in handling TBs of data across use cases - batch, streaming, information reporting
5+ years in a customer-facing role (pre-sales, technical architecture, or consulting) with expertise in: big data engineering (e.g., Spark, Hadoop, Kafka, pandas), data warehousing & ETL (e.g., SQL, OLTP/OLAP/DSS), data science/machine learning (nice to have)
Proven experience designing and implementing complex distributed systems, leading and mentoring teams, and fluency in Python, SQL. and
Experience with Master Data Management, ETL, Data Quality, metadata management, data profiling, and handling batch and streaming data.
Extensive experience with CI/CD platforms (e.g., GitLab CI, GitHub Actions, Azure Pipelines, Jenkins).
Debug and development experience of Python
Ability to translate business needs to technical solutions and establish buy-in with stakeholders.
Experienced at designing, architecting, and presenting data systems for customers and managing the delivery of production solutions of those data architectures.","Azure Pipelines, DSS, GitLab CI, GitHub Actions, SQL OLTP, Master Data Management, Machine Learning, Hadoop, Metadata Management, OLAP, Kafka, Azure Databricks, Sql, Big Data Technologies, Jenkins, Data Quality, Pandas, Data Science, Spark, Python, Data Profiling, Etl"
Cloud Data Architect,Go Digital Technology Consulting LLP,10-12 Years,,"Mumbai, India",Login to check your skill match score,"Job Description
At Go Digital Technology Consulting LLP (GDTC), we are redefining the future of data consulting and services. Our expertise spans Data Engineering, Analytics, and Data Science, enabling us to craft cutting-edge cloud data solutions tailored to our clients unique needs. Specializing in AWS, Azure, and Snowflake, we empower organizations to harness the full potential of their data, transforming it into actionable insights that drive impact.
Our team is a dynamic mix of technologists, product managers, thinkers, and architects, unified by the vision to deliver exceptional value. By blending technological expertise with a deep understanding of business needs, we create solutions that not only meet expectations but set new industry benchmarks.
Join us on a transformative journey where your skills and vision will directly shape the future of cloud data architecture.
Role Summary
As a Cloud Data Architect, you will play a pivotal role in designing and implementing scalable, high-performance cloud-native data architectures. You will lead the architecture of complex, large-scale data environments while fostering innovation and delivering robust solutions that drive business value.
Key Technologies / Skills
Mastery of SQL, Python, PySpark, and Shell scripting.
Expertise in data modeling and big data ecosystems (e.g., Hadoop, Hive) and ETL pipelines.
Deep understanding of cloud-native data solutions across AWS, Azure, or GCP, with experience in Snowflake.
Strong knowledge of modern data architectures, including serverless computing, data lakes, and analytics solutions.
Responsibilities
Design and implement innovative cloud-native data architectures, translating business needs into scalable and sustainable solutions.
Lead technology selection, ensuring alignment with client needs and organizational standards.
Develop comprehensive high-level design documents and frameworks to guide project execution.
Architect and optimize ETL pipelines, ensuring efficient data ingestion, transformation, and storage in the cloud.
Champion best practices for data governance, security, and compliance in cloud environments.
Conduct performance tuning and optimization of cloud-based data platforms.
Collaborate with stakeholders to align architectural solutions with business objectives.
Support pre-sales efforts by providing technical insights and creating compelling proposals.
Mentor and coach technical leads, enabling their growth into future data architects.
Stay ahead of emerging technologies and trends, driving continuous innovation in data engineering and architecture.
Required Qualifications
10+ years of experience in data engineering and architecture, with a focus on cloud hyperscalers (AWS, Azure, GCP).
Proven expertise in cloud-native data solutions, including Snowflake and modern data lake architectures.
Advanced proficiency in Python, PySpark, and SQL, with experience in NoSQL databases.
Extensive experience with data warehousing, ETL pipelines, and big data ecosystems.
Strong knowledge of ITIL service management practices and design processes.
Demonstrated leadership and collaboration skills, with the ability to engage diverse stakeholders.
A bachelor's or master's degree in Computer Science, Engineering, or a related field.
Why Join Us
Be part of a forward-thinking organization that values innovation, collaboration, and excellence.
Work on pioneering projects using state-of-the-art technologies in cloud data engineering.
Enjoy competitive compensation, comprehensive benefits, and flexible work arrangements.
Thrive in an environment that encourages personal growth and career advancement.
As a Cloud Data Architect at GDTC, your expertise will shape the future of data solutions, empowering clients to unlock the full potential of their data. Together, let's build a smarter, data-driven world.","ITIL service management practices, NoSQL databases, Analytics Solutions, data lakes, snowflake, ETL pipelines, Serverless Computing, Hadoop, Pyspark, Data Warehousing, Data Modeling, Sql, Hive, Gcp, Shell scripting, Azure, Python, AWS"
Data Architect,NewVision Software,Fresher,,"Pune, India",Login to check your skill match score,"Primary Skills:
Data Engineering using Azure stack - Data Handling, Data Modeling, Data Integration, Data Governance
Azure Data Lake Analytics,
Azure Synapse Analytics Engineering,
Azure Data Factory,
Azure Databricks,
Azure Dataflows,
Power BI(Expert in DAX)
Scala or Python,
Pyspark, Hadoop, Spark, Hive, Kafka, Sqoop
T-SQL,
NoSQL,
Certified Azure Solution Architect
Secondary Skills:
Source code control systems such as GIT, AzureDevops
DevOps Basics.
Key Responsibilities:
1. Designing Solutions related to data to handle data silos of our customers.
2. Creating Data Models as part of solutions to suffice customer's needs while following best practices.
3. Handling Data Integration with structured and Unstructured sources of data.
4. Implementing Data Governance and security on top of the existing DWH to enhance reliability.
5. Monitoring the existing data flows and maintaining them thereby helping to resolve bugs.
6. Collaborating with a team whenever needed and providing business intelligence solutions.
7. Actively participating in PI Planning and assisting the team during Sprints.
8. Work as part of a team to develop Cloud Data and Analytics solutions.","Certified Azure Solution Architect, DevOps Basics, AzureDevops, Azure Synapse Analytics Engineering, Azure Data Lake Analytics, Azure Dataflows, Pyspark, T-sql, Hadoop, Power Bi, Scala, Kafka, Azure Databricks, Nosql, Git, Azure Data Factory, Hive, Sqoop, Spark, Dax, Python"
Freelance Data Architect,Flexing It,Fresher,,"Mumbai, India",Login to check your skill match score,"Our Client is a leading management consulting firm and is looking for a Data Architect Consultant. The client want someone with experience working with banking and core banking systems.
Key Deliverables:-
1. Design the data lake, data products, and data lakehouse for the bank.
2. Design data models, information flow, ingestion, and consumption architecture.
3. Guide a team of data engineers.
4. Collaborate with partners to deliver the designed patterns.
Location:- Mumbai, Onsite
Duration:- 6 Months
Skills Required
1.Hands on experience in Hadoop and Cloudera hadoop
Experience as an architect in distribution on premise
3. Experience with programming languages like UNIX shell scripting, Python etc.
4. Exposure to data modeling, and distributed computing Strong analytical and
problem-solving skills
5. Good communication and collaboration skills
6. Experience working in agile environments
7. Experience working with banking and core banking systems is preferred","Cloudera Hadoop, Distributed Computing, Hadoop, Data Modeling, Unix Shell Scripting, Python"
Senior Data Architect,Sysmedac,Fresher,,"Chennai, India",Login to check your skill match score,"Data Architect
Job location - Dubai
Contract
Job Summary:
We are seeking a highly skilled Senior Data Architect to lead the strategic design, development, and documentation of our enterprise data architecture. The ideal candidate will play a pivotal role in evaluating the current data environment, identifying architectural gaps, and designing a future-state architecture that supports scalable integration, governance, and analytics.
Key Responsibilities:
1. Current State Analysis
Conduct in-depth assessments of existing data assets, systems, flows, and infrastructure.
Document the current state data architecture, including data sources, data models, integration methods, and reporting platforms.
Create detailed data flow diagrams, system interaction maps, and metadata documentation.
2. Gap Analysis & Future-State Design
Analyze gaps between the current and desired future data architecture.
Identify inefficiencies, technical debt, and areas lacking governance or integration.
Define and document a scalable, secure, and business-aligned future state data architecture.
3. Enterprise Data Modeling
Design and maintain the Enterprise Data Model (conceptual, logical, and physical layers).
Ensure alignment between business definitions, data structures, and analytic/reporting requirements.
Collaborate with business and data stewards to validate data domains and relationships.
4. Integration Data Architecture
Design integration architecture for real-time and batch data movement using APIs, ETL/ELT tools, and streaming platforms.
Ensure consistency, performance, and scalability of data flows across systems.
Work with application teams and data engineers to enforce standardized integration practices.
5. Architecture Roadmap & Governance
Develop a comprehensive roadmap to transition from current to target data architecture.
Define milestones, dependencies, tools/platforms, and required skillsets.
Support governance efforts by embedding architectural principles into data policies and standards.
6. Data Quality Framework Design
Define a Data Quality Framework aligned with business rules and operational KPIs.
Identify critical data elements (CDEs), define quality dimensions, thresholds, and remediation processes.
Collaborate with data governance and engineering teams to implement quality checks, dashboards, and monitoring mechanisms.","Streaming Platforms, Data Quality Framework, Enterprise Data Architecture, Apis, Data Modeling, Integration Architecture, ELT, Etl"
Data Architect,Wavicle Data Solutions,5-7 Years,,India,Login to check your skill match score,"Multiple years of hands-on experience working as a data architect on Azure or AWS or Snowflake cloud ecosystems.
Designing & implementing the organization's data architecture, including data models, data flows, and data storage solutions.
Creating and maintaining data dictionaries and data maps to document data sources, data types, and data relationships.
Collaborating with business analysts to understand business requirements and develop data solutions that meet those requirements.
Working with data scientists and data analysts to ensure that data is available, accurate, and accessible for analysis.
Developing data security and privacy policies and procedures to ensure that sensitive data is protected.
Evaluating new data technologies and tools to determine if they are appropriate for the organization's needs.
Working with IT teams to implement data solutions and ensure that they integrate with existing systems and applications.
Providing technical guidance and support to other teams and stakeholders to ensure that they can effectively use and manage
Need to have a strong background in data management, database design, and programming.
Good experience with data modeling and data visualization tools and familiar with data warehousing and business intelligence concepts
AWS certifications, such as AWS Certified Solutions Architect and AWS Certified Data Analytics - Specialty, are preferred.
Snowflake certifications, such as SnowPro Core and SnowPro Advanced, are preferred.
Azure certifications, such as Azure Solutions Architect Expert and Azure Data Engineer Associate, are preferred.
Experience in designing and implementing data solutions on Azure cloud platforms, such as Azure SQL Database, Azure Cosmos DB, and Azure Synapse Analytics.
Experience in designing and implementing data solutions on the Snowflake cloud platform, including data modeling, data flow, and data storage solutions.
Experience in designing and implementing data solutions on the AWS cloud platform, such as AWS Redshift, AWS S3, and AWS RDS.","Business intelligence, snowflake, Data visualization tools, Database Design, Data Security, Data Management, Data Modeling, Programming, Data Warehousing, Azure, AWS"
Staff Specialist IT - PLM Data Architect,Infineon Technologies,8-10 Years,,"Ahmedabad, India",Semiconductor Manufacturing,"As a PLM Data Architect, you will be responsible for designing, implementing, and maintaining the data architecture of our Product Lifecycle Management (PLM) system. You will work closely with cross-functional teams to ensure data consistency, integrity, and quality across the entire product lifecycle. If you have a strong background in data architecture, PLM systems, and a passion for data-driven decision-making, we encourage you to apply for this exciting opportunity.
Job Description
In your new role you will:
Design and Create the framework for managing the organization's enterprise data architecture.
Focus on identifying and using the right tools and technologies, technical methodologies, technical guardrails and guidelines ,integration with broader technical environment, etc.
Design and implement a scalable and flexible data architecture for the PLM system, ensuring data consistency, integrity, and quality across the entire product lifecycle.
Develop and maintain data models, data flows, and data governance policies to ensure data accuracy, completeness, and compliance with industry standards and regulations.
Collaborate with cross-functional teams, including engineering, manufacturing, and quality, to ensure data requirements are met and data is properly integrated across systems.
Develop and maintain data interfaces, APIs, and data migration strategies to ensure seamless data exchange between PLM and other systems.
Ensure data security, access controls, and auditing mechanisms are in place to protect sensitive product data.
Develop and maintain data analytics and reporting capabilities to support business decision-making and product development.
Your Profile
You are best equipped for this task if you have:
Bachelor's or Master's degree (Computer Science or Related) with 8+years of relevant experience.
Proven experience as Data Architecture or in a similar role in an R&D environment.
Experience in implementing data management, data integration and reporting technologies.
Good Knowledge of data governance, data quality, and data security best practices.
Knowledge in enterprise systems like PLM, ERP, MD systems.
Proficiency in data modelling and design.
Experience working with Data Platforms, Data Catalogues, API Management and Event Driven Architecture.
Knowledge of programming languages Python or Java , NoSQL databases, data visualization, data virtualization.
Solid understanding of cloud services, architectures, and storage solutions.
Excellent problem-solving and communication skills.
#WeAreIn for driving decarbonization and digitalization.
As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.
Are you in
We are on a journey to create the best Infineon for everyone.
This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicants experience and skills.
Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.
Click here for more information about Diversity & Inclusion at Infineon.","cloud services architectures, Storage Solutions, NoSQL databases, reporting technologies, data virtualization, Data Platforms, Event Driven Architecture, enterprise systems, Data Catalogues, Data Management, Java, Api Management, Data Security, Data Modelling, Data Architecture, Data Quality, Data Governance, Data Visualization, Data Integration, Python"
Data Architect - Enterprise Architecture,Viasat,Fresher,,"Chennai, India",Login to check your skill match score,"About Us
One team. Global challenges. Infinite opportunities. At Viasat, we're on a mission to deliver connections with the capacity to change the world. For more than 35 years, Viasat has helped shape how consumers, businesses, governments and militaries around the globe communicate. We're looking for people who think big, act fearlessly, and create an inclusive environment that drives positive impact to join our team.
What You'll Do
The Enterprise Architecture team is focused on providing solutions to enable an effective software engineering workforce that can scale to the business needs. This includes exploring how the business needs map to the application portfolio, business processes, APIs, and data elements across the organization. As a member of this team, you will build up a vast knowledge in software development, cloud application engineering, automation, and container orchestration. Our ideal candidate values communication, learning, adaptability, creativity, and ingenuity. They also enjoy working on challenging technical issues and use creative, innovative techniques to develop and automate solutions. This team is focused on providing our executive and business leadership with visibility into how the software organization is functioning and what opportunities lie to transform the business. In this position you will be an integral part of the Enterprise Architecture team and make meaningful impacts in our journey towards digital transformation.
The day-to-day
A Strong Data-Model and High-Quality Data are pre-requisites to provide better insights and enable solid data-driven decision making. This is also key to take advantage of various technological advances in Artificial Intelligence and Machine Learning. Your responsibilities will involve build out of data models for various aspects of our enterprise in conjunction with domain experts. Examples include but are not limited to Network, Capacity, Finance, Business Support Systems etc. Responsibilities also include working with software product teams to improve data quality across the organization.
What You'll Need
Bachelor's degree or higher in Computer Science & Applications, Computer Science and Computer & Systems Engineering, Computer Science & Engineering, Computer Science & Mathematics, Computer Science & Network Security and Math & Computer Science, and/or a related field
Solid understanding of Data Architecture and Data Engineering principles
Experience building out data models
Experience performing data analysis and presenting data in easy to comprehend manner.
Experience in working with Relational Databases, NoSQL, Large Scale Data technologies (Kafka, Big Query, Snowflake etc)
Experience with digital transformation across multiple cloud platforms like AWS and GCP.
Experience in modernizing data platforms especially in GCP is highly preferred.
Partner with members of Data Platform team and others to build out Data Catalog and map to the data model
Detail Oriented to ensure that the catalog represents quality data
Solid communication skills and ability to work on a distributed team
Tenacity to remain focused on the mission and overcome obstacles
Ability to perform hands-on work with development teams and guide them to building necessary data models.
Experience setting up governance structure and changing the organization culture by influence
What Will Help You On The Job
Experience with Cloud Technologies: AWS, GCP, and/or Azure, etc.
Expertise in GCP data services like Cloud Pub/Sub, Dataproc, Dataflow, BigQuery, and related technologies preferred.
Experience with Airflow, DBT and SQL.
Experience with Open-source software like Logstash, ELK stack, Telegraf, Prometheus and OpenTelemetry is a plus.
Passionate to deliver solutions that improve developer experience and promote API-first principles and microservices architecture.
Experience with Enterprise Architecture and related principles
EEO Statement
Viasat is proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, ancestry, physical or mental disability, medical condition, marital status, genetics, age, or veteran status or any other applicable legally protected status or characteristic. If you would like to request an accommodation on the basis of disability for completing this on-line application, please click here.","Large Scale Data technologies, Big Query, Airflow, Relational Databases, OpenTelemetry, dbt, snowflake, Telegraf, Cloud Pub Sub, Data Analysis, data engineering, Enterprise Architecture, Prometheus, Elk Stack, Kafka, Nosql, Data Architecture, AWS, Logstash, Dataproc, Sql, Cloud Technologies, Gcp, DataFlow"
Sr. Data Architect- SnowFlake,DYNE,8-10 Years,,"Chennai, India",Login to check your skill match score,"Job Description:A minimum of 8-10 years of experience in data engineering, encompassing the development and scaling of data
warehouse and data lake platforms.Working hours-8 hours , with a few hours of overlap during EST Time zone. This overlap hours is mandatory asmeetings happen during this overlap hours. Working hours will be 12 PM-9 PM
Responsibilities
Mandatory Skills: Snowflake experiance, Data Architectureexperiance, ETL process experiance, Large Datamigration solutioning experiance
Lead the design and architecture of data solutions leveraging Snowflake, ensuring scalability, performance, andreliability.
Collaborate with stakeholders to understand business requirements and translate them into technicalspecifications and data models.
Develop and maintain data architecture standards, guidelines, and best practices, including data governanceprinciples and DataOps methodologies.
Oversee the implementation of data pipelines, ETL processes, and data governance frameworks withinSnowflake environments.
Provide technical guidance and mentorship to data engineering teams, fostering skill development andknowledge sharing.
Conduct performance tuning and optimization of Snowflake databases and queries.
Stay updated on emerging trends and advancements in Snowflake, cloud data technologies, data governance ,and Data Ops practices.
Primary Skills:
Extensive experience in designing and implementing data solutions using Snowflake. DBT,
Proficiency in data modeling, schema design, and optimization within Snowflake environments.
Strong understanding of cloud data warehousing concepts and best practices, particularly with Snowflake. Expertise in Dimension Modeling is a must
Expertise in python/java/scala, SQL, ETL processes, and data integration techniques, with a focus on Snowflake.
Familiarity with other cloud platforms and data technologies (e.g., AWS, Azure, GCP )
Demonstrated experience in implementing data governance frameworks and DataOps practices.
Working experience in SAP environments
Familiarity with realtime streaming technologies and Change Data Capture (CDC) mechanisms.
Knowledge of data governance principles and DataOps methodologies
Proven track record of architecting and delivering complex data solutions in cloud platforms/ Snowflake.
SecondarySkills (If Any):
Experience with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data security and compliance standards
Excellent communication and presentation skills, with the ability to convey complex technical concepts tojuniors, non-technical stakeholders.
Strong problem-solving and analytical skills-Ability to work effectively in a collaborative team environment andlead cross-functional initiatives.
EducationalQualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or related field.
Certifications Required(If Any):
Certifications related Snowflake (e.g., SnowPro core/Snowpro advanced Architect/Snowpro advance Data
Engineer ) are desirable but not mandatory.","snowflake, Data governance frameworks, Data integration techniques, Dimension Modeling, ETL processes, DataOps practices, Sql, Data Modeling, Data Architecture, Java, Schema Design, AWS, Python, Azure, Gcp, Scala"
Data Architect,CG-VAK Software & Exports Ltd.,10-15 Years,,India,Login to check your skill match score,"Company Size
Mid-Sized
Experience Required
10 - 15 years
Working Days
5 days/week
Office Location
Remote Working
Role & Responsibilities
Lead and mentor a team of data engineers, ensuring high performance and career growth.
Architect and optimize scalable data infrastructure, ensuring high availability and reliability.
Drive the development and implementation of data governance frameworks and best practices.
Work closely with cross-functional teams to define and execute a data roadmap.
Optimize data processing workflows for performance and cost efficiency.
Ensure data security, compliance, and quality across all data platforms.
Foster a culture of innovation and technical excellence within the data team.
Ideal Candidate
10+ years of experience in software/data engineering, with at least 3+ years in a leadership role.
Expertise in backend development with programming languages such as Java, PHP, Python, Node.JS, GoLang, JavaScript, HTML, and CSS.
Proficiency in SQL, Python, and Scala for data processing and analytics.
Strong understanding of cloud platforms (AWS, GCP, or Azure) and their data services.
Strong foundation and expertise in HLD and LLD, as well as design patterns, preferably using Spring Boot or Google Guice
Experience in big data technologies such as Spark, Hadoop, Kafka, and distributed computing frameworks.
Hands-on experience with data warehousing solutions such as Snowflake, Redshift, or BigQuery
Deep knowledge of data governance, security, and compliance (GDPR, SOC2, etc.).
Experience in NoSQL databases like Redis, Cassandra, MongoDB, and TiDB.
Familiarity with automation and DevOps tools like Jenkins, Ansible, Docker, Kubernetes, Chef, Grafana, and ELK.
Proven ability to drive technical strategy and align it with business objectives.
Strong leadership, communication, and stakeholder management skills.
Preferred Qualifications
Experience in machine learning infrastructure or MLOps is a plus.
Exposure to real-time data processing and analytics.
Interest in data structures, algorithm analysis and design, multicore programming, and scalable architecture.
Prior experience in a SaaS or high-growth tech company.
Perks, Benefits and Work Culture
Testimonial from a designer:One of the things I love about the design team at Wingify is the fact that every designer has a style which is unique to them. The second best thing is non-compliance to pre-existing rules for new products. So I just don't follow guidelines, I help create them.
Skills: scala,hld,hadoop,kafka,python,javascript,lld,big data technologies,azure,sql,gdpr,google guice,kubernetes,gcp,soc2,compliance,aws,chef,data warehousing,docker,apache,php,leadership,golang,ansible,redshift,data,tidb,mongodb,grafana,machine learning infrastructure,css,cassandra,data architect,backend development,elk,snowflake,spring boot,bigquery,nosql,real-time data processing,html,spark,redis,automation tools,data engineering,devops tools,data governance,node.js,jenkins,cloud platforms,mlops,java","Chef, SOC2, snowflake, TiDB, Hld, Gdpr, Elk, Lld, Cassandra, Kafka, Spring Boot, Grafana, HTML, Javascript, Docker, Php, Python, AWS, Java, BigQuery, Hadoop, CSS, Scala, Node.JS, google guice, Redshift, Redis, Sql, Jenkins, Gcp, Ansible, Spark, MongoDB, Azure, Kubernetes, Golang"
Data Architect,DATAECONOMY,15-17 Years,,"Hyderabad, India",Login to check your skill match score,"About Us
About DATAECONOMY: We are a fast-growing data & analytics company headquartered in Dublin with offices inDublin, OH, Providence, RI, and an advanced technology center in Hyderabad,India. We are clearly differentiated in the data & analytics space via our suite of solutions, accelerators, frameworks, and thought leadership.
Job Description
15+ years of professional experience in the field of Data Architecture, Design and Development of Data Lifecycle Management (DLM) and Data Governance projects
Experience should include hands-on experience in implementing a Customer Data Hub Customer Information Management system or Customer Data Mart which includes Modeling Customer Data Model in the 3NF / Dimensional Model, defining and developing integration patterns like Batch and real-time, defining & developing the physical storage layer, defining & developing the consumption pattern
Experience with Data Modeling is a MUST, experience in defining standards and data architecture best practices is a must
Experience with Data Management/Architecture Practices like Data Integration, Data Governance, Data Quality, Metadata management, Data Security, Data Encryption etc. is a must
Must have hands-on experience with developing large Data Mart or Data warehouse or Data hub involving data modelling, data ingestion, data processing and extract generation to the downstream systems
Must have experience with any ETL tool like Informatica or Data Stage or other for data processing requirements.
Experience with Identity resolution products for Name and Address standardization like Informatica IDQ / Address Doctor /IBM Quality stage is required.
Experience with any Data Quality product like Informatica IDQ, Quality Stage or Trillium will be an added advantage
Must have strong experience with writing SQL for pulling and analyzing source/data platforms
Experience with Data Science models, model validation, model tuning and management will be an added advantage.
Proactively communicate and collaborate with business stakeholders to understand the business requirements, translate them into design, and develop the respective module based on the requirements.
Must have experience working in an Agile implementation set-up with Product Backlogs, User Stories, Scrum and sprint planning etc.
Strong verbal and written communication and English language skills
Strong consulting skills and consulting experience are strongly desired.
Requirements
Developing Data Architecture and Design documents for Data Lake, Data warehouse & Data Marts corresponding to Customer Information Management Systems
Experience in Data Lifecycle Management (DLM)
Configuring Solutions and Coding any customization required for the Data platform including Data Lake, Data warehouse and Data Marts
Working with the clients to understand the requirements. Develop the required codebase for the functional needs
Develop Source to Target mapping document for all the attributes that are required to be captured for Data Lake, Data warehouse and Data Marts
Configure and develop code required for Upstream and downstream system communication in a Batch and real-time mode
Provide clarifications for any questions that the Business / SME team or any other stakeholder has on Data implementations.
Participate in system and acceptance testing along with the stakeholders
Benefits
Standard Company Benefits","Agile implementation, Data Encryption, Customer Data Hub, Data Science models, Customer Data Mart, Customer Information Management, Metadata Management, Data Modeling, Data Integration, Sql, Data Quality, Data Architecture, Data Security, Data Governance"
Technical Architect- (Data Architect),Simpplr,8-10 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Who We Are
Simpplr is the AI-powered platform that unifies the digital workplace bringing together engagement, enablement, and services to transform the employee experience. It streamlines communication, simplifies interactions, automates workflows, and elevates the everyday experience of work. The platform is intuitive, highly extensible, and built to integrate seamlessly with your existing technology.
More than 1,000 leading organizations including AAA, the NHS, Penske, and Moderna trust Simpplr to foster a more aligned and productive workforce. Headquartered in Silicon Valley with global offices, Simpplr is backed by Norwest Ventures, Sapphire Ventures, Salesforce Ventures, and Tola Capital. Learn more at simpplr.com.
Job Title: Technical Architect - Analytics
Location: Gurgaon Or Bangalore - Hybrid India
Employment Type: Full-Time
The opportunity
We are looking for a hands-on Technical Architect Analytics who will be responsible for designing, developing, and optimizing our data and analytics architecture. You will play a critical role in defining the data strategy, designing scalable data pipelines, and implementing best practices for real-time and batch analytics solutions. This role requires a strong technical leader who is passionate about data engineering, analytics, and driving data-driven decision-making across the organization.
Key Responsibilities
Data Architecture & Design: Define and own the architecture for data processing, analytics, and reporting systems, ensuring scalability, reliability, and performance.
Data Engineering: Design and implement highly efficient, scalable, and reliable data pipelines for structured and unstructured data.
Big Data & Real-Time Analytics: Architect and optimize data processing workflows for batch, real-time, and streaming analytics.
Cross-Functional Collaboration: Work closely with Product Managers, Data Scientists, Analysts, and Software Engineers to translate business requirements into scalable data architectures.
Technology & Best Practices: Stay ahead of industry trends, introduce modern data technologies, and drive best practices in data architecture, governance, and security.
Code Reviews & Mentorship: Review code, enforce data engineering best practices, and mentor engineers to build a high-performance analytics team.
Data Governance & Compliance: Ensure data security, integrity, and compliance with regulations (GDPR, CCPA, etc.).
Optimization & Performance Tuning: Identify performance bottlenecks in data pipelines and analytics workloads, optimizing for cost, speed, and efficiency.
Cloud & Infrastructure: Lead cloud-based data platform initiatives, ensuring high availability, fault tolerance, and cost optimization.
What Makes You a Great Fit for Us
Experience: 8+ years of experience in data architecture, analytics, and big data processing.
Proven Track Record: Experience designing and implementing end-to-end data platforms for high-scale applications.
Strong Data Engineering Background: Expertise in ETL/ELT pipelines, data modeling, data warehousing, and stream processing.
Analytics & Reporting Expertise: Experience working with BI tools, data visualization, and reporting platforms.
Deep Knowledge of Modern Data Technologies:
Big Data & Analytics: Spark, Kafka, Hadoop, Druid, ClickHouse, Presto, Snowflake, Redshift, BigQuery.
Databases: PostgreSQL, MongoDB, Cassandra, ElasticSearch.
Cloud Platforms: AWS, GCP, Azure (experience with cloud data warehouses like AWS Redshift, Snowflake is a plus).
Programming & Scripting: Python, SQL, Java, Scala.
Microservices & Event-Driven Architecture: Understanding of real-time event processing architectures.
Strategic Thinking: Ability to design and implement long-term data strategies aligned with business goals.
Problem-Solving & Optimization: Strong analytical skills with a deep understanding of performance tuning for large-scale data systems.
Visionary Leadership: Ability to think strategically and drive engineering excellence within the team.
Communication Skills: Strong interpersonal and communication skills to collaborate effectively across teams.
Attention to Detail: An eye for detail with the ability to translate ideas into tangible, impactful outcomes.
Agility: Comfortable managing and delivering work in a fast-paced, dynamic environment.
Preferred Skills (Good To Have)
Hands-on experience with AWS Public Cloud.
Experience with Machine Learning Pipelines and AI-driven analytics.
Hands-on experience with Kubernetes, Terraform, and Infrastructure-as-Code (IaC) for data platforms.
Certifications in AWS Data Analytics, Google Professional Data Engineer, or equivalent.
Experience with data security, encryption, and access control mechanisms.
Experience in Event/Data Streaming platforms
Experience in risk management and compliance frameworks
Simpplr's Hub-Hybrid-Remote Model
At Simpplr we believe that when work is good, life is better and that belief guides all we do. Including how we approach our flexible work model. Simpplr operates with a Hub-Hybrid-Remote model. This model is role-based with exceptions and provides employees with the flexibility that many have told us they want.
Hub - 100% work from Simpplr office. Role requires Simpplifier to be in the office full-time.
Hybrid - Hybrid work from home and office. Role dictates the ability to work from home, plus benefit from in-person collaboration on a regular basis.
Remote - 100% remote. Role can be done anywhere within your country of hire, as long as the requirements of the role are met.","Event-Driven Architecture, stream processing, Druid, Real-Time Analytics, snowflake, ClickHouse, Data Architecture Design, Bi Tools, data engineering, Data Modeling, Cassandra, PostgreSQL, Kafka, ELT, Microservices, Elasticsearch, Python, AWS, Java, BigQuery, Hadoop, Scala, Big Data, Redshift, Sql, Gcp, Presto, Spark, Data Visualization, Data Warehousing, MongoDB, Azure, Etl"
Sr Data Architect,HMH Tech India,5-7 Years,,"Pune, India",Login to check your skill match score,"HMH is a learning technology company committed to delivering connected solutions that engage learners, empower educators and improve student outcomes. As a leading provider of K12 core curriculum, supplemental and intervention solutions, and professional learning services, HMH partners with educators and school districts to uncover solutions that unlock students potential and extend teachers capabilities.
HMH serves more than 50 million students and 4 million educators in 150 countries. HMH Technology India Pvt. Ltd. is our technology and innovation arm in India focused on developing novel products and solutions using cutting-edge technology to better serve our clients globally. HMH aims to help employees grow as people, and not just as professionals. For more information, visit www.hmhco.com
The data architect is responsible for designing, creating, and managing an organization's data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, accessible, secure, and aligned with business objectives. The data architect designs data models, warehouses, file systems and databases, and defines how data will be collected and organized.
Responsibilities
Interprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps
Designs the structure and layout of data systems, including databases, warehouses, and lakes
Selects and designs database management systems that meet the organization's needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures
Defines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms
Designs processes for the ETL process from various sources into the organization's data systems
Translates high-level business requirements into data models and appropriate metadata, test data, and data quality standards
Manages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps
Simplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company
Leads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums
Defines and manages standards, guidelines, and processes to ensure data quality
Works with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions
Evaluates and recommends emerging technologies for data management, storage, and analytics
Design, create, and implement logical and physical data models for both IT and business solutions to capture the structure, relationships, and constraints of relevant datasets
Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions
Effectively collaborate and communicate with various stakeholders to understand data and business requirements and translate them into data models
Create entity-relationship diagrams (ERDs), data flow diagrams, and other visualization tools to represent data models
Collaborate with database administrators and software engineers to implement and maintain data models in databases, data warehouses, and data lakes
Develop data modeling best practices, and use these standards to identify and resolve data modeling issues and conflicts
Conduct performance tuning and optimization of data models for efficient data access and retrieval
Incorporate core data management competencies, including data governance, data security and data quality
Education
Job Requirements
A bachelor's degree in computer science, data science, engineering, or related field
Experience
At least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives
Experience leading projects involving data warehousing, data modeling, and data analysis
Design experience in Azure Databricks, PySpark, and Power BI/Tableau
Skills
Ability in programming languages such as Java, Python, and C/C++
Ability in data science languages/tools such as SQL, R, SAS, or Excel
Proficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)
Experience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata
Understanding of entity-relationship modeling, metadata systems, and data quality tools and techniques
Ability to think strategically and relate architectural decisions and recommendations to business needs and client culture
Ability to assess traditional and modern data architecture components based on business needs
Experience with business intelligence tools and technologies such as ETL, Power BI, and Tableau
Ability to regularly learn and adopt new technology, especially in the ML/AI realm
Strong analytical and problem-solving skills
Ability to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings
Ability to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders
Ability to guide solution design and architecture to meet business needs
Expert knowledge of data modeling concepts, methodologies, and best practices
Proficiency in data modeling tools such as Erwin or ER/Studio
Knowledge of relational databases and database design principles
Familiarity with dimensional modeling and data warehousing concepts
Strong SQL skills for data querying, manipulation, and optimization, and knowledge of other data science languages, including JavaScript, Python, and R
Ability to collaborate with cross-functional teams and stakeholders to gather requirements and align on data models
Excellent analytical and problem-solving skills to identify and resolve data modeling issues
Strong communication and documentation skills to effectively convey complex data modeling concepts to technical and business stakeholders
HMH Technology Private Limited is an Equal Opportunity Employer and considers applicants for all positions without regard to race, colour, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. We are committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit https://careers.hmhco.com/ . Follow us on Twitter, Facebook, LinkedIn, and YouTube.","snowflake, R, Teradata, Tableau, Databricks, Sql, Java, Excel, Er Studio, SAS, DataFlow, Hadoop, Erwin, Pyspark, Kafka, Power Bi, Etl, Azure Databricks, AWS, Oracle, Python, Nosql, Azure, Gcp"
Data Architect,VLink Inc,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"VLink Inc Hiring: Data Architect | Bangalore | Hybrid Work Mode
We're looking for expert Data Architects with 1215 years of experience to join us immediately in Bangalore.
We are seeking experienced Data Architects to design and implement enterprise data solutions, ensuring data governance, quality, and advanced analytics capabilities. The ideal candidate will have expertise in defining data policies, managing metadata, and leading data migrations from legacy systems to Microsoft Fabric/DataBricks/Snowflake. Experience and deep knowledge about at least one of these 3 platforms is critical. Additionally, they will play a key role in identifying use cases for advanced analytics and developing machine learning models to drive business insights.
Key Responsibilities:
1. Data Governance & Management Establish and maintain a Data Usage Hierarchy to ensure structured data access. Define data policies, standards, and governance frameworks to ensure consistency and compliance. Implement Data Quality Management practices to improve accuracy, completeness, and reliability. Oversee Metadata and Master Data Management (MDM) to enable seamless data integration across platforms.
2. Data Architecture & Migration Lead the migration of data systems from legacy infrastructure to Microsoft Fabric. Design scalable, high-performance data architectures that support business intelligence and analytics. Collaborate with IT and engineering teams to ensure efficient data pipeline development.
3. Advanced Analytics & Machine Learning Identify and define use cases for advanced analytics that align with business objectives. Design and develop machine learning models to drive data-driven decision-making. Work with data scientists to operationalize ML models and ensure real-world applicability.
Required Qualifications:
Proven experience as a Data Architect or similar role in data management and analytics. Strong knowledge of data governance frameworks, data quality management, and metadata management. Hands-on experience with Microsoft Fabric and data migration from legacy systems. Expertise in advanced analytics, machine learning models, and AI-driven insights. Familiarity with data modelling, ETL processes, and cloud-based data solutions (Azure, AWS, or GCP). Strong communication skills with the ability to translate complex data concepts into business insights.
Preferred candidate profile
Data Modeling (Conceptual, Logical, Physical)- Minimum 5 years
Database Technologies (SQL Server, Oracle, PostgreSQL, NoSQL)- Minimum 5 years
Cloud Platforms (AWS, Azure, GCP) - Minimum 3 Years
ETL Tools (Informatica, Talend, Apache Nifi) - Minimum 3 Years
Big Data Technologies (Hadoop, Spark, Kafka) - Minimum 5 Years
Data Governance & Compliance (GDPR, HIPAA) - Minimum 3 years
Master Data Management (MDM) - Minimum 3 years
Data Warehousing (Snowflake, Redshift, BigQuery)- Minimum 3 years
API Integration & Data Pipelines - Good to have.
Performance Tuning & Optimization - Minimum 3 years
business Intelligence (Power BI, Tableau)- Minimum 3 years
Interested candidate can share their updated profile on below mentioned mail:-
[HIDDEN TEXT]
Regards As Ever
Ankit Malik","snowflake, Microsoft Fabric, Master Data Management, Data Quality Management, Oracle, Apache Nifi, Hipaa, Data Warehousing, Hadoop, Tableau, Kafka, BigQuery, Power Bi, Etl, Nosql, Informatica, PostgreSQL, SQL Server, Gdpr, Metadata Management, Machine Learning, Talend, Data Governance, Data Modeling, Redshift, Spark, DataBricks"
Azure Data Architect,Veracity Software Inc,10-12 Years,,"Pune, India",Login to check your skill match score,"Job Details:
Position: Data Architect
Experience: 10-12 years
Work Mode: Onsite
Location: Pune
Notice Period: Immediate
Job Responsibilities:
The ideal profile should have a strong foundation in data concepts, design, and strategy, with the ability to
work across diverse technologies in an agnostic manner.
Transactional Database Architecture
Design and implement high-performance, reliable, and scalable transactional database architectures.
Collaborate with cross-functional teams to understand transactional data requirements and create
solutions that ensure data consistency, integrity, and availability.
Optimize database designs and recommend best practices and technology stacks.
Oversee the management of entire transactional databases, including modernization and de-
duplication initiatives.
Data Lake Architecture
Design and implement data lakes that consolidate data from disparate sources into a unified, scalable
storage solution.
Architect and deploy cloud-based or on-premises data lake infrastructure.
Ensure self-service capabilities across the data engineering space for the business.
Work closely with Data Engineers, Product Owners, and Business teams.
Data Integration & Governance:
Understand ingestion and orchestration strategies.
Implement data sharing, data exchange, and assess data sensitivity and criticality to recommend
appropriate designs.
Basic understanding of data governance practices.
Innovation
Evaluate and implement new technologies, tools, and frameworks to improve data accessibility,
performance, and scalability.
Stay up to date with industry trends and best practices to continuously innovate and enhance the data
architecture strategy.","Data Lake Architecture, Data accessibility, Data governance practices, Transactional Database Architecture, Data Integration, Scalability"
Senior Manager - Enterprise Data Architect,METRO Global Solution Center IN,10-12 Years,,"Pune, India",Login to check your skill match score,"Metro Global Solution Center (MGSC) is internal solution partner for METRO, a 30.5 Billion international wholesaler with operations in 31 countries through 625 stores & a team of 93,000 people globally. Metro operates in a further 10 countries with its Food Service Distribution (FSD) business and it is thus active in a total of 34 countries.
MGSC, location wise is present in Pune (India), Dsseldorf (Germany) and Szczecin (Poland). We provide HR, Finance, IT & Business operations support to 31 countries, speak 24+ languages and process over 18,000 transactions a day. We are setting tomorrow's standards for customer focus, digital solutions, and sustainable business models. For over 10 years, we have been providing services and solutions from our two locations in Pune and Szczecin. This has allowed us to gain extensive experience in how we can best serve our internal customers with high quality and passion. We believe that we can add value, drive efficiency, and satisfy our customers.
Website: https://www.metro-gsc.in
Company Size: 600-650
Headquarters: Pune, Maharashtra, India
Type: Privately Held
Inception: 2011
Job Description
Role & Responsibility:
Own the data Architecture Principles; enterprise data flow, know how on enterprise data objects and their dependency to business processes;
Contribute to the Data Strategy;
Co-own the enterprise data model on the Conceptional Level;
Own the enterprise data model on Logical Level;
Consult and oversee solution architect on the Physical Level;
Collaborate with data governance, Quality, Security, Privacy to define the data architectural principles.
Should Be able to communicate the value of data on all levels and be actively shaping how Metro is handling data today and tomorrow;
Have strong communication skills as a key task is to connect people from different business units and different roles;
Be able to work conceptually and be able to communicate the impact of this type of work;
Be able to define and foster data architecture principles;
Own enterprise data flow (safeguard + design new when tech is introduced/decommissioned), know and understand enterprise data objects and the processes that are supported by data, design migration strategy in large scale projects
Collaborate with business process architects and enterprise architecture to define the data architecture principles and act as quality gate in change and run the business;
Qualifications
Bachelors in the domain of computer science.
Have minimum 10+ years of experience in the data field (engineering and data architecting);
Have minimum 2 years of experience in the position of Enterprise Data Architect or minimum 3 years experience in the position of Data Architect conducting large data transformation programs;
Have a considerable interest in the business processes and business challenges a company like Metro is facing; experience in the wholesale/retail domain is a plus but not mandatory;
Have deep knowledge in data processing technologies; data models and interdependencies between data objects;
Understanding /know how of different database technologies and public cloud Have a consulting mindset;
Should be familiar with agile software delivery by teams across various locations.","Data Strategy, agile software delivery, data models, data architecture principles, enterprise data flow, enterprise data model, data processing technologies, Data Governance, Database Technologies, Public Cloud"
Big Data Architect,Airtel Digital,8-17 Years,,"Pune, India",Login to check your skill match score,"Key Responsibilities
Design, build, and maintain scalable big data architectures on Azure and AWS - Select and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )
Lead data migration from legacy systems to cloud-based solutions - Develop and optimize ETL pipelines and data processing workflows.
Ensure data infrastructure meets performance, scalability, and security requirements.
Collaborate with development teams to implement microservices and backend solutions for big data applications.
Oversee the end-to-end SDLC for big data projects, from planning to deployment.
Mentor junior engineers and contribute to architectural best practices.
Prepare architecture documentation and technical reports.
Required Skills & Qualifications
Bachelor's/Master's degree in Computer Science, Engineering, or related field.
817 years of experience in big data and cloud architecture.
Proven hands-on expertise with Azure and AWS big data services (e.g., Azure Synapse, AWS Redshift, S3, Glue, Data Factory).
Strong programming skills in Python, Java, or Scala[9].
Solid understanding of SDLC and agile methodologies.
Experience in designing and deploying microservices, preferably for backend data systems.
Knowledge of data storage, database management (relational and NoSQL), and data security best practices
Excellent problem-solving, communication, and team leadership skills","Glue, Java, S3, Aws Redshift, Hadoop, Scala, Kafka, Microservices, Nosql, Azure Synapse, Azure Data Factory, Database Management, Spark, Data Security, Azure, Python, AWS"
Data Architect,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Responsibilities:
Design, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.
Enhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.
Implement best practices for data management, storage and security to ensure data integrity and compliance with regulations.
Own the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.
Participate in code reviews to ensure code quality and share knowledge.
Lead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.
Define and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.
Mentor junior members of the team, providing guidance and support in their professional development.
Collaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration
A little more about you:
Bachelor's degree or higher in Computer Science, Engineering, or a related field.
10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.
Proficient in SQL and Python, with the ability to translate complexity into efficient code.
Experience with data workflow development and management tools (dbt, Airflow).
Solid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.
Strong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Experience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Gcp, Azure, Python, Sql, AWS"
Data Architect,Impelsys,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Software Skills
Technical Skill Data Architect, ETL, Data Engineer, Python, SQL and Any
Cloud
Job Description
Develop architectural strategies for data modelling, design and implementation to meet stated requirements for metadata management, master data management, Data warehouses, ETL and ELT.
Analyzing business requirements, designing scalable/ robust data models, documenting conceptual, logical & physical data model design, helping developers in development/ creating DB structures and supporting developers throughout the project life cycle.
Lead and Mentor Data Engineers: This role will be responsible for leading and developing a team of data engineers focused on the growth in the team's skills and ability to execute as a team using DevOps and Data Ops principles.
Investigate new technologies, data modelling methods and information management systems to determine which ones should be incorporated onto data architectures, and develop implementation timelines and milestones.
Recognizes and resolves conflicts between models, ensuring that information and data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions).
Participates in the design of the information architecture: supports projects, reviews information elements including models, glossary, flows, data usage.
Provides guidance to the team in achieving the project goals/milestones.
Works independently within broad guidelines and policies, with guidance in only the most complex situations.
Contribute as an expert to multiple delivery teams, defining best practices, building reusable design & components, capability building, aligning industry trends and actively engaging with wider data communities.
Candidate Profile
10+ years of relevant experience in Data modelling for DW & analytics applications / Database related technologies.
Expert data modelling skills (i.e. conceptual, logical and physical model design, experience with Enterprise Data Warehouses and Data Marts).
Solid understanding of cloud database technologies and services (eg..AWS, Azure , Redshift, Aurora, DynamoDB, Snowflake etc).
Experience in data lake technologies involving S3, Databricks Delta Lake, etc.
Experience in working with data governance, data quality, and data security teams.
Experienced in knowledge-driven data processing techniques like data curation, representation, standardization, normalization and any other type of processing that prepares the data for integration, persistence, analysis, exchange/share and so forth.
Experience in handling very large DBs and large data volumes .
Strong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production.
Ability to lead and mentor teams for effective delivery.
Crisp and effective executive communication skills, including significant experience. presenting cross-functionally and across all levels.
Education
Graduate or post graduate in Computer science/Electronics/Software engineering","Aurora, data marts, ETL processes, snowflake, Enterprise Data Warehouses, Delta Lake, Data lake technologies, S3, Data Architect, Data Governance, data curation, data preparation, Database Technologies, Data Integration, Data Engineer, Data Modelling, Python, AWS, Data Security, Dynamodb, Data Quality, Redshift, Sql, Cloud, Databricks, Azure, Etl"
Cloud Data Architect,PURVIEW,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Job Description
A Cloud Data Architect who has experience working on Google Cloud Platform and have built solutions using Data Services of GCP
Experience of 10+ years and who understands the data principles and experience dealing with migrating BI data warehouses, BigData platforms into Google Cloud
Having experience on how to extract, transform, load the data into BI systems and experience with Datawarehousing will help
Experience building solutions using Google BigQuery, Google Dataflow, Google Cloud Storage, Google Pubsub which we use on our data platform
Experience working with business, engineering, data modeling teams in defining the architecture solution
Understands the architecture risks, design principles and suggest ways to business/engineering teams on tactical (vs) strategic solutions depending on various scenarios one would see
About Company :
Purview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.
We have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.
In
Company Info:
3rd Floor, Sonthalia Mind Space
Near Westin Hotel, Gafoor Nagar
Hitechcity, Hyderabad
Phone: +91 40 48549120 / +91 8790177967
Uk
Gyleview House, 3 Redheughs Rigg,
South Gyle, Edinburgh, EH12 9DQ.
Phone: +44 7590230910
Email: [HIDDEN TEXT]
Login to Apply !","Google Dataflow, Google Cloud Storage, Google Pubsub, Google BigQuery, Data Services of GCP, Google Cloud Platform"
Manager_Data Architect,VOIS,Fresher,,"Pune, India",Login to check your skill match score,"Join Us
At Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.
About VOIS
VOIS (Vodafone Intelligent Solutions) is a strategic arm of Vodafone Group Plc, creating value and enhancing quality and efficiency across 28 countries, and operating from 7 locations: Albania, Egypt, Hungary, India, Romania, Spain and the UK.
Over 29,000 highly skilled individuals are dedicated to being Vodafone Group's partner of choice for talent, technology, and transformation. We deliver the best services across IT, Business Intelligence Services, Customer Operations, Business Operations, HR, Finance, Supply Chain, HR Operations, and many more.
Established in 2006, VOIS has evolved into a global, multi-functional organisation, a Centre of Excellence for Intelligent Solutions focused on adding value and delivering business outcomes for Vodafone.
About VOIS India
In 2009, VOIS started operating in India and now has established global delivery centres in Pune, Bangalore and Ahmedabad. With more than 14,500 employees, VOIS India supports global markets and group functions of Vodafone, and delivers best-in-class customer experience through multi-functional services in the areas of Information Technology, Networks, Business Intelligence and Analytics, Digital Business Solutions (Robotics & AI), Commercial Operations (Consumer & Business), Intelligent Operations, Finance Operations, Supply Chain Operations and HR Operations and more.
What You'll Do
Strong understanding of end-to-end impact assessment across all subject areas.
Creating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications
Creating and maintaining data models for databases, data warehouses, and data lakes, defining relationships between data entities to optimize data retrieval and analysis.
Designing and implementing data pipelines to integrate data from multiple sources, ensuring data consistency and quality across systems.
Collaborating with business stakeholders to define the overall data strategy, aligning data needs with business requirements.
Support migration of new & changed software, elaborate and perform production checks
Need to effectively communicate complex data concepts to both technical and non-technical stakeholders.
GCP Knowledge/exp with Cloud Composer, BigQuery, Pub/Sub, Cloud Functions.
Who You Are
E2E Impact assessment should be done for all demands.
Creating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications
Manage database related refresh and decommissioning programs whilst maintaining the highest level of Service availability to Vodafone customers
Assure correct database configuration and proper documentation of all relevant changes within the DB infrastructure
Support supplier's 3rd level and engineering in root cause analysis and remediation of problems in their products
To come up with ideas to enhance the system
VOIS Equal Opportunity Employer Commitment India
VOIS is proud to be an Equal Employment Opportunity Employer. We celebrate differences and we welcome and value diverse people and insights. We believe that being authentically human and inclusive powers our employees growth and enables them to create a positive impact on themselves and society. We do not discriminate based on age, colour, gender (including pregnancy, childbirth, or related medical conditions), gender identity, gender expression, national origin, race, religion, sexual orientation, status as an individual with a disability, or other applicable legally protected characteristics.
As a result of living and breathing our commitment, our employees have helped us get certified as a Great Place to Work in India for four years running. We have been also highlighted among the Top 5 Best Workplaces for Diversity, Equity, and Inclusion, Top 10 Best Workplaces for Women, Top 25 Best Workplaces in IT & IT-BPM and 14th Overall Best Workplaces in India by the Great Place to Work Institute in 2023. These achievements position us among a select group of trustworthy and high-performing companies which put their employees at the heart of everything they do.
By joining us, you are part of our commitment. We look forward to welcoming you into our family which represents a variety of cultures, backgrounds, perspectives, and skills!
Apply now, and we'll be in touch!
Not a perfect fit
Worried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.
What's In It For You
Who we are
We are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.
Belonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.
If you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.
Together we can.","Pub Sub, data models, Cloud Composer, Cloud Functions, data pipelines, Data Flow Diagrams, data architecture documentation, GCP Knowledge, Technical Specifications, data consistency, BigQuery"
Senior Data Architect,Tredence Inc.,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Designation: Senior Manager (Databricks Architect)
Location: Bengaluru, Chennai, Kolkata, Gurugram, Pune and Hyderabad.
Profile Summary: We are seeking an experienced professional who apart from the required mathematical and statistical expertise also possesses the natural curiosity and creative mind to ask questions, connect the dots, and uncover opportunities that lie hidden with the ultimate goal of realizing the data's full potential.
Roles and Responsibilities:
Developing Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack
Ability to provide solutions that are forward-thinking in data engineering and analytics space
Collaboration with DW/BI leads to understanding new ETL pipeline development requirements.
Triage issues to find gaps in existing pipelines and fix the issues
Work with business to understand the need in reporting layer and develop datamodel to fulfill reporting needs
Help joiner team members to resolve issues and technical challenges.
Drive technical discussion with client architect and team members
Orchestrate the data pipelines in scheduler via Airflow Skills
Qualifications:
Bachelor's and/or master's degree in computer science or equivalent experience.
Must have total 12+ yrs. of IT experience and 3+ years experience in Data warehouse/ETL projects.
Deep understanding of Star and Snowflake dimensional modelling.
Strong knowledge of Data Management principles
Good understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture
Should have hands-on experience in SQL, Python and Spark (PySpark)
Candidate must have experience in AWS/ Azure stack
Desirable to have ETL with batch and streaming (Kinesis).
Experience in building ETL / data warehouse transformation processes
Experience with Apache Kafka for use with streaming data / event-based data
Experience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)
Experience with Open Source non-relational/ NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
Experience working with structured and unstructured data including imaging & geospatial data.
Experience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.
Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot
Databricks Certified Data Engineer Associate/Professional Certification (Desirable).
Should have experience working in Agile methodology. Job Description:
Good Communication and presentations skills.
At least 3 years experience in Databricks implementations, 2large scale data warehouse end-to-end implementation experience.","Azure Stack, CircleCI, Hadoop, Cassandra, Pyspark, Pl Sql, Impala, Sql, Pig, Git, Hive, Kinesis, RDBMS, Neo4j, Terraform, Unix Shell Scripting, Apache Kafka, Spark, Databricks, MongoDB, Python, Etl, AWS"
Data Architect,ECI,12-15 Years,,"Indore, India",Login to check your skill match score,"ECI is the leading global provider of managed services, cybersecurity, and business transformation for mid-market financial services organizations across the globe. From its unmatched range of services, ECI provides stability, security and improved business performance, freeing clients from technology concerns and enabling them to focus on running their businesses. More than 1,000 customers worldwide with over $3 trillion of assets under management put their trust in ECI.
At ECI, we believe success is driven by passion and purpose. Our passion for technology is only surpassed by our commitment to empowering our employees around the world.
The Opportunity:
ECI has an exciting opportunity for an experienced Data Architect, who will work with our clients in building robust data centric applications. Client satisfaction is our primary objective; all available positions are customer facing requiring EXCELLENT communication and people skills. A positive attitude, rigorous work habits and professionalism in the work place are a must. Fluency in English, both written and verbal are required.
This is an onsite role with work timings, 1 PM IST 10 PM IST / 2 PM IST 11 PM IST.
What you will do:
Design and develop data architecture for large enterprise application
Should be able to build and demonstrate quick POC
Review customer environment for master data processes and help with overall data solution & governance model
Work closely with team business and IT stakeholders to understand master data requirements and current constraints
Should be able to mentor technically to junior resources
Should be able to set industry standards with his own work.
Who you are:
12 to 15 years of experience as a Data Architect
Hands on experience in full life cycle Master Data Management
Hands of experience in ADF, Azure Purview, Databricks, Azure Fabric Services
Lead Data architecture roadmaps, defined business cases and implementations for clients
Experience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture
Review customer environment for master data processes and help with overall data governance model
Hands on experience in building cloud based later enterprise data warehouses.
Experience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture
Implementing best practices for data governance, data modeling, and data migrations
Should be a good team player
Bonus points if you have:
Deep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies
Strong knowledge of ETL and Data Modeling
Deep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies
ECI's culture is all about connection - connection with our clients, our technology and most importantly with each other. In addition to working with an amazing team around the world, ECI also offers a competitive compensation package and so much more! If you believe you would be a great fit and are ready for your best job ever, we would like to hear from you!
Love Your Job, Share Your Technology Passion, Create Your Future Here!","Azure Fabric Services, Data Migrations, Azure Purview, Master Data Management, Data Modeling, Adf, Data Architecture, Databricks, Data Governance"
MS Azure Data Architect,3Pillar,8-10 Years,,India,Login to check your skill match score,"We are seeking a highly experienced Data Architect with a strong background in designing scalable data solutions and leading data engineering teams. The ideal candidate will have deep expertise in Microsoft Azure, ETL processes, and modern data architecture principles. This role involves close collaboration with stakeholders, engineering teams, and business units to design and implement robust data pipelines and architectures.
Assessments of existing data components, Performing POCs, Consulting to the stakeholders
Proposing end to end solutions to an enterprise's data specific business problems, and taking care of data collection, extraction, integration, cleansing, enriching and data visualization
Ability to design large data platforms to enable Data Engineers, Analysts & scientists
Strong exposure to different Data architectures, data lake & data warehouse
Design and implement end-to-end data architecture solutions on Azure cloud platform.
Lead the design and development of scalable ETL/ELT pipelines using tools such as Azure Data Factory (ADF).
Architect data lakes using Azure Data Lake Storage (ADLS) and integrate with Azure Synapse Analytics for enterprise-scale analytics.
Collaborate with business analysts, data scientists, and engineers to understand data needs and deliver high-performing solutions.
Define data models, metadata standards, data quality rules, and security protocols.
Define tools & technologies to develop automated data pipelines, write ETL processes, develop dashboard & report and create insights
Continually reassess current state for alignment with architecture goals, best practices and business needs
DB modeling, deciding best data storage, creating data flow diagrams, maintaining related documentation
Taking care of performance, reliability, reusability, resilience, scalability, security, privacy & data governance while designing a data architecture
Apply or recommend best practices in architecture, coding, API integration, CI/CD pipelines
Coordinate with data scientists, analysts, and other stakeholders for data-related needs
Help the Data Science & Analytics Practice grow by mentoring junior Practice members, leading initiatives, leading Data Practice Offerings
Provide thought leadership by representing the Practice / Organization on internal / external platforms
Qualificatons:
8+ years of experience in data architecture, data engineering, or related roles.
Translate business requirements into data requests, reports and dashboards.
Strong Database & modeling concepts with exposure to SQL & NoSQL Databases
Expertise in designing and writing ETL processes in Python/PySpark
Strong data architecture patterns & principles, ability to design secure & scalable data lakes, data warehouse, data hubs, and other event-driven architectures
Proven expertise in Microsoft Azure data services, especially ADF, ADLS, Synapse Analytics.
Strong hands-on experience in designing and building ETL/ELT pipelines.
Proficiency in data modeling, SQL, and performance tuning.
Demonstrated leadership experience, with the ability to manage and mentor technical teams.
Excellent communication and stakeholder management skills.
Proficiency in data visualization tools like Tableau, Power BI or similar to create meaningful insights.
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Good to have:
Azure certifications (e.g., Azure Data Engineer Associate, Azure Solutions Architect).
Experience with modern data platforms, data governance frameworks, and real-time data processing tools.
Benefits:
Imagine a flexible work environment whether it's the office, your home, or a blend of both. From interviews to onboarding, we embody a remote-first approach.
You will be part of a global team, learning from top talent around the world and across cultures, speaking English everyday. Our global workforce enables our team to leverage global resources to accomplish our work in efficient and effective teams.
We're big on your well-being as a company, we spend a whole trimester in our annual cycle focused on wellbeing. Whether it is taking advantage of fitness offerings, mental health plans (country-dependent), or simply leveraging generous time off, we want all of our team members operating at their best.
Our professional services model enables us to accelerate career growth and development opportunities - across projects, offerings, and industries.
We are an equal opportunity employer. It goes without saying that we live by values like Intrinsic Dignity and Open Collaboration to create cutting-edge technology AND reinforce our commitment to diversity - globally and locally.
Join us and be a part of a global tech community! Check out our Linkedin site and Careers page to learn more about what it's like to be part of our #oneteam!","NoSQL Databases, ETL processes, Azure Synapse Analytics, Pyspark, Microsoft Azure, Data Modeling, Data Architecture, Sql, Python"
Senior Data Architect (Technical),PURVIEW,5-7 Years,,"Pune, India",Login to check your skill match score,"Job Description
Principal Responsibilities:
Evolve WCS IT landscape with Data-driven approach, define, design and drive Data architecture roadmap for WCS IT including enterprise data architecture.
Closely work with Product Owners, Business & IT stakeholders across WCS IT landscape to understand the requirements and provide solutions fit-for-purpose
Create DFDs and data lineage diagrams to understand the current state and work with stakeholders to align to the future roadmap adhering to the HSBC defined standards
Understand system integrations across and outside WCS IT applications, API integration (and others), understand and document JSON, XML or any other formats being used and decode the interfaces
Understand business requirements on data flows across applications / systems and work with Product owners and POD teams for development aligning to data architecture and designs
Design and Develop Tools for automation and scripting for data extraction, transformation and loading as per business needs.
Work with Program, Department and Enterprise architects of HSBC to drive data architecture and to deliver expected business outcome by designing, developing and implementing solutions
Follow DevOps Model in day-to-day delivery
Innovation is part of team's culture and everyone expected to come up with their own ideas / thoughts / solutions, develop PoC, explore new technologies, participate in hackathon events, attend forums, sessions
Ways of working is of prime importance and must follow the best practices and guidelines laid down including Data security standards of BFS
Develop deep know-how of WCIT applications - jou eys, features, platform, further develop understanding of other applications in Wholesale Client Services IT landscape
Guide junior members in the team and help other members when needed
Must Have Requirements
Reach experience with Data architecture, defining and driving enterprise data architecture
Understanding of micro services architecture and other architecture patte s, REST APIs
Reading, parsing of JSON, XML and other structures and develop understanding of data attributes and values between two systems
Use of various data modelling, DFD, Data lineage tools like Visio, reverse engineering tools
Must have experience working in Banking and Financial service industry specifically with a MNC Bank of size of HSBC
Must have experience analysing large size data, JSONs, XMLs, other formats and writing scripts to extract, transform and load using tools or developed automation tools
Must have worked on PostgreSQL, Oracle, MongoDB and such databases
Must have experience working in Agile and DevSecOps.
Must have an inclination to explore new technologies, explore and innovate other than project work
Must have excellent communication skills written and verbal and should be able to articulate thoughts clearly
About Company :
Purview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.
We have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.
In
Company Info:
3rd Floor, Sonthalia Mind Space
Near Westin Hotel, Gafoor Nagar
Hitechcity, Hyderabad
Phone: +91 40 48549120 / +91 8790177967
Uk
Gyleview House, 3 Redheughs Rigg,
South Gyle, Edinburgh, EH12 9DQ.
Phone: +44 7590230910
Email: [HIDDEN TEXT]
Login to Apply !","DFD, Data lineage tools, PostgreSQL, Json, Data Architecture, DevSecOps, Automation Tools, Xml, Agile, MongoDB, Rest Apis, Oracle"
Data Architect,Gramener,Fresher,,"Hyderabad, India",Login to check your skill match score,"Location: Hyderabad/ Bengaluru, India (Hybrid Mode 3 Days/Week in Office)
Job Description
Collaborate with stakeholders to develop a data strategy that meets enterprise needs and industry requirements.
Create an inventory of the data necessary to build and implement a data architecture.
Envision data pipelines and how data will flow through the data landscape.
Evaluate current data management technologies and what additional tools are needed.
Determine upgrades and improvements to current data architectures.
Design, document, build and implement database architectures and applications. Should have hands-on experience in building high scale OLAP systems.
Build data models for database structures, analytics, and use cases.
Develop and enforce database development standards with solid DB/ Query optimizations capabilities.
Integrate new systems and functions like security, performance, scalability, governance, reliability, and data recovery.
Research new opportunities and create methods to acquire data.
Develop measures that ensure data accuracy, integrity, and accessibility.
Continually monitor, refine, and report data management system performance.
Required Qualifications And Skillset
Extensive knowledge of Azure, GCP clouds, and DataOps Data Eco-System (super strong in one of the two clouds and satisfactory in the other one)
Hands-on expertise in systems like Snowflake, Synapse, SQL DW, BigQuery, and Cosmos DB. (Expertise in any 3 is a must)
Azure Data Factory, Dataiku, Fivetran, Google Cloud Dataflow (Any 2)
Hands-on experience in working with services/technologies like Apache Airflow, Cloud Composer, Oozie, Azure Data Factory, and Cloud Data Fusion (Expertise in any 2 is required)
Well-versed with Data services, integration, ingestion, ELT/ETL, Data Governance, Security, and Meta-driven Development.
Expertise in RDBMS (relational database management system) writing complex SQL logic, DB/Query optimization, Data Modelling, and managing high data volume for mission-critical applications.
Strong grip on programming using Python and PySpark.
Clear understanding of data best practices prevailing in the industry.
Preference to candidates having Azure or GCP architect certification. (Either of the two would suffice)
Strong networking and data security experience.
Awareness Of The Following
Application development understanding (Full Stack)
Experience on open-source tools like Kafka, Spark, Splunk, Superset, etc.
Good understanding of Analytics Platform Landscape that includes AI/ML
Experience in any Data Visualization tool like PowerBI / Tableau / Qlik /QuickSight etc.
About Us
Gramener is a design-led data science company. We build custom Data & AI solutions that help solve complex business problems with actionable insights and compelling data stories. We partner with enterprise data and digital transformation teams to improve the data-driven decision-making culture across the organization. Our open standard low-code platform, Gramex, rapidly builds engaging Data & AI solutions across multiple business verticals and use cases. Our solutions and technology have been recognized by analysts such as Gartner and Forrester and have won several awards.
We offer you
a chance to try new things & take risks.
meaningful problems you'll be proud to solve.
people you will be comfortable working with.
transparent and innovative work environment.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","DB Query optimization, Cloud Composer, DataOps, Dataiku, snowflake, Synapse SQL DW, Google Cloud Dataflow, Cloud Data Fusion, Fivetran, BigQuery, Data Modelling, Pyspark, Sql, Apache Airflow, Azure Data Factory, Gcp, RDBMS, Oozie, Cosmos DB, Azure, Python"
Data Architect,DigitalXNode,5-10 Years,,India,Login to check your skill match score,"No. Of Position: 4
Experience: 5Y to 10Y
Salary: Competitive as per market standards
Notice period: Immediate to 30 days
Skills : Azure, ADLS, Kafka, Apache Delta, Databricks/Spark, Hadoop ecosystem, SQL, RDBMS, Data Lakes and Warehouses
Location: Delhi NCR/Pune/Bangalore/Chennai/Hyderabad/Remote
Role & Responsibilities
Subject matter knowledge in creating data models for corporate analytics that are in compliance with standards, allowing for usability and conformance throughout the enterprise.
In charge of creating data strategies, vocabulary consistency, and transformations via intricate analytical relationships and access paths, as well as data mappings at the data-field level.
Collaborate with Product Management and Business stakeholders to ascertain and assess the data sources required to fulfill project and business objectives.
Collaborate with Tech Leads and Product Architects to comprehend end-to-end data implications, data integration, and functioning business systems.
Work with DQ Leads, for data integrity improvement/quality resolution so that these improvements are addressed at the source.
Domain knowledge in supply chain, retail, or inventory management.
Critical Skills To Have
Five or more years of experience in the field of information technology
Has a general understanding of several software platforms and development technologies
Has experience with SQL, RDBMS, Data Lakes, and Warehouses Knowledge of the Hadoop ecosystem, Azure, ADLS, Kafka, Apache Delta, and Databricks/Spark.
Possessing knowledge of any data modeling tool, such as ERStudio or Erwin, is advantageous.
Collaboration history with Product Managers, Technology teams, and Business Partners
Strong familiarity with Agile and DevOps techniques
Excellent communication skills both in writing and speaking
Preferred Qualifications
A bachelor's degree in business information technology, computer science, or a similar discipline.
Please apply for a resume online, and the digitalxnode evaluation team will reach out to you in case your profile gets screen-selected. We will keep your data in our repository, and our team may reach out to you for other positions.
Technology: IT
Job Type: Full Time
Job Location: Bangalore Bengaluru Delhi Kolkata Navi Mumbai","Azure ADLS, Data Lakes, Warehouses, Apache Delta, RDBMS, Hadoop Ecosystem, Spark, Kafka, Databricks, Sql"
GCP BIG DATA ARCHITECT- Chennai,Grid Dynamics,Fresher,,"Chennai, India",Login to check your skill match score,"Details on tech stack
GCP Services: BigQuery, Cloud Dataflow, Pub/Sub, Dataproc, Cloud Storage.
Data Processing: Apache Beam (batch/stream), Apache Kafka, Cloud Dataprep.
Programming: Python, Java/Scala, SQL.
Orchestration: Apache Airflow (Cloud Composer), Terraform.
Security: IAM, Cloud Identity, Cloud Security Command Center.
Containerization: Docker, Kubernetes (GKE).
Machine Learning: Google AI Platform, TensorFlow, AutoML.
Certifications: Google Cloud Data Engineer, Cloud Architect (preferred).
Proven ability to design scalable and robust AI/ML systems in production, with a focus on high-performance and cost-effective solutions.
Strong experience with cloud platforms (Google Cloud, AWS, Azure) and cloud-native AI/ML services (e.g., Vertex AI, SageMaker).
Expertise in implementing MLOps practices, including model deployment, monitoring, retraining, and version control.
Strong leadership skills with the ability to guide teams, mentor engineers, and collaborate with cross-functional teams to meet business objectives.
Deep understanding of frameworks like TensorFlow, PyTorch, and Scikit-learn for designing, training, and deploying models.
Experience with data engineering principles, scalable pipelines, and distributed systems (e.g., Apache Kafka, Spark, Kubernetes).
Nice to have requirements to the candidate
Strong leadership and mentorship capabilities, guiding teams toward best practices and high-quality deliverables.
Excellent problem-solving skills, with a focus on designing efficient, high-performance systems.
Effective project management abilities to handle multiple initiatives and ensure timely delivery.
Strong emphasis on collaboration and teamwork, fostering a positive and productive work environment.","Cloud Dataprep, Cloud Dataflow, Google AI Platform, Pub Sub, Cloud Security Command Center, Cloud Composer, Cloud Identity, Vertex AI, GCP Services, GKE, Scikit-learn, SageMaker, AutoML, Apache Airflow, Tensorflow, Cloud Storage, Pytorch, Docker, Terraform, Python, AWS, Java, BigQuery, Scala, Dataproc, Sql, MLops, Iam, Apache Kafka, Apache Beam, Azure, Kubernetes"
Data Architect,Grid Dynamics,12-16 Years,,"Hyderabad, India",Login to check your skill match score,"Grid Dynamics Hiring Data/Cloud Architect
Experience: 12- 16 Years
Notice period: Immediate - 30 Days
Location : Hyderabad
Role:
A result- oriented thought-leader to drive the development of the data engineering practice
A trusted advisor and business partner to customers across verticals, and consulting team leader who establishes engineering processes and skill development.
Responsibilities:
Trusted Advisor:
Provide direction to clients in different verticals on their selection of data and ML platforms, data strategies, expected impact, and relevant tools and methodologies that they can deploy to accelerate Big Data and advanced analytics programs.
Pre-Sales and Consulting:
Lead and drive technology consulting engagements and pre-sales, architecture assessments, and discovery workshops. Plan data engineering programs of Fortune-1000 enterprises and lead a team of principal consultants and data engineers who work on client accounts and drive consulting engagements to success.
Technology Strategy and R&D:
Create and bring to the market solutions in data engineering, MLOps, data governance. Responsible for driving innovation through research and development activities on industry trends, defining Go-To-Market strategies, developing assets and solutions strategy across multiple industries.
Engineering:
Working with Grid Dynamics's delivery organisation to ensure that the right tools, technologies, and processes are in place across the firm to transform and continuously improve the quality and efficiency of our clients data platforms and data management processes.
Business Development & Partnership:
Manage relationships with key technology partners(AWS, GCP, Azure) and industry analysts.
Requirements:
Extensive practical experience in Big Data engineering, data governance, and cloud data
platforms.
Strong understanding of cloud-based architectures for data collection, data aggregation, reporting, and BI.
Strong understanding of ML platforms and tooling including open-source, cloud native, and proprietary.
Deep domain knowledge in at least one of the following industries: Retail, Finance, Manufacturing, Healthcare, Life Sciences.
Experience in managing and delivering sophisticated analytical and data engineering programs at the enterprise scale.
Managed key client relations worldwide and advised global technology and business leaders on innovation and data strategy.
Good experience with Pre-sales activities.
Technology skills (any of below):
Data engineering: analytical data platforms, streaming, big data, EDW, data lakes, data governance, data mesh, Spark, Kafka, Snowflake, (2 from below: AWS, GCP, Azure) Transactional databases: Redis, Cassandra, MongoDB, (2 from below: AWS, GCP, Azure) ML and MLOps: VertexAI, Sagemaker, Dataiku, Databricks, mlflow.
About Grid :
Grid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, and advanced analytics services. Fusing technical vision with business acumen, we enable positive business outcomes for enterprise companies undergoing business transformation by solving their most pressing technical challenges. A key differentiator for Grid Dynamics is our 7+ years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization, and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India. Follow us on LinkedIn.","ML and MLOps, Sagemaker, snowflake, Data Collection, cloud-based architectures, Big Data engineering, Transactional databases, mlflow, Dataiku, data mesh, streaming big data, Reporting, ML platforms, analytical data platforms, data aggregation, data lakes, VertexAI, Databricks, Kafka, Cassandra, data engineering, Gcp, Data Governance, BI, AWS, Redis, Edw, Azure, MongoDB, Spark"
Data Architect,66degrees,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees
66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.
At 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.
Overview of Role
As a Data Architect with 66degrees, you are responsible for assisting customers in their journey to Google Cloud through the design and implementation of a migration strategy. The Data Architect should be comfortable translating business and technical requirements into scalable and cost-effective database solutions.
Responsibilities
Facilitate, guide, and influence the client and teams towards an effective architectural pattern and becoming an interface between business leadership, technology leadership and the delivery teams.
Perform Migration Assessments and Produce Migration Plans that include
Total Cost of Ownership (TCO)
Migration Architecture
Migration Timelines
Application Waves
Designing of solution architecture on Google Cloud to support critical workloads
Heterogenous Oracle Migrations to Postgres or Spanner
Design a migration path that accounts for the conversion of:
Application Dependencies
Database objects
Data
Data Pipelines
Orchestration
Users and Security
Oversee migration activities and provide troubleshooting support including:
Translation of DDL and DML
Executing data transfers using native Google Cloud and 3rd party tools
Setup and configuration of relative Google Cloud components
Engage with customer teams as a Google Cloud expert to provide:
Education Workshops
Architectural Recommendations
Technology reviews and recommendations
Qualifications
5+ years of Oracle database management and IT experience.
Experience with Oracle Database adjacent products like Golden Gate and Data Guard.
3+ years of PostgreSQL experience
5+ years Consulting experience
Proven experience in performing performance testing and applying remediations to address performance issues.
Experience in designing data models
Advanced SQL skills, including the ability to write, tune, and interpret SQL queries; tool specific experience in the database platforms listed above is ideal.
Proven experience in migrating and or implementing cloud databases like Cloud SQL, Spanner, and Bigtable
2+ years of cloud experience; Google Cloud preferred
Experience in defining technical architecture in cloud environments; Google Cloud preferred
Google Cloud Professional Architect and or Data Engineer Certification is preferred
A Bachelor's degree in Computer Science, Computer Engineering, or related or equivalent work experience required.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","bigtable, Cloud SQL, Spanner, Advanced Sql, Data Guard, Oracle Database, PostgreSQL, Google Cloud, Golden Gate"
Data Architect,Impelsys,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Software Skills
Technical Skill Data Architect, ETL, Data Engineer, Python, SQL and Any
Cloud
Job Description
Develop architectural strategies for data modelling, design and implementation to meet stated requirements for metadata management, master data management, Data warehouses, ETL and ELT.
Analyzing business requirements, designing scalable/ robust data models, documenting conceptual, logical & physical data model design, helping developers in development/ creating DB structures and supporting developers throughout the project life cycle.
Lead and Mentor Data Engineers: This role will be responsible for leading and developing a team of data engineers focused on the growth in the team's skills and ability to execute as a team using DevOps and Data Ops principles.
Investigate new technologies, data modelling methods and information management systems to determine which ones should be incorporated onto data architectures, and develop implementation timelines and milestones.
Recognizes and resolves conflicts between models, ensuring that information and data models are consistent with the ecosystem model (e.g., entity names, relationships and definitions).
Participates in the design of the information architecture: supports projects, reviews information elements including models, glossary, flows, data usage.
Provides guidance to the team in achieving the project goals/milestones.
Works independently within broad guidelines and policies, with guidance in only the most complex situations.
Contribute as an expert to multiple delivery teams, defining best practices, building reusable design & components, capability building, aligning industry trends and actively engaging with wider data communities.
Candidate Profile
10+ years of relevant experience in Data modelling for DW & analytics applications / Database related technologies.
Expert data modelling skills (i.e. conceptual, logical and physical model design, experience with Enterprise Data Warehouses and Data Marts).
Solid understanding of cloud database technologies and services (eg..AWS, Azure , Redshift, Aurora, DynamoDB, Snowflake etc).
Experience in data lake technologies involving S3, Databricks Delta Lake, etc.
Experience in working with data governance, data quality, and data security teams.
Experienced in knowledge-driven data processing techniques like data curation, representation, standardization, normalization and any other type of processing that prepares the data for integration, persistence, analysis, exchange/share and so forth.
Experience in handling very large DBs and large data volumes .
Strong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production.
Ability to lead and mentor teams for effective delivery.
Crisp and effective executive communication skills, including significant experience. presenting cross-functionally and across all levels.
Education
Graduate or post graduate in Computer science/Electronics/Software engineering","Aurora, data marts, ETL processes, snowflake, Enterprise Data Warehouses, Delta Lake, Data lake technologies, S3, Data Architect, Data Governance, data curation, data preparation, Database Technologies, Data Integration, Data Engineer, Data Modelling, Python, AWS, Data Security, Dynamodb, Data Quality, Redshift, Sql, Cloud, Databricks, Azure, Etl"
Data Architect,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Responsibilities:
Design, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.
Enhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.
Implement best practices for data management, storage and security to ensure data integrity and compliance with regulations.
Own the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.
Participate in code reviews to ensure code quality and share knowledge.
Lead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.
Define and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.
Mentor junior members of the team, providing guidance and support in their professional development.
Collaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration
A little more about you:
Bachelor's degree or higher in Computer Science, Engineering, or a related field.
10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.
Proficient in SQL and Python, with the ability to translate complexity into efficient code.
Experience with data workflow development and management tools (dbt, Airflow).
Solid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.
Strong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Experience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Gcp, Azure, Python, Sql, AWS"
Data Architect,ACL Digital,8-10 Years,,"Pune, India",Login to check your skill match score,"Total yrs of Exp: 8+ yrs
Location:Balewadi, Pune
Technical skills and core competencies
Strong understanding of Data Architecture and models and leading data driven projects.
Solid expertise and strong opinions on Data Modelling paradigms such as Kimball, Inmon, Data Marts, Data Vault, Medallion etc.
Strong experience with Cloud Based data strategies and big data technologies AWS Preferred.
Solid experience in designing data pipelines for ETL with expert knowledge on ingestion, transformation, and data quality is a must
Hands-on experience in SQL is a must.
Deep understanding of PostGreSQL development, query optimization and designing indexes is a must.
An ability to understand and manipulate intermediate to complex level of SQL
Thorough knowledge of Postgres PL/SQL to work with complex warehouse workflows.
Ability to use advanced SQL concepts such as RANK, DENSE_RANK along with applying advanced statistical concepts through SQL is required.
Working experience with PostGres SQL extensions like PostGIS is desired.
Expertise writing ETL pipelines combining Python + SQL is required.
Understanding of data manipulation libraries in python like Pandas, Polars, DuckDB is desired
Experience in designing Data visualization with different tools such as Tableau and PowerBI is desirable.
Responsibilities
Participate in the design and developing features in the existing Data Warehouse.
Provide leadership in establishing connection between Engineering, product and analytics/data scientists team.
Design, implement, update existing/new batch ETL pipelines
Define and implement data architecture.
Partner with both engineers and data analysts to build reliable datasets that can be trusted, understood, and used by the rest of the company.
Work with various data orchestration tools (Apache Airflow, Dagster, Prefect and others)
Embrace a fast-paced start-up environment.
You should be passionate about your job and enjoy a fast-paced international working environment.
Background or experience in the telecom industry is a plus but not a requirement.
Love automating and enjoy monitoring","Cloud Based data strategies, PostGreSQL development, Postgres PL SQL, Query Optimization, Sql, Data Architecture"
Senior Data Architect (Technical),PURVIEW,5-7 Years,,"Pune, India",Login to check your skill match score,"Job Description
Principal Responsibilities:
Evolve WCS IT landscape with Data-driven approach, define, design and drive Data architecture roadmap for WCS IT including enterprise data architecture.
Closely work with Product Owners, Business & IT stakeholders across WCS IT landscape to understand the requirements and provide solutions fit-for-purpose
Create DFDs and data lineage diagrams to understand the current state and work with stakeholders to align to the future roadmap adhering to the HSBC defined standards
Understand system integrations across and outside WCS IT applications, API integration (and others), understand and document JSON, XML or any other formats being used and decode the interfaces
Understand business requirements on data flows across applications / systems and work with Product owners and POD teams for development aligning to data architecture and designs
Design and Develop Tools for automation and scripting for data extraction, transformation and loading as per business needs.
Work with Program, Department and Enterprise architects of HSBC to drive data architecture and to deliver expected business outcome by designing, developing and implementing solutions
Follow DevOps Model in day-to-day delivery
Innovation is part of team's culture and everyone expected to come up with their own ideas / thoughts / solutions, develop PoC, explore new technologies, participate in hackathon events, attend forums, sessions
Ways of working is of prime importance and must follow the best practices and guidelines laid down including Data security standards of BFS
Develop deep know-how of WCIT applications - jou eys, features, platform, further develop understanding of other applications in Wholesale Client Services IT landscape
Guide junior members in the team and help other members when needed
Must Have Requirements
Reach experience with Data architecture, defining and driving enterprise data architecture
Understanding of micro services architecture and other architecture patte s, REST APIs
Reading, parsing of JSON, XML and other structures and develop understanding of data attributes and values between two systems
Use of various data modelling, DFD, Data lineage tools like Visio, reverse engineering tools
Must have experience working in Banking and Financial service industry specifically with a MNC Bank of size of HSBC
Must have experience analysing large size data, JSONs, XMLs, other formats and writing scripts to extract, transform and load using tools or developed automation tools
Must have worked on PostgreSQL, Oracle, MongoDB and such databases
Must have experience working in Agile and DevSecOps.
Must have an inclination to explore new technologies, explore and innovate other than project work
Must have excellent communication skills written and verbal and should be able to articulate thoughts clearly
About Company :
Purview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.
We have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.
In
Company Info:
3rd Floor, Sonthalia Mind Space
Near Westin Hotel, Gafoor Nagar
Hitechcity, Hyderabad
Phone: +91 40 48549120 / +91 8790177967
Uk
Gyleview House, 3 Redheughs Rigg,
South Gyle, Edinburgh, EH12 9DQ.
Phone: +44 7590230910
Email: [HIDDEN TEXT]
Login to Apply !","DFD, Data lineage tools, PostgreSQL, Json, Data Architecture, DevSecOps, Automation Tools, Xml, Agile, MongoDB, Rest Apis, Oracle"
Senior Data Architect,Tredence Inc.,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Designation: Senior Manager (Databricks Architect)
Location: Bengaluru, Chennai, Kolkata, Gurugram, Pune and Hyderabad.
Profile Summary: We are seeking an experienced professional who apart from the required mathematical and statistical expertise also possesses the natural curiosity and creative mind to ask questions, connect the dots, and uncover opportunities that lie hidden with the ultimate goal of realizing the data's full potential.
Roles and Responsibilities:
Developing Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack
Ability to provide solutions that are forward-thinking in data engineering and analytics space
Collaboration with DW/BI leads to understanding new ETL pipeline development requirements.
Triage issues to find gaps in existing pipelines and fix the issues
Work with business to understand the need in reporting layer and develop datamodel to fulfill reporting needs
Help joiner team members to resolve issues and technical challenges.
Drive technical discussion with client architect and team members
Orchestrate the data pipelines in scheduler via Airflow Skills
Qualifications:
Bachelor's and/or master's degree in computer science or equivalent experience.
Must have total 12+ yrs. of IT experience and 3+ years experience in Data warehouse/ETL projects.
Deep understanding of Star and Snowflake dimensional modelling.
Strong knowledge of Data Management principles
Good understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture
Should have hands-on experience in SQL, Python and Spark (PySpark)
Candidate must have experience in AWS/ Azure stack
Desirable to have ETL with batch and streaming (Kinesis).
Experience in building ETL / data warehouse transformation processes
Experience with Apache Kafka for use with streaming data / event-based data
Experience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)
Experience with Open Source non-relational/ NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
Experience working with structured and unstructured data including imaging & geospatial data.
Experience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.
Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot
Databricks Certified Data Engineer Associate/Professional Certification (Desirable).
Should have experience working in Agile methodology. Job Description:
Good Communication and presentations skills.
At least 3 years experience in Databricks implementations, 2large scale data warehouse end-to-end implementation experience.","Azure Stack, CircleCI, Hadoop, Cassandra, Pyspark, Pl Sql, Impala, Sql, Pig, Git, Hive, Kinesis, RDBMS, Neo4j, Terraform, Unix Shell Scripting, Apache Kafka, Spark, Databricks, MongoDB, Python, Etl, AWS"
Data Architect,ECI,12-15 Years,,"Indore, India",Login to check your skill match score,"ECI is the leading global provider of managed services, cybersecurity, and business transformation for mid-market financial services organizations across the globe. From its unmatched range of services, ECI provides stability, security and improved business performance, freeing clients from technology concerns and enabling them to focus on running their businesses. More than 1,000 customers worldwide with over $3 trillion of assets under management put their trust in ECI.
At ECI, we believe success is driven by passion and purpose. Our passion for technology is only surpassed by our commitment to empowering our employees around the world.
The Opportunity:
ECI has an exciting opportunity for an experienced Data Architect, who will work with our clients in building robust data centric applications. Client satisfaction is our primary objective; all available positions are customer facing requiring EXCELLENT communication and people skills. A positive attitude, rigorous work habits and professionalism in the work place are a must. Fluency in English, both written and verbal are required.
This is an onsite role with work timings, 1 PM IST 10 PM IST / 2 PM IST 11 PM IST.
What you will do:
Design and develop data architecture for large enterprise application
Should be able to build and demonstrate quick POC
Review customer environment for master data processes and help with overall data solution & governance model
Work closely with team business and IT stakeholders to understand master data requirements and current constraints
Should be able to mentor technically to junior resources
Should be able to set industry standards with his own work.
Who you are:
12 to 15 years of experience as a Data Architect
Hands on experience in full life cycle Master Data Management
Hands of experience in ADF, Azure Purview, Databricks, Azure Fabric Services
Lead Data architecture roadmaps, defined business cases and implementations for clients
Experience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture
Review customer environment for master data processes and help with overall data governance model
Hands on experience in building cloud based later enterprise data warehouses.
Experience in leading, evaluating and designing Data Architecture based on the overall Enterprise Data Strategy / Architecture
Implementing best practices for data governance, data modeling, and data migrations
Should be a good team player
Bonus points if you have:
Deep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies
Strong knowledge of ETL and Data Modeling
Deep knowledge of Master Data Management (MDM) principles, processes, architectures, protocols, patterns, and technologies
ECI's culture is all about connection - connection with our clients, our technology and most importantly with each other. In addition to working with an amazing team around the world, ECI also offers a competitive compensation package and so much more! If you believe you would be a great fit and are ready for your best job ever, we would like to hear from you!
Love Your Job, Share Your Technology Passion, Create Your Future Here!","Azure Fabric Services, Data Migrations, Azure Purview, Master Data Management, Data Modeling, Adf, Data Architecture, Databricks, Data Governance"
Manager_Data Architect,VOIS,Fresher,,"Pune, India",Login to check your skill match score,"Join Us
At Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.
About VOIS
VOIS (Vodafone Intelligent Solutions) is a strategic arm of Vodafone Group Plc, creating value and enhancing quality and efficiency across 28 countries, and operating from 7 locations: Albania, Egypt, Hungary, India, Romania, Spain and the UK.
Over 29,000 highly skilled individuals are dedicated to being Vodafone Group's partner of choice for talent, technology, and transformation. We deliver the best services across IT, Business Intelligence Services, Customer Operations, Business Operations, HR, Finance, Supply Chain, HR Operations, and many more.
Established in 2006, VOIS has evolved into a global, multi-functional organisation, a Centre of Excellence for Intelligent Solutions focused on adding value and delivering business outcomes for Vodafone.
About VOIS India
In 2009, VOIS started operating in India and now has established global delivery centres in Pune, Bangalore and Ahmedabad. With more than 14,500 employees, VOIS India supports global markets and group functions of Vodafone, and delivers best-in-class customer experience through multi-functional services in the areas of Information Technology, Networks, Business Intelligence and Analytics, Digital Business Solutions (Robotics & AI), Commercial Operations (Consumer & Business), Intelligent Operations, Finance Operations, Supply Chain Operations and HR Operations and more.
What You'll Do
Strong understanding of end-to-end impact assessment across all subject areas.
Creating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications
Creating and maintaining data models for databases, data warehouses, and data lakes, defining relationships between data entities to optimize data retrieval and analysis.
Designing and implementing data pipelines to integrate data from multiple sources, ensuring data consistency and quality across systems.
Collaborating with business stakeholders to define the overall data strategy, aligning data needs with business requirements.
Support migration of new & changed software, elaborate and perform production checks
Need to effectively communicate complex data concepts to both technical and non-technical stakeholders.
GCP Knowledge/exp with Cloud Composer, BigQuery, Pub/Sub, Cloud Functions.
Who You Are
E2E Impact assessment should be done for all demands.
Creating detailed data architecture documentation, including data models, data flow diagrams, and technical specifications
Manage database related refresh and decommissioning programs whilst maintaining the highest level of Service availability to Vodafone customers
Assure correct database configuration and proper documentation of all relevant changes within the DB infrastructure
Support supplier's 3rd level and engineering in root cause analysis and remediation of problems in their products
To come up with ideas to enhance the system
VOIS Equal Opportunity Employer Commitment India
VOIS is proud to be an Equal Employment Opportunity Employer. We celebrate differences and we welcome and value diverse people and insights. We believe that being authentically human and inclusive powers our employees growth and enables them to create a positive impact on themselves and society. We do not discriminate based on age, colour, gender (including pregnancy, childbirth, or related medical conditions), gender identity, gender expression, national origin, race, religion, sexual orientation, status as an individual with a disability, or other applicable legally protected characteristics.
As a result of living and breathing our commitment, our employees have helped us get certified as a Great Place to Work in India for four years running. We have been also highlighted among the Top 5 Best Workplaces for Diversity, Equity, and Inclusion, Top 10 Best Workplaces for Women, Top 25 Best Workplaces in IT & IT-BPM and 14th Overall Best Workplaces in India by the Great Place to Work Institute in 2023. These achievements position us among a select group of trustworthy and high-performing companies which put their employees at the heart of everything they do.
By joining us, you are part of our commitment. We look forward to welcoming you into our family which represents a variety of cultures, backgrounds, perspectives, and skills!
Apply now, and we'll be in touch!
Not a perfect fit
Worried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.
What's In It For You
Who we are
We are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.
Belonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.
If you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.
Together we can.","Pub Sub, data models, Cloud Composer, Cloud Functions, data pipelines, Data Flow Diagrams, data architecture documentation, GCP Knowledge, Technical Specifications, data consistency, BigQuery"
Data Architect,Jio,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Skills:
Data Architect, On-premise, Python, Kafka, Cloudera, Spark, Hadoop, Hive,
Company Overview
Jio Platforms Limited is the driving force behind India's leading telecom operator Jio, serving 400M+ customers. We offer digital apps & services, end-to-end 5G solutions, AI/ML platforms, and cloud-native OSS/BSS solutions.
Job Overview
Experienced Data Architect role at Jio Platforms Limited, Mumbai, Bangalore India. Full-Time position with more than 10 years of experience. Join a dynamic company leading India's telecom market with 400M+ customers and innovative digital solutions for B2C and B2B sectors.
Qualifications And Skills
Expertise in Hadoop, Hive, Apache Spark, Python, Kafka, Cloudera
Experience with On-premise and Microsoft Azure environments
Strong data management and data governance skills
Excellent problem-solving and analytical abilities
Effective communication and collaboration skills
Bachelors or masters degree in computer science, Information Technology or related field of study. MCA preferable.
12 - 15 years of overall experience.
Certification MS Azure AZ-304, AZ-303,
6+ years of large-scale software development or application engineering with recent coding experience in two or more of the following: Java, JavaScript, Node.js, .NET, Python, MSSQL.
4+ years of experience as a technical specialist in customer-facing roles.
Experience architecting highly available systems that utilize load balancing, horizontal scalability and high availability.
Good exposure to Agile software development and DevOps practices such as Infrastructure as Code (IaC), Continuous Integration and automated deployment Continuous Integration (CI) tools (e.g. Jenkins).
Strong, in-depth and demonstrable hands-on experience with the following technologies: Microsoft Azure and its relevant build, deployment, automation, networking and security technologies in cloud and hybrid environments.
Exposure to Agile development methodologies and deployment strategies.
Strong practical application development experience on Linux and Windows-based systems.
Excellent knowledge of cloud computing technologies and current computing trends.
Experience working directly with customers, partners or third-party developers.
Effective communication skills (written and verbal) to properly articulate complicated cloud reports to management and other IT development partners.
Positive attitude and a strong commitment to delivering quality work.
Roles And Responsibilities
Design and implement data architecture solutions to meet business needs.
Develop data models, database design, and data migration strategies.
Collaborate with cross-functional teams to ensure data integration and data quality.
Implement data security and compliance measures.
Optimize data infrastructure and performance for scalable solutions.
Architect, design, and develop Products on the Azure platform.
Design and develop solutions for Data Platforms ranging from Batch Data management to real-time data feeds.
Leverage new technology paradigms (e.g., serverless, containers, microservices, Api Management, Data Storage).
Develop solutions for the cloud and for Azure storage.
Design identity & security and data platform solutions.
Design Azure infrastructure strategy.
Work closely with Business Analysts, Product Managers, Data Managers and other team members to ensure successful production of application software.
Work closely with IT security to monitor the company's cloud privacy.
Respond to technical issues in a professional and timely manner.
Offer guidance in infrastructure movement techniques including bulk application transfers into the cloud.
Identify the top cloud architecture solution patterns to successfully meet the strategic needs of the company.
Location: - Mumbai, Bangalore","Java, Hadoop, .NET, Apache Spark, Node.js, Kafka, Windows, Mssql, Jenkins, Hive, Javascript, Linux, Cloudera, Microsoft Azure, Python"
Senior Data Architect,Uplers,15-17 Years,,"Thiruvananthapuram, Thiruvananthapuram / Trivandrum, India",Login to check your skill match score,"Experience: 13.00 + years
Salary: Confidential (based on experience)
Shift: (GMT+05:30) Asia/Kolkata (IST)
Opportunity Type: Remote
Placement Type: Full time Permanent Position
(*Note: This is a requirement for one of Uplers client - Forbes Advisor)
What do you need for this opportunity
Must have skills required:
DBT, GCP, DWH, Data Modelling, Data Governance, data quality, data monitoring, Cost Management, Multi Cloud
Forbes Advisor is Looking for:
Company Description
Forbes Advisor, part of the Forbes Marketplace family, provides consumers with
expert-written insights, news, and reviews on personal finance, health, business, and
everyday life decisions. We empower our audience with data-driven knowledge so
they can make informed choices confidentlybalancing the agility of a startup with
the stability of a seasoned enterprise
Role Overview
The Senior Data Architect is a strategic, senior leadership role responsible for
setting the vision and direction of our data warehousing function. You will architect,
implement, and maintain a state-of-the-art data warehouse that drives actionable
insights across revenue, subscriptions, paid marketing channels, and operational
functions. Your leadership will ensure data quality, robust pipeline design, and
seamless integration with business intelligence tools. This role requires a strong mix
of technical acumen, team management, and cross-functional collaboration
especially with teams focused on SEM, Digital Experiences, and revenue attribution
Job Description
Key Responsibilities
Strategic Data Architecture & Pipeline Leadership
Vision & Strategy:
Define and execute the long-term strategy for our data warehousing
platform using medallion architecture (Bronze, Silver, Gold layers) and
modern cloud-based solutions.
End-to-End Pipeline Oversight:
Oversee data ingestion (via Google Ads, Bing Ads, Facebook Ads, GA,
APIs, SFTP, etc.), transformation (leveraging DBT, and SQL [via
BigQuery]), and reporting, ensuring that our pipelines are robust and
scalable.
Data Modeling Best Practices:
Champion best practices in data modeling, including the effective use
of DBT packages to streamline complex transformations.
Data Quality, Governance & Attribution
Quality & Validation:
Establish and enforce rigorous data quality standards, governance
policies, and automated validation frameworks across all data streams.
Standardization & Visibility:
Collaborate with the Data Engineering, Insights and BIOps team to
standardize data definitions (including engagement metrics and
revenue attribution) and ensure consistency across all reports.
Attribution Focus:
Develop frameworks to reconcile revenue discrepancies and unify
validation across Finance, SEM, and Analytics teams.
Ensure accurate attribution of revenue and paid marketing channel
performance, working closely with SEM and Digital Experiences teams.
Monitoring & Alerting:
Implement robust monitoring and alerting systems (e.g., Slack and
email notifications) to quickly identify, diagnose, and resolve data
pipeline issues.
Team Leadership & Cross-Functional Collaboration
People & Process:
Lead, mentor, and grow a high-performing team of data warehousing
specialists, fostering a culture of accountability, innovation, and
continuous improvement.
Stakeholder Engagement:
Partner with RevOps, Analytics, SEM, Finance, and Product teams to align
the data infrastructure with business objectives.
Serve as the primary data warehouse expert in discussions around
revenue attribution and paid marketing channel performance, ensuring
that business requirements drive technical solutions.
Communication:
Translate complex technical concepts into clear business insights for
both technical and non-technical stakeholders.
Operational Excellence & Process Improvement
Deployment & QA:
Oversee deployment processes, including staging, QA, and rollback
strategies, to ensure minimal disruption during updates.
Continuous Optimization:
Regularly assess and optimize data pipelines for performance,
scalability, and reliability while reducing operational overhead.
Legacy to Cloud Transition:
Lead initiatives to transition from legacy on-premise systems to
modern cloud-based architectures for improved agility and cost
efficiency.
Innovation & Thought Leadership
Emerging Trends:
Stay abreast of emerging trends and technologies in data warehousing,
analytics, and cloud solutions.
Pilot Projects:
Propose and lead innovative projects to enhance our data capabilities,
with a particular focus on predictive and prescriptive analytics.
Executive Representation:
Represent the data warehousing function in senior leadership
discussions and strategic planning sessions
Qualifications
Education & Experience
Bachelor's or Master's degree in Computer Science, Data Science, Information
Systems, or a related field.
15+ years of experience in data engineering, warehousing, or analytics roles,
with at least 5+ years in a leadership capacity.
Proven track record in designing and implementing scalable data
warehousing solutions in cloud environments.
Technical Expertise
Deep experience with medallion architecture and modern data pipeline tools,
including DBT (and DBT packages), Databricks, SQL, and cloud-based data
platforms.
Strong understanding of ETL/ELT best practices, data modeling (logical and
physical), and large-scale data processing.
Hands-on experience with BI tools (e.g., Tableau, Looker) and familiarity with
Google Analytics, and other tracking systems.
Solid understanding of attribution models (first-touch, last-touch, multi-
touch) and experience working with paid marketing channels.
Leadership & Communication
Excellent leadership and team management skills with the ability to mentor
and inspire cross-functional teams.
Outstanding communication skills, capable of distilling complex technical
information into clear business insights.
Demonstrated ability to lead strategic initiatives, manage competing
priorities, and deliver results in a fast-paced environment.
Perks & Benefits
Flexible/Remote Working: Enjoy flexible work arrangements in a collaborative,
distributed team culture.
Competitive Compensation: Attractive salary, performance-based bonuses,
and comprehensive benefits.
Time Off: Generous paid time off, parental leave policies, and a dedicated day
off on the 3rd Friday of each month.
If you are a visionary leader with a passion for building resilient data infrastructures,
a deep understanding of revenue attribution and paid marketing channels, and a
proven ability to drive strategic business outcomes through data, we invite you to
join our Data & Analytics team and shape the future of our data warehousing
function.
How to apply for this opportunity
Step 1: Click On Apply! And Register or Login on our portal.
Step 2: Complete the Screening Form & Upload updated Resume
Step 3: Increase your chances to get shortlisted & meet the client for the Interview!
About Uplers:
Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement.
(Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).
So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!","Multi Cloud, Cost Management, Looker, dbt, Data Modelling, BigQuery, Tableau, Sql, Data Quality, data monitoring, Gcp, Dwh, Google Analytics, Data Governance"
Data Architect - Senior Manager,PwC Acceleration Centers in India,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in integration and platform architecture focus on designing and implementing seamless integration solutions and robust platform architectures for clients. They enable efficient data flow and optimise technology infrastructure for enhanced business performance. Those in solution architecture at PwC will design and implement innovative technology solutions to meet clients business needs. You will leverage your experience in analysing requirements, developing technical designs to enable the successful delivery of solutions.
Growing as a strategic advisor, you leverage your influence, expertise, and network to deliver quality results. You motivate and coach others, coming together to solve complex problems. As you increase in autonomy, you apply sound judgment, recognising when to take action and when to escalate. You are expected to solve through complexity, ask thoughtful questions, and clearly communicate how things fit together. Your ability to develop and sustain high performing, diverse, and inclusive teams, and your commitment to excellence, contributes to the success of our Firm.
Skills
Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:
Craft and convey clear, impactful and engaging messages that tell a holistic story.
Apply systems thinking to identify underlying problems and/or opportunities.
Validate outcomes with clients, share alternative perspectives, and act on client feedback.
Direct the team through complexity, demonstrating composure through ambiguous, challenging and uncertain situations.
Deepen and evolve your expertise with a focus on staying relevant.
Initiate open and honest coaching conversations at all levels.
Make difficult decisions and take action to resolve issues hindering team effectiveness.
Model and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.
Senior Data Architect
Job Description
Senior Data Modeler with experience in leading and delivering data modeling solutions involving complex data landscapes and legacy modernization projects. The ideal candidate will have extensive experience with industry-standard data modeling methodologies, cloud data lake platforms, and NoSQL data modeling. This role requires a strong leader capable of guiding data teams and driving the design and implementation of scalable data architectures.
Key Responsibilities
Design and implement scalable and efficient data models for complex systems, ensuring alignment with business and technical requirements.
Apply industry-standard data modeling methodologies to create conceptual, logical, and physical data models for structured, semi-structured, and unstructured data.
Develop and optimize data models for cloud data lake platforms (e.g., AWS, Azure, Google Cloud).
Create and manage NoSQL data models tailored to non-relational databases (e.g., MongoDB, Cassandra, DynamoDB).
Lead cross-functional data teams, overseeing the delivery of large-scale, multi-technology projects.
Collaborate with stakeholders to define data architecture strategies and ensure models align with enterprise data governance standards.
Integrate and manage data across diverse systems, ensuring consistency, integrity, and accuracy.
Evaluate emerging technologies and tools to enhance data modeling practices and infrastructure.
Provide technical guidance and mentorship to junior team members.
Document data modeling processes and create reusable frameworks for future projects.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
13+ years of experience in data modeling, including conceptual, logical, and physical data design.
5 8 years of experience in cloud data lake platforms such as AWS Lake Formation, Delta Lake, Snowflake or Google Big Query.
Proven experience with NoSQL databases and data modeling techniques for non-relational data.
Experience with data warehousing concepts, ETL/ELT processes, and big data frameworks (e.g., Hadoop, Spark).
Hands-on experience delivering complex, multi-module projects in diverse technology ecosystems.
Strong understanding of data governance, data security, and compliance best practices.
Proficiency with data modeling tools (e.g., ER/Studio, ERwin, PowerDesigner).
Excellent leadership and communication skills, with a proven ability to manage teams and collaborate with stakeholders.
Preferred Skills
Experience with modern data architectures, such as data fabric or data mesh.
Knowledge of graph databases and modeling for technologies like Neo4j.
Proficiency with programming languages like Python, Scala, or Java.
Understanding of CI/CD pipelines and DevOps practices in data engineering.","Data modeling methodologies, Data Security, Data Governance, Data Warehousing Concepts"
Senior Data Architect,Uplers,15-17 Years,,"Thiruvananthapuram, Thiruvananthapuram / Trivandrum, India",Login to check your skill match score,"Experience: 13.00 + years
Salary: Confidential (based on experience)
Shift: (GMT+05:30) Asia/Kolkata (IST)
Opportunity Type: Remote
Placement Type: Full time Permanent Position
(*Note: This is a requirement for one of Uplers client - Forbes Advisor)
What do you need for this opportunity
Must have skills required:
DBT, GCP, DWH, Data Modelling, Data Governance, data quality, data monitoring, Cost Management, Multi Cloud
Forbes Advisor is Looking for:
Company Description
Forbes Advisor, part of the Forbes Marketplace family, provides consumers with
expert-written insights, news, and reviews on personal finance, health, business, and
everyday life decisions. We empower our audience with data-driven knowledge so
they can make informed choices confidentlybalancing the agility of a startup with
the stability of a seasoned enterprise
Role Overview
The Senior Data Architect is a strategic, senior leadership role responsible for
setting the vision and direction of our data warehousing function. You will architect,
implement, and maintain a state-of-the-art data warehouse that drives actionable
insights across revenue, subscriptions, paid marketing channels, and operational
functions. Your leadership will ensure data quality, robust pipeline design, and
seamless integration with business intelligence tools. This role requires a strong mix
of technical acumen, team management, and cross-functional collaboration
especially with teams focused on SEM, Digital Experiences, and revenue attribution
Job Description
Key Responsibilities
Strategic Data Architecture & Pipeline Leadership
Vision & Strategy:
Define and execute the long-term strategy for our data warehousing
platform using medallion architecture (Bronze, Silver, Gold layers) and
modern cloud-based solutions.
End-to-End Pipeline Oversight:
Oversee data ingestion (via Google Ads, Bing Ads, Facebook Ads, GA,
APIs, SFTP, etc.), transformation (leveraging DBT, and SQL [via
BigQuery]), and reporting, ensuring that our pipelines are robust and
scalable.
Data Modeling Best Practices:
Champion best practices in data modeling, including the effective use
of DBT packages to streamline complex transformations.
Data Quality, Governance & Attribution
Quality & Validation:
Establish and enforce rigorous data quality standards, governance
policies, and automated validation frameworks across all data streams.
Standardization & Visibility:
Collaborate with the Data Engineering, Insights and BIOps team to
standardize data definitions (including engagement metrics and
revenue attribution) and ensure consistency across all reports.
Attribution Focus:
Develop frameworks to reconcile revenue discrepancies and unify
validation across Finance, SEM, and Analytics teams.
Ensure accurate attribution of revenue and paid marketing channel
performance, working closely with SEM and Digital Experiences teams.
Monitoring & Alerting:
Implement robust monitoring and alerting systems (e.g., Slack and
email notifications) to quickly identify, diagnose, and resolve data
pipeline issues.
Team Leadership & Cross-Functional Collaboration
People & Process:
Lead, mentor, and grow a high-performing team of data warehousing
specialists, fostering a culture of accountability, innovation, and
continuous improvement.
Stakeholder Engagement:
Partner with RevOps, Analytics, SEM, Finance, and Product teams to align
the data infrastructure with business objectives.
Serve as the primary data warehouse expert in discussions around
revenue attribution and paid marketing channel performance, ensuring
that business requirements drive technical solutions.
Communication:
Translate complex technical concepts into clear business insights for
both technical and non-technical stakeholders.
Operational Excellence & Process Improvement
Deployment & QA:
Oversee deployment processes, including staging, QA, and rollback
strategies, to ensure minimal disruption during updates.
Continuous Optimization:
Regularly assess and optimize data pipelines for performance,
scalability, and reliability while reducing operational overhead.
Legacy to Cloud Transition:
Lead initiatives to transition from legacy on-premise systems to
modern cloud-based architectures for improved agility and cost
efficiency.
Innovation & Thought Leadership
Emerging Trends:
Stay abreast of emerging trends and technologies in data warehousing,
analytics, and cloud solutions.
Pilot Projects:
Propose and lead innovative projects to enhance our data capabilities,
with a particular focus on predictive and prescriptive analytics.
Executive Representation:
Represent the data warehousing function in senior leadership
discussions and strategic planning sessions
Qualifications
Education & Experience
Bachelor's or Master's degree in Computer Science, Data Science, Information
Systems, or a related field.
15+ years of experience in data engineering, warehousing, or analytics roles,
with at least 5+ years in a leadership capacity.
Proven track record in designing and implementing scalable data
warehousing solutions in cloud environments.
Technical Expertise
Deep experience with medallion architecture and modern data pipeline tools,
including DBT (and DBT packages), Databricks, SQL, and cloud-based data
platforms.
Strong understanding of ETL/ELT best practices, data modeling (logical and
physical), and large-scale data processing.
Hands-on experience with BI tools (e.g., Tableau, Looker) and familiarity with
Google Analytics, and other tracking systems.
Solid understanding of attribution models (first-touch, last-touch, multi-
touch) and experience working with paid marketing channels.
Leadership & Communication
Excellent leadership and team management skills with the ability to mentor
and inspire cross-functional teams.
Outstanding communication skills, capable of distilling complex technical
information into clear business insights.
Demonstrated ability to lead strategic initiatives, manage competing
priorities, and deliver results in a fast-paced environment.
Perks & Benefits
Flexible/Remote Working: Enjoy flexible work arrangements in a collaborative,
distributed team culture.
Competitive Compensation: Attractive salary, performance-based bonuses,
and comprehensive benefits.
Time Off: Generous paid time off, parental leave policies, and a dedicated day
off on the 3rd Friday of each month.
If you are a visionary leader with a passion for building resilient data infrastructures,
a deep understanding of revenue attribution and paid marketing channels, and a
proven ability to drive strategic business outcomes through data, we invite you to
join our Data & Analytics team and shape the future of our data warehousing
function.
How to apply for this opportunity
Step 1: Click On Apply! And Register or Login on our portal.
Step 2: Complete the Screening Form & Upload updated Resume
Step 3: Increase your chances to get shortlisted & meet the client for the Interview!
About Uplers:
Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant contractual onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement.
(Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).
So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!","Multi Cloud, Cost Management, Looker, dbt, Data Modelling, BigQuery, Tableau, Sql, Data Quality, data monitoring, Gcp, Dwh, Google Analytics, Data Governance"
Data Architect,G10X,15-17 Years,,India,Login to check your skill match score,"Data Architect
Exp: 15 yrs
Location: Remote / Kochi, Kerala
Notice Period: Immediate to 30 days
Architect and implement data ingestion, data validation, and data transformation pipelines. Collaborate with the Data teams to design and maintain batch and streaming integrations across a variety of data domains and platforms. Take ownership in building solutions and proposing architectural designs related to building efficient and timely data ingestion and transformation processes geared towards analytics workloads. Manage code deployment to various environments. Be proficient at positively critiquing and suggesting improvements via code reviews Work with stakeholders to define and develop data ingest, validation, and transform pipelines. Troubleshoot data pipelines and resolve issues in alignment with SDLC. Ability to diagnose and troubleshoot data issues, recognizing common data integration and transformation patterns Estimate, track, and communicate status of assigned items to a diverse group of stakeholders required
Collaborate with application architects, database administrators, and business stakeholders to ensure seamless data transition and alignment Design ETL (Extract, Transform, Load) processes to support data migration, ensuring high availability and performance. Define data mapping, transformation, and validation strategies during migration. Provide leadership and mentorship to other members of the data team and stakeholders","Data ingestion, Code deployment, Streaming integrations, Data Mapping, Data Validation, Data Migration, Data Transformation"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Gurugram, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata
work Mode- Hybrid
Roles And Responsibilities
Mandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization
Solution Architect for Data modelling Understanding of Enterprise datasets Sales,
Procurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle
etc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building
Data lake foundation, Maintenance etc)
Collaborate with the product/business teams, understand related business processes and
document business requirements and then write high level specifications/requirements for DEs
Develop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout
for right grain of data either in True source systems or in Data WHs and build reusable data
models in intermediary layers before creating physical consumable views from data mart
Understand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data
Governance, Data Quality frameworks, Data Observality and the candidate should be:
Familiar with DevOps process
Knowing how to check existing tables, data dictionary, table structures
Experienced with normalizing tables
Having good understanding of Landing, Bronze, Silver and Gold layers and concepts
Familiar with Agile techniques
Create business process map, user journey map and data flow integration diagrams; Understand
Integration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of
models
Stakeholder management with data engineering, product owners, central data modelling team,
data governance & stewards, Scrum master, project team and sponsor.
Ability to handle large implementation program with multiple projects spanning over an year.
Skills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Data Observability, Data lake foundation, DataOps, Data Quality frameworks, Agile techniques, data vault, FTP, SAP, Data Architect, Data Modeling, Data Modeler, Dimensional Modeling, Sql, Cloud Architecture, Devops, Sftp, Data Governance, Azure, Data Warehouse, Oracle"
Data Architect,VidPro Consultancy Services,8-15 Years,,India,Login to check your skill match score,"Role: Data Architect
Experience: 8-15 years
Location: Bangalore, Chennai, Gurgaon, Pune, and Kolkata
Mandatory Skills: Python, Pyspark, SQL, ETL, Pipelines, Azure Databricks, Azure Data Factory, & Architect Designing.
Primary Roles and Responsibilities:
Developing Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack
Ability to provide solutions that are forward-thinking in data engineering and analytics space
Collaborate with DW/BI leads to understand new ETL pipeline development requirements.
Triage issues to find gaps in existing pipelines and fix the issues
Work with business to understand the need in reporting layer and develop data model to fulfill reporting needs
Help joiner team members to resolve issues and technical challenges.
Drive technical discussion with client architect and team members
Orchestrate the data pipelines in scheduler via Airflow
Skills and Qualifications:
Bachelor's and/or master's degree in computer science or equivalent experience.
Must have total 8+ yrs. of IT experience and 5+ years experience in Data warehouse/ETL projects.
Deep understanding of Star and Snowflake dimensional modelling.
Strong knowledge of Data Management principles
Good understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture
Should have hands-on experience in SQL, Python and Spark (PySpark)
Candidate must have experience in AWS/ Azure stack
Desirable to have ETL with batch and streaming (Kinesis).
Experience in building ETL / data warehouse transformation processes
Experience with Apache Kafka for use with streaming data / event-based data
Experience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)
Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
Experience working with structured and unstructured data including imaging & geospatial data.
Experience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.
Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot
Databricks Certified Data Engineer Associate/Professional Certification (Desirable).
Comfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects
Should have experience working in Agile methodology
Strong verbal and written communication skills.
Strong analytical and problem-solving skills with a high attention to detail.","Airflow, CircleCI, Apache Kafka, Databricks, Sql, Pig, Pl Sql, Impala, Azure Data Factory, RDBMS, Hadoop, Pyspark, Etl, AWS, Unix Shell Scripting, Hive, Cassandra, Python, Neo4j, Terraform, Git, MongoDB"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
What You'll Do
Define and design future state data architecture for HR reporting, forecasting and analysis products.
Partner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.
Engage with line of business, operations, and project partners to gather process improvements.
Lead to design / build new models to efficiently deliver the financial results to senior management.
Evaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.
Collaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing POCs to ensure those standards are implemented.
Provide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.
Develop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.
Create and maintain conceptual / logical data models to identify key business entities and visual relationships.
Work with business and IT teams to understand data requirements.
Maintain a data dictionary consisting of table and column definitions.
Review data models with both technical and business audience
What You'll Bring
Essential Education
Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
Additional Certification in Data Management or cloud data platforms like Snowflake preferred
Essential Experience & Job Requirements
12+ years of IT experience with major focus on data warehouse/database related projects
Expertise in cloud databases like Snowflake, Redshift etc.
Expertise in Data Warehousing Architecture; BI/Analytical systems; Data cataloguing; MDM etc
Proficient in Conceptual, Logical, and Physical Data Modelling
Proficient in documenting all the architecture related work performed.
Proficient in data storage, ETL/ELT and data analytics tools like AWS Glue, DBT/Talend, FiveTran, APIs, Tableau, Power BI, Alteryx etc
Experience in building Data Solutions to support Comp Benchmarking, Pay Transparency / Pay Equity and Total Rewards use cases preferred.
Experience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake a plus
Experience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs) a plus
Excellent written, oral communication and presentation skills to present architecture, features, and solution recommendations is a must
Additional info
YOU'RE GOOD AT
Design, document & train the team on the overall processes and process flows for the Data architecture.
Resolve technical challenges in critical situations that require immediate resolution.
Develop relationships with external stakeholders to maintain awareness of data and security issues and trends.
Review work from other tech team members and provide feedback for growth.
Implement Data security policies that align with governance objectives and regulatory requirements.
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","Cloud Databases, Cloud Big Data Technologies, FiveTran, Physical Data Modelling, Data Analytics Tools, dbt, MDM, Meta Scrum, Data Cataloguing, Kanban, Data Warehousing Architecture, BI Analytical Systems, Alteryx, AWS Glue, Tableau, ELT, Data Architecture, Scrum, Talend, AWS, Apis, Power Bi, Agile Methodologies, Gcp, Azure, Etl"
Data Architect,CriticalRiver Inc.,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Role: Data Architect
Experience:- 10 Years
Work Location :- Hyderabad
Key Responsibilities:
Design and implement scalable, high-performance data architectures using Snowflake.
Develop ETL/ELT pipelines using dbt (Data build tool), FiveTran and Airflow to ingest, transform, and process large volumes of data.
Strong experience of data modelling using dbt
Optimize data pipelines for efficiency, reliability, and scalability.
Ensure data integrity, governance, and security across the data ecosystem.
Strong understanding of dimensional data modelling and data warehousing concepts.
Strong experience in SQL and Python
Collaborate with data engineers, analysts, and business stakeholders to define data requirements and architecture.
Work with cloud platforms (AWS, GCP or Azure) to manage and scale data infrastructure.
Implement reverse ETL solutions using Hightouch for data activation and operational analytics.
Monitor and troubleshoot data pipelines and workflows to ensure smooth operations.
Drive best practices in data modeling, performance tuning, and cost optimization.
Stay updated with emerging technologies and trends in cloud-based data engineering.
Understanding of ERP (NetSuite) data is added advantage.
Strong communication skills","Airflow, snowflake, dbt, Hightouch, dimensional data modelling, data integrity governance, FiveTran, Sql, AWS, Data Warehousing, Python, Azure, Gcp"
Senior Manager - Data Architect - Pune,Telecoms Management,12-15 Years,,"Pune, India",Login to check your skill match score,"Hybrid
Senior Manager - Data Architect - Pune
Pune, Maharashtra, India
Apply Now
Find out how well you match with this job
Requisition ID
260603
Date posted
04/23/2025
Organisational Unit
Data & Analytics
Join Us
At Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.
What You'll Do
Perform ETL design, data architecture, data management and data analysis.
Develop and enforce data management policies, standards, and best practices.
Work with stakeholders to define data models, metadata, and integration strategies.
Experience in Architecting and designing solutions leveraging capabilities of ETL tools like Ab-Initio.
Accountable to business and technology management for end to end application scoping, planning, development and delivery that meets and exceeds quality standards
Optimize data storage, processing, and retrieval to improve performance and cost efficiency.
Ensure system alignment to Enterprise Architecture policies and best practices; ensure that process methodologies are followed in system development
Assist Project manager with the estimation of technical timelines and allocation of the technical resources to specific task
Contribution to the organization in terms of process adoption, resource optimization, tools adoption to bring in efficiency and uplift quality
Contribution to the Guilds with the technical expertise
Who You Are
Experience on ETL solutions using Ab-Initio & Teradata, Data Modelling, GCP.
Exposure on Data Modelling tools eg. Power Designer, Erwin etc.
Knowledge on Data Warehousing, Data Modelling, Data Profiling, GCP Cloud migration etc.
Overall experience of 12-15 years
Relevant experience of 4-5 years
Not a perfect fit
Worried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.
What's In It For You
Who we are
We are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.
Belonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.
If you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.
Together we can.
Insights from previous hires
Top skills
Agile
Business and Commercial Acumen
Communication
Coaching
Business Partnering
Business Development
Engineering
Change and Adaptability
Budgeting
Budget Management
Previously worked at
Previously worked as
Senior Manager
Data Architect
Senior Data Architect
Senior Manager Technology
Lead Specialist
Requisition ID
260603
Date posted
04/23/2025
Organisational Unit
Data & Analytics
Join Us
At Vodafone, we're not just shaping the future of connectivity for our customers we're shaping the future for everyone who joins our team. When you work with us, you're part of a global mission to connect people, solve complex challenges, and create a sustainable and more inclusive world. If you want to grow your career whilst finding the perfect balance between work and life, Vodafone offers the opportunities to help you belong and make a real impact.
Who You Are
Experience on ETL solutions using Ab-Initio & Teradata, Data Modelling, GCP.
Exposure on Data Modelling tools eg. Power Designer, Erwin etc.
Knowledge on Data Warehousing, Data Modelling, Data Profiling, GCP Cloud migration etc.
Overall experience of 12-15 years
Relevant experience of 4-5 years
Not a perfect fit
Worried that you don't meet all the desired criteria exactly At Vodafone we are passionate about empowering people and creating a workplace where everyone can thrive, whatever their personal or professional background. If you're excited about this role but your experience doesn't align exactly with every part of the job description, we encourage you to still apply as you may be the right candidate for this role or another opportunity.
Who We Are
We are a leading international Telco, serving millions of customers. At Vodafone, we believe that connectivity is a force for good. If we use it for the things that really matter, it can improve people's lives and the world around us. Through our technology we empower people, connecting everyone regardless of who they are or where they live and we protect the planet, whilst helping our customers do the same.
Belonging at Vodafone isn't a concept; it's lived, breathed, and cultivated through everything we do. You'll be part of a global and diverse community, with many different minds, abilities, backgrounds and cultures. ;We're committed to increase diversity, ensure equal representation, and make Vodafone a place everyone feels safe, valued and included.
If you require any reasonable adjustments or have an accessibility request as part of your recruitment journey, for example, extended time or breaks in between online assessments, please refer to https://careers.vodafone.com/application-adjustments/ for guidance.
Together we can.","power designer, Teradata, Erwin, Data Modelling, Gcp, Data Profiling, Data Warehousing, Cloud Migration, Ab-initio, Etl"
Big Data Architect,Skyhigh Security,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title:
Big Data Architect
About Skyhigh Security:
Skyhigh Security is a dynamic, fast-paced, cloud company that is a leader in the security industry. Our mission is to protect the world's data, and because of this, we live and breathe security. We value learning at our core, underpinned by openness and transparency.
Since 2011, organizations have trusted us to provide them with a complete, market-leading security platform built on a modern cloud stack. Our industry-leading suite of products radically simplifies data security through easy-to-use, cloud-based, Zero Trust solutions that are managed in a single dashboard, powered by hundreds of employees across the world. With offices in Santa Clara, Aylesbury, Paderborn, Bengaluru, Sydney, Tokyo and more, our employees are the heart and soul of our company.
Skyhigh Security Is more than a company; here, when you invest your career with us, we commit to investing in you. We embrace a hybrid work model, creating the flexibility and freedom you need from your work environment to reach your potential. From our employee recognition program, to our Blast Talks learning series, and team celebrations (we love to have fun!), we strive to be an interactive and engaging place where you can be your authentic self.
We are on these too! Follow us on LinkedIn and Twitter@SkyhighSecurity.
Role Overview:
The Big Data Architect will be responsible for the design, implementation, and management of the organization's big data infrastructure. The ideal candidate will have a strong technical background in big data technologies, excellent problem-solving skills, and the ability to work in a fast-paced environment. The role requires a deep understanding of data architecture, data modeling, and data integration techniques.
About the Role:
Design and implement scalable and efficient big data architecture solutions to meet business requirements.
Develop and maintain data pipelines, ensuring the availability and quality of data.
Collaborate with data scientists, data engineers, and other stakeholders to understand data needs and provide technical solutions.
Lead the evaluation and selection of big data tools and technologies.
Ensure data security and privacy compliance.
Optimize and tune big data systems for performance and cost-efficiency.
Document data architecture, data flows, and processes.
Stay up-to-date with the latest industry trends and best practices in big data technologies.
About You:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
over all 10+ years exp with 5+ years of experience in big data architecture and engineering.
Proficiency in big data technologies such as Hadoop mapredue, Spark batch and streaming, Kafka, HBase, Scala, Elastic Search and others.
Experience with AWS cloud platform.
Strong knowledge of data modeling, ETL processes, and data warehousing.
Proficiency in programming languages such as Java, Scala, Spark
Familiarity with data visualization tools and techniques.
Excellent communication and collaboration skills.
Strong problem-solving abilities and attention to detail.
Company Benefits and Perks:
We work hard to embrace diversity and inclusion and encourage everyone to bring their authentic selves to work every day. We offer a variety of social programs, flexible work hours and family-friendly benefits to all of our employees.
Retirement Plans
Medical, Dental and Vision Coverage
Paid Time Off
Paid Parental Leave
Support for Community Involvement
We're serious about our commitment to diversity which is why we prohibit discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status.","ETL processes, Data visualization tools, Java, Hadoop, Scala, Kafka, Data Modeling, Data Warehousing, HBase, Spark, Elastic Search, AWS"
Cloud Data Architect,myCloudDoor,3-5 Years,,"Jodhpur, India",Login to check your skill match score,"Do you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!
Who we are
myCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.
Tasks
The Selected Person Will Do The Following Tasks
Definition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...
Maintenance of data solutions, failure analysis and solution proposal.
Communication with customers: proposal solutions, technical trainings...
The profile
We are looking for a person who fit the following requirements:
+3 years of experience years of experience in a similar role.
Experience in Azure projects
Real experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...
Real Experience in AI
Experience in presales and proposals
What we offer you
Career Path
Remote working
Training: Internal and technical certifications
Think you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.","Presales, Proposals, Ai, Data Lakes, Azure, Data Architecture, Data Warehouse"
Data Architect,LSEG (London Stock Exchange Group),12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Data Architect Corporate Engineering
Company Profile
LSEG (London Stock Exchange Group) is a world-leading financial markets infrastructure and data business. We are dedicated, open-access partners with a commitment to excellence in delivering services across Data & Analytics, Capital Markets, and Post Trade.
Backed by three hundred years of experience, innovative technologies, and a team of over 23,000 people in 70 countries, our purpose is driving financial stability, empowering economies, and enabling customers to create sustainable growth.
Role Profile
In this role, you'll be joining our CRM, External Digital and Marketing team within Data and Integrations Team, Corporate Engineering (CE) as a Data Architect. This team works on data and integration requests by guiding customers on data migration cleansing, data quality techniques. This role impacts all divisional users of CRM Technology and downstream systems reliant on Customer Data 7k users across Capital Markets, Post Trade and Data & Analytics.
The data and integrations team do this by:
Conducting data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion/AWS Glue).
Building Enterprise Data Architecture or Data Lake for the Migration projects
Defining the data quality of the sources and lists the data quality metrics of the source systems.
Working on detailed scoping and requirements working with business customers, SMEs, Technology Partner organizations and internal CRM Technology and Corporate Technology teams.
Building and maintaining Customer Master and Product Master.
Continuously review operating metrics and data to find opportunities to improve.
Tech Profile/Essential Skills
12 years of technical experience.
7 years of experience on data migration and reporting using ETL and Reporting Tools
5 years of experience on ETL/Database Development
2 years of experience on Salesforce Data Migration projects
Expert level skill on Informatica IICS or Matillion or AWS Glue or equivalent ETL tool which covers from extracting the source to building the complex mappings.
Proficient in the use or extract of data from Salesforce.
Must have knowledge on the Salesforce Data modelling.
Must have Experience on Snowflake Storage and Database.
Expert level coding knowledge on Python to do ETL.
Able to translate business requirements into data solutions.
Understanding of Salesforce concepts.
Snowflake development.
Experience with providing technical solutions and supporting documentation.
Preferred Skills And Experience
Experience of Tableau and/or Power BI reporting preferred for management reporting.
Must have experience of working with Cloud Native based applications.
Understanding of the SDLC and agile delivery methodology.
Experience working with databases and data, performing data cleanup, and/or data manipulation and migration to and from Salesforce.com.
Should have experience with Enterprise Architect or Erwin Data modeler
Ability to handle own work and multitask to meet tight deadlines without losing sight of priorities under minimum supervision.
Highly motivated, self-directed individual with a positive & pro-active demeanor to work.
Customer and service focused, with determination to meet their needs and expectations.
Be driven and committed to the goals and objectives of the team and organization.
Education and Professional Skills
Professional qualification or equivalent.
BS/MS degree in Computer Science, Software Engineering or STEM degree (Desirable).
Curious about new technologies and tools, creative thinking and initiative taking.
Agile related certifications preferable.
Detailed Responsibilities
Proficient in data discovery, data migration, data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion Preferable).
Analyses the data quality of the sources and lists the data quality metrics of the source systems.
Domain expert in MDM and Customer data.
Lead the Single Customer View and Customer 360 implementations.
Lead the data migration strategies for large scale programs.
Data mining to uncover patterns, anomalies, and correlations in large data sets.
Data management to efficiently and cost-effectively collect, store, and use data.
Coding languages like Python and Java to develop applications for data analysis.
Machine learning to build scalable systems for handling Big Data Systems.
Structured query language (SQL) to manipulate data.
Data modelling tools like Erwin or Visio to visualize metadata and database schema.
Creating and implementing data management processes and procedures
Researching data acquisition opportunities.
Developing application programming interfaces (APIs) to retrieve data.
Develops and improves data governance and business data processes within the Technology and business organizations and understands client requirements, specifying and analyzing these to a sufficient level of detail to ensure transparency of definition and ability for technical teams to translate to a technical solution design.
Works with developers, architects, and solution designers to translate sophisticated business requirements and provides feedback on technical solutions proposed.
Responsible for building a relationship with partners, collaborators and impacted users.
Demonstrates proposed solutions and seeks and addresses feedback.
Proactively identifies, recommends, and implements improvements to the process as it relates to assigned projects.
Flexible in approach, adapting plans and strategies to help handle risks around ambiguity.
Strategic problem solver with strong intuition for business and well-versed in current technological trends and business concepts.
LSEG Benefits
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate based on anyone's race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.
LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.
Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.
Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.
Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.
If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","IICS, Salesforce Data Migration, Snowflake Storage and Database, Matillion, Power Bi, AWS Glue, Tableau, Sql, Python, Etl"
CDC Data Architect,LSEG (London Stock Exchange Group),8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Design and Development: Build, enhance, and lead the company's logical, conceptual, and physical data models, demonstrating Snowflake's advanced capabilities.
Data Integration: Lead all aspects of the development of comprehensive data integration processes using ETL for various data types, including structured, semi-structured, and unstructured data, ensuring seamless integration with Snowflake.
Performance Tuning: Implement and fine-tune Snowflake features such as resource monitors, RBAC controls, scalable virtual warehouses, SQL performance tuning, zero copy clone, and time travel to optimize performance.
Data Security: Ensure robust data security and handle access controls effectively within the Snowflake environment.
Cloud Integration: Deploy cloud-based enterprise data warehouse solutions, Leverage AWS services such as S3, Glue, Athena, CloudWatch, and EMR to enhance data storage, processing, and analytics capabilities and integrate seamlessly with platforms like AWS and applying Snowflake's cloud-native architecture.
Data Governance: Uphold consistent data governance, testing, and continuous delivery practices, ensuring data integrity and compliance within Snowflake.
AI Integration: Incorporate AI and machine learning models into the data architecture, demonstrating Snowflake's capabilities to handle large-scale data processing and real-time analytics.
Snowpark/Python Development: Use Python/Snowpark for developing data pipelines, ETL processes, and automation scripts, ensuring efficient data handling and processing within the Snowflake environment
Teamwork: Serve as a data domain expert, working closely with various teams to ensure standard methodologies in data management are followed, and facilitate the integration of AI insights into business processes !
Candidate Profile / Key skills
Experience: At least 8 years in Data Engineering or Data Management Solutions, with a proven track record of improving data pipeline processes and leading initiatives.
Snowflake Expertise: A minimum of 5 years of meaningful experience with Snowflake.
Client Leadership: Skilled in leading data-centric client engagements.
Technical Proficiency: Demonstrable skills in sophisticated SQL, Unix Shell/Python scripting, performance tuning, and database optimization.
Cloud Technologies: Expertise in AWS services, including S3, EC2, Lambda, and Redshift.
Data Handling & Migration: Proficient in managing semi-structured data (JSON, XML) and using Snowflake's VARIANT attribute. Experience in migrating data from on-premises databases to Snowflake.
Automation: Skilled in crafting and developing automated data pipelines using Snowpipe and other relavant tools !
Database Experience: Hands-on experience with databases and data warehousing solutions such as Oracle, Microsoft SQL Server, AWS Redshift, or Snowflake.
Cloud Experience: Experience with AWS or Azure is a plus.
SQL Analysis: Strong SQL analysis skills and familiarity with tools like JIRA, Asana, or other relevant defect tracking tools. Experience in implementing data quality frameworks is an added advantage.
Programming Skills: Proficiency in Python, PySpark, or Snowpark is helpful. Additional expertise in Cortex or AI capabilities is a big plus.
LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.
Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.
Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.
Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.
If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","snowflake, Snowpark, Data Handling Migration, AI Integration, Automation, Sql, Cloud Technologies, Performance Tuning, AWS, Data Security, Etl, Data Integration, Python, Data Governance, Unix Shell"
Data Architect,Tarento Group,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Tarento Technologies is a dynamic and innovative technology solutions provider, specializing in delivering cutting-edge IT services and solutions. With a strong focus on software development, data analytics, cloud computing, and enterprise applications, Tarento aims to empower businesses to thrive in the digital age. The company's team of experts combines industry knowledge with technical expertise to deliver tailored solutions that meet the unique needs of each client.
At Tarento, we believe in fostering a collaborative, growth-oriented environment, where creativity and continuous learning are encouraged. As we expand, we are looking for passionate individuals to join our team and help shape the future of technology-driven solutions. If you're ready to take on exciting challenges and grow professionally, Tarento Technologies offers the ideal platform to advance your career.
We are looking for Technically Data Architects with hands-on experience in Apache Spark and Databricks skills with expertise on big data processing, data warehousing, and cloud platforms like Azure, AWS, or GCP.
Key Responsibilities
Design, develop, and optimize ETL/ELT pipelines using Databricks and Apache Spark.
Implement data lakes, data warehouses, and lakehouses using Delta Lake.
Develop scalable, high-performance data solutions for batch and streaming data processing.
Optimize Spark jobs for performance and cost efficiency.
Implement data governance, security, and compliance best practices.
Work with CI/CD pipelines for data workflows using tools like Terraform, Git, and DevOps practices.
Collaborate with data analysts, scientists, and business teams to understand data requirements.
Required Skills & Experience
10 years of experience in data engineering.
Strong expertise in Databricks, Apache Spark (PySpark/Scala/Java), and Delta Lake.
Proficiency in SQL, Python, or Scala.
Hands-on experience with ETL/ELT pipeline development.
Experience with cloud platforms (Azure Data Factory, AWS Glue, GCP Dataflow).
Knowledge of data modeling, data lakes, and data warehousing
Understanding of CI/CD, Git, and DevOps tools.
Strong troubleshooting and performance optimization skills.
Work Location : Bangalore","GCP Dataflow, CI CD, Delta Lake, Scala, AWS Glue, Apache Spark, Sql, ELT, Devops, Git, Azure Data Factory, Databricks, Python, Etl"
Data Architect,Velotio Technologies,6-9 Years,,"Pune, India",Login to check your skill match score,"Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.
As a Data Architect you'll be leading the effort to establish world-class data foundations in the world while working with real-time streaming plant data, cutting-edge ETL pipeline software tools, and solving complex data problems.
Responsibilities :
Lead the technical delivery of the company for large enterprises through the customer lifecycle: discovery, implementation, expansion, and ongoing support.
Design and build complex, streaming data pipelines that synthesize disparate manufacturing data sources using a proprietary ETL engine.
Collaborate with Data Scientists to design and implement advanced analytics using ML/AI.
Analyze key customer use cases, identify data sources, and architect common data solutions that unlock insights to enable data-driven continuous improvement.
Partner with customer teams to establish trust.
Drive adoption through internal and external data validation exercises.
Contribute to data dictionaries and process flow diagrams for complex data solutions.
Investigate, diagnose, and resolve data challenges using common data mining techniques that often involve creating custom Python notebooks or constructing complex SQL queries.
Collaborate with product and platform engineering teams to define and optimize new features at scale.
Desired Skills & Experience:
6 to 9 years of experience as a Data Architect.
Ability to work independently and collaboratively with other teams to achieve goals and represent the business.
Good to have experience working with batch or streaming data processes.
Strong analytical ability and problem-solving skills.
Must be comfortable working with customers to relate the concerns and suggest best possible practices.
Good to Have :
Experience with technologies such as Python, Java, Git, Pandas, or R
Experience working in a manufacturing environment or on 6 sigma Projects.
Experience in data analysis with SQL or NoSQL databases.
Our Culture:
We have an autonomous and empowered work culture encouraging individuals to take ownership and grow quickly
Flat hierarchy with fast decision making and a startup-oriented get things done culture
A strong, fun & positive environment with regular celebrations of our success. We pride ourselves in creating an inclusive, diverse & authentic environment
At Velotio, we embrace diversity. Inclusion is a priority for us, and we are eager to foster an environment where everyone feels valued. We welcome applications regardless of ethnicity or cultural background, age, gender, nationality, religion, disability or sexual orientation.","R, Nosql, Java, Git, Pandas, Sql, Python, Etl"
Cloud Data Architect,PURVIEW,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Job Description
A Cloud Data Architect who has experience working on Google Cloud Platform and have built solutions using Data Services of GCP
Experience of 10+ years and who understands the data principles and experience dealing with migrating BI data warehouses, BigData platforms into Google Cloud
Having experience on how to extract, transform, load the data into BI systems and experience with Datawarehousing will help
Experience building solutions using Google BigQuery, Google Dataflow, Google Cloud Storage, Google Pubsub which we use on our data platform
Experience working with business, engineering, data modeling teams in defining the architecture solution
Understands the architecture risks, design principles and suggest ways to business/engineering teams on tactical (vs) strategic solutions depending on various scenarios one would see
About Company :
Purview is a leading Digital Cloud & Data Engineering company headquartered in Edinburgh, United Kingdom having a presence in 14 countries India (Hyderabad, Bangalore, Chennai and Pune), Poland, Germany, Finland, Netherlands, Ireland, USA, UAE, Oman, Singapore, Hong Kong, Malaysia and Australia.
We have a strong presence in UK, Europe and APEC, providing services to Captive Clients (HSBC, NatWest, Northern Trust, IDFC First Bank, Nordia Bank etc) in fully managed solutions and co-managed capacity models. Also, we support various top IT tier 1 organisations (Capgemini, Deloitte, Wipro, Virtusa, L&T, CoForge, TechM and more) to deliver solutions and workforce/resources.
In
Company Info:
3rd Floor, Sonthalia Mind Space
Near Westin Hotel, Gafoor Nagar
Hitechcity, Hyderabad
Phone: +91 40 48549120 / +91 8790177967
Uk
Gyleview House, 3 Redheughs Rigg,
South Gyle, Edinburgh, EH12 9DQ.
Phone: +44 7590230910
Email: [HIDDEN TEXT]
Login to Apply !","Google Dataflow, Google Cloud Storage, Google Pubsub, Google BigQuery, Data Services of GCP, Google Cloud Platform"
Data Architect,ACL Digital,8-10 Years,,"Pune, India",Login to check your skill match score,"Total yrs of Exp: 8+ yrs
Location:Balewadi, Pune
Technical skills and core competencies
Strong understanding of Data Architecture and models and leading data driven projects.
Solid expertise and strong opinions on Data Modelling paradigms such as Kimball, Inmon, Data Marts, Data Vault, Medallion etc.
Strong experience with Cloud Based data strategies and big data technologies AWS Preferred.
Solid experience in designing data pipelines for ETL with expert knowledge on ingestion, transformation, and data quality is a must
Hands-on experience in SQL is a must.
Deep understanding of PostGreSQL development, query optimization and designing indexes is a must.
An ability to understand and manipulate intermediate to complex level of SQL
Thorough knowledge of Postgres PL/SQL to work with complex warehouse workflows.
Ability to use advanced SQL concepts such as RANK, DENSE_RANK along with applying advanced statistical concepts through SQL is required.
Working experience with PostGres SQL extensions like PostGIS is desired.
Expertise writing ETL pipelines combining Python + SQL is required.
Understanding of data manipulation libraries in python like Pandas, Polars, DuckDB is desired
Experience in designing Data visualization with different tools such as Tableau and PowerBI is desirable.
Responsibilities
Participate in the design and developing features in the existing Data Warehouse.
Provide leadership in establishing connection between Engineering, product and analytics/data scientists team.
Design, implement, update existing/new batch ETL pipelines
Define and implement data architecture.
Partner with both engineers and data analysts to build reliable datasets that can be trusted, understood, and used by the rest of the company.
Work with various data orchestration tools (Apache Airflow, Dagster, Prefect and others)
Embrace a fast-paced start-up environment.
You should be passionate about your job and enjoy a fast-paced international working environment.
Background or experience in the telecom industry is a plus but not a requirement.
Love automating and enjoy monitoring","Cloud Based data strategies, PostGreSQL development, Postgres PL SQL, Query Optimization, Sql, Data Architecture"
Enterprise Data Architect,ACA Group,7-10 Years,,"Pune, India",Login to check your skill match score,"About ACA:
ACA was founded in 2002 by four former SEC regulators and one former state regulator. The founders saw a need for investment advisers to receive expert guidance on existing and new regulations. Over the years, ACA has grown both organically and by acquisition to expand our GRC business and technology solutions. Our services now include GIPS standards verification, cybersecurity and technology risk, regulatory technology, ESG advisory, AML and financial crimes, financial and regulatory reporting, and Mirabella for establishing EU operations.
ACA is an equal opportunity employer that values diversity. We conduct our business without regard to actual or perceived age, race, color, religion, disability, caregiver, marital or partnership status, pregnancy (including childbirth, breastfeeding, or related medical conditions), ancestry, national origin and citizenship, sex, gender identity and expression, sexual orientation, sexual and reproductive health decisions, military or veteran status, creed, genetic predisposition, carrier status or any other category protected by federal, state and local law.
Position Summary:
As an Enterprise Architect at ACA you will be responsible for overseeing the development and use of data systems. You will discover efficient ways to organize, store, and analyze data with attention to security and confidentiality. A great asset for a data-driven company, your strategic planning and oversight will help us improve our operational efficiency and drive business growth.
Job Duties:
Develop and implement data management strategies that align with company goals.
Oversee the collection, storage, management, quality, and protection of data.
Ensure data accuracy and accessibility and reduce data redundancy.
Collaborate with IT teams and management to devise a data strategy that addresses industry requirements.
Lead and mentor a team of data professionals
Contribute to the design and development of the core Enterprise Data Services hub
Work directly with internal business stakeholders to drive out requirements and gain alignment on solution options.
Education, Experience and Skills:
Bachelor's degree in Computer Science, Data Science or related field and a minimum of seven (7) years of experience OR a minimum of ten (10) years of applicable industry experience (if no bachelor's degree)
Knowledge of data modeling practices with experience building enterprise data solutions
Experience designing and building reporting and dashboarding solutions
Working understanding of Microsoft Azure cloud environment and applicable data technologies Azure Data Factory, Event Hubs, Synapse etc.
Willingness to learn and experiment with new technologies to provide best in class solutions
Good communication skills ability to discuss technical topics with business stakeholders
Working experience to pull and push data from various APIs, including REST
Good understanding of using Postman
Working experience to read and write SQL, stored procedures and functions
Good understanding and demonstrated experience with Azure Data Factory pipelines
Good understanding and demonstrated experience with Azure data landscape.
Experience with DAX and M query formations.
Preferred Education and Experience:
Experience working in Power BI
Experience building SSAS cubes, particularly tabular modeling
C#, Python or other object-oriented programming experience a plus used currently to write Azure
Functions as needed
AI/ML knowledge in support of long-term strategy
Required Skills and Attributes:
Experienced in Data Modeling
Experience with Azure Cloud and relevant data technologies
Experience building reports and dashboards preferably in Power BI.
Ability to work directly with and communicate well with all levels of leadership and business partners.
Must be comfortable balancing several projects at once and able to pivot as business need arise
Preferred Licenses and Certification(s):
Microsoft Certification in Azure or Equivalent
Data Science Certifications
Data management Certification
Why join our team
We are the leading governance, risk, and compliance (GRC) advisor in financial services. When you join ACA, you'll become part of a team whose unique combination of talent includes the industry's largest team of former regulators, compliance professionals, legal professionals, and GIPS standards verifiers in the industry, along with practitioners in cybersecurity, ESG, and regulatory technology.
Our team enjoys an entrepreneurial work environment by offering innovative and tailored solutions for our clients. We encourage creative thinking and making the most of your experience at ACA by offering multiple career paths. We foster a culture of growth by focusing on continuous learning through inquiry and curiosity, and transparency. If you're ready to be part of an award-winning, global team of thoughtful, talented, and committed professionals, you've come to the right place.","SSAS cubes, SQL stored procedures and functions, Synapse, Event Hubs, Reporting and dashboarding solutions, Data modeling practices, Enterprise data solutions, M query formations, Azure Data Factory, Power Bi, Dax, Postman, Python, Microsoft Azure"
GCP Data Architect,Lingaro,10-12 Years,,India,Login to check your skill match score,"Job Title: Senior Data Architect (GCP)
Location: India (Remote)
About Lingaro:
Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.
Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.
About Data Management Competency: Focused on Data Governance and Quality Management, establishing and enforcing policies, processes, and practices to ensure the integrity, availability, and reliability of data across the organization.
Duties:
Formulate and communicate the organization's data strategy, including data quality standards, data flow, and data security measures.
Provides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements.
Define and implement data governance policies, procedures, and frameworks to ensure data integrity and compliance.
Collaborate with stakeholders to align data strategy with business goals and objectives, document current and target state in the form of business process and data journey diagrams.
Design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.
Define data standards, naming conventions, and data classification guidelines. Ensure data models are scalable, efficient, and optimized for performance.
Evaluate and select appropriate database technologies and solutions based on organizational needs and requirements.
Design and oversee the implementation of data platforms, including relational databases, NoSQL databases, data warehousing, and Big Data solutions.
Optimize database performance, ensure data security, and implement backup and recovery strategies.
Design data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, document source to target mappings.
Collaborate with IT team and data experts to identify opportunities for data acquisition.
Understand and follow data architecture patterns for various types of data systems, e.g. data lakehouse platforms, master data management systems, ML enriched data flows.
Implement data profiling and data cleansing processes to identify and resolve data quality issues.
Establish data quality standards and implement processes to measure, monitor, and improve data quality.
Facilitate discussions and workshops to gather requirements and align data initiatives with business goals, prepare data inventory documentation.
Communicate complex technical concepts effectively to both technical and non-technical stakeholders.
Stay abreast of industry trends and emerging technologies in data management, analytics, and security.
Evaluate and recommend new tools, technologies, and frameworks to enhance data architecture capabilities.
Provide guidance and support to developers and other team members on data-related topics.
Conduct knowledge sharing sessions and training programs to promote understanding and adoption of data architecture best practices.
Requirements:
Bachelor's or master's degree in computer science, Information Systems, or a related field.
10+ years experience as a Data Architect or in a similar role, ideally in a complex organizational setting.
Strong understanding of data management principles, data modeling techniques, database design and data integration flows.
Experience in developing and implementing data governance policies, procedures, and frameworks to ensure data integrity and compliance.
Familiarity with industry best practices and emerging trends in data management and governance.
Ability to design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.
Strong understanding of data modeling best-practices, e.g. data normalization, denormalization, generalization, and performance optimization techniques.
Expertise in working with various database technologies, such as relational databases (e.g., Oracle, SQL Server, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB).
Knowledge of database management systems, data warehousing, data integration technologies, and Big Data solutions.
Familiarity with cloud-based database, warehouse, and lakehouse platforms.
Experience in designing and implementing data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, understanding of data integration patterns and best practices.
Understanding of Business Intelligence and data analytics requirements, optimization of data storage and processing for reporting needs.
Excellent communication and presentation skills to effectively articulate complex technical concepts to both technical and non-technical stakeholders.
Ability to collaborate with cross-functional teams, including business analysts, developers, and data scientists, to understand requirements and drive data-related initiatives.
Strong analytical and problem-solving abilities to identify data-related issues, propose solutions, and make data-driven decisions.
Familiarity with data profiling, data cleansing, data harmonization, and data quality assessment techniques.
Knowledge of data security and privacy regulations, such as GDPR or CCPA, understanding of data encryption, access controls, and data masking techniques to ensure the security of sensitive data.
Professional certification in data management or related field would be advantageous.
Why join us:
Stable employment. On the market since 2008, 1300+ talents currently on board in 7 global sites.
100% remote.
Flexibility regarding working hours.
Full-time position
Comprehensive online onboarding program with a Buddy from day 1.
Cooperation with top-tier engineers and experts.
Unlimited access to the Udemylearning platform from day 1.
Certificate training programs. Lingarians earn 500+ technology certificates yearly.
Upskilling support. Capability development programs, Competency Centers, knowledge sharing sessions, community webinars, 110+ training opportunities yearly.
Grow as we grow as a company. 76% of our managers are internal promotions.
A diverse, inclusive, and values-driven community.
Autonomy to choose the way you work. We trust your ideas.
Create our community together. Refer your friends to receive bonuses.
Activities to support your well-being and health.
Plenty of opportunities to donate to charities and support the environment.","Relational Databases, data quality assessment, Big Data solutions, NoSQL databases, ETL processes, Data Integration, Data Modeling, Data Governance, Database Design, Data Profiling, Data Warehousing, Data Security, Data Cleansing"
CFIN Data Architect,ABB,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN Data Architect
At ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.
Write the next chapter of your ABB story.
This position reports to
Head of Central Finance
Your role and responsibilities
We are looking for an experienced and technically proficient Data Architect to lead the design, integration, and optimization of the technical solutions within the Central Finance (CFIN) landscape. The Data architect will be responsible for ensuring that data replication and technical activities are fully aligned with business needs, effectively integrated with other enterprise applications, and supported by automated solutions to enhance operational efficiency. This role involves close collaboration with various internal teams, including Finance, IS Architecture, and external vendors, to maintain and evolve the data architecture, ensuring it meets business requirements and is fully compliant with ABB's standards.
The work model for the role is:
This role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.
You will be mainly accountable for:
Solution Design & Validation: Review and validate the design of all Data & Technical related solutions within the CFIN framework, ensuring they are aligned with business goals and technical requirements.
Ownership of Data Architecture: Define, document, and own the overall data architecture within the CFIN ecosystem, including technical components, modules, and integration with other applications.
Data Replication and automation: Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG and Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data
Integration with other processes: Collaborate with other business streams (O2C, P2P, P2D, R2R, TAX, Treasury) to ensure data standards are maintained and design comprehensive data solutions that incorporate all work streams
Maintain Solution Roadmap: Keep the target Data solution architecture up-to-date, documenting changes to the roadmap and their impact on the broader enterprise architecture. Collaboration with Stakeholders: Work closely with the CFIN solution team, IS architects, vendors, and business stakeholders (including Finance, Process, Data, and Systems Finance teams) to configure, maintain, and enhance the CFIN landscape, ensuring business continuity.
Business Process Alignment: Collaborate with Data Global Process Owners (GPOs) and business teams to define and implement robust Data solutions that align with business requirements and global best practices. Automation & Innovation: Drive the regular implementation of automation solutions within the CFIN system to streamline Data processes, reduce manual effort, and improve efficiency.
Requirements Validation: Support the validation of business and functional requirements alongside Process Owners, FPDS team, and Technical Leads, ensuring processes are allocated to the appropriate applications and technologies.
Compliance & Standards: Ensure that all Data & technical solutions and work processes are compliant with ABB's internal standards, policies, and regulatory requirements. Continuous Improvement: Maintain and enhance domain expertise in Data and related technologies, keeping abreast of industry trends and ABB standards to drive continuous improvement within the organization.
Qualifications for the role
Education: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in FICO SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.
At least 7-10 years of experience in Data Architect, SAP Architect, or a similar role, with deep knowledge of Data processes and system integration.
Advanced expertise in SAP Central Finance (CFIN), SAP S/4HANA, or other ERP systems. Proficient in data process automation tools and strategies.
Extensive experience with data migration and replication between SAP systems. In-depth knowledge of SAP Business Technology Platform (BTP), FIORI, and other related applications.
Strong understanding of real-time data replication and automation standards. Strong leadership and team management skills, with the ability to motivate and guide cross-functional teams.
Excellent collaboration skills with the ability to coordinate between different stakeholders, including business leaders, technical teams, and external partners.
A strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.
Experience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.
More about us
Finance Services is ABB's shared services organization which delivers operational and expert services in Finance, with employees based in five main hubs and front offices, finance service provides mainly Business services to ABB teams across the globe as well as supports with external customer inquiries.
We value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory
It has come to our attention that the name of ABB is being used for asking candidates to make payments for job opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made available on our career portal for all fitting the criteria to apply. ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions. For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Real-time data replication and automation standards, Data migration and replication between SAP systems, Data process automation tools, Fiori"
Lead Data Architect,Chevron,10-15 Years,,"Bengaluru, India",Login to check your skill match score,"About The Position
Lead Data architects lead the design and implementation of data collection, storage, transformation, orchestration (movement) and consumption to achieve optimum value from data. They are the technical leaders within data delivery teams. They play a key role in modeling data for optimal reuse, interoperability, security and accessibility as well as in the design of efficient ingestion and transformation pipelines. They ensure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration. And they instill trust through the employment of data quality frameworks and tools.
The data architect at Chevron predominantly works within the Azure Data Analytics Platform, but they are not limited to it. The Senior Data architect is responsible for optimizing costs for delivering data. They are also responsible for ensuring compliance to enterprise standards and are expected to contribute to the evolution of those standards resulting from changing technologies and best practices.
Key Responsibilities
Design and overseeing the entire data architecture strategy
Mentor junior data architects to ensure skill development in alignment with the team strategy
Design and implement complex scalable, high-performance data architectures that meet business requirements
Model data for optimal reuse, interoperability, security and accessibility
Develop and maintain data flow diagrams, and data dictionaries
Collaborate with stakeholders to understand data needs and translate them into technical solutions
Ensure data accessibility through a performant, cost-effective consumption layer that supports use by citizen developers, data scientists, AI, and application integration
Ensure data quality, integrity, and security across all data systems
Required Qualifications
Bachelor's degree in computer science, Information Technology, or a related field (or equivalent experience)
Overall 10-15 years of experience with at least 5 years of proven experience as a Data Architect or similar role
Strong knowledge of data modeling, data warehousing, and data integration techniques
Proficiency in database management systems (e.g., SQL Server, Oracle, PostgreSQL)
Experience with big data technologies (e.g., Hadoop, Spark) and data lake solutions (e.g., Azure Data Lake, AWS Lake Formation)
Experience with big data technologies data lake solutions DBMS and cloud platforms
Experience in data modeling, ERDs, Star and/or Snowflake, and physical model design for analytics and application integration
Experience in designing data pipelines for optimal performance, resiliency, and cost efficiency
Experience translating business objectives and goals into technical architecture for data solutions
Familiarity with cloud platforms (e.g., Microsoft Azure, AWS, Google Cloud Platform)
Strong understanding of data governance and security best practices
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Track record for defining/implementing data architecture framework and governance around master data, meta data, modeling
Preferred Qualifications
Experience in Erwin, Azure Synapse, Azure Databricks, Azure DevOps, SQL, Power BI, Spark, Python, R
Ability to drive business results by building optimal cost data landscapes
Familiarity with Azure AI/ML Services, Azure Analytics: Event Hub, Azure Stream Analytics, Scripting: Ansible
Experience with machine learning and advanced analytics
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes)
Understanding of CI/CD pipelines and automated testing frameworks
Certifications such as AWS Certified Solutions Architect, IBM certified data architect or similar are a plus
Chevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm.
Chevron participates in E-Verify in certain locations as required by law.","CI CD pipelines, data integration techniques, Azure Analytics, Azure AI ML Services, Azure Stream Analytics, data pipelines, R, Event Hub, AWS Lake Formation, Hadoop, Erwin, Power Bi, Azure Databricks, Data Warehousing, PostgreSQL, Azure DevOps, SQL Server, Data Modeling, Data Governance, Ansible, AWS, Oracle, Kubernetes, Python, Azure Synapse, Docker, Azure Data Lake, Microsoft Azure, Google Cloud Platform, Spark"
Principal Data Architect,JPMorganChase,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
Your unmatched expertise and unrelenting quest for outcomes are the driving forces for transformation that inspire high-quality solutions. You are a crucial member of a diverse team of thought leaders committed to leaving a positive mark on the industry.
As a Principal Data Architect at JPMorgan Chase within the Consumer and Community Banking, you will provide expertise to enhance and develop data architecture platforms based on modern cloud-based technologies as well as support the adoption of strategic global solutions. You will leverage your advanced data architecture capabilities and collaborate with colleagues across the organization to drive best-in-class outcomes to achieve the target state architecture goals.
Job Responsibilities
Advises cross-functional teams on data architecture solutions to achieve the target state architecture and improve current technologies.
Lead the design, implementation, and maintenance of scalable, high-performance data architectures, including data lakes, data warehouses, and data integration solutions.
Collaborate with cross-functional teams, including IT, business units, and analytics teams, to ensure data architecture supports business needs and enables data-driven decision-making.
Drive the adoption of emerging data technologies and methodologies to enhance data capabilities and improve efficiency
Develops multi-year roadmaps aligned with business and data architecture strategy and priorities
Creates complex and scalable data frameworks using appropriate software design
Reviews and debugs code written by others to deliver secure, high-quality production code
Serves as the function's go-to subject matter expert
Contributes to the development of technical methods in specialized fields in line with the latest product development methodologies
Creates durable, reusable data frameworks using new technology to meet the needs of the business
Champions the firm's culture of diversity, equity, inclusion, and respect. Mentors and coaches junior architects and technologists
Required Qualifications, Capabilities, And Skills
Formal training or certification on data management concepts and 10+ years applied experience. In addition, 5+ years of experience leading technologists to manage, anticipate and solve complex technical items within your domain of expertise.
Hands-on practical experience delivering system design, application development, testing, and operational stability
Expert in one or more architecture disciplines and programming languages
Deep knowledge of data architecture, best practices, and industry trends
Experience with cloud-based data platforms (e.g., AWS, Azure, Google Cloud) and big data technologies (e.g., Hadoop, Spark).
Advanced knowledge of application development and technical processes with considerable in-depth knowledge in one or more technical disciplines (e.g., cloud, artificial intelligence, machine learning, mobile, etc.)
Experience applying expertise and new methods to determine solutions for complex architecture problems in one or more technical disciplines
Ability to present and effectively communicate with Senior Leaders and Executives
About Us
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We're proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions all while ranking first in customer satisfaction.","cloud-based technologies, data integration solutions, Mobile, data lakes, Hadoop, Google Cloud, Machine Learning, data warehouses, AWS, Data Architecture, Azure, Application Development, Artificial Intelligence, Spark"
Big Data Architect,Airtel Digital,8-17 Years,,"Pune, India",Login to check your skill match score,"Key Responsibilities
Design, build, and maintain scalable big data architectures on Azure and AWS - Select and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )
Lead data migration from legacy systems to cloud-based solutions - Develop and optimize ETL pipelines and data processing workflows.
Ensure data infrastructure meets performance, scalability, and security requirements.
Collaborate with development teams to implement microservices and backend solutions for big data applications.
Oversee the end-to-end SDLC for big data projects, from planning to deployment.
Mentor junior engineers and contribute to architectural best practices.
Prepare architecture documentation and technical reports.
Required Skills & Qualifications
Bachelor's/Master's degree in Computer Science, Engineering, or related field.
817 years of experience in big data and cloud architecture.
Proven hands-on expertise with Azure and AWS big data services (e.g., Azure Synapse, AWS Redshift, S3, Glue, Data Factory).
Strong programming skills in Python, Java, or Scala[9].
Solid understanding of SDLC and agile methodologies.
Experience in designing and deploying microservices, preferably for backend data systems.
Knowledge of data storage, database management (relational and NoSQL), and data security best practices
Excellent problem-solving, communication, and team leadership skills","Glue, Java, S3, Aws Redshift, Hadoop, Scala, Kafka, Microservices, Nosql, Azure Synapse, Azure Data Factory, Database Management, Spark, Data Security, Azure, Python, AWS"
Data Architect,NTT DATA Global Delivery Services Limited,1-5 Years,,Pune,Information Technology,"NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Architect to join our team in Pune, Mahrshtra (IN-MH), India (IN).
Job Duties: Role: Data Architect
Job Description:
Work as Data architect/Senior data engineer to design and develop cost effective and reliable data solutions on any cloud platform like Azure or AWS.
Should be able to understand client requirements and convert them into technical solution leveraging cloud capabilities and modern technologies.
Should be able to contribute into company's internal innovation projects by conducting proof of concepts and developing frameworks using state of the art technologies.
Should have prior experience in developing data pipelines using Pyspark, SQL and Python
Should have good understanding of Snowflake and Azure cloud services like Synapse, Azure Databricks, Azure Data Factory, ADLS etc.
Should have prior understanding of applying ETL and ELT concepts and principals especially when migrating data from any legacy system to modern cloud platform.
Any experience in real time data processing using PySpark, Python and Kafka is an advantage.
Work as individual contributor and spend 70% of the time writing code in different languages, frameworks, and technology stacks.
Should be familiar with emerging technologies like GenAI, Snowflake Cortex and Databricks
Minimum Skills Required: Mandatory skills: Any cloud experience (mainly Azure/AWS), SQL, Python, Pyspark, Snowflake
Good to have: AI/ML, GenAI, Databricks","Data Pipeline, Azure, Aws"
Data Architect,HDFC,12-17 Years,,"Gurugram, Bengaluru, Mumbai",Banking,"Responsible for building data lake, data foundation and analytical solution with standard design and modern cloud/hybrid architecture pattern
Work with cross functional teams to make the data usable for functional users and applications to enable delivery of business values to customers
Provide architectural leadership and vision for Bank's Next Gen Data Platform and Data Lakehouse
Develop and maintain architectural roadmap for data products and data services plus ensuring alignment with the business and enterprise architecture strategies and standards
Drive design and architecture for Data Transformation & aggregations, design and development of a roadmap, and implementation based upon current vs. future state in a cohesive architecture viewpoint
Review and understand business requirements and technical designs for physical data design, data pipelines (ETL) and other technical integrations
Build data pipelines for multiple storage solutions, including distributed platforms such as Databricks, Trino, Hadoop and MPP databases and cloud Data warehouse
Design and implement low latency analytical platform services leveraging open source and cloud technology
Design and implement data governance and MDM tools
Designing and building data capabilities to support cloud data strategy, including fully automated data Pipelines, data curation and consumption
Experience:
Experienced technology leader with a minimum of 12+ years of software development experience including 10+ years of data application or data platform architecture experience with deep technology expertise
Deep knowledge and hands on experience in design and implementation of Big Data technologies (Apache Spark, Apache Airflow, Apache Flink, Streaming data, ADLS, S3, Object Data store, Trino, Databricks, Unity Catalog, Data governance tools) and familiarity with data architecture patterns (data warehouse, data lake, data lakehouse, data ingestion, curation and consumption)","Apache Airflow, Apache, Azure"
Data Architect,NTT DATA Global Delivery Services Limited,3-6 Years,,Pune,"IT Management, Consulting","Job Duties: Role: Data Architect
Job Description:
Work as Data architect/Senior data engineer to design and develop cost effective and reliable data solutions on any cloud platform like Azure or AWS.
Should be able to understand client requirements and convert them into technical solution leveraging cloud capabilities and modern technologies.
Should be able to contribute into company's internal innovation projects by conducting proof of concepts and developing frameworks using state of the art technologies.
Should have prior experience in developing data pipelines using Pyspark, SQL and Python
Should have good understanding of Snowflake and Azure cloud services like Synapse, Azure Databricks, Azure Data Factory, ADLS etc.
Should have prior understanding of applying ETL and ELT concepts and principals especially when migrating data from any legacy system to modern cloud platform.
Any experience in real time data processing using PySpark, Python and Kafka is an advantage.
Work as individual contributor and spend 70% of the time writing code in different languages, frameworks, and technology stacks.
Should be familiar with emerging technologies like GenAI, Snowflake Cortex and Databricks
Minimum Skills Required: Mandatory skills: Any cloud experience (mainly Azure/AWS), SQL, Python, Pyspark, Snowflake
Good to have: AI/ML, GenAI, Databricks","Pyspark, Sql, Python"
Data Architect,Adobe,10-12 Years,,Bengaluru,Software,"Data Architect AEP Competency
Position Summary
Experienced data modelers, SQL, ETL, with some development background to provide defining new data schemas, data ingestion for Adobe Experience Platform customers. Interface directly with enterprise customers and collaborate with internal teams.
What you'll do
Interface with Adobe customers to gather requirements, design solutions & make recommendations
Lead customer project conference calls or interface with a Project Manager
Deliver Technical Specifications documents for customer review
Strong collaboration with team software engineer consultants onshore & offshore
Leverage understanding of data relationships and schemas to structure data to allow clients to perform dynamic customer-level analysis
Construct processes to build Customer ID mapping files for use in building 360 degree view of customer across data sources.
Leverage scripting languages to automate key processes governing data movement, cleansing, and processing activities
Bill & forecast time toward customer projects
Innovate on new ideas to solve customer needs
Requirements
10+ years of strong experience with data transformation & ETL on large data sets
Experience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale etc.)
5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)
5+ years of complex SQL or NoSQL experience
Experience in advanced Data Warehouse concepts
Experience in industry ETL tools (i.e., Informatica, Unifi)
Experience with Business Requirements definition and management, structured analysis, process design, use case documentation
Experience with Reporting Technologies (i.e., Tableau, PowerBI)
Experience in professional software development
Demonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects
Strong verbal & written communication skills to interface with Sales team & lead customers to successful outcome
Must be self-managed, proactive and customer focused
Degree in Computer Science, Information Systems, Data Science, or related field
Special Consideration given for
Experience & knowledge with Adobe Experience Cloud solutions
Experience & knowledge with Digital Analytics or Digital Marketing
Experience in programming languages (Python, Java, or Bash scripting)
Experience with Big Data technologies (i.e., Hadoop, Spark, Redshift, Snowflake, Hive, Pig etc.)
Experience as an enterprise technical or engineer consultant","Java, Hive, Hadoop, Redshift, Python"
Data Architect,Wipro Limited,11-13 Years,,"Gurugram, Gurugram",IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.
Job Description
Role:
Service Desk Manager
Band C1 Role (Data Architect)
Location Chennai, Noida
Total exp 11+ Years
The candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.
Must have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop
Must have hands on in writing complex use case driven SQLs
Should have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.
Should have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3
Should have been involved in On-Prem to Cloud Migration process.
Should have good knowledge with HIVE / Spark / Scala scripts
Should have good knowledge on Unix Shell scripting
Should be flexible to overlap US business hours
Should be able to drive technical design on Cloud applications
Should be able to guide & drive the team members for cloud implementations
Should be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.
AWS Certified applicants preferable
Competencies
Client Centricity
Execution Excellence
Collaborative Working
Problem Solving & Decision Making
Effective communication
Reinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, S3, Sql, Unix Shell Scripting, Aws Cloud, Emr, Hadoop, Etl, Sqoop, Hive, Scala, Data Warehouse, Redshift, Spark"
Data Architect,Wipro Limited,11-13 Years,,Chennai,IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.
Job Description
Role:
Service Desk Manager
Band C1 Role (Data Architect)
Location Chennai, Noida
Total exp 11+ Years
The candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.
Must have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop
Must have hands on in writing complex use case driven SQLs
Should have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.
Should have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3
Should have been involved in On-Prem to Cloud Migration process.
Should have good knowledge with HIVE / Spark / Scala scripts
Should have good knowledge on Unix Shell scripting
Should be flexible to overlap US business hours
Should be able to drive technical design on Cloud applications
Should be able to guide & drive the team members for cloud implementations
Should be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.
AWS Certified applicants preferable
Competencies
Client Centricity
Passion for Results
Collaborative Working
Problem Solving & Decision Making
Effective communication
Reinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, On-Prem to Cloud Migration, Data Warehouse, Sql, S3, Unix Shell Scripting, Emr, Aws Cloud, Hadoop, Etl, Sqoop, Hive, Scala, Redshift, Spark"
Data Modelling Architect (CoE),NTT Data,10-12 Years,,"Bengaluru, India",IT/Computers - Software,"Req ID:312265
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Modelling Architect (CoE) to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Title: Data Modelling Architect (CoE)
Position Overview:
We are seeking a highly skilled and experienced Data Modelling Architect to join our dynamic team. The ideal candidate will have a strong background in full data modelling life cycle, e.g. design, implement, and maintain complex data models that align with organisational goals and industry standards. This role requires a deep understanding of data architecture, data modelling methodologies, and ideally in real-time data integrations. The successful candidate will collaborate with cross-functional teams to ensure optimal data structures that support business intelligence, analytics, and operational requirements. This role is primarily within the Centre of Excellence of the Data Products Factory, creating, assuring, and overseeing the implementation of data models within analytical and real-time streaming domains.
Key Responsibilities
Develop conceptual, logical, and physical data models to support data analytics, streaming and data products implementation.
Define and maintain data architecture standards, principles, and best practices.
Ensure data models are aligned with business requirements and scalable for future needs.
Work closely with business stakeholders, data engineers, data solution architects, and data product teams to gather requirements and design solutions.
Provide guidance on data integration, transformation, and migration strategies.
Establish and maintain enterprise data models, data dictionaries, metadata repositories, and data lineage documentation.
Ensure data models comply with organisational policies and regulatory requirements.
Optimise data products and their components for performance, scalability, and reliability.
Evaluate and recommend data modelling tools and technologies.
Stay updated on industry trends and emerging technologies in data architecture.
Identify and resolve data inconsistencies, redundancies, and performance issues.
Provide technical leadership in addressing complex data-related challenges.
Required Skills and Qualifications
10+ years of experience in data architecture and modelling.
Proven experience in data modelling, data architecture, and data products design.
Proven experience and expertise in data modelling standards, techniques (e.g. dimensional model, 3NF, Vault 2.0)
Familiarity with both analytical and real-time/ streaming data solutions (Kafka/Airflow).
Hands-on experience with data modelling tools (e.g., Erwin, Lucidchart, SAP PowerDesigner).
Expertise in Python and SQL (e.g., Snowflake, Kafka).
Experience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway
Knowledge of data warehouse design, ETL/ ELT processes, and big data technologies (e.g., Snowflake, Spark).
Familiarity with data governance and compliance frameworks (e.g., GDPR, HIPAA).
Strong communication and stakeholder management skills.
Analytical mindset with attention to detail.
Ability to lead and mentor teams on best practices in data modelling.
Education: Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Preferred Skills and Qualifications
- Certifications in data modelling, cloud platforms, or database technologies.
- Experience in developing and implementing enterprise data models.
- Experience with Interface/ API data modelling.
- Experience with CI/CD GITHUB Actions (or similar)
- AWS fundamentals (e.g., AWS Certified Data Engineer)
- Knowledge of Snowflake/ SQL
- Knowledge of Apache Airflow
- Knowledge of DBT
- Familiarity with Atlan for data catalog and metadata management
- Understanding of iceberg tables
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at
NTT DATA endeavors to make accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click . If you'd like more information on your EEO rights under the law, please click . For Pay Transparency information, please click.","snowflake, data warehouse design, EKS, 3NF, Airflow, SAP PowerDesigner, Lucidchart, data products design, Vault 2.0, AWS cloud services, dimensional model, Sql, Hipaa, Lambda, Api Gateway, Erwin, Kafka, Etl, Data Architecture, S3, ELT, Big Data Technologies, Gdpr, Data Governance, Sns, Python, Data Modelling, Spark"
Network Data Architect,Aspire Systems India Private Limited,12-16 Years,,Chennai,Software,"Minimum 12+ years of relevant hands on experience in configuring and troubleshooting Network devices, Cisco routers, ACI, Switches, Firewalls and F5 Load Balancers.
Ability to troubleshoot and upgrade Cisco routers, Switches, Firewalls and F5 Load Balancers.
Must be skilled to perform router/switch network hardware/software upgrades.
Must have hands on experience in Routing Protocol Development (BGP, OSPF, VPN and Static.), providing IP Addressing Strategy - NAT, Re-Addressing, WAN - IP Routed Network Integration Wireless LAN technologies.
Experience with Cisco LAN routing, switching, Security, Voice, and Wireless LAN products.
Experience in network protocols VPN, OSPF, BGP, TCP/IP.
Experience in Cisco nexus and Datacenter support.
Experience and hands on with Cisco Client/ Cisco Prime/ Public DNS services.
Experience in providing Network Engineering services in support of the large enterprise customer data networks.
Configure, administer firewall infrastructure, working with Cisco and Palo alto.
Work with customer technical teams to provide solutions for customers internetworking communications requirements.
Experience in proposal presentation and participating in Pre-sales engagement is an added advantage.
Provide valuable solutions for colleagues and stakeholders, both tactically and strategically and ensure compliance to process, procedure, and tools within the operations.
Excellent written and verbal communications skills are essential.
Proven analytic skills and the ability to isolate and resolve complex issues.
Good understanding of Migration Effort, Resources and Timelines estimation.
Design public cloud infrastructure and DevOps solutions, architectures and roadmaps.
Evolve and build best practice materials for Infrastructure-as-Code and Configuration Management.
Enable sales including knowledge transfer, architecture and design, as well as participate in the sales cycle as needed.
Participate in community and market activities including participation in trade shows.
Create, build and grow partnerships with various organizations relevant to the practice,
Enable development teams to leverage cloud and cloud-native architectures with new and existing applications.","Network Data Architect, Tcp/ip, BGP, OSPF"
Senior Data Architect,Adobe,10-12 Years,,Noida,Software,"Position Summary
Experienced data modelers, SQL, ETL, with some development background to provide defining new data schemas, data ingestion for Adobe Experience Platform customers. Interface directly with enterprise customers and collaborate with internal teams.
What you'll do
Interface with Adobe customers to gather requirements, design solutions & make recommendations
Lead customer project conference calls or interface with a Project Manager
Deliver Technical Specifications documents for customer review
Strong collaboration with team software engineer consultants onshore & offshore
Leverage understanding of data relationships and schemas to structure data to allow clients to perform dynamic customer-level analysis
Construct processes to build Customer ID mapping files for use in building 360 degree view of customer across data sources.
Leverage scripting languages to automate key processes governing data movement, cleansing, and processing activities
Bill & forecast time toward customer projects
Innovate on new ideas to solve customer needs
Requirements
10+ years of strong experience with data transformation & ETL on large data sets
Experience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale etc.)
5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)
5+ years of complex SQL or NoSQL experience
Experience in advanced Data Warehouse concepts
Experience in industry ETL tools (i.e., Informatica, Unifi)
Experience with Business Requirements definition and management, structured analysis, process design, use case documentation
Experience with Reporting Technologies (i.e., Tableau, PowerBI)
Experience in professional software development
Demonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects
Strong verbal & written communication skills to interface with Sales team & lead customers to successful outcome
Must be self-managed, proactive and customer focused
Degree in Computer Science, Information Systems, Data Science, or related field
Special Consideration given for
Experience & knowledge with Adobe Experience Cloud solutions
Experience & knowledge with Digital Analytics or Digital Marketing
Experience in programming languages (Python, Java, or Bash scripting)
Experience with Big Data technologies (i.e., Hadoop, Spark, Redshift, Snowflake, Hive, Pig etc.)
Experience as an enterprise technical or engineer consultant","data ingestion, Big Data Technologies, Data Warehousing, Sql, Etl"
Cloud Data Architect,Birlasoft Limited,13-23 Years,,Pune,Consulting,"About Birlasoft:
Birlasoft, a powerhouse where domain expertise, enterprise solutions, and digital technologies converge to redefine business processes. We take pride in our consultative and design thinking approach, driving societal progress by enabling our customers to run businesses with unmatched efficiency and innovation. As part of the CK Birla Group, a multibillion-dollar enterprise, we boast a 12,500+ professional team committed to upholding the Group's 162-year legacy. Our core values prioritize Diversity, Equity, and Inclusion (DEI) initiatives, along with Corporate Sustainable Responsibility (CSR) activities, demonstrating our dedication to building inclusive and sustainable communities. Join us in shaping a future where technology seamlessly aligns with purpose.
About the Job:
Birlasoft is seeking a visionary Senior/Lead Cloud and Data Architect with proven leadership capabilities to drive enterprise-level data solutions. The ideal candidate will have expertise in cloud platforms (AWS, Azure, or GCP), mandatory experience in Databricks or Snowflake, and a strong foundation in data warehousing, data modelling, DevOps, and DataOps. Additionally, the role demands a strategic leader with an understanding of machine learning (ML) concepts and the ability to function as an Enterprise Architect, providing guidance and alignment across diverse teams and initiatives.
Title: Senior/Lead Cloud and Data Architect
Location:Pune/Mumbai
Educational Background:Any Graduation
Key Responsibilities:
Leadership and Collaboration:
Lead and inspire cross-functional teams, including data engineers, data scientists, ML engineers, and BI analysts, fostering a culture of collaboration, innovation, and continuous improvement.
Act as the central technical point of contact for enterprise-wide data and analytics initiatives, ensuring alignment with business objectives.
Collaborate with senior leadership and business stakeholders to define and deliver on data strategy, ensuring scalability and alignment with enterprise goals.
Mentor and upskill team members, providing guidance on best practices, emerging technologies, and professional development.
Enterprise Architecture:
Develop and maintain enterprise-level architectural blueprints for cloud and data platforms, ensuring interoperability and scalability.
Create a cohesive architecture that integrates data platforms, ML systems, and business intelligence tools while adhering to governance and compliance requirements.
Provide strategic input on technology roadmaps, ensuring alignment with organizational vision and future-proofing investments.
Evaluate and recommend emerging technologies and frameworks to enhance enterprise data capabilities.
Cloud and Data Platform Engineering:
Design and implement cutting-edge cloud architectures using AWS, Azure, or GCP, focusing on scalability, security, and cost optimization.
Build and optimize modern data platforms using Databricks or Snowflake for advanced analytics and real-time data processing.
Lead the development of end-to-end data pipelines, ensuring robust integration between data sources, platforms, and analytics tools.
Data and ML Integration:
Collaborate with ML teams to deploy machine learning pipelines and operationalize AI models within enterprise systems.
Provide architectural support for ML initiatives, including data preparation, feature engineering, and model lifecycle management.
Enable seamless integration of ML and analytics into business workflows, enhancing decision-making and operational efficiency.
Data Warehousing and Modeling:
Architect enterprise data warehouses with robust data models, ensuring high performance and reliability for analytics workloads.
Lead the development of advanced data models that cater to both analytical and operational requirements, emphasizing scalability and data quality.
DevOps and DataOps Practices:
Establish and enforce DevOps and DataOps pipelines to automate deployments, enhance agility, and ensure operational excellence.
Drive initiatives for continuous improvement in data delivery, ensuring high availability, data quality, and scalability.
Governance, Security, and Compliance:
Define and enforce data governance policies, including data security, lineage, and compliance with industry regulations.
Implement robust security measures to protect sensitive data across platforms, adhering to privacy standards such as GDPR and CCPA.
Skill Required:
10+ years in data architecture, cloud platforms, and advanced analytics.
Hands-on experience with at least two cloud platforms (AWS, Azure, or GCP) is mandatory.
Expertise in Databricks or Snowflake is required.
Comprehensive understanding of data warehousing, data modelling, and data integration.
Experience implementing ML pipelines or integrating ML solutions with enterprise systems.
Proven track record in enterprise architecture, aligning technology solutions with business goals.
Preferred Qualifications:
Certifications in AWS, Azure, GCP, Databricks, or Snowflake.
Experience working in enterprise-scale environments with complex data landscapes.
Exposure to industry-specific data challenges in domains like finance, insurance, or healthcare.
Additional Requirements
This role offers the opportunity to lead transformative data initiatives at an enterprise level, combining cutting-edge cloud and data technologies with leadership and innovation. If you have a passion for building scalable data solutions and leading diverse teams to success, we encourage you to apply.","Cloud Analytics, Big Data, Azure, Cloud Architect"
Data Warehouse Architect,Icici Bank Limited,10-12 Years,,Hyderabad,Banking,"Key Responsibilities:
Data Pipeline Design: Responsible for designing and developing ETL data pipelines that can help in organising large volumes of data. Use of data warehousing technologies to ensure that the data warehouse is efficient, scalable, and secure.
Issue Management: Responsible for ensuring that the data warehouse is running smoothly. Monitor system performance, diagnose and troubleshoot issues, and make necessary changes to optimize system performance.
Collaboration: Collaborate with cross-functional teams to implement upgrades, migrations and continuous improvements.
Data Integration and Processing: Responsible for processing, cleaning, and integrating large data sets from various sources to ensure that the data is accurate, complete, and consistent.
Data Modelling: Responsible for designing and implementing data modelling solutions to ensure that the organization's data is properly structured and organized for analysis.
Key Qualifications & Skills:
Education Qualification: B.E./B. Tech. in Computer Science, Information Technology or equivalent domain with 10 to 12 years of experience and at least 5 years or relevant work experience in Datawarehouse/mining/BI/MIS.
Experience in Data Warehousing: Knowledge on ETL and data technologies like OLTP, OLAP (Oracle / MSSQL). Data Modelling, Data Analysis and Visualization experience (Analytical tools experience like Power BI / SAS / ClickView / Tableau etc.). Good to have exposure to Azure Cloud Data platform services like COSMOS, Azure Data Lake, Azure Synapse, and Azure Data factory.
Certification: Azure certified DP 900, PL 300, DP 203 or any other Data platform/Data Analyst certifications.","Data Analysis, Data Warehousing, Data Modelling, Etl"
Data Engineer Architect,TechnoGen,10-19 Years,,Hyderabad,"Consulting, Information Services","Basic Qualifications
Bachelors degree in computer science, engineering or a related field
Data: 8+ years of experience with data analytics and warehousing inInvestment& Finance Domain
SQL: Deep knowledge of SQL and query optimization
ELT: Good understanding of ELT methodologies and tools
Troubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues
Communication: Excellent communication, problem solving and organizational and analytical skills
Able to work independently and to provide leadership to small teams of developers
3+ years of coding and scripting (Python, Java, Scala) and design experience.","Java, Scala, Data Analytics, Sql, Python, data engineering, Data Architecture"
Data Architect,Spectral Consultants,10-13 Years,,Chennai,Consulting,"10-13 years of experience in Business Intelligence application development and support
Hands-on experience on the data pipeline setup, data management, importing and cleansing data, preparing enterprise wide data catalog and data mining
Worked on data analysis methodologies and modern data warehouse implementation to capture analytics and metrics
Exposure to ETL process, data visualization tools and statutory report development
Experience on Implementation with large scale data sets, handling XML, JSON structures
Strong program/project management skills from initiation to implementation in an enterprise
Track record of implementing innovative solutions to address business challenges
Strong exposure to Microsoft Azure and Google native Cloud services for Big data pipeline implementation, data processing, data transformation etc.
Strong business analytical and communication skills
Good influential skills and ability to operate in a fast-paced global environment with urgency, ownership, and accountability
Strong oral and written skills
Strong knowledge of data warehouse concepts
Knowledge of SAP
Knowledge of Microsoft Business Intelligence stack
Experience with Big data concepts a plus
Education:
BE Degree in Information Systems, Computer Science or related technical discipline or equivalent","Data Information Architecture, Data Pipeline, Data Governance, Azure, DataFlow"
Data Architect,Right Advisors Private Limited,10-14 Years,,Bengaluru,Software,"Requirements
10+ years of strong experience with data transformation & ETL on large data sets
Experience with designing customer centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale etc.)
5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data)
5+ years of complex SQL or NoSQL experience Experience in advanced Data Warehouse concepts
Experience in industry ETL tools (i.e., Informatica, Unifi)
Experience with Business Requirements definition and management, structured analysis, process design, use case documentation
Experience with Reporting Technologies (i.e., Tableau, PowerBI) Experience in professional software development Demonstrate exceptional organizational skills and ability to multi-task simultaneous different customer projects
Strong verbal & written communication skills to interface with Sales team & lead customers to successful outcome Must be self-managed, proactive and customer focused Degree in Computer Science, Information Systems, Data Science, or related field","Data Modelling, Power Bi, Sql, Etl, Data Transformation"
Data Architect,Gainwell Technologies LLC,8-12 Years,,Bengaluru,IT Management,"Job description
Summary
Were looking for a Solution Architect to join our product development team to drive a cloud based data platform architecture that will power BI and advanced analytics at Gainwell Technologies. You will have a direct impact onthe design, architecture, and implementation of BI and data science use cases. The ideal candidate will have industry-leading programming skills and established knowledge of implementing, designing, deploying, and maintaining big data analytics platforms in a cloud environment.
The solution architect will use knowledge of healthcare data to influence the implementation and governance of our data architecture. Your designs will account for data movement, storage, compute and BI consumption, and will comply with security and data governance standards. You will work closely with a variety of partners within the Data and Analytics organization.
Your role in our mission
Provide thought leadership and technical direction to the data engineering team in building analytic data products
Understand and translate business requirements to data strategies that align with overall technology vision
Design, develop and enforce standards for the data storage, processing and governance across all environments
Work closely with enterprise and application architects to align the data engineering team to the overall company SDLC standards, practices, and data access patterns
Develop, document and maintain overall view of data platform architecture, data acquisition, data quality, and data retention
Provide formal and informal training for data engineers, platform engineers and ETL developers
Maintain knowledge of emerging technologies and architectures
Document and publish technical principles and standards, and mentor the data engineering team to incorporate them into their daily practices
Champion and present the technical vision to the executive team and business stakeholders
What we're looking for
Basic Qualifications
Bachelor's degree with a preferred area of study in information technology, computer science, computer engineering or related fields
8+ years of overall experience in big data, database and enterprise data architecture and delivery
8+ years of programming proficiency in a subset of Python, Java, and Scala
5+ years of hands-on experience building solutions on distributed processing frameworks such as Spark, Hadoop or Databricks
5+ years of experience architecting, developing, releasing, and maintaining enterprise data lake platforms
3+ years of experience implementing cloud-based systems. AWS and/or Databricks preferred
Strong SQL skills to create/maintain DB objects, query/load required data using data governance (e.g. business glossary, data dictionary, data catalog, data quality, master data management, etc.) and visualization tools to bring data literacy to the organization.
Practical experience on workload management, monitoring, and performance tuning Apache Spark jobs
Broad knowledge of data technologies, tools, and disciplines including data modeling, dimensional models, third-normal-form structures, ETL/ELT, change data capture and slowly changing dimensions
Experience with healthcare data a big plus
Experience with Machine Learning & MLOPs is a big plus","Java, Scala, Aws"
Data Architect,Citiustech Healthcare Technology Private Limited,5-10 Years,,"Mumbai City, Bengaluru, Mumbai",Health Care,"We are looking for a data architect with below skillset
Hands on skills withImage data (preferably DICOM) parsing
Architect and implement data warehousing solutions using AWS (Redshift, S3, Lambda)
Develop and maintain ETL pipelines using Informatica PowerCenter or AWS Glue
Collaborate with cross-functional teams to identify and prioritize data requirements
Analyze complex data sets using SQL, Python, and Java and or/Apex languageto inform business decisions
Implement data visualization tools (Tableau, Power BI) for stakeholder reporting
Ensure data integrity, security, and compliance with HIPAA, GDPR, and CCPA
Requirements:> 5 years of experience in data management with focus in image metadata/bigdata
Strong expertise in:
PACS& RIS, SYNAPSE/XNAT, systems
AWS (Redshift, S3, Lambda, Glue)
Data governance, quality, and security
ETL pipelines (Informatica PowerCenter or AWS Glue)
Data warehousing and visualization","Data Warehousing, Tableau, Python, AWS"
Data Architect,Ifintalent Global Private Limited,5-10 Years,,Mumbai,Financial Services,"Job Description
Direct Responsibilities
Engage with key business stakeholders to assist with establishing fundamental data governance processes
Define key data quality metrics and indicators and facilitate the development and implementation of supporting standards
Help to identify and deploy enterprise data best practices such as data scoping, metadata standardization, data lineage, data deduplication, mapping and transformation and business validation
Structures the information in the Information System (any data modelling tool like Abacus), i.e. the way information is grouped, as well as the navigation methods and the terminology used within the Information Systems of the entity, as defined by the lead data architects.
Creates and manages data models (Business Flows of Personal Data with process involved) in all their forms, including conceptual models, functional database designs, message models and others in compliance with the data framework policy
Allows people to step logically through the Information System (be able to train them to use tools like Abacus)
Contribute and enrich the Data Architecture framework through the material collected during analysis, projects and IT validations Update all records in Abacus collected from stakeholder interviews/ meetings.
Skill Area
Communicating between the technical and the non-technical
Is able to communicate effectively across organisational, technical and political boundaries, understanding the context. Makes complex and technical information and language simple and accessible for non- technical audiences. Is able to advocate and communicate what a team does to create trust and authenticity, and can respond to challenge.
Data Modelling (Business Flows of Data in Abacus)
Produces data models and understands where to use different types of data models. Understands different tools and is able to compare between different data models.
Able to reverse engineer a data model from a live system. Understands industry recognized data modelling patterns and standards.
Data Standards (Rules defined to manage/ maintain Data)
Develops and sets data standards for an organisation.
Communicates the business benefit of data standards, championing and governing those standards across the organisation.
Metadata Management
Understands a variety of metadata management tools. Designs and maintains the appropriate metadata repositories to enable the organization to understand their data assets.
Turning business problems into data design
Works with business and technology stakeholders to translate business problems into data designs. Creates optimal designs through iterative processes, aligning user needs with organisational objectives and system requirements.","Data Process, Data Architect, Data Governance"
Data Architect,Paypal,10-15 Years,,Bengaluru,Financial Services,"PayPal Data Architecture team seeks a highly skilled Senior Data Architect to lead the modernization of our applications towards a Kappa or Lambda architecture. The ideal candidate will have a deep understanding of data modeling, domain-driven design, and CQRS, as well as experience with real-time data processing, analytics and distributed systems.Job Description:
Meet our team : PayPal Data Architecture team is on a mission to build a robust and scalable data platform that empowers our customers. We're looking for a passionate Data Architect to join us in designing and implementing secure, high-performing data solutions that leverage a variety of technologies.
What you need to know about the role:- In this role, you'll be a trusted advisor to various PayPal business units, partnering with their product engineering teams to architect data solutions that fuel high-priority initiatives. You'll be embedded with these teams, playing a key role throughout the entire data lifecycle.
Your day to day:
Develop and implement a long-term data strategy aligned with business objectives, focusing on event-driven architectures and real-time analytics.
Demonstrate deep expertise in Kappa and Lambda architectures, data modeling, domain-driven design, CQRS, and stream processing frameworks.
Address complex data challenges, such as large-scale data integration, real-time analytics, and data governance.
Design and model data structures, schemas, and relationships to optimize performance and scalability.
Lead the transition of existing applications to event-driven architectures, leveraging event modeling, domain-driven design, and CQRS.
Collaborate with development teams to ensure seamless integration of data models and architectures.
Mentor and guide junior data professionals, fostering a culture of innovation and excellence.
Stay updated on emerging technologies and evaluate their suitability for our organization.
Your way to impact:-
Hybrid Data Architecture Expertise. Craft solutions that seamlessly integrate on-premises, cloud-based, and hybrid data storage options, ensuring optimal data management across the organization.
Compliance & Governance. Ensure your data architectures and designs adhere to PayPal's specific enterprise data architecture standards for each data store type.
Data Access Pattern & Structure Design. Design data structures and access patterns that meet both functional and non-functional requirements.
Functional. Align with the specific business needs of the project.
Non-Functional. Prioritize security, availability, performance, scalability to create a robust and user-friendly data platform.
As a Customer Champion, you'll collaborate with domain architects to identify opportunities for data platform improvements.
Data Security. Enhance data security posture by minimizing risks and vulnerabilities.
Fault Tolerance. Design data solutions to be highly available and resilient to failures.
Scalability. Ensure solutions can accommodate future growth.
Query Performance. Optimize data structures and access patterns for faster queries, ultimately improving the customer experience.
Drive Improvement Implementation. Not only identify improvement opportunities but also actively work towards their implementation.
What do you need to bring-
10+ years of experience as a Data Architect or a similar role.
Proven track record of leading successful data modernization projects.
Deep understanding of data modeling techniques.
Expertise in domain-driven design principles and their application to data modeling.
Proficiency in Kappa and Lambda architectures, including their benefits, challenges, and best practices.
Strong experience with stream processing frameworks (e.g., Apache Flink, Apache Spark Streaming).
Knowledge of data lake and data warehouse concepts.
Experience with cloud platforms (e.g., AWS, Azure, GCP) and their data services.
Certifications related to data architecture, cloud platforms, or data analytics.
Excellent communication and collaboration skills.","stream processing, Data Modeling, Data Architect, cloud platform"
Data Architect,STATS PERFORM,16-20 Years,,Hyderabad,Fitness,"DAZN Group is looking for Data Architect to join our dynamic team and embark on a rewarding career journey A Data Architect is a professional who is responsible for designing, building, and maintaining an organization's data architecture
Designing and implementing data models, data integration solutions, and data management systems that ensure data accuracy, consistency, and security
Developing and maintaining data dictionaries, metadata, and data lineage documents to ensure data governance and compliance
Data Architect should have a strong technical background in data architecture and management, as well as excellent communication skills
Strong problem-solving skills and the ability to think critically are also essential to identify and implement solutions to complex data issues","Data Management, Data Architecture, Data Integration"
Data Architect,Aeries Technology,10-15 Years,,Bengaluru,Information Technology,"Job description
We are looking for a Senior Data Engineer with an architect s mindset and thought leadership approach on the Data & Analytics team. This role goes beyond development its about end-to-end thinking, mentoring and collaborating with a global team.
As a key technical leader, you will align data strategies with business needs, and drive best practices in data modeling, governance, and performance optimization. you will play a pivotal role in streamlining tooling for Democratized Development within Snowflake, DBT related data platforms and build high impact data models.
You will drive performance optimization, cost efficiency, data governance, and model architecture, ensuring that our data infrastructure is scalable, secure, and high-performing.
This role requires deep expertise in cloud-based data engineering, a strong problem-solving mindset, and the ability to collaborate with business and analytics teams . You will work on optimizing compute resources, improving data quality, and enforcing governance standards , support code reviews, and mentor team members, fostering a high-performing data engineering culture.
Key Responsibilities
Data Acquisition & Pipeline Development
Develop and maintain scalable data pipelines for efficient data ingestion, transformation, and integration.
Work with Fivetran, Python, and other ETL/ELT tools to automate and optimize data acquisition from various sources.
Ensure reliable data movement from SaaS platforms (eg, Salesforce, Gong, Google Analytics) and operational databases into Snowflake.
Monitor and enhance pipeline performance, identifying areas for optimization and fault tolerance.
Evaluate and recommend new technologies for data ingestion, transformation, and orchestration.
Democratized Development & Tooling
Enable and streamline self-service data development for analysts and data practitioners.
Drive best practices in DBT and Snowflake for modular, reusable, and governed data modeling.
Design and maintain CI/CD pipelines to support version control, testing, and deployment in a modern data stack.
Performance Optimization & Cost Efficiency
Optimize and tune Snowflake queries and workloads for performance and cost efficiency.
Implement warehouse resource scaling strategies to reduce compute costs while maintaining SLAs.
Monitor and analyze query performance, storage consumption, and data usage patterns to identify optimization opportunities.
Data Quality, Governance & Security
Establish data quality monitoring frameworks, integrating automated validation and anomaly detection.
Enforce data governance policies, including access controls, lineage tracking, and compliance standards.
Work closely with security teams to implement data protection strategies in Snowflake.
Data Model Development & Architecture Review
Design and develop scalable, well-structured data models in Snowflake.
Perform data model reviews to ensure consistency, efficiency, and alignment with business needs.
Collaborate with Analytics & BI teams to define metrics layers and transformation logic in DBT.
Collaboration & Agile Execution
Collaborate with BI teams to ensure data models meet reporting requirements in Power BI.
Partner with cross-functional teams including Sales, Marketing, Customer Support, Finance, and Product to deliver trusted, high-quality data solutions.
Work within an Agile framework, delivering iterative improvements to data infrastructure.
Stay ahead of industry trends in modern data engineering, Snowflake, and DBT to drive innovation.
Qualifications
Bachelor s or Master s degree in Computer Science, Information Technology, or a related field.
5+ years of experience in Data Engineering with strong expertise in Snowflake.
2+ years of hands-on experience in DBT for data modeling and transformation.
2+ years of experience in Python, particularly for data pipelines and automation.
Strong expertise in SQL optimization and performance tuning.
Deep understanding of ETL/ELT architectures, data warehousing, and cloud data management best practices.
Experience implementing cost monitoring and optimization techniques in Snowflake.
Strong problem-solving skills and ability to troubleshoot complex data issues.
Excellent communication skills and ability to work in collaborative, cross-functional teams.
Experience with Agile methodologies and iterative data development processes is a plus.
Role:Data Science & Machine Learning - Other
Industry Type:IT Services & Consulting
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate
PG:Any Postgraduate","Consultant, Automation, Information Technology, Performance Tuning"
Data Architect,GlobalLogic Inc,10-15 Years,,"Bengaluru, Noida",Software,"Description
Mandatory Skills: SQL or No SQL database
Experience in any cloud (AWS or Azure)
Can have : Any scripting language (Python, Java or others)
Requirements
Proficient in database technologies (SQL, NoSQL, and data warehousing solutions).
Familiarity with cloud platforms (AWS, Azure, Google Cloud).
Experience with data integration tools, ETL processes, and data modeling.
Knowledge of programming/scripting languages like Python, Java, or Scala.
Experience with big data technologies (Hadoop, Spark, Kafka) is a plus
Design and implement scalable and secure data architectures.
Develop and optimize data models for structured and unstructured data.
Define data governance policies and best practices.
Collaborate with business and IT teams to understand data needs and implement solutions.
Select and implement appropriate database technologies (SQL, NoSQL, etc.).
Work with ETL/ELT processes to integrate and transform data from multiple sources.
Ensure data integrity, security, and compliance with regulations.
Improve data accessibility and performance across systems.
Guide development teams in best practices for data management and storage.
Job responsibilities
Expertise in SQL, NoSQL, and data modeling techniques.
Strong knowledge of cloud platforms (AWS, Azure, GCP) and data services.
Experience with ETL/ELT pipelines, data lakes, and data warehouses.
Understanding of big data technologies (Hadoop, Spark, Kafka) is a plus.
Knowledge of data security, compliance (GDPR, HIPAA, etc.), and governance.
Strong problem-solving and analytical skills.
Excellent communication and stakeholder management abilities.
What we offer
Culture of caring.At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you'll experience an inclusive culture of acceptance and belonging, where you'll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders.
Learning and development.We are committed to your continuous learning and development. You'll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally.
Interesting & meaningful work.GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you'll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what's possible and bring new solutions to market. In the process, you'll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today.
Balance and flexibility.We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way!
High-trust organization.We are a high-trust organization where integrity is key.By joining GlobalLogic, you're placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do.
About GlobalLogic
GlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world's largest and most forward-thinking companies. Since 2000, we've been at the forefront of the digital revolution helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services.","Data Base, Programming Language, Ms Sql"
Data Architect,Citiustech Healthcare Technology Private Limited,10-15 Years,,"Mumbai City, Mumbai, Pune",Health Care,"Role & responsibilities
Demonstrate problem-solving abilities and strategic thinking to drive continuous improvement in client system and applications Identify the improvement areas in application and provide the solution.
Creation of designing and implementing effective documentation templates.
Mentor and develop new embers in accounts for, problem solving and customer centricity. Periodically review projects and Identify opportunities for process optimization, areas of improvement, and efficiency improvements within the systems
Strategic Thinking
Understanding of Agile/Scrum processes and capability to write User stories and acceptance criteria & work closely with the scrum teams at all levels
Ensure compliance with HIPAA regulations and requirements.
Demonstrate commitment to the Companys core values.
Excellent communication skills
Good to have skills Development/Data analyst:
experience SQL, PySpark, Databricks, delta lake, delta table, Azure data factory in SQL, PySpark Exposure on the CICD pipelines for the azure Databricks, delta lake, delta table, Azure data factory, Data Warehousing/data understanding","Databricks, Sql"
Data Architect I,Bottom Line,10-15 Years,,Bengaluru,Software,"Job description
Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.
Collaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.
Help estimate the size, scope, and timeframes for deliverables.
Evaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.
Define and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.
Design and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.
Develop and implement data governance policies and procedures to ensure data integrity and security.
Develop and maintain documentation for data architecture, standards, policies, and procedures.
Provide technical guidance and mentorship to other members of the data and analytics team.
Stay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.
Requirements
Bachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.
10+ years of demonstrated experience as a Data Architect or similar role.
Strong knowledge of data architecture principles, data modeling techniques, and best practices.
Proficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.
Hands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.
Familiarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).
Strong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.
Excellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.
Proven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.","data architecture principles, snowflake, data modeling techniques, SQL Server"
Data Architect I,Bottom Line,10-15 Years,,Kolkata,Software,"Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.
Collaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.
Help estimate the size, scope, and timeframes for deliverables.
Evaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.
Define and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.
Design and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.
Develop and implement data governance policies and procedures to ensure data integrity and security.
Develop and maintain documentation for data architecture, standards, policies, and procedures.
Provide technical guidance and mentorship to other members of the data and analytics team.
Stay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.
Requirements
Bachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.
10+ years of demonstrated experience as a Data Architect or similar role.
Strong knowledge of data architecture principles, data modeling techniques, and best practices.
Proficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.
Hands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.
Familiarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).
Strong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.
Excellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.
Proven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.
Role:Data Science & Machine Learning - Other
Industry Type:Software Product
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate
PG:Any Postgraduate","Machine Learning, Data Management, My Sql, Data Architecture, Aws"
Data Architect I,Bottom Line,10-15 Years,,"Delhi, Mumbai, Pune",Software,"Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.
Collaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.
Help estimate the size, scope, and timeframes for deliverables.
Evaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.
Define and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.
Design and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.
Develop and implement data governance policies and procedures to ensure data integrity and security.
Develop and maintain documentation for data architecture, standards, policies, and procedures.
Provide technical guidance and mentorship to other members of the data and analytics team.
Stay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.
Requirements
Bachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.
10+ years of demonstrated experience as a Data Architect or similar role.
Strong knowledge of data architecture principles, data modeling techniques, and best practices.
Proficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.
Hands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.
Familiarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).
Strong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.
Excellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.
Proven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.
Role:Data Science & Machine Learning - Other
Industry Type:Software Product
Department:Data Science & Analytics
Employment Type:Full Time, Permanent
Role Category:Data Science & Machine Learning
Education
UG:Any Graduate
PG:Any Postgraduate","Machine Learning, Data Management, My Sql, Data Architecture, Aws"
Data Architect I,Bottom Line,10-15 Years,,"Hyderabad, Chennai, Pune",Software,"Job description
Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.
Collaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.
Help estimate the size, scope, and timeframes for deliverables.
Evaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.
Define and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.
Design and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.
Develop and implement data governance policies and procedures to ensure data integrity and security.
Develop and maintain documentation for data architecture, standards, policies, and procedures.
Provide technical guidance and mentorship to other members of the data and analytics team.
Stay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.
Requirements
Bachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.
10+ years of demonstrated experience as a Data Architect or similar role.
Strong knowledge of data architecture principles, data modeling techniques, and best practices.
Proficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.
Hands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.
Familiarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).
Strong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.
Excellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.
Proven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.","data architecture principles, snowflake, data modeling techniques, SQL Server"
Data Architect I,Bottom Line,10-15 Years,,"Delhi, Kolkata, Mumbai",Software,"Job description
Design and develop scalable and efficient data architectures that meet the needs of the organizations operational, business intelligence, and analytics initiatives.
Collaborate with business stakeholders to understand their data requirements and translate them into data models and technical specifications.
Help estimate the size, scope, and timeframes for deliverables.
Evaluate and recommend data management technologies, tools, and frameworks to support the organizations data architecture and business intelligence goals.
Define and implement data integration processes to ensure data consistency and accuracy across various systems and data sources.
Design and optimize operational data stores, data lakes, data warehouses, and analytics stores to facilitate efficient data storage, retrieval, and analysis.
Develop and implement data governance policies and procedures to ensure data integrity and security.
Develop and maintain documentation for data architecture, standards, policies, and procedures.
Provide technical guidance and mentorship to other members of the data and analytics team.
Stay up-to-date with the latest trends and advancements in data architecture, data management, and business intelligence.
Requirements
Bachelors degree in Computer Science, Information Systems, or related field; Masters degree preferred.
10+ years of demonstrated experience as a Data Architect or similar role.
Strong knowledge of data architecture principles, data modeling techniques, and best practices.
Proficient in SQL and experience with database platforms such as Snowflake, SQL Server, or MySQL.
Hands-on experience with enterprise-level BI and data integration tools such as Tableau, Power BI, Informatica, or Talend.
Familiarity with data warehousing concepts and technologies (e.g., Kimball, Inmon).
Experience with cloud-based data platforms (e.g., AWS, Azure, GCP) and their associated services (e.g., Redshift, BigQuery).
Strong analytical and problem-solving skills, with the ability to analyze complex data requirements and design effective solutions.
Excellent communication and collaboration skills, with the ability to effectively translate technical concepts to non-technical stakeholders.
Proven ability to work in a fast-paced environment, meet tight deadlines, and prioritize tasks effectively.","data architecture principles, snowflake, data modeling techniques, SQL Server"
"Vice President, Data Tech Solutions Architect",Genpact,Fresher,,"Gurugram, Gurugram",IT/Computers - Hardware & Networking,"Ready to shape the future of work
At Genpact, we don&rsquot just adapt to change&mdashwe drive it. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.
If you thrive in a fast-moving, tech-driven environment, love solving real-world problems, and want to be part of a team that&rsquos shaping the future, this is your moment.
Genpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.
Inviting applications for the role of Vice President, Data Tech Solutions Architect!
In this role, we are seeking candidates who have deep technical knowledge and a strategic bent to leverage technology for better business outcomes for the Banking and Capital Markets vertical. The candidate must have detailed understanding of a modern Tech stack, hyperscale offerings, key digital technologies Generative AI, AI/ML, etc.
Role will be part of the Global Banking and Capital Markets Solutions team, as the evangelist for Alliance Partner Technology, Data/ AI led transformation opportunities in all Solutions created for clients. The role will constantly identify solution gaps, understand emerging client needs and market trends, to create and own a mid to long term Data/ Tech solution roadmap. All of this will be done in close partnership with rest of Genpact ecosystem, i.e., Alliance Partners, Service Lines, Data and Tech Practices in order to deliver top notch solutions.
Responsibilities
Provide technical leadership to evaluate and adopt Data, Tech and AI solutions from Alliance Partners and Genpact offerings
Run the solution management cadence, build and own a periodically refreshed roadmap of Data/ Tech new solutionsidentified and prioritized through Client and Sales feedback
Aim for unified solutioning through consolidated business needs to avoid multiple design and build efforts
Collaborate with Alliance Partners and internal teams to create a Tech driven PoV for all solutions, facilitating higher conversions with Clients
Fully own first and second level solutioning, orchestrating initial conversations and engage more specialized Genpact teams and Alliance Partners for deeper solutioning
Provide thought leadership to Lead Solution Architects on RFPs, with an objective to steer Tech and Data led transformation agenda
Build and maintain a comprehensive tech stack view for priority clients to facilitate proactive engagements
Create architectural artifacts for new solutions developed, and drive consistency to elevate the Tech quotient in Client engagements
Qualifications we seek in you!
Minimum Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field.
Relevant years of experience in IT architecture, with a focus on financial services.
Technical Skills:
The mandate for this role will be the same as rest of the Global Solutions team, to cover all geographies and to include all Banking and Capital Markets clients where we are responding to Request for Proposals or working on contract renewals or proactively stocking the shelf for targeted solutions.
Requires strong organizational, technical and communication skills. Requires strong stakeholder management and ability to influence
Proficiency in designing IT solutions.
Experience with cloud platforms (e.g., AWS, Azure, Google Cloud).
Experience in Data Platforms, AI/ GenAI Solutions
Experience in architecting and designing solutions using Microsoft Power Platform, including Power Apps, Power Automate, and Power BI.
Must have a understanding of low-code/no-code environments and the ability to create scalable, secure and efficient solutions that align with business requirements and best practices.
Familiarity with DevOps practices and tools.
Knowledge of concept and principles of agile methodology ability to apply appropriate agile approaches in the processes of software development and delivery.
The mandate for this role will be the same as rest of the Global Solutions team, to cover all geographies and to include all Banking and Capital Markets clients where we are responding to Request for Proposals or working on contract renewals or proactively stocking the shelf for targeted solutions.
Requires strong organizational, technical and communication skills. Requires strong stakeholder management and ability to influence across multiple levels of leadership, while leveraging the different capabilities at Genpact to deliver maximum value for Clients. Must work well in a dynamic, complex environment and under deadline pressures.
Preferred Qualifications/ Skills
Consulting experience - demonstrated ability to convey value proposition, differentiators in a compelling manner to win new relationships/opportunities
Transformation experience - demonstrated ability to design and execute operational transformation. Experience of running day to day operations would be an added advantage
Why join Genpact
Be a transformation leader - Work at the cutting edge of AI, automation, and digital innovation
Make an impact - Drive change for global enterprises and solve business challenges that matter
Accelerate your career - Get hands-on experience, mentorship, and continuous learning opportunities
Work with the best - Join 140,000+ bold thinkers and problem-solvers who push boundaries every day
Thrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress
Come join the tech shapers and growth makers at Genpact and take your career in the only direction that matters: Up.
Let&rsquos build tomorrow together.
Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.
Furthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Data warehouse Architect / Consultant,Robotics Technologies,5-15 Years,INR 12.5 - 22.5 LPA,"Navi Mumbai, Mumbai, Pune",Software Engineering,"Description
We are seeking a skilled Data Warehouse Architect / Consultant to design and implement robust data warehouse solutions. The ideal candidate will have extensive experience in data warehousing concepts, ETL processes, and data modeling, with a proven track record of delivering high-quality data solutions.
Responsibilities
Design and implement data warehouse solutions that meet business needs.
Collaborate with stakeholders to gather requirements and translate them into technical specifications.
Develop ETL processes to extract, transform, and load data from multiple sources into the data warehouse.
Optimize data models for performance and scalability.
Ensure data quality and integrity throughout the data lifecycle.
Provide technical guidance and support to data engineers and analysts.
Stay updated with industry trends and best practices in data warehousing.
Skills and Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
5-15 years of experience in data warehousing, data modeling, and ETL development.
Strong knowledge of SQL and experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL).
Proficiency in data warehousing tools such as Amazon Redshift, Snowflake, or Google BigQuery.
Experience with ETL tools like Informatica, Talend, or Apache NiFi.
Familiarity with data visualization tools such as Tableau, Power BI, or Looker.
Understanding of data governance and data management best practices.
Excellent analytical and problem-solving skills.
Strong communication and interpersonal skills to collaborate with cross-functional teams.","Business Intelligence, Data Modeling, Cloud Services, Big Data, Data Warehousing, Data Governance, Sql, Data Integration, Etl, Performance Tuning"
Data warehouse Architect / Consultant,Robotics Technologies,5-15 Years,INR 18.5 - 25 LPA,"Noida, Delhi NCR, Pune",Software,"Description
We are seeking an experienced Data Warehouse Architect/Consultant to design, implement, and maintain robust data warehousing solutions. The ideal candidate will have a strong background in ETL processes, data modeling, and performance optimization to support our data-driven decision-making.
Responsibilities
Design and implement data warehousing solutions based on business requirements.
Develop and maintain ETL processes to ensure data integrity and availability.
Collaborate with data analysts and business stakeholders to gather requirements and translate them into technical specifications.
Optimize data warehouse performance and troubleshoot issues as they arise.
Create documentation for data warehouse architecture, ETL processes, and data modeling.
Ensure compliance with data governance and security policies.
Skills and Qualifications
5-15 years of experience in data warehousing and ETL development.
Proficient in SQL and experience with database management systems like Oracle, SQL Server, or PostgreSQL.
Experience with ETL tools such as Informatica, Talend, or Apache Nifi.
Strong understanding of data modeling concepts and methodologies (dimensional modeling, star schema, snowflake schema).
Familiarity with cloud data warehousing solutions such as Amazon Redshift, Google BigQuery, or Snowflake.
Knowledge of data visualization tools like Tableau, Power BI, or Looker is a plus.
Excellent analytical and problem-solving skills.
Strong communication skills to work effectively with team members and stakeholders.","Business Intelligence, Cloud Computing, Etl Tools, Data Modeling, Big Data, Data Warehousing, Data Governance, Sql, Data Integration, Performance Tuning"
"Senior Principal Consultant - Data Architect, Data Engineering",Genpact,Fresher,,Bengaluru,IT/Computers - Hardware & Networking,"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose - the relentless pursuit of a world that works better for people - we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.
Inviting Applications for Senior Principal Consultant - Data Architect, Data Engineering!
A Data Architect who can design and implement data modernization solutions in the cloud is responsible for developing and implementing data architecture strategies for organizations transitioning their data systems to the cloud. They collaborate with stakeholders to understand business requirements and translate them into scalable, secure, and high-performing data solutions. The Data Solution Architect works closely with data engineers, data scientists, and other IT professionals to design and deliver robust data platforms and architectures.
Responsibilities:
Collaborate with stakeholders to understand business requirements and translate them into data architecture strategies and solutions.
Design and develop scalable, secure, and high-performing data architectures in the cloud, leveraging cloud technologies and services.
Modernize legacy data systems and migrate them to cloud-based architectures, ensuring smooth transition and minimal disruption.
Collaborate with data engineers, data scientists, and other IT professionals to design and implement data pipelines, data transformation processes, and data integration solutions.
Ensure data governance principles and best practices are implemented, including data quality controls, data privacy, and compliance.
Optimize data platforms for performance, scalability, and cost efficiency, monitoring and troubleshooting any issues that may arise.
Stay updated with the latest trends and advancements in cloud technologies and data architecture, continuously enhancing knowledge and skills.
Provide guidance and mentorship to junior team members, fostering a culture of learning and growth.
Act as a subject matter expert in data architecture and cloud technologies, providing recommendations and insights to stakeholders and senior management
Lead RFP responses by working with the Sales, Vertical and competency teams. Groom the team members in the pre-sales area.
Qualifications we seek in you!
Minimum Qualifications
Data Architecture: In-depth knowledge and experience in designing and implementing scalable, secure, and high-performing data architectures in the cloud.
Cloud Technologies: Proficiency in cloud platforms such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP), including their data services like AWS Glue, Azure Data Factory, or Google BigQuery.
Data Modernization: Expertise in modernizing legacy data systems and transitioning them to cloud-based architectures using industry best practices.
Data Integration: Strong understanding of data integration techniques, including ETL (Extract, Transform, Load) processes, data pipelines, and data streaming using Python , Kafka for streams ,Pyspark , DBT , and ETL services provided by cloud providers and SQL/PLSQL
Database Technologies: Familiarity with various database technologies, both relational and non-relational, such as SQL databases, NoSQL databases, data lakes, and data warehouses. Should have strong hands-on knowledge in Data Modelling techniques using ERWIN
Data Governance: Knowledge of data governance principles and practices, including data quality, data lineage, data privacy, and compliance.
Data Security: Understanding of data security principles, including encryption, access controls, and data masking techniques.
Programming and Scripting: Proficiency in programming languages such as Python, Java, or Scala, and experience with scripting languages like SQL or Shell scripting.
Communication and Collaboration: Excellent communication and collaboration skills to effectively work with stakeholders, business users, and technical teams.
Pre-Sales :Ability to lead/Coordinate the medium to large scale RFPs and construct the end to end big-data, analytics technical solutions and articulate the same using Power point or word document. Participate the client presentations and present the solutions prepared.
Preferred Qualifications
Hybrid cloud domain exposure including end to end data architecture experience.
Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation. Get to know us at and on , , , and .
Furthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Data Architect,Wipro Limited,11-13 Years,,Chennai,IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.
Job Description
Role:
Service Desk Manager
Band C1 Role (Data Architect)
Location Chennai, Noida
Total exp 11+ Years
The candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.
Must have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop
Must have hands on in writing complex use case driven SQLs
Should have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.
Should have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3
Should have been involved in On-Prem to Cloud Migration process.
Should have good knowledge with HIVE / Spark / Scala scripts
Should have good knowledge on Unix Shell scripting
Should be flexible to overlap US business hours
Should be able to drive technical design on Cloud applications
Should be able to guide & drive the team members for cloud implementations
Should be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.
AWS Certified applicants preferable
Competencies
Client Centricity
Passion for Results
Collaborative Working
Problem Solving & Decision Making
Effective communication
Reinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, On-Prem to Cloud Migration, Data Warehouse, Sql, S3, Unix Shell Scripting, Emr, Aws Cloud, Hadoop, Etl, Sqoop, Hive, Scala, Redshift, Spark"
Data Architect,Wipro Limited,11-13 Years,,"Gurugram, Gurugram",IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.
Job Description
Role:
Service Desk Manager
Band C1 Role (Data Architect)
Location Chennai, Noida
Total exp 11+ Years
The candidate must have overall 11+ years of experience in ETL and Data Warehouse of which 3-4 years on Hadoop platform and at least 2 year in Cloud Big Data Environment.
Must have hands on experience on Hadoop services like HIVE/ Spark/ Scala /Sqoop
Must have hands on in writing complex use case driven SQLs
Should have about 3+ years of hands-on good knowledge of AWS Cloud and On-Prem related key services and concepts.
Should have 3+ years of working experience with AWS Cloud tools like EMR, Redshift, Glue S3
Should have been involved in On-Prem to Cloud Migration process.
Should have good knowledge with HIVE / Spark / Scala scripts
Should have good knowledge on Unix Shell scripting
Should be flexible to overlap US business hours
Should be able to drive technical design on Cloud applications
Should be able to guide & drive the team members for cloud implementations
Should be well versed with the costing model and best practices of the services to be used for Data Processing Pipelines in Cloud Environment.
AWS Certified applicants preferable
Competencies
Client Centricity
Execution Excellence
Collaborative Working
Problem Solving & Decision Making
Effective communication
Reinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.","Glue, S3, Sql, Unix Shell Scripting, Aws Cloud, Emr, Hadoop, Etl, Sqoop, Hive, Scala, Data Warehouse, Redshift, Spark"
Data Architect,DBiz.ai,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"About the Company:
Dbiz is a high-performing product and engineering company that partners with organizations to build out digital solutions using the right technology at the right time. We pride ourselves on our innovative use of technology in various ways.
Role Summary:
We are seeking a Solution Architect with deep expertise in microservices architecture and AWS cloud technologies. Knowledge of Azure is a plus. The ideal candidate will be passionate about big data, analytics, and AI, with a strong background in designing scalable, secure, and cost-effective data architectures.
Responsibilities:
Data Architecture Design: Create and implement scalable data architectures using AWS, supporting BI, analytics, and AI initiatives.
Cloud Data Solutions: Lead AWS-based data ecosystems using services like S3, Redshift, Athena, AWS Data Lakehouse, and more.
Microservices Architecture: Design and implement microservices for distributed data processing, ensuring efficient communication and scalability.
Data Modelling & Integration: Develop data models (relational & non-relational), integrate diverse data sources (databases, APIs, real-time streaming).
ETL & Data Pipelines: Architect end-to-end pipelines with tools like AWS Glue, Lambda, ensuring seamless ingestion and transformation.
Data Governance & Security: Implement policies for data security, privacy, and integrity; leverage AWS services like IAM and encryption.
Performance Optimization: Enhance performance and cost efficiency for data storage, retrieval, and processing.
Collaboration & Stakeholder Engagement: Work cross-functionally with product, data engineering, and business teams to align architecture with goals.
Emerging Technologies: Stay updated on cloud tech, data management, and adopt new tools/frameworks.
Leadership: Provide technical guidance, mentor data engineers, and lead architectural reviews and decisions.
Requirements:
Experience: Minimum 10 years in data architecture focusing on AWS cloud services.
Microservices Expertise: Proven ability to design scalable data solutions on AWS using microservices.
AWS Technologies: Proficiency with S3, Redshift, Athena, AWS Glue, etc.
Collaboration: Strong communication skills for working with diverse teams.
Leadership: Experience guiding architectural decisions and mentoring technical teams.
Good to Have:
ETL Pipelines: Experience with AWS Glue, Lambda, or similar.
Data Modeling: Skills in both relational and NoSQL databases.
Data Governance: Understanding of data security, compliance, and governance.
Optimization: Track record of cost-effective, high-performance data architecture.
Problem-Solving: Strategic mindset for building resilient data solutions.
Life at Dbiz:
Competitive salary and attractive benefits
Dynamic and innovative work environment
Opportunities for personal growth and development
Engaging and collaborative company culture","microservices architecture, data architecture design, Ai, AWS cloud technologies, ETL data pipelines, AWS Data Lakehouse, Big Data Analytics, Data Modeling, Data Governance, AWS Glue, Encryption"
Microsoft Fabric Data Architect - Manager,KPMG India,9-11 Years,,"Pune, India",Login to check your skill match score,"Job Description
About KPMG in India
KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.
KPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focussed and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.
The person will work on a variety of projects in a highly collaborative, fast-paced environment. The person will be responsible for software development activities of KPMG, India. Part of the development team, he/she will work on the full life cycle of the process, develop code and unit testing. He/she will work closely with Technical Architect, Business Analyst, user interaction designers, and other software engineers to develop new product offerings and improve existing ones. Additionally, the person will ensure that all development practices are in compliance with KPMG's best practices policies and procedures. This role requires quick ramp up on new technologies whenever required.
Responsibilities
Role : Microsoft Fabric Data Architect
Location: Pune
Experience: 9 to 11 years
Responsibilities
Data Architecture: Design end-to-end data architecture leveraging Microsoft Fabric's capabilities.
Data Flows: Design data flows within the Microsoft Fabric environment.
Storage Strategies: Implement OneLake storage strategies.
Analytics Configuration: Configure Synapse Analytics workspaces.
Integration Patterns: Establish Power BI integration patterns.
Data Integration: Architect data integration patterns between systems using Azure Databricks and Microsoft Fabric.
Delta Lake Architecture: Design Delta Lake architecture and implement medallion architecture (Bronze/Silver/Gold layers).
Real-Time Data Ingestion: Create real-time data ingestion patterns and establish data quality frameworks.
Data Governance: Establish data governance frameworks incorporating Microsoft Purview for data quality, lineage, and compliance.
Security: Implement row-level security, data masking, and audit logging mechanisms.
Pipeline Development: Design scalable data pipelines using Azure Databricks for ETL/ELT processes and real-time data integration.
Performance Optimization: Implement performance tuning strategies for large-scale data processing and analytics workloads.
Experience: Proven experience in data architecture, particularly with Microsoft Fabric and Azure Databricks.
Technical Skills: Proficiency in Microsoft Fabric, Azure Databricks, Synapse Analytics, and data modeling.
Analytical Skills: Strong analytical and problem-solving skills.
Communication: Excellent communication and teamwork skills.
Certifications: Relevant certifications in Microsoft data platforms are a plus
Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Equal Opportunity Employer
KPMG India has a policy of providing equal opportunity for all applicants and employees regardless of their color, caste, religion, age, sex/gender, national origin, citizenship, sexual orientation, gender identity or expression, disability or other legally protected status. KPMG India values diversity and we request you to submit the details below to support us in our endeavor for diversity. Providing the below information is voluntary and refusal to submit such information will not be prejudicial to you.","Real-Time Data Ingestion, Data Flows, Microsoft Fabric, Power BI Integration Patterns, Microsoft Purview, Audit Logging, Synapse Analytics, pipeline development, Delta Lake Architecture, Performance Optimization, Row-Level Security, OneLake Storage Strategies, ELT, data masking, Data Architecture, Azure Databricks, Etl, Data Governance, Data Modeling"
Data Architect,DHL,Fresher,,"Indore, India",Login to check your skill match score,"Start your IT career with us!
or
Your IT Future, Delivered
Solutions Architect
Open to all PAN India candidates.
With a global team of 6000+ IT professionals, DHL IT Services connects people and keeps the global economy running by continuously innovating and creating sustainable digital solutions. We work beyond global borders and push boundaries across all dimensions of logistics. You can leave your mark shaping the technology backbone of the biggest logistics company of the world. Our offices in Cyberjaya, Prague, and Chennai have earned #GreatPlaceToWork certification, reflecting our commitment to exceptional employee experiences.
Digitalization. Simply delivered.
At IT Services, we are passionate about Solution Architect in datawarehouse and business intelligence space. Our Customer Service Complex Data Solution team is continuously expanding. No matter your level of Solution Architect proficiency, you can always grow within our diverse environment.
#DHL #DHLITServices #GreatPlace #ppmt #Kart #cscombine
Grow together
We strive to deliver efficient and optimized business solutions in the Area of Customer Service Complex Data Solutions for our business. You will work as Solutions Architect for existing and new applications to provide end to end Architecture expertise on wide range of technologies like Snowflake, Teradata, Power BI, Matillion, Azure Cloud and many more.
You will be our main Architect providing guidance and direction on the implementation of Analytics, Data Warehousing & Reporting products. You will ensure that the Analytics & Reporting solutions meets the required performance benchmark and adheres to standards & guidelines.
You will work with project teams to ensure Business Requirements are delivered keeping in mind the end-to-end Solution & Data Architecture. You will get to work with some of the complex data structures that will need your expertise to Data Modelling & Design. You will be involved in Optimizing the performance and resource utilization of the existing solutions.
You will guide the development team with technical expertise for ensuring business requirements are implemented as expected. This would mean you sometime have to get down to coding and provide a solution or high-level approach to achieve the requirement to give direction to the Dev Team.
As a senior member in the team, you will collaborate with business users on Requirements and ensure that the requirements are well defined before assigning for development. Lead discussion with Business during UAT Defects review.
You will be working on latest technologies like Snowflake, Matilllion, Teradata, ERWIN, Microservices, Data pipelines, Jenkins, Jira/Confluence, Splunk etc. You will get ample opportunities to grow within the organization and with focus on continuous learning will get opportunity to work & learn many different technologies.
Ready to embark on the journey Here's what we are looking for:
As a Solution Architect, having excellent skill in understanding the latest technology relation to the business knowledge of customer service experience is a huge plus. Very good knowledge of data modeling will also be an integral part of this role and experience in implementation of customer facing application. Been part of the Agile / Scrum team experience is useful. Well versed in Architecture design, software development experiences especially in Python, familiarity of development framework and also analytics and problem solving skills.
You are a business intelligence technology aficionado, therefore you have a good understanding of latest analytics skill sets and experience in implementation of MVP and POC rapid prototyping experience is good to have also in the AI space of new technology adoptions. You are able to work independently prioritize and organize your tasks under time and workload pressure. Working in a multinational environment, you can expect cross-region collaboration with teams around the globe, thus being advanced in spoken and written English will be certainly useful. Basic certification / knowledge of AWS / Azure/ Snowflake/ Teradata/ Power BI related too is a plus.
An array of benefits for you:
Hybrid work arrangements to balance in-office collaboration and home flexibility.
Annual Leave: 42 days off apart from Public / National Holidays.
Medical Insurance: Self + Spouse + 2 children. An option to opt for Voluntary
Parental Insurance (Parents / Parent -in-laws) at a nominal premium covering pre existing disease.
In House training programs: professional and technical training certifications.","Matillion, Teradata, Analytics, snowflake, Data pipelines, Data Modelling, Power Bi, Jira, Microservices, Jenkins, Confluence, Azure Cloud, Splunk, Python"
Data Center Architect - Delivery,Lenovo India,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Resource who is expert in Data Center System Integration (SI) services discovery, assessment, Planning, Design, Setup, Configure, and Migration Services with Data Center products & solutions such as VMware, VCF, Nutanix, Azure Stack HCI, backup, Storage, SDDC, containers, Red hat KVM, Open-shift etc.
Training & Certifications: VMware , Nutanix, Server, Backup, Storage, HCI
Experience Required: 12-15 Years of Experience in Data Center services, Hybrid Cloud deployment on premises experience is mandatory.","Azure Stack HCI, Backup Storage, Data Center System Integration, SDDC, Red Hat KVM, VMware VCF, Nutanix, Containers"
AWS Data & AI Architect,Cloud202,3-5 Years,,India,Login to check your skill match score,"Location: Remote (India)
Job Type: Full-time | Permanent | Remote
About Cloud202:
As an AWS Advanced Tier Services Partner, we specialize in helping organizations accelerate innovation through modern cloud-native solutions, data platforms, and AI applicationsincluding Generative AI. With a global presence and a deep commitment to business impact, we work with leading enterprises to design intelligent, ethical, and scalable systems.
Role Overview:
We are looking for a Data & AI Architect with deep AWS expertise to lead the design and implementation of modern, cloud-native data and AI platforms. This role will combine architectural vision with hands-on technical delivery, driving end-to-end solutions that enable advanced analytics, data governance, and enterprise-scale AIincluding Generative AI use cases.
Key Responsibilities:
Design and lead scalable data and AI architectures using AWS services such as SageMaker, Bedrock, Glue, Redshift, Athena, EMR, and Lake Formation.
Architect end-to-end data pipelines that support AI/ML workloads, data lakehouse strategies, and real-time analytics.
Drive implementation of Retrieval-Augmented Generation (RAG) pipelines, vector databases, and LLM-based inference workflows.
Collaborate with business and technical stakeholders to understand use cases, data requirements, and success metrics.
Embed data governance, security, lineage, and MLOps best practices into platform designs.
Mentor engineering teams and provide technical leadership across data and AI initiatives.
Work closely with partner and client teams to co-create Generative AI MVPs, production-grade applications, and reusable blueprints.
Ensure architectural alignment with AWS Well-Architected principles and sustainability guidelines.
Required Qualifications:
3+ years of experience in data engineering, analytics, or AI roles, with at least 1+ years in architecture.
Strong expertise in AWS analytics and AI services, including SageMaker, Bedrock, Glue, Lake Formation, Redshift, and Athena.
Proficient in SQL, Python, and PySpark, with hands-on experience in building data pipelines and model workflows.
Solid understanding of Generative AI patterns (e.g., RAG, prompt engineering, fine-tuning, grounding).
Experience with data governance, metadata management, and secure data sharing practices.
Strong stakeholder management and communication skills.
Preferred Qualifications:
AWS Certified (Solutions Architect Professional, Data Analytics Specialty, or Machine Learning Specialty).
Experience with LangChain, vector databases (e.g., FAISS, Pinecone, OpenSearch), and MLOps tools.
Familiarity with open-source LLMs and foundation models, fine-tuning workflows, and responsible AI practices.
Background in consulting or customer-facing delivery roles is a plus.
Why Join Cloud202:
100% remote work from anywhere in India with flexible hours.
Work on impactful, AWS Generative AI and data modernization projects.
Access to global clients and innovation programs across sectors.
Flat, transparent, and collaborative culture that values continuous learning.
Certifications, mentorship, and growth opportunities in cutting-edge AI and cloud domains.","OpenSearch, Lake Formation, Athena, vector databases, SageMaker, Glue, bedrock, LangChain, Pinecone, FAISS, Generative AI, Sql, Emr, Pyspark, MLops, AWS, Python, Redshift"
Data Analytics Architect (Looker),66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees
66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.
At 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.
Overview of Role
We're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.
Responsibilities
Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
Work with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.
Advise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.
Utilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.
Write clean, efficient, and scalable code (LookML, Python as applicable)
Conduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.
Stay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.
Collaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.
Provide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.
Excellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.
Experience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.
Strong proficiency with SQL required.
Demonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.
Excellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.
Solid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.
Strong problem-solving skills and the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Google Cloud certifications and any analytics platform certifications are a plus.
A desire to stay ahead of the curve and continuously learn new technologies and techniques.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, Data Modeling, Python, Sql, Google Cloud"
Data center Architect,Kyndryl India,7-9 Years,,"Mumbai, India",Login to check your skill match score,"Who We Are
At Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.
The Role
Job Description
Infrastructure Specialists at Kyndryl are project-based subject matter experts in all things infrastructure good at providing analysis, documenting and diagraming work for hand-off, offering timely solutions, and generally figuring it out. This is a hands-on role where your feel for the interaction between a system and its environment will be invaluable to every one of your clients.
There are two halves to this role: First, contributing to current projects where you analyze problems and tech issues, offer solutions, and test, modify, automate, and integrate systems. And second, long-range strategic planning of IT infrastructure and operational execution. This role isn't specific to any one platform, so you'll need a good feel for all of them. And because of this, you'll experience variety and growth at Kyndryl that you won't find anywhere else.
You'll be involved early to offer solutions, help decide whether something can be done, and identify the technical and timeline risks up front. This means dealing with both client expectations and internal challenges in other words, there are plenty of opportunities to make a difference, and a lot of people will witness your contributions. In fact, a frequent sign of success for our Infrastructure Specialists is when clients come back to us and ask for the same person by name. That's the kind of impact you can have!
This is a project-based role where you'll enjoy deep involvement throughout the lifespan of a project, as well as the chance to work closely with Architects, Technicians, and PMs. Whatever your current level of tech savvy or where you want your career to lead, you'll find the right opportunities and a buddy to support your growth. Boredom Trust us, that won't be an issue.
Your future at Kyndryl
There are lots of opportunities to gain certification and qualifications on the job, and you'll continuously grow as a Cloud Hyperscaler. Many of our Infrastructure Specialists are on a path toward becoming either an Architect or Distinguished Engineer, and there are opportunities at every skill level to grow in either of these directions.
Who You Are
You're good at what you do and possess the required experience to prove it. However, equally as important you have a growth mindset; keen to drive your own personal and professional development. You are customer-focused someone who prioritizes customer success in their work. And finally, you're open and borderless naturally inclusive in how you work with others.
Required Technical and Professional Expertise
Minimum of 7 years in IT infrastructure design and management, with demonstrable experience in on-premises solutions.
Experience in developing and maintaining high-quality documentation for complex technical systems.
In-depth understanding of on-premises infrastructure components including servers, storage, networking, virtualization, and security.
Familiarity with automation, monitoring, and management tools.
Architect and deploy scalable, secure, and resilient on-premises systems including servers, storage, networking, and virtualization.
Develop technical blueprints and solution designs that align with business objectives and industry best practices.
Evaluate emerging technologies and recommend enhancements to improve system performance and security.
Develop, maintain, and update detailed technical documentation, including system architecture diagrams, design specifications, standard operating procedures (SOPs), and disaster recovery plans.
Create training materials and conduct technical sessions for internal teams to ensure proper system usage and maintenance.
Preferred Technical And Professional Experience
Bachelor's degree in computer science, Information Technology, or a related field. Master's degree is a plus.
Relevant certifications such as TOGAF, ITIL, PMP, or similar are preferred.
Ensure all infrastructure solutions meet relevant regulatory requirements, security standards, and compliance guidelines.
Monitor system performance and implement optimization measures to enhance efficiency and reliability.
Coordinate with external vendors and service providers as needed to support infrastructure initiatives.
Being You
Diversity is a whole lot more than what we look like or where we come from, it's how we think and who we are. We welcome people of all cultures, backgrounds, and experiences. But we're not doing it single-handily: Our Kyndryl Inclusion Networks are only one of many ways we create a workplace where all Kyndryls can find and provide support and advice. This dedication to welcoming everyone into our company means that Kyndryl gives you and everyone next to you the ability to bring your whole self to work, individually and collectively, and support the activation of our equitable culture. That's the Kyndryl Way.
What You Can Expect
With state-of-the-art resources and Fortune 100 clients, every day is an opportunity to innovate, build new capabilities, new relationships, new processes, and new value. Kyndryl cares about your well-being and prides itself on offering benefits that give you choice, reflect the diversity of our employees and support you and your family through the moments that matter wherever you are in your life journey. Our employee learning programs give you access to the best learning in the industry to receive certifications, including Microsoft, Google, Amazon, Skillsoft, and many more. Through our company-wide volunteering and giving platform, you can donate, start fundraisers, volunteer, and search over 2 million non-profit organizations. At Kyndryl, we invest heavily in you, we want you to succeed so that together, we will all succeed.
Get Referred!
If you know someone that works at Kyndryl, when asked How Did You Hear About Us during the application process, select Employee Referral and enter your contact's Kyndryl email address.","management tools, on-premises solutions, documentation for complex technical systems"
Data Platform Architect,SpurTree Technologies,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"As a Data Platform Architect, you will play a pivotal role in the design, implementation, and optimization of scalable data architecture and systems. You will be responsible for creating data-driven solutions that enable effective data storage, integration, processing, and analytics. You will collaborate closely with data engineers, data scientists, and business stakeholders to build a robust data infrastructure that supports the organization's data strategy.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, Data Engineering, or a related field.
Proven experience (typically 7+ years) in designing and implementing complex data architectures and platforms.
Expertise in cloud platforms (AWS, Azure, GCP) and their data services (e.g., Amazon Redshift, Azure Synapse, BigQuery).
Strong knowledge of data integration, ETL/ELT tools, and data pipelines.
Hands-on experience with big data technologies (e.g., Hadoop, Spark, Kafka).
Proficiency in database technologies (SQL, NoSQL, columnar, graph, etc.).
Familiarity with data modeling techniques and practices.
Strong understanding of data security, governance, and privacy practices.
Experience with data visualization and reporting tools (e.g., Power BI, Tableau).
Excellent problem-solving, analytical, and troubleshooting skills.
Strong communication and collaboration skills with technical and non-technical teams.
Preferred Skills
Certifications in cloud platforms (e.g., AWS Certified Solutions Architect, Microsoft Certified: Azure Data Engineer Associate).
Experience with containerized environments (e.g., Docker, Kubernetes).
Familiarity with DevOps practices and CI/CD pipelines.
Knowledge of machine learning and artificial intelligence applications in data platforms.
Experience with Agile methodologies and project management tools.","BigQuery, Hadoop, Power Bi, Kafka, Tableau, Sql, ELT, Nosql, Azure Synapse, Gcp, Docker, Amazon Redshift, Spark, Data Visualization, Azure, Kubernetes, AWS, Etl"
Data Modelling Architect (CoE),NTT DATA North America,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Req ID: 312265
NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Data Modelling Architect (CoE) to join our team in Bangalore, Karntaka (IN-KA), India (IN).
Title: Data Modelling Architect (CoE)
Position Overview:
We are seeking a highly skilled and experienced Data Modelling Architect to join our dynamic team. The ideal candidate will have a strong background in full data modelling life cycle, e.g. design, implement, and maintain complex data models that align with organisational goals and industry standards. This role requires a deep understanding of data architecture, data modelling methodologies, and ideally in real-time data integrations. The successful candidate will collaborate with cross-functional teams to ensure optimal data structures that support business intelligence, analytics, and operational requirements. This role is primarily within the Centre of Excellence of the Data Products Factory, creating, assuring, and overseeing the implementation of data models within analytical and real-time streaming domains.
Key Responsibilities
Develop conceptual, logical, and physical data models to support data analytics, streaming and data products implementation.
Define and maintain data architecture standards, principles, and best practices.
Ensure data models are aligned with business requirements and scalable for future needs.
Work closely with business stakeholders, data engineers, data solution architects, and data product teams to gather requirements and design solutions.
Provide guidance on data integration, transformation, and migration strategies.
Establish and maintain enterprise data models, data dictionaries, metadata repositories, and data lineage documentation.
Ensure data models comply with organisational policies and regulatory requirements.
Optimise data products and their components for performance, scalability, and reliability.
Evaluate and recommend data modelling tools and technologies.
Stay updated on industry trends and emerging technologies in data architecture.
Identify and resolve data inconsistencies, redundancies, and performance issues.
Provide technical leadership in addressing complex data-related challenges.
Required Skills And Qualifications
10+ years of experience in data architecture and modelling.
Proven experience in data modelling, data architecture, and data products design.
Proven experience and expertise in data modelling standards, techniques (e.g. dimensional model, 3NF, Vault 2.0)
Familiarity with both analytical and real-time/ streaming data solutions (Kafka/Airflow).
Hands-on experience with data modelling tools (e.g., Erwin, Lucidchart, SAP PowerDesigner).
Expertise in Python and SQL (e.g., Snowflake, Kafka).
Experience in AWS cloud services, particularly Lambda, SNS, S3, and EKS, API Gateway
Knowledge of data warehouse design, ETL/ ELT processes, and big data technologies (e.g., Snowflake, Spark).
Familiarity with data governance and compliance frameworks (e.g., GDPR, HIPAA).
Strong communication and stakeholder management skills.
Analytical mindset with attention to detail.
Ability to lead and mentor teams on best practices in data modelling.
Education: Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Preferred Skills And Qualifications
Certifications in data modelling, cloud platforms, or database technologies.
Experience in developing and implementing enterprise data models.
Experience with Interface/ API data modelling.
Experience with CI/CD GITHUB Actions (or similar)
AWS fundamentals (e.g., AWS Certified Data Engineer)
Knowledge of Snowflake/ SQL
Knowledge of Apache Airflow
Knowledge of DBT
Familiarity with Atlan for data catalog and metadata management
Understanding of iceberg tables
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com
NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Airflow, AWS cloud services, Lucidchart, snowflake, dimensional model, Vault 2.0, data warehouse design, 3NF, EKS, SAP PowerDesigner, data products design, Gdpr, S3, Data Architecture, Kafka, Big Data Technologies, Hipaa, ELT, Lambda, Sns, Python, Api Gateway, Data Modelling, Sql, Erwin, Spark, Data Governance, Etl"
Analytics Head / Data Model Architect,Deepak Group Co,10-12 Years,,"Vadodara, India",Login to check your skill match score,"Job Title: Analytics Head / Data Model Architect
Job Summary:
We are seeking a seasoned Analytics Head / Data Model Architect to lead our data strategy, design scalable data models, and drive analytical innovation across the organization. This role combines leadership in data science and business analytics with deep technical expertise in data architecture and modelling. The ideal candidate will be a strategic thinker, technical expert, and effective communicator capable of aligning data initiatives with business objectives.
Key Responsibilities:
Leadership & Strategy
Lead and manage a team of data scientists, analysts, and data architects.
Define and drive the enterprise analytics strategy aligned with business goals.
Collaborate with executive leadership to identify data-driven growth opportunities.
Data Architecture & Modeling
Design and implement robust, scalable, and high-performance data models (OLAP/OLTP, dimensional, relational, NoSQL).
Develop enterprise data architecture standards, policies, and best practices.
Oversee data governance, data quality, and metadata management initiatives.
Advanced Analytics & Insights
Build advanced analytics solutions including predictive modeling, statistical analysis, and machine learning frameworks.
Translate complex data into actionable insights for stakeholders across departments.
Evaluate and implement modern BI tools and platforms (e.g., Power BI, Tableau, Looker).
Collaboration & Integration
Partner with data engineering teams to ensure efficient ETL/ELT pipelines and data lake/warehouse infrastructure.
Work closely with business units to understand needs and deliver customized data solutions.
Support data privacy, security, and compliance initiatives (e.g., GDPR, HIPAA, SOC 2).
Required Qualifications:
Bachelor's or master's in computer science, Data Science, Statistics, or related field. PhD is a plus.
10+ years of experience in analytics, data architecture, or related roles.
Strong knowledge of data modeling techniques (3NF, Star Schema, Snowflake, Data Vault, etc.).
Expertise in SQL, Python, R, and at least one cloud platform (AWS, Azure, GCP).
Experience with modern data warehousing tools (Snowflake, BigQuery, Redshift) and orchestration (Airflow, DBT).
Proven leadership and team-building skills.
Preferred Skills:
Experience with AI/ML model deployment in production.
Familiarity with data mesh, data fabric, or modern data stack concepts.
Knowledge of industry-specific data standards (e.g., HL7 for healthcare, ACORD for insurance).","Airflow, dimensional, Looker, R, relational, dbt, snowflake, BigQuery, Machine Learning, Power Bi, OLAP, Tableau, Redshift, Sql, Nosql, Gcp, Predictive Modeling, Azure, Python, AWS, Oltp, Statistical Analysis"
Data Analytics Architect (Looker),66degrees,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees
66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.
At 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.
Overview of Role
We're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.
Responsibilities
Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
Work with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.
Advise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.
Utilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.
Write clean, efficient, and scalable code (LookML, Python as applicable)
Conduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.
Stay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.
Collaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.
Provide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.
Excellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.
Experience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.
Strong proficiency with SQL required.
Demonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.
Excellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.
Solid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.
Strong problem-solving skills and the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Google Cloud certifications and any analytics platform certifications are a plus.
A desire to stay ahead of the curve and continuously learn new technologies and techniques.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, Microstrategy, Looker, LookML, Azure, Python, Sql, Google Cloud, AWS"
Big Data-Cloud Architect,Airtel Digital,8-12 Years,,"Pune, India",Login to check your skill match score,"We are seeking an experienced Big Data Cloud Architect with 812 years in designing and implementing scalable big data solutions on Azure and AWS. The ideal candidate will have deep hands-on expertise in cloud-based big data technologies, a strong background in the software development life cycle (SDLC), and experience in microservices and backend development.
**Key Responsibilities**
Design, build, and maintain scalable big data architectures on Azure and AWS
Select and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )
Lead data migration from legacy systems to cloud-based solutions
Develop and optimize ETL pipelines and data processing workflows.
Ensure data infrastructure meets performance, scalability, and security requirements.
Collaborate with development teams to implement microservices and backend solutions for big data applications.
Oversee the end-to-end SDLC for big data projects, from planning to deployment.
Mentor junior engineers and contribute to architectural best practices.
Prepare architecture documentation and technical reports.
#ADL","Azure Data Factory, Hadoop, Spark, Kafka, Azure, Etl, AWS"
Celonis Data Architect,Harman Connected Services Corporation India Private Limited,12-15 Years,INR 2 - 3 LPA,Bengaluru,Software,"A Celonis data architect is responsible for designing and implementing the data architecture required for effective utilization of the Celonis process mining tool within an organization. They work closely with various stakeholders, including data engineers, business analysts, and process owners, to define data requirements, ensure data integrity, and enable seamless data integration.
The role requires a strong understanding of data architecture principles, data modeling techniques, and relational and non-relational databases. Candidate should also be proficient in ETL processes and tools, as well as have experience in data integration and data governance. Additionally, candidate should have a good understanding of business processes, data visualization, and analytics to effectively implement process mining initiatives within the customer organization
Role and Responsibilities
Over all 12-15 years of IT experience with minimum 5+ years experience in Process Mining using Celonis
Strong experience in Data Integration, Data Modeling, Data quality, Data pipeline management, Dashboard design
Experience in delivering at least 1 to 2 large scale end-to-end Celonis process mining implementations as Data Architect
Experience in leading team of data modelers, data engineers
Strong experience in providing multiple solutions and reviewing the implementations in parallel
Expertise in defining the governance, security, roles around Data pools and dashboards in Celonis
Experience in implementing object centric process mining
Major accountabilities:
Collaborating with business stakeholders to understand their data requirements and process mining goals.
Engage with customers C-level their strategic objectives with the Celonis technical strategy
Designing and implementing data models, schemas, and structures that align with the organization's data governance policies and standards.
Identifying and documenting data sources, including databases, systems, and applications, that need to be integrated with Celonis.
Ensuring proper data extraction, transformation, and loading (ETL) processes to populate the necessary data into Celonis.
Supervising data engineers in the development and maintenance of data pipelines and workflows.
Assessing data quality, consistency, and accuracy and taking proactive measures to resolve issues. Implementing data security measures, including access controls and data encryption, to protect sensitive information within Celonis.
Providing technical support and guidance to business analysts and end-users in data preparation, analysis, and reporting using Celonis.
Staying updated with the latest trends and advancements in data management and process mining technologies, including Celonis updates and new features.","Celonis Data Architect, Etl"
Big Data Architect,Persistent,10-12 Years,,Hyderabad,Information Technology,"We are looking for a Data Architect with creativity and results-oriented critical thinking to meet complex challenges and develop new strategies for acquiring, analyzing, modeling and storing data.
In this role you will guide the company into the future and utilize the latest technology and information management methodologies to meet our requirements for effective logical data modeling, metadata management and database warehouse domains. You will be working with experts in a variety of industries, including computer science and software development, as well as department heads and senior executives to integrate new technologies and refine system performance.
We reward dedicated performance with exceptional pay and benefits, as well as tuition reimbursement and career growth opportunities.
What Youll Do
Define data retention policies
Monitor performance and advise any necessary infrastructure changes
Mentor junior engineers and work with other architects to deliver best in class solutions
Implement ETL / ELT process and orchestration of data flows
Recommend and drive adoption of newer tools and techniques from the big data ecosystem
Expertise Youll Bring
10+ years in industry, building and managing big data systems
Building, monitoring, and optimizing reliable and cost-efficient pipelines for SaaS is a must
Building stream-processing systems, using solutions such as Storm or Spark-Streaming
Dealing and integrating with data storage systems like SQL and NoSQL databases, file systems and object storage like s3
Reporting solutions like Pentaho, PowerBI, Looker including customizations
Developing high concurrency, high performance applications that are database-intensive and have interactive, browser-based clients
Working with SaaS based data management products will be an added advantage
Proficiency and expertise in Cloudera / Hortonworks
Spark
HDF and NiFi
RDBMS, NoSQL like Vertica, Redshift, Data Modelling with physical design and SQL performance optimization
Messaging systems, JMS, Active MQ, Rabbit MQ, Kafka
Big Data technology like Hadoop, Spark, NoSQL based data-warehousing solutions
Data warehousing, reporting including customization, Hadoop, Spark, Kafka, Core java, Spring/IOC, Design patterns
Big Data querying tools, such as Pig, Hive, and Impala
Open-source technologies and databases (SQL & NoSQL)
Proficient understanding of distributed computing principles
Ability to solve any ongoing issues with operating the cluster
Scale data pipelines using open-source components and AWS services
Cloud (AWS), provisioning, capacity planning and performance analysis at various levels
Web-based SOA architecture implementation with design pattern experience will be an added advantage","Databases, Big Data, Etl"
Big Data Architect,Trellix,10-15 Years,,Bengaluru,Software,"Role Overview:
The Big Data Architect will be responsible for the design, implementation, and management of the organizations big data infrastructure. The ideal candidate will have a strong technical background in big data technologies, excellent problem-solving skills, and the ability to work in a fast-paced environment. The role requires a deep understanding of data architecture, data modeling, and data integration techniques.
About the Role:
Design and implement scalable and efficient big data architecture solutions to meet business requirements.
Develop and maintain data pipelines, ensuring the availability and quality of data.
Collaborate with data scientists, data engineers, and other stakeholders to understand data needs and provide technical solutions.
Lead the evaluation and selection of big data tools and technologies.
Ensure data security and privacy compliance.
Optimize and tune big data systems for performance and cost-efficiency.
Document data architecture, data flows, and processes.
Stay up to date with the latest industry trends and best practices in big data technologies.
About You:
Bachelors or master's degree in computer science, Information Technology, or a related field.
Overall 10+ years exp with 5+ years of experience in big data architecture and engineering.
Proficiency in big data technologies such as Hadoop MapReduce, Spark batch and streaming, Kafka, HBase, Scala, Elastic Search and others.
Experience with AWS cloud platform.
Strong knowledge of data modeling, ETL processes, and data warehousing.
Proficiency in programming languages such as Java, Scala, Spark
Familiarity with data visualization tools and techniques.
Excellent communication and collaboration skills.
Strong problem-solving abilities and attention to detail.","ETL processes, Java, Scala, Data Warehousing, Big Data, Kafka, Data Modeling"
Data Analytics Architect (Looker),66degrees,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Overview of 66degrees
66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.
At 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.
Overview of Role
We're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.
Responsibilities
Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
Work with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.
Advise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.
Utilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.
Write clean, efficient, and scalable code (LookML, Python as applicable)
Conduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.
Stay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.
Collaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.
Provide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.
Excellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.
Experience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.
Strong proficiency with SQL required.
Demonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.
Excellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.
Solid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.
Strong problem-solving skills and the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Google Cloud certifications and any analytics platform certifications are a plus.
A desire to stay ahead of the curve and continuously learn new technologies and techniques.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, Microstrategy, Looker, LookML, Azure, Python, Sql, Google Cloud, AWS"
Analytics Head / Data Model Architect,Deepak Group Co,10-12 Years,,"Vadodara, India",Login to check your skill match score,"Job Title: Analytics Head / Data Model Architect
Job Summary:
We are seeking a seasoned Analytics Head / Data Model Architect to lead our data strategy, design scalable data models, and drive analytical innovation across the organization. This role combines leadership in data science and business analytics with deep technical expertise in data architecture and modelling. The ideal candidate will be a strategic thinker, technical expert, and effective communicator capable of aligning data initiatives with business objectives.
Key Responsibilities:
Leadership & Strategy
Lead and manage a team of data scientists, analysts, and data architects.
Define and drive the enterprise analytics strategy aligned with business goals.
Collaborate with executive leadership to identify data-driven growth opportunities.
Data Architecture & Modeling
Design and implement robust, scalable, and high-performance data models (OLAP/OLTP, dimensional, relational, NoSQL).
Develop enterprise data architecture standards, policies, and best practices.
Oversee data governance, data quality, and metadata management initiatives.
Advanced Analytics & Insights
Build advanced analytics solutions including predictive modeling, statistical analysis, and machine learning frameworks.
Translate complex data into actionable insights for stakeholders across departments.
Evaluate and implement modern BI tools and platforms (e.g., Power BI, Tableau, Looker).
Collaboration & Integration
Partner with data engineering teams to ensure efficient ETL/ELT pipelines and data lake/warehouse infrastructure.
Work closely with business units to understand needs and deliver customized data solutions.
Support data privacy, security, and compliance initiatives (e.g., GDPR, HIPAA, SOC 2).
Required Qualifications:
Bachelor's or master's in computer science, Data Science, Statistics, or related field. PhD is a plus.
10+ years of experience in analytics, data architecture, or related roles.
Strong knowledge of data modeling techniques (3NF, Star Schema, Snowflake, Data Vault, etc.).
Expertise in SQL, Python, R, and at least one cloud platform (AWS, Azure, GCP).
Experience with modern data warehousing tools (Snowflake, BigQuery, Redshift) and orchestration (Airflow, DBT).
Proven leadership and team-building skills.
Preferred Skills:
Experience with AI/ML model deployment in production.
Familiarity with data mesh, data fabric, or modern data stack concepts.
Knowledge of industry-specific data standards (e.g., HL7 for healthcare, ACORD for insurance).","Airflow, dimensional, Looker, R, relational, dbt, snowflake, BigQuery, Machine Learning, Power Bi, OLAP, Tableau, Redshift, Sql, Nosql, Gcp, Predictive Modeling, Azure, Python, AWS, Oltp, Statistical Analysis"
Data Center Architect - Delivery,Lenovo India,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Resource who is expert in Data Center System Integration (SI) services discovery, assessment, Planning, Design, Setup, Configure, and Migration Services with Data Center products & solutions such as VMware, VCF, Nutanix, Azure Stack HCI, backup, Storage, SDDC, containers, Red hat KVM, Open-shift etc.
Training & Certifications: VMware , Nutanix, Server, Backup, Storage, HCI
Experience Required: 12-15 Years of Experience in Data Center services, Hybrid Cloud deployment on premises experience is mandatory.","Azure Stack HCI, Backup Storage, Data Center System Integration, SDDC, Red Hat KVM, VMware VCF, Nutanix, Containers"
Data Analytics Architect (Looker),66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees
66degrees is a leading consulting and professional services company specializing in developing AI-focused, data-led solutions leveraging the latest advancements in cloud technology. With our unmatched engineering capabilities and vast industry experience, we help the world's leading brands transform their business challenges into opportunities and shape the future of work.
At 66degrees, we believe in embracing the challenge and winning together. These values not only guide us in achieving our goals as a company but also for our people. We are dedicated to creating a significant impact for our employees by fostering a culture that sparks innovation and supports professional and personal growth along the way.
Overview of Role
We're on the hunt for a seasoned Data Analytics Architect who's not afraid to roll up their sleeves and dive into the data trenches with us. As a Data Analytics Architect, you'll be the mastermind behind designing and implementing cutting-edge data solutions that solve real-world business problems for our clients across various industries. You'll be the bridge between the technical and the business worlds, translating complex data insights into actionable strategies that drive tangible results.
Responsibilities
Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
Work with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.
Advise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.
Utilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.
Write clean, efficient, and scalable code (LookML, Python as applicable)
Conduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.
Stay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.
Collaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.
Provide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.
Excellent comprehension of Looker and it's implementation, with additional analytics platform experience a major plus - especially MicroStrategy.
Experience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.
Strong proficiency with SQL required.
Demonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.
Excellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.
Solid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.
Strong problem-solving skills and the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Google Cloud certifications and any analytics platform certifications are a plus.
A desire to stay ahead of the curve and continuously learn new technologies and techniques.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, Data Modeling, Python, Sql, Google Cloud"
Big Data-Cloud Architect,Airtel Digital,8-12 Years,,"Pune, India",Login to check your skill match score,"We are seeking an experienced Big Data Cloud Architect with 812 years in designing and implementing scalable big data solutions on Azure and AWS. The ideal candidate will have deep hands-on expertise in cloud-based big data technologies, a strong background in the software development life cycle (SDLC), and experience in microservices and backend development.
**Key Responsibilities**
Design, build, and maintain scalable big data architectures on Azure and AWS
Select and integrate big data tools and frameworks (e.g., Hadoop, Spark, Kafka, Azure Data Factory, )
Lead data migration from legacy systems to cloud-based solutions
Develop and optimize ETL pipelines and data processing workflows.
Ensure data infrastructure meets performance, scalability, and security requirements.
Collaborate with development teams to implement microservices and backend solutions for big data applications.
Oversee the end-to-end SDLC for big data projects, from planning to deployment.
Mentor junior engineers and contribute to architectural best practices.
Prepare architecture documentation and technical reports.
#ADL","Azure Data Factory, Hadoop, Spark, Kafka, Azure, Etl, AWS"
Big Data Architect,Trellix,10-15 Years,,Bengaluru,Software,"Role Overview:
The Big Data Architect will be responsible for the design, implementation, and management of the organizations big data infrastructure. The ideal candidate will have a strong technical background in big data technologies, excellent problem-solving skills, and the ability to work in a fast-paced environment. The role requires a deep understanding of data architecture, data modeling, and data integration techniques.
About the Role:
Design and implement scalable and efficient big data architecture solutions to meet business requirements.
Develop and maintain data pipelines, ensuring the availability and quality of data.
Collaborate with data scientists, data engineers, and other stakeholders to understand data needs and provide technical solutions.
Lead the evaluation and selection of big data tools and technologies.
Ensure data security and privacy compliance.
Optimize and tune big data systems for performance and cost-efficiency.
Document data architecture, data flows, and processes.
Stay up to date with the latest industry trends and best practices in big data technologies.
About You:
Bachelors or master's degree in computer science, Information Technology, or a related field.
Overall 10+ years exp with 5+ years of experience in big data architecture and engineering.
Proficiency in big data technologies such as Hadoop MapReduce, Spark batch and streaming, Kafka, HBase, Scala, Elastic Search and others.
Experience with AWS cloud platform.
Strong knowledge of data modeling, ETL processes, and data warehousing.
Proficiency in programming languages such as Java, Scala, Spark
Familiarity with data visualization tools and techniques.
Excellent communication and collaboration skills.
Strong problem-solving abilities and attention to detail.","ETL processes, Java, Scala, Data Warehousing, Big Data, Kafka, Data Modeling"
Sr Data Architect,PHOTON,8-13 Years,,Chennai,Information Technology,"Job Summary
Experience Architecting Enterprise Data Platforms for large Enterprises
Evolved point of view and wide ranging experience in Data Governance,. Data Quality, Data Lineage
Very strong, hands-on expertise in Data Supply Chain, Data Pipelines
Strong experience in Google Cloud, Cloud Data Platforms
Experience creating Data Analytics workspaces
Leading and Guiding the team on all aspects of Data Engineering, Data Analytics
Experience working Banking Domain, especially Private Wealth Management strongly preferred.
Tech Skill Sets: Google Cloud Data Platform, NoSQL
AbInitio
Reporting: Looker, Tableau, Cognos","Google Cloud Data, Cloud Data Platforms, Nosql, Abinitio, Tableau"
Azure Data Lake Architect,Expleo Solutions Limited,10-13 Years,,Chennai,Information Technology,"Job Title : Azure Data Lake Architect
Experience:10+ Years
Work Location Candidate should work in Client Location (Egmore - Chennai) (Monday to Friday) (General Shift Timings) (100% Compulsory Work From Office)
Job Description :
Azure Data Lake
Azure Databricks
Azure Data factory
Azure Synapse Analytics
Blob Storage
ETL
SQL
Job Overview:
We are seeking a highly skilled and motivated individual for the role of Azure Data Lake Architect. You will be responsible for the end-to-end management and optimization of our Azure-based data lake ecosystem. This includes overseeing ETL jobs in ADF, Azure Data Lake, Azure Databricks, Azure Synapse Analytics, Blob Storage to ensure a robust and efficient data management infrastructure.
Preferred Qualifications:
Minimum 5 to 10 years of relevant experience
Proven experience in designing, implementing, and managing Azure Data Lake solutions.
Strong expertise in ETL processes using ADF, Azure Databricks, Azure Synapse Analytics, Blob Storage, Proficient in SQL,
Excellent problem-solving and troubleshooting skills.
Ability to work collaboratively in a team environment and communicate effectively with stakeholders.
Desirable to have:
Expertise in Python programming language with Pandas and NumPy
Certifications: Microsoft Certified: Azure Data Engineer Associate, Microsoft Certified: Azure Enterprise Data Analyst Associate","Azure Data Lake, Azure Databricks, Azure Data Factory, Sql, Etl"
Celonis Data Architect,Harman Connected Services Corporation India Private Limited,12-15 Years,INR 2 - 3 LPA,Bengaluru,Software,"A Celonis data architect is responsible for designing and implementing the data architecture required for effective utilization of the Celonis process mining tool within an organization. They work closely with various stakeholders, including data engineers, business analysts, and process owners, to define data requirements, ensure data integrity, and enable seamless data integration.
The role requires a strong understanding of data architecture principles, data modeling techniques, and relational and non-relational databases. Candidate should also be proficient in ETL processes and tools, as well as have experience in data integration and data governance. Additionally, candidate should have a good understanding of business processes, data visualization, and analytics to effectively implement process mining initiatives within the customer organization
Role and Responsibilities
Over all 12-15 years of IT experience with minimum 5+ years experience in Process Mining using Celonis
Strong experience in Data Integration, Data Modeling, Data quality, Data pipeline management, Dashboard design
Experience in delivering at least 1 to 2 large scale end-to-end Celonis process mining implementations as Data Architect
Experience in leading team of data modelers, data engineers
Strong experience in providing multiple solutions and reviewing the implementations in parallel
Expertise in defining the governance, security, roles around Data pools and dashboards in Celonis
Experience in implementing object centric process mining
Major accountabilities:
Collaborating with business stakeholders to understand their data requirements and process mining goals.
Engage with customers C-level their strategic objectives with the Celonis technical strategy
Designing and implementing data models, schemas, and structures that align with the organization's data governance policies and standards.
Identifying and documenting data sources, including databases, systems, and applications, that need to be integrated with Celonis.
Ensuring proper data extraction, transformation, and loading (ETL) processes to populate the necessary data into Celonis.
Supervising data engineers in the development and maintenance of data pipelines and workflows.
Assessing data quality, consistency, and accuracy and taking proactive measures to resolve issues. Implementing data security measures, including access controls and data encryption, to protect sensitive information within Celonis.
Providing technical support and guidance to business analysts and end-users in data preparation, analysis, and reporting using Celonis.
Staying updated with the latest trends and advancements in data management and process mining technologies, including Celonis updates and new features.","Celonis Data Architect, Etl"
Big Data Architect,Persistent,10-12 Years,,Hyderabad,Information Technology,"We are looking for a Data Architect with creativity and results-oriented critical thinking to meet complex challenges and develop new strategies for acquiring, analyzing, modeling and storing data.
In this role you will guide the company into the future and utilize the latest technology and information management methodologies to meet our requirements for effective logical data modeling, metadata management and database warehouse domains. You will be working with experts in a variety of industries, including computer science and software development, as well as department heads and senior executives to integrate new technologies and refine system performance.
We reward dedicated performance with exceptional pay and benefits, as well as tuition reimbursement and career growth opportunities.
What Youll Do
Define data retention policies
Monitor performance and advise any necessary infrastructure changes
Mentor junior engineers and work with other architects to deliver best in class solutions
Implement ETL / ELT process and orchestration of data flows
Recommend and drive adoption of newer tools and techniques from the big data ecosystem
Expertise Youll Bring
10+ years in industry, building and managing big data systems
Building, monitoring, and optimizing reliable and cost-efficient pipelines for SaaS is a must
Building stream-processing systems, using solutions such as Storm or Spark-Streaming
Dealing and integrating with data storage systems like SQL and NoSQL databases, file systems and object storage like s3
Reporting solutions like Pentaho, PowerBI, Looker including customizations
Developing high concurrency, high performance applications that are database-intensive and have interactive, browser-based clients
Working with SaaS based data management products will be an added advantage
Proficiency and expertise in Cloudera / Hortonworks
Spark
HDF and NiFi
RDBMS, NoSQL like Vertica, Redshift, Data Modelling with physical design and SQL performance optimization
Messaging systems, JMS, Active MQ, Rabbit MQ, Kafka
Big Data technology like Hadoop, Spark, NoSQL based data-warehousing solutions
Data warehousing, reporting including customization, Hadoop, Spark, Kafka, Core java, Spring/IOC, Design patterns
Big Data querying tools, such as Pig, Hive, and Impala
Open-source technologies and databases (SQL & NoSQL)
Proficient understanding of distributed computing principles
Ability to solve any ongoing issues with operating the cluster
Scale data pipelines using open-source components and AWS services
Cloud (AWS), provisioning, capacity planning and performance analysis at various levels
Web-based SOA architecture implementation with design pattern experience will be an added advantage","Databases, Big Data, Etl"
Big Data Architect,Persistent,10-13 Years,,Bengaluru,Information Technology,"We are looking for a Data Architect with creativity and results-oriented critical thinking to meet complex challenges and develop new strategies for acquiring, analyzing, modeling and storing data.
In this role you will guide the company into the future and utilize the latest technology and information management methodologies to meet our requirements for effective logical data modeling, metadata management and database warehouse domains. You will be working with experts in a variety of industries, including computer science and software development, as well as department heads and senior executives to integrate new technologies and refine system performance.
We reward dedicated performance with exceptional pay and benefits, as well as tuition reimbursement and career growth opportunities.
What Youll Do
Define data retention policies
Monitor performance and advise any necessary infrastructure changes
Mentor junior engineers and work with other architects to deliver best in class solutions
Implement ETL / ELT process and orchestration of data flows
Recommend and drive adoption of newer tools and techniques from the big data ecosystem
Expertise Youll Bring
10+ years in industry, building and managing big data systems
Building, monitoring, and optimizing reliable and cost-efficient pipelines for SaaS is a must
Building stream-processing systems, using solutions such as Storm or Spark-Streaming
Dealing and integrating with data storage systems like SQL and NoSQL databases, file systems and object storage like s3
Reporting solutions like Pentaho, PowerBI, Looker including customizations
Developing high concurrency, high performance applications that are database-intensive and have interactive, browser-based clients
Working with SaaS based data management products will be an added advantage
Proficiency and expertise in Cloudera / Hortonworks
Spark
HDF and NiFi
RDBMS, NoSQL like Vertica, Redshift, Data Modelling with physical design and SQL performance optimization
Messaging systems, JMS, Active MQ, Rabbit MQ, Kafka
Big Data technology like Hadoop, Spark, NoSQL based data-warehousing solutions
Data warehousing, reporting including customization, Hadoop, Spark, Kafka, Core java, Spring/IOC, Design patterns
Big Data querying tools, such as Pig, Hive, and Impala
Open-source technologies and databases (SQL & NoSQL)
Proficient understanding of distributed computing principles
Ability to solve any ongoing issues with operating the cluster
Scale data pipelines using open-source components and AWS services
Cloud (AWS), provisioning, capacity planning and performance analysis at various levels
Web-based SOA architecture implementation with design pattern experience will be an added advantage","Databases, Big Data, cloud platform, Sql"
Data Engineer - Architect,HR Addons,10-13 Years,,Pune,Information Technology,"Job Description: The Senior Data & AI Architect designs, develops, and implements complex data architectures, machine learning (ML) models, and AI-driven solutions using Azure (preferred), AWS, and GCP. This role requires hands-on expertise in data engineering, ML model development, big data frameworks, and cloud infrastructure. The architect will manage large-scale data processing, optimize ML pipelines, and deploy AI solutions while ensuring security, scalability, and performance.
Key Responsibilities:
Data & AI Architecture Design:
Design scalable, reliable data pipelines using Azure Data Factory, Synapse, and Data Lake.
Architect ML models with cloud integration for business use cases using Azure ML.
Design data lakes/warehouses for batch and real-time processing.
Integrate AI models with real-time data systems (Azure Event Hub, Stream Analytics).
Cloud Architecture:
Build AI solutions using Azure AI services (Cognitive Services, ML, etc.).
Implement strong data governance with Azure Data Catalog, Policy, and Security Center.
Ensure high availability, fault tolerance, and disaster recovery using Azure services.
Performance Optimization:
Optimize distributed data processing with Apache Spark on Azure Databricks.
Improve query performance in Synapse with partitioning, indexing, and caching.
Optimize ML model efficiency and inference times using Azure ML AutoML.
ML Lifecycle Management:
Implement CI/CD pipelines for ML models using Azure DevOps.
Apply MLOps for model monitoring, retraining, and lifecycle management.
Ensure scalable model serving with Azure Kubernetes Service (AKS).
Real-time Data & Integration:
Deploy real-time data pipelines with Azure Event Hub, Stream Analytics, and Kafka.
Design data integration strategies using Azure Data Factory and APIs.
AI Solution Delivery:
Lead AI product development (recommendation engines, predictive analytics).
Drive end-to-end AI solution delivery, integrating models into business workflows.
Candidate Profile:
10+ years of experience in data architecture, data engineering, AI/ML, and cloud computing.
Expertise in Azure (Data Factory, Synapse, ML, AKS); AWS/GCP experience is a plus.
Strong knowledge of big data frameworks (Apache Spark, Databricks), Python, SQL, Terraform, and Kubernetes.
Key Attributes:
Excellent problem-solving skills with a focus on optimizing performance, cost, and scalability.
Experience designing secure, cloud-native AI systems.
Ability to manage complex challenges independently in a fast-paced environment.","AI Architecture Design, Synapse, AI/ML, Google Cloud Platform, Cloud Architecture, Scala, Kafka, Apache, Data Architecture, Spark, Azure, Python, Kubernetes"
Solution Cloud Data Architect,CIGNEX Technologies Private Limited,12-18 Years,,Bengaluru,"Recruiting, Staffing Agency","Responsibilities
Develop and deliver detailed technology solutions through consulting project activities.
Evaluate and recommend emerging cloud data technologies and tools to drive innovation and competitive advantage, with a focus on Azure services such as Azure Data Lake, Azure Synapse Analytics, and Azure Databricks.
Lead the design and implementation of cloud-based data architectures using Azure and Databricks to support the company's strategic initiatives and exploratory projects.
Collaborate with cross-functional teams to understand business requirements, architect data solutions, and drive the development of innovative data platforms and analytics solutions.
Define cloud data architecture standards, best practices, and guidelines to ensure scalability, reliability, and security across the organization.
Design and implement data pipelines, ETL processes, and data integration solutions to ingest, transform, and load structured and unstructured data from multiple sources into Azure data platforms.
Provide technical leadership and mentorship to junior team members, fostering a culture of collaboration, continuous learning, and innovation in cloud data technologies.
Collaborate with Azure and Databricks experts within the organization and the broader community to stay abreast of the latest developments and best practices in cloud data architecture and analytics.","snowflake, Solutioning Cloud, Azure Data, Azure Data Bricks"
Azure Data Engineer/Architect,Quinnox Consultancy Services Limited,12-18 Years,,"Bengaluru, Mumbai",Software,"We are seeking a highly skilled and experiencedAzure Data Engineerto join our team. The ideal candidate will have a strong background in cloud-based data integration, data transformation, and analytics solutions, with a focus on Azure services. This role involves designing, implementing, and maintaining robust data pipelines and analytics frameworks to support business intelligence and advanced analytics initiatives.
Key Responsibilities:
Data Engineering and Development:
Design and implement scalable and efficient ETL/ELT processes usingAzure Data FactoryandAzure Databricks (PySpark).
Manage and optimize data storage and access inAzure Data Lake Storage.
Data Architecture:
Develop and maintain aLakehouse architecturefor data analytics and reporting.
Ensure best practices in data organization, partitioning, and access control in Azure Data Lake.
Collaborate on the design and implementation ofMicrosoft Fabric architectureto enhance data accessibility and governance.
Data Analysis and Reporting:
Create and optimize reports and dashboards usingPower BIto enable data-driven decision-making.
Develop T-SQL scripts and stored procedures for data transformation and analysis.
Cloud Infrastructure and DevOps:
Leverage knowledge ofAzure IaaS servicesfor data platform setup and maintenance.
Implement and manage CI/CD pipelines usingAzure DevOpsto automate data pipeline deployments.
Technical Expertise and Collaboration:
Provide strong technical guidance inSpark architectureand ensure best practices for PySpark code development.
Collaborate with cross-functional teams to align on data strategies and solutions.
Troubleshoot and resolve complex data engineering challenges efficiently.
Mandatory Skills:
Proficiency inAzure Data Factory,Azure Data Lake Storage, andAzure Databricks (PySpark).
Strong understanding ofETL and ELTprinciples.
Deep knowledge ofLakehouse architectureand its implementation.
Expertise inPySparkandSpark architecture.
Solid understanding ofAzure Data Lake architectureand access control mechanisms
Strong command ofT-SQLfor advanced querying and data manipulation..
Good to Have:
Experience withPower BIfor visualization and reporting.
Familiarity withMicrosoft Fabric architecture.
Knowledge ofAzure IaaS services.
Understanding ofCI/CD processesand tools likeAzure DevOps.
Qualifications:
Bachelors degree in Computer Science, Information Technology, or a related field.
5+ years of experience in data engineering with a focus on Azure services.
Proven expertise in designing and implementing data pipelines and architectures in cloud environments.
Strong problem-solving skills and ability to work collaboratively in a team.
Preferred Certifications:
Microsoft Certified: Azure Data Engineer Associate.
Microsoft Certified: Azure Solutions Architect Expert.
Microsoft Certified: Power BI Data Analyst Associate","Architect, Power Bi, Azure"
Sr Data Architect,PHOTON,8-13 Years,,Chennai,Information Technology,"Job Summary
Experience Architecting Enterprise Data Platforms for large Enterprises
Evolved point of view and wide ranging experience in Data Governance,. Data Quality, Data Lineage
Very strong, hands-on expertise in Data Supply Chain, Data Pipelines
Strong experience in Google Cloud, Cloud Data Platforms
Experience creating Data Analytics workspaces
Leading and Guiding the team on all aspects of Data Engineering, Data Analytics
Experience working Banking Domain, especially Private Wealth Management strongly preferred.
Tech Skill Sets: Google Cloud Data Platform, NoSQL
AbInitio
Reporting: Looker, Tableau, Cognos","Google Cloud Data, Cloud Data Platforms, Nosql, Abinitio, Tableau"
Azure Data Lake Architect,Expleo Solutions Limited,10-13 Years,,Chennai,Information Technology,"Job Title : Azure Data Lake Architect
Experience:10+ Years
Work Location Candidate should work in Client Location (Egmore - Chennai) (Monday to Friday) (General Shift Timings) (100% Compulsory Work From Office)
Job Description :
Azure Data Lake
Azure Databricks
Azure Data factory
Azure Synapse Analytics
Blob Storage
ETL
SQL
Job Overview:
We are seeking a highly skilled and motivated individual for the role of Azure Data Lake Architect. You will be responsible for the end-to-end management and optimization of our Azure-based data lake ecosystem. This includes overseeing ETL jobs in ADF, Azure Data Lake, Azure Databricks, Azure Synapse Analytics, Blob Storage to ensure a robust and efficient data management infrastructure.
Preferred Qualifications:
Minimum 5 to 10 years of relevant experience
Proven experience in designing, implementing, and managing Azure Data Lake solutions.
Strong expertise in ETL processes using ADF, Azure Databricks, Azure Synapse Analytics, Blob Storage, Proficient in SQL,
Excellent problem-solving and troubleshooting skills.
Ability to work collaboratively in a team environment and communicate effectively with stakeholders.
Desirable to have:
Expertise in Python programming language with Pandas and NumPy
Certifications: Microsoft Certified: Azure Data Engineer Associate, Microsoft Certified: Azure Enterprise Data Analyst Associate","Azure Data Lake, Azure Databricks, Azure Data Factory, Sql, Etl"
Data Engineer - Architect,HR Addons,10-13 Years,,Pune,Information Technology,"Job Description: The Senior Data & AI Architect designs, develops, and implements complex data architectures, machine learning (ML) models, and AI-driven solutions using Azure (preferred), AWS, and GCP. This role requires hands-on expertise in data engineering, ML model development, big data frameworks, and cloud infrastructure. The architect will manage large-scale data processing, optimize ML pipelines, and deploy AI solutions while ensuring security, scalability, and performance.
Key Responsibilities:
Data & AI Architecture Design:
Design scalable, reliable data pipelines using Azure Data Factory, Synapse, and Data Lake.
Architect ML models with cloud integration for business use cases using Azure ML.
Design data lakes/warehouses for batch and real-time processing.
Integrate AI models with real-time data systems (Azure Event Hub, Stream Analytics).
Cloud Architecture:
Build AI solutions using Azure AI services (Cognitive Services, ML, etc.).
Implement strong data governance with Azure Data Catalog, Policy, and Security Center.
Ensure high availability, fault tolerance, and disaster recovery using Azure services.
Performance Optimization:
Optimize distributed data processing with Apache Spark on Azure Databricks.
Improve query performance in Synapse with partitioning, indexing, and caching.
Optimize ML model efficiency and inference times using Azure ML AutoML.
ML Lifecycle Management:
Implement CI/CD pipelines for ML models using Azure DevOps.
Apply MLOps for model monitoring, retraining, and lifecycle management.
Ensure scalable model serving with Azure Kubernetes Service (AKS).
Real-time Data & Integration:
Deploy real-time data pipelines with Azure Event Hub, Stream Analytics, and Kafka.
Design data integration strategies using Azure Data Factory and APIs.
AI Solution Delivery:
Lead AI product development (recommendation engines, predictive analytics).
Drive end-to-end AI solution delivery, integrating models into business workflows.
Candidate Profile:
10+ years of experience in data architecture, data engineering, AI/ML, and cloud computing.
Expertise in Azure (Data Factory, Synapse, ML, AKS); AWS/GCP experience is a plus.
Strong knowledge of big data frameworks (Apache Spark, Databricks), Python, SQL, Terraform, and Kubernetes.
Key Attributes:
Excellent problem-solving skills with a focus on optimizing performance, cost, and scalability.
Experience designing secure, cloud-native AI systems.
Ability to manage complex challenges independently in a fast-paced environment.","AI Architecture Design, Synapse, AI/ML, Google Cloud Platform, Cloud Architecture, Scala, Kafka, Apache, Data Architecture, Spark, Azure, Python, Kubernetes"
Solution Cloud Data Architect,CIGNEX Technologies Private Limited,12-18 Years,,Bengaluru,"Recruiting, Staffing Agency","Responsibilities
Develop and deliver detailed technology solutions through consulting project activities.
Evaluate and recommend emerging cloud data technologies and tools to drive innovation and competitive advantage, with a focus on Azure services such as Azure Data Lake, Azure Synapse Analytics, and Azure Databricks.
Lead the design and implementation of cloud-based data architectures using Azure and Databricks to support the company's strategic initiatives and exploratory projects.
Collaborate with cross-functional teams to understand business requirements, architect data solutions, and drive the development of innovative data platforms and analytics solutions.
Define cloud data architecture standards, best practices, and guidelines to ensure scalability, reliability, and security across the organization.
Design and implement data pipelines, ETL processes, and data integration solutions to ingest, transform, and load structured and unstructured data from multiple sources into Azure data platforms.
Provide technical leadership and mentorship to junior team members, fostering a culture of collaboration, continuous learning, and innovation in cloud data technologies.
Collaborate with Azure and Databricks experts within the organization and the broader community to stay abreast of the latest developments and best practices in cloud data architecture and analytics.","snowflake, Solutioning Cloud, Azure Data, Azure Data Bricks"
Azure Data Engineer/Architect,Quinnox Consultancy Services Limited,12-18 Years,,"Bengaluru, Mumbai",Software,"We are seeking a highly skilled and experiencedAzure Data Engineerto join our team. The ideal candidate will have a strong background in cloud-based data integration, data transformation, and analytics solutions, with a focus on Azure services. This role involves designing, implementing, and maintaining robust data pipelines and analytics frameworks to support business intelligence and advanced analytics initiatives.
Key Responsibilities:
Data Engineering and Development:
Design and implement scalable and efficient ETL/ELT processes usingAzure Data FactoryandAzure Databricks (PySpark).
Manage and optimize data storage and access inAzure Data Lake Storage.
Data Architecture:
Develop and maintain aLakehouse architecturefor data analytics and reporting.
Ensure best practices in data organization, partitioning, and access control in Azure Data Lake.
Collaborate on the design and implementation ofMicrosoft Fabric architectureto enhance data accessibility and governance.
Data Analysis and Reporting:
Create and optimize reports and dashboards usingPower BIto enable data-driven decision-making.
Develop T-SQL scripts and stored procedures for data transformation and analysis.
Cloud Infrastructure and DevOps:
Leverage knowledge ofAzure IaaS servicesfor data platform setup and maintenance.
Implement and manage CI/CD pipelines usingAzure DevOpsto automate data pipeline deployments.
Technical Expertise and Collaboration:
Provide strong technical guidance inSpark architectureand ensure best practices for PySpark code development.
Collaborate with cross-functional teams to align on data strategies and solutions.
Troubleshoot and resolve complex data engineering challenges efficiently.
Mandatory Skills:
Proficiency inAzure Data Factory,Azure Data Lake Storage, andAzure Databricks (PySpark).
Strong understanding ofETL and ELTprinciples.
Deep knowledge ofLakehouse architectureand its implementation.
Expertise inPySparkandSpark architecture.
Solid understanding ofAzure Data Lake architectureand access control mechanisms
Strong command ofT-SQLfor advanced querying and data manipulation..
Good to Have:
Experience withPower BIfor visualization and reporting.
Familiarity withMicrosoft Fabric architecture.
Knowledge ofAzure IaaS services.
Understanding ofCI/CD processesand tools likeAzure DevOps.
Qualifications:
Bachelors degree in Computer Science, Information Technology, or a related field.
5+ years of experience in data engineering with a focus on Azure services.
Proven expertise in designing and implementing data pipelines and architectures in cloud environments.
Strong problem-solving skills and ability to work collaboratively in a team.
Preferred Certifications:
Microsoft Certified: Azure Data Engineer Associate.
Microsoft Certified: Azure Solutions Architect Expert.
Microsoft Certified: Power BI Data Analyst Associate","Architect, Power Bi, Azure"
Azure Data Engineer/Architect,Quinnox Consultancy Services Limited,12-18 Years,,"Bengaluru, Mumbai",Information Technology,"Description
We are looking for an experienced Azure Data Engineer/Architect to join our team. The ideal candidate should have 12-18 years of experience in the job market context of India and should be well-versed in Azure cloud technologies. The candidate should have strong technical skills, a problem-solving mindset, and an ability to work independently and as part of a team.
Responsibilities
Design and build scalable data pipelines on Azure cloud platform
Create and maintain data models, data flows, and data integration processes
Develop and maintain ETL processes
Design and implement data security and compliance policies
Implement and maintain data quality standards
Collaborate with cross-functional teams to identify and solve complex data-related problems
Provide technical guidance to junior team members
Stay up-to-date with emerging data technologies and trends
Skills and Qualifications
12-18 years of experience in data engineering and architecture
Expertise in Azure cloud platform, including Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and Azure SQL Database
Strong programming skills in Python and/or Scala
Experience in data modeling, data warehousing, and data integration
Experience in ETL development and maintenance
Knowledge of data security and compliance policies
Experience in implementing data quality standards
Excellent problem-solving and analytical skills
Ability to work independently and as part of a team
Excellent communication and interpersonal skills
Bachelor's or Master's degree in Computer Science or a related field","Nosql, Data Migration, Data Security, Data Integration, Data Modeling, Data Governance, Data Warehousing, Azure, Sql, Etl"
Data Solution Architect (Data Engineering),Amgen Inc,7-11 Years,,Hyderabad,Pharmaceutical,"Design and implement scalable, modular, and future-proof data architectures that support enterprise data lakes, data warehouses, and real-time analytics.
Develop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.
Define data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.
Lead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.
Optimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.
Establish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.
Drive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.
Implement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.
Lead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.
Collaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.
Act as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.
Must-Have Skills:
Experience in data architecture, enterprise data management, and cloud-based analytics solutions.
Expertise in Databricks, cloud-native data platforms, and distributed computing frameworks.
Strong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.
Experience designing high-performance ETL/ELT pipelines and real-time data processing solutions.
Deep understanding of data governance, security, metadata management, and access control frameworks.
Hands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaaC).
Proven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.
Strong problem-solving, strategic thinking, and technical leadership skills.
Experienced with SQL/NOSQL database, vector database for large language models
Experienced with data modeling and performance tuning for both OLAP and OLTP databases
Experienced with Apache Spark
Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops
Good-to-Have Skills:
Good to have deep expertise in Biotech & Pharma industries
Experience with Data Mesh architectures and federated data governance models.
Certification in cloud data platforms or enterprise architecture frameworks.
Knowledge of AI/ML pipeline integration within enterprise data architectures.
Familiarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.
Education and Professional Certifications
Doctorate Degree with 6-8 + years of experience in Computer Science, IT or related field
Master's degree with 8-10 + years of experience in Computer Science, IT or related field
Bachelor's degree with 10-12 + years of experience in Computer Science, IT or related field
AWS Certified Data Engineer preferred
Databricks Certificate preferred
Soft Skills:
Excellent analytical and troubleshooting skills.
Strong verbal and written communication skills
Ability to work effectively with global, virtual teams
High degree of initiative and self-motivation.
Ability to manage multiple priorities successfully.
Team-oriented, with a focus on achieving team goals.
Ability to learn quickly, be organized and detail oriented.
Strong presentation and public speaking skills.","CI/CD, Data Architecture, Apache Spark, Databricks, Data Governance, Etl"
Data Modeller/Architect,Coforge,10-16 Years,,"Hyderabad, Noida, Pune",Information Technology,"Design and develop data models, architecture, and metadata for large-scale enterprise systems using Erwin tools.
Collaborate with cross-functional teams to identify business requirements and translate them into technical designs.
Develop complex data models using Snowflake as the primary database management system.
Ensure compliance with industry standards, best practices, and company policies for data governance.
Job Requirements :
10-16 years of experience in Data Modeling, Architecture, or related field.
Strong expertise in Erwin tools (e.g., Erwin Modeler) for designing and developing data models.
Proficiency in Snowflake as a primary database management system.
Experience working on large-scale enterprise projects involving multiple stakeholders.","enterprise systems, Architecture, snowflake, Erwin, Data Modeling, Data Governance"
AWS Dig Data Solution Architect,Birlasoft Limited,15-20 Years,,"Bengaluru, Noida",Software,"As an Amazon Web Services (AWS) certified solutions architect you would work as cloud network specialist who designs, creates, and maintains their employer's AWS-based services. Companies can use Amazon Web Services to run many business systems, such as analytics, security, storage, or database services.
Title: AWS Dig Data Solution Architect
Location: Pune/Mumbai/Bangalore/Noida
Educational Background : Masters/Professional Degree
Key Responsibilities :
Design and architect end to end solutions on AWS and create the HLD and LLD documents
Experience in real-time Data Ingestion and Processing
Hands-on experience in AWS S3, Kafka, Glue, PySpark, Python, Redshift, AWS Cloud best practices and optimization
Experience with integration of different data sources with Data Lake is required
Experience in creating data lakes for Reporting, AI and Machine Learning
Experience of data modelling and data architecture concepts
Good in Creating Technical Specifications and Data Flow document
To be able to clearly articulate pros and cons of various technologies and platforms
Experience in create the Technical Specification Design.
Skills Required:
15 years of IT experience with 4+ years of AWS Big Data Services Architecture experience
Client facing experience.
In-depth knowledge of domain Industry and business environment
Analytical and problem-solving capabilities","AWS Dig Data Solution Architect, Pyspark, AWS"
AWS Dig Data Solution Architect,Birlasoft Limited,15-20 Years,,"Mumbai, Pune",Software,"As an Amazon Web Services (AWS) certified solutions architect you would work as cloud network specialist who designs, creates, and maintains their employer's AWS-based services. Companies can use Amazon Web Services to run many business systems, such as analytics, security, storage, or database services.
Title: AWS Dig Data Solution Architect
Location: Pune/Mumbai/Bangalore/Noida
Educational Background : Masters/Professional Degree
Key Responsibilities :
Design and architect end to end solutions on AWS and create the HLD and LLD documents
Experience in real-time Data Ingestion and Processing
Hands-on experience in AWS S3, Kafka, Glue, PySpark, Python, Redshift, AWS Cloud best practices and optimization
Experience with integration of different data sources with Data Lake is required
Experience in creating data lakes for Reporting, AI and Machine Learning
Experience of data modelling and data architecture concepts
Good in Creating Technical Specifications and Data Flow document
To be able to clearly articulate pros and cons of various technologies and platforms
Experience in create the Technical Specification Design.
Skills Required:
15 years of IT experience with 4+ years of AWS Big Data Services Architecture experience
Client facing experience.
In-depth knowledge of domain Industry and business environment
Analytical and problem-solving capabilities","AWS Dig Data Solution Architect, Pyspark, AWS"
Sr Data Migration Architect (Windchill),Birlasoft Limited,3-5 Years,,Bengaluru,Software Engineering,"Educational Background
UG. - B. Tech /B. E in any specialization
PG. - MCA/MSC/MTech in Computers
Key Responsibilities -
Hands-on experience as Data Migration Architect with PLM Enterprise Systems, mainly Windchill and Thing Worx.
Has strong experience as a Solution Architect as well for Windchill and Thing Worx applications.
Strong Experience in UDI and other Medical Devices aspects.
Strong Experience in Medical Devices Industry with deep insights into regulatory, change control, compliances, and other nuggets.
Has done windchill migrations on cloud as a target system.
While being hands-on for Migration on Windchill PLM, the person also can help with data migration strategies; also show and tell the customer on best practices and strategies.
Strong hands-on with data migration and has handled large business transformation programs.
Has worked directly with onsite and offshore teams from execution standpoint.
Should be expert in Windchill Migration using Windchill Bulk Migrator (WBM) - at least have executed 3-4 Windchill migration project using WBM.
Should be expert in WBM tool execution (Extraction, Transformation & Loading).
Skills Required -
Experience in data migration including CAD Data migration.
Experience in at least one non-Windchill to Windchill data migration.
Should have good understanding of Windchill Architecture, database etc.
Should have good understanding of Windchill object models, relationships, content.
Should have experience on working with Customer for Migration Requirements Gathering, Source Data Analysis and Data Mapping.
Scripting Knowledge on Database - Oracle/SQL Server with large data set analysis.","Data Migration Architect, Windchill, Windchill PLM"
GCP Data architect,EAGateway Services India Private Limited,10-19 Years,,Remote,Information Technology,"Dear Candidate,
We identified your profile as a Job Seeker.
We found your profile suitable for one of Permanent Position.
If you are interested and the JD suits your profile, Please revert with the details requested below along with your latest CV and we will process your candidature.
Please Share below details:-
LinkedIn ID:
Total Experience:
Relevant Hands-on Experience in:
Current Salary (Fixed + Variables):
Expected Salary:
Current Company:
Payroll Company:
Are you an Immediate joiner / Serving any Notice Period:
Official Notice Period:
Last working day, if you have already resigned:
Reason for Relieving:
Current Location:
Preferred Location :
Is there any OFFER in your Hand:
Could you please share your relieving letter or Resignation acceptance screenshot for Confirmation.
JOB DESCRIPTION:
Role: Data Architect (GCP)
Total Exp: 10+ yrs.
Location: Remote
Job Type: Permanent with Lingaro
Notice Period: Immediate- 30 Days
About Lingaro:
Lingaro Group is the end-to-end data services partner to global brands and enterprises. We lead our clients through their data journey, from strategy through development to operations and adoption, helping them to realize the full value of their data.
Since 2008, Lingaro has been recognized by clients and global research and advisory firms for innovation, technology excellence, and the consistent delivery of highest-quality data services. Our commitment to data excellence has created an environment that attracts the brightest global data talent to our team.
About DS/AI Competency Center:
Focuses on leveraging data, analytics, and artificial intelligence (AI) technologies to extract insights, build predictive models, and develop AI powered solutions. Utilizes Exploratory Data Analysis, Statistical Modeling and Machine Learning, Model Deployment, and Integration as well as Model Monitoring and Maintenance. Delivers business solutions using multiple AI techniques and tools.
Website:
https://lingarogroup.com/
Mandatory :
GCP cloud platform (at least 6+ years), Data modelling (5+ years)
Experience in Dataflow, Cloud composer, Pub/Sub, Cloud storage, Big query
Requirements:
Bachelor's or master's degree in computer science, Information Systems, or a related field.
10+ years experience as a Data Architect or in a similar role, ideally in a complex organizational setting.
Strong understanding of data management principles, data modeling techniques, database design and data integration flows.
Experience in developing and implementing data governance policies, procedures, and frameworks to ensure data integrity and compliance.
Familiarity with industry best practices and emerging trends in data management and governance.
Ability to design and develop conceptual, logical, and physical data models that meet business requirements and align with the overall data strategy.
Strong understanding of data modeling best-practices, e.g. data normalization, denormalization, generalization, and performance optimization techniques.
Expertise in working with various database technologies, such as relational databases (e.g., Oracle, SQL Server, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB).
Knowledge of database management systems, data warehousing, data integration technologies, and Big Data solutions.
Familiarity with cloud-based database, warehouse, and lakehouse platforms.
Experience in designing and implementing data integration solutions, including Extract, Transform, Load (ETL) processes and data pipelines, understanding of data integration patterns and best practices.
Understanding of Business Intelligence and data analytics requirements, optimization of data storage and processing for reporting needs.
Excellent communication and presentation skills to effectively articulate complex technical concepts to both technical and non-technical stakeholders.
Ability to collaborate with cross-functional teams, including business analysts, developers, and data scientists, to understand requirements and drive data-related initiatives.
Strong analytical and problem-solving abilities to identify data-related issues, propose solutions, and make data-driven decisions.
Familiarity with data profiling, data cleansing, data harmonization, and data quality assessment techniques.
Knowledge of data security and privacy regulations, such as GDPR or CCPA, understanding of data encryption, access controls, and data masking techniques to ensure the security of sensitive data.
Professional certification in data management or related field would be advantageous.
Thanks & Regards
Nikhil Dasyam
EAGateway Services India Pvt Ltd
Cell: +91 6305027687
email: nikhil@eagateways.net |Web: http://www.eagateways.net","Pub/Sub, Cloud composer, BigQuery, Gcp, Data Modeling, Cloud Storage, Data Architect, DataFlow"
Azure Data Architect,Nityo Infotech Services Pte. Ltd,5-10 Years,,India,IT/Computers - Software,"Job Description
Azure Data Architect: 1. Minimum of 5 years of experience in designing and implementing data solutions using Microsoft Azure. 2. Experience in data modeling, integration, and performance optimization. 3. Familiarity with Azure security features and cloud migration. 4. Thorough understanding and working knowledge of Azure data services, such as Azure Data Lake Storage, Azure SQL Database, and Azure Synapse Analytics, Azure Data Factory (ADF). 5. Security features of Azure data services and ability to design and implement security solutions to ensure that data is protected from unauthorized access. 6. Must be able to migrate on-premises data to the cloud and capable of planning, designing, and implementing the migration process while minimizing downtime.
Experience Required
5-10 years
Industry Type
IT
Employment Type
Permanent","Azure Data Lake Storage, Azure security features, performance optimization, Azure SQL Database, Cloud Migration, Azure Synapse Analytics, Integration, Microsoft Azure, Security Solutions, Data Modeling"
Enterprise Data Architect - Ramboll Tech,Ramboll,5-7 Years,,"Chennai, India",Login to check your skill match score,"Company Description
About Ramboll
Founded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.
Equality, diversity, and inclusion are at the heart of what we do
We believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.
Job Description
Were passionate about using data to drive sustainability. You too Join us as an Enterprise Data Architect at Ramboll Tech, and shape our architecture to be data centric.
We are looking for a highly skilled and experienced Data Architect to join our team. You will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth. This role is vital to ensuring the seamless integration, management and deployment of data and analytics solutions that support our business needs. You will identify, analyse and proactively recommend how information assets drive business outcomes, to share consistent data throughout Ramboll.
You will be a part of our exciting digital transformation journey and play an active role in turning our data ambitions into concrete actions to deliver optimisations and revenue growth targets!
What You Will Do
You will join Technology & Data Architecture in Ramboll Tech. You will be working across Ramboll, with focus on enterprise data layer, in collaboration with Domain Enterprise Architects, Data Strategy and Data Platform teams. You will partner with Innovation and Digital Transformation Directorswho drive digitalisation, innovation, and scaling of digital solutions & products across their respective business domains.
As our Enterprise Data Architect, you will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth.
Your focus will be on delivering value by shaping data strategies, roadmaps, and solutions that directly address the challenges and opportunities of our business areas. This role requires expertise in modern data management and analysis technologies, alongside a deep understanding of corporate data management best practices.
As we operate in Architecture, Engineering and Consulting (AEC), you can expect a considerable focus on Building Information Modelling (BIM). We are looking for a candidate eager to engage in applying data-centric principles to BIM data to leverage generating valuable business insights.
Where you will make an impact:
Business needs: Translate complex business requirements into robust, scalable, and secure data architectures that are aligned with enterprise-wide data strategy. We want to unlock new revenue streams and optimise operational efficiency, by leveraging our Enterprise Data Architecture to transform our data into valuable business assets, and implement Al-enabled solutions.
Design and implement modern data architectures: Lead the design and implementation of scalable, robust and secure data systems using modern data stack technologies. Ensure architecture is aligned with business objectives.
Improve and ensure the overall quality of data, including ease of access and use of data assets. Define information and data strategies as a part of an overall business and technology strategy. Design and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.
Support data platform teams: Work with data platform teams to design and implement operating models that support efficient data processing, integrations, reporting and AI model deployment and execution.
Data modelling: Design and develop data models that support business processes, analytics and reporting requirements. Ensure that data models are optimised for performance and scalability.
Technology roadmaps: Actively engage with technology providers to understand critical, data relevant, technology roadmaps. Explore innovative solutions and evaluate their alignment with business goals and enterprise-wide data strategy
Key responsibilities:
Work with solution architects, developers and engineers to operationalise Data Strategy and build robust and scalable technology stacks and platforms for D&A solutions
Partner with our Domain Enterprise Architects to understand business objectives. Develop and manage cloud-based data solutions on platforms such as Microsoft Azure, GCP or AWS. Optimise cloud infrastructure for performance, cost and scalability.
Design and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.
Work closely with cross-functional teams, including business stakeholders, data scientists, data engineer and other SME colleagues in Ramboll Tech, to understand data requirements and deliver solutions that meet business needs.
Maintaining repositories for representing the data elements including the entities, relationships and attributes, the information flows, and business glossary
Provide mentorship and guidance to junior data engineers and other team members. Foster a culture of continuous learning and improvement within the team
Continuously evaluate and recommend new tools and technologies that can improve the efficiency and effectiveness of data engineering processing
Coach and mentor other architects, product teams and business stakeholders to instil architectural thinking, with focus on business, data/information and solution architecture.
Effectively provide guiding principles, standard and minimal viable architectures, technology governance model informed by business strategy and corporate governance.
Qualifications
Education:
Bachelors or Masters degree in Computer Science, Engineering, or a related field.
Experience:
5+ years of professional experience in data architecture, with a strong focus on cloud architecture, data integration and data modelling.
Deep understanding of the modern data stack and its components.
Proven experience with cloud platforms such as Microsoft Azure, GCP or AWS.
Previous engagement in establishing data strategy and data governance.
Experience in leading data modelling, database design, data warehousing, master data management, data governance practices. Strong skills in data modelling, ETL processes and data integration, having worked with data platform teams.
Experience in securing and protection of sensitive information
Experience in AEC industry is great to have. An ideal candidate will have a strong understanding of Building Information Modelling (BIM) principles and applications. Applied data-centric principles to BIM data, enabling integration with enterprise systems (e.g., improved project management, cost control, and facilities management). Developed data pipelines to extract, transform, and load BIM data for analysis and reporting. Explored opportunities to leverage BIM data for generating valuable insights, predictive and prescriptive maintenance
Skills:
Exceptional analytical and problem-solving skills with the ability to design innovative solutions to complex data challenges.
Data Modelling (Conceptual, Logical, Physical), Data Warehousing, Data Lakes, Data Mesh, Data Governance, Metadata Management, Master Data Management.
Familiarity with the DAMA Data Management Framework (DAMA-DMBOK) and its application in managing data as a strategic asset
Understanding of BIM principles, data structures (IFC), BIM software APIs, and BIM data integration with enterprise systems. (Great to have skill)
Advanced knowledge of data modelling tools, data lakes, SQL, and pipeline design
Excellent communication and interpersonal skills, with the ability to convey technical concepts to non-technical stakeholders
A strategic mindset with the ability to navigate and influence within a matrixed organization
Proven ability to lead projects and mentor junior team members
Relevant certifications in cloud technologies are a plus, such as: Microsoft Certified Azure Solutions Architect Expert, or Data Engineering certifications, Snowflake certifications, Databricks certifications, Kafka certifications
We encourage you to apply even if your profile does not meet all the requirements for the role. We are looking for a diverse range of experiences, skills, and interests to enrich our team.
Additional Information
What defines us: CURIOSITY, OPTIMISM, AMBITION, EMPATHY
Our team at Ramboll Tech is currently on a steep growth trajectory while maintaining a strong team culture.
We are curious about other people and their motivations; about new business models and technologies; about each other and the future.
We are optimistic, focusing on solutions rather than problems; we plan for success and are willing to take calculated risks instead of playing it safe.
We are ambitious, setting our own standards higher than others expectations, and we celebrate each other&aposs successes.
We are empathetic, taking ourselves, others, and each other seriously without prejudgment, and we help each other and our clients, colleagues, and the world.
How We Work As a Team
Our team culture is crucial to us; that&aposs why we take time daily to exchange ideas and discuss our work priorities. We support each other when facing challenges and foster a strong team spirit. We aim to learn and grow continuously from one another. We value diversity, and although we&aposre not perfect, we regularly engage in open discussions about how we can improve in this area.
Our current hybrid work approach focuses on adapting to different needs, including increased flexibility that works best for our global team and individuals, with as much autonomy as possible.
Who Is Ramboll
Ramboll is a global architecture, engineering, and consultancy firm. We believe sustainable change&aposs aim is to create a livable world where people thrive in healthy nature. Our strength is our employees, and our history is rooted in a clear vision of how a responsible company should act. Openness and curiosity are cornerstones of our corporate culture, fostering an inclusive mindset that seeks new, diverse, and innovative perspectives. We respect and welcome all forms of diversity and focus on creating an inclusive environment where everyone can thrive and reach their full potential.
What Does Ramboll Tech Do
Ramboll Tech / GBA Innovation and Digital Transformation accelerates innovation and digital transformation for the entire Ramboll Group/ GBA and directs all AI initiatives within the company. We digitally enable and co-create with Rambolls world-class experts to deliver solutions that drive sustainable change for our clients and society. This includes collaborating with Global Business Areas and Enabling Functions at Ramboll on their digital journey, developing proprietary AI and digital products for Ramboll and our clients, following our product life cycle. Ramboll Tech also works on larger technology projects within Ramboll and provides world-class Technology Operations. Ramboll Tech currently has over 300 employees globally, with strongholds across Nordics, Germany, the USA, and India. We are looking to quickly expand in key areas across Europe and the globe.
Important Information
We don&apost require a cover letterjust send us your current CV through our application tool, and we&aposre eager to get to know you better in a conversation.
Do you have any questions Feel free to contact Head of Technology & Data Architecture: Elvira Janas ([HIDDEN TEXT]). Thanks
Show more Show less","Data Lakes, ETL processes, Master Data Management, Data Modelling, Data Integration, Data Warehousing, Data Architecture, Data Governance"
Cloud Data Architect,owow,8-16 Years,,"Bengaluru, India",Login to check your skill match score,"Technical Skills:
Skillsets we are looking for:
8 to 16 years of working experience in data engineering.
7+ years exp in PySprak
7+ years exp in AWS Glue
7+ years exp in AWS Redshift
7+ years exp in in AWS CI/CD pipeline like codebuild, codecommit, codedeploy and codepipeline
Strong proficiency in AWS services such as S3, EC2, EMR, SNS, Lambda, StepFunctions
Experience implementing automated testing platforms like PyTest
Strong proficiency in Python, Hadoop, Spark and or PySpark is required
Skill of writing clean, readable, commented and easily maintainable code
Understanding of fundamental design principles for building a scalable solution
Skill for writing reusable libraries
Proficiency in understanding code versioning tools such as Git, SVN, TFS etc.,
Interpersonal skills:
Excellent communication and collaboration skills.
Ability as part of a team.
Strong problem-solving and analytical skills.
Must have worked with US customers and should have provided at least 3-4 hours overlap with Pacific Time (PT)
Bonus points:
Certifications in cloud platforms
Show more Show less","Hadoop, Aws Redshift, Pyspark, Spark, AWS Glue, Python"
Senior Azure Data Architect,MonoSpear Technologies LLC,7-9 Years,,"Hyderabad, India",Login to check your skill match score,"Experience: 7+ Years
Work Mode: Remote
Work Type: Contract
Shift Timings: 2 Pm to 11 Pm
Availability: Immediate
Freelancers may also apply.
Key Responsibilities:
Data Architecture Design:
Design, develop, and maintain the enterprise data architecture, including data models, database schemas, and data flow diagrams.
Develop a data strategy and roadmap that aligns with business objectives and ensures the scalability of data systems.
Architect both transactional (OLTP) and analytical (OLAP) databases, ensuring optimal performance and data consistency.
Data Integration & Management:
Oversee the integration of disparate data sources into a unified data platform, leveraging ETL/ELT processes and data integration tools.
Design and implement data warehousing solutions, data lakes, and/or data marts that enable efficient storage and retrieval of large datasets.
Ensure proper data governance, including the definition of data ownership, security, and privacy controls in accordance with compliance standards (GDPR, HIPAA, etc.).
Collaboration with Stakeholders:
Work closely with business stakeholders, including analysts, developers, and executives, to understand data requirements and ensure that the architecture supports analytics and reporting needs.
Collaborate with DevOps and engineering teams to optimize database performance and support large-scale data processing pipelines.
Technology Leadership:
Guide the selection of data technologies, including databases (SQL/NoSQL), data processing frameworks (Hadoop, Spark), cloud platforms (Azure is a must), and analytics tools.
Stay updated on emerging data management technologies, trends, and best practices, and assess their potential application within the organization.
Data Quality & Security:
Define data quality standards and implement processes to ensure the accuracy, completeness, and consistency of data across all systems.
Establish protocols for data security, encryption, and backup/recovery to protect data assets and ensure business continuity.
Mentorship & Leadership:
Lead and mentor data engineers, data modelers, and other technical staff in best practices for data architecture and management.
Provide strategic guidance on data-related projects and initiatives, ensuring that all efforts are aligned with the enterprise data strategy.
Required Skills & Experience:
Extensive Data Architecture Expertise:
Over 7 years of experience in data architecture, data modeling, and database management.
Proficiency in designing and implementing relational (SQL) and non-relational (NoSQL) database solutions.
Strong experience with data integration tools (Azure Tools are a must + any other third party tools), ETL/ELT processes, and data pipelines.
Advanced Knowledge of Data Platforms:
Expertise in Azure cloud data platform is a must. Other platforms such as AWS (Redshift, S3), Azure (Data Lake, Synapse), and/or Google Cloud Platform (Big Query, Dataproc) is a bonus.
Experience with big data technologies (Hadoop, Spark) and distributed systems for large-scale data processing.
Hands-on experience with data warehousing solutions and BI tools (e.g., Power BI, Tableau, Looker).
Data Governance & Compliance:
Strong understanding of data governance principles, data lineage, and data stewardship.
Knowledge of industry standards and compliance requirements (e.g., GDPR, HIPAA, SOX) and the ability to architect solutions that meet these standards.
Technical Leadership:
Proven ability to lead data-driven projects, manage stakeholders, and drive data strategies across the enterprise.
Strong programming skills in languages such as Python, SQL, R, or Scala.
Certification:
Azure Certified Solution Architect, Data Engineer, Data Scientist certifications are mandatory.
Pre-Sales Responsibilities:
Stakeholder Engagement: Work with product stakeholders to analyze functional and non-functional requirements, ensuring alignment with business objectives.
Solution Development: Develop end-to-end solutions involving multiple products, ensuring security and performance benchmarks are established, achieved, and maintained.
Proof of Concepts (POCs): Develop POCs to demonstrate the feasibility and benefits of proposed solutions.
Client Communication: Communicate system requirements and solution architecture to clients and stakeholders, providing technical assistance and guidance throughout the pre-sales process.
Technical Presentations: Prepare and deliver technical presentations to prospective clients, demonstrating how proposed solutions meet their needs and requirements.
Additional Responsibilities:
Stakeholder Collaboration: Engage with stakeholders to understand their requirements and translate them into effective technical solutions.
Technology Leadership: Provide technical leadership and guidance to development teams, ensuring the use of best practices and innovative solutions.
Integration Management: Oversee the integration of solutions with existing systems and third-party applications, ensuring seamless interoperability and data flow.
Performance Optimization: Ensure solutions are optimized for performance, scalability, and security, addressing any technical challenges that arise.
Quality Assurance: Establish and enforce quality assurance standards, conducting regular reviews and testing to ensure robustness and reliability.
Documentation: Maintain comprehensive documentation of the architecture, design decisions, and technical specifications.
Mentoring: Mentor fellow developers and team leads, fostering a collaborative and growth-oriented environment.
Qualifications:
Education: Bachelors or masters degree in computer science, Information Technology, or a related field.
Experience: Minimum of 7 years of experience in data architecture, with a focus on developing scalable and high-performance solutions.
Technical Expertise: Proficient in architectural frameworks, cloud computing, database management, and web technologies.
Analytical Thinking: Strong problem-solving skills, with the ability to analyze complex requirements and design scalable solutions.
Leadership Skills: Demonstrated ability to lead and mentor technical teams, with excellent project management skills.
Communication: Excellent verbal and written communication skills, with the ability to convey technical concepts to both technical and non-technical stakeholders.",", Looker, R, ELT, Sql, Data Warehousing, Hadoop, Tableau, Power Bi, Etl, Database Management, Nosql, Python, Azure, Scala, Data Modeling, Spark"
"Data Architect (Financial Crime), Director",NatWest Group,Fresher,,"Bengaluru, India",Login to check your skill match score,"Join us as a Data Architect (Financial Crime)
For someone with a background in defining business and application architectures and roadmaps for complex enterprises, this is an excellent opportunity to join our business. You'll be defining the intentional architecture foryour assigned scope in order to ensure that the current architecture being delivered by engineers best supports the enterprise and its long term strategy
You'll provide advisory support and embed governance to ensure projects align to our simplification strategy and comply with data standards while promoting and supporting effective data modelling, metadata management and alignment to enterprise data model and data controls
With valuable exposure, you'll be building and leveraging relationships with colleagues across the bank to ensure commercially focused decisions and to create long term value for the bank
We're offering this role at director level
What you'll do
As a Data Architect, you'll be defining and communicating the current, resultant and target state data architecture for your assigned scope. You'll advise domain and project teams to ensure alignment with data architecture standards, data principles, data controls, target data architectures, data patterns and the banks simplification strategy.
We'll look to you to influence the development of business strategies at an organisational level, identifying transformational opportunities for Financial Crime associated with both new and existing data solutions.
As Well As This, You'll Be
Translating architecture roadmaps into packages of work that allow frequent incremental delivery of value to be included in product backlog
Defining, creating and maintaining architecture models, roadmaps, standards and outcomes, using architecture strategies to ensure alignment to adjacent and higher level model
Collaborate and support data management SMEs and data stewards ensuring metadata management aligns with enterprise data model
Conduct data design reviews to ensure solutions meet data standards, data principles, data controls, data patterns and target data architectures
Seeking out and utilising continuous feedback, fostering adaptive design and engineering practices to drive the collaboration of programmes and teams around a common technical vision
The skills you'll need
To succeed in this role, you'll need expert knowledge of application architecture, and at least one business, data or infrastructure architecture with working knowledge of the remaining disciplines. You'll have excellent communication skills with the ability to clearly communicate complex technical concepts to colleagues, up to senior leadership level, along with a good understanding of Agile methodologies with experience of working in an Agile team.
You'll Also Demonstrate
Good collaboration and stakeholder management skills
Experience of developing, syndicating and communicating architectures, designs and proposals for action
An understanding of industry architecture frameworks, such as TOGAF and ArchiMate
Experience in data modelling methodologies (3NF, Dimensional, Data Vault, Star Schema, etc). Knowledge of data related regulations such as GDPR, CCPA,PCI DSS, BCBS 239
Experience of working with business solution vendors, technology vendors and products within the market
A background in systems development change life cycles, best practices and approaches
Knowledge of hardware, software, application and systems engineering","BCBS 239, Systems engineering, archimate, CCPA, data modelling methodologies, Pci Dss, Gdpr, Togaf, Agile Methodologies, Application Architecture, Infrastructure Architecture"
GridOS Data Architect,GE Vernova,Fresher,,"Hyderabad, India",Login to check your skill match score,"Job Description Summary
As a Data Architect, you will play a pivotal role in defining and implementing common data models, API standards, and leveraging the Common Information Model (CIM) standard across a portfolio of products deployed in Critical National Infrastructure (CNI) environments globally.
GE Vernova is the leading software provider for the operations of national and regional electricity grids worldwide. Our software solutions range from supporting electricity markets, enabling grid and network planning, to real-time electricity grid operations.
In this senior technical role, you will collaborate closely with lead software architects to ensure secure, performant, and composable designs and implementations across our portfolio.
Job Description
Grid Software (a division of GE Vernova) is driving the vision of GridOS - a portfolio of software running on a common platform to meet the fast-changing needs of the energy sector and support the energy transition. Grid Software has extensive and well-established software stacks that are progressively being ported to a common microservice architecture, delivering a composable suite of applications. Simultaneously, new applications are being designed and built on the same common platform to provide innovative solutions that enable our customers to accelerate the energy transition.
Responsibilities
This role is for a senior data architect who understands the core designs, principles, and technologies of GridOS. Key responsibilities include:
Formalizing Data Models and API Standards: Lead the formalization and standardization of data models and API standards across products to ensure interoperability and efficiency.
Leveraging CIM Standards: Implement and advocate for the Common Information Model (CIM) standards to ensure consistent data representation and exchange across systems.
Architecture Reviews and Coordination: Contribute to architecture reviews across the organization as part of Architecture Review Boards (ARB) and the Architecture Decision Record (ADR) process.
Knowledge Transfer and Collaboration: Work with the Architecture SteerCo and Developer Standard Practices team to establish standard pratcise around data modeling and API design.
Documentation: Ensure that data modeling and API standards are accurately documented and maintained in collaboration with documentation teams.
Backlog Planning and Dependency Management: Work across software teams to prepare backlog planning, identify, and manage cross-team dependencies when it comes to data modeling and API requirements.
Key Knowledge Areas and Expertise
Data Architecture and Modeling: Extensive experience in designing and implementing data architectures and common data models.
API Standards: Expertise in defining and implementing API standards to ensure seamless integration and data exchange between systems.
Common Information Model (CIM): In-depth knowledge of CIM standards and their application within the energy sector.
Data Mesh and Data Fabric: Understanding of data mesh and data fabric principles, enabling software composability and data-centric design trade-offs.
Microservice Architecture: Understandig of microservice architecture and software development
Kubernetes: Understanding of Kubernetes, including software development in an orchestrated microservice architecture. This includes Kubernetes API, custom resources, API aggregation, Helm, and manifest standardization.
CI/CD and DevSecOps: Experience with CI/CD pipelines, DevSecOps practices, and GitOps, especially in secure, air-gapped environments.
Mobile Software Architecture: Knowledge of mobile software architecture for field crew operations, offline support, and near-realtime operation.
Additional Knowledge (Advantageous But Not Essential)
Energy Industry Technologies: Familiarity with key technologies specific to the energy industry, such as Supervisory Control and Data Acquisition (SCADA), Geospatial network modeling, etc.
This is a critical role within Grid Software, requiring a broad range of knowledge and strong organizational and communication skills to drive common architecture, software standards, and principles across the organization.
Additional Information
Relocation Assistance Provided: No","Data Mesh and Data Fabric, Data Architecture and Modeling, CI CD and DevSecOps, API Standards, Common Information Model CIM, Mobile Software Architecture, Microservice Architecture, Kubernetes"
Data Architect,DATAECONOMY,15-17 Years,,"Hyderabad, India",Login to check your skill match score,"About Us
About DATAECONOMY: We are a fast-growing data & analytics company headquartered in Dublin with offices inDublin, OH, Providence, RI, and an advanced technology center in Hyderabad,India. We are clearly differentiated in the data & analytics space via our suite of solutions, accelerators, frameworks, and thought leadership.
Job Description
15+ years of professional experience in the field of Data Architecture, Design and Development of Data Lifecycle Management (DLM) and Data Governance projects
Experience should include hands-on experience in implementing a Customer Data Hub Customer Information Management system or Customer Data Mart which includes Modeling Customer Data Model in the 3NF / Dimensional Model, defining and developing integration patterns like Batch and real-time, defining & developing the physical storage layer, defining & developing the consumption pattern
Experience with Data Modeling is a MUST, experience in defining standards and data architecture best practices is a must
Experience with Data Management/Architecture Practices like Data Integration, Data Governance, Data Quality, Metadata management, Data Security, Data Encryption etc. is a must
Must have hands-on experience with developing large Data Mart or Data warehouse or Data hub involving data modelling, data ingestion, data processing and extract generation to the downstream systems
Must have experience with any ETL tool like Informatica or Data Stage or other for data processing requirements.
Experience with Identity resolution products for Name and Address standardization like Informatica IDQ / Address Doctor /IBM Quality stage is required.
Experience with any Data Quality product like Informatica IDQ, Quality Stage or Trillium will be an added advantage
Must have strong experience with writing SQL for pulling and analyzing source/data platforms
Experience with Data Science models, model validation, model tuning and management will be an added advantage.
Proactively communicate and collaborate with business stakeholders to understand the business requirements, translate them into design, and develop the respective module based on the requirements.
Must have experience working in an Agile implementation set-up with Product Backlogs, User Stories, Scrum and sprint planning etc.
Strong verbal and written communication and English language skills
Strong consulting skills and consulting experience are strongly desired.
Requirements
Developing Data Architecture and Design documents for Data Lake, Data warehouse & Data Marts corresponding to Customer Information Management Systems
Experience in Data Lifecycle Management (DLM)
Configuring Solutions and Coding any customization required for the Data platform including Data Lake, Data warehouse and Data Marts
Working with the clients to understand the requirements. Develop the required codebase for the functional needs
Develop Source to Target mapping document for all the attributes that are required to be captured for Data Lake, Data warehouse and Data Marts
Configure and develop code required for Upstream and downstream system communication in a Batch and real-time mode
Provide clarifications for any questions that the Business / SME team or any other stakeholder has on Data implementations.
Participate in system and acceptance testing along with the stakeholders
Benefits
Standard Company Benefits","Data Encryption, Customer Data Hub, Data Science Models, Customer Data Mart, Customer Information Management, Metadata Management, Data Modeling, Sql, Data Integration, Data Quality, Data Architecture, Data Security, Data Governance"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Pune, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata
work Mode- Hybrid
Roles And Responsibilities
Mandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization
Solution Architect for Data modelling Understanding of Enterprise datasets Sales,
Procurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle
etc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building
Data lake foundation, Maintenance etc)
Collaborate with the product/business teams, understand related business processes and
document business requirements and then write high level specifications/requirements for DEs
Develop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout
for right grain of data either in True source systems or in Data WHs and build reusable data
models in intermediary layers before creating physical consumable views from data mart
Understand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data
Governance, Data Quality frameworks, Data Observality and the candidate should be:
Familiar with DevOps process
Knowing how to check existing tables, data dictionary, table structures
Experienced with normalizing tables
Having good understanding of Landing, Bronze, Silver and Gold layers and concepts
Familiar with Agile techniques
Create business process map, user journey map and data flow integration diagrams; Understand
Integration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of
models
Stakeholder management with data engineering, product owners, central data modelling team,
data governance & stewards, Scrum master, project team and sponsor.
Ability to handle large implementation program with multiple projects spanning over an year.
Skills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Data Quality frameworks, Agile techniques, DataOps, Data lake foundation, Data Observability, SAP, Sql, Data Modeling, Cloud Architecture, Data Governance, Devops, Dimensional Modeling, FTP, Data Modeler, Data Visualization, Oracle, Data Architect, Data Warehouse, Azure, Data Governance, Sftp, data vault"
Data Architect,Aumni Techworks,8-10 Years,,"Pune, India",Login to check your skill match score,"Position Summary:
We are looking for a highly skilled and experienced Data Engineer who focus on leading the development, and implementation of our Data Warehouse/Lakehouse solution, ensuring it serves as the foundation for scalable, high-performance analytics.
Responsibilities:
Lakehouse Design & Implementation:
Lead the end-to-end development and deployment of a scalable and secure Lakehouse architecture.
Define best practices for data ingestion, storage, transformation, and processing using modern cloud technologies.
Architect data pipelines using ETL/ELT frameworks to support structured, semi-structured, and unstructured data.
Optimize data modeling strategies to meet the analytical and performance needs of stakeholders.
Evaluate and select appropriate cloud technologies, frameworks, and architectures.
Requirement:
Experience:
8+ years of experience in data engineering, with a proven track record of implementing large-scale data solutions.
Extensive experience with cloud platforms (AWS, GCP, or Azure), specifically in data warehouse/lakehouse implementations.
Expertise in modern data architectures with tools like Databricks, Snowflake, or BigQuery.
Strong background in SQL, Python, and distributed computing frameworks (Spark, Dataflow, etc.).
In-depth knowledge of data modeling principles (e.g., Star Schema, Snowflake Schema).
Experience in enabling AI tools to consume data from the Lakehouse.
About Aumni Techworks:
Established in 2016, Aumni Techworks partners with its multinational clients to incubate and operate remote teams in India using the AumniBOT model. With a team of 250 and growing, our mission is to provide a quality alternative to project-based outsourcing.
Benefits of working at Aumni Techworks:
Work within a product team on cutting edge tech with one of the best pay packages.
No politics, no bench, voice your opinion, flat hierarchy, and global exposure
Work environment to re-live our fun college days (awarded as Best culture by Pune Mirror)
Recharge frequently with Friday socials, dance classes, theme parties and monsoon picnic.
Breakout spaces at the office Gym, Pool, TT, Foosball and Carrom
Health focused Insurance coverage and get in shape with AumniFit (Do not miss our 4 PM plank!)","Snowflake Schema, Data ingestion, snowflake, ELT frameworks, Lakehouse architecture, BigQuery, Data Modeling, Cloud Technologies, Sql, Gcp, Spark, Databricks, DataFlow, Azure, Star Schema, Python, AWS"
Cloud Data Architect,Allianz Services,7-12 Years,,"Pune, India",Insurance,"Designation - Senior Cloud Data Architect
Experience Range - 7 to 12 Years
Job Location - Pune OR Trivandrum
Key Responsibilities:
Collaborate with agile and analytics data teams to design and implement scalable data products using cutting-edge technologies.
Provide strategic insights and expertise on cloud capabilities, focusing on AWS or Azure, ensuring compliance with global and local regulations, security, and risk management.
Serve as a Subject Matter Expert for end-to-end cloud data architecture at Allianz.
Apply Agile and DevOps methodologies and implementation approaches in project delivery.
Utilize strong communication skills to understand and deliver change and BAU requirements effectively.
Drive innovative engineering, design, and strategy with your experience and ideas.
Mentor and upskill team members, fostering growth and knowledge sharing.
Contribute to internal networks and special interest groups, enhancing our knowledge base and community.
Desired Experience:
We are looking for a new team member with strong technical skills, but equally value strong communication skills and adaptability, and a willingness to learn and respond to emerging needs:
Expertise in managing Kubernetes components, scalability, and cost/consumption monitoring.
Deep understanding of managing relational and non-relational data stores, scalability, and cost/consumption monitoring.
Provide advisory and thought leadership on cloud-based analytics environments and big data technologies, integrating with existing data and analytics platforms.
Hands-on experience with:
Big Data stack (e.g., SPARK, Kafka)
Relational/non-relational stores (e.g., Postgres, MongoDB, Cassandra)
Related open-source software platforms and languages (e.g., Java, Apache, Python, Scala)
Develop solutions architecture and evaluate architectural alternatives for private, public, and hybrid cloud models, including IaaS, PaaS, and other cloud services.
Ensure timely delivery of solutions, working with project sponsors to size, estimate (capacity planning), and manage scope and risk.
Provide support and technical governance, offering expertise related to cloud architectures, deployment, and operations.","Java, Cassandra, Scala, Postgres, MongoDB, Azure, Apache, Python, Kubernetes, AWS"
Data Architect,Tredence Inc.,8-14 Years,,"Chennai, India",Login to check your skill match score,"Overall professional experience of the candidate should be atleast 8 years with a maximum experience upto 14 years.
The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.
Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.
Role Description:
Developing Modern Data Warehouse solutions using Databricks and Azure Stack
Ability to provide solutions that are forward-thinking in data engineering and analytics space
Collaborate with DW/BI leads to understand new ETL pipeline development requirements.
Triage issues to find gaps in existing pipelines and fix the issues
Work with business to understand the need in reporting layer and develop data model to fulfill reporting needs
Drive technical discussion with client architect and team members
Orchestrate the data pipelines in scheduler via Airflow Skills and Qualifications:
Bachelor's and/or master's degree in computer science or equivalent experience.
Good understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture
Should have hands-on experience in SQL, Python and Spark (PySpark)
Experience in building ETL / data warehouse transformation processes
Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot
Databricks Certified Data Engineer Associate/Professional Certification (Desirable).
Comfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects
Should have experience working in Agile methodology
Strong verbal and written communication skills.
Strong analytical and problem-solving skills with a high attention to detail.
Mandatory Skills
Azure Databricks, Pyspark, Azure Data Factory, Azure Data Lake.","Azure Stack, Spark, Sql, Databricks, Pl Sql, Azure Data Factory, RDBMS, Pyspark, Data Quality, Unix Shell Scripting, Data Governance, Python, Azure Data Lake, Data Integration"
Data Architect - RWD and Analytics Products,Norstella,Fresher,,India,Login to check your skill match score,"Description
About Norstella
At Norstella, our mission is simple: to help our clients bring life-saving therapies to market quickerand help patients in need.
Founded in 2022, but with history going back to 1939, Norstella unites best-in-class brands to help clients navigate the complexities at each step of the drug development life cycle and get the right treatments to the right patients at the right time.
Each Organization (Citeline, Evaluate, MMIT, Panalgo, The Dedham Group) Delivers Must-have Answers For Critical Strategic And Commercial Decision-making. Together, Via Our Market-leading Brands, We Help Our Clients
Citeline accelerate the drug development cycle
Evaluate bring the right drugs to market
MMIT identify barrier to patient access
Panalgo turn data into insight faster
The Dedham Group think strategically for specialty therapeutics
By combining the efforts of each organization under Norstella, we can offer an even wider breadth of expertise, cutting-edge data solutions and expert advisory services alongside advanced technologies such as real-world data, machine learning and predictive analytics. As one of the largest global pharma intelligence solution providers, Norstella has a footprint across the globe with teams of experts delivering world class solutions in the USA, UK, The Netherlands, Japan, China and India.
Job Description
We are seeking a Technical Product Solutions Product Management Director to lead the design and implementation of scalable real-world data (RWD) solutions architecture. This role sits within the Product team but maintains strong collaboration with Engineering to ensure technical feasibility and execution. The ideal candidate has expertise in healthcare data, claims, EHR, lab and other types of RWD and is skilled in translating business needs into scalable, high-impact data products.
This role will be instrumental in shaping data-driven products, optimizing data architectures, and ensuring the integration of real-world data assets into enterprise solutions that support life sciences, healthcare, and payer analytics.
Responsibilities
Define and drive the requirements for RWD data products.
Collaborate with leadership, product managers, customers, and data scientists to identify high-value use cases.
Translate business and regulatory requirements into scalable and performant data models and solutions.
Develop architectures to support payer claims, labs, ehr-sourced insight generation and analytics.
Partner with healthcare providers, payers, and life sciences companies to enhance data interoperability.
Work closely with Engineering to design and implement responsive analytics layer and data architectures.
Provide technical guidance on ETL pipelines, data normalization, and integration with third-party RWD sources.
Architect solutions to aggregate, standardize, and analyze EHR and molecular data, ensuring compliance with healthcare regulations (HIPAA, GDPR).
Define best practices for claims data ingestion, quality control, and data transformations.
Develop frameworks for processing structured and unstructured EHR data, leveraging NLP and data harmonization techniques.
Ensure compliance with HIPAA, GDPR, and regulatory frameworks for healthcare data products.
Define and implement data governance strategies to maintain high data integrity and lineage tracking.
Requirements
Deep understanding of payer data, claims lifecycle, EHR, labs and real-world data applications.
Ability to translate business needs into technical solutions and drive execution.
Strong understanding of data product lifecycle and product management principles.
Experience working with cross-functional teams, including Product, Engineering, Clinical, Business and Customer Success.
Excellent communication skills to engage with both technical and non-technical stakeholders.
Expertise in RWD and payer data structures (claims, EMR/EHR, registry data, prescription data, etc.).
Proficiency in SQL and NoSQL databases (PostgreSQL, Snowflake, MongoDB, etc.).
Strong knowledge of ETL processes and data pipeline orchestration.
Experience with big data processing (Spark, Databricks, Hadoop).
Understanding of payer and provider data models used in healthcare analytics.
Strong presentation and documentation skills to articulate solutions effectively.
Experience working with payer organizations, PBMs, life sciences, and health plans.
Experience with OMOP, FHIR, HL7, and other healthcare data standards.
Knowledge of data governance, metadata management, and lineage tracking tools.
Experience in pharmaceutical RWE studies and market access analytics.
Familiarity with BI tools (Tableau, Power BI, Looker).
Understanding of data mesh and federated data architectures.
Benefits
Health Insurance
Provident Fund
Life Insurance
Reimbursement of Certification Expenses
Gratuity
24x7 Health Desk
Our guiding principles for success at Norstella
01: Bold, Passionate, Mission-First
We have a lofty mission to Smooth Access to Life Saving Therapies and we will get there by being bold and passionate about the mission and our clients. Our clients and the mission in what we are trying to accomplish must be in the forefront of our minds in everything we do.
02: Integrity, Truth, Reality
We make promises that we can keep, and goals that push us to new heights. Our integrity offers us the opportunity to learn and improve by being honest about what works and what doesn't. By being true to the data and producing realistic metrics, we are able to create plans and resources to achieve our goals.
03: Kindness, Empathy, Grace
We will empathize with everyone's situation, provide positive and constructive feedback with kindness, and accept opportunities for improvement with grace and gratitude. We use this principle across the organization to collaborate and build lines of open communication.
04: Resilience, Mettle, Perseverance
We will persevere even in difficult and challenging situations. Our ability to recover from missteps and failures in a positive way will help us to be successful in our mission.
05: Humility, Gratitude, Learning
We will be true learners by showing humility and gratitude in our work. We recognize that the smartest person in the room is the one who is always listening, learning, and willing to shift their thinking.
Norstella is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people's differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual's abilities, skills, performance and behavior and our business requirements. Norstella operates a zero tolerance policy to any form of discrimination, abuse or harassment.
Sometimes the best opportunities are hidden by self-doubt. We disqualify ourselves before we have the opportunity to be considered. Regardless of where you came from, how you identify, or the path that led you here- you are welcome. If you read this job description and feel passion and excitement, we're just as excited about you.","data architectures, FHIR, snowflake, ETL processes, federated data architectures, OMOP, NoSQL databases, Looker, data models, healthcare data, lineage tracking, Claims, data mesh, big data processing, data normalization, Metadata Management, PostgreSQL, Tableau, Ehr, Hadoop, Power Bi, Bi Tools, Sql, Hl7, Spark, Data Governance, Databricks, MongoDB"
Data Architect,Ideas2IT Technologies,Fresher,,"Chennai, India",Login to check your skill match score,"Why Choose Ideas2IT
Ideas2IT has all the good attributes of a product startup and a services company. Since we launch our products, you will have ample opportunities to learn and contribute. However, single-product companies stagnate in the technologies they use. In our multiple product initiatives and customer-facing projects, you will have the opportunity to work on various technologies.
AGI is going to change the world. Big companies like Microsoft are betting heavily on this (see here and here). We are following suit. As a Data Engineer, exclusively focus on engineering data pipelines for complex products
What's in it for you
A robust distributed platform to manage a self-healing swarm of bots onunreliable network / compute
Large-scale Cloud-Native applications
Document Comprehension Engine leveraging RNN and other latest OCR techniques
Completely data-driven low-code platform
You will leverage cutting-edge technologies like Blockchain, IoT, and Data Science as you work on projects for leading Silicon Valley startups.
Your role does not start or end with just Java development; you will enjoy the freedom to share your suggestions on the choice of tech stacks across the length of the project
If there is a certain technology you would like to explore, you can do your Technical PoCs
Work in a culture that values capability over experience and continuous learning as a core tenet
Here's what you'll bring
Proficiency in SQL and experience with database technologies (e.g., MySQL, PostgreSQL, SQL Server).Experience in any one of the cloud environments AWS, Azure
Experience with data modeling, data warehousing, and building ETL pipelines.
Experience building large-scale data pipelines and data-centric applications using any distributed storage platform
Experience in data processing tools like Pandas, pyspark.
Experience in cloud services like S3, Lambda, SQS, Redshift, Azure Data Factory, ADLS, Function Apps, etc.
Expertise in one or more high-level languages (Python/Scala)
Ability to handle large-scale structured and unstructured data from internal and third-party sources
Ability to collaborate with analytics and business teams to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision-making across the organization
Experience with data visualization tools like PowerBI, Tableau
Experience in containerization technologies like Docker , Kubernetes
About Us
Ideas2IT stands at the intersection of Technology, Business, and Product Engineering, offering high-caliber Product Development services. Initially conceived as a CTO consulting firm, we've evolved into thought leaders in cutting-edge technologies such as Generative AI, assisting our clients in embracing innovation.
Our forte lies in applying technology to address business needs, demonstrated by our track record of developing AI-driven solutions for industry giants like Facebook, Bloomberg, Siemens, Roche, and others. Harnessing our product-centric approach, we've incubated several AI-based startupsincluding Pipecandy, Element5, IdeaRx, and Carefi. inthat have flourished into successful ventures backed by venture capital.
With fourteen years of remarkable growth behind us, we're steadfast in pursuing ambitious objectives.
P.S. We're all about diversity, and our doors are wide open to everyone. Join us in celebrating the awesomeness of differences!","Function Apps, data-centric applications, data visualization tools, ETL pipelines, large-scale data pipelines, ADLS, S3, PostgreSQL, Data Warehousing, Pyspark, Tableau, Data Modeling, Lambda, Docker, MySQL, Python, AWS, Scala, SQL Server, Redshift, Sql, Azure Data Factory, Pandas, Powerbi, Sqs, Azure, Kubernetes"
MS Azure Data Architect,3Pillar,8-10 Years,,India,Login to check your skill match score,"We are seeking a highly experienced Data Architect with a strong background in designing scalable data solutions and leading data engineering teams. The ideal candidate will have deep expertise in Microsoft Azure, ETL processes, and modern data architecture principles. This role involves close collaboration with stakeholders, engineering teams, and business units to design and implement robust data pipelines and architectures.
Assessments of existing data components, Performing POCs, Consulting to the stakeholders
Proposing end to end solutions to an enterprise's data specific business problems, and taking care of data collection, extraction, integration, cleansing, enriching and data visualization
Ability to design large data platforms to enable Data Engineers, Analysts & scientists
Strong exposure to different Data architectures, data lake & data warehouse
Design and implement end-to-end data architecture solutions on Azure cloud platform.
Lead the design and development of scalable ETL/ELT pipelines using tools such as Azure Data Factory (ADF).
Architect data lakes using Azure Data Lake Storage (ADLS) and integrate with Azure Synapse Analytics for enterprise-scale analytics.
Collaborate with business analysts, data scientists, and engineers to understand data needs and deliver high-performing solutions.
Define data models, metadata standards, data quality rules, and security protocols.
Define tools & technologies to develop automated data pipelines, write ETL processes, develop dashboard & report and create insights
Continually reassess current state for alignment with architecture goals, best practices and business needs
DB modeling, deciding best data storage, creating data flow diagrams, maintaining related documentation
Taking care of performance, reliability, reusability, resilience, scalability, security, privacy & data governance while designing a data architecture
Apply or recommend best practices in architecture, coding, API integration, CI/CD pipelines
Coordinate with data scientists, analysts, and other stakeholders for data-related needs
Help the Data Science & Analytics Practice grow by mentoring junior Practice members, leading initiatives, leading Data Practice Offerings
Provide thought leadership by representing the Practice / Organization on internal / external platforms
Qualificatons:
8+ years of experience in data architecture, data engineering, or related roles.
Translate business requirements into data requests, reports and dashboards.
Strong Database & modeling concepts with exposure to SQL & NoSQL Databases
Expertise in designing and writing ETL processes in Python/PySpark
Strong data architecture patterns & principles, ability to design secure & scalable data lakes, data warehouse, data hubs, and other event-driven architectures
Proven expertise in Microsoft Azure data services, especially ADF, ADLS, Synapse Analytics.
Strong hands-on experience in designing and building ETL/ELT pipelines.
Proficiency in data modeling, SQL, and performance tuning.
Demonstrated leadership experience, with the ability to manage and mentor technical teams.
Excellent communication and stakeholder management skills.
Proficiency in data visualization tools like Tableau, Power BI or similar to create meaningful insights.
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Good to have:
Azure certifications (e.g., Azure Data Engineer Associate, Azure Solutions Architect).
Experience with modern data platforms, data governance frameworks, and real-time data processing tools.
Benefits:
Imagine a flexible work environment whether it's the office, your home, or a blend of both. From interviews to onboarding, we embody a remote-first approach.
You will be part of a global team, learning from top talent around the world and across cultures, speaking English everyday. Our global workforce enables our team to leverage global resources to accomplish our work in efficient and effective teams.
We're big on your well-being as a company, we spend a whole trimester in our annual cycle focused on wellbeing. Whether it is taking advantage of fitness offerings, mental health plans (country-dependent), or simply leveraging generous time off, we want all of our team members operating at their best.
Our professional services model enables us to accelerate career growth and development opportunities - across projects, offerings, and industries.
We are an equal opportunity employer. It goes without saying that we live by values like Intrinsic Dignity and Open Collaboration to create cutting-edge technology AND reinforce our commitment to diversity - globally and locally.
Join us and be a part of a global tech community! Check out our Linkedin site and Careers page to learn more about what it's like to be part of our #oneteam!","NoSQL Databases, ETL processes, Azure Synapse Analytics, Pyspark, Data Modeling, Microsoft Azure, Data Architecture, Sql, Python"
Data Architect / Lead,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Position: FTE, Remote, India
Availability - Immediate -1 week
Candidate proficiency level: Senior(10+)
Responsibilities:
Design, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.
Enhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.
Implement best practices for data management, storage and security to ensure data integrity and compliance with regulations.
Own the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.
Participate in code reviews to ensure code quality and share knowledge.
Lead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.
Define and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.
Mentor junior members of the team, providing guidance and support in their professional development.
Collaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration
A little more about you:
Bachelor's degree or higher in Computer Science, Engineering, or a related field.
10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.
Proficient in SQL and Python, with the ability to translate complexity into efficient code.
Experience with data workflow development and management tools (dbt, Airflow).
Solid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.
Strong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Experience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Sql, AWS, Python, Azure, Gcp"
Data Architect - Enterprise Architecture,Viasat,Fresher,,"Chennai, India",Login to check your skill match score,"About Us
One team. Global challenges. Infinite opportunities. At Viasat, we're on a mission to deliver connections with the capacity to change the world. For more than 35 years, Viasat has helped shape how consumers, businesses, governments and militaries around the globe communicate. We're looking for people who think big, act fearlessly, and create an inclusive environment that drives positive impact to join our team.
What You'll Do
The Enterprise Architecture team is focused on providing solutions to enable an effective software engineering workforce that can scale to the business needs. This includes exploring how the business needs map to the application portfolio, business processes, APIs, and data elements across the organization. As a member of this team, you will build up a vast knowledge in software development, cloud application engineering, automation, and container orchestration. Our ideal candidate values communication, learning, adaptability, creativity, and ingenuity. They also enjoy working on challenging technical issues and use creative, innovative techniques to develop and automate solutions. This team is focused on providing our executive and business leadership with visibility into how the software organization is functioning and what opportunities lie to transform the business. In this position you will be an integral part of the Enterprise Architecture team and make meaningful impacts in our journey towards digital transformation.
The day-to-day
A Strong Data-Model and High-Quality Data are pre-requisites to provide better insights and enable solid data-driven decision making. This is also key to take advantage of various technological advances in Artificial Intelligence and Machine Learning. Your responsibilities will involve build out of data models for various aspects of our enterprise in conjunction with domain experts. Examples include but are not limited to Network, Capacity, Finance, Business Support Systems etc. Responsibilities also include working with software product teams to improve data quality across the organization.
What You'll Need
Bachelor's degree or higher in Computer Science & Applications, Computer Science and Computer & Systems Engineering, Computer Science & Engineering, Computer Science & Mathematics, Computer Science & Network Security and Math & Computer Science, and/or a related field
Solid understanding of Data Architecture and Data Engineering principles
Experience building out data models
Experience performing data analysis and presenting data in easy to comprehend manner.
Experience in working with Relational Databases, NoSQL, Large Scale Data technologies (Kafka, Big Query, Snowflake etc)
Experience with digital transformation across multiple cloud platforms like AWS and GCP.
Experience in modernizing data platforms especially in GCP is highly preferred.
Partner with members of Data Platform team and others to build out Data Catalog and map to the data model
Detail Oriented to ensure that the catalog represents quality data
Solid communication skills and ability to work on a distributed team
Tenacity to remain focused on the mission and overcome obstacles
Ability to perform hands-on work with development teams and guide them to building necessary data models.
Experience setting up governance structure and changing the organization culture by influence
What Will Help You On The Job
Experience with Cloud Technologies: AWS, GCP, and/or Azure, etc.
Expertise in GCP data services like Cloud Pub/Sub, Dataproc, Dataflow, BigQuery, and related technologies preferred.
Experience with Airflow, DBT and SQL.
Experience with Open-source software like Logstash, ELK stack, Telegraf, Prometheus and OpenTelemetry is a plus.
Passionate to deliver solutions that improve developer experience and promote API-first principles and microservices architecture.
Experience with Enterprise Architecture and related principles
EEO Statement
Viasat is proud to be an equal opportunity employer, seeking to create a welcoming and diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, ancestry, physical or mental disability, medical condition, marital status, genetics, age, or veteran status or any other applicable legally protected status or characteristic. If you would like to request an accommodation on the basis of disability for completing this on-line application, please click here.","Large Scale Data technologies, Big Query, Airflow, Relational Databases, OpenTelemetry, snowflake, dbt, Telegraf, Cloud Pub Sub, Data Analysis, data engineering, Enterprise Architecture, Prometheus, Kafka, Elk Stack, Nosql, Data Architecture, AWS, Logstash, Dataproc, Sql, Cloud Technologies, Gcp, DataFlow"
Staff Specialist IT - PLM Data Architect,Infineon Technologies,8-10 Years,,"Ahmedabad, India",Semiconductor Manufacturing,"As a PLM Data Architect, you will be responsible for designing, implementing, and maintaining the data architecture of our Product Lifecycle Management (PLM) system. You will work closely with cross-functional teams to ensure data consistency, integrity, and quality across the entire product lifecycle. If you have a strong background in data architecture, PLM systems, and a passion for data-driven decision-making, we encourage you to apply for this exciting opportunity.
Job Description
In your new role you will:
Design and Create the framework for managing the organization's enterprise data architecture.
Focus on identifying and using the right tools and technologies, technical methodologies, technical guardrails and guidelines ,integration with broader technical environment, etc.
Design and implement a scalable and flexible data architecture for the PLM system, ensuring data consistency, integrity, and quality across the entire product lifecycle.
Develop and maintain data models, data flows, and data governance policies to ensure data accuracy, completeness, and compliance with industry standards and regulations.
Collaborate with cross-functional teams, including engineering, manufacturing, and quality, to ensure data requirements are met and data is properly integrated across systems.
Develop and maintain data interfaces, APIs, and data migration strategies to ensure seamless data exchange between PLM and other systems.
Ensure data security, access controls, and auditing mechanisms are in place to protect sensitive product data.
Develop and maintain data analytics and reporting capabilities to support business decision-making and product development.
Your Profile
You are best equipped for this task if you have:
Bachelor's or Master's degree (Computer Science or Related) with 8+years of relevant experience.
Proven experience as Data Architecture or in a similar role in an R&D environment.
Experience in implementing data management, data integration and reporting technologies.
Good Knowledge of data governance, data quality, and data security best practices.
Knowledge in enterprise systems like PLM, ERP, MD systems.
Proficiency in data modelling and design.
Experience working with Data Platforms, Data Catalogues, API Management and Event Driven Architecture.
Knowledge of programming languages Python or Java , NoSQL databases, data visualization, data virtualization.
Solid understanding of cloud services, architectures, and storage solutions.
Excellent problem-solving and communication skills.
#WeAreIn for driving decarbonization and digitalization.
As a global leader in semiconductor solutions in power systems and IoT, Infineon enables game-changing solutions for green and efficient energy, clean and safe mobility, as well as smart and secure IoT. Together, we drive innovation and customer success, while caring for our people and empowering them to reach ambitious goals. Be a part of making life easier, safer and greener.
Are you in
We are on a journey to create the best Infineon for everyone.
This means we embrace diversity and inclusion and welcome everyone for who they are. At Infineon, we offer a working environment characterized by trust, openness, respect and tolerance and are committed to give all applicants and employees equal opportunities. We base our recruiting decisions on the applicants experience and skills.
Please let your recruiter know if they need to pay special attention to something in order to enable your participation in the interview process.
Click here for more information about Diversity & Inclusion at Infineon.","Storage Solutions, Data virtualization, Cloud services architectures, reporting technologies, PLM systems, NoSQL databases, Data Platforms, Event Driven Architecture, Data security best practices, Data Catalogues, Java, Api Management, Data Quality, Data Governance, Data Visualization, Data Management, Data Integration, Data Architecture, Data Modelling, Python"
Senior Manager_Data Architect-Tech Ops,PwC Acceleration Centers in India,13-15 Years,,"Bengaluru, India",Login to check your skill match score,"Summary about Organization A career in our Advisory Acceleration Center is the natural extension of PwC's leading global delivery capabilities. The team consists of highly skilled resources that can assist in the areas of helping clients transform their business by adopting technology using bespoke strategy, operating model, processes and planning. You'll be at the forefront of helping organizations around the globe adopt innovative technology solutions that optimize business processes or enable scalable technology. Our team helps organizations transform their IT infrastructure, modernize applications and data management to help shape the future of business. An essential and strategic part of Advisory's multi-sourced, multi-geography Global Delivery Model, the Acceleration Centers are a dynamic, rapidly growing component of our business. The teams out of these Centers have achieved remarkable results in process quality and delivery capability, resulting in a loyal customer base and a reputation for excellence. .
Job Description
Senior Data Architect with experience in design, build, and optimization of complex data landscapes and legacy modernization projects. The ideal candidate will have deep expertise in database management, data modeling, cloud data solutions, and ETL (Extract, Transform, Load) processes. This role requires a strong leader capable of guiding data teams and driving the design and implementation of scalable data architectures.
Key areas of expertise include
Design and implement scalable and efficient data architectures to support business needs.
Develop data models (conceptual, logical, and physical) that align with organizational goals.
Lead the database design and optimization efforts for structured and unstructured data.
Establish ETL pipelines and data integration strategies for seamless data flow.
Define data governance policies, including data quality, security, privacy, and compliance.
Work closely with engineering, analytics, and business teams to understand requirements and deliver data solutions.
Oversee cloud-based data solutions (AWS, Azure, GCP) and modern data warehouses (Snowflake, BigQuery, Redshift).
Ensure high availability, disaster recovery, and backup strategies for critical databases.
Evaluate and implement emerging data technologies, tools, and frameworks to improve efficiency.
Conduct data audits, performance tuning, and troubleshooting to maintain optimal performance
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
13+ years of experience in data modeling, including conceptual, logical, and physical data design.
5 8 years of experience in cloud data lake platforms such as AWS Lake Formation, Delta Lake, Snowflake or Google Big Query.
Proven experience with NoSQL databases and data modeling techniques for non-relational data.
Experience with data warehousing concepts, ETL/ELT processes, and big data frameworks (e.g., Hadoop, Spark).
Hands-on experience delivering complex, multi-module projects in diverse technology ecosystems.
Strong understanding of data governance, data security, and compliance best practices.
Proficiency with data modeling tools (e.g., ER/Studio, ERwin, PowerDesigner).
Excellent leadership and communication skills, with a proven ability to manage teams and collaborate with stakeholders.
Preferred Skills
Experience with modern data architectures, such as data fabric or data mesh.
Knowledge of graph databases and modeling for technologies like Neo4j.
Proficiency with programming languages like Python, Scala, or Java.
Understanding of CI/CD pipelines and DevOps practices in data engineering.","data fabric, DevOps practices, NoSQL databases, cloud data solutions, snowflake, data mesh, Java, BigQuery, Hadoop, Data Security, Scala, Data Modeling, Redshift, Database Management, Gcp, Spark, Data Governance, Data Warehousing Concepts, Azure, Python, AWS"
Data Architect - Manager,PwC Acceleration Centers in India,5-9 Years,,"Bengaluru, India",Login to check your skill match score,"At PwC, our people in software and product innovation focus on developing cutting-edge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions. Those in software engineering at PwC will focus on developing innovative software solutions to drive digital transformation and enhance business performance. In this field, you will use your knowledge to design, code, and test cutting-edge applications that revolutionise industries and deliver exceptional user experiences.
Enhancing your leadership style, you motivate, develop and inspire others to deliver quality. You are responsible for coaching, leveraging team member's unique strengths, and managing performance to deliver on client expectations. With your growing knowledge of how business works, you play an important role in identifying opportunities that contribute to the success of our Firm. You are expected to lead with integrity and authenticity, articulating our purpose and values in a meaningful way. You embrace technology and innovation to enhance your delivery and encourage others to do the same.
Skills
Examples of the skills, knowledge, and experiences you need to lead and deliver value at this level include but are not limited to:
Analyse and identify the linkages and interactions between the component parts of an entire system.
Take ownership of projects, ensuring their successful planning, budgeting, execution, and completion.
Partner with team leadership to ensure collective ownership of quality, timelines, and deliverables.
Develop skills outside your comfort zone, and encourage others to do the same.
Effectively mentor others.
Use the review of work as an opportunity to deepen the expertise of team members.
Address conflicts or issues, engaging in difficult conversations with clients, team members and other stakeholders, escalating where appropriate.
Uphold and reinforce professional and technical standards (e.g. refer to specific PwC tax and audit guidance), the Firm's code of conduct, and independence requirements.
Data Modeler
Job Summary
Looking for candidates with a strong background in data modeling, metadata management, and data system optimization. You will be responsible for analyzing business needs, developing longterm data models, and ensuring the efficiency and consistency of our data systems.
Key Responsibilities
Analyze and translate business needs into long term solution data models.
Evaluate existing data systems and recommend improvements.
Define rules to translate and transform data across data models.
Work with the development team to create conceptual data models and data flows.
Develop best practices for data coding to ensure consistency within the system.
Review modifications of existing systems for cross compatibility.
Implement data strategies and develop physical data models.
Update and optimize local and metadata models.
Utilize canonical data modeling techniques to enhance data system efficiency.
Evaluate implemented data systems for variances, discrepancies, and efficiency.
Troubleshoot and optimize data systems to ensure optimal performance.
Required Qualifications
Bachelor's degree in Engineering or a related field.
5 to 9 years of experience in data modeling or a related field.
4+ years of hands-on experience with dimensional and relational data modeling.
Expert knowledge of metadata management and related tools.
Proficiency with data modeling tools such as Erwin, Power Designer, or Lucid.
Knowledge of transactional databases and data warehouses.
Desired Skills And Competencies
Advanced troubleshooting skills.
Excellent communication and presentation skills.
Strong interpersonal skills to collaborate effectively with various teams.","Data system optimization, Canonical data modeling techniques, Transactional databases, Metadata Management, relational data modeling, data warehouses, Data Modeling"
Data Center Architect,Mindsprint,Fresher,,"Chennai, India",Login to check your skill match score,"Job Title: Datacentre Architect
Location: Chennai, Tamil Nadu
Company: Mindsprint
About Mindsprint: Mindsprint is at the forefront of innovation, driving transformative changes in the IT landscape. As we embark on a significant journey of de-merging our IT infrastructure and application landscape, we are seeking a highly skilled and experienced De-Merger as a Service Architect to lead this critical initiative.
Job Summary: Data Centre Architect will be responsible for designing and implementing the de-merger strategy for Mindsprint's IT infrastructure and application landscape. This role requires a deep understanding of IT architecture, project management, and the ability to work collaboratively with various stakeholders to ensure a seamless transition.
Key Responsibilities:
Develop and execute the de-merger strategy for Mindsprint's IT infrastructure and application landscape.
Assess current IT systems and applications to identify dependencies and potential risks associated with the de-merger.
Design and implement solutions to separate IT systems and applications while ensuring minimal disruption to business operations.
Collaborate with cross-functional teams, including IT, business units, and external partners, to ensure alignment and successful execution of the de-merger plan.
Provide technical leadership and guidance throughout the de-merger process, ensuring best practices and industry standards are followed.
Develop detailed project plans, timelines, and budgets for the de-merger initiative.
Monitor progress, identify issues, and implement corrective actions as needed to keep the project on track.
Ensure compliance with all relevant regulations and standards during the de-merger process.
Communicate effectively with stakeholders at all levels, providing regular updates on project status and addressing any concerns.
Qualifications:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Proven experience in IT architecture, with a focus on de-mergers, mergers, or large-scale IT transformations.
Strong project management skills, with experience leading complex IT projects.
Excellent problem-solving and analytical skills.
Ability to work effectively in a fast-paced, dynamic environment.
Strong communication and interpersonal skills, with the ability to build relationships and influence stakeholders.
Knowledge of relevant regulations and standards related to IT de-mergers.
Preferred Skills:
Experience as a Data Center Architect, Network Architect, and Cloud Architect.
Familiarity with IT security best practices and compliance requirements.
Certification in project management (e.g., PMP, PRINCE2) or IT architecture (e.g., TOGAF).","de-mergers, project management, Pmp, IT transformations, compliance requirements, Prince2, Togaf, it security best practices, IT architecture"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.
Working with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.
Analyzing requirements: Performing requirement analysis and creating architectural models.
Identifying issues: Identifying operational issues and recommending strategies to resolve them.
Communicating with business users: Communicating technical solutions to business users and addressing their questions.
Validating solutions: Ensuring solutions align with corporate standards and compliance requirements.
Developing technical specifications: Creating technical design specifications for solutions and systems engineers.
Assessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.
Data engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.
About Us
We help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Databases, Apis, Web Scraping, Python"
Data Architect / Lead,Veracity Software Inc,10-12 Years,,India,Login to check your skill match score,"Position: FTE, Remote, India
Availability - Immediate -1 week
Candidate proficiency level: Senior(10+)
Responsibilities:
Design, build and maintain core data infrastructure pieces that allow Aircall to support our many data use cases.
Enhance the data stack, lineage monitoring and alerting to prevent incidents and improve data quality.
Implement best practices for data management, storage and security to ensure data integrity and compliance with regulations.
Own the core company data pipeline, responsible for converting business needs to efficient & reliable data pipelines.
Participate in code reviews to ensure code quality and share knowledge.
Lead efforts to evaluate and integrate new technologies and tools to enhance our data infrastructure.
Define and manage evolving data models and data schemas. Manage SLA for data sets that power our company metrics.
Mentor junior members of the team, providing guidance and support in their professional development.
Collaborate with data scientists, analysts and other stakeholders to drive efficiencies for their work, supporting complex data processing, storage and orchestration
A little more about you:
Bachelor's degree or higher in Computer Science, Engineering, or a related field.
10+ years of experience in data engineering, with a strong focus on designing and building data pipelines and infrastructure.
Proficient in SQL and Python, with the ability to translate complexity into efficient code.
Experience with data workflow development and management tools (dbt, Airflow).
Solid understanding of distributed computing principles and experience with cloud-based data platforms such as AWS, GCP, or Azure.
Strong analytical and problem-solving skills, with the ability to effectively troubleshoot complex data issues.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Experience with data tooling, data governance, business intelligence and data privacy is a plus.","Airflow, dbt, Sql, AWS, Python, Azure, Gcp"
Data Architect,Jio,12-15 Years,,"Mumbai, India",Login to check your skill match score,"Company Overview
Jio Platforms Limited is the driving force behind India's leading telecom operator Jio, serving 400M+ customers. We offer digital apps & services, end-to-end 5G solutions, AI/ML platforms, and cloud-native OSS/BSS solutions.
Job Overview
Experienced Data Architect role at Jio Platforms Limited, Mumbai, Bangalore India. Full-Time position with more than 10 years of experience. Join a dynamic company leading India's telecom market with 400M+ customers and innovative digital solutions for B2C and B2B sectors.
Qualifications and Skills
Expertise in Hadoop, Hive, Apache Spark, Python, Kafka, Cloudera
Experience with On-premise and Microsoft Azure environments
Strong data management and data governance skills
Excellent problem-solving and analytical abilities
Effective communication and collaboration skills
Bachelors or masters degree in computer science, Information Technology or related field of study. MCA preferable.
12 - 15 years of overall experience.
Certification MS Azure AZ-304, AZ-303,
6+ years of large-scale software development or application engineering with recent coding experience in two or more of the following: Java, JavaScript, Node.js, .NET, Python, MSSQL.
4+ years of experience as a technical specialist in customer-facing roles.
Experience architecting highly available systems that utilize load balancing, horizontal scalability and high availability.
Good exposure to Agile software development and DevOps practices such as Infrastructure as Code (IaC), Continuous Integration and automated deployment Continuous Integration (CI) tools (e.g. Jenkins).
Strong, in-depth and demonstrable hands-on experience with the following technologies: Microsoft Azure and its relevant build, deployment, automation, networking and security technologies in cloud and hybrid environments.
Exposure to Agile development methodologies and deployment strategies.
Strong practical application development experience on Linux and Windows-based systems.
Excellent knowledge of cloud computing technologies and current computing trends.
Experience working directly with customers, partners or third-party developers.
Effective communication skills (written and verbal) to properly articulate complicated cloud reports to management and other IT development partners.
Positive attitude and a strong commitment to delivering quality work.
Roles and Responsibilities
Design and implement data architecture solutions to meet business needs.
Develop data models, database design, and data migration strategies.
Collaborate with cross-functional teams to ensure data integration and data quality.
Implement data security and compliance measures.
Optimize data infrastructure and performance for scalable solutions.
Architect, design, and develop Products on the Azure platform.
Design and develop solutions for Data Platforms ranging from Batch Data management to real-time data feeds.
Leverage new technology paradigms (e.g., serverless, containers, microservices, Api Management, Data Storage).
Develop solutions for the cloud and for Azure storage.
Design identity & security and data platform solutions.
Design Azure infrastructure strategy.
Work closely with Business Analysts, Product Managers, Data Managers and other team members to ensure successful production of application software.
Work closely with IT security to monitor the company's cloud privacy.
Respond to technical issues in a professional and timely manner.
Offer guidance in infrastructure movement techniques including bulk application transfers into the cloud.
Identify the top cloud architecture solution patterns to successfully meet the strategic needs of the company.
Location: - Mumbai, Bangalore","Java, Hadoop, .NET, Apache Spark, Node.js, Kafka, Windows, Mssql, Hive, Javascript, Linux, Cloudera, Microsoft Azure, Python"
Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.
Role
Design, implement and lead Data Architecture, Data Quality, Data Governance
Defining data modeling standards and foundational best practices
Develop and evangelize data quality standards and practices
Establish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data
Drive the successful adoption of organizational data utilization and self-serviced data platforms
Create and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset
Develop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data
Design data schemes, object models, and flow diagrams to structure, store, process, and integrate data
Provide architectural assessments, strategies, and roadmaps for data management
Apply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms
Implement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD
Translate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models
Define templates and processes for the design and analysis of data models, data flows, and integration
Lead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms
Qualifications
B.S. or M.S. in Computer Science, or equivalent degree
10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting
7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more
Extensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse
Highly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools
Proven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker
Knowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with hands-on experience in Amazon Web Services (AWS)
Strong verbal and written communications skills are a must and should work effectively across internal and external organizations and virtual teams
Demonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies
Strong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem
Deep knowledge of data structures and algorithms
Experience working in large teams using CI/CD and agile methodologies","Airflow, Data Lake Technologies, CI CD, Hive Catalog, Big Data platforms, ML and Data Science platforms, Delta Lake, S3, Kafka, Tableau, Redshift, Sql, Data Quality, Hive, Docker, Spark, Data Architecture, Databricks, Data Governance, Kubernetes, Python, Etl, Aws S3"
Senior Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.
Role:
Design, implement and lead Data Architecture, Data Quality, Data Governance across
Defining data modeling standards and foundational best practices
Develop and evangelize data quality standards and practices
Establish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data.
Drive the successful adoption of organizational data utilization and self-serviced data platforms.
Create and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset
Develop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data.
Design data schemes, object models, and flow diagrams to structure, store, process, and integrate data
Provide architectural assessments, strategies, and roadmaps for data management.
Apply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms.
Implement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD
Translate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models.
Define templates and processes for the design and analysis of data models, data flows, and integration.
Lead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms
Mandatory Qualifications:
Qualifications: B.S. or M.S. in Computer Science, or equivalent degree
10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting.
7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more extensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse.
Highly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools.
Proven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker
Knowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with Amazon Web Services (AWS)
Strong verbal and written communications skills are a must and work effectively across internal and external organizations and virtual teams.
Demonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies.
Strong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem
Deep knowledge of data structures and algorithms.
Experience in working in large teams using CI/CD and agile methodologies.","Airflow, Delta Lake, CI CD, Data Lake Technologies, , Parquet, Big Data platforms, Hive Catalog, ML and Data Science platforms, Docker, Sql, Databricks, Tableau, Kafka, Avro, Etl, Hive, S3, Aws S3, Data Quality, Python, Kubernetes, Data Governance, Spark, Redshift"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Chennai, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata
work Mode- Hybrid
Roles And Responsibilities
Mandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization
Solution Architect for Data modelling Understanding of Enterprise datasets Sales,
Procurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle
etc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building
Data lake foundation, Maintenance etc)
Collaborate with the product/business teams, understand related business processes and
document business requirements and then write high level specifications/requirements for DEs
Develop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout
for right grain of data either in True source systems or in Data WHs and build reusable data
models in intermediary layers before creating physical consumable views from data mart
Understand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data
Governance, Data Quality frameworks, Data Observality and the candidate should be:
Familiar with DevOps process
Knowing how to check existing tables, data dictionary, table structures
Experienced with normalizing tables
Having good understanding of Landing, Bronze, Silver and Gold layers and concepts
Familiar with Agile techniques
Create business process map, user journey map and data flow integration diagrams; Understand
Integration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of
models
Stakeholder management with data engineering, product owners, central data modelling team,
data governance & stewards, Scrum master, project team and sponsor.
Ability to handle large implementation program with multiple projects spanning over an year.
Skills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Data projects, Agile techniques, DataOps, Data Observability, Data Lake, Sql, Data Modeling, Cloud Architecture, Devops, Data Modeler, FTP, Data Warehouse, Data Quality, Data Architect, Azure, Data Governance, Sftp"
Cloud Data Architect,owow,8-16 Years,,"Bengaluru, India",Login to check your skill match score,"Technical Skills:
Skillsets we are looking for:
8 to 16 years of working experience in data engineering.
7+ years exp in PySprak
7+ years exp in AWS Glue
7+ years exp in AWS Redshift
7+ years exp in in AWS CI/CD pipeline like codebuild, codecommit, codedeploy and codepipeline
Strong proficiency in AWS services such as S3, EC2, EMR, SNS, Lambda, StepFunctions
Experience implementing automated testing platforms like PyTest
Strong proficiency in Python, Hadoop, Spark and or PySpark is required
Skill of writing clean, readable, commented and easily maintainable code
Understanding of fundamental design principles for building a scalable solution
Skill for writing reusable libraries
Proficiency in understanding code versioning tools such as Git, SVN, TFS etc.,
Interpersonal skills:
Excellent communication and collaboration skills.
Ability as part of a team.
Strong problem-solving and analytical skills.
Must have worked with US customers and should have provided at least 3-4 hours overlap with Pacific Time (PT)
Bonus points:
Certifications in cloud platforms
Show more Show less","Hadoop, Aws Redshift, Pyspark, Spark, AWS Glue, Python"
Enterprise Data Architect - Ramboll Tech,Ramboll,5-7 Years,,"Chennai, India",Login to check your skill match score,"Company Description
About Ramboll
Founded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.
Equality, diversity, and inclusion are at the heart of what we do
We believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.
Job Description
We're passionate about using data to drive sustainability. You too Join us as an Enterprise Data Architect at Ramboll Tech, and shape our architecture to be data centric.
We are looking for a highly skilled and experienced Data Architect to join our team. You will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth. This role is vital to ensuring the seamless integration, management and deployment of data and analytics solutions that support our business needs. You will identify, analyse and proactively recommend how information assets drive business outcomes, to share consistent data throughout Ramboll.
You will be a part of our exciting digital transformation journey and play an active role in turning our data ambitions into concrete actions to deliver optimisations and revenue growth targets!
What You Will Do
You will join Technology & Data Architecture in Ramboll Tech. You will be working across Ramboll, with focus on enterprise data layer, in collaboration with Domain Enterprise Architects, Data Strategy and Data Platform teams. You will partner with Innovation and Digital Transformation Directorswho drive digitalisation, innovation, and scaling of digital solutions & products across their respective business domains.
As our Enterprise Data Architect, you will play a key role in transforming data into a strategic asset, by ensuring it is well-structured, governed and leveraged effectively for business growth.
Your focus will be on delivering value by shaping data strategies, roadmaps, and solutions that directly address the challenges and opportunities of our business areas. This role requires expertise in modern data management and analysis technologies, alongside a deep understanding of corporate data management best practices.
As we operate in Architecture, Engineering and Consulting (AEC), you can expect a considerable focus on Building Information Modelling (BIM). We are looking for a candidate eager to engage in applying data-centric principles to BIM data to leverage generating valuable business insights.
Where you will make an impact:
Business needs: Translate complex business requirements into robust, scalable, and secure data architectures that are aligned with enterprise-wide data strategy. We want to unlock new revenue streams and optimise operational efficiency, by leveraging our Enterprise Data Architecture to transform our data into valuable business assets, and implement Al-enabled solutions.
Design and implement modern data architectures: Lead the design and implementation of scalable, robust and secure data systems using modern data stack technologies. Ensure architecture is aligned with business objectives.
Improve and ensure the overall quality of data, including ease of access and use of data assets. Define information and data strategies as a part of an overall business and technology strategy. Design and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.
Support data platform teams: Work with data platform teams to design and implement operating models that support efficient data processing, integrations, reporting and AI model deployment and execution.
Data modelling: Design and develop data models that support business processes, analytics and reporting requirements. Ensure that data models are optimised for performance and scalability.
Technology roadmaps: Actively engage with technology providers to understand critical, data relevant, technology roadmaps. Explore innovative solutions and evaluate their alignment with business goals and enterprise-wide data strategy
Key responsibilities:
Work with solution architects, developers and engineers to operationalise Data Strategy and build robust and scalable technology stacks and platforms for D&A solutions
Partner with our Domain Enterprise Architects to understand business objectives. Develop and manage cloud-based data solutions on platforms such as Microsoft Azure, GCP or AWS. Optimise cloud infrastructure for performance, cost and scalability.
Design and implement integration solutions that connect disparate business systems and data sources to ensure seamless data flow across the organisation.
Work closely with cross-functional teams, including business stakeholders, data scientists, data engineer and other SME colleagues in Ramboll Tech, to understand data requirements and deliver solutions that meet business needs.
Maintaining repositories for representing the data elements including the entities, relationships and attributes, the information flows, and business glossary
Provide mentorship and guidance to junior data engineers and other team members. Foster a culture of continuous learning and improvement within the team
Continuously evaluate and recommend new tools and technologies that can improve the efficiency and effectiveness of data engineering processing
Coach and mentor other architects, product teams and business stakeholders to instil architectural thinking, with focus on business, data/information and solution architecture.
Effectively provide guiding principles, standard and minimal viable architectures, technology governance model informed by business strategy and corporate governance.
Qualifications
Education:
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Experience:
5+ years of professional experience in data architecture, with a strong focus on cloud architecture, data integration and data modelling.
Deep understanding of the modern data stack and its components.
Proven experience with cloud platforms such as Microsoft Azure, GCP or AWS.
Previous engagement in establishing data strategy and data governance.
Experience in leading data modelling, database design, data warehousing, master data management, data governance practices. Strong skills in data modelling, ETL processes and data integration, having worked with data platform teams.
Experience in securing and protection of sensitive information
Experience in AEC industry is great to have. An ideal candidate will have a strong understanding of Building Information Modelling (BIM) principles and applications. Applied data-centric principles to BIM data, enabling integration with enterprise systems (e.g., improved project management, cost control, and facilities management). Developed data pipelines to extract, transform, and load BIM data for analysis and reporting. Explored opportunities to leverage BIM data for generating valuable insights, predictive and prescriptive maintenance
Skills:
Exceptional analytical and problem-solving skills with the ability to design innovative solutions to complex data challenges.
Data Modelling (Conceptual, Logical, Physical), Data Warehousing, Data Lakes, Data Mesh, Data Governance, Metadata Management, Master Data Management.
Familiarity with the DAMA Data Management Framework (DAMA-DMBOK) and its application in managing data as a strategic asset
Understanding of BIM principles, data structures (IFC), BIM software APIs, and BIM data integration with enterprise systems. (Great to have skill)
Advanced knowledge of data modelling tools, data lakes, SQL, and pipeline design
Excellent communication and interpersonal skills, with the ability to convey technical concepts to non-technical stakeholders
A strategic mindset with the ability to navigate and influence within a matrixed organization
Proven ability to lead projects and mentor junior team members
Relevant certifications in cloud technologies are a plus, such as: Microsoft Certified Azure Solutions Architect Expert, or Data Engineering certifications, Snowflake certifications, Databricks certifications, Kafka certifications
We encourage you to apply even if your profile does not meet all the requirements for the role. We are looking for a diverse range of experiences, skills, and interests to enrich our team.
Additional Information
What defines us: CURIOSITY, OPTIMISM, AMBITION, EMPATHY
Our team at Ramboll Tech is currently on a steep growth trajectory while maintaining a strong team culture.
We are curious about other people and their motivations; about new business models and technologies; about each other and the future.
We are optimistic, focusing on solutions rather than problems; we plan for success and are willing to take calculated risks instead of playing it safe.
We are ambitious, setting our own standards higher than others expectations, and we celebrate each other's successes.
We are empathetic, taking ourselves, others, and each other seriously without prejudgment, and we help each other and our clients, colleagues, and the world.
How We Work As a Team
Our team culture is crucial to us; that's why we take time daily to exchange ideas and discuss our work priorities. We support each other when facing challenges and foster a strong team spirit. We aim to learn and grow continuously from one another. We value diversity, and although we're not perfect, we regularly engage in open discussions about how we can improve in this area.
Our current hybrid work approach focuses on adapting to different needs, including increased flexibility that works best for our global team and individuals, with as much autonomy as possible.
Who Is Ramboll
Ramboll is a global architecture, engineering, and consultancy firm. We believe sustainable change's aim is to create a livable world where people thrive in healthy nature. Our strength is our employees, and our history is rooted in a clear vision of how a responsible company should act. Openness and curiosity are cornerstones of our corporate culture, fostering an inclusive mindset that seeks new, diverse, and innovative perspectives. We respect and welcome all forms of diversity and focus on creating an inclusive environment where everyone can thrive and reach their full potential.
What Does Ramboll Tech Do
Ramboll Tech / GBA Innovation and Digital Transformation accelerates innovation and digital transformation for the entire Ramboll Group/ GBA and directs all AI initiatives within the company. We digitally enable and co-create with Ramboll's world-class experts to deliver solutions that drive sustainable change for our clients and society. This includes collaborating with Global Business Areas and Enabling Functions at Ramboll on their digital journey, developing proprietary AI and digital products for Ramboll and our clients, following our product life cycle. Ramboll Tech also works on larger technology projects within Ramboll and provides world-class Technology Operations. Ramboll Tech currently has over 300 employees globally, with strongholds across Nordics, Germany, the USA, and India. We are looking to quickly expand in key areas across Europe and the globe.
Important Information
We don't require a cover letterjust send us your current CV through our application tool, and we're eager to get to know you better in a conversation.
Do you have any questions Feel free to contact Head of Technology & Data Architecture: Elvira Janas ([HIDDEN TEXT]). Thanks","pipeline design, Data Lakes, ETL processes, BIM principles and applications, Master Data Management, Data Modelling, Data Integration, Data Warehousing, Data Governance, Sql"
Senior Lead - Data Architect,KK Wind Solutions,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"An Azure Data Architect is a strategic role in the modern data-driven enterprise. Responsible for designing and implementing effective database solutions and models to store and retrieve company data, the Azure Data Architect ensures that enterprise data is efficiently stored, managed, and available to use in decision-making across the organization.
Responsibilities:
Design and implement data architecture solutions on the Azure platform to support company data initiatives
Develop and optimize data models for transactional and analytical systems, ensuring they align with business objectives
Build and manage ETL/ELT workflows using tools such as Azure Data Factory, Databricks to ingest, transform, and integrate data across various sources
Architect and implement Azure Synapse Analytics or other data warehousing solutions for business intelligence and reporting
Implement data governance practices to ensure data quality, consistency, and security
Monitor and optimize the performance of Azure-based data solutions to ensure high availability and responsiveness
Work closely with business stakeholders to understand their data needs and translate them into technical requirements
s
Our Requirement:
Minimum of 5 years of experience in data architecture or a related field
Strong knowledge of Azure data services with hands-on experience, including Azure SQL Database and Azure Data Lake
Expertise in data integration tools and techniques, such as Azure Data Factory and Azure Databricks
Proficiency in programming and scripting languages, such as SQL, Python, and PowerShell
Relevant certifications in Azure, such as Microsoft Certified: Azure Solutions Architect","Azure SQL Database, Azure Data Factory, PowerShell, Azure Data Lake, Azure Databricks, Sql, Python"
Data Architect,Datacrew.ai,8-10 Years,,"Chennai, India",Login to check your skill match score,"We are looking for a highly skilled Data Architect with a strong background in designing, implementing, and managing modern data architectures. The ideal candidate should have deep expertise in data modeling, data integration, data warehousing, and cloud data platforms, along with hands-on experience in SQL, Python, and data engineering tools. This role will be responsible for defining and implementing scalable data solutions, ensuring high performance, security, and data governance compliance.
Location: Chennai, Tamil Nadu
Mode: WFO
Experience: 8+ yrs
Immediate Joiners are only preferred
Key Responsibilities:
Design and implement end-to-end data architectures, including OLTP, OLAP, data lakes, and data warehouses.
Develop and maintain data models, metadata management, and data cataloging frameworks.
Define data integration strategies using ETL/ELT pipelines for structured and unstructured data.
Work closely with data engineers and business teams to ensure data availability, consistency, and quality across all platforms.
Establish and enforce data governance policies and security standards.
Optimize query performance and data storage for efficiency in processing large datasets.
Evaluate and implement modern cloud-based data solutions (AWS, Azure, GCP) as per business requirements.
Collaborate with data scientists, analysts, and software engineers to enable AI/ML and advanced analytics use cases.
Support real-time data processing and streaming architectures for high-velocity data environments.
Troubleshoot data architecture issues and provide solutions to enhance data workflows.
Required Skills & Qualifications:
Technical Skills:
Strong proficiency in SQL (T-SQL, PL/SQL, or PostgreSQL) for complex queries and performance tuning.
Hands-on experience in Python for data engineering, automation, and analytics tasks.
Expertise in data modeling (conceptual, logical, physical) and database design.
Experience with data warehousing solutions (Snowflake, Redshift, BigQuery, Synapse, etc.).
Solid understanding of ETL/ELT tools (dbt, Talend, Apache NiFi, Data Factory, Glue, Airflow).
Experience with big data processing frameworks (Spark, Databricks, Hadoop).
Proficiency in cloud data platforms (AWS, Azure, GCP) and serverless architectures.
Knowledge of real-time data streaming tools (Kafka, Kinesis, Pulsar).
Experience with NoSQL databases (MongoDB, Cassandra) and graph databases is a plus.
Strong understanding of data security, access control, and compliance regulations (GDPR, HIPAA, NDMO in Saudi Arabia, etc.).
Soft Skills:
Excellent problem-solving and analytical skills.
Strong communication skills with the ability to translate technical concepts to business stakeholders.
Ability to lead and mentor data engineers and collaborate across teams.
Strong documentation and architectural planning skills.
Ability to manage multiple priorities and work in a fast-paced environment.
Education & Experience:
Bachelor's/Master's degree in Computer Science, Information Systems, Data Engineering, or a related field.
8+ years of experience in data architecture, database design, and data engineering.
Hands-on experience in designing and implementing enterprise-scale data solutions.
Certifications in cloud platforms (AWS Certified Data Analytics, Azure Data Engineer, GCP Professional Data Engineer) are a plus.
Preferred Qualifications (Nice to Have):
Experience with DataOps and MLOps frameworks.
Hands-on experience in metadata management and data lineage tools (Collibra, Alation).
Exposure to graph databases and semantic web technologies.
Prior experience in data mesh and decentralized data architectures","Pulsar, Gdpr, Hadoop, Data Modeling, Cassandra, Hipaa, Kafka, Sql, ELT, Kinesis, Gcp, Spark, graph databases, Data Warehousing, Databricks, Data Security, MongoDB, Azure, Python, AWS, Etl"
"Data Architect (Financial Crime), Director",NatWest Group,Fresher,,"Bengaluru, India",Login to check your skill match score,"Join us as a Data Architect (Financial Crime)
For someone with a background in defining business and application architectures and roadmaps for complex enterprises, this is an excellent opportunity to join our business. You'll be defining the intentional architecture foryour assigned scope in order to ensure that the current architecture being delivered by engineers best supports the enterprise and its long term strategy
You'll provide advisory support and embed governance to ensure projects align to our simplification strategy and comply with data standards while promoting and supporting effective data modelling, metadata management and alignment to enterprise data model and data controls
With valuable exposure, you'll be building and leveraging relationships with colleagues across the bank to ensure commercially focused decisions and to create long term value for the bank
We're offering this role at director level
What you'll do
As a Data Architect, you'll be defining and communicating the current, resultant and target state data architecture for your assigned scope. You'll advise domain and project teams to ensure alignment with data architecture standards, data principles, data controls, target data architectures, data patterns and the banks simplification strategy.
We'll look to you to influence the development of business strategies at an organisational level, identifying transformational opportunities for Financial Crime associated with both new and existing data solutions.
As Well As This, You'll Be
Translating architecture roadmaps into packages of work that allow frequent incremental delivery of value to be included in product backlog
Defining, creating and maintaining architecture models, roadmaps, standards and outcomes, using architecture strategies to ensure alignment to adjacent and higher level model
Collaborate and support data management SMEs and data stewards ensuring metadata management aligns with enterprise data model
Conduct data design reviews to ensure solutions meet data standards, data principles, data controls, data patterns and target data architectures
Seeking out and utilising continuous feedback, fostering adaptive design and engineering practices to drive the collaboration of programmes and teams around a common technical vision
The skills you'll need
To succeed in this role, you'll need expert knowledge of application architecture, and at least one business, data or infrastructure architecture with working knowledge of the remaining disciplines. You'll have excellent communication skills with the ability to clearly communicate complex technical concepts to colleagues, up to senior leadership level, along with a good understanding of Agile methodologies with experience of working in an Agile team.
You'll Also Demonstrate
Good collaboration and stakeholder management skills
Experience of developing, syndicating and communicating architectures, designs and proposals for action
An understanding of industry architecture frameworks, such as TOGAF and ArchiMate
Experience in data modelling methodologies (3NF, Dimensional, Data Vault, Star Schema, etc). Knowledge of data related regulations such as GDPR, CCPA,PCI DSS, BCBS 239
Experience of working with business solution vendors, technology vendors and products within the market
A background in systems development change life cycles, best practices and approaches
Knowledge of hardware, software, application and systems engineering","BCBS 239, Systems engineering, archimate, CCPA, data modelling methodologies, Pci Dss, Gdpr, Togaf, Agile Methodologies, Application Architecture, Infrastructure Architecture"
Data Architect,Impetus,10-12 Years,,"Indore, India",Login to check your skill match score,"Qualifications
Degree Graduates/Postgraduate in CSE or related field
looking for candidates with hands on experience in Big Data and Cloud Technologies.
10+ Years of experience Expertise in designing and developing applications using Big Data and Cloud technologies Must Have
Expertise and hands-on experience* on Spark, and Hadoop echo system components Must Have
Expertise and hand-on experience* of any of the Cloud (AWS/Azure/GCP) Must Have
Good knowledge of Shell script & Java/Python Must Have
Good knowledge of migration projects on Hadoop Good to Have
Good Knowledge of one of the Workflow engines like Oozie, Autosys Good to Have
Good knowledge of Agile Development Good to Have
Passionate about exploring new technologies Must Have
Automation approach - Good to Have
Responsibilities
Define Data Warehouse modernization approach and strategy for the customer
Align the customer on the overall approach and solution
Design systems for meeting performance SLA
Resolve technical queries and issues for team
Work with the team to establish an end-to-end migration approach for one use case so that the team can replicate the same for other iterations","Java, Hadoop, Big Data, Autosys, Cloud Technologies, Gcp, Spark, Shell script, Oozie, Azure, Python, AWS"
Head of Data Architect,Azilen Technologies,10-12 Years,,"Ahmedabad, India",Login to check your skill match score,"Job Purpose:
We are seeking an experienced and visionary Head of Data to lead our data strategy, architecture, and engineering initiatives. This role requires a strong technical leader with hands-on expertise in data architecture, data engineering, and enterprise data solutions. The ideal candidate will have deep experience in designing and implementing large-scale data platforms, data warehousing, and big data solutions while also demonstrating the ability to build and manage high-performing data teams.
Who you are:
10+ years of experience in data architecture, data engineering, or related roles, with at least 3 years in a leadership capacity.
Strong hands-on expertise in big data technologies (Hadoop, Spark, Kafka, Flink, etc.).
Deep knowledge of data warehousing solutions (Databricks, Snowflake, Redshift, BigQuery, Synapse, etc.).
Experience with cloud platforms (AWS, Azure, GCP) and hybrid data architectures.
Proven ability to design and implement ETL/ELT processes and data pipeline automation.
Strong background in SQL, NoSQL, and Graph Databases.
Knowledge of data governance, security, and compliance frameworks.
Exposure to AI/ML-driven data solutions and MLOps practices is a plus.
Leadership & Client Management
Demonstrated experience in managing data teams and driving data strategy.
Proven track record of leading enterprise-level data projects from ideation to execution.
Strong stakeholder management, with the ability to engage with C-level executives and technical teams.
Excellent problem-solving, analytical, and communication skills.
Ability to handle ambiguity and drive solutions in fast-paced environments.
Preferred Qualifications:
Master's or Bachelor's degree in Computer Science, Data Engineering, or a related field.
Industry certifications (AWS Certified Data Analytics, Google Professional Data Engineer, Snowflake Architect, etc.).
Experience in data mesh, data fabric, or real-time analytics is a plus.
What will excite us :
Strategic Leadership & Architecture
Define and execute the overall data strategy, ensuring alignment with business goals and client needs.
Architect and implement scalable enterprise data solutions, including but not limited to big data, data warehousing, and real-time data streaming solutions.
Lead data governance, security, and compliance efforts to ensure data integrity and adherence to regulatory requirements.
Drive modernization efforts, including cloud migration, data lake implementations, and AI-driven analytics platforms.
Serve as a trusted advisor to clients, understanding their business challenges and translating them into scalable data solutions.
Lead client workshops, technical deep-dives, and solutioning sessions, ensuring innovative and tailored recommendations.
Independently engage with mid-size to enterprise clients to assess data maturity, define roadmaps, and execute digital transformation initiatives.
Technical Expertise & Hands-on Execution
Provide hands-on expertise in designing and implementing data platforms using cloud and on-premise technologies (AWS, Azure, GCP, Hadoop, Spark, Snowflake, Databricks, etc.).
Establish best practices for ETL/ELT processes, data pipeline automation, and data lake architectures.
Oversee data modeling, database optimization, and performance tuning for high-scale environments.
Lead the integration of AI/ML capabilities into data platforms to drive actionable insights.
Hire, mentor, and manage a high-performing team of data engineers, architects, and analysts.
Foster a culture of innovation, continuous learning, and collaboration within the data team.
Define team goals, measure performance, and drive efficiency improvements.
What will excite you:
Opportunity to lead and shape the Data & AI practice in a fast-growing technology organization.
Work on cutting-edge data solutions for enterprise clients across diverse industries.
Collaborative and innovative culture with a focus on continuous learning.
Competitive compensation, benefits, and career growth opportunities.
Job Location:
Ahmedabad - WFO","Flink, MLOps practices, data pipeline automation, snowflake, cloud platforms, Ai, ML-driven data solutions, Synapse, security and compliance frameworks, Kafka, ELT, Nosql, AWS, BigQuery, Hadoop, Redshift, Sql, Gcp, Spark, Data Governance, graph databases, Databricks, Azure, Etl"
Data Architect,McCain Foods,8-10 Years,,"Delhi, India",Login to check your skill match score,"Position Title: Data Architect
Position Type: Regular - Full-Time
Position Location: New Delhi
Grade: Grade 05
Requisition ID: 34658
Job Purpose
Reporting to the Data Architect Lead, Global Data Architect will take a lead role in creating the enterprise data model for McCain Foods, bringing together data assets across agriculture, manufacturing, supply chain and commercial. This data model will be the foundation for our analytics program that seeks to bring together McCain's industry-leading operational data sets, with 3rd party data sets, to drive world-class analytics. Working with a diverse team of data governance experts, data integration architects, data engineers and our analytics team including data scientist, you will play a key role in creating a conceptual, logical and physical data model that underpins the Global Digital & Data team's activities. .
Job Responsibilities
Develop an understanding of McCain's key data assets and work with data governance team to document key data sets in our enterprise data catalog
Work with business stakeholders to build a conceptual business model by understanding the business end to end process, challenges, and future business plans.
Collaborate with application architects to bring in the analytics point of view when designing end user applications.
Develop Logical data model based on business model and align with business teams
Work with technical teams to build physical data model, data lineage and keep all relevant documentations
Develop a process to manage to all models and appropriate controls
With a use-case driven approach, enhance and expand enterprise data model based on legacy on-premises analytics products, and new cloud data products including advanced analytics models
Design key enterprise conformed dimensions and ensure understanding across data engineering teams (including third parties); keep data catalog and wiki tools current
Primary point of contact for new Digital and IT programs, to ensure alignment to enterprise data model
Be a clear player in shaping McCain's cloud migration strategy, enabling advanced analytics and world-leading Business Intelligence analytics
Work in close collaboration with data engineers ensuring data modeling best practices are followed
Measures Of Success
Demonstrated history of driving change in a large, global organization
A true passion for well-structured and well-governed data; you know and can explain to others the real business risk of too many mapping tables
You live for a well-designed and well-structured conformed dimension table
Focus on use-case driven prioritization; you are comfortable pushing business teams for requirements that connect to business value and also able to challenge requirements that will not achieve the business goals
Developing data models that are not just elegant, but truly optimized for analytics, both advanced analytics use cases and dashboarding / BI tools
A coaching mindset wherever you go, including with the business, data engineers and other architects
A infectious enthusiasm for learning: about our business, deepening your technical knowledge and meeting our teams
Have a get things done attitude. Roll up the sleeves when necessary; work with and through others as needed
Key Qualification & Experiences
Data Design and Governance
At least 5 years of experience with data modeling to support business process
Ability to design complex data models to connect and internal and external data
Nice to have: Ability profile the data for data quality requirements
At least 8 years of experience with requirement analysis; experience working with business stakeholders on data design
Experience on working with real-time data.
Nice to have: experience with Data Catalog tools
Ability to draft accurate documentation that supports the project management effort and coding
Technical skills
At least 5 years of experience designing and working in Data Warehouse solutions building data models; preference for having S4 hana knowledge.
At least 2 years of experience in visualization tools preferably Power BI or similar tools.e
At least 2 years designing and working in Cloud Data Warehouse solutions; preference for Azure Databricks, Azure Synapse or earlier Microsoft solutions
Experience Visio, Power Designer, or similar data modeling tools
Nice to have: Experience in data profiling tools informatica, Collibra or similar data quality tools
Nice to have: Working experience on MDx
Experience in working in Azure cloud environment or similar cloud environment
Must have : Ability to develop queries in SQL for assessing , manipulating, and accessing data stored in relational databases , hands on experience in PySpark, Python
Nice to have: Ability to understand and work with unstructured data
Nice to have at least 1 successful enterprise-wide cloud migration being the data architect or data modeler. - mainly focused on building data models.
Nice to have: Experience on working with Manufacturing /Digital Manufacturing.
Nice to have: experience designing enterprise data models for analytics, specifically in a PowerBI environment
Nice to have: experience with machine learning model design (Python preferred)
Behaviors and Attitudes
Comfortable working with ambiguity and defining a way forward.
Experience challenging current ways of working
A documented history of successfully driving projects to completion
Excellent interpersonal skills
Attention to the details.
Good interpersonal and communication skills
Comfortable leading others through change
McCain Foods is an equal opportunity employer. We see value in ensuring we have a diverse, antiracist, inclusive, merit-based, and equitable workplace. As a global family-owned company we are proud to reflect the diverse communities around the world in which we live and work. We recognize that diversity drives our creativity, resilience, and success and makes our business stronger.
McCain is an accessible employer. If you require an accommodation throughout the recruitment process (including alternate formats of materials or accessible meeting rooms), please let us know and we will work with you to meet your needs.
Your privacy is important to us. By submitting personal data or information to us, you agree this will be handled in accordance with the Global Employee Privacy Policy
Job Family: Information Technology
Division: Global Digital Technology
Department: Data Architect
Location(s): IN - India : Haryana : Gurgaon
Company: McCain Foods(India) P Ltd","Machine Learning Model Design, MDx, Data Catalog Tools, Data Profiling Tools, Data Warehouse Solutions, Data Modeling, Collibra, Power Bi, Pyspark, S4 Hana, Azure Databricks, Informatica, Sql, Cloud Migration, Azure Synapse, Data Governance, Python"
Data Architect,ACL Digital,10-12 Years,,"Pune, India",Login to check your skill match score,"Exp: 10+ yrs
Location: Balewadi, Pune
Notice Period: Immediate to 15 Days
Responsibilities
Strong understanding of Data Architecture and models and experience leading data driven projects.
Solid expertise with strong opinions on Data Modelling paradigms such as Kimball, Inmon, Data Marts, Data Vault, Medallion etc.
Strong experience with Cloud Based data strategies and big data technologies AWS Preferred. Ability to create backend services in Python that enables the data pipelines is required.
Demonstrated experience on designing data platforms on AWS for batch and stream processing pipelines.
Hands-on experience using AWS Managed and other big data services such as EMR, Glue, S3, Kinesis, DynamoDB, ECS is a must.
Strong understanding of working of Apache Spark is a must.
Strong understanding of various Data Lake/Lakehouse storage formats such as Delta, Iceberg, Hudi.
Experience designing data lakehouse with Medallion architecture is desirable.
Solid experience in designing data pipelines for ETL with expert knowledge on ingestion, transformation, and data quality is a must.
Hands-on experience in SQL is a must.
Expertise designing ETL pipelines combining Python + SQL is required.
Understanding of data manipulation libraries in python like Pandas, Polars, DuckDB is desired.
Experience in designing the Data visualization with different tools such as Tableau and PowerBI is desirable.
Working knowledge of other Data Platforms on Azure, Databricks, Snowflake is desirable but not must.","data manipulation libraries, snowflake, DuckDB, Glue, Lakehouse, Polars, data pipelines, Delta, Hudi, Iceberg, Cloud Based data strategies, Big Data Technologies, Databricks, Sql, Emr, Dynamodb, Data Lake, Tableau, Pandas, Etl, ECS, S3, Data Architecture, Apache Spark, Kinesis, AWS, Powerbi, Python, Azure, Data Modelling"
Data Architect,CDK Global,10-12 Years,,"Pune, India",Login to check your skill match score,"Position Scope
A Senior Data Engineer with Architect level experience and exposure
May lead project teams or project phases of larger scope
Works independently with minimal guidance and direction
Impacts a range of customer, operational, project or service activities within own team and related work teams
Contributes to the development of concepts, methods, and techniques
Moderate impact on the functional/business unit
Functional Knowledge
Requires in-depth knowledge of principles, concepts within own function/specialty and basic knowledge of other related areas.
Applies broader knowledge of industry standards/practices to assignments
Problem Solving & Critical Thinking
Solves variety of problems of moderately complex or unusual within own area
Applies independent judgement to develop creative and practical solutions based on the analysis of multiple factors
Anticipates and identifies problems and issues
Leadership
Guided by area goals and objectives
May provide technical direction to others around the completion of short-term work goals
Collaboration
Trains and guides others in work area on technical skills
Networks with senior colleagues in own area of expertise
Education & Experience
Bachelor's degree in computer science, Engineering, or related field with at least 10-12 years of experience or a Masters degree; OR in lieu of a Bachelor's degree, at least 12-14 years of experience
Good Experience with Looker, Snowflake, Postgres, Azure, AWS, Terraform, Kafka, SQL Server, CI/CD, CDC
Strong in Data warehousing using SQL Server and strong TSQL
Understanding of utilizing Agile software development methodologies
Deep knowledge of at least one programming language along with ability to execute on complex programming tasks.
Ability to document, track and monitor a problem/issue to a timely resolution
Knowledge of operating systems
Collaborative problem-solving ability and self-motivated
Strong verbal and written communication skills along with prioritization of duties
At CDK, we believe inclusion and diversity are essential in inspiring meaningful connections to our people, customers and communities. We are open, curious and encourage different views, so that everyone can be their best selves and make an impact.
CDK is an Equal Opportunity Employer committed to creating an inclusive workforce where everyone is valued. Qualified applicants will receive consideration for employment without regard to race, color, creed, ancestry, national origin, gender, sexual orientation, gender identity, gender expression, marital status, creed or religion, age, disability (including pregnancy), results of genetic testing, service in the military, veteran status or any other category protected by law.
Applicants for employment in the US must be authorized to work in the US. CDK may offer employer visa sponsorship to applicants.","cdc, snowflake, Looker, CI CD, Terraform, Postgres, SQL Server, Kafka, Tsql, Azure, AWS"
Data Architect,NewVision Software,Fresher,,"Pune, India",Login to check your skill match score,"Primary Skills:
Data Engineering using Azure stack - Data Handling, Data Modeling, Data Integration, Data Governance
Azure Data Lake Analytics,
Azure Synapse Analytics Engineering,
Azure Data Factory,
Azure Databricks,
Azure Dataflows,
Power BI(Expert in DAX)
Scala or Python,
Pyspark, Hadoop, Spark, Hive, Kafka, Sqoop
T-SQL,
NoSQL,
Certified Azure Solution Architect
Secondary Skills:
Source code control systems such as GIT, AzureDevops
DevOps Basics.
Key Responsibilities:
1. Designing Solutions related to data to handle data silos of our customers.
2. Creating Data Models as part of solutions to suffice customer's needs while following best practices.
3. Handling Data Integration with structured and Unstructured sources of data.
4. Implementing Data Governance and security on top of the existing DWH to enhance reliability.
5. Monitoring the existing data flows and maintaining them thereby helping to resolve bugs.
6. Collaborating with a team whenever needed and providing business intelligence solutions.
7. Actively participating in PI Planning and assisting the team during Sprints.
8. Work as part of a team to develop Cloud Data and Analytics solutions.","Power BI Expert in DAX, Certified Azure Solution Architect, DevOps Basics, AzureDevops, Scala or Python, Azure Synapse Analytics Engineering, Azure Data Lake Analytics, Azure Dataflows, Pyspark, T-sql, Hadoop, Kafka, Azure Databricks, Nosql, Git, Azure Data Factory, Hive, Sqoop, Spark"
Salesforce Data Cloud Architect,NTT DATA North America,Fresher,,"Hyderabad, India",Login to check your skill match score,"NTT DATA strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Salesforce Data Cloud Architect to join our team in Hyderabad, Telangana, India.
Salesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.
Data Modeling: Strong experience in designing and implementing data models.
Data Integration: Experience with data integration tools and techniques.
Data Quality: Understanding of data quality concepts and practices.
Data Governance: Knowledge of data governance principles and practices.
SQL: Proficiency in SQL for data querying and manipulation.
Problem-Solving: Strong analytical and problem-solving skills.
Communication: Excellent communication and collaboration skills.
#Salesforce
About NTT DATA
NTT DATA is a $30 billion trusted global innovator of business and technology services. We serve 75% of the Fortune Global 100 and are committed to helping clients innovate, optimize and transform for long term success. As a Global Top Employer, we have diverse experts in more than 50 countries and a robust partner ecosystem of established and start-up companies. Our services include business and technology consulting, data and artificial intelligence, industry solutions, as well as the development, implementation and management of applications, infrastructure and connectivity. We are one of the leading providers of digital and AI infrastructure in the world. NTT DATA is a part of NTT Group, which invests over $3.6 billion each year in R&D to help organizations and society move confidently and sustainably into the digital future. Visit us at us.nttdata.com
NTT DATA endeavors to make https://us.nttdata.com accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at https://us.nttdata.com/en/contact-us . This contact information is for accommodation requests only and cannot be used to inquire about the status of applications. NTT DATA is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. For our EEO Policy Statement, please click here . If you'd like more information on your EEO rights under the law, please click here . For Pay Transparency information, please click here .","Salesforce Data Cloud, Data Quality, Data Modeling, Data Governance, Sql, Data Integration"
Sr. Data Architect - Snowflake,Arting Digital,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect - Snowflake
Experience: 12+ years
Budget: Up to 38 LPA
Notice Period: Immediate to 30 days
Location: Trivandrum, Bangalore, Chennai
Education: Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Skill Set: Snowflake experience, Data Architecture experience, ETL process experience, Large Data migration solutions experience , data modelling, schema design DBT , Python/Java/Scala, SQL, ETL process , cloud data warehousing concept , data integration , AWS, Azure, and GCP , CDC , DataOps methodologies , cloud platforms/Snow-flak , data visualisation tools (e.g., Tableau, Power BI , data security and compliance standard
Job Description:
We are seeking an experienced Senior Data Architect Snowflake to lead and design scalable, high-performance data solutions. The ideal candidate should have extensive expertise in data architecture, large-scale data migration, ETL processes, and cloud-based data warehousing. You will play a key role in designing optimized data models, ensuring efficient data integration, and implementing best practices for Snowflake and other cloud platforms.
Key Responsibilities:
Architect, design, and implement scalable Snowflake-based data solutions.
Develop data models, schema designs, and ETL pipelines to support business requirements.
Lead large-scale data migration projects while ensuring performance optimization.
Implement DataOps methodologies for efficient data management and automation.
Work with AWS, Azure, and GCP to deploy cloud-based data architectures.
Ensure data security and compliance with industry standards.
Optimize CDC (Change Data Capture) processes for real-time data updates.
Utilize DBT, Python, Java, or Scala to enhance data transformation and integration.
Design and implement data visualisation solutions using tools like Tableau and Power BI.
Required Skills & Expertise:
Strong expertise in Snowflake and cloud data warehousing concepts.
Hands-on experience with ETL processes, data modelling, and schema design.
Proficiency in SQL, DBT, Python, Java, or Scala for data transformation and automation.
Experience in data integration and large-scale data migration solutions.
Knowledge of CDC (Change Data Capture) methodologies.
Familiarity with DataOps practices and modern data engineering workflows.
Exposure to AWS, Azure, and GCP cloud platforms.
Strong understanding of data security, governance, and compliance.
Experience with data visualisation tools such as Tableau or Power BI.","cdc, Data security and compliance, DataOps methodologies, snowflake, dbt, ETL processes, Cloud data warehousing, Large Data migration solutions, Java, Scala, Schema Design, Sql, Gcp, Data Integration, Data Architecture, Data Modelling, Azure, Python, AWS"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
What You'll Do
Be part of a team that is transforming BCG into a bionic company! We are building a centralized Business Intelligence & Analytics function that will simplify and automate information deliveryproviding advanced insights and analysis to support decision making. To date, the team has launched and operationalized several global scale products and dashboards, enhancing how our leaders engage in information to manage the business. The next wave of digital reporting is underway which will help to unlock further value for BCG functions and leadership with best-in-class business intelligence and analytics.
The Global IT Data Architect Sr. Manager works as an integral part of an Agile team delivering scalable digital reporting solutions for global stakeholders. This role will help design optimal solutions to support the digital product portfolio. Your responsibilities include: -
Define data architecture for solutions BI&A reporting.
Evaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.
Collaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing PoCs to ensure those standards are implemented
Provide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.
Develop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.
Create and maintain conceptual / logical data models to identify key business entities and visual relationships.
Work with business and IT teams to understand data requirements.
Maintain a data dictionary consisting of table and column definitions.
Review data models with both technical and business audiences.
Partner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.
Lead to design / build new models to efficiently deliver the financial results to senior management.
What You'll Bring
Bachelor's degree or equivalent combination of education and experience.
Bachelor's degree in information science, data management, computer science or related field preferred.
12+ years IT experience, including 8+ years in ELT/Data Integration for BI.
Experience of working on large transformational program with end-to-end ownership of implementation.
Hands-on implementation experience with Data Warehousing including design, modelling, testing, security, administration and optimization.
Expertise in cloud databases like Snowflake/RedShift, data catalogue, MDM etc.
Expertise in writing SQL and database procedures.
Proficient in Data Modelling- Conceptual, logical, and Physical modelling
Proficient in documenting all the architecture related work performed.
Experience in data storage, ETL/ELT and data analytics tools and technologies e.g., Snowflake, DBT, Tableau, Power BI, etc
Experienced in Data Warehousing design/development and BI/ Analytical systems
Experience working projects using Agile methodologies
Strong hands-on experience with data and analytics data architecture, solution design, and engineering experience
Experience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake
Experience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs)
Excellent written, oral communication and presentation skills to present architecture, features, and solution recommendations
Experience in all aspects of Agile SDLC, and end to end participation in a project lifecycle
Experience in Python, Data frames is a plus
Experience within GenAI space is a plus
Reporting tool experience Tableau, PowerBI, SAP BO is a plus
Who You'll Work With
Working closely with the rest of the Business Intelligence delivery teams, the product owner, Analyst, and Engineers. Architect will also work with external developers, business contacts across BCG functions, DBA's, and Infrastructure teams.
Additional info
YOU'RE GOOD AT
This Position Will Involve Daily Collaboration With The Architect And Other Development Teams, Vendors And Stakeholders Throughout Agile Design, Plus The Development, Implementation And Operations Of Both Infrastructure And Business Mappings. The Successful Candidate Will Demonstrate
Strong analytical abilities and creative problem solving
Ability to work independently with general direction and flexibility in a fast-paced environment
Good organization and excellent communication skills across cultures
Integrity and a positive attitude, especially while handling stressful situations
Work with project stakeholders (technical as well as end users) to understand business requirements and implement database solutions for diverse problems
Research viable technical and/or non-technical solutions, evaluate new technology and advocate, influence, and build consensus for innovations that satisfy business needs
Design, document & train the team on the overall processes and process flows for the Data architecture.
Resolve technical challenges in critical situations that require immediate resolution.
Develop relationships with external stakeholders to maintain awareness of data and security issues and trends.
Review work from other tech team members and provide feedback for growth.
Implement Data Architecture and Data security policies that align with governance objectives and regulatory requirements
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","Sql, Data Architecture, Data Warehousing, Agile Methodologies"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata
work Mode- Hybrid
Roles And Responsibilities
Mandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization
Solution Architect for Data modelling Understanding of Enterprise datasets Sales,
Procurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle
etc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building
Data lake foundation, Maintenance etc)
Collaborate with the product/business teams, understand related business processes and
document business requirements and then write high level specifications/requirements for DEs
Develop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout
for right grain of data either in True source systems or in Data WHs and build reusable data
models in intermediary layers before creating physical consumable views from data mart
Understand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data
Governance, Data Quality frameworks, Data Observality and the candidate should be:
Familiar with DevOps process
Knowing how to check existing tables, data dictionary, table structures
Experienced with normalizing tables
Having good understanding of Landing, Bronze, Silver and Gold layers and concepts
Familiar with Agile techniques
Create business process map, user journey map and data flow integration diagrams; Understand
Integration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of
models
Stakeholder management with data engineering, product owners, central data modelling team,
data governance & stewards, Scrum master, project team and sponsor.
Ability to handle large implementation program with multiple projects spanning over an year.
Skills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Agile techniques, Integration through API, DataOps, Data observability, Data Modeling, Sql, Cloud Architecture, Devops, Dimensional Modeling, SAP, FTP, Data Modeler, Data Quality, Oracle, Data Governance, Data Architect, Azure, Sftp, Data Lake, Webservices"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
What You'll Do
Define and design future state data architecture for HR reporting, forecasting and analysis products.
Partner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.
Engage with line of business, operations, and project partners to gather process improvements.
Lead to design / build new models to efficiently deliver the financial results to senior management.
Evaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.
Collaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing POCs to ensure those standards are implemented.
Provide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.
Develop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.
Create and maintain conceptual / logical data models to identify key business entities and visual relationships.
Work with business and IT teams to understand data requirements.
Maintain a data dictionary consisting of table and column definitions.
Review data models with both technical and business audience
What You'll Bring
Essential Education
Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
Additional Certification in Data Management or cloud data platforms like Snowflake preferred
Essential Experience & Job Requirements
12+ years of IT experience with major focus on data warehouse/database related projects
Expertise in cloud databases like Snowflake, Redshift etc.
Expertise in Data Warehousing Architecture; BI/Analytical systems; Data cataloguing; MDM etc
Proficient in Conceptual, Logical, and Physical Data Modelling
Proficient in documenting all the architecture related work performed.
Proficient in data storage, ETL/ELT and data analytics tools like AWS Glue, DBT/Talend, FiveTran, APIs, Tableau, Power BI, Alteryx etc
Experience in building Data Solutions to support Comp Benchmarking, Pay Transparency / Pay Equity and Total Rewards use cases preferred.
Experience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake a plus
Experience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs) a plus
Excellent written, oral communication and presentation skills to present architecture, features, and solution recommendations is a must
Additional info
YOU'RE GOOD AT
Design, document & train the team on the overall processes and process flows for the Data architecture.
Resolve technical challenges in critical situations that require immediate resolution.
Develop relationships with external stakeholders to maintain awareness of data and security issues and trends.
Review work from other tech team members and provide feedback for growth.
Implement Data security policies that align with governance objectives and regulatory requirements.
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","Cloud Databases, Cloud Big Data Technologies, FiveTran, Physical Data Modelling, Data Analytics Tools, dbt, MDM, Meta Scrum, Data Cataloguing, Kanban, Data Warehousing Architecture, BI Analytical Systems, Alteryx, AWS Glue, Tableau, ELT, Data Architecture, Scrum, Talend, AWS, Apis, Power Bi, Agile Methodologies, Gcp, Azure, Etl"
Data Architect,Arcadis,7-9 Years,,"Noida, India",Login to check your skill match score,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world's most complex challenges and deliver more impact together.
Individual Accountabilities
Collaboration
Collaborates with domain architects in the DSS, OEA, EUS, and HaN towers and if appropriate, the respective business stakeholders in architecting data solutions for their data service needs.
Collaborates with the Data Engineering and Data Software Engineering teams to effectively communicate the data architecture to be implemented. Contributes to prototype or proof of concept efforts.
Collaborates with InfoSec organization to understand corporate security policies and how they apply to data solutions.
Collaborates with the Legal and Data Privacy organization to understand the latest policies so they may be incorporated into every data architecture solution.
Suggest architecture design with Ontologies, MDM team.
Technical Skills & Design
Significant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS and data warehouses.
Deep understanding of modern data services in leading cloud environments, and able to select and assemble data services with maximum cost efficiency while meeting business requirements of speed, continuity, and data integrity.
Creates data architecture artifacts such as architecture diagrams, data models, design documents, etc.
Guides domain architect on the value of a modern data and analytics platform.
Research, design, test, and evaluate new technologies, platforms and third-party products.
Working experience with Azure Cloud, Data Mesh, MS Fabric, Ontologies, MDM, IoT, BI solution and AI would be greater assets.
Expert troubleshoot skills and experience.
Leadership
Mentors aspiring data architects typically operating in data engineering and software engineering roles.
Key Shared Accountabilities
Leads medium to large data services projects.
Provides technical partnership to product owners
Shared stewardship, with domains architects, of the Arcadis data ecosystem.
Actively participates in Arcadis Tech Architect community.
Key Profile Requirements
Minimum of 7 years of experience in designing and implementing modern solutions as part of variety of data ingestion and transformation pipelines
Minimum of 5 years of experience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.
Experience working in large scale development and cloud environment.
Why Arcadis
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It's why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You'll do meaningful work, and no matter what role, you'll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day, which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people.","modern data services, structured and unstructured data, Ai, ontologies, data architecture artifacts, architecture diagrams, cloud environments, data models, MDM, MS Fabric, Data Mesh, Iot, RDBMS, Azure Cloud, data warehouses"
Data Architect,Aumni Techworks,8-10 Years,,"Pune, India",Login to check your skill match score,"Position Summary:
We are looking for a highly skilled and experienced Data Engineer who focus on leading the development, and implementation of our Data Warehouse/Lakehouse solution, ensuring it serves as the foundation for scalable, high-performance analytics.
Responsibilities:
Lakehouse Design & Implementation:
Lead the end-to-end development and deployment of a scalable and secure Lakehouse architecture.
Define best practices for data ingestion, storage, transformation, and processing using modern cloud technologies.
Architect data pipelines using ETL/ELT frameworks to support structured, semi-structured, and unstructured data.
Optimize data modeling strategies to meet the analytical and performance needs of stakeholders.
Evaluate and select appropriate cloud technologies, frameworks, and architectures.
Requirement:
Experience:
8+ years of experience in data engineering, with a proven track record of implementing large-scale data solutions.
Extensive experience with cloud platforms (AWS, GCP, or Azure), specifically in data warehouse/lakehouse implementations.
Expertise in modern data architectures with tools like Databricks, Snowflake, or BigQuery.
Strong background in SQL, Python, and distributed computing frameworks (Spark, Dataflow, etc.).
In-depth knowledge of data modeling principles (e.g., Star Schema, Snowflake Schema).
Experience in enabling AI tools to consume data from the Lakehouse.
About Aumni Techworks:
Established in 2016, Aumni Techworks partners with its multinational clients to incubate and operate remote teams in India using the AumniBOT model. With a team of 250 and growing, our mission is to provide a quality alternative to project-based outsourcing.
Benefits of working at Aumni Techworks:
Work within a product team on cutting edge tech with one of the best pay packages.
No politics, no bench, voice your opinion, flat hierarchy, and global exposure
Work environment to re-live our fun college days (awarded as Best culture by Pune Mirror)
Recharge frequently with Friday socials, dance classes, theme parties and monsoon picnic.
Breakout spaces at the office Gym, Pool, TT, Foosball and Carrom
Health focused Insurance coverage and get in shape with AumniFit (Do not miss our 4 PM plank!)","Snowflake Schema, Data ingestion, snowflake, ELT frameworks, Lakehouse architecture, BigQuery, Data Modeling, Cloud Technologies, Sql, Gcp, Spark, Databricks, DataFlow, Azure, Star Schema, Python, AWS"
Data Architect,LSEG (London Stock Exchange Group),12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Data Architect Corporate Engineering
Company Profile
LSEG (London Stock Exchange Group) is a world-leading financial markets infrastructure and data business. We are dedicated, open-access partners with a commitment to excellence in delivering services across Data & Analytics, Capital Markets, and Post Trade.
Backed by three hundred years of experience, innovative technologies, and a team of over 23,000 people in 70 countries, our purpose is driving financial stability, empowering economies, and enabling customers to create sustainable growth.
Role Profile
In this role, you'll be joining our CRM, External Digital and Marketing team within Data and Integrations Team, Corporate Engineering (CE) as a Data Architect. This team works on data and integration requests by guiding customers on data migration cleansing, data quality techniques. This role impacts all divisional users of CRM Technology and downstream systems reliant on Customer Data 7k users across Capital Markets, Post Trade and Data & Analytics.
The data and integrations team do this by:
Conducting data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion/AWS Glue).
Building Enterprise Data Architecture or Data Lake for the Migration projects
Defining the data quality of the sources and lists the data quality metrics of the source systems.
Working on detailed scoping and requirements working with business customers, SMEs, Technology Partner organizations and internal CRM Technology and Corporate Technology teams.
Building and maintaining Customer Master and Product Master.
Continuously review operating metrics and data to find opportunities to improve.
Tech Profile/Essential Skills
12 years of technical experience.
7 years of experience on data migration and reporting using ETL and Reporting Tools
5 years of experience on ETL/Database Development
2 years of experience on Salesforce Data Migration projects
Expert level skill on Informatica IICS or Matillion or AWS Glue or equivalent ETL tool which covers from extracting the source to building the complex mappings.
Proficient in the use or extract of data from Salesforce.
Must have knowledge on the Salesforce Data modelling.
Must have Experience on Snowflake Storage and Database.
Expert level coding knowledge on Python to do ETL.
Able to translate business requirements into data solutions.
Understanding of Salesforce concepts.
Snowflake development.
Experience with providing technical solutions and supporting documentation.
Preferred Skills And Experience
Experience of Tableau and/or Power BI reporting preferred for management reporting.
Must have experience of working with Cloud Native based applications.
Understanding of the SDLC and agile delivery methodology.
Experience working with databases and data, performing data cleanup, and/or data manipulation and migration to and from Salesforce.com.
Should have experience with Enterprise Architect or Erwin Data modeler
Ability to handle own work and multitask to meet tight deadlines without losing sight of priorities under minimum supervision.
Highly motivated, self-directed individual with a positive & pro-active demeanor to work.
Customer and service focused, with determination to meet their needs and expectations.
Be driven and committed to the goals and objectives of the team and organization.
Education and Professional Skills
Professional qualification or equivalent.
BS/MS degree in Computer Science, Software Engineering or STEM degree (Desirable).
Curious about new technologies and tools, creative thinking and initiative taking.
Agile related certifications preferable.
Detailed Responsibilities
Proficient in data discovery, data migration, data quality analysis, end to end data reconciliations by profiling the source systems using ETL /SQL (IICS /Matillion Preferable).
Analyses the data quality of the sources and lists the data quality metrics of the source systems.
Domain expert in MDM and Customer data.
Lead the Single Customer View and Customer 360 implementations.
Lead the data migration strategies for large scale programs.
Data mining to uncover patterns, anomalies, and correlations in large data sets.
Data management to efficiently and cost-effectively collect, store, and use data.
Coding languages like Python and Java to develop applications for data analysis.
Machine learning to build scalable systems for handling Big Data Systems.
Structured query language (SQL) to manipulate data.
Data modelling tools like Erwin or Visio to visualize metadata and database schema.
Creating and implementing data management processes and procedures
Researching data acquisition opportunities.
Developing application programming interfaces (APIs) to retrieve data.
Develops and improves data governance and business data processes within the Technology and business organizations and understands client requirements, specifying and analyzing these to a sufficient level of detail to ensure transparency of definition and ability for technical teams to translate to a technical solution design.
Works with developers, architects, and solution designers to translate sophisticated business requirements and provides feedback on technical solutions proposed.
Responsible for building a relationship with partners, collaborators and impacted users.
Demonstrates proposed solutions and seeks and addresses feedback.
Proactively identifies, recommends, and implements improvements to the process as it relates to assigned projects.
Flexible in approach, adapting plans and strategies to help handle risks around ambiguity.
Strategic problem solver with strong intuition for business and well-versed in current technological trends and business concepts.
LSEG Benefits
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate based on anyone's race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.
LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.
Our purpose is the foundation on which our culture is built. Our values of Integrity, Partnership, Excellence and Change underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.
Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone's race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants and employees religious practices and beliefs, as well as mental health or physical disability needs.
Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it's used for, and how it's obtained, your rights and how to contact us as a data subject.
If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.","Erwin Data modeler, snowflake, Salesforce Data Migration, IICS, Matillion, Sql, AWS Glue, Tableau, Power Bi, Etl, Enterprise Architect, Python, Informatica"
Data Architect,McCain Foods,8-10 Years,,"Delhi, India",Login to check your skill match score,"Position Title: Data Architect
Position Type: Regular - Full-Time
Position Location: New Delhi
Grade: Grade 05
Requisition ID: 34658
Job Purpose
Reporting to the Data Architect Lead, Global Data Architect will take a lead role in creating the enterprise data model for McCain Foods, bringing together data assets across agriculture, manufacturing, supply chain and commercial. This data model will be the foundation for our analytics program that seeks to bring together McCain's industry-leading operational data sets, with 3rd party data sets, to drive world-class analytics. Working with a diverse team of data governance experts, data integration architects, data engineers and our analytics team including data scientist, you will play a key role in creating a conceptual, logical and physical data model that underpins the Global Digital & Data team's activities. .
Job Responsibilities
Develop an understanding of McCain's key data assets and work with data governance team to document key data sets in our enterprise data catalog
Work with business stakeholders to build a conceptual business model by understanding the business end to end process, challenges, and future business plans.
Collaborate with application architects to bring in the analytics point of view when designing end user applications.
Develop Logical data model based on business model and align with business teams
Work with technical teams to build physical data model, data lineage and keep all relevant documentations
Develop a process to manage to all models and appropriate controls
With a use-case driven approach, enhance and expand enterprise data model based on legacy on-premises analytics products, and new cloud data products including advanced analytics models
Design key enterprise conformed dimensions and ensure understanding across data engineering teams (including third parties); keep data catalog and wiki tools current
Primary point of contact for new Digital and IT programs, to ensure alignment to enterprise data model
Be a clear player in shaping McCain's cloud migration strategy, enabling advanced analytics and world-leading Business Intelligence analytics
Work in close collaboration with data engineers ensuring data modeling best practices are followed
Measures Of Success
Demonstrated history of driving change in a large, global organization
A true passion for well-structured and well-governed data; you know and can explain to others the real business risk of too many mapping tables
You live for a well-designed and well-structured conformed dimension table
Focus on use-case driven prioritization; you are comfortable pushing business teams for requirements that connect to business value and also able to challenge requirements that will not achieve the business goals
Developing data models that are not just elegant, but truly optimized for analytics, both advanced analytics use cases and dashboarding / BI tools
A coaching mindset wherever you go, including with the business, data engineers and other architects
A infectious enthusiasm for learning: about our business, deepening your technical knowledge and meeting our teams
Have a get things done attitude. Roll up the sleeves when necessary; work with and through others as needed
Key Qualification & Experiences
Data Design and Governance
At least 5 years of experience with data modeling to support business process
Ability to design complex data models to connect and internal and external data
Nice to have: Ability profile the data for data quality requirements
At least 8 years of experience with requirement analysis; experience working with business stakeholders on data design
Experience on working with real-time data.
Nice to have: experience with Data Catalog tools
Ability to draft accurate documentation that supports the project management effort and coding
Technical skills
At least 5 years of experience designing and working in Data Warehouse solutions building data models; preference for having S4 hana knowledge.
At least 2 years of experience in visualization tools preferably Power BI or similar tools.e
At least 2 years designing and working in Cloud Data Warehouse solutions; preference for Azure Databricks, Azure Synapse or earlier Microsoft solutions
Experience Visio, Power Designer, or similar data modeling tools
Nice to have: Experience in data profiling tools informatica, Collibra or similar data quality tools
Nice to have: Working experience on MDx
Experience in working in Azure cloud environment or similar cloud environment
Must have : Ability to develop queries in SQL for assessing , manipulating, and accessing data stored in relational databases , hands on experience in PySpark, Python
Nice to have: Ability to understand and work with unstructured data
Nice to have at least 1 successful enterprise-wide cloud migration being the data architect or data modeler. - mainly focused on building data models.
Nice to have: Experience on working with Manufacturing /Digital Manufacturing.
Nice to have: experience designing enterprise data models for analytics, specifically in a PowerBI environment
Nice to have: experience with machine learning model design (Python preferred)
Behaviors and Attitudes
Comfortable working with ambiguity and defining a way forward.
Experience challenging current ways of working
A documented history of successfully driving projects to completion
Excellent interpersonal skills
Attention to the details.
Good interpersonal and communication skills
Comfortable leading others through change
McCain Foods is an equal opportunity employer. We see value in ensuring we have a diverse, antiracist, inclusive, merit-based, and equitable workplace. As a global family-owned company we are proud to reflect the diverse communities around the world in which we live and work. We recognize that diversity drives our creativity, resilience, and success and makes our business stronger.
McCain is an accessible employer. If you require an accommodation throughout the recruitment process (including alternate formats of materials or accessible meeting rooms), please let us know and we will work with you to meet your needs.
Your privacy is important to us. By submitting personal data or information to us, you agree this will be handled in accordance with the Global Employee Privacy Policy
Job Family: Information Technology
Division: Global Digital Technology
Department: Data Architect
Location(s): IN - India : Haryana : Gurgaon
Company: McCain Foods(India) P Ltd","Machine Learning Model Design, MDx, Data Catalog Tools, Data Profiling Tools, Data Warehouse Solutions, Data Modeling, Collibra, Power Bi, Pyspark, S4 Hana, Azure Databricks, Informatica, Sql, Cloud Migration, Azure Synapse, Data Governance, Python"
Senior Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.
Role:
Design, implement and lead Data Architecture, Data Quality, Data Governance across
Defining data modeling standards and foundational best practices
Develop and evangelize data quality standards and practices
Establish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data.
Drive the successful adoption of organizational data utilization and self-serviced data platforms.
Create and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset
Develop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data.
Design data schemes, object models, and flow diagrams to structure, store, process, and integrate data
Provide architectural assessments, strategies, and roadmaps for data management.
Apply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms.
Implement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD
Translate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models.
Define templates and processes for the design and analysis of data models, data flows, and integration.
Lead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms
Mandatory Qualifications:
Qualifications: B.S. or M.S. in Computer Science, or equivalent degree
10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting.
7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more extensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse.
Highly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools.
Proven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker
Knowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with Amazon Web Services (AWS)
Strong verbal and written communications skills are a must and work effectively across internal and external organizations and virtual teams.
Demonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies.
Strong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem
Deep knowledge of data structures and algorithms.
Experience in working in large teams using CI/CD and agile methodologies.","Airflow, Delta Lake, CI CD, Data Lake Technologies, , Parquet, Big Data platforms, Hive Catalog, ML and Data Science platforms, Docker, Sql, Databricks, Tableau, Kafka, Avro, Etl, Hive, S3, Aws S3, Data Quality, Python, Kubernetes, Data Governance, Spark, Redshift"
Data Architect,Litmus7,10-12 Years,,"India, Cochin / Kochi / Ernakulam",Login to check your skill match score,"Data Architect is responsible to define and lead the Data Architecture, Data Quality, Data Governance, ingesting, processing, and storing millions of rows of data per day. This hands-on role helps solve real big data problems. You will be working with our product, business, engineering stakeholders, understanding our current eco-systems, and then building consensus to designing solutions, writing codes and automation, defining standards, establishing best practices across the company and building world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker passionate about building systems at scale.
Role
Design, implement and lead Data Architecture, Data Quality, Data Governance
Defining data modeling standards and foundational best practices
Develop and evangelize data quality standards and practices
Establish data governance processes, procedures, policies, and guidelines to maintain the integrity and security of the data
Drive the successful adoption of organizational data utilization and self-serviced data platforms
Create and maintain critical data standards and metadata that allows data to be understood and leveraged as a shared asset
Develop standards and write template codes for sourcing, collecting, and transforming data for streaming or batch processing data
Design data schemes, object models, and flow diagrams to structure, store, process, and integrate data
Provide architectural assessments, strategies, and roadmaps for data management
Apply hands-on subject matter expertise in the Architecture and administration of Big Data platforms, Data Lake Technologies (AWS S3/Hive), and experience with ML and Data Science platforms
Implement and manage industry best practice tools and processes such as Data Lake, Databricks, Delta Lake, S3, Spark ETL, Airflow, Hive Catalog, Redshift, Kafka, Kubernetes, Docker, CI/CD
Translate big data and analytics requirements into data models that will operate at a large scale and high performance and guide the data analytics engineers on these data models
Define templates and processes for the design and analysis of data models, data flows, and integration
Lead and mentor Data Analytics team members in best practices, processes, and technologies in Data platforms
Qualifications
B.S. or M.S. in Computer Science, or equivalent degree
10+ years of hands-on experience in Data Warehouse, ETL, Data Modeling & Reporting
7+ years of hands-on experience in productionizing and deploying Big Data platforms and applications, Hands-on experience working with: Relational/SQL, distributed columnar data stores/NoSQL databases, time-series databases, Spark streaming, Kafka, Hive, Delta Parquet, Avro, and more
Extensive experience in understanding a variety of complex business use cases and modeling the data in the data warehouse
Highly skilled in SQL, Python, Spark, AWS S3, Hive Data Catalog, Parquet, Redshift, Airflow, and Tableau or similar tools
Proven experience in building a Custom Enterprise Data Warehouse or implementing tools like Data Catalogs, Spark, Tableau, Kubernetes, and Docker
Knowledge of infrastructure requirements such as Networking, Storage, and Hardware Optimization with hands-on experience in Amazon Web Services (AWS)
Strong verbal and written communications skills are a must and should work effectively across internal and external organizations and virtual teams
Demonstrated industry leadership in the fields of Data Warehousing, Data Science, and Big Data related technologies
Strong understanding of distributed systems and container-based development using Docker and Kubernetes ecosystem
Deep knowledge of data structures and algorithms
Experience working in large teams using CI/CD and agile methodologies","Airflow, Data Lake Technologies, CI CD, Hive Catalog, Big Data platforms, ML and Data Science platforms, Delta Lake, S3, Kafka, Tableau, Redshift, Sql, Data Quality, Hive, Docker, Spark, Data Architecture, Databricks, Data Governance, Kubernetes, Python, Etl, Aws S3"
Data Center Architect,Mindsprint,Fresher,,"Chennai, India",Login to check your skill match score,"Job Title: Datacentre Architect
Location: Chennai, Tamil Nadu
Company: Mindsprint
About Mindsprint: Mindsprint is at the forefront of innovation, driving transformative changes in the IT landscape. As we embark on a significant journey of de-merging our IT infrastructure and application landscape, we are seeking a highly skilled and experienced De-Merger as a Service Architect to lead this critical initiative.
Job Summary: Data Centre Architect will be responsible for designing and implementing the de-merger strategy for Mindsprint's IT infrastructure and application landscape. This role requires a deep understanding of IT architecture, project management, and the ability to work collaboratively with various stakeholders to ensure a seamless transition.
Key Responsibilities:
Develop and execute the de-merger strategy for Mindsprint's IT infrastructure and application landscape.
Assess current IT systems and applications to identify dependencies and potential risks associated with the de-merger.
Design and implement solutions to separate IT systems and applications while ensuring minimal disruption to business operations.
Collaborate with cross-functional teams, including IT, business units, and external partners, to ensure alignment and successful execution of the de-merger plan.
Provide technical leadership and guidance throughout the de-merger process, ensuring best practices and industry standards are followed.
Develop detailed project plans, timelines, and budgets for the de-merger initiative.
Monitor progress, identify issues, and implement corrective actions as needed to keep the project on track.
Ensure compliance with all relevant regulations and standards during the de-merger process.
Communicate effectively with stakeholders at all levels, providing regular updates on project status and addressing any concerns.
Qualifications:
Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
Proven experience in IT architecture, with a focus on de-mergers, mergers, or large-scale IT transformations.
Strong project management skills, with experience leading complex IT projects.
Excellent problem-solving and analytical skills.
Ability to work effectively in a fast-paced, dynamic environment.
Strong communication and interpersonal skills, with the ability to build relationships and influence stakeholders.
Knowledge of relevant regulations and standards related to IT de-mergers.
Preferred Skills:
Experience as a Data Center Architect, Network Architect, and Cloud Architect.
Familiarity with IT security best practices and compliance requirements.
Certification in project management (e.g., PMP, PRINCE2) or IT architecture (e.g., TOGAF).","de-mergers, project management, Pmp, IT transformations, compliance requirements, Prince2, Togaf, it security best practices, IT architecture"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
What You'll Do
Be part of a team that is transforming BCG into a bionic company! We are building a centralized Business Intelligence & Analytics function that will simplify and automate information deliveryproviding advanced insights and analysis to support decision making. To date, the team has launched and operationalized several global scale products and dashboards, enhancing how our leaders engage in information to manage the business. The next wave of digital reporting is underway which will help to unlock further value for BCG functions and leadership with best-in-class business intelligence and analytics.
The Global IT Data Architect Sr. Manager works as an integral part of an Agile team delivering scalable digital reporting solutions for global stakeholders. This role will help design optimal solutions to support the digital product portfolio. Your responsibilities include: -
Define data architecture for solutions BI&A reporting.
Evaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.
Collaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing PoCs to ensure those standards are implemented
Provide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.
Develop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.
Create and maintain conceptual / logical data models to identify key business entities and visual relationships.
Work with business and IT teams to understand data requirements.
Maintain a data dictionary consisting of table and column definitions.
Review data models with both technical and business audiences.
Partner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.
Lead to design / build new models to efficiently deliver the financial results to senior management.
What You'll Bring
Bachelor's degree or equivalent combination of education and experience.
Bachelor's degree in information science, data management, computer science or related field preferred.
12+ years IT experience, including 8+ years in ELT/Data Integration for BI.
Experience of working on large transformational program with end-to-end ownership of implementation.
Hands-on implementation experience with Data Warehousing including design, modelling, testing, security, administration and optimization.
Expertise in cloud databases like Snowflake/RedShift, data catalogue, MDM etc.
Expertise in writing SQL and database procedures.
Proficient in Data Modelling- Conceptual, logical, and Physical modelling
Proficient in documenting all the architecture related work performed.
Experience in data storage, ETL/ELT and data analytics tools and technologies e.g., Snowflake, DBT, Tableau, Power BI, etc
Experienced in Data Warehousing design/development and BI/ Analytical systems
Experience working projects using Agile methodologies
Strong hands-on experience with data and analytics data architecture, solution design, and engineering experience
Experience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake
Experience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs)
Excellent written, oral communication and presentation skills to present architecture, features, and solution recommendations
Experience in all aspects of Agile SDLC, and end to end participation in a project lifecycle
Experience in Python, Data frames is a plus
Experience within GenAI space is a plus
Reporting tool experience Tableau, PowerBI, SAP BO is a plus
Who You'll Work With
Working closely with the rest of the Business Intelligence delivery teams, the product owner, Analyst, and Engineers. Architect will also work with external developers, business contacts across BCG functions, DBA's, and Infrastructure teams.
Additional info
YOU'RE GOOD AT
This Position Will Involve Daily Collaboration With The Architect And Other Development Teams, Vendors And Stakeholders Throughout Agile Design, Plus The Development, Implementation And Operations Of Both Infrastructure And Business Mappings. The Successful Candidate Will Demonstrate
Strong analytical abilities and creative problem solving
Ability to work independently with general direction and flexibility in a fast-paced environment
Good organization and excellent communication skills across cultures
Integrity and a positive attitude, especially while handling stressful situations
Work with project stakeholders (technical as well as end users) to understand business requirements and implement database solutions for diverse problems
Research viable technical and/or non-technical solutions, evaluate new technology and advocate, influence, and build consensus for innovations that satisfy business needs
Design, document & train the team on the overall processes and process flows for the Data architecture.
Resolve technical challenges in critical situations that require immediate resolution.
Develop relationships with external stakeholders to maintain awareness of data and security issues and trends.
Review work from other tech team members and provide feedback for growth.
Implement Data Architecture and Data security policies that align with governance objectives and regulatory requirements
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","Sql, Data Architecture, Data Warehousing, Agile Methodologies"
Data Architect,Arcadis,7-9 Years,,"Noida, India",Login to check your skill match score,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world's most complex challenges and deliver more impact together.
Individual Accountabilities
Collaboration
Collaborates with domain architects in the DSS, OEA, EUS, and HaN towers and if appropriate, the respective business stakeholders in architecting data solutions for their data service needs.
Collaborates with the Data Engineering and Data Software Engineering teams to effectively communicate the data architecture to be implemented. Contributes to prototype or proof of concept efforts.
Collaborates with InfoSec organization to understand corporate security policies and how they apply to data solutions.
Collaborates with the Legal and Data Privacy organization to understand the latest policies so they may be incorporated into every data architecture solution.
Suggest architecture design with Ontologies, MDM team.
Technical Skills & Design
Significant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS and data warehouses.
Deep understanding of modern data services in leading cloud environments, and able to select and assemble data services with maximum cost efficiency while meeting business requirements of speed, continuity, and data integrity.
Creates data architecture artifacts such as architecture diagrams, data models, design documents, etc.
Guides domain architect on the value of a modern data and analytics platform.
Research, design, test, and evaluate new technologies, platforms and third-party products.
Working experience with Azure Cloud, Data Mesh, MS Fabric, Ontologies, MDM, IoT, BI solution and AI would be greater assets.
Expert troubleshoot skills and experience.
Leadership
Mentors aspiring data architects typically operating in data engineering and software engineering roles.
Key Shared Accountabilities
Leads medium to large data services projects.
Provides technical partnership to product owners
Shared stewardship, with domains architects, of the Arcadis data ecosystem.
Actively participates in Arcadis Tech Architect community.
Key Profile Requirements
Minimum of 7 years of experience in designing and implementing modern solutions as part of variety of data ingestion and transformation pipelines
Minimum of 5 years of experience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.
Experience working in large scale development and cloud environment.
Why Arcadis
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It's why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You'll do meaningful work, and no matter what role, you'll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day, which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people.","modern data services, structured and unstructured data, Ai, ontologies, data architecture artifacts, architecture diagrams, cloud environments, data models, MDM, MS Fabric, Data Mesh, Iot, RDBMS, Azure Cloud, data warehouses"
Global IT Data Architect Senior Manager,Boston Consulting Group (BCG),12-14 Years,,"Gurugram, India",Login to check your skill match score,"Who We Are
Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.
To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital venturesand business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.
What You'll Do
Define and design future state data architecture for HR reporting, forecasting and analysis products.
Partner with Technology, Data Stewards and various Products teams in an Agile work stream while meeting program goals and deadlines.
Engage with line of business, operations, and project partners to gather process improvements.
Lead to design / build new models to efficiently deliver the financial results to senior management.
Evaluate Data related tools and technologies and recommend appropriate implementation patterns and standard methodologies to ensure our Data ecosystem is always modern.
Collaborate with Enterprise Data Architects in establishing and adhering to enterprise standards while also performing POCs to ensure those standards are implemented.
Provide technical expertise and mentorship to Data Engineers and Data Analysts in the Data Architecture.
Develop and maintain processes, standards, policies, guidelines, and governance to ensure that a consistent framework and set of standards is applied across the company.
Create and maintain conceptual / logical data models to identify key business entities and visual relationships.
Work with business and IT teams to understand data requirements.
Maintain a data dictionary consisting of table and column definitions.
Review data models with both technical and business audience
What You'll Bring
Essential Education
Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
Additional Certification in Data Management or cloud data platforms like Snowflake preferred
Essential Experience & Job Requirements
12+ years of IT experience with major focus on data warehouse/database related projects
Expertise in cloud databases like Snowflake, Redshift etc.
Expertise in Data Warehousing Architecture; BI/Analytical systems; Data cataloguing; MDM etc
Proficient in Conceptual, Logical, and Physical Data Modelling
Proficient in documenting all the architecture related work performed.
Proficient in data storage, ETL/ELT and data analytics tools like AWS Glue, DBT/Talend, FiveTran, APIs, Tableau, Power BI, Alteryx etc
Experience in building Data Solutions to support Comp Benchmarking, Pay Transparency / Pay Equity and Total Rewards use cases preferred.
Experience with Cloud Big Data technologies such as AWS, Azure, GCP and Snowflake a plus
Experience working with agile methodologies (Scrum, Kanban) and Meta Scrum with cross-functional teams (Product Owners, Scrum Master, Architects, and data SMEs) a plus
Excellent written, oral communication and presentation skills to present architecture, features, and solution recommendations is a must
Additional info
YOU'RE GOOD AT
Design, document & train the team on the overall processes and process flows for the Data architecture.
Resolve technical challenges in critical situations that require immediate resolution.
Develop relationships with external stakeholders to maintain awareness of data and security issues and trends.
Review work from other tech team members and provide feedback for growth.
Implement Data security policies that align with governance objectives and regulatory requirements.
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.
BCG is an E - Verify Employer. Click here for more information on E-Verify.","Cloud Databases, Cloud Big Data Technologies, FiveTran, Physical Data Modelling, Data Analytics Tools, dbt, MDM, Meta Scrum, Data Cataloguing, Kanban, Data Warehousing Architecture, BI Analytical Systems, Alteryx, AWS Glue, Tableau, ELT, Data Architecture, Scrum, Talend, AWS, Apis, Power Bi, Agile Methodologies, Gcp, Azure, Etl"
Data Modeler/Data Architect,VidPro Consultancy Services,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"Location -Bangalore,Pune,chennai,Gurgaon,Kolkata
work Mode- Hybrid
Roles And Responsibilities
Mandatory Skills - Data Modeler,Data Architect,Azure,Sql,Dimensional ,Data Vizualization
Solution Architect for Data modelling Understanding of Enterprise datasets Sales,
Procurement, Finance, Supply Chain, Logistics, R&D, Advanced Planning systems (SAP/Oracle
etc); Have worked for minimum 5 years for Data projects ( Upgradation, Migration, Building
Data lake foundation, Maintenance etc)
Collaborate with the product/business teams, understand related business processes and
document business requirements and then write high level specifications/requirements for DEs
Develop logical/physical data models as per Medallion architecture/EDW/Kimball model; Scout
for right grain of data either in True source systems or in Data WHs and build reusable data
models in intermediary layers before creating physical consumable views from data mart
Understand Cloud architecture, solution components in Cloud, DevOps, DataOps; Data
Governance, Data Quality frameworks, Data Observality and the candidate should be:
Familiar with DevOps process
Knowing how to check existing tables, data dictionary, table structures
Experienced with normalizing tables
Having good understanding of Landing, Bronze, Silver and Gold layers and concepts
Familiar with Agile techniques
Create business process map, user journey map and data flow integration diagrams; Understand
Integration needs (Integration through API, FTP, webservices and SFTP) for E2E deployment of
models
Stakeholder management with data engineering, product owners, central data modelling team,
data governance & stewards, Scrum master, project team and sponsor.
Ability to handle large implementation program with multiple projects spanning over an year.
Skills: agile,data observability,oracle,retail,cloud,data modeling,data architect,dimensional data modeling,projects,azure,er/studio,azure functions,erwin,api,architecture,models,devops,data modeler,data architects,devops practices,data,data vault,data warehouse,map,data governance,data visualization,sql,data quality,integration,supply chain,dimensional modeling,sap","Agile techniques, Integration through API, DataOps, Data observability, Data Modeling, Sql, Cloud Architecture, Devops, Dimensional Modeling, SAP, FTP, Data Modeler, Data Quality, Oracle, Data Governance, Data Architect, Azure, Sftp, Data Lake, Webservices"
Technical Architect- (Data Architect),Simpplr,8-10 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Who We Are
Simpplr is Modern Intranet and EX unified. Our platform unifies employee engagement, enablement, and services, leveraging state-of-the-art AI models to deliver a seamless, cohesive, and personalized employee experience for everyone - wherever and however they work.
Our mission is to transform the work experience for billions of people across the world. Because we believe that when work is good, life is better.
Trusted by more than 1,000+ leading brands, including DocuSign, Penske, Splunk, Nutanix, Okta, Eurostar, and SoFi, our customers are achieving measurable improvements in employee engagement, productivity, and accelerated business performance.
Simpplr is headquartered in Silicon Valley, CA with offices in the UK, Canada, and India, and is backed by Sapphire Ventures, Norwest Venture Partners, Salesforce Ventures, and Tola Capital. Learn more at simpplr.com.
Job Title: Technical Architect - Analytics
Location: Gurgaon Or Bangalore - Hybrid India
Employment Type: Full-Time
The opportunity
We are looking for a hands-on Technical Architect Analytics who will be responsible for designing, developing, and optimizing our data and analytics architecture. You will play a critical role in defining the data strategy, designing scalable data pipelines, and implementing best practices for real-time and batch analytics solutions. This role requires a strong technical leader who is passionate about data engineering, analytics, and driving data-driven decision-making across the organization.
Key Responsibilities
Data Architecture & Design: Define and own the architecture for data processing, analytics, and reporting systems, ensuring scalability, reliability, and performance.
Data Engineering: Design and implement highly efficient, scalable, and reliable data pipelines for structured and unstructured data.
Big Data & Real-Time Analytics: Architect and optimize data processing workflows for batch, real-time, and streaming analytics.
Cross-Functional Collaboration: Work closely with Product Managers, Data Scientists, Analysts, and Software Engineers to translate business requirements into scalable data architectures.
Technology & Best Practices: Stay ahead of industry trends, introduce modern data technologies, and drive best practices in data architecture, governance, and security.
Code Reviews & Mentorship: Review code, enforce data engineering best practices, and mentor engineers to build a high-performance analytics team.
Data Governance & Compliance: Ensure data security, integrity, and compliance with regulations (GDPR, CCPA, etc.).
Optimization & Performance Tuning: Identify performance bottlenecks in data pipelines and analytics workloads, optimizing for cost, speed, and efficiency.
Cloud & Infrastructure: Lead cloud-based data platform initiatives, ensuring high availability, fault tolerance, and cost optimization.
What Makes You a Great Fit for Us
Experience: 8+ years of experience in data architecture, analytics, and big data processing.
Proven Track Record: Experience designing and implementing end-to-end data platforms for high-scale applications.
Strong Data Engineering Background: Expertise in ETL/ELT pipelines, data modeling, data warehousing, and stream processing.
Analytics & Reporting Expertise: Experience working with BI tools, data visualization, and reporting platforms.
Deep Knowledge of Modern Data Technologies:
Big Data & Analytics: Spark, Kafka, Hadoop, Druid, ClickHouse, Presto, Snowflake, Redshift, BigQuery.
Databases: PostgreSQL, MongoDB, Cassandra, ElasticSearch.
Cloud Platforms: AWS, GCP, Azure (experience with cloud data warehouses like AWS Redshift, Snowflake is a plus).
Programming & Scripting: Python, SQL, Java, Scala.
Microservices & Event-Driven Architecture: Understanding of real-time event processing architectures.
Strategic Thinking: Ability to design and implement long-term data strategies aligned with business goals.
Problem-Solving & Optimization: Strong analytical skills with a deep understanding of performance tuning for large-scale data systems.
Visionary Leadership: Ability to think strategically and drive engineering excellence within the team.
Communication Skills: Strong interpersonal and communication skills to collaborate effectively across teams.
Attention to Detail: An eye for detail with the ability to translate ideas into tangible, impactful outcomes.
Agility: Comfortable managing and delivering work in a fast-paced, dynamic environment.
Preferred Skills (Good To Have)
Hands-on experience with AWS Public Cloud.
Experience with Machine Learning Pipelines and AI-driven analytics.
Hands-on experience with Kubernetes, Terraform, and Infrastructure-as-Code (IaC) for data platforms.
Certifications in AWS Data Analytics, Google Professional Data Engineer, or equivalent.
Experience with data security, encryption, and access control mechanisms.
Experience in Event/Data Streaming platforms
Experience in risk management and compliance frameworks
Simpplr's Hub-Hybrid-Remote Model
At Simpplr we believe that when work is good, life is better and that belief guides all we do. Including how we approach our flexible work model. Simpplr operates with a Hub-Hybrid-Remote model. This model is role-based with exceptions and provides employees with the flexibility that many have told us they want.
Hub - 100% work from Simpplr office. Role requires Simpplifier to be in the office full-time.
Hybrid - Hybrid work from home and office. Role dictates the ability to work from home, plus benefit from in-person collaboration on a regular basis.
Remote - 100% remote. Role can be done anywhere within your country of hire, as long as the requirements of the role are met.","Event-Driven Architecture, stream processing, Druid, Real-Time Analytics, snowflake, ClickHouse, Data Architecture Design, data engineering, Bi Tools, Data Modeling, Cassandra, PostgreSQL, Kafka, ELT, Microservices, Elasticsearch, Python, AWS, Java, BigQuery, Hadoop, Scala, Big Data, Redshift, Sql, Gcp, Presto, Data Visualization, Spark, Data Warehousing, MongoDB, Azure, Etl"
Azure Data Architect,Veracity Software Inc,10-12 Years,,"Pune, India",Login to check your skill match score,"Job Details:
Position: Data Architect
Experience: 10-12 years
Work Mode: Onsite
Location: Pune
Notice Period: Immediate
Job Responsibilities:
The ideal profile should have a strong foundation in data concepts, design, and strategy, with the ability to
work across diverse technologies in an agnostic manner.
Transactional Database Architecture
Design and implement high-performance, reliable, and scalable transactional database architectures.
Collaborate with cross-functional teams to understand transactional data requirements and create
solutions that ensure data consistency, integrity, and availability.
Optimize database designs and recommend best practices and technology stacks.
Oversee the management of entire transactional databases, including modernization and de-
duplication initiatives.
Data Lake Architecture
Design and implement data lakes that consolidate data from disparate sources into a unified, scalable
storage solution.
Architect and deploy cloud-based or on-premises data lake infrastructure.
Ensure self-service capabilities across the data engineering space for the business.
Work closely with Data Engineers, Product Owners, and Business teams.
Data Integration & Governance:
Understand ingestion and orchestration strategies.
Implement data sharing, data exchange, and assess data sensitivity and criticality to recommend
appropriate designs.
Basic understanding of data governance practices.
Innovation
Evaluate and implement new technologies, tools, and frameworks to improve data accessibility,
performance, and scalability.
Stay up to date with industry trends and best practices to continuously innovate and enhance the data
architecture strategy.
Show more Show less","Data Lake Architecture, Data Integration Governance, Data accessibility, Data governance practices, Transactional Database Architecture, Scalability"
Big Data Architect,Quick Heal,15-17 Years,,"Pune, India",Login to check your skill match score,"About Quick Heal
Quick Heal is one of the leading IT security solutions company with a global presence in 38 cities in India and 40 countries across globe. Each Quick Heal product is designed to simplify IT security management across the length and depth of devices and on multiple platforms. They are customized to suit consumers, small businesses, Government establishments and corporate houses.
Seqrite is the enterprise arm of India's leading and only listed cybersecurity products company Quick Heal Technologies Ltd. What sets Seqrite apart is our state-of-the-art Zero Trust technology, primed and ready to take on the market. We believe in a security paradigm where trust is never assumed, but rather consistently verified. Our Zero Trust solutions suite enables organizations to secure their endpoints, data, networks, and users across geographies, providing a robust defense against modern cyber threats.
Seqrite is also powered by state-of-the-art Seqrite Labs that continuously mines Threat Research, Real-time Detection, and Threat Intelligence.
In the recent successful project of our nation Chandrayaan 3, Seqrite solutions have played an important role in securing the command & control center of ISRO from Cyber Threats.
Seqrite has a dedicated Services wing. This division specialises in delivering comprehensive cybersecurity consulting services to a diverse clientele that includes Corporates, PSUs, Government, and Law Enforcement Agencies. Seqrite has a global marquee clientele across BFSI, Pharma, Manufacturing, Government, and Mid & Large industries.
Core Purpose:Innovate to simplify securing digital experience.
Mission:Empowering the team to solve business problems.
Vision:To be trusted by our customers in securing the digital world and aim to grow as reputable global market leader.
What makes us different:
Seqrite is one of the most successful purpose-led businesses enabling employees to thrive and unleash their potential to innovate. We invest in career development opportunities for our employees and celebrate our diverse perspectives every step of the way. We provide you an opportunity to work on new technologies. You will be surrounded by passionate and committed colleagues and work together to create a digital safe world for everyone.
Job Description
Position: Big Data Technical Architect
Experience Required: 15 plus years
Role:
Designed and built robust, high-performance, micro-service-based solution for modern enterprise platform (Java, Big Data/Relational/No-SQL/OLAP)
Provide technical thought leadership on High Level Architecture and Design, data modelling, Big Data strategy & adoption for Ingestion and Analytics Applications within the enterprise security domain
Demonstrates careful attention to quality and accuracy
Ability to convince his solutions and motivate the team
Research, design, develop and document cutting-edge generative AI and ML algorithms to address real-world challenges
Extensive working knowledge of various AWS Services including serverless, data streaming, big data, security related services, cloud formation, CDK
Responsibilities:
Large scale software integration experience
Good to have: Experience with Agile process methodology, CI/CD automation, TDD, Terraform
Research, design, develop and document cutting-edge generative AI and ML algorithms to address real-world challenges
Key Deliverables:
Strong understanding of development, architecture multi-threading,
Good understanding of algorithms and data structure to implement Real-time inline data processing
Good knowledge of Window/Linux at a systems level
Strong analytical and troubleshooting skills using debuggers.
Knowledge of various unit testing, performance testing frameworks
Required Skills:
Hands-on development experience
Big Data, Java, Golang, Spring MVC, Microservices, AWS,
Kubernetes, SQL/NoSQL, OLAP Databases ( ElasticSearch/ClickHouse, MySQL/PostgreSQL, MongoDB/Cassandra DB)
Desired Traits:
Knowledge of the following areas would be good to have:
Good understanding of cybersecurity domain will be added advantage
The candidate should think about the issues/solutions analytically and effectively. He should have zeal to learn new languages and technologies, stay updated on new technological trends and how those can be applied within the organization.
Thank you for your consideration to become Quick Heal Family Member","ClickHouse, OLAP Databases, Cassandra DB, Java, Golang, PostgreSQL, Big Data, Spring MVC, Sql, Microservices, Elasticsearch, Nosql, MySQL, MongoDB, Kubernetes, AWS"
"Data Architect, Director",NatWest Group,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"Our people work differently depending on their jobs and needs. From hybrid working to flexible hours, we have plenty of options that help our people to thrive.
This role is based in India and as such all normal working days must be carried out in India.
Job Description
Join us as a Data Architect
For someone with a background in defining business and application architectures and roadmaps for complex enterprises, this is an excellent opportunity to join our business. You'll be defining the intentional architecture for your assigned scope in order to ensure that the current architecture being delivered by engineers best supports the enterprise and its long term strategy
You'll provide advisory support and governance to ensure projects align to simplification strategy and comply with data standards while promoting and supporting effective data modelling, metadata management and alignment to enterprise data model and data controls
With valuable exposure, you'll be building and leveraging relationships with colleagues across the bank to ensure commercially focused decisions and to create long term value for the bank
We're offering this role at director level
What you'll do
As a Data Architect, you'll be defining and communicating the current, resultant and target state architecture for your assigned scope. You'll advise domain and project teams to ensure alignment with data architecture standards, data principles, data controls, target data architectures, data patterns and the banks simplification strategy
We'll look to you to influence the development of business strategies at an organisational level, identifying transformational opportunities for our businesses and technology areas associated with both new and existing data solutions.
As well as this, you'll be:
Translating architecture roadmaps into packages of work that allow frequent incremental delivery of value to be included in product backlog
Defining, creating and maintaining architecture models, roadmaps, standards and outcomes, using architecture strategies to ensure alignment to adjacent and higher level model
Collaborate and support data management SMEs and data stewards ensuring metadata management aligns with enterprise data model
Conduct data design reviews to ensure solutions meet data standards, data principles, data controls, data patterns and target data architectures
Seeking out and utilising continuous feedback, fostering adaptive design and engineering practices to drive the collaboration of programmes and teams around a common technical vision
The skills you'll need
To succeed in this role, you'll need expert knowledge of application architecture, and at least one business, data or infrastructure architecture with working knowledge of the remaining disciplines. You'll have excellent communication skills with the ability to clearly communicate complex technical concepts to colleagues, up to senior leadership level, along with a good understanding of Agile methodologies with experience of working in an Agile team.
You'll also demonstrate:
Good collaboration and stakeholder management skills
Experience of developing, syndicating and communicating architectures, designs and proposals for action
An understanding of industry architecture frameworks, such as TOGAF and ArchiMate
Experience in data modelling methodologies (3NF, Dimensional, Data Vault, Star Schema, etc). Knowledge of data related regulations such as GDPR, CCPA,PCI DSS, BCBS 239
Experience of working with business solution vendors, technology vendors and products within the market
A background in systems development change life cycles, best practices and approaches
Knowledge of hardware, software, application and systems engineering","BCBS 239, archimate, CCPA, systems development change life cycles, hardware software application and systems engineering, data modelling methodologies, Pci Dss, Gdpr, Agile Methodologies, Application Architecture, Infrastructure Architecture, Togaf"
Sr. Data Architect - Snowflake,Arting Digital,12-14 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Senior Data Architect - Snowflake
Experience: 12+ years
Budget: Up to 38 LPA
Notice Period: Immediate to 30 days
Location: Trivandrum, Bangalore, Chennai
Education: Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
Skill Set: Snowflake experience, Data Architecture experience, ETL process experience, Large Data migration solutions experience , data modelling, schema design DBT , Python/Java/Scala, SQL, ETL process , cloud data warehousing concept , data integration , AWS, Azure, and GCP , CDC , DataOps methodologies , cloud platforms/Snow-flak , data visualisation tools (e.g., Tableau, Power BI , data security and compliance standard
Job Description:
We are seeking an experienced Senior Data Architect Snowflake to lead and design scalable, high-performance data solutions. The ideal candidate should have extensive expertise in data architecture, large-scale data migration, ETL processes, and cloud-based data warehousing. You will play a key role in designing optimized data models, ensuring efficient data integration, and implementing best practices for Snowflake and other cloud platforms.
Key Responsibilities:
Architect, design, and implement scalable Snowflake-based data solutions.
Develop data models, schema designs, and ETL pipelines to support business requirements.
Lead large-scale data migration projects while ensuring performance optimization.
Implement DataOps methodologies for efficient data management and automation.
Work with AWS, Azure, and GCP to deploy cloud-based data architectures.
Ensure data security and compliance with industry standards.
Optimize CDC (Change Data Capture) processes for real-time data updates.
Utilize DBT, Python, Java, or Scala to enhance data transformation and integration.
Design and implement data visualisation solutions using tools like Tableau and Power BI.
Required Skills & Expertise:
Strong expertise in Snowflake and cloud data warehousing concepts.
Hands-on experience with ETL processes, data modelling, and schema design.
Proficiency in SQL, DBT, Python, Java, or Scala for data transformation and automation.
Experience in data integration and large-scale data migration solutions.
Knowledge of CDC (Change Data Capture) methodologies.
Familiarity with DataOps practices and modern data engineering workflows.
Exposure to AWS, Azure, and GCP cloud platforms.
Strong understanding of data security, governance, and compliance.
Experience with data visualisation tools such as Tableau or Power BI.","cdc, Data security and compliance, DataOps methodologies, snowflake, dbt, ETL processes, Cloud data warehousing, Large Data migration solutions, Java, Scala, Schema Design, Sql, Gcp, Data Integration, Data Architecture, Data Modelling, Azure, Python, AWS"
Data Center Architect,Coforge,12-14 Years,,"Delhi, India",Login to check your skill match score,"Designation Data Centre IT networking & architecture expert
Experience 12 Years and above
Job Location New Delhi
Understanding the Project Requirements, Technical Specifications & Scope of work
Experience in Data Centre Planning, Designing and Implementing at least 2 large data centres including Network, Compute, & Storage.
Hands On Experience with (Multi/Cross Platform Products/solutions)
Hands-on experience in Architecture, Design, Deployment, managing the High Availability
Solution for networking & security products.
Development, Review/Rework of HLD, LLD, SoPs, ATP document, DC-DR Rack Layouts for Racking and Stacking
Design, Deploy, and Test Disaster Recovery for the products/Solutions at DC and DR.
Should have prior experience in data centre designing for high-density RACKS
Design and implement power distribution systems, optimise power usage efficiency and ensure redundancy to minimise downtime risks.
Architect network infrastructure for Client data centre environments, including switches, routers, firewalls and other security & utility solutions.
Implement high-speed interconnects and design network topologies to support scalable and resilient connectivity.
Develop rack layouts and configurations to maximise space utilization and airflow management, ensuring the Facilitation of RU Space for the smooth integration of additional planned security solutions (such as Antiapt Solutions, HIDS/HIPS, ZTA, etc), and take care of Intelligent cabling for these futuristic requirements.
Design fault-tolerant architectures to ensure high availability and minimise service disruptions.
Architect Networking, utility solutions tailored to meet performance, capacity, and data protection requirements.
Optimise compute resources through virtualisation and containerization technologies.
Experience in the integration of different IT Infra solutions.
Support in VAPT
Desirable: Scripting hands-on / knowledge of PowerShell/Bash/Perl/Automation of migration process.
Mandatory skills: Data Centre IT networking & architecture expert, Date C , Server Deployment
Note: This position doesn't require expertise/experience in MEP&FP (Mechanical, Electrical, Plumbing, and Fire Protection) but should understand the concepts used for design.","Data Centre Planning, Networking Utility Solutions, Rack Layouts and Configurations, Fault-tolerant Architectures, High-speed Interconnects, Power Distribution Systems, Containerization Technologies, Perl, PowerShell, Bash, Network Topologies"
Data Center Architect,ITC Infotech,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Hi, Please find the detailed JD for the Datacenter Practice Lead role, if interested please send your profile to [HIDDEN TEXT]
The Role:
As a DataCenter Practice Lead, you will own, lead and focus on skills and competency building for the Compute technologies for the practice of the organization, as your primary role and also support ongoing Delivery programs and manage the PnL for the ame as a secondary role.
You are expected to stay current with present and emerging database technologies/trends utilizing new features when applicable to the various environments. Automate reoccurring complex workflows to free up time for higher value work.
The work you'll do:
Technology support and expertise to the team on the ground on issues and problems faced in the current Delivery, if any.
Facilitate a Cross functional interaction with product managers, domain experts, engineers and consultants during the solution build and design phases of new and existing deals and RFPs/RFIs/RFQs.
Assist and work with the teams in support process enhancements and technology transformation themes which will enhance the model of operation and bring in value to the end customers.
Be the escalation point for DataCentre and Compute related delivery issues and assist the team either by driving technical solutions and know-how on solving technical issues, or assist in building a bridge with the OEMs to help the team resolve issues on the ground.
Build competency across Data Centre technologies and create a centre of excellence to be able to act as a first line of defence during critical incidents and transformation initiatives.
Build and assist in creating templates and practice standardizations across the DB practice
Interface with customer to understand delivery concerns, receive feedback on support quality and derive models/solutions to improve the operating model to be implemented within the accounts.
Work with various OEMs and partners to evaluate tools and productivity solutions in the market to keep abreast of the trend and consult with the customer to see what is best fit for their environments, in order to up-sell and cross-sell services
Effective Team Member
Lead cross functional team capacity planning exercises related to environment and applications ensuring vendor best practices.
Collaborate and consult with users, system administrators, and systems programmers to overcome significant operational and/or technical issues and problems.
Utilize strong interpersonal skills in dealing effectively with diverse skill sets and personalities and work effectively as a team player.
Leadership
Mentor and teach less experienced team members preparing them for more responsibility while focusing on optimization and best practices.
Lead presentations to clients, upper management, and peers as it pertains to database technology roadmap, architecture, engineering, and provisioning.
Manage the completion of database administrator work, meeting deadlines, and providing deliverables to the customers.
Serve as the subject matter expert of all database related workflows. May perform additional duties related to agile project management.
Work directly with Project Managers to update project plans and communicate project status.
Participate in operating model activities related to product and service ownership.
Meet and communicate with stakeholders, document project definition, and provide direction and leadership in project estimates and sequencing.
Build, edit, and maintain team backlog; prioritize and document objectives for project sprints.
The qualifications you need:
You should have that rare combinationa sharp technical brain and a bent for business.
At least 15+ years of experience with Data Centre technologies such as Vmware, Citrix, Netapp, Storage, Network and Database knowledge
Be able to build and contribute to solutioning and technical reviews of complex operational issues and problems.
Have technical knowledge and have had hands on exposure to multi-technology towers
Expertise to at least one of the following domains: business intelligence, data engineering, business analytics or a similar field.
Strong Solution Designing skills with capability to understanding of business processes
Strong service management exposure and on ITSM processes
Ability to communicate effectively and build a good rapport with team members and with Customers
Soft skills: -
Passionate about technology, solving web-focused problems, and how good data analysis can impact strategy
Strong analytical and quantitative reasoning
Extremely organized
Detail-oriented
Problem-solving skills
Time management skills
Comfort and effectiveness in translating between people needs and database output
Strong oral and written communication skills
Interpersonal skills
People management and teaching skills
Service and mission- driven.","Business intelligence, Business analytics, ITSM processes, Citrix, Database Knowledge, Solution Designing, Netapp, data engineering, storage network, Vmware"
Architect- data Platform,Myntra,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"About Team
Myntra's Engineering team builds the technology platform that empowers our customers shopping experience and enables the smooth flow of products from suppliers to our customers doorsteps. We work on areas such as building massive-scale web-applications, engaging user-interfaces, big-data analytics, mobile apps, workflow systems, inventory-management etc. We are a small technology team where each individual has a huge impact. You will have the opportunity to be part of a rapidly growing organization and gain exposure to all the parts of a comprehensive ecommerce platform.
Myntra's cloud based big data platform is highly scalable and processes over 6 billion events per day. Over the last one year, we are on a journey to product-ize data platform and offer Data ingestion, streaming, processing and visualization as self-serve offerings for Myntra's data consumers. We use the best-of-breed open source components as starting points to build out these capabilities. The team has built a Big Data Platform to ingest data from a variety of data sources, standardize metrics and build data & analytics products .
Data and ML Platform engineering employs new-age technologies such as Distributed Computing constructs, Real Time model predictions, Deep Learning, Accelerated Compute (GPU); scalable feature stores Cassandra, MySQL, Elastic Search, Solr, Aerospike; scalable programming constructs in, Python and ML Frameworks (TensorFlow, Pytorch, etc).
Roles and Responsibilities
Drive the data architecture, data modelling, design, and implementation of data applications using standard open source big data tech stack, Data Warehouse / MPP databases and distributed systems. Gather business and functional requirements from external and/or internal users, and translate requirements into technical specifications to build robust, scalable, supportable solutions. Participate and drive the full development lifecycle.
Build the Standards and best practices around a Common Data Model and Architecture, Data Governance, Data Quality and Security for multiple business areas across Myntra. Collaborate with platform, product and other engineering and business teams to evangelise those Standards for adoption across the org.
Mentor data engineers at various levels of seniority by doing their design and code reviews, providing constructive and timely feedback on code quality, design issues, technology choices with performance and scalability being critical drivers. Manage resources on multiple technical projects and ensure schedules, milestones, and priorities are compatible with technology and business goals.
Setting up best practices to help the team achieve the above and constantly thinking about improving the technology use are your responsibilities. Driving the adoption of these best practices around coding, design, quality, performance in your team.Stay abreast of the technology industry, market trends in the field of data architecture and development.
Demonstrates understanding of data lifecycle (data modelling, processing, data quality, data evolution) and underlying tech stacks (Hadoop, Spark, MPP). Drives setting data architecture standards encompassing complete data life cycle (ingestion, modelling, processing, consumption, change management, quality, anomaly detection).
Challenge the status quo and propose innovative ways to process, model, consume data when it comes to tech stack choices or design principles.
Implementation of long term technology vision for your team.
Active participant in technology forums; represent Myntra in external forums.
Qualifications & Experience
12 - 15 years of experience in software development
5+ years of development and / or DBA experience in Relational Database Management Systems[RDBMS] (MySql, SQLServer, etc.)
8+ years of hands-on experience in implementation and performance tuning MPP databases (Microsoft SQL DW, AWS Redshift, Teradata, Vertica, etc.)
Experience designing database environments, analyzing production deployments, and making recommendations to optimize performance
Problem solving skills for complex & large scale data applications problems.
Technical Breadth - Exposure to a wide variety of problem spaces, technologies in data e.g. real-time and batch data processing, options in commercial vs open source tech stack.
Hands-on experience with Enterprise Data Warehouse and Big data storage and computation frameworks like OLAP Systems, MPP (SQL DW, Redshift, Oracle RAC, Teradata, Druid), Hadoop Compute (MR, Spark, Flink, Hive). Awareness of pitfalls & use cases for a large variety of solutions. Ability to drive capacity planning, performance optimization and large-scale system integrations.
Expertise in designing, implementing, and operating stable, scalable, solutions to flow data from production systems into analytical data platforms (big data tech stack + MPP) and into end-user facing applications for both real-time and batch use cases.
Data modelling skills (relational, multi-dimensional) and proficiency in one of the programming languages preferably Java, Scala or Python.
Drive design and development of automated monitoring, alerting, self healing (restartability / graceful failures) features while building the consumption pipelines.
Mentoring skills - Be the technical mentor to your team.
B. Tech. or higher in Computer Science or equivalent required.","Flink, MPP databases, Druid, Teradata, Real Time model predictions, OLAP Systems, Accelerated Compute, Aws Redshift, Cassandra, Gpu, Tensorflow, MySQL, Aerospike, Python, Solr, Hadoop, Big Data, Deep Learning, Hive, Distributed Computing, Pytorch, Spark, Vertica, Elastic Search"
Data Scientist Frontend Architect,Ericsson,Fresher,,"Bengaluru, India",Login to check your skill match score,"About this opportunity:
Team is now looking for an experienced Front-End Architect and Front -End developer with strong technical expertise to design and lead the development of scalable, high-performance front-end applications of AI/ML. The ideal candidate should possess a deep understanding of modern front-end technologies, a passion for UX, and experience in integrating various third-party libraries and APIs. The role requires excellent problem-solving skills and the ability to collaborate with cross-functional teams, including back-end developers, DevOps, , UI/UX designers and AI Knowledge.
What you will do:
Architect, design, and implement front-end solutions using Angular (12+), TypeScript, JavaScript, and related frameworks.
Develop and maintain scalable and reusable front-end components, ensuring a seamless user experience.
Integrate CSS3, HTML5, and frameworks like Bootstrap or Angular Material to enhance UI functionality and aesthetics.
Work with RxJS, MomentJS, UnderscoreJS, ReactJS, NodeJS, or other third-party libraries as needed to meet project requirements.
Design testable and maintainable code, incorporating Jasmine and Karma for testing when appropriate.
Collaborate with the DevOps team to manage builds and CI/CD pipelines using Jira, GitLab, and other tools.
Facilitate seamless communication between front-end and back-end teams using REST, JSON, and SOAP for API integration.
Leverage cloud-native development practices and contribute to architecture discussions.
Communicate effectively with a diverse set of technical audiences to convey complex concepts.
What you will Bring:
Angular (12+), TypeScript, JavaScript expertise.
Proficiency in RxJS, CSS3, HTML5, and Bootstrap/Angular Material.
Experience with third-party library integration such as MomentJS, UnderscoreJS, ReactJS, NodeJS, or similar.
Familiarity with REST, JSON, and SOAP integration.
Solid understanding of cloud-native development and CI/CD tools, especially Jira and GitLab.
Why join Ericsson
At Ericsson, youll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of whats possible. To build solutions never seen before to some of the world's toughest problems. Youll be challenged, but you won't be alone. Youll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.
What happens once you apply
Click Here to find all you need to know about what our typical hiring process looks like.
Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we champion it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team. Ericsson is proud to be an Equal Opportunity Employer. learn more.
Primary country and city: India (IN) || Bangalore
Req ID: 766743","MomentJS, Underscorejs, Nodejs, Jasmine, Soap, Json, Jira, Css3, Typescript, REST, Javascript, Angular 12, Html5, Reactjs, Rxjs, Bootstrap, Gitlab, Angular Material, Karma"
Data Integration Architect,Alstom Transportation,10-14 Years,,Bengaluru,Transportation,"RESPONSIBILITIES:
Technical -
Hands-on-experience architecting and delivering solutions related to enterprise integration, APIs, service-oriented architecture, and technology modernizations
3-4 years hands-on experience with the design, and implementation of integrations in the area of Dell Boomi
Understanding the Business requirements and Functional requirement Documents and Design a Technical Solution as per the needs
Person should be good with Master Data Management, Migration and Governance best practices
Extensive data quality and data migration experience including proficiency in data warehousing, data analysis and conversion planning for data migration activities
Lead and build data migration objects as needed for conversions of data from different sources
Should have architected integration solutions using Dell Boomi for cloud, hybrid and on-premise integration landscapes
Ability to build and architect a high performing, highly available, highly scale Boomi Molecule Infrastructure
In depth understanding of enterprise integration patterns and prowess to apply them in the customers IT landscape
Assists project teams during system design to promote the efficient re-use of IT assets Advises project team during system development to assure compliance with architectural principles, guidelines and standards
Adept in building the Boomi processes with Error handling and email alerts logging best practices
Should be proficient in using Enterprise level and Database connectors
Extensive data quality and data migration experience including proficiency in data warehousing, data analysis and conversion planning for data migration activities
Excellent understanding on REST with in-depth understanding on how Boomi processes can expose consume services using the different http methods, URI and Media type
Understand Atom, Molecule, Atmosphere Configuration and Management, Platform Monitoring, Performance Optimization Suggestions, Platform Extension, User Permissions Control Skills.
Knowledge on API governance and skills like caching, DB management and data warehousing
Should have hands on experience in configuring AS2, https, SFTP involving different authentication methods
Thorough knowledge on process deployment, applying extensions, setting up schedules, Web Services user management process filtering and process reporting
Should be expert with XML and JSON activities like creation, mapping and migrations
Person should have worked on integration on SAP, SuccessFactors, Sharepoint, cloud-based apps, Web applications and engineering application
Support and resolve issues related to data integration deliveries or platform
Project Management
Person should deliver Data Integration projects using data integration platform
Manage partner deliveries by setting up governance of their deliveries
Understand project priorities, timelines, budget, and deliverables and the need to proactively push yourself and others to achieve project goals
Managerial:
Person is individual contributor and operationally managing small technical team
Qualifications & Skills:
10+ years of experience in the area of enterprise integrations
Minimum 3-4 years of experience with Dell boomi
Should have working experience with database like sql server, Data warehousing
Hands on experience on REST, SOAP, XML, JSON, SFTP, EDI
Should have worked on integration of multiple technologies like SAP, Web, cloud based apps.","Caching, Sharepoint, Db Management, SAP, Successfactors, Data Management"
"Manager, Data Science, Solution Architect",Mondelez,10-14 Years,,Mumbai,Food and Beverage,"You will work closely with the enterprise architecture team to chart technology roadmaps, standards, best practices and guiding principles, providing your subject matter expertise and technical capabilities to oversee specific applications in architecture forums and when participating in workshops for business function requirements
In collaboration with the enterprise architecture and solution teams, you will help evaluate specific applications with a goal to bring in new capabilities based on the strategic roadmap
You will also deliver seamless integration with the existing ecosystem and support project teams in implementing these new technologies, offer input regarding decisions on key technology purchases to align IT investments with business strategies while reducing risk and participate in technology product evaluation processes and Architecture Review Board governance for project solutions
What you will bring
A desire to drive your future and accelerate your career. You will bring experience and knowledge in:
Defining and driving successful solution architecture strategy and standards
Components of holistic enterprise architecture
Teamwork, facilitation and negotiation
Prioritizing and introducing new data sources and tools to drive digital innovation
New information technologies and their application
Problem solving, analysis and communication
Governance, security, application life-cycle management and data privacy
Purpose of Role
The Solution Architect will provide end-to-end solution architecture guidance for data science initiatives. A successful candidate will be able to handle multiple projects at a time and drive the right technical architecture decisions for specific business problems. The candidate will also execute PoC/PoVs for emerging AI/ML technologies, support the strategic roadmap and define reusable patterns from which to govern project designs.
Main Responsibilities: -
Determine which technical architectures are appropriate for which models and solutions.
Recommend what technologies and associated configurations are best to solve business problems.
Define and document data science architectural patterns.
Ensure project compliance with architecture guidelines and processes.
Provide guidance and support to development teams during the implementation process.
Develop and implement processes to execute AI/ML workloads.
Configure and optimize the AI/ML systems for performance and scalability.
Stay up to date on the latest AI/ML features and best practices.
Integrating SAP BW with SAP S/4HANA and other data sources.
Review and sign off on high-level architecture designs.
Career Experiences Required Role Implications
Bachelor s degree in computer science or related field of study.
10+ years of experience in a global company in data-related roles (5+ years in data science).
Strong proficiency in Databricks and analytical application frameworks (Dash, Shiny, React).
Experience with data engineering using common frameworks (Python, Spark, distributed SQL, NoSQL).
Experience leading complex solution designs in a multi-cloud environment.
Experience with a variety of analytics techniques: statistics, machine learning, optimization, and simulation.
Experience with software engineering practices and tools (design, testing, source code management, CI/CD).
Deep understanding of algorithms and tools for developing efficient data processing systems.","technical architectures, AI/ML technologies, SAP S/4HANA, Spark, Databricks, Python, Sap Bw"
Data Solution Architect,Atos Global It Solutions And Services Private Limited,5-8 Years,,Thane,Software,"Certified Cloud Solution Architect with 5-8 years HANDS ON work experience designing, executing, and supporting IT Cloud solutions.
Experience on setting up data lake / data warehousing for large enterprise solution with massive data volumes.
Experience in Docker containers, coding, with cloud native cloud agnostic tools, creation consumption of high-performance APIs programs on architectural framework and guidelines with Delta Lake, Data Lake for Data warehousing solutions to create and nurture highly available systems on Kubernetes.
Structural YAML with Parameterization and Dynamic Configurations
Experience in Databricks, Data Factory pipelines Automation tooling systems.
Having capabilities in API Management , API modeling, scripting/coding languages and experience with relational databases.
Work with internal external teams to design a full-stack solution using software that fully integrates with customer s existing cloud infrastructure from data flow, security, DevOps, GitHub and troubleshoot performance and functional issues.
Develop and organize cloud systems and work closely with IT security to monitor the company s cloud privacy.
Good Interpersonal skills and Communication skills: require working in a cross-functional team, as well as with various stakeholders with minimal supervision. So, it is vital that they can establish rapport and manage relationship with the people they have on their project team.
Learn more about us
At Atos, we embrace diversity as the ultimate engine of ingenuity for our clients, and we constantly strive to create a culture where people feel supported and encouraged. Read more about our commitment here .
Whether it is fighting climate change, promoting digital inclusion, or ensuring trust in data management tech for good sits at the core of our identity. With numerous global recognitions for our ESG practices, we are committed to building a better future for all by harnessing the power of technology. Learn more here","Data Solution Architect, Api"
Software Architect (Data Engineering OR RoR,Velotio Technologies,5-11 Years,,Pune,Software,"Job description
About Velotio:
Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.
We are looking for an experiencedSoftware Architectwith deep expertise inData Engineeringand proficiency inRuby on Rails (RoR). The ideal candidate will lead architectural decisions, optimize data pipelines, and ensure scalable, efficient, and high-performance data processing solutions. WhileData Engineering skillsare the primary focus,RoR experiencewould be a valuable addition
7+ years of experience in software development, with a focus on data engineering and cloud architectures.
Strong experience with Snowflake, DBT, and Data Pipeline design.
Expertise in Kafka, Argo, Kubernetes, and ML Flow.","Snow Flake, Argo, Kafka, Kubernetes"
Senior Enterprise Architect - Data,Bread Financial,10-15 Years,,Bengaluru,Financial Services,"Sr Enterprise Architect designs and implements enterprise-wide IT solutions to align technology initiatives with business goals. Serve as a strategic advisor to leadership, ensuring architecture principles are followed.
Essential Job Functions
Develop and maintain enterprise architecture frameworks and standards. -
Oversee enterprise data strategies, designing platforms for advanced analytics, AI/ML, and regulatory adherence
Evaluate new technologies and identify opportunities for system enhancements.
Collaborate with stakeholders to analyze business requirements and translate them into technical solutions.
Ensure technology initiatives align with the organizations overall architectural vision.
Conduct system audits and maintain documentation for architecture solutions.
Minimum Qualifications
Bachelor s Degree or equivalent education and / or experience in computer Science, Engineering, Mathematics or related field.
10+ years in Information Technology
Preferred Qualifications
Relevant certifications, such as TOGAF or AWS Solutions Architect
Skills
Application Development
Business Alignment
Business Process Modeling
Business Case Development
Code Inspection
Cloud Architectures
Enterprise Architecture Framework
IT Architecture
IT Roadmap
Solution Architecture
Reports To : Manager and above
Direct Reports : 0
Work Environment
Normal office environment, hybrid.","Architecture, Business process modeling, Solution architecture, Enterprise Architecture, Application Development"
Data & Analytics Architect,Flexera,15-17 Years,,India,Login to check your skill match score,"Flexera saves customers billions of dollars in wasted technology spend. A pioneer in Hybrid ITAM and FinOps, Flexera provides award-winning, data-oriented SaaS solutions for technology value optimization (TVO), enabling IT, finance, procurement and cloud teams to gain deep insights into cost optimization, compliance and risks for each business service. Flexera One solutions are built on a set of definitive customer, supplier and industry data, powered by our Technology Intelligence Platform, that enables organizations to visualize their Enterprise Technology Blueprint in hybrid environmentsfrom on-premises to SaaS to containers to cloud.
We're transforming the software industry. We're Flexera. With more than 50,000 customers across the world, we're achieving that goal. But we know we can't do any of that without our team. Ready to help us re-imagine the industry during a time of substantial growth and ambitious plans Come and see why we're consistently recognized by Gartner, Forrester and IDC as a category leader in the marketplace. Learn more at flexera.com
At Flexera, we're on a mission to empower global enterprises by transforming IT insights into decisive actions. We are looking for an architect/principal engineer with deep expertise in analytics, including data modeling, semantic modeling, to take our world-class reference data in Technopedia to the next level, through deeper insights, richer features and more data sets. The ideal candidate will have a strong track record of providing technical leadership, deep technical expertise and delivering big data solutions through data lakehouse and large scale-processing technologies.
As an architect/principal engineer, you will be a key player in architecting, designing, developing, and maintaining our common ontology and data models, transformations, and analytics capabilities for our industry-leading reference data. You will collaborate closely with other architects and guide, influence and help engineering teams to ensure seamless integration with other components of our system.
What you'll do:
Architect, design and develop our big data platform, processing pipelines and content reference data.
Drive innovation, enabling capabilities to enhance speed to innovation for our analytics, AI/ML, and product development teams.
Define data models and semantic models for our wide range of data sets to provide insights and optimized user experiences.
Provide guidance, influence and help engineering teams to deliver deep insights from our large and broad data sets.
Continuously enhance your technical skills and mentor other engineers.
Promote a high-standard engineering culture and operational excellence within the team.
Collaborate with product and design teams to ensure the platform meets user needs.
Define project requirements, technical artifacts, and designs, driving consensus across Product Management, architects, and engineering teams.
Balance priorities between new feature development, architectural enhancements, and technical debt reduction for a sustainable and scalable platform.
Champion continuous improvement in product quality, security, and performance standards within the development team.
You'll be expected to have:
Bachelor's or higher degree in Computer Science, Software Engineering, or related field
Minimum 15 years relevant experience in software development, including 8+ years of expertise in data modeling, semantic modeling, and data visualization.
Strong expertise in other big data technologies, such as metadata catalogs, data lineage, and orchestration.
Strong technical expertise in distributed systems, big data, and cloud computing.
Strong problem-solving skills and ability to troubleshoot complex issues.
Good understanding of streaming technologies, including Kafka, with experience in defining message schemas and data models.
Outstanding communication skills and emotional intelligence to collaborate effectively with teams across the organization.
Excellent written and verbal communication skills.
Ability to work effectively both independently and in a collaborative team environment.
Prior experience mentoring and providing technical guidance to junior engineers.
At Flexera, we foster a fun and engaged hybrid working environment where collaboration and innovation thrive. We value diversity and encourage applicants from underrepresented groups in technology to apply.
Join our team to not only contribute to a world-class global product but also to grow in your career. At Flexera, we encourage continuous learning and provide opportunities for professional development.
Flexera is proud to be an equal opportunity employer. Qualified applicants will be considered for open roles regardless of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by local/national laws, policies and/or regulations.
Flexera understands the value that results from employing a diverse, equitable, and inclusive workforce. We recognize that equity necessitates acknowledging past exclusion and that inclusion requires intentional effort. Our DEI (Diversity, Equity, and Inclusion) council is the driving force behind our commitment to championing policies and practices that foster a welcoming environment for all.
We encourage candidates requiring accommodations to please let us know by emailing [HIDDEN TEXT].","streaming technologies, semantic modeling, metadata catalogs, Distributed Systems, Big Data Technologies, Data Modeling, Data Lineage, Orchestration, Cloud Computing, Data Visualization, Kafka"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Chennai, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune
Experience: 7-16 Years
Work Mode: Hybrid
Mandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)
Job Description
We are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.
Key Responsibilities
Design, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.
Collaborate with data analysts, data architects, and business stakeholders to align data models with business needs.
Leverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.
Manage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.
Contribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.
Apply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.
Stay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.
Develop and maintain data models using data modeling tools such as ER/Studio and Hackolade.
Drive the adoption of best practices and standards for data modeling within the organization.
Skills And Qualifications
Minimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.
Expertise in Azure and Databricks for building data solutions.
Proficiency in ER/Studio, Hackolade, and other data modeling tools.
Strong understanding of data modeling principles and techniques (e.g., ERD, UML).
Experience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Solid understanding of data warehousing, ETL processes, and data integration.
Familiarity with big data technologies such as Hadoop and Spark is an advantage.
Industry Knowledge: A background in supply chain is preferred but not mandatory.
Excellent analytical and problem-solving skills.
Strong communication skills, with the ability to interact with both technical and non-technical stakeholders.
Ability to work well in a collaborative, fast-paced environment.
Education
B.Tech in any branch or specialization
Skills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
Data Bricks Architect,Gramener,7-9 Years,,"Chennai, India",Login to check your skill match score,"Data bricks Architect
Work Location: Hyderabad/Bangalore/Chennai/Noida/Mumbai
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
You will engage in diverse impactful customer technical Big Data projects, including developing reference architectures, how-to guides, and minimally viable products (MVPs).
Lead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading big data and AI applications.
Provide architectural guidance that fosters the adoption of Databricks across business-facing functions.
Collaborate with platform engineering teams to effectively implement Databrick services within our infrastructure.
Evaluate and assess new features and enhancements on the Databricks platform to ensure we leverage the latest capabilities.
Designed and implemented various integration patterns involving Databricks and third-party tools such as Collibra and data quality solutions.
Utilize your AWS administration and architecture expertise to optimize cloud resources and ensure seamless integration with Databricks.
Leverage your hands-on experience with Databricks Unity Catalog to implement robust data governance and lineage capabilities.
Advocate for and implement CI/CD practices to streamline the deployment of Databricks solutions.
Contribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.
Understand and articulate the analytics capabilities of Databricks, enabling teams to derive actionable insights from their data.
Skills And Qualifications
7+ years of experience with Big Data technologies, including Apache Spark, cloud-native data lakes, and data mesh platforms, in a technical architecture or consulting role.
5+ years of independent experience in Big Data architecture.
Proficiency in Python coding and familiarity with data engineering best practices.
Extensive experience working with AWS cloud platforms, including a solid understanding of AWS services and architecture.
Substantial documentation and whiteboarding skills to effectively communicate complex ideas.
In-depth knowledge of the latest services offered by Databricks, with the ability to evaluate and integrate these services into our platform.
Proven experience implementing solutions using Databricks Unity Catalog, focusing on data governance and lineage tracking.
Demonstrated expertise in migrating from Databricks classic platform to Lakehouse architecture, utilizing Delta file format and/or Delta Live Tables.
A collaborative mindset with the ability to work effectively across teams and functions.
About Us
We consult and deliver solutions to organizations where data is the core of decision-making. We undertake strategic data consulting for organizations, laying out the roadmap for data-driven decision-making. This helps organizations convert data into a strategic differentiator. Through a host of our products, solutions, and Service Offerings, we analyze and visualize large amounts of data.
To learn more about us, visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Databricks Unity Catalog, Data Mesh, Apache Spark, Databricks, Python, AWS"
"Data Analytics Architect (Looker), Contract",66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees
66degrees is a leading Google Cloud Premier Partner. We believe that engineering takes heart. Focusing exclusively on Google Cloud, we help our clients achieve the most innovative and disruptive transformations in their industries.
66degrees is seeking a senior contractor- Data Analytics Architect (Looker) to engage on a 8 weeks of remote assignment for approximately 40 hours per week with potential to extend. Interested candidates should have the following required skills in Golang.
Please note: ***Candidates must be available to join by first week of May, 2025***
Responsibilities
Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
Work with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.
Advise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.
Utilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.
Write clean, efficient, and scalable code (LookML, Python as applicable)
Conduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.
Stay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.
Collaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.
Provide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.
Excellent comprehension of Looker and its implementation, with additional analytics platform experience a major plus - especially MicroStrategy.
Experience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.
Strong proficiency with SQL required.
Demonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.
Excellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.
Solid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.
Strong problem-solving skills and the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Google Cloud certifications and any analytics platform certifications are a plus.
A desire to stay ahead of the curve and continuously learn new technologies and techniques.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, LookML, Golang, Data Modeling, Google Cloud, Sql, Python"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.
Working with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.
Analyzing requirements: Performing requirement analysis and creating architectural models.
Identifying issues: Identifying operational issues and recommending strategies to resolve them.
Communicating with business users: Communicating technical solutions to business users and addressing their questions.
Validating solutions: Ensuring solutions align with corporate standards and compliance requirements.
Developing technical specifications: Creating technical design specifications for solutions and systems engineers.
Assessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.
Data engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.
About Us
We help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Databases, Python, Web Scraping, Apis"
Data Bricks Architect,Gramener,7-9 Years,,"Chennai, India",Login to check your skill match score,"Data bricks Architect
Work Location: Hyderabad/Bangalore/Chennai/Noida/Mumbai
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
You will engage in diverse impactful customer technical Big Data projects, including developing reference architectures, how-to guides, and minimally viable products (MVPs).
Lead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading big data and AI applications.
Provide architectural guidance that fosters the adoption of Databricks across business-facing functions.
Collaborate with platform engineering teams to effectively implement Databrick services within our infrastructure.
Evaluate and assess new features and enhancements on the Databricks platform to ensure we leverage the latest capabilities.
Designed and implemented various integration patterns involving Databricks and third-party tools such as Collibra and data quality solutions.
Utilize your AWS administration and architecture expertise to optimize cloud resources and ensure seamless integration with Databricks.
Leverage your hands-on experience with Databricks Unity Catalog to implement robust data governance and lineage capabilities.
Advocate for and implement CI/CD practices to streamline the deployment of Databricks solutions.
Contribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.
Understand and articulate the analytics capabilities of Databricks, enabling teams to derive actionable insights from their data.
Skills And Qualifications
7+ years of experience with Big Data technologies, including Apache Spark, cloud-native data lakes, and data mesh platforms, in a technical architecture or consulting role.
5+ years of independent experience in Big Data architecture.
Proficiency in Python coding and familiarity with data engineering best practices.
Extensive experience working with AWS cloud platforms, including a solid understanding of AWS services and architecture.
Substantial documentation and whiteboarding skills to effectively communicate complex ideas.
In-depth knowledge of the latest services offered by Databricks, with the ability to evaluate and integrate these services into our platform.
Proven experience implementing solutions using Databricks Unity Catalog, focusing on data governance and lineage tracking.
Demonstrated expertise in migrating from Databricks classic platform to Lakehouse architecture, utilizing Delta file format and/or Delta Live Tables.
A collaborative mindset with the ability to work effectively across teams and functions.
About Us
We consult and deliver solutions to organizations where data is the core of decision-making. We undertake strategic data consulting for organizations, laying out the roadmap for data-driven decision-making. This helps organizations convert data into a strategic differentiator. Through a host of our products, solutions, and Service Offerings, we analyze and visualize large amounts of data.
To learn more about us, visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Databricks Unity Catalog, data mesh architecture, Apache Spark, Data Governance, Databricks, Python, Big Data Technologies, AWS"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.
Working with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.
Analyzing requirements: Performing requirement analysis and creating architectural models.
Identifying issues: Identifying operational issues and recommending strategies to resolve them.
Communicating with business users: Communicating technical solutions to business users and addressing their questions.
Validating solutions: Ensuring solutions align with corporate standards and compliance requirements.
Developing technical specifications: Creating technical design specifications for solutions and systems engineers.
Assessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.
Data engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.
About Us
We help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Databases, Python, Web Scraping, Apis"
Data & Analytics Architect,AuxoAI,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
As a Data & Analytics Architect, you will lead key data initiatives, including cloud transformation, data governance, and AI projects. You'll define cloud architectures, guide data science teams in model development, and ensure alignment with data architecture principles across complex solutions. Additionally, you will create and govern architectural blueprints, ensuring standards are met and promoting best practices for data integration and consumption.
Responsibilities
Play a key role in driving a number of data and analytics initiatives including cloud data transformation, data governance, data quality, data standards, CRM, MDM, Generative AI and data science.
Define cloud reference architectures to promote reusable patterns and promote best practices for data integration and consumption.
Guide the data science team in implementing data models and analytics models.
Serve as a data science architect delivering technology and architecture services to the data science community.
In addition, you will also guide application development teams in the data design of complex solutions, in a large data eco-system, and ensure that teams are in alignment with the data architecture principles, standards, strategies, and target states.
Create, maintain, and govern architectural views and blueprints depicting the Business and IT landscape in its current, transitional, and future state.
Define and maintain standards for artifacts containing architectural content within the operating model.
Requirements
Strong cloud data architecture knowledge (preference for Microsoft Azure)
8-10+ years of experience in data architecture, with proven experience in cloud data transformation, MDM, data governance, and data science capabilities.
Design reusable data architecture and best practices to support batch/streaming ingestion, efficient batch, real-time, and near real-time integration/ETL, integrating quality rules, and structuring data for analytic consumption by end uses.
Ability to lead software evaluations including RFP development, capabilities assessment, formal scoring models, and delivery of executive presentations supporting a final recommendation.
Well versed in the Data domains (Data Warehousing, Data Governance, MDM, Data Quality, Data Standards, Data Catalog, Analytics, BI, Operational Data Store, Metadata, Unstructured Data, non-traditional data and multi-media, ETL, ESB).
Experience with cloud data technologies such as Azure data factory, Azure Data Fabric, Azure storage, Azure data lake storage, Azure data bricks, Azure AD, Azure ML etc.
Experience with big data technologies such as Cloudera, Spark, Sqoop, Hive, HDFS, Flume, Storm, and Kafka.","Azure Data Lake Storage, HDFS, Azure Data Fabric, Azure Ad, Flume, Azure Storage, Cloudera, Azure Data Factory, Storm, Kafka, Sqoop, Hive, Microsoft Azure, Azure ML, Spark"
Data Platform Architect,Gramener,Fresher,,"Chennai, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore/Chennai
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career path, steady growth prospects with great scope to innovate. Our goal is to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
You will engage in a diverse range of impactful customer technical Data platform projects, Including the development.
Lead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading.
Collaborate with platform engineering teams to effectively implement Data Brick services within our infrastructure.
Leverage your hands-on experience with Data Bricks Unity Catalog to implement robust data governance and lineage capabilities.
Advocate for and implement CI/CD practices to streamline the deployment of Data bricks solutions.
Contribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.
Understand and articulate the analytics capabilities of the Data platform, enabling teams to derive actionable insights from their data.
Skills And Qualifications
Cloud and Architectures:
Azure Architecture and Platform: Expertise in Azure Data Lake, AI/ML model hosting, Key Vault, Event Hub, Logic Apps, and other Azure cloud services.
Databricks Development: Strong integration with Azure, workflow orchestration, and governance.
Data Engineering & Architecture: Hands-on experience with scalable ETL/ELT pipelines, Delta Lake, and enterprise data management.
Coding And Implementation
Software Engineering: Modular design, CI/CD, version control, and best coding and design practices.
Python and PySpark: Experience in building reusable packages and components for both technical and business users, including data validation, lineage modules, and accelerators for data processing.
Process And Compliance
Enterprise Process Understanding: Experience with large-scale workflows, structured environments, and compliance frameworks.
Governance and Security: Knowledge of data lineage, GxP, HIPAA, GDP compliance, and regulatory requirements.
Pharma, MedTech, Life Sciences (Good to Have): Understanding industry-specific data, regulatory constraints, and security considerations.
About Us
We help consult and deliver solutions to organizations where data acts as the core of decision making. We undertake strategic data consulting for organizations in laying out the roadmap for data-driven decision making. It helps organizations to convert data into a strategic differentiator. Through a host of our product and Solutions, and Service Offerings, we analyze and visualize large amounts of data.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Logic Apps, Key Vault, Databricks Development, Event Hub, CI CD, AI ML model hosting, enterprise data management, GDP compliance, Azure Architecture, ETL ELT pipelines, Delta Lake, Data Validation, enterprise process understanding, Security, Governance, Data Lineage, Version Control, Pyspark, Software Engineering, Azure Data Lake, Python"
CFIN P2D & Data Solution Architect,ABB,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN P2D & Data Solution Architect
At ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.
Write the next chapter of your ABB story.
This position reports to
P2D & Data Solution Lead
Your role and responsibilities
We are seeking a skilled P2D & Data Solution architect to play a key role in configuring, implementing, and supporting P2D/ Data processes within the Central Finance (CFIN) system. The P2D & Data Solution architect will collaborate closely with business stakeholders, process owners, and technical teams to ensure seamless integration of P2D functions within the CFIN framework. This role requires deep functional knowledge of CO processes, as well as the ability to translate business requirements into system configurations that optimize efficiency, accuracy, and business performance.
This position requires close coordination with Deployment team, Functional architects and external vendors, to maintain and evolve the P2D & Data architecture, ensuring it meets business needs and complies with ABB's standards.
The work model for the role is:
This role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.
You will be mainly accountable for:
P2D Process Configuration: Configure and maintain P2D processes such as include Contribution margin reporting, COPA characteristics derivation and compliance with local and global P2D regulations within the Central Finance (CFIN) system, ensuring alignment with business needs and industry best practices.
Data replication: Oversee the configuration, implementation, and ongoing support of Data related topics within the CFIN system, ensuring processes are optimized and aligned with organizational goals. Expertise in Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data
Requirements Gathering & Analysis: Collaborate with business stakeholders and process owners to thoroughly understand their requirements, document functional specifications, and translate these into system configurations.
System Integration: Facilitate seamless integration of P2D & Data processes within the CFIN system, as well as with other enterprise systems (Local ERP, etc.), ensuring smooth data flow and automation of processes across platforms. User Support & Training: Provide ongoing functional support to end-users, addressing issues, offering solutions, and conducting training to ensure optimal utilization of the system and full understanding of P2D processes.
Testing & Validation: Participate in testing and validation activities for new configurations, enhancements, and fixes, ensuring they meet business and functional requirements. Process Optimization: Regularly monitor and evaluate P2D & Data processes to identify opportunities for automation, efficiency improvements, and best practice implementation within the CFIN system.
Documentation & Compliance: Develop and maintain comprehensive documentation for P2D system configurations, process flows, and integration points, ensuring compliance with internal standards and regulatory guidelines. Collaboration with Technical Teams: Work closely with IS architects, developers, and technical teams to ensure that functional requirements are correctly implemented and aligned with system design specifications.
Troubleshooting & Issue Resolution: Provide expert troubleshooting support for P2D -related system issues, working collaboratively with cross-functional teams to resolve any challenges promptly. Continuous Improvement: Stay informed about the latest industry trends, best practices, and system updates to continuously enhance the efficiency and effectiveness of P2D processes within CFIN.
Project and New Demand Management: Take ownership of configuring new demands or changes in system functionality, ensuring proper alignment with system design documentation and business requirements. Data Management: Oversee the management and accuracy of all data related topics within O2C/ P2P/ TRE / R2R / TAX within the system, ensuring data integrity, consistency, and compliance with business rules across processes.
Qualifications for the role
Education: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in CO - SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.
Proven experience (5+ years) in FICO processes, with a strong background in system configuration and implementation within SAP or similar ERP environments. Experience in configuring CO processes in SAP or similar ERP systems, with a solid understanding of integration points and data flows across systems.
Familiarity with Central Finance (CFIN) and integration with other finance-related modules. Experience with requirements gathering, business analysis, and documentation of functional specifications.
High level understanding of local ERP Contribution margin Reporting, COPA characteristics Derivation, COGS split, Price Difference Split, Profit Center, Cost Center, WBS Element, Order master Data mapping
Strong analytical skills and attention to detail. Excellent communication skills, with the ability to collaborate effectively with cross-functional teams, stakeholders, and technical teams.
A strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.
Experience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.
More about us
ABB Finance is a trusted partner to the business and a world-class team who delivers forward-looking insights that drive sustainable long-term results and operates with the highest standards.
We value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory
It has come to our attention that the name of ABB is being used for asking candidates to make payments for job
opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made
available on our career portal for all fitting the criteria to apply.
ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions.
For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Error resolution, AIF monitoring, Currency Values mismatch, Process Optimization, Troubleshooting, Continuous Improvement, Documentation compliance, SLT based filtering, Testing and validation, CO processes, PC G L CC Map, P2D Data processes, Mdg, System integration, Sap Ecc, Data Management, Data Replication, Requirements Gathering, User Support"
Cloud Data & AI Architect,myCloudDoor,3-5 Years,,"Kolkata, India",Login to check your skill match score,"Description
Do you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!
Who we are
myCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.
Tasks
The Selected Person Will Do The Following Tasks:
Definition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...
Maintenance of data solutions, failure analysis and solution proposal.
Communication with customers: proposal solutions, technical trainings...
The profile
We are looking for a person who fit the following requirements:
+3 years of experience years of experience in a similar role.
Experience in Azure projects
Real experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...
Real Experience in AI
Experience in presales and proposals
What we offer you
Career Path
Remote working
Training: Internal and technical certifications
Think you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.
>
Do you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!
Who we are
myCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.
Tasks
The Selected Person Will Do The Following Tasks:
Definition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...
Maintenance of data solutions, failure analysis and solution proposal.
Communication with customers: proposal solutions, technical trainings...
The profile
We are looking for a person who fit the following requirements:
+3 years of experience years of experience in a similar role.
Experience in Azure projects
Real experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...
Real Experience in AI
Experience in presales and proposals
What we offer you
Career Path
Remote working
Training: Internal and technical certifications
Think you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.","Presales, Ai, Data Lakes, Azure, Data Architecture, Data Warehouse"
Salesforce Data Cloud Architect,VAYUZ Technologies,Fresher,,"Bengaluru, India",Login to check your skill match score,"JOB DESCRIPTION
Job Locations: Bangalore, Hyderabad, Chennai, Noida, Pune, Gurgaon, Mumbai
Role Expectations:
1) Salesforce Data Cloud Expertise
Leverage in-depth knowledge of Salesforce Data Cloud to implement and manage scalable solutions.
Stay current with platform updates and recommend best practices for optimization and innovation.
2) Data Modeling
Design and develop efficient and scalable data models that align with business requirements.
Collaborate with stakeholders to translate data needs into logical and physical data models.
3) Data Integration
Lead the integration of external and internal data sources using tools like MuleSoft, Informatica, or native Salesforce connectors.
Ensure seamless data flow across systems, maintaining integrity and performance.
4) Data Quality
Establish and enforce data quality standards, including validation, cleansing, and enrichment processes.
Monitor and improve data accuracy, consistency, and reliability within the Salesforce Data Cloud.
5) Data Governance:
Implement data governance frameworks, ensuring compliance with organizational policies and industry regulations.
Define data ownership, access controls, and stewardship roles.
6) SQL Proficiency
Write and optimize SQL queries to extract, manipulate, and analyze data from multiple sources.
Support reporting, dashboard development, and data validation tasks using SQL.
7) Problem Solving and Analysis
Analyze complex data challenges, identify root causes, and implement effective solutions.
Collaborate across teams to troubleshoot data-related issues and deliver actionable insights
Qualifications:
Salesforce Data Cloud Expertise: Extensive knowledge of Salesforce Data Cloud features, capabilities, and best practices.
Data Modeling: Strong experience in designing and implementing data models.
Data Integration: Experience with data integration tools and techniques.
Data Quality: Understanding of data quality concepts and practices.
Data Governance: Knowledge of data governance principles and practices.
SQL: Proficiency in SQL for data querying and manipulation.
Problem-Solving: Strong analytical and problem-solving skills.
Communication: Excellent communication and collaboration skills.","Salesforce Data Cloud, Data Quality, Data Modeling, Mulesoft, Informatica, Data Governance, Sql, Data Integration"
Data Domain Architect Assoc - Conversational AI Annotation,JPMorganChase,3-5 Years,,"Bengaluru, India",Login to check your skill match score,"Job Description
Join us for an exciting opportunity to leverage your advanced data annotation skills in the financial industry and contribute to cutting-edge machine learning models.
As a Data Domain Architect Associate within the Consumer & Community Banking team, you will conduct ML assisted data labeling to evaluate and train machine learning models. You will apply your technical knowledge and problem-solving skills across multiple applications, supporting data quality and summarization to enable Operations Management to achieve strategic objectives while ensuring compliance with all controls, policies, and procedures.
Job Responsibilities
Build & Annotate banking domain text data for LLM/Gen AI models using various data labeling tools, taxonomy, and guidelines.
Analyze structured and unstructured text data, identify labels through context and disambiguation, and annotate with the correct label.
Understand the nuances of language used in the financial industry and stay updated with the business aspects of products.
Validate business results from a model perspective and provide feedback for model improvement using formulated metricies.
Conduct retrospective data analyses and contribute to clarifying business definitions and concepts.
Provide feedback and suggestions for tool improvements to enhance efficiency and accuracy.
Work closely with stakeholders, including machine learning engineers, data scientists, data engineers, and product managers across Chase's lines of businesses.
Required Qualifications, Capabilities, And Skills
Bachelor's or Master's degree in Statistics, Engineering, Computer Science, Information Technology, Finance, or a related field.
Three years of hands-on experience working with data as a data analyst (conversational AI), data annotator, or data architect.
Basic understanding of LLM/Gen AI with expertise in text data labeling processes and quality control.
Analytical and problem-solving skills, along with good project management skills (self-driven, well-organized, ability to meet tight deadlines).
Experience using Python script, GIT version control, at least one annotation tool.
Intermediate experience with Microsoft Office suite.
Experience in conversational AI/ Chat bots training data needs.
Preferred Qualifications, Capabilities, And Skills
Understanding of LLM and Gen AI concepts.
Excellent verbal and written communication skills to clearly present analytical findings and business recommendations to global stakeholders.
Familiarity or experience with Data Analytics and Visualization.
ABOUT US
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our Consumer & Community Banking division serves our Chase customers through a range of financial services, including personal banking, credit cards, mortgages, auto financing, investment advice, small business loans and payment processing. We're proud to lead the U.S. in credit card sales and deposit growth and have the most-used digital solutions all while ranking first in customer satisfaction.
The CCB Data & Analytics team responsibly leverages data across Chase to build competitive advantages for the businesses while providing value and protection for customers. The team encompasses a variety of disciplines from data governance and strategy to reporting, data science and machine learning. We have a strong partnership with Technology, which provides cutting edge data and analytics infrastructure. The team powers Chase with insights to create the best customer and business outcomes.","data annotation, conversational AI, Microsoft Office Suite, GIT version control, text data labeling, LLM Gen AI, Python"
Data & Analytics Architect,Flexera,15-17 Years,,India,Login to check your skill match score,"Flexera saves customers billions of dollars in wasted technology spend. A pioneer in Hybrid ITAM and FinOps, Flexera provides award-winning, data-oriented SaaS solutions for technology value optimization (TVO), enabling IT, finance, procurement and cloud teams to gain deep insights into cost optimization, compliance and risks for each business service. Flexera One solutions are built on a set of definitive customer, supplier and industry data, powered by our Technology Intelligence Platform, that enables organizations to visualize their Enterprise Technology Blueprint in hybrid environmentsfrom on-premises to SaaS to containers to cloud.
We're transforming the software industry. We're Flexera. With more than 50,000 customers across the world, we're achieving that goal. But we know we can't do any of that without our team. Ready to help us re-imagine the industry during a time of substantial growth and ambitious plans Come and see why we're consistently recognized by Gartner, Forrester and IDC as a category leader in the marketplace. Learn more at flexera.com
At Flexera, we're on a mission to empower global enterprises by transforming IT insights into decisive actions. We are looking for an architect/principal engineer with deep expertise in analytics, including data modeling, semantic modeling, to take our world-class reference data in Technopedia to the next level, through deeper insights, richer features and more data sets. The ideal candidate will have a strong track record of providing technical leadership, deep technical expertise and delivering big data solutions through data lakehouse and large scale-processing technologies.
As an architect/principal engineer, you will be a key player in architecting, designing, developing, and maintaining our common ontology and data models, transformations, and analytics capabilities for our industry-leading reference data. You will collaborate closely with other architects and guide, influence and help engineering teams to ensure seamless integration with other components of our system.
What you'll do:
Architect, design and develop our big data platform, processing pipelines and content reference data.
Drive innovation, enabling capabilities to enhance speed to innovation for our analytics, AI/ML, and product development teams.
Define data models and semantic models for our wide range of data sets to provide insights and optimized user experiences.
Provide guidance, influence and help engineering teams to deliver deep insights from our large and broad data sets.
Continuously enhance your technical skills and mentor other engineers.
Promote a high-standard engineering culture and operational excellence within the team.
Collaborate with product and design teams to ensure the platform meets user needs.
Define project requirements, technical artifacts, and designs, driving consensus across Product Management, architects, and engineering teams.
Balance priorities between new feature development, architectural enhancements, and technical debt reduction for a sustainable and scalable platform.
Champion continuous improvement in product quality, security, and performance standards within the development team.
You'll be expected to have:
Bachelor's or higher degree in Computer Science, Software Engineering, or related field
Minimum 15 years relevant experience in software development, including 8+ years of expertise in data modeling, semantic modeling, and data visualization.
Strong expertise in other big data technologies, such as metadata catalogs, data lineage, and orchestration.
Strong technical expertise in distributed systems, big data, and cloud computing.
Strong problem-solving skills and ability to troubleshoot complex issues.
Good understanding of streaming technologies, including Kafka, with experience in defining message schemas and data models.
Outstanding communication skills and emotional intelligence to collaborate effectively with teams across the organization.
Excellent written and verbal communication skills.
Ability to work effectively both independently and in a collaborative team environment.
Prior experience mentoring and providing technical guidance to junior engineers.
At Flexera, we foster a fun and engaged hybrid working environment where collaboration and innovation thrive. We value diversity and encourage applicants from underrepresented groups in technology to apply.
Join our team to not only contribute to a world-class global product but also to grow in your career. At Flexera, we encourage continuous learning and provide opportunities for professional development.
Flexera is proud to be an equal opportunity employer. Qualified applicants will be considered for open roles regardless of age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by local/national laws, policies and/or regulations.
Flexera understands the value that results from employing a diverse, equitable, and inclusive workforce. We recognize that equity necessitates acknowledging past exclusion and that inclusion requires intentional effort. Our DEI (Diversity, Equity, and Inclusion) council is the driving force behind our commitment to championing policies and practices that foster a welcoming environment for all.
We encourage candidates requiring accommodations to please let us know by emailing [HIDDEN TEXT].","streaming technologies, semantic modeling, metadata catalogs, Orchestration, Data Lineage, Distributed Systems, Big Data Technologies, Kafka, Data Visualization, Data Modeling, Cloud Computing"
Lead Data Integration Architect,Hitachi Digital,10-12 Years,,India,Login to check your skill match score,"Our Company
We're Hitachi Digital, a company at the forefront of digital transformation and the fastest growing division of Hitachi Group. We're crucial to the company's strategy and ambition to become a premier global player in the massive and fast-moving digital transformation market.
Our group companies, including GlobalLogic, Hitachi Digital Services, Hitachi Vantara and more, offer comprehensive services that span the entire digital lifecycle, from initial idea to full-scale operation and the infrastructure to run it on. Hitachi Digital represents One Hitachi, integrating domain knowledge and digital capabilities, and harnessing the power of the entire portfolio of services, technologies, and partnerships, to accelerate synergy creation and make real-world impact for our customers and society as a whole.
Imagine the sheer breadth of talent it takes to unleash a digital future. We don't expect you to fit every requirement your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.
Preferred job location: Bengaluru, Hyderabad, Pune, New Delhi or Remote
The team
Hitachi Digital is a leader in digital transformation, leveraging advanced AI and data technologies to drive innovation and efficiency across various operational companies (OpCos) and departments. We are seeking a highly experienced Lead Data Integration Architect to join our dynamic team and contribute to the development of robust data integration solutions.
The role
Lead the design, development, and implementation of data integration solutions using SnapLogic, MuleSoft, or Pentaho.
Develop and optimize data integration workflows and pipelines.
Collaborate with cross-functional teams to integrate data solutions into existing systems and workflows.
Implement and integrate VectorAI and Agent Workspace for Google Gemini into data solutions.
Conduct research and stay updated on the latest advancements in data integration technologies.
Troubleshoot and resolve complex issues related to data integration systems and applications.
Document development processes, methodologies, and best practices.
Mentor junior developers and participate in code reviews, providing constructive feedback to team members.
Provide strategic direction and leadership in data integration and technology adoption.
What You'll Bring
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
10+ years of experience in data integration, preferably in the Banking or Finance industry.
Extensive experience with SnapLogic, MuleSoft, or Pentaho (at least one is a must).
Experience with Talend and Alation is a plus.
Strong programming skills in languages such as Python, Java, or SQL.
Technical proficiency in data integration tools and platforms.
Knowledge of cloud platforms, particularly Google Cloud Platform (GCP).
Experience with VectorAI and Agent Workspace for Google Gemini.
Comprehensive knowledge of financial products, regulatory reporting, credit risk, and counterparty risk.
Prior strategy consulting experience with a focus on change management and program delivery preferred.
Excellent problem-solving skills and the ability to work independently and as part of a team.
Strong communication skills and the ability to convey complex technical concepts to non-technical stakeholders.
Proven leadership skills and experience in guiding development projects from conception to deployment.
Preferred Qualifications:
Familiarity with data engineering tools and techniques.
Previous experience in a similar role within a tech-driven company.
About Us
We're a global, 1000-stong, diverse team of professional experts, promoting and delivering Social Innovation through our One Hitachi initiative (OT x IT x Product) and working on projects that have a real-world impact. We're curious, passionate and empowered, blending our legacy of 110 years of innovation with our shaping our future. Here you're not just another employee; you're part of a tradition of excellence and a community working towards creating a digital future.
Championing diversity, equity, and inclusion
Diversity, equity, and inclusion (DEI) are integral to our culture and identity. Diverse thinking, a commitment to allyship, and a culture of empowerment help us achieve powerful results. We want you to be you, with all the ideas, lived experience, and fresh perspective that brings. We support your uniqueness and encourage people from all backgrounds to apply and realize their full potential as part of our team.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We're also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We're always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you'll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We're proud to say we're an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.","Alation, VectorAI, Agent Workspace for Google Gemini, Pentaho, Java, Google Cloud Platform, Mulesoft, Snaplogic, Talend, Sql, Python"
Data Engineering Architect,MindBrain,8-10 Years,,India,Login to check your skill match score,"Job Title: Data Engineering Architect
Location: Remote
Duration: 6 Months-12 months
Start Date: Within 2 Weeks
Client Interview Required: Yes
Job Summary
We are seeking a highly skilled Data Engineering Architect to design and build a scalable, secure, and analytics-ready HR data platform. This is a strategic and technical leadership role at the core of our product team, enabling data-driven decision-making and delivering a secure, multi-tenant, cloud-based architecture for HR data domains such as Employees, Jobs, Payroll, and Performance Management.
Key Responsibilities1. Data Modeling
Design and maintain scalable, efficient, and HR-centric data models (Employees, Jobs, Payroll, Performance).
Implement Slowly Changing Dimensions (SCD) for historical tracking (e.g., job changes, salary revisions).
Ensure data schemas are analytics- and reporting-ready for downstream consumption.
Collaborate with product and analytics teams to understand and implement data requirements.
2. ETL/ELT Development
Build and maintain robust ETL/ELT pipelines using modern tools such as dbt, Apache Airflow, Fivetran, or Informatica.
Ingest and harmonize data from various internal and third-party HR systems.
Implement data quality checks, consistency validation, and reconciliation processes.
Automate and monitor data pipelines for performance and reliability.
3. Cloud Data Warehouse Architecture
Architect and manage scalable data solutions on Snowflake, Google BigQuery, or Amazon Redshift.
Optimize cloud data warehouse infrastructure for performance, cost-efficiency, and storage scalability.
Implement data partitioning, clustering, and performance tuning strategies.
4. Data Security & Compliance
Apply robust Role-Based Access Control (RBAC) and tenant-level isolation for secure data access.
Ensure PII masking, secure data handling, and compliance with GDPR, CCPA, and other data regulations.
Maintain data lineage, audit trails, and metadata management for governance.
5. Multi-Tenant SaaS Enablement
Architect systems to support multi-tenant SaaS environments with secure data segregation.
Develop strategies for tenant-level metadata, access control, and data isolation.
Support tenant-specific configurations and onboarding in the data architecture.
6. Analytics & Business Intelligence (BI) Enablement
Design materialized views, summary tables, and data marts for downstream analytics.
Enable seamless integration with Looker, Power BI, and Tableau for BI reporting.
Work closely with analytics and business teams to build dashboards and ensure timely insights.
Required Skills & Experience
8+ years of experience in Data Engineering, Data Warehousing, or similar technical fields.
Proven expertise in SQL and hands-on experience with cloud data warehouses:
Snowflake, Google BigQuery, or Amazon Redshift.
Strong working knowledge of modern ETL/ELT frameworks:
dbt, Apache Airflow, Fivetran, Informatica.
Deep understanding of HR data models including:
Employees, Payroll, Job Changes, Performance Management.
Strong experience in data security, PII protection, and compliance (GDPR/CCPA).
Experience building and managing multi-tenant data architectures in SaaS platforms.
Solid understanding of data governance, metadata management, and auditing frameworks.
Proficiency in data pipeline automation, monitoring, and CI/CD practices for data workflows.
Excellent problem-solving ability and attention to detail.
Strong verbal and written communication and documentation skills.
Preferred Qualifications
Experience working in a SaaS or multi-tenant environment.
Familiarity with DevOps practices and tools for data infrastructure:
CI/CD, Terraform, GitHub Actions, etc.
Experience with HR tech platforms:
Workday, SAP SuccessFactors, Oracle HCM, BambooHR.
Certifications in cloud technologies (any of the below is a plus):
Snowflake SnowPro, AWS Certified Data Analytics, Google Cloud Professional Data Engineer.","PII protection, snowflake, dbt, Google BigQuery, Fivetran, CCPA, multi-tenant data architectures, auditing frameworks, data pipeline automation, CI CD practices, Informatica, Sql, Apache Airflow, Gdpr, Data Governance, Metadata Management, Amazon Redshift, Data Security"
Data Domain Architect Associate,JPMorganChase,4-6 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
You are a highly motivated individual with strong skills in advanced analytics, problem-solving, influencing, interpersonal communication, and collaboration, then you have found the right team. Join us as an Automation and Analytics Developer within the Conduct, Compliance, and Operational Risk (CCOR) domain. In this role, you will develop and maintain a diverse range of analytical tools, reports, and dashboards to uncover insights and manage risks effectively. You will work closely with Compliance and Operational Risk Managers, as well as key partners across various functions, including Data Science, Technology, and Business colleagues.
As a Developer in our CCOR Data Analytics team, your responsibilities will include promoting and identifying opportunities for operational reengineering, and leading automation initiatives to streamline processes. You will have the chance to shape and improve our risk and control processes using data transformation tools such as Alteryx, Python, and SQL, while applying your knowledge of data science principles.
Job Responsibilities
Understands and develops automation solutions for the compliance and operational risk managers; builds analytical tools, reports, and dashboards to optimize and reduce manual processes.
Identifies and leads automation initiatives to streamline processes.
Creates reporting, interprets results, and conveys information in a concise, straightforward, and professional manner for all levels of operational staff from supervisors to senior-level management.
Comprehends data requirements and accurately addresses related data quality edits efficiently.
Spearheads projects and tasks by ensuring timely completion and articulates any issues and risks to management.
Ensures the integrity of data through automated extraction, translation, processing, analysis, and reporting.
Required Qualifications, Skills, And Capabilities
Tool Sets Strong knowledge of Alteryx, Tableau, Python or UIPath. Proficiency in Tableau is essential for creating interactive and insightful dashboards.
Analytical Independent, logical problem solver with the ability to synthesize data, identify trends, and project outcomes. Strong understanding of data science concepts and methodologies.
Technical Proficiency in PowerPoint and Excel; knowledge of databases and API connectivity.
Fast Paced Multi-Tasker Ability to organize and prioritize multiple projects and responsibilities with accuracy, attention to detail, and limited supervision with very short turnaround times. Demonstrates the ability to react quickly and decisively in high-stress situations.
Interpersonal Strong written and verbal communication skills with the ability to influence and work collaboratively with diverse/cross-functional teams. Develop and maintain effective relationships with a wide range of stakeholders.
Presentation Ability to create presentations for all levels of management and effectively report with an executive presence. Experience creating complex reporting with compelling key messages.
Risk & Controls Ability to work on Audit, Compliance, Risk, Control and Regulatory requirements in accordance with established procedures. Demonstrates accountability for work processes and the associated risks and controls. Demonstrates the ability to raise issues to relevant stakeholders or management with respect to the control environment.
Project Management Ability to lead an initiative, prioritize work, and meet deadlines, escalating any issues to management.
Preferred Qualifications, Skills, And Capabilities
Alteryx/Tableau/Python certification
Bachelor's Degree, preferably majored in Computer Science, Statistics, Math, Business Administration, Finance, or Economics
At least 4 years of experience in a related field.
Experience in data science projects or coursework, with a focus on data analysis and predictive modeling
ABOUT US
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Powerpoint, Alteryx, Tableau, Uipath, Excel, Sql, Python"
Data Modeller - Architect-10+ Years-Immediate,Cortex Consultants LLC,10-14 Years,,"Pune, India",Login to check your skill match score,"Job Title: Data Modeller - Architect
Experience: 10 to 14 Years
Location: Chennai, Hyderabad, Bangalore, Pune, Delhi
Job Type: Permanent Role
Notice Period: Immediate Joiners Only
Job Description
We are looking for an experienced Data Modeller - Architect with 10 to 14 years of expertise in designing and architecting data models for large-scale enterprise systems. The ideal candidate will have in-depth experience in data architecture, data modeling, and building scalable solutions. This is a permanent role with immediate joiner requirements across multiple locations.
Key Responsibilities
Design and architect data models for complex data environments and large datasets
Develop high-level data architecture solutions and manage data integration across systems
Collaborate with stakeholders to understand business requirements and translate them into effective data models
Lead and mentor teams in the development and implementation of data modeling strategies
Optimize database performance, ensuring data integrity and efficiency
Define and enforce best practices for data management and modeling across the organization
Work closely with IT, data engineering, and analytics teams to align data architecture with business goals
Ensure the scalability, security, and performance of data solutions
Required Skills & Qualifications
10 to 14 years of experience in data modeling and architecture
Strong expertise in relational and non-relational databases, data warehousing, and cloud data platforms
Proficiency in designing data models for complex, high-volume systems
Hands-on experience with SQL, NoSQL, and big data technologies
Proven track record of leading data architecture initiatives and cross-functional teams
Experience in data integration, ETL processes, and data governance
Immediate joiners only
Skills: non-relational databases,etl,nosql,data technologies,data architecture,analytics solutions,software development,sql,cloud platforms,relational databases,modeling,cloud data platforms,etl processes,big data technologies,data integration,data governance,data modeling,data management,data,data warehousing","Analytics Solutions, Relational Databases, ETL processes, cloud data platforms, non-relational databases, Data Management, Data Architecture, Data Warehousing, Big Data Technologies, Data Modeling, Sql, Nosql, Data Governance, Data Integration"
Data Modeller - Architect-10+ Years-Immediate,Cortex Consultants LLC,10-14 Years,,"Delhi, India",Login to check your skill match score,"Job Title: Data Modeller - Architect
Experience: 10 to 14 Years
Location: Chennai, Hyderabad, Bangalore, Pune, Delhi
Job Type: Permanent Role
Notice Period: Immediate Joiners Only
Job Description
We are looking for an experienced Data Modeller - Architect with 10 to 14 years of expertise in designing and architecting data models for large-scale enterprise systems. The ideal candidate will have in-depth experience in data architecture, data modeling, and building scalable solutions. This is a permanent role with immediate joiner requirements across multiple locations.
Key Responsibilities
Design and architect data models for complex data environments and large datasets
Develop high-level data architecture solutions and manage data integration across systems
Collaborate with stakeholders to understand business requirements and translate them into effective data models
Lead and mentor teams in the development and implementation of data modeling strategies
Optimize database performance, ensuring data integrity and efficiency
Define and enforce best practices for data management and modeling across the organization
Work closely with IT, data engineering, and analytics teams to align data architecture with business goals
Ensure the scalability, security, and performance of data solutions
Required Skills & Qualifications
10 to 14 years of experience in data modeling and architecture
Strong expertise in relational and non-relational databases, data warehousing, and cloud data platforms
Proficiency in designing data models for complex, high-volume systems
Hands-on experience with SQL, NoSQL, and big data technologies
Proven track record of leading data architecture initiatives and cross-functional teams
Experience in data integration, ETL processes, and data governance
Immediate joiners only
Skills: non-relational databases,etl,nosql,data technologies,data architecture,analytics solutions,software development,sql,cloud platforms,relational databases,modeling,cloud data platforms,etl processes,big data technologies,data integration,data governance,data modeling,data management,data,data warehousing","Analytics Solutions, Relational Databases, ETL processes, cloud data platforms, non-relational databases, Data Management, Data Architecture, Data Warehousing, Big Data Technologies, Data Modeling, Sql, Nosql, Data Governance, Data Integration"
Data Science Architect,Birdeye,7-10 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Job Type
Full-time
Description
Why Birdeye
Birdeye is the highest-rated reputation, social media, and customer experience platform for local businesses and brands. Over 150,000 businesses use Birdeye's AI-powered platform to effortlessly manage online reputation, connect with prospects through social media and digital channels, and gain customer experience insights to grow sales and thrive.
At Birdeye, innovation isn't just a goal it's our driving force. Our commitment to pushing boundaries and redefining industry standards has earned us accolades as one of the foremost providers of AI, Reputation Management, Social Media, and Customer Experience software by G2.
Founded in 2012 and headquartered in Palo Alto, Birdeye is led by a team of industry experts and innovators from Google, Amazon, Salesforce, and Yahoo. Birdeye is backed by the who's who of Silicon Valley - Salesforce founder Marc Benioff, Yahoo co-founder Jerry Yang, Trinity Ventures, World Innovation Lab, and Accel-KKR.
Roles & Responsibilities:
Design and implement scalable and robust ML infrastructure to support end-to-end machine learning workflows.
Develop and maintain CI/CD pipelines for ML models, ensuring smooth deployment and monitoring in production environments.
Collaborate with data scientists and software engineers to streamline the model development lifecycle, from experimentation to deployment and monitoring.
Implement best practices for version control, testing, and validation of ML models.
Ensure high availability and reliability of ML systems, including performance monitoring and troubleshooting.
Develop automation tools to facilitate data processing, model training, and deployment.
Stay up-to-date with the latest advancements in MLOps and integrate new technologies and practices as needed.
Mentor junior team members and provide guidance on MLOps best practices.
Requirements
Bachelor's/Master's degree in Computer Science, Engineering, or a related technical field with 7-10 years of experience.
Experience in designing and implementing ML infrastructure and MLOps pipelines.
Proficiency in cloud platforms such as AWS, Azure, or GCP.
Strong experience with containerization and orchestration tools like Docker and Kubernetes.
Experience with CI/CD tools such as Jenkins, GitLab CI, or CircleCI.
Solid programming skills in Python and familiarity with other programming languages such as Java or Scala.
Understanding of ML model lifecycle management, including versioning, monitoring, and retraining.
Experience with infrastructure-as-code tools like Terraform or CloudFormation.
Familiarity with data engineering tools and frameworks, such as Apache Spark, Hadoop, or Kafka.
Knowledge of security best practices for ML systems and data privacy regulations.
Excellent problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Experience with ML frameworks such as TensorFlow, PyTorch, or Scikit-learn.
Knowledge of data visualization tools and techniques.
Understanding of A/B testing and experimental design.
Strong analytical and troubleshooting skills.
Excellent communication and documentation skills.
Experience with monitoring and logging tools like Prometheus, Grafana, or ELK stack.
Knowledge of serverless architecture and functions-as-a-service (e.g., AWS Lambda).
Familiarity with ethical considerations in AI and machine learning.
Proven ability to mentor and train team members on MLOps practices.
Why You'll Join Us:
At Birdeye, we are relentless innovators driven by a singular goal: to lead our category with unparalleled excellence. We don't just set goals we surpass them. We're a team of doers who roll up our sleeves and get the job done, delivering on our promises with unwavering dedication.
Working here means embracing a culture of action and accountability, where every person is empowered to make an impact. We don't just talk about making a difference we make it happen.","CircleCI, ML infrastructure, Scikit-learn, MLOps pipelines, GitLab CI, Prometheus, Elk Stack, Kafka, Grafana, Tensorflow, Pytorch, Docker, Terraform, Python, AWS, Java, Aws Lambda, Hadoop, Cloudformation, Scala, Apache Spark, Jenkins, Gcp, Azure, Kubernetes"
Data Domain Architect Associate,JPMorganChase,4-6 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
You are a highly motivated individual with strong skills in advanced analytics, problem-solving, influencing, interpersonal communication, and collaboration, then you have found the right team. Join us as an Automation and Analytics Developer within the Conduct, Compliance, and Operational Risk (CCOR) domain. In this role, you will develop and maintain a diverse range of analytical tools, reports, and dashboards to uncover insights and manage risks effectively. You will work closely with Compliance and Operational Risk Managers, as well as key partners across various functions, including Data Science, Technology, and Business colleagues.
As a Developer in our CCOR Data Analytics team, your responsibilities will include promoting and identifying opportunities for operational reengineering, and leading automation initiatives to streamline processes. You will have the chance to shape and improve our risk and control processes using data transformation tools such as Alteryx, Python, and SQL, while applying your knowledge of data science principles.
Job Responsibilities
Understands and develops automation solutions for the compliance and operational risk managers; builds analytical tools, reports, and dashboards to optimize and reduce manual processes.
Identifies and leads automation initiatives to streamline processes.
Creates reporting, interprets results, and conveys information in a concise, straightforward, and professional manner for all levels of operational staff from supervisors to senior-level management.
Comprehends data requirements and accurately addresses related data quality edits efficiently.
Spearheads projects and tasks by ensuring timely completion and articulates any issues and risks to management.
Ensures the integrity of data through automated extraction, translation, processing, analysis, and reporting.
Required Qualifications, Skills, And Capabilities
Tool Sets Strong knowledge of Alteryx, Tableau, Python or UIPath. Proficiency in Tableau is essential for creating interactive and insightful dashboards.
Analytical Independent, logical problem solver with the ability to synthesize data, identify trends, and project outcomes. Strong understanding of data science concepts and methodologies.
Technical Proficiency in PowerPoint and Excel; knowledge of databases and API connectivity.
Fast Paced Multi-Tasker Ability to organize and prioritize multiple projects and responsibilities with accuracy, attention to detail, and limited supervision with very short turnaround times. Demonstrates the ability to react quickly and decisively in high-stress situations.
Interpersonal Strong written and verbal communication skills with the ability to influence and work collaboratively with diverse/cross-functional teams. Develop and maintain effective relationships with a wide range of stakeholders.
Presentation Ability to create presentations for all levels of management and effectively report with an executive presence. Experience creating complex reporting with compelling key messages.
Risk & Controls Ability to work on Audit, Compliance, Risk, Control and Regulatory requirements in accordance with established procedures. Demonstrates accountability for work processes and the associated risks and controls. Demonstrates the ability to raise issues to relevant stakeholders or management with respect to the control environment.
Project Management Ability to lead an initiative, prioritize work, and meet deadlines, escalating any issues to management.
Preferred Qualifications, Skills, And Capabilities
Alteryx/Tableau/Python certification
Bachelor's Degree, preferably majored in Computer Science, Statistics, Math, Business Administration, Finance, or Economics
At least 4 years of experience in a related field.
Experience in data science projects or coursework, with a focus on data analysis and predictive modeling
ABOUT US
JPMorganChase, one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.
We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. We also make reasonable accommodations for applicants and employees religious practices and beliefs, as well as mental health or physical disability needs. Visit our FAQs for more information about requesting an accommodation.
About The Team
Our professionals in our Corporate Functions cover a diverse range of areas from finance and risk to human resources and marketing. Our corporate teams are an essential part of our company, ensuring that we're setting our businesses, clients, customers and employees up for success.","Powerpoint, Alteryx, Tableau, Uipath, Excel, Sql, Python"
Data & Analytics Architect,AuxoAI,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
As a Data & Analytics Architect, you will lead key data initiatives, including cloud transformation, data governance, and AI projects. You'll define cloud architectures, guide data science teams in model development, and ensure alignment with data architecture principles across complex solutions. Additionally, you will create and govern architectural blueprints, ensuring standards are met and promoting best practices for data integration and consumption.
Responsibilities
Play a key role in driving a number of data and analytics initiatives including cloud data transformation, data governance, data quality, data standards, CRM, MDM, Generative AI and data science.
Define cloud reference architectures to promote reusable patterns and promote best practices for data integration and consumption.
Guide the data science team in implementing data models and analytics models.
Serve as a data science architect delivering technology and architecture services to the data science community.
In addition, you will also guide application development teams in the data design of complex solutions, in a large data eco-system, and ensure that teams are in alignment with the data architecture principles, standards, strategies, and target states.
Create, maintain, and govern architectural views and blueprints depicting the Business and IT landscape in its current, transitional, and future state.
Define and maintain standards for artifacts containing architectural content within the operating model.
Requirements
Strong cloud data architecture knowledge (preference for Microsoft Azure)
8-10+ years of experience in data architecture, with proven experience in cloud data transformation, MDM, data governance, and data science capabilities.
Design reusable data architecture and best practices to support batch/streaming ingestion, efficient batch, real-time, and near real-time integration/ETL, integrating quality rules, and structuring data for analytic consumption by end uses.
Ability to lead software evaluations including RFP development, capabilities assessment, formal scoring models, and delivery of executive presentations supporting a final recommendation.
Well versed in the Data domains (Data Warehousing, Data Governance, MDM, Data Quality, Data Standards, Data Catalog, Analytics, BI, Operational Data Store, Metadata, Unstructured Data, non-traditional data and multi-media, ETL, ESB).
Experience with cloud data technologies such as Azure data factory, Azure Data Fabric, Azure storage, Azure data lake storage, Azure data bricks, Azure AD, Azure ML etc.
Experience with big data technologies such as Cloudera, Spark, Sqoop, Hive, HDFS, Flume, Storm, and Kafka.","Azure Data Lake Storage, HDFS, Azure Data Fabric, Azure Ad, Flume, Azure Storage, Cloudera, Azure Data Factory, Storm, Kafka, Sqoop, Hive, Microsoft Azure, Azure ML, Spark"
CFIN P2D & Data Solution Architect,ABB,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"CFIN P2D & Data Solution Architect
At ABB, we are dedicated to addressing global challenges. Our core values: care, courage, curiosity, and collaboration - combined with a focus on diversity, inclusion, and equal opportunities - are key drivers in our aim to empower everyone to create sustainable solutions.
Write the next chapter of your ABB story.
This position reports to
P2D & Data Solution Lead
Your role and responsibilities
We are seeking a skilled P2D & Data Solution architect to play a key role in configuring, implementing, and supporting P2D/ Data processes within the Central Finance (CFIN) system. The P2D & Data Solution architect will collaborate closely with business stakeholders, process owners, and technical teams to ensure seamless integration of P2D functions within the CFIN framework. This role requires deep functional knowledge of CO processes, as well as the ability to translate business requirements into system configurations that optimize efficiency, accuracy, and business performance.
This position requires close coordination with Deployment team, Functional architects and external vendors, to maintain and evolve the P2D & Data architecture, ensuring it meets business needs and complies with ABB's standards.
The work model for the role is:
This role is contributing to the Finance Services business Finance Process Data Systems division in Bangalore, India.
You will be mainly accountable for:
P2D Process Configuration: Configure and maintain P2D processes such as include Contribution margin reporting, COPA characteristics derivation and compliance with local and global P2D regulations within the Central Finance (CFIN) system, ensuring alignment with business needs and industry best practices.
Data replication: Oversee the configuration, implementation, and ongoing support of Data related topics within the CFIN system, ensuring processes are optimized and aligned with organizational goals. Expertise in Error resolution based through AIF monitoring, Clearing/reference info, reconciliation based on PC G/L CC, Map managed experience for maintain & troubleshooting map managed errors, Enhancement capabilities of replications to address complex business scenarios, SLT based filtering, Deep knowledge on migration front, Replication assistance in relation to MDG Experienced in recognizing & providing solutions on Currency/Values mismatch for real time replicated data
Requirements Gathering & Analysis: Collaborate with business stakeholders and process owners to thoroughly understand their requirements, document functional specifications, and translate these into system configurations.
System Integration: Facilitate seamless integration of P2D & Data processes within the CFIN system, as well as with other enterprise systems (Local ERP, etc.), ensuring smooth data flow and automation of processes across platforms. User Support & Training: Provide ongoing functional support to end-users, addressing issues, offering solutions, and conducting training to ensure optimal utilization of the system and full understanding of P2D processes.
Testing & Validation: Participate in testing and validation activities for new configurations, enhancements, and fixes, ensuring they meet business and functional requirements. Process Optimization: Regularly monitor and evaluate P2D & Data processes to identify opportunities for automation, efficiency improvements, and best practice implementation within the CFIN system.
Documentation & Compliance: Develop and maintain comprehensive documentation for P2D system configurations, process flows, and integration points, ensuring compliance with internal standards and regulatory guidelines. Collaboration with Technical Teams: Work closely with IS architects, developers, and technical teams to ensure that functional requirements are correctly implemented and aligned with system design specifications.
Troubleshooting & Issue Resolution: Provide expert troubleshooting support for P2D -related system issues, working collaboratively with cross-functional teams to resolve any challenges promptly. Continuous Improvement: Stay informed about the latest industry trends, best practices, and system updates to continuously enhance the efficiency and effectiveness of P2D processes within CFIN.
Project and New Demand Management: Take ownership of configuring new demands or changes in system functionality, ensuring proper alignment with system design documentation and business requirements. Data Management: Oversee the management and accuracy of all data related topics within O2C/ P2P/ TRE / R2R / TAX within the system, ensuring data integrity, consistency, and compliance with business rules across processes.
Qualifications for the role
Education: Bachelor's or master's degree in computer science, Finance, Information Systems, Business Administration, or a related field. Relevant certifications in CO - SAP ECC, SAP S/4HANA, SAP CFIN, or IT architecture.
Proven experience (5+ years) in FICO processes, with a strong background in system configuration and implementation within SAP or similar ERP environments. Experience in configuring CO processes in SAP or similar ERP systems, with a solid understanding of integration points and data flows across systems.
Familiarity with Central Finance (CFIN) and integration with other finance-related modules. Experience with requirements gathering, business analysis, and documentation of functional specifications.
High level understanding of local ERP Contribution margin Reporting, COPA characteristics Derivation, COGS split, Price Difference Split, Profit Center, Cost Center, WBS Element, Order master Data mapping
Strong analytical skills and attention to detail. Excellent communication skills, with the ability to collaborate effectively with cross-functional teams, stakeholders, and technical teams.
A strong focus on continuous improvement and automation, with a passion for driving innovation within enterprise systems.
Experience in managing relationships with external vendors and third-party service providers to ensure the delivery of high-quality solutions. Ability to adapt to a fast-paced, dynamic work environment and manage multiple priorities effectively.
More about us
ABB Finance is a trusted partner to the business and a world-class team who delivers forward-looking insights that drive sustainable long-term results and operates with the highest standards.
We value people from different backgrounds. Apply today for your next career step within ABB and visit www.abb.com to learn about the impact of our solutions across the globe. #MyABBStory
It has come to our attention that the name of ABB is being used for asking candidates to make payments for job
opportunities (interviews, offers). Please be advised that ABB makes no such requests. All our open positions are made
available on our career portal for all fitting the criteria to apply.
ABB does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection to recruitment with ABB, even if is claimed that the money is refundable. ABB is not liable for such transactions.
For current open positions you can visit our career website https://global.abb/group/en/careers and apply. Please refer to detailed recruitment fraud caution notice using the link https://global.abb/group/en/careers/how-to-apply/fraud-warning","Error resolution, AIF monitoring, Currency Values mismatch, Process Optimization, Troubleshooting, Continuous Improvement, Documentation compliance, SLT based filtering, Testing and validation, CO processes, PC G L CC Map, P2D Data processes, Mdg, System integration, Sap Ecc, Data Management, Data Replication, Requirements Gathering, User Support"
Data Platform Architect,Gramener,Fresher,,"Chennai, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore/Chennai
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career path, steady growth prospects with great scope to innovate. Our goal is to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
You will engage in a diverse range of impactful customer technical Data platform projects, Including the development.
Lead strategic initiatives encompassing the end-to-end design, build, and deployment of industry-leading.
Collaborate with platform engineering teams to effectively implement Data Brick services within our infrastructure.
Leverage your hands-on experience with Data Bricks Unity Catalog to implement robust data governance and lineage capabilities.
Advocate for and implement CI/CD practices to streamline the deployment of Data bricks solutions.
Contribute to developing a data mesh architecture, promoting decentralized data ownership and accessibility across the organization.
Understand and articulate the analytics capabilities of the Data platform, enabling teams to derive actionable insights from their data.
Skills And Qualifications
Cloud and Architectures:
Azure Architecture and Platform: Expertise in Azure Data Lake, AI/ML model hosting, Key Vault, Event Hub, Logic Apps, and other Azure cloud services.
Databricks Development: Strong integration with Azure, workflow orchestration, and governance.
Data Engineering & Architecture: Hands-on experience with scalable ETL/ELT pipelines, Delta Lake, and enterprise data management.
Coding And Implementation
Software Engineering: Modular design, CI/CD, version control, and best coding and design practices.
Python and PySpark: Experience in building reusable packages and components for both technical and business users, including data validation, lineage modules, and accelerators for data processing.
Process And Compliance
Enterprise Process Understanding: Experience with large-scale workflows, structured environments, and compliance frameworks.
Governance and Security: Knowledge of data lineage, GxP, HIPAA, GDP compliance, and regulatory requirements.
Pharma, MedTech, Life Sciences (Good to Have): Understanding industry-specific data, regulatory constraints, and security considerations.
About Us
We help consult and deliver solutions to organizations where data acts as the core of decision making. We undertake strategic data consulting for organizations in laying out the roadmap for data-driven decision making. It helps organizations to convert data into a strategic differentiator. Through a host of our product and Solutions, and Service Offerings, we analyze and visualize large amounts of data.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Logic Apps, Key Vault, Databricks Development, Event Hub, CI CD, AI ML model hosting, enterprise data management, GDP compliance, Azure Architecture, ETL ELT pipelines, Delta Lake, Data Validation, enterprise process understanding, Security, Governance, Data Lineage, Version Control, Pyspark, Software Engineering, Azure Data Lake, Python"
Cloud Data & AI Architect,myCloudDoor,3-5 Years,,"Kolkata, India",Login to check your skill match score,"Description
Do you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!
Who we are
myCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.
Tasks
The Selected Person Will Do The Following Tasks:
Definition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...
Maintenance of data solutions, failure analysis and solution proposal.
Communication with customers: proposal solutions, technical trainings...
The profile
We are looking for a person who fit the following requirements:
+3 years of experience years of experience in a similar role.
Experience in Azure projects
Real experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...
Real Experience in AI
Experience in presales and proposals
What we offer you
Career Path
Remote working
Training: Internal and technical certifications
Think you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.
>
Do you have experience as Data and AI Architect Are you looking for your next professional challenge Would you like to participate in innovative projects At myCloudDoor are looking for you!
Who we are
myCloudDoor is a 100% Cloud company that, since our founding in 2011 in the United States, we have expanded to 12 countries, creating an environment where innovation and professional growth are the norm. With more than 200 technology experts, we offer unique opportunities to develop advanced skills and lead the global digital transformation. Our business areas - DISCOVER, CYBERSEC, TRANSFORM, OPTIMIZE, INNOVATE and EMPOWER - not only reflect our mission, but also represent pathways for personal and professional development. Take advantage of this opportunity to work on international projects in an environment that fosters excellence and continuous improvement.
Tasks
The Selected Person Will Do The Following Tasks:
Definition and development of data and AI solutions: defining the right architecture for the customer's needs, implementation of services, managing access control and governance policies, security control...
Maintenance of data solutions, failure analysis and solution proposal.
Communication with customers: proposal solutions, technical trainings...
The profile
We are looking for a person who fit the following requirements:
+3 years of experience years of experience in a similar role.
Experience in Azure projects
Real experience in data architecture: migrations, design and implementations of Data Lakes and Data Warehouse...
Real Experience in AI
Experience in presales and proposals
What we offer you
Career Path
Remote working
Training: Internal and technical certifications
Think you can fit in it Do you want know more details Do not hesitate to apply for the offer! We are waiting for you.","Presales, Ai, Data Lakes, Azure, Data Architecture, Data Warehouse"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Chennai, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune
Experience: 7-16 Years
Work Mode: Hybrid
Mandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)
Job Description
We are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.
Key Responsibilities
Design, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.
Collaborate with data analysts, data architects, and business stakeholders to align data models with business needs.
Leverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.
Manage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.
Contribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.
Apply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.
Stay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.
Develop and maintain data models using data modeling tools such as ER/Studio and Hackolade.
Drive the adoption of best practices and standards for data modeling within the organization.
Skills And Qualifications
Minimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.
Expertise in Azure and Databricks for building data solutions.
Proficiency in ER/Studio, Hackolade, and other data modeling tools.
Strong understanding of data modeling principles and techniques (e.g., ERD, UML).
Experience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Solid understanding of data warehousing, ETL processes, and data integration.
Familiarity with big data technologies such as Hadoop and Spark is an advantage.
Industry Knowledge: A background in supply chain is preferred but not mandatory.
Excellent analytical and problem-solving skills.
Strong communication skills, with the ability to interact with both technical and non-technical stakeholders.
Ability to work well in a collaborative, fast-paced environment.
Education
B.Tech in any branch or specialization
Skills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Hyderabad, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune
Experience: 7-16 Years
Work Mode: Hybrid
Mandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)
Job Description
We are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.
Key Responsibilities
Design, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.
Collaborate with data analysts, data architects, and business stakeholders to align data models with business needs.
Leverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.
Manage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.
Contribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.
Apply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.
Stay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.
Develop and maintain data models using data modeling tools such as ER/Studio and Hackolade.
Drive the adoption of best practices and standards for data modeling within the organization.
Skills And Qualifications
Minimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.
Expertise in Azure and Databricks for building data solutions.
Proficiency in ER/Studio, Hackolade, and other data modeling tools.
Strong understanding of data modeling principles and techniques (e.g., ERD, UML).
Experience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Solid understanding of data warehousing, ETL processes, and data integration.
Familiarity with big data technologies such as Hadoop and Spark is an advantage.
Industry Knowledge: A background in supply chain is preferred but not mandatory.
Excellent analytical and problem-solving skills.
Strong communication skills, with the ability to interact with both technical and non-technical stakeholders.
Ability to work well in a collaborative, fast-paced environment.
Education
B.Tech in any branch or specialization
Skills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
Senior Data Modeller/Data Modelling(Architect),VidPro Consultancy Services,7-16 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"Location: Hyderabad / Bangalore / Pune
Experience: 7-16 Years
Work Mode: Hybrid
Mandatory Skills: ERstudio, Data Warehouse, Data Modelling, Databricks, ETL, PostgreSQL, MySQL, Oracle, NoSQL, Hadoop, Spark, Dimensional Modelling ,OLAP, OLTP, Erwin, Data Architect and Supplychain (preferred)
Job Description
We are looking for a talented and experienced Senior Data Modeler to join our growing team. As a Senior Data Modeler, you will be responsible for designing, implementing, and maintaining data models to enhance data quality, performance, and scalability. You will collaborate with cross-functional teams including data analysts, architects, and business stakeholders to ensure that the data models align with business requirements and drive efficient data management.
Key Responsibilities
Design, implement, and maintain data models that support business requirements, ensuring high data quality, performance, and scalability.
Collaborate with data analysts, data architects, and business stakeholders to align data models with business needs.
Leverage expertise in Azure, Databricks, and data warehousing to create and manage data solutions.
Manage and optimize relational and NoSQL databases such as Teradata, SQL Server, Oracle, MySQL, MongoDB, and Cassandra.
Contribute to and enhance the ETL processes and data integration pipelines to ensure smooth data flows.
Apply data modeling principles and techniques, such as ERD and UML, to design and implement effective data models.
Stay up-to-date with industry trends and emerging technologies, such as big data technologies like Hadoop and Spark.
Develop and maintain data models using data modeling tools such as ER/Studio and Hackolade.
Drive the adoption of best practices and standards for data modeling within the organization.
Skills And Qualifications
Minimum of 6+ years of experience in data modeling, with a proven track record of implementing scalable and efficient data models.
Expertise in Azure and Databricks for building data solutions.
Proficiency in ER/Studio, Hackolade, and other data modeling tools.
Strong understanding of data modeling principles and techniques (e.g., ERD, UML).
Experience with relational databases (e.g., Teradata, SQL Server, Oracle, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Solid understanding of data warehousing, ETL processes, and data integration.
Familiarity with big data technologies such as Hadoop and Spark is an advantage.
Industry Knowledge: A background in supply chain is preferred but not mandatory.
Excellent analytical and problem-solving skills.
Strong communication skills, with the ability to interact with both technical and non-technical stakeholders.
Ability to work well in a collaborative, fast-paced environment.
Education
B.Tech in any branch or specialization
Skills: databricks,azure,data models,datafactory,spark,dimensional modelling,models,modeling,supplychain,oltp,er studio,databases,mysql,data architect,postgresql,olap,aws,python,erstudio,data modelling,data,pyspark,hadoop,nosql,data modeling,skills,oracle,erwin,online transaction processing (oltp),etl,sql,data warehouse","ERstudio, Dimensional Modelling, Erwin Data Architect, Data Modelling, Hadoop, PostgreSQL, OLAP, Data Warehouse, Nosql, MySQL, Spark, Databricks, Oracle, Oltp, Etl"
"Data Analytics Architect (Looker), Contract",66degrees,5-7 Years,,India,Login to check your skill match score,"Overview of 66degrees
66degrees is a leading Google Cloud Premier Partner. We believe that engineering takes heart. Focusing exclusively on Google Cloud, we help our clients achieve the most innovative and disruptive transformations in their industries.
66degrees is seeking a senior contractor- Data Analytics Architect (Looker) to engage on a 8 weeks of remote assignment for approximately 40 hours per week with potential to extend. Interested candidates should have the following required skills in Golang.
Please note: ***Candidates must be available to join by first week of May, 2025***
Responsibilities
Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
Work with Clients to enable them on the Looker Platform, teaching them how to construct an analytics ecosystem in Looker from the ground up.
Advise clients on how to develop their analytics centers of excellence, defining and designing processes to promote a scalable, governed analytics ecosystem.
Utilize Looker to design and develop interactive and visually appealing dashboards and reports for end-users.
Write clean, efficient, and scalable code (LookML, Python as applicable)
Conduct performance tuning and optimization of data analytics solutions to ensure efficient processing and query performance.
Stay up to date with the latest trends and best practices in cloud data analytics, big data technologies, and data visualization tools.
Collaborate with other teams to ensure seamless integration of data analytics solutions with existing systems and processes.
Provide technical guidance and mentorship to junior team members, sharing knowledge and promoting best practices.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven track record as a Data Analytics Architect or a similar role, with a minimum of 5 years experience in data analytics and visualization.
Excellent comprehension of Looker and its implementation, with additional analytics platform experience a major plus - especially MicroStrategy.
Experience with Google Cloud and its data services is preferred, however experience with other major cloud platforms (AWS, Azure) and their data services will be considered.
Strong proficiency with SQL required.
Demonstrated experience working with clients in various industry verticals, understanding their unique data challenges and opportunities.
Excellent programming skills in Python and experience working with python-enabled capabilities in analytics platforms.
Solid understanding of data modeling, ETL processes, and data integration techniques. Experience working with dbt or dataform is a plus.
Strong problem-solving skills and the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.
Google Cloud certifications and any analytics platform certifications are a plus.
A desire to stay ahead of the curve and continuously learn new technologies and techniques.
66degrees is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, religion, sex, gender, gender identity, national origin, age, weight, height, marital status, sexual orientation, veteran status, disability status or other legally protected class.","dataform, dbt, ETL processes, Microstrategy, Looker, LookML, Golang, Data Modeling, Google Cloud, Sql, Python"
Data & AI Architect,Cognologix Technologies,12-14 Years,,"Pune, India",Login to check your skill match score,"You Will Work On
We are looking for self driven Data Professional to be a key member of our Data Practice and help our customers to solve their critical data challenges. Expected to play a lead role in delivering data solutions, engineering assets and processes to support the modernization, transformation requirements. The ideal candidate is passionate about defining & delivering cutting edge solutions to support the changing business needs and technology landscape.
What You Will Do
Lead analysis, architecture, design and development of medium scale data and analytics solutions
Design & deliver architectural solutions for key customer projects across the Data & AI practice within the organization
Apply enterprise vision to all portfolio projects, understanding and communicating different architectural strategies that are consistent with business strategies
Contribute in Technical, Architectural presentation & discussions with existing, new customers through presales to delivery phases
Participate in strengthening engineering practices and developing the future state data architecture frameworks, patterns, standards, guidelines, and principles.
Research and maintain knowledge in emerging technologies and solutions to solve business problems
What You Bring
Bachelors or equivalent degree in Engineering, Computer Science or related technical field
12+ years of relevant experience in Data & Analytics technology stack
5+ years knowledge of Big Data technologies & Cloud Tech stack, AWS or GCP preferred
Applied Experience and Expertise in Data Architecture aspects ( Data Management, Data Governance, Data Models Valult, Dimensional etc)
Experience in delivering end-to-end data platform solutions, from initiation phase to delivery, including supervision of developers/engineers.
Demonstrate the ability to create high quality technical artifacts such as Technical Architecture document, detailed design documents etc
Awareness of diverse tech stack in Data space and latest industry trends
Excellent Analytical and Problem Solving Skills, High attention to details
High impact communication skills, Effective interpersonal and collaboration skills
Experience in Agile Methodologies and DevOps aspects
Must be willing to both architect solutions & get deep into the weeds of delivering solutions
Advantage Cognologix
A higher degree of autonomy, startup culture & small teams.
Opportunities to become an expert in emerging technologies.
Remote working options for the right maturity level.
Competitive salary & family benefits.
Performance-based career advancement.
About Cognologix
Cognologix helps companies disrupt by reimagining their business models and innovate like a Startup. We are at the forefront of digital disruption and take a business-first approach to help meet our client's strategic goals.
We are a Data focused organization helping our clients to deliver their next generation of products in the most efficient, modern and cloud-native way.
Minimum Experience
12
Top Skill
Data Architecture
Submit Your Application
You have successfully applied
You have errors in applying
Apply With Resume *
First Name*
Middle Name
Last Name*
Email*
Mobile
Phone
Social Network and Web Links
Provide us with links to see some of your work (Git/ Dribble/ Behance/ Pinterest/ Blog/ Medium)
Employer
Education","data models, Data Management, Big Data Technologies, Data Architecture, Data Governance, Devops, Agile Methodologies"
Python Data Engineer Architect,Gramener,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Work Location: Hyderabad/Bangalore
What Gramener offers you
Gramener will offer you an inviting workplace, talented colleagues from diverse backgrounds, career paths, and steady growth prospects with great scope to innovate. We aim to create an ecosystem of easily configurable data applications focused on storytelling for public and private use.
Roles and Responsibilities
10+yrs Designing and building data models: Creating the design for a database to meet the organization's strategic data needs.
Working with stakeholders: Collaborating with technical and business stakeholders to understand needs and develop solutions.
Analyzing requirements: Performing requirement analysis and creating architectural models.
Identifying issues: Identifying operational issues and recommending strategies to resolve them.
Communicating with business users: Communicating technical solutions to business users and addressing their questions.
Validating solutions: Ensuring solutions align with corporate standards and compliance requirements.
Developing technical specifications: Creating technical design specifications for solutions and systems engineers.
Assessing impacts: Assessing the impacts of proposed solutions and the resources needed to implement them.
Data engineers use Python libraries to gather data for projects. They can use Python to interact with APIs, connect with databases, and perform web scraping.
About Us
We help consult and deliver solutions to organizations where data is at the core of decision-making. We undertake strategic data consulting for organizations to lay the roadmap for data-driven decision-making and equip them to convert data into a strategic differentiator. We analyze and visualize large amounts of data through a host of our product and service offerings.
To know more about us visit Gramener Website and Gramener Blog.
Apply for this role Apply for this Role","Databases, Python, Web Scraping, Apis"
Data Engineering Architect,Quest Global,15-25 Years,,"Hyderabad, India",Login to check your skill match score,"Considering Only Immediate Joinee & Candidates willing for Hyderabad Office based role
Strong hands-on experience with Data Engineering, AWS, Java, Kubernetes, Microservices, React. Apple Technology Stack will be a plus
Experience : 15 - 25 Years
Job Requirements
Quest Global is seeking a highly skilled, hands-on Solution Architect to drive innovation and deliver impactful, scalable solutions tailored to our customers needs. In this pivotal role, you will develop proof of concepts, design and lead solution implementations that enhance our offerings. After successful POC completion, you will mentor and guide engineers in developing a production-ready solution.
Key Responsibilities:
Overall responsibility to build architectures (new or rearchitect existing prods)
Build POCs to justify new Enterprise architectures (Cloud) including but not limited to AI/ Gen AI/ Data pipe enhancements, performance improvement, adoption of new technologies
Collaborate with client architects, Prod Managers and Engineering leaders to align with the product vision and help build corresponding response from Quest Global along with the VBU and project teams
Hands on Technical
Skill Level Expectation from Candidate
Data Engineering (Big Data, Spark, ETL pipelines, and Airflow)
Have built solutions and gone live with that
Must have built high scale, high performance solutions which are live in production
Have built solutions and gone live with that
Cloud - AWS (No Azure), Hybrid solutions (On Prem/ Cloud combos)
Have built solutions and gone live with that
Java + Microservices
Can work independently
Rest APIs
Can work independently
AI/ Machine Learning (On Premise/ Cloud)
Can work independently
Dockers/ Kubernates (Only consumer and NOT the CI/CD set up view)
Can work independently
Python Programming (in context of AI/ML)
Can work independently
Building / fintuning of LLM Models
Tech know how with limited hands on (done POCs)
Usage of LLM models
Can work independently","Airflow, Ai, ETL pipelines, data engineering, Java, Machine Learning, Dockers, Rest Apis, Big Data, Microservices, React, Cloud, Spark, Kubernetes, Python, AWS"
"Assistant Vice President, Lead Solutions Architect- Data Engineering",Genpact,Fresher,,"Gurugram, Gurugram",IT/Computers - Hardware & Networking,"Ready to build the future with AI
At Genpact, we don&rsquot just keep up with technology&mdashwe set the pace. AI and digital innovation are redefining industries, and we&rsquore leading the charge. Genpact&rsquos , our industry-first accelerator, is an example of how we&rsquore scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to , our breakthrough solutions tackle companies most complex challenges.
If you thrive in a fast-moving, innovation-driven environment, love building and deploying cutting-edge AI solutions, and want to push the boundaries of what&rsquos possible, this is your moment.
Genpact (NYSE: G) is anadvanced technology services and solutions company that deliverslastingvalue for leading enterprisesglobally.Through ourdeep business knowledge, operational excellence, and cutting-edge solutions - we help companies across industries get ahead and stay ahead.Powered by curiosity, courage, and innovation,our teamsimplementdata, technology, and AItocreate tomorrow, today.Get to know us atand on,,, and.
Inviting applications for the role of Assistant Vice President, Lead Solutions Architect- Data Engineering!
The Head of Solutioning will lead the solutioning for Genpact, working on scalable solutions for business opportunities. This leadership role requires a blend of technical expertise, business acumen, and creative problem-solving skills to conceptualize and deliver cutting-edge solutions aligned with market trends and organizational goals.
Responsibilities
Support the sales team by providing subject matter expertise and solutioning inputs for RFPs, proposals, and client presentations.
Actively participate in the sales cycle, helping to close deals by demonstrating the value of the proposed solutions.
Work closely with internal and external stakeholders to understand their needs and translate them into actionable solution strategies.
Foster a culture of continuous innovation and experimentation to drive differentiation in the market.
Define governance frameworks and risk management strategies for new solutions, ensuring that they meet quality, security, and compliance standards.
Lead and mentor a high-performance team of solution architects, engineers, and business strategists.
Qualifications we seek in you!
Minimum Qualifications / Skills
Should have experience in Multimillion complex data engineering and needs right from the get to go.
Bachelor&rsquos degree in Business Administration, Engineering, Computer Science, or a related field (Master&rsquos or MBA preferred).
Relevant years in IT services with strong background in solutioning leadership role.
Preferred Qualifications/ Skills
Proven track record of creating and delivering new business concepts, products, or solutions that have contributed to revenue growth.
Strong understanding of business strategy, P&L management, and market dynamics.
Ability to translate business goals into actionable, scalable solutions.
Exceptional leadership skills with the ability to drive change, inspire teams, and influence stakeholders.
Strong problem-solving skills and the ability to manage ambiguity.
Preferred Certifications:
. PMP or similar project management certification.
. TOGAF or enterprise architecture certification.
. Agile or SAFe certifications (e.g., Certified Scrum Master, SAFe Program Consultant).
Why join Genpact
Lead AI-first transformation - Build and scale AI solutions that redefine industries
Make an impact - Drive change for global enterprises and solve business challenges that matter
Accelerate your career&mdashGain hands-on experience, world-class training, mentorship, and AI certifications to advance your skills
Grow with the best - Learn from top engineers, data scientists, and AI experts in a dynamic, fast-moving workplace
Committed to ethical AI - Work in an environment where governance, transparency, and security are at the core of everything we build
Thrive in a values-driven culture - Our courage, curiosity, and incisiveness - built on a foundation of integrity and inclusion - allow your ideas to fuel progress
Come join the 140,000+ coders, tech shapers, and growth makers at Genpact and take your career in the only direction that matters: Up.
Let&rsquos build tomorrow together.
Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation.
Furthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Solution Architect - Data,PreludeSys,7-9 Years,,"Chennai, India",Login to check your skill match score,"As a Solution Architectyou will:
Define and evolve the company's AI vision and execution plan.
Ensure alignment of AI initiatives with business goals and measurable outcomes.
Lead the design, development, and deployment of AI models.
Work across the full AI lifecyclefrom problem scoping and data exploration to model development and production integration.
Stay actively engaged with machine learning, deep learning, NLP, and generative AI.
Experiment with and implement cutting-edge models like LLMs (e.g., GPT, BERT).
Partner with internal teams and external clients to gather requirements, provide architectural guidance, and ensure seamless integration of AI solutions into existing systems.
Mentor junior team members and promote a culture of continuous learning and excellence.
Advocate for responsible AI practices, ensuring models are ethical, explainable, and scalable.
Skills Required:
7+ years in AI/ML solution architecture, data science, or enterprise AI product development.
Proficiency in Python and hands-on experience with ML frameworks like TensorFlow, PyTorch, Scikit-learn, and Hugging Face.
Solid understanding of algorithms, model design, evaluation, and deployment workflows.
Experience working with transformer-based models and NLP pipelines using tools like GPT, BERT, or similar.
Knowledge of model lifecycle tools (e.g., MLflow, Kubeflow), containerization (Docker), and API frameworks (FastAPI, Flask).
Proven experience deploying AI solutions on Azure, AWS, or GCP, utilizing native AI/ML services.
Master's degree in Data Science, AI, Machine Learning, or a related technical field (PhD preferred).
Azure AI Engineer Associate or equivalent certifications preferred.
Research publications, patents, or contributions to open-source AI/ML projects will be an added advantage.","Hugging Face, Scikit-learn, MLflow, GPT, Kubeflow, BERT, Tensorflow, Gcp, Pytorch, Docker, Flask, FastAPI, Azure, Python, AWS"
Senior Enterprise Architect - Data,Bread Financial,10-12 Years,,India,Login to check your skill match score,"Every career journey is personal. That's why we empower you with the tools and support to create your own success story.
Be challenged. Be heard. Be valued. Be you ... be here.
Job Summary
Sr Enterprise Architect designs and implements enterprise-wide IT solutions to align technology initiatives with business goals. Serve as a strategic advisor to leadership, ensuring architecture principles are followed.
Essential Job Functions
Develop and maintain enterprise architecture frameworks and standards. -
Oversee enterprise data strategies, designing platforms for advanced analytics, AI/ML, and regulatory adherence
Evaluate new technologies and identify opportunities for system enhancements.
Collaborate with stakeholders to analyze business requirements and translate them into technical solutions.
Ensure technology initiatives align with the organization's overall architectural vision.
Conduct system audits and maintain documentation for architecture solutions.
Minimum Qualifications
Bachelor's Degree or equivalent education and / or experience in computer Science, Engineering, Mathematics or related field.
10+ years in Information Technology
Preferred Qualifications
Relevant certifications, such as TOGAF or AWS Solutions Architect
Skills
Application Development
Business Alignment
Business Process Modeling
Business Case Development
Code Inspection
Cloud Architectures
Enterprise Architecture Framework
IT Architecture
IT Roadmap
Solution Architecture
Reports To: Manager and above
Direct Reports: 0
Work Environment
Normal office environment, hybrid.
Other Duties
This job description is illustrative of the types of duties typically performed by this job. It is not intended to be an exhaustive listing of each and every essential function of the job. Because job content may change from time to time, the Company reserves the right to add and/or delete essential functions from this job at any time.
About Bread Financial
At Bread Financial, you'll have the opportunity to grow your career, give back to your community, and be part of our award-winning culture. We've been consistently recognized as a best place to work nationally and in many markets and we're proud to promote an environment where you feel appreciated, accepted, valued, and fulfilledboth personally and professionally. Bread Financial supports the overall wellness of our associates with a diverse suite of benefits and offers boundless opportunities for career development and non-traditional career progression.
Bread Financial (NYSE: BFH) is a tech-forward financial services company that provides simple, personalized payment, lending, and saving solutions to millions of U.S consumers. Our payment solutions, including Bread Financial general purpose credit cards and savings products, empower our customers and their passions for a better life. Additionally, we deliver growth for some of the most recognized brands in travel & entertainment, health & beauty, jewelry and specialty apparel through our private label and co-brand credit cards and pay-over-time products providing choice and value to our shared customers.
To learn more about Bread Financial, our global associates and our sustainability commitments, visit breadfinancial.com or follow us on Instagram and LinkedIn.
All job offers are contingent upon successful completion of credit and background checks.
Bread Financial is an Equal Opportunity Employer.
Job Family
Information Technology
Job Type
Regular","IT Roadmap, business case development, Cloud Architectures, Code Inspection, Business Process Modeling, Enterprise Architecture Framework, business alignment, Application Development, IT Architecture, Solution Architecture"
Architect (Data Engineering),Amgen Inc,10-15 Years,,Hyderabad,Biotechnology,"Roles & Responsibilities:
Design and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.
Develop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.
Define data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.
Lead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.
Optimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.
Establish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.
Drive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.
Implement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.
Lead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.
Collaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.
Act as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.
Must-Have Skills:
Experience in data architecture, enterprise data management, and cloud-based analytics solutions.
Well versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.
Expertise in Databricks, cloud-native data platforms, and distributed computing frameworks.
Strong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.
Experience designing high-performance ETL/ELT pipelines and real-time data processing solutions.
Deep understanding of data governance, security, metadata management, and access control frameworks.
Hands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).
Proven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.
Strong problem-solving, strategic thinking, and technical leadership skills.
Experienced with SQL/NOSQL database, vector database for large language models
Experienced with data modeling and performance tuning for both OLAP and OLTP databases
Experienced with Apache Spark, Apache Airflow
Experienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps
Good-to-Have Skills:
Experience with Data Mesh architectures and federated data governance models.
Certification in cloud data platforms or enterprise architecture frameworks.
Knowledge of AI/ML pipeline integration within enterprise data architectures.
Familiarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.
Education and Professional Certifications
9 to 12 years of experience in Computer Science, IT or related field
AWS Certified Data Engineer preferred
Databricks Certificate preferred","Apache Airflow, Apache Spark, Cloud Devops, Sql, Aws"
Azure Data Lead or Architect,Coforge,8-12 Years,,Noida,Information Technology,"Experience Required:- 8 to 12 Years
Are you a seasonedAzure Data Architect or Leadwith 8+ years of experience, passionate about building robust and scalable data solutions on Azure We're looking for a talented individual to join our team in Greater Noida and drive our data strategy forward!
About the Role:
As a Senior Data Architect, you'll be instrumental in designing and implementing cutting-edge data architecture frameworks on Azure, ensuring our data infrastructure aligns with business objectives and technical requirements. You'll play a key role in defining reference architectures, optimizing data flow, and improving cost efficiency.
Job Description
Responsibilities:
* Design and build data architecture frameworks leveraging Azure services (Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure Data Lake Storage, Azure SQL Database, etc.).
* Define and implement reference architectures and data flow for future infrastructure development.
* Conduct cost analysis and optimize existing data infrastructure for efficient data handling.
* Develop and implement an organization-wide data strategy aligned with business processes.
* Hands-on development with SQL, ETL processes, Python/PySpark.
* Collaborate with network and infrastructure teams to ensure seamless integration.
* Implement data modeling best practices (dimensional modeling, data vault).
* Ensure data security and compliance using Azure security tools (Azure Active Directory, Azure Key Vault).
* Work with BI tools (specify tools, example Power BI) to deliver data insights.
* Implement data governance and data quality processes.
* Utilize version control tools (specify tools, example Git).
* Work with Infrastructure as Code (specify tools, example Terraform, ARM templates).
* Work within an Agile environment (specify agile method, example Scrum).
* Effectively communicate with stakeholders at all levels.
Requirements:-
* 8+ years of experience in Data Warehousing and Azure Cloud technologies.
* Strong hands-on experience with SQL, ETL, Python/PySpark.
* Proven expertise in designing and implementing data architectures on Azure.
* Exposure to Azure DevOps and Business Intelligence.
* Solid understanding of data governance, data security, and compliance.
* Excellent communication and collaboration skills.
* Ability to work effectively in a UK shift (1 PM IST to 9:30 PM IST).
* Ability to work in a hybrid environment, with 3 days/week in office.
* Location: Greater Noida.
Key Improvements:
* Specific Azure Services: Added examples of relevant Azure services.
* Data Modeling: Explicitly mentioned data modeling.
* Cloud Security: Added examples of Azure security tools.
* BI Tools: added a placeholder to add specific tools.
* Version control and IaC: added placeholders to specify the used tools.
* Agile: added a placeholder to specify the agile methodology.
* Clear Call to Action: Included a To Apply section.
* Relevant Hashtags: Added relevant hashtags for increased visibility.
* Data Quality: Added Data quality to the responsibilities.
* Clear location and work schedule","Azure Data, Pyspark, Sql, Python, Etl, Azure Devops, Data Architecture"
Data and Analytics Architect,Wipro Limited,4-8 Years,,Chennai,Software,"The purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.
Do
Define and Develop Data Architecture that aids organization and clients in new/ existing deals
Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation
Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy
Create data strategy and road maps for the Reference Data Architecture as required by the clients
Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request
Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise
Develop, communicate, support and monitor compliance with Data Modelling standards
Oversee and monitor all frameworks to manage data across organization
Provide insights for database storage and platform for ease of use and least manual work
Collaborate with vendors to ensure integrity, objectives and system configuration
Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization
Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage
Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes
Knowledge of all the Data service provider platforms and ensure end to end view.
Oversight all the data standards/ reference/ papers for proper governance
Promote, guard and guide the organization towards common semantics and the proper use of metadata
Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control
Provide solution of RFPs received from clients and ensure overall implementation assurance
Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives
Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data
Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology
Define and understand current issues and problems and identify improvements
Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout
Understand the root cause problem in integrating business and product units
Validate the solution/ prototype from technology, cost structure and customer differentiation point of view
Collaborating with sales and delivery leadership teams to identify future needs and requirements
Tracks industry and application trends and relates these to planning current and future IT needs
Building enterprise technology environment for data architecture management
Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes
Evaluate all the implemented systems to determine their viability in terms of cost effectiveness
Collect all the structural and non-structural data from different places integrate all the data in one database form
Work through every stage of data processing: analysing, creating, physical data model designs, solutions and reports
Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices
Implement the best security practices across all the data bases based on the accessibility and technology
Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)
Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration
Enable Delivery Teams by providing optimal delivery solutions/ frameworks
Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor
Define database physical structure, functional capabilities, security, back-up and recovery specifications
Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results
Monitor system capabilities and performance by performing tests and configurations
Integrate new solutions and troubleshoot previously occurred errors
Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards
Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects
Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams
Recommend tools for reuse, automation for improved productivity and reduced cycle times
Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.
Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams
Ensures architecture principles and standards are consistently applied to all the projects
Ensure optimal Client Engagement
Support pre-sales team while presenting the entire solution design and its principles to the client
Negotiate, manage and coordinate with the client teams to ensure all requirements are met
Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor.","Data and Analytics Architect, Data Modelling, Frameworks"
Systems and Product Architect (Data Cloud),Creditsafe Technology,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"ABOUT THE TEAM
Join our dynamic team of expert engineers at Creditsafe, where we are revolutionizing the data ecosystem through strategic innovation and cutting-edge architectural modernization. As a System & Product Architect, you will lead the transformation of our systems and product architecture on AWS, managing billions of data objects with daily increments exceeding 25 million. Your expertise will be pivotal in ensuring high availability, data integrity, and outstanding performance, powering our APIs and file delivery systems to deliver seamless data experiences to our global clients. Be at the forefront of data innovation and make an impact on a global scale.
ABOUT THE ROLE
This role places you at the center of Creditsafe's transformation journey. You will define architectural standards, design patterns, and technical roadmaps that guide our shift to a modern cloud infrastructure. Collaborating with technologies such as Python, Linux, Airflow, AWS DynamoDB, S3, Glue, Athena, Redshift, Lambda, API Gateway, Terraform, and CI/CD pipelines, you will ensure our platform is scalable, resilient, and ready for the future.
KEY DUTIES AND RESPONSIBILITIES
Drive the technical vision, architecture, and design principles for system replatforming and migration.
Design scalable, distributed architecture patterns that optimize for throughput, resilience, and maintainability.
Create and maintain system architecture documentation, including diagrams, data flows, and design decisions.
Establish governance frameworks for technical debt management and architectural compliance.
Design event-driven architectures for distributed data processing using AWS technologies.
Work with team to support & build APIs capable of supporting 1000+ transactions per second.
Mentor engineers on architectural best practices and system design principles.
Partner with security teams to ensure architectures meet compliance requirements.
Contribute to technical roadmap aligned with company's vision & Product roadmap.
SKILLS AND QUALIFICATIONS
8+ years of software engineering experience, with at least 4 years in system architecture.
Proven track record in large-scale replatforming and system modernization initiatives.
Cloud-native architecture expertise, particularly with AWS services (Redshift, S3, DynamoDB, Lambda, API Gateway).
Solid understanding of data platforms, ETL/ELT pipelines, and data warehousing.
Experience with serverless architectures, microservices, and event-driven design patterns.
Strong technical skills with Python, Terraform and modern DevOps practices.
Experience designing high-throughput, low-latency API solutions.
Demonstrated technical leadership and mentoring abilities.
Clear communication skills, with the ability to translate complex technical concepts.
Strategic thinker, love white-boarding, and keen on mentoring engineers.
Desirable:
Experience with AI and machine learning architecture patterns.
AWS Solution Architect Pro certification.","Airflow, Glue, Athena, S3, Dynamodb, Redshift, Lambda, Terraform, Linux, Python, AWS, Api Gateway"
Chief Architect - Data & AI,Orion Innovation,Fresher,,India,Login to check your skill match score,"Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.
We are seeking a dynamic and experienced leader for our Data Architecture and Data Science Practice. This role will be instrumental in shaping our organization's data strategy, driving innovation through advanced analytics, and ensuring robust data architecture to support our business objectives.
Responsibilities
Strategic Leadership: Develop and implement data strategies that align with organizational goals and objectives. Drive innovation and efficiency through the effective use of data.
Team Management: Lead and mentor a team of data architects, data engineers, and data scientists. Provide guidance and support to foster professional growth and collaboration within the team.
Data Architecture: Designing and maintaining scalable and efficient solutions to ensure data integrity, availability, and security across an organization's infrastructure. This includes translating business requirements into logical and physical data models, ensuring data is transformed correctly from source to target systems through detailed mapping, and converting business models into a comprehensive data platform. Data integration combines data from various sources into a unified view using ETL/ELT processes, data pipelines, and APIs, centralizing storage in data lakes and warehouses. An audit framework tracks and monitors data activities to ensure compliance and transparency, while scheduling and monitoring tools ensure data processes run smoothly and on time. Adhering to Service Level Agreements (SLAs) involves defining SLAs, tracking key metrics, managing incidents efficiently, and providing regular reports to stakeholders. This holistic approach supports business needs, ensures data quality, and maintains operational efficiency.
Advanced Analytics: Oversee the development and implementation of advanced analytics techniques, including machine learning, predictive modeling, and optimization algorithms. Drive the adoption of best practices and methodologies in data science.
Stakeholder Collaboration: Collaborate with stakeholders across the organization to understand business requirements and priorities. Translate business needs into data initiatives and deliver actionable insights to drive decision-making.
Technology Evaluation: Stay updated on emerging technologies and trends in data management and analytics. Evaluate new tools, platforms, and methodologies to enhance the organization's data capabilities.
Governance and Compliance: Establish and enforce data governance policies and procedures to ensure regulatory compliance, data privacy, and security. Implement best practices for data quality management and data lineage tracking.
Performance Monitoring: Define key performance indicators (KPIs) to measure the effectiveness of data practices. Monitor performance metrics, analyze trends, and identify areas for improvement.
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, Information Systems, or a related field.
Strong leadership experience in data architecture, data engineering, or data science.
In-depth knowledge of data architecture principles, data modeling techniques, and database technologies.
Proficiency in programming languages such as Python, R, SQL, etc.
Strong communication skills with the ability to translate technical concepts into business terms.
Experience working in a fast-paced environment and managing multiple priorities effectively
Orion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Candidate Privacy Policy
Orion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, Orion, we Or us) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (Notice) Explains
What information we collect during our application and recruitment process and why we collect it;
How we handle that information; and
How to access and update that information.
Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.","Data lakes, R, Data architecture principles, Data pipelines, Data modeling techniques, Apis, Optimization Algorithms, Sql, ELT, Database Technologies, Machine Learning, Predictive Modeling, Python, Etl"
Senior Principal Consultant-?SFDC Data Cloud Technical?Architect,Genpact,Fresher,,Noida,IT/Computers - Hardware & Networking,"Genpact (NYSE: G) is a global professional services and solutions firm delivering outcomes that shape the future. Our 125,000+ people across 30+ countries are driven by our innate curiosity, entrepreneurial agility, and desire to create lasting value for clients. Powered by our purpose - the relentless pursuit of a world that works better for people - we serve and transform leading enterprises, including the Fortune Global 500, with our deep business and industry knowledge, digital operations services, and expertise in data, technology, and AI.
Inviting applications for the role of Senior Principal Consultant- SFDC Data Cloud Technical Architect
A Salesforce Data Cloud Tech Architect is responsible for designing and implementing data solutions within the Salesforce ecosystem, leveraging Salesforce Data Cloud and related technologies. Here's an overview of the role:
Responsibilities:
. Design scalable and efficient data architectures using Salesforce Data Cloud.
. Collaborate with stakeholders to understand business requirements and translate them into technical solutions.
. Ensure data security, compliance, and governance within the Salesforce platform.
. Lead the integration of Salesforce Data Cloud with other systems and tools.
. Optimize data solutions for performance, scalability, and cost-efficiency
. Provide technical guidance and mentorship to development teams.
. Stay updated with Salesforce advancements and best practices.
Qualifications we seek in you!
Minimum qualifications:
. B.E or B.Tech or MCA
. Expertise in Salesforce Data Cloud and related tools.
. Strong knowledge of data modeling, ETL processes, and data integration.
. Proficiency in programming languages like Apex, Java, or Python
. Familiarity with Salesforce APIs, Lightning, and Visualforce.
. Excellent problem-solving and communication skills.
Preferred qualifications
. Salesforce PD II, Sales Cloud, Service Cloud and Marketing Cloud Certifications etc...
Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values respect and integrity, customer focus, and innovation. Get to know us at genpact.com and on LinkedIn, X, YouTube, and Facebook.
Furthermore, please do note that Genpact does not charge fees to process job applications and applicants are not required to pay to participate in our hiring process in any other way. Examples of such scams include purchasing a 'starter kit,' paying to apply, or purchasing equipment or training.",
Solution Architect -Data Engineering and Analytics,Eaton,10-12 Years,,"Pune, India",Login to check your skill match score,"Job Summary
As a Solutions Architect, the candidate will be responsible for understanding requirements and building solution architectures for the Data Engineering and Advanced Analytics Capability. The role will require a mix of technical knowledge and finance domain functional knowledge while the functional knowledge is not necessarily a must have.The candidate will apply best practices to create data architectures that are secure, scalable, cost-effective, efficient, reusable, and resilient. The candidate will participate in technical discussions, present their architectures to stakeholders for feedback, and incorporate their input. The candidate will evaluate, recommend, and integrate SaaS applications to meet business needs, and provide architectures for integrating existing Eaton applications or developing new ones with a cloud-first mindset. The candidate will offer design oversight and guidance during project execution, ensuring solutions align with strategic business and IT goals.
As a hands-on technical leader, the candidate will also drive Snowflake architecture. The candidate will collaborate with both technical teams and business stakeholders, providing insights on best practices and guiding data-driven decision-making. This role demands expertise in Snowflake's advanced features and cloud platforms, along with a passion for mentoring junior engineers.
Job Responsibilities
Collaborate with data engineers, system architects, and product owners to implement and support Eaton's data mesh strategy, ensuring scalability, supportability, and reusability of data products.
Lead the design and development of data products and solutions that meet business needs and align with the overall data strategy, creating complex enterprise datasets adhering to technology and data protection standards.
Deliver strategic infrastructure and data pipelines for optimal data extraction, transformation, and loading, documenting solutions with architecture diagrams, dataflows, code comments, data lineage, entity relationship diagrams, and metadata.
Design, engineer, and orchestrate scalable, supportable, and reusable datasets, managing non-functional requirements, technical specifications, and compliance.
Assess technical capabilities across Value Streams to select and align technical solutions following enterprise guardrails, executing proof of concepts (POCs) where applicable.
Oversee enterprise solutions for various data technology patterns and platforms, collaborating with senior business stakeholders, functional analysts, and data scientists to deliver robust data solutions aligned with quality measures.
Support continuous integration and continuous delivery, maintaining architectural runways for products within a Value Chain, and implement data governance frameworks and tools to ensure data quality, privacy, and compliance.
Develop and support advanced data solutions and tools, leveraging advanced data visualization tools like Power BI to enhance data insights, and manage data sourcing and consumption integration patterns from Eaton's data platform, Snowflake.
Accountable for end-to-end delivery of source data acquisition, complex transformation and orchestration pipelines, and front-end visualization.
Strong communication and presentation skills, leading collaboration with business stakeholders to deliver rapid, incremental business value/outcomes.
Lead and participate in the planning, definition, development, and high-level design of solutions and architectural alternatives.
Participate in solution planning, incremental planning, product demos, and inspect and adapt events.
Plan and develop the architectural runway for products that support desired business outcomes.
Provide technical oversight and encourage security, quality, and automation.
Support the team with a techno-functional approach as needed.
Qualifications
BE in Computer Science, Electrical, Electronics/ Any other equivalent Degree
Education level required: 10 years
Experience or knowledge of Snowflake, including administration/architecture.
Expertise in complex SQL, Python scripting, and performance tuning.
Understanding of Snowflake data engineering practices and dimensional modeling for performance and scalability.
Experience with data security, access controls, and setting up security frameworks and governance (e.g., SOX).
Technical Skills
Advanced SQL skills for building queries and resource monitors in Snowflake.
Proficiency in automating Snowflake admin tasks and handling concepts like RBAC controls, virtual warehouses, resource monitors, SQL performance tuning, zero-copy clone, and time travel.
Experience in re-clustering data in Snowflake and understanding micro-partitions.
Excellent analysis, documentation, communication, presentation, and interpersonal skills.
Ability to work under pressure, meet deadlines, and manage, mentor, and coach a team of analysts.
Strong analytical skills for complex problem-solving and understanding business problems.
Experience in data engineering, data visualization, and creating interactive analytics solutions using Power BI and Python.
Extensive experience with cloud platforms like Azure and cloud-based data storage and processing technologies.
Expertise in dimensional and transactional data modeling using OLTP, OLAP, NoSQL, and Big Data technologies.
Familiarity with data frameworks and storage platforms like Cloudera, Databricks, Dataiku, Snowflake, dbt, Coalesce, and data mesh.
Experience developing and supporting data pipelines, including code, orchestration, quality, and observability.
Expert-level programming ability in multiple data manipulation languages (Python, Spark, SQL, PL-SQL).
Intermediate experience with DevOps, CI/CD principles, and tools, including Azure Data Factory.
Experience with data governance frameworks and tools to ensure data quality, privacy, and compliance.
Solid understanding of cybersecurity concepts such as encryption, hashing, and certificates.
Strong analytical skills to evaluate data, reconcile conflicts, and abstract information.
Continual learning of new modules, ETL tools, and programming techniques.
Awareness of new technologies relevant to the environment.
Established as a key data leader at the enterprise level.
]]>","Cloud Platforms, snowflake, Data Mesh, data engineering, Etl Tools, Power Bi, Dimensional Modeling, Sql, Devops, Data Visualization, Data Governance, Azure, Python"
Sr. Data Migration Architect,Birlasoft Limited,10-15 Years,,"Hyderabad, Bengaluru, Mumbai",Software,"1.About the Job - The candidate should require mandatory Sr. Data Migration Architect Experience.
2.Job Title Sr. Data Migration Architect
3.Location- Noida, Mumbai, Pune, Bangalore, Hyderabad, Chennai
4.Educational Background - UG. -B. Tech /B. E in any specialization
PG. -MCA/MSC/MTech in Computers
5.Key Responsibilities -
Hands-on experience as Data Migration Architect with PLM Enterprise Systems, mainly Windchill and Thing Worx.
Has strong experience as a Solution Architect as well for Windchill and Thing Worx applications.
Strong Experience in UDI and other Medical Devices aspects.
Strong Experience in Medical Devices Industry with deep insights into regulatory, change control, compliances, and other nuggets.
Has done windchill migrations on cloud as a target system.
While being hands-on for Migration on Windchill PLM, the person also can help with data migration strategies; also show and tell the customer on best practices and strategies.
Strong hands-on with data migration and has handled large business transformation programs.
Has worked directly with onsite and offshore teams from execution standpoint.
Should be expert in Windchill Migration using Windchill Bulk Migrator (WBM) - at least have executed 3-4 Windchill migration project using WBM.
Should be expert in WBM tool execution (Extraction, Transformation & Loading).
6.Skills Required-
Experience in data migration including CAD Data migration.
Experience in at least one non-Windchill to Windchill data migration.
Should have good understanding of Windchill Architecture, database etc.
Should have good understanding of Windchill object models, relationships, content.
Should have experience on working with Customer for Migration Requirements Gathering, Source Data Analysis and Data Mapping.
Scripting Knowledge on Database - Oracle/SQL Server with large data set analysis.","windchill 9.1, Windchill, Data Migration"
Data Engineering Architect,Cybage Software Private Limited,10-15 Years,,Pune,Software,"This role involves leveraging advanced data technologies to drive business outcomes in the media and advertising sector, focusing on scalability and performance optimization. Successful candidates will be adept at collaborating across teams to implement comprehensive data strategies and ensure robust, secure data management.
Technical and Professional Requirements
Candidate should have good experience into the following tech stack:
Big Query / Big Data / Hadoop / Snowflake
Programming / Scripting Languages like Python / Java / Go
Hands on experience in defining and implementing various Machine Learning models for different business needs.
Hands on experience in handling various performance and data scalability problems.
Knowledge in Advertising Domain and varied experience in working on different data sets such as Campaign Performance, Ad Performance, Attribution, Audience Data etc.
Job Responsibilities:
Designing and implementing an overall data strategy as per business requirements. The strategy includes data model designs, database development standards, implementation and management of data warehouses and data analytics systems.
Identification of data sources, internal and external, and defining a plan for data management as per business data strategy.
Collaborating with cross-functional teams for the smooth functioning of the enterprise data system.
Managing end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.
Planning and execution of big data solutions using Big Query, Big Data, Hadoop, Snowflake.
Integrating technical functionality, ensuring data accessibility, accuracy, and security.","Data Engineering Architect, Hadoop, Python, Java"
Data Engineering Architect (ATC),Virtusa,10-13 Years,,Hyderabad,IT Management,"Job description
Experience 10 years of experience in data engineering, ETL processes, and cloud based data solutions.
AWS Expertise
Hands-on experience with Amazon S3 for data storage, management, and retrieval.
Proficient in AWS Athena for serverless querying and data analysis.
Experience with AWS Glue for ETL jobs and data cataloging.
Familiarity with AWS Data Pipeline or similar orchestration tools.
Programming: Proficiency in SQL, Python, or other data engineering programming languages.
Data Engineering Tools: Familiarity with tools like Apache Spark, Apache Airflow, or similar data orchestration and processing frameworks.
Database Management: Experience with relational databases (MySQL, PostgreSQL) and NoSQL databases DynamoDB, Redshift.
Big Data Experience with large-scale data processing and optimization for big data workloads.
Cloud Technologies: Knowledge of cloud infrastructure, networking, and security best practices.
Analytical Skills
Strong problem-solving skills and the ability to work with complex datasets.
Communication: Excellent communication skills and ability to work collaboratively across teams.
Design, build, and maintain scalable and efficient data pipelines using AWS technologies (S3, Athena, Glue, Lambda, etc.
Architect and optimize data storage solutions on Amazon S3, ensuring cost effectiveness and efficient retrieval of large datasets.
Develop automated data workflows using AWS Data Pipeline or similar orchestration tools for seamless data movement and processing.
Monitor and troubleshoot data pipeline performance, ensuring high availability, reliability, and security
Stay up to date with emerging technologies and trends in cloud data engineering and AWS services.","Analytics - Kinesis, S3, Aws Lambda, Platform, Pyspark, Apache Kafka, Redshift, Python"
Data Engineering Architect (ATC),Virtusa,10-12 Years,,Hyderabad,IT Management,"Job description
Experience 10 years of experience in data engineering, ETL processes, and cloud based data solutions.
AWS Expertise
Hands-on experience with Amazon S3 for data storage, management, and retrieval.
Proficient in AWS Athena for serverless querying and data analysis.
Experience with AWS Glue for ETL jobs and data cataloging.
Familiarity with AWS Data Pipeline or similar orchestration tools.
Programming: Proficiency in SQL, Python, or other data engineering programming languages.
Data Engineering Tools: Familiarity with tools like Apache Spark, Apache Airflow, or similar data orchestration and processing frameworks.
Database Management: Experience with relational databases (MySQL, PostgreSQL) and NoSQL databases DynamoDB, Redshift.
Big Data Experience with large-scale data processing and optimization for big data workloads.
Cloud Technologies: Knowledge of cloud infrastructure, networking, and security best practices.
Analytical Skills
Strong problem-solving skills and the ability to work with complex datasets.
Communication: Excellent communication skills and ability to work collaboratively across teams.
Design, build, and maintain scalable and efficient data pipelines using AWS technologies (S3, Athena, Glue, Lambda, etc.
Architect and optimize data storage solutions on Amazon S3, ensuring cost effectiveness and efficient retrieval of large datasets.
Develop automated data workflows using AWS Data Pipeline or similar orchestration tools for seamless data movement and processing.
Monitor and troubleshoot data pipeline performance, ensuring high availability, reliability, and security
Stay up to date with emerging technologies and trends in cloud data engineering and AWS services.","Analytics - Kinesis, S3, Aws Lambda, Platform, Pyspark, Apache Kafka, Redshift, Python"
Data Engineering Architect (ATC),Virtusa,10-13 Years,,Hyderabad,IT Management,"Job description
Experience 10 years of experience in data engineering, ETL processes, and cloud based data solutions.
AWS Expertise
Hands-on experience with Amazon S3 for data storage, management, and retrieval.
Proficient in AWS Athena for serverless querying and data analysis.
Experience with AWS Glue for ETL jobs and data cataloging.
Familiarity with AWS Data Pipeline or similar orchestration tools.
Programming: Proficiency in SQL, Python, or other data engineering programming languages.
Data Engineering Tools: Familiarity with tools like Apache Spark, Apache Airflow, or similar data orchestration and processing frameworks.
Database Management: Experience with relational databases (MySQL, PostgreSQL) and NoSQL databases DynamoDB, Redshift.
Big Data Experience with large-scale data processing and optimization for big data workloads.
Cloud Technologies: Knowledge of cloud infrastructure, networking, and security best practices.
Analytical Skills
Strong problem-solving skills and the ability to work with complex datasets.
Communication: Excellent communication skills and ability to work collaboratively across teams.
Design, build, and maintain scalable and efficient data pipelines using AWS technologies (S3, Athena, Glue, Lambda, etc.
Architect and optimize data storage solutions on Amazon S3, ensuring cost effectiveness and efficient retrieval of large datasets.
Develop automated data workflows using AWS Data Pipeline or similar orchestration tools for seamless data movement and processing.
Monitor and troubleshoot data pipeline performance, ensuring high availability, reliability, and security
Stay up to date with emerging technologies and trends in cloud data engineering and AWS services.","Analytics - Kinesis, S3, Aws Lambda, Platform, Pyspark, Apache Kafka, Redshift, Python"
Sr. Data Migration Architect,Birlasoft Limited,10-15 Years,,"Hyderabad, Bengaluru, Mumbai",Software,"1.About the Job - The candidate should require mandatory Sr. Data Migration Architect Experience.
2.Job Title Sr. Data Migration Architect
3.Location- Noida, Mumbai, Pune, Bangalore, Hyderabad, Chennai
4.Educational Background - UG. -B. Tech /B. E in any specialization
PG. -MCA/MSC/MTech in Computers
5.Key Responsibilities -
Hands-on experience as Data Migration Architect with PLM Enterprise Systems, mainly Windchill and Thing Worx.
Has strong experience as a Solution Architect as well for Windchill and Thing Worx applications.
Strong Experience in UDI and other Medical Devices aspects.
Strong Experience in Medical Devices Industry with deep insights into regulatory, change control, compliances, and other nuggets.
Has done windchill migrations on cloud as a target system.
While being hands-on for Migration on Windchill PLM, the person also can help with data migration strategies; also show and tell the customer on best practices and strategies.
Strong hands-on with data migration and has handled large business transformation programs.
Has worked directly with onsite and offshore teams from execution standpoint.
Should be expert in Windchill Migration using Windchill Bulk Migrator (WBM) - at least have executed 3-4 Windchill migration project using WBM.
Should be expert in WBM tool execution (Extraction, Transformation & Loading).
6.Skills Required-
Experience in data migration including CAD Data migration.
Experience in at least one non-Windchill to Windchill data migration.
Should have good understanding of Windchill Architecture, database etc.
Should have good understanding of Windchill object models, relationships, content.
Should have experience on working with Customer for Migration Requirements Gathering, Source Data Analysis and Data Mapping.
Scripting Knowledge on Database - Oracle/SQL Server with large data set analysis.","windchill 9.1, Windchill, Data Migration"
Data Platform Architect,DIAGEO India,2-6 Years,,Bengaluru,Food and Beverage,"Preferred
Experience in Databricks Lakehouse architecture with experience using Azure Databricks.
Experience working on File formats (Parquet/ORC/AVRO/Delta/Hudi etc.)
Exposure to using CI/CD tools like Azure DevOps
Experience and knowledge on Azure data offerings","Delta, orc, platform architecture, Azure Databricks, Avro, Azure Devops"
EY - GDS Consulting - AI - DATA - GCP Architect - Manager,Ernst and young LLP,8-13 Years,,Bengaluru,Consulting,"At EY, you'll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture, and technology to become the best version of you. And we're counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.
EY GDS Data and Analytics (D&A) GCP Data Engineer - Manager
We're looking for candidates with strong technology and data understanding in the data engineering space, having proven delivery capability. This is a fantastic opportunity to be part of a leading firm as well as a part of a growing Data and Analytics team.
The ideal candidate will have deep expertise in cloud architecture, security, networking, and automation, with hands-on experience in large-scale migrations.
Your key responsibilities
Cloud Strategy & Architecture:
Design and implement scalable, secure, and cost-effective GCP architectures for a multi-cloud environment.
Migration Planning:
Develop and execute migration strategies for applications, data, and infrastructure from on-premise or other cloud platforms (AWS/Azure) to GCP.
Infrastructure as Code (IaC):
Utilize Terraform, Ansible, or other IaC tools for automated provisioning and management of cloud resources.
Security & Compliance:
Ensure cloud environments adhere to industry security best practices, compliance standards (e.g., ISO, SOC, HIPAA), and Google Cloud security frameworks.
CI/CD & DevOps Integration:
Work with DevOps teams to integrate CI/CD pipelines using Azure DevOps, GitHub Actions, or Jenkins for cloud deployments.
Networking & Hybrid Cloud:
Design and implement hybrid and multi-cloud networking solutions, including VPNs, interconnects, and service mesh (Anthos, Istio).
Performance & Cost Optimization:
Monitor, optimize, and provide recommendations for cloud resource utilization, cost efficiency, and performance enhancements.
Stakeholder Collaboration:
Work closely with business, security, and engineering teams to align cloud solutions with organizational goals.
Incident Management & Troubleshooting:
Provide technical leadership for incident resolution, root cause analysis, and continuous improvement in cloud operations.
Skills and attributes for success
8 to 13 years of Hands-on experience in the field of data warehousing, ETL.
Strong hands-on experience with GCP services (Compute Engine, GKE, Cloud Functions, Big Query, IAM, Cloud Armor, etc.).
Familiarity with AWS and/or Azure services and cross-cloud integrations.
Proficiency in Terraform, Ansible, or other IaC tools.
Experience with containerization (Docker, Kubernetes) and microservices architecture.
Strong networking skills, including VPC design, Cloud Interconnect, and hybrid cloud solutions.
Understanding of security best practices, encryption, and identity management in a multi-cloud setup.
Experience in Migration from on-prem to GCP or hybrid cloud architectures.
Experience with Anthos, Istio, or service mesh technologies.
Strong scripting skills in Python, Bash, or Go for automation.
To qualify for the role, you must have
Be a computer science graduate or equivalent with 8 to 13 years of industry experience.
Have working experience in an Agile-based delivery methodology (Preferable).
Flexible and proactive/self-motivated working style with strong personal ownership of problem resolution.
Good analytical skills and enjoys solving complex technical problems.
Proficiency in Software Development Best Practices.
Good debugging and optimization skills.
Good communicator (written and verbal formal and informal).
Client management skills are good to have.
What working at EY offers
At EY, we're dedicated to helping our clients, from start-ups to Fortune 500 companies and the work we do with them is as varied as they are.
You get to work with inspiring and meaningful projects. Our focus is education and coaching alongside practical experience to ensure your personal development. We value our employees and you will be able to control your own development with an individual progression plan. You will quickly grow into a responsible role with challenging and stimulating assignments. Moreover, you will be part of an interdisciplinary environment that emphasizes high quality and knowledge exchange. Plus, we offer:
Support, coaching, and feedback from some of the most engaging colleagues around.
Opportunities to develop new skills and progress your career.
The freedom and flexibility to handle your role in a way that's right for you.
EY | Building a better working world
EY exists to build a better working world, helping to create long-term value for clients, people, and society and build trust in the capital markets.
Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform, and operate.
Working across assurance, consulting, law, strategy, tax, and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.","Infrastructure as Code (IaC), Cloud Strategy & Architecture, Cloud"
GDS Consulting - AI and DATA - Azure Architect - Manager,Ernst and young LLP,10-12 Years,,Cochin / Kochi / Ernakulam,Consulting,"EY GDS Data and Analytics (D&A) Cloud Architect
As part of our EY-GDS D&A (Data and Analytics) team, we help our clients solve complex business challenges with the help of data and technology. We dive deep into data to extract the greatest value and discover opportunities in key business and functions like Banking, Insurance, Manufacturing, Healthcare, Retail, Manufacturing and Auto, Supply Chain, and Finance.
The opportunity
We're looking for Managers (GTM +Cloud/ Big Data Architects) with strong technology and data understanding having proven delivery capability in delivery and pre sales. This is a fantastic opportunity to be part of a leading firm as well as a part of a growing Data and Analytics team.
Your key responsibilities
Have proven experience in driving Analytics GTM/Pre-Sales by collaborating with senior stakeholder/s in the client and partner organization in BCM, WAM, Insurance. Activities will include pipeline building, RFP responses, creating new solutions and offerings, conducting workshops as well as managing in flight projects focused on cloud and big data.
Need to work with client in converting business problems/challenges to technical solutions considering security, performance, scalability etc. [ 10- 15 years]
Need to understand current & Future state enterprise architecture.
Need to contribute in various technical streams during implementation of the project.
Provide product and design level technical best practices
Interact with senior client technology leaders, understand their business goals, create, architect, propose, develop and deliver technology solutions
Define and develop client specific best practices around data management within a Hadoop environment or cloud environment
Recommend design alternatives for data ingestion, processing and provisioning layers
Design and develop data ingestion programs to process large data sets in Batch mode using HIVE, Pig and Sqoop, Spark
Develop data ingestion programs to ingest real-time data from LIVE sources using Apache Kafka, Spark Streaming and related technologies
Skills and attributes for success
Architect in designing highly scalable solutions Azure, AWS and GCP.
Strong understanding & familiarity with all Azure/AWS/GCP /Bigdata Ecosystem components
Strong understanding of underlying Azure/AWS/GCP Architectural concepts and distributed computing paradigms
Hands-on programming experience in Apache Spark using Python/Scala and Spark Streaming
Hands on experience with major components like cloud ETLs,Spark, Databricks
Experience working with NoSQL in at least one of the data stores - HBase, Cassandra, MongoDB
Knowledge of Spark and Kafka integration with multiple Spark jobs to consume messages from multiple Kafka partitions
Solid understanding of ETL methodologies in a multi-tiered stack, integrating with Big Data systems like Cloudera and Databricks.
Strong understanding of underlying Hadoop Architectural concepts and distributed computing paradigms
Experience working with NoSQL in at least one of the data stores - HBase, Cassandra, MongoDB
Good knowledge in apache Kafka & Apache Flume
Experience in Enterprise grade solution implementations.
Experience in performance bench marking enterprise applications
Experience in Data security [on the move, at rest]
Strong UNIX operating system concepts and shell scripting knowledge
To qualify for the role, you must have
Flexible and proactive/self-motivated working style with strong personal ownership of problem resolution.
Excellent communicator (written and verbal formal and informal).
Ability to multi-task under pressure and work independently with minimal supervision.
Strong verbal and written communication skills.
Must be a team player and enjoy working in a cooperative and collaborative team environment.
Adaptable to new technologies and standards.
Participate in all aspects of Big Data solution delivery life cycle including analysis, design, development, testing, production deployment, and support
Responsible for the evaluation of technical risks and map out mitigation strategies
Experience in Data security[on the move, at rest]
Experience in performance bench marking enterprise applications
Working knowledge in any of the cloud platform, AWS or Azure or GCP
Excellent business communication, Consulting, Quality process skills
Excellent Consulting Skills
Excellence in leading Solution Architecture, Design, Buildand Execute for leading clients in Banking, Wealth Asset Management, or Insurance domain.
Minimum 7 years hand-on experience in one or more of the above areas.
Minimum 10 years industry experience
Ideally, you'll also have
Strong project management skills
Client management skills
Solutioning skills
What we look for
People with technical experience and enthusiasm to learn new things in this fast-moving environment","Python/Scala, Hadoop architecture, Spark Streaming, Apache Spark, Apache Kafka"
Architect - Data Gov. & Stewardship,Pepsico india,4-7 Years,,Hyderabad,Food and Beverage,"We are seeking a Data Governance & Stewardship Business Analysts to join our growing team. In this role, you will be responsible for creating, developing, and implementing training content and frameworks to support data governance and stewardship initiatives. This individual will play a key role in introducing new technologies, managing the training playbook, and maintaining a centralized training repository. You will also help develop a consistent and effective training delivery model that empowers teams with the knowledge and tools necessary for successful data governance practices.
Responsibilities
Develop comprehensive training content covering all aspects of data governance, data stewardship, and relevant Informatica technologies.,Design engaging and interactive materials, including presentations, e-learning modules, handbooks, and guides, that cater to different levels of users (e.g., technical and non-technical).,Create content tailored to various aspects of data governance, including data quality management, data stewardship processes, metadata management, and compliance.,Continuously update the training playbook to reflect new content, processes, tools, and user feedback.,Develop and manage a centralized training playbook that provides a structured approach to data governance training, including best practices, workflows, and step-by-step guides.,Work closely with business and technical stakeholders to identify knowledge gaps and training needs related to data governance and stewardship.
Qualifications
Undergraduate in Management Information System, Data Analytics, or Ai DG Certifications","Mis, Architect, Stewardship, Data Analytics, Data Governance"
MDM Architect- Data Governance,Fractal Analytics,8-11 Years,,"Noida, Mumbai, Pune",Consulting,"Responsibilities
You will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management
Lead the end-to-end design, architecture, and implementation of MDM solutions
Define and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.
Collaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.
Design data models and hierarchies for Customer, Product, Vendor, and other master domains.
Develop and operationalize Data Governance frameworks aligned to business and compliance needs.
Enable data stewardship workflows, match & merge rules, and exception management.
Integrate MDM systems with upstream and downstream applications across the enterprise.
Lead workshops and training sessions on MDM and Data Governance for client teams.
Support RFPs, proposals, and client presentations with MDM/DG expertise
Assess, validate and implement MDM architecture foron-prem, cloud and hybridenvironments with expertise in MDM solutions likeOracle MDM solutions (ERP, CDH, CDM),SAP MDG(S4/Hana, ERP), Informatica IDMCand other modern MDM solutions.
Develop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.
Collaborate with stakeholders to define and implementMDM strategies, standards, and operating models.
Develop and enforceend-to-end master data lifecycle processesincluding data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.
Collaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.
Support leadership in designing thought leadership, publish POV s/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.
Good to Have
Hands-on implementation experience with on-premMDMandERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG,SAP S4/HANA, Reltio, Stiboor hybrid deployments with other modern MDM, ERP and CRM platforms.
Strong knowledge ofreference data managementanddata stewardship workflows.
Experience withETL/ELT toolsand integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).
Familiarity withdata governance tools(e.g., Collibra, Alation, Informatica CDGC, etc.) andDQ tools(e.g., Informatica IDQ, CDQ, etc.).
Basic understanding of data governance best practices, data quality management, data privacy regulations
Familiarity with Agentic AI, machine learning, and analytics technologies
Knowledge of SQL, PL/SQl, Python, or any other database
Excellent communication and interpersonal skillsto collaborate effectively with clients and internal teams.","CDQ, Alation, S4hana, Informatica Idq, Collibra, PL/SQl, Sql, Python"
MDM Architect- Data Governance,Fractal Analytics,8-11 Years,,"Gurugram, Bengaluru, Chennai",Consulting,"Responsibilities
You will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management
Lead the end-to-end design, architecture, and implementation of MDM solutions
Define and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.
Collaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.
Design data models and hierarchies for Customer, Product, Vendor, and other master domains.
Develop and operationalize Data Governance frameworks aligned to business and compliance needs.
Enable data stewardship workflows, match & merge rules, and exception management.
Integrate MDM systems with upstream and downstream applications across the enterprise.
Lead workshops and training sessions on MDM and Data Governance for client teams.
Support RFPs, proposals, and client presentations with MDM/DG expertise
Assess, validate and implement MDM architecture foron-prem, cloud and hybridenvironments with expertise in MDM solutions likeOracle MDM solutions (ERP, CDH, CDM),SAP MDG(S4/Hana, ERP), Informatica IDMCand other modern MDM solutions.
Develop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.
Collaborate with stakeholders to define and implementMDM strategies, standards, and operating models.
Develop and enforceend-to-end master data lifecycle processesincluding data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.
Collaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.
Support leadership in designing thought leadership, publish POV s/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.
Good to Have
Hands-on implementation experience with on-premMDMandERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG,SAP S4/HANA, Reltio, Stiboor hybrid deployments with other modern MDM, ERP and CRM platforms.
Strong knowledge ofreference data managementanddata stewardship workflows.
Experience withETL/ELT toolsand integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).
Familiarity withdata governance tools(e.g., Collibra, Alation, Informatica CDGC, etc.) andDQ tools(e.g., Informatica IDQ, CDQ, etc.).
Basic understanding of data governance best practices, data quality management, data privacy regulations
Familiarity with Agentic AI, machine learning, and analytics technologies
Knowledge of SQL, PL/SQl, Python, or any other database
Excellent communication and interpersonal skillsto collaborate effectively with clients and internal teams.","CDQ, Alation, S4hana, Informatica Idq, Collibra, PL/SQl, Sql, Python"
Architect - Data Engineer,Pepsico india,10-15 Years,,Hyderabad,Food and Beverage,"Responsibilities:Qualifications
Lead the migration and modernization of data platforms, moving applications and pipelines to Azure-based solutions.
Actively contribute to code development in projects and services.
Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.
Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.
Implement best practices around systems integration, security, performance, and data management.
Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.
Develop and optimize procedures to transition data into production.
Define and manage SLAs for data products and operational processes.
Prototype and build scalable solutions for data engineering and analytics.
Research and apply state-of-the-art methodologies in data and Platform engineering.
Create and maintain technical documentation for knowledge sharing.
Develop reusable packages and libraries to enhance development efficiency.
Qualifications:
Bachelor's degree in Computer Science, MIS, Business Management, or related field
10 + years experience in Information Technology
4 + years of Azure, AWS and Cloud technologies
Experience in data platform engineering, with a focus on cloud transformation and modernization.
Strong knowledge of Azure services, including Databricks, Azure Data Factory, Synapse Analytics, and Azure DevOps (ADO).
Proficiency in SQL, Python, and Spark for data engineering tasks.
Hands-on experience building and scaling data pipelines in cloud environments.
Experience with CI/CD pipeline management in Azure DevOps (ADO).
Understanding of data governance, security, and compliance best practices.
Experience working in an Agile development environment.
Prior experience in migrating applications from legacy platforms to the cloud.
Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.
Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.
Experience with lagacy RDBMS (Oracl, DB2, Teradata)
Background in supporting data science models in production.","Sql, Python, Spark, Databricks, Azure Devops, Data Architect, Kafka"
Network Architect - Data Center,Mind Pool Technologies,10-17 Years,INR 63.5 - 70 LPA,United Arab Emirates,Login to check your skill match score,"Expertise in Datacenter network design and implementation on SDN technologies e.g. Cisco ACI and NSX solutions
Experience in designing integrated SDN solution to integrate with orchestration tools
Expertise in planning and designing DC transformation and migration methods
Design, Solution, Build and Deployment Experience on below Advanced technologies
Hierarchical/Extended Datacenter networks
DR solutions (Logical DC networks)
Cisco ACI
Vmware NSX
Load balancing (F5, Citrix NetScaler, Cisco ACE)
Wan optimization (Riverbed and Cisco)
DNS/DHCP/IPAM (InfoBlock)
Proxy solutions
Expertise in preparing high quality HLD and LLD design documents",Cisco Aci
"Architect , Data Science [GEN AI, LLM, NLP, Conversational AI]",Birdeye,Fresher,,India,Login to check your skill match score,"Birdeye is looking for a highly experienced Data Science Architect to design, build, and scale our AI/ML infrastructure. This high-impact, high-visibility role will be at the forefront of architecting next-generation AI solutions, optimizing ML pipelines, and deploying state-of-the-art models into production.
You will work closely with data scientists, ML engineers, and software developers to bridge the gap between AI research and real-world applications. If you are passionate about MLOps, cloud-native AI architectures, and building scalable machine learning solutions, this role is for you!
Key Responsibilities
AI/ML Architecture & Infrastructure Development
Design and build highly scalable, production-ready ML architectures for real-time and batch AI processing.
Develop end-to-end ML workflows, including data ingestion, feature engineering, model training, deployment, and monitoring.
Architect and optimize distributed AI computing pipelines for large-scale applications.
Implement fault-tolerant, high-availability ML infrastructure for mission-critical systems.
ML Model Deployment & Lifecycle Management
Develop and maintain CI/CD pipelines for ML models, ensuring automated deployment, version control, and rollback mechanisms.
Implement model monitoring, logging, and drift detection to maintain performance in production.
Automate model retraining, hyperparameter tuning, and A/B testing using scalable MLOps frameworks.
Ensure secure AI deployments, enforcing role-based access control, encryption, and compliance.
Data Engineering & Processing
Design and maintain scalable data pipelines for AI models, ensuring efficient ETL and real-time data streaming.
Work with big data frameworks (Apache Spark, Hadoop, Kafka) to process petabyte-scale datasets.
Implement feature stores and data versioning for reproducible AI experiments.
Performance Optimization & Reliability
Continuously optimize AI/ML infrastructure for latency, scalability, and cost-efficiency.
Implement observability & monitoring using Prometheus, Grafana, ELK Stack, and other tools.
Develop automated failover & self-healing mechanisms to ensure ML system reliability.
Collaboration & Leadership
Work cross-functionally with data scientists, software engineers, and DevOps teams to streamline AI model deployment.
Advocate MLOps best practices, enabling teams to rapidly prototype and deploy AI solutions.
Mentor junior engineers and data scientists on AI engineering and operationalization.
Stay ahead of AI/ML industry trends, evaluating new technologies to enhance AI/ML capabilities at Birdeye.","Model monitoring, Big data frameworks, Drift detection, Real-time data streaming, Performance optimization, Observability monitoring, Cloud-native AI architectures, Scalable machine learning solutions, CI CD pipelines, AI ML Architecture, Data pipelines, Data versioning, Hyperparameter tuning, Feature stores, Hadoop, Prometheus, Apache Spark, Kafka, Grafana, Elk Stack, MLops, Etl, Logging"
Solution Architect Data Engineering,Techs to Suit Inc,14-17 Years,,"Hyderabad, India",Login to check your skill match score,"Job Location - Hyderabad, Gurgaon, Ahmedabad, Indore
Exp- 14 to 17 Years
Joining Time- Max 30 days
Work from Office, All Days
Max Salary 45 Lakhs Fixed
Job Summary:
As a Solution Architect, you will collaborate with our sales, presales and COE teams to provide technical expertise and support throughout the new business acquisition process. You will play a crucial role in understanding customer requirements, presenting our solutions, and demonstrating the value of our products.
You thrive in high-pressure environments, maintaining a positive outlook and understanding that career growth is a journey that requires making strategic choices. You possess good communication skills, both written and verbal, enabling you to convey complex technical concepts clearly and effectively. You are a team player, customer-focused, self-motivated, responsible individual who can work under pressure with a positive attitude. You must have experience in managing and handling RFPs/ RFIs, client demos and presentations, and converting opportunities into winning bids. You possess a strong work ethic, positive attitude, and enthusiasm to embrace new challenges. You can multi-task and prioritize (good time management skills), willing to display and learn. You should be able to work independently with less or no supervision. You should be process-oriented, have a methodical approach and demonstrate a quality-first approach.
Ability to convert client's business challenges/ priorities into winning proposal/ bid through excellence in technical solution will be the key performance indicator for this role.
What you'll do
Architecture & Design: Develop high-level architecture designs for scalable, secure, and robust solutions.
Technology Evaluation: Select appropriate technologies, frameworks, and platforms for business needs.
Cloud & Infrastructure: Design cloud-native, hybrid, or on-premises solutions using AWS, Azure, or GCP.
Integration: Ensure seamless integration between various enterprise applications, APIs, and third-party services.
Design and develop scalable, secure, and performant data architectures on Microsoft Azure and/or new generation analytics platform like MS Fabric.
Translate business needs into technical solutions by designing secure, scalable, and performant data architectures on cloud platforms.
Select and recommend appropriate Data services (e.g. Fabric, Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, Power BI etc) to meet specific data storage, processing, and analytics needs.
Develop and recommend data models that optimize data access and querying. Design and implement data pipelines for efficient data extraction, transformation, and loading (ETL/ELT) processes.
Ability to understand Conceptual/Logical/Physical Data Modelling.
Choose and implement appropriate data storage, processing, and analytics services based on specific data needs (e.g., data lakes, data warehouses, data pipelines).
Understand and recommend data governance practices, including data lineage tracking, access control, and data quality monitoring.
What you will Bring
10+ years of working in data analytics and AI technologies from consulting, implementation and design perspectives
Certifications in data engineering, analytics, cloud, AI will be a certain advantage
Bachelor's in engineering/ technology or an MCA from a reputed college is a must
Prior experience of working as a solution architect during presales cycle will be an advantage","Architecture Design, Data Governance"
Enterprise Architect - Data & AI,Emids,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Role
The Enterprise Data Architect proactively leads and supports software developers, database architects, data analysts, and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects
This role will be responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams in alignment with architectural governance and standards
The strategy includes data model designs, database development standards, implementation and management of data warehouses and data analytics systems
Identifying data sources, both internal and external, and working out a plan for data management that is aligned with organizational data strategy
Coordinating and collaborating with cross-functional teams, stakeholders, and vendors for the smooth functioning of the enterprise data system
Managing end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution
Ability to analyze and optimize cloud costs.
Key Responsibilities
Strategic Data Leadership: Define and drive the data architecture and AI strategy in alignment with organizational goals, focusing on data-driven decision-making and improving operational efficiency.
Architecture Design: Lead the design and implementation of scalable and efficient data architectures that support advanced analytics, reporting, and machine learning applications.
Integration Solutions: Oversee the development of data integration strategies, enabling seamless data flow across various healthcare systems, including RCMs, EHRs, Claims, Policy Admin and Billing systems, and analytics platforms.
Data Governance & Compliance: Establish and maintain data governance frameworks, ensuring compliance with regulations such as HIPAA, FedRAMP, and HITRUST while promoting data integrity, security, and quality.
Collaboration with Stakeholders: Partner with executive leadership, enterprise architects, data scientists, analysts, and IT teams to understand data needs, provide architectural guidance, and ensure successful project execution.
Emerging Technology Assessment: Evaluate and recommend emerging technologies and tools that can enhance healthcare data architecture and analytics capabilities.
Performance Metrics: Establish key performance indicators (KPIs) and metrics to evaluate product success and drive continuous improvement based on user feedback and data analysis.
Documentation and Standards: Develop comprehensive documentation of data architecture standards, best practices, and processes for internal teams.
Mentorship and Team Development: Mentor and guide data architects and engineers, fostering a culture of continuous learning and innovation within the data team.
Qualifications
15+ years of experience in data architecture, with a minimum of 3+ years in the healthcare domain.
Must have 5+ years of experience in any AWS/Azure/GCP cloud technology.
Expertise in data engineering frameworks and tools
Must have 5+ years of experience in Databricks, with the ability to architect, implement, and scale workloads.","data engineering frameworks and tools, AWS, Databricks, Azure, Gcp"
IT Office Business Solutions (Azure Data & AI Solutions Architect) - req32808,The World Bank,7-9 Years,,"Chennai, India",Login to check your skill match score,"IT Office Business Solutions (Azure Data & AI Solutions Architect)
Job #: req32808
Organization: World Bank
Sector: Information Technology
Grade: GF
Term Duration: 4 years 0 months
Recruitment Type: Local Recruitment
Location: Chennai,India
Required Language(s): English
Preferred Language(s)
Closing Date: 5/4/2025 (MM/DD/YYYY) at 11:59pm UTC
Description
Do you want to build a career that is truly worthwhile Working at the World Bank Group provides a unique opportunity for you to help our clients solve their greatest development challenges. The World Bank Group is one of the largest sources of funding and knowledge for developing countries; a unique global partnership of five institutions dedicated to ending extreme poverty, increasing shared prosperity and promoting sustainable development. With 189 member countries and more than 130 offices worldwide, we work with public and private sector partners, investing in groundbreaking projects and using data, research, and technology to develop solutions to the most urgent global challenges. For more information, visit www.worldbank.org
ITS Vice Presidency Context
The Information and Technology Solutions (ITS) Vice Presidential Unit (VPU) enables the World Bank Group to achieve its mission of ending extreme poverty and boost shared prosperity on a livable planet by delivering transformative information and technologies to its staff working in over 150+ locations. For more information on ITS, see this video:https://www.youtube.com/watchreload=9&v=VTFGffa1Y7w
ITS shapes its strategy in response to changing business priorities and leverages new technologies to achieve three high-level business outcomes: business enablement, by providing Bank Group units with innovative digital tools and technologies to transform how they deliver value for their clients; empowerment & effectiveness, by ensuring that all Bank Group staff are connected, able to find information, and productive to accelerate the delivery of development solutions globally; and resilience, by equipping the Bank Group to provide risk-based cybersecurity and robust data protection for a global network and a growing cloud platform. The ITS mission is to leverage information and technology as a force multiplier to accelerate, deepen, and sustain development impact. ITS is on an Agile transformation journey, reshaping all aspects of the operating model to increase and accelerate value creation.
Implementation of the strategy is guided by three core principles. The first is to deliver solutions for business partners that are customer-centric, innovative, and transformative. The second is to provide the Bank Group with value for money with selective and standard technologies. The third principle is to excel at the basics by providing a high performing, robust, and resilient IT environment for the organization.
WBG Finance (ITSFI) is responsible for providing high quality, streamlined information and technology solutions for the World Bank's Financial services, which include Corporate Finance, Risk Management, Controls, Treasury, Loans, Accounting, and Concessional finance (handling donor contributions from inception to the point of final disbursement, including IDA, Financial Intermediary Funds and Trust Funds). ITSFI is additionally responsible for building its IT services using a shared platform that provides scale, leverage, reliability, and control while at the same time improving responsiveness to emerging business needs. The ITSFI team is accountable for the implementation of the ITS Strategy supporting WBG core finance business processes.
As a unit within the ITSFI department, ITSFT is responsible for providing core accounting solutions for the World Bank Group staff that are in around 185 countries across the world. The major areas include applications in the Accounts Payable, Accounts Receivable, General Ledger and Asset accounting business processes to support the operations of World Bank Group Finance & Accounting, Treasury and Development Finance Vice presidencies.
Job Opportunity
ITSFT is looking for an experienced Business Solutions Architect to lead the design and implementation of advanced business solutions on Azure, focusing on application engineering, data modeling, and secure cloud engineering practices. The role demands extensive expertise in the financial domain, particularly in building tools and frameworks for AI and data science applications on Azure, with exclusive experience in developing generative AI applications that meet WBG security, compliance and performance standards.
Roles & Responsibilities
Architect, design and implement highly scalable, available, and fault tolerant systems in the Cloud (Azure) and on-prem that will tightly integrate with SAP S4 HANA, Data lakes and other enterprise systems.
Lead the technical engagement for defining and documenting application architectures aligned with enterprise standards, guidelines, and best practices.
Ensure solution architectures for enterprise solutions drive efficiencies, improve performance and reliability, increase innovation, and reduce costs. These solution architectures could be cloud-native, on-premises or leverage a hybrid cloud model.
Ensure solutions leverage industry standard enterprise integration technologies to integrate disparate systems on-premises, in the cloud and across cloud providers.
Oversight of the contractual resources that support existing financial applications on SAP and cloud.
Build, maintain key architectural artifacts and ensure alignment with architectural guidelines, best practices, and blueprints
Lead and/or take part in deep-dive education and design exercises along with technical discussions to create enterprise grade solutions.
Define transition approaches and roadmaps for migrating legacy systems
Provide strategic direction in AI and data architecture, guiding the organization towards innovative and effective solutions.
Develop and deliver compelling presentations and demos to achieve buy-in from relevant stakeholders and to communicate the same to implementation teams
Engage with project teams to provide technical guidance while ensuring alignment with defined architectural blueprints.
Drive continuous improvement and innovation in AI and data science practices, ensuring the organization remains at the forefront of technological advancements.
Selection Criteria
Master's degree with 5 years relevant experience or bachelor's degree with a minimum of 7 years of relevant experience
Minimum 7 years of experience as a Data and AI Architect, with a proven track record in application engineering, data modeling, and AI solutions on Azure.
Proficiency in Azure, Power BI, semantic modeling, and financial reporting. Extensive experience in building tools and frameworks for AI and data science applications.
Strong background in the financial domain
Good to have extensive knowledge in SAP, including SAP HANA migration
Demonstrated ability to lead and manage teams, providing strategic direction and fostering a collaborative environment.
Good knowledge of information management and information technology including system design, planning, and project delivery management.
Demonstrated analytical ability and problem-solving skills in translating business requirements into technical requirements/business systems.
Demonstrated ability to quickly resolve technical problems in complex and time sensitive production systems.
Ability to express thoughts and ideas effectively in oral and written communications.
Ability to act sensitively in multicultural environments and build effective working relations with internal and external partners.
Highest ethical standards.
Microsoft Certified Azure Solutions Architect Expert, Microsoft Certified Azure AI Engineer
In addition to the above mandatory requirements, the following are highly desired-
WBG experience and knowledge of WBG specific custom processes and applications.
Experience in Dremio and SAP.
World Bank Group Core Competencies
Deliver Results for Clients
Proactively addresses clients stated and unstated needs : Adds value by constantly looking for a better way to get more impactful results; sets challenging stretch goals for oneself. Immerses oneself in client experiences and perspective by asking probing questions to understand unmet needs. Demonstrates accountability for achieving results that have a development impact and financial, environmental and social sustainability. Identifies and proposes solutions to mitigate and manage risks.
Collaborate Within Teams and Across Boundaries
Collaborates across boundaries, gives own perspective and willingly receives diverse perspectives : Appropriately involves others in decision making and communicates with key stakeholders. Approaches conflicts as common problems to be solved. Actively seeks and considers diverse ideas and approaches displaying a sense of mutuality and respect. Integrates WBG perspective into work
Lead and Innovate
Develops innovative solutions : Contributes new insights to understand situations and develops solutions to resolve complex problems. Adapts as circumstances require and manages impact of own behavior on others in context of WBG's values and mission. Identifies and pursues innovative approaches to resolve issues
Create, Apply and Share Knowledge
Applies knowledge across WBG to strengthen solutions for internal and/or external clients : Leverages department's expertise and body of knowledge across WBG to strengthen internal and/or external client solutions. Seeks to learn from more experienced staff to deepen or strengthen their professional knowledge and helps others to learn. Builds personal and professional networks inside and outside the department unit
Make Smart Decisions
Interprets a wide range of information and pushes to move forward : Seeks diversity of information and inputs, researches possible solutions, and generates recommended options. Identifies and understands risks and proposes recommendations. Based on risk analysis makes decisions in a timely manner within own area of responsibility, considering the interests and concerns of stakeholders.
World Bank Group Core Competencies
The World Bank Group offers comprehensive benefits, including a retirement plan; medical, life and disability insurance; and paid leave, including parental leave, as well as reasonable accommodations for individuals with disabilities.
We are proud to be an equal opportunity and inclusive employer with a dedicated and committed workforce, and do not discriminate based on gender, gender identity, religion, race, ethnicity, sexual orientation, or disability.
Learn more about working at the World Bank and IFC , including our values and inspiring stories.","Azure Power BI, Dremio, Application Engineering, AI solutions, Sap S4 Hana, Data Modeling, Azure"
Architect - Data Center,Bechtel Corporation,6-8 Years,,India,Login to check your skill match score,"Requisition ID: 282943
Relocation Authorized: National - Family
Telework Type: Full-Time Office/Project
Work Location: Various Bechtel Project Locations
Extraordinary Teams Building Inspiring Projects
Since 1898, we have helped customers complete more than 25,000 projects in 160 countries on all seven continents that have created jobs, grown economies, improved the resiliency of the world's infrastructure, increased access to energy, resources, and vital services, and made the world a safer, cleaner place.
Differentiated by the quality of our people and our relentless drive to deliver the most successful outcomes, we align our capabilities to our customers objectives to create a lasting positive impact. We serve the Infrastructure; Nuclear, Security & Environmental; Energy; Mining & Metals, and the Manufacturing and Technology markets. Our services span from initial planning and investment, through start-up and operations.
Core to Bechtel is our Vision, Values and Commitments . They are what we believe, what customers can expect, and how we deliver. Learn more about our extraordinary teams building inspiring projects in our Impact Report .
Bechtel India is a global operation that supports execution of projects and services around the world. Working seamlessly with business line home offices, project sites, customer organizations and suppliers, our teams have delivered more than 125 projects since our inception in 1994.
Our offices in Gurgaon, Vadodara and Chennai will grow significantly and sustainably with exciting career opportunities for both professionals and young graduates who are passionate about creating a cleaner, greener, and safer world; building transformational infrastructure; making decarbonization a reality; and protecting people and the environment.
Job Summary
Architect with more than 6 years of experience in Manufacturing and Infrastructure projects (with relevant experience in Data center) projects in a design office of repute. A candidate who has completed at least 1 data center project shall be given more weightage. Should have exposure to design office practices and familiarity with computer aided design and 3D modeling tools like Revit.
Major Responsibilities
Shall perform / supervise calculations, prepare material requisitions, service requisitions, code analysis, standard details, technical specifications and quantity take-offs and provide input for design drawings during all phases of projects.
Shall check / review the drawings prepared by the designers.
Shall assist in the development of basic layout drawings.
Shall lead conceptual studies and inter-disciplinary reviews
Shall create perspectives and presentation of design to client.
Provide technical training.
Perform feasibility studies for site development, building configuration, climate studies.
Education And Experience Requirements
Minimum 5-year degree in Architecture from an accredited college or university.
Candidate with Master's degree is desirable.
Professional license from a recognized licensing board and/or LEED certification shall be of added advantage.
Experience of making Architectural presentation shall be an added advantage.
Level I: 6 - 8 years of relevant work experience
Level II: 8 - 10 years of relevant work experience
Required Knowledge And Skills
Knowledge of architectural techniques and design principles with a basic knowledge of the types of data center, data hall layout, mechanical systems, electrical systems, and security systems needed to design a data center.
Knowledge of precedents and latest trends in architectural engineering, the principles and practices of related technical areas.
Knowledge of Engineering Procedures and design guides.
Thorough knowledge of the roles played by other engineering disciplines on projects.
Knowledge of regulatory Indian and International codes and standards, industry practices and design criteria pertinent to architectural engineering design, including fire life safety codes.
Knowledge of constructability and applicable standards and codes
Proficiency in the use of Revit, Navisworks and exposure to BIM is essential.
Proficient in selection of material from constructability and total installed cost perspective.
Skill in oral and written communication.
Should be proficient in using MS office tools.
Proved ability of managing a team of architects and designers will be an added advantage.
Previous experience of Data center design including administration building, process/ non process buildings and other ancillary facilities like maintenance building, guard house, substation, electrical buildings, pump house, etc.
Knowledge of master planning and fire life safety design guidelines
Good knowledge of faade design and material selection (including interior & exterior finishes)
Experience in working on EPC projects shall be an added advantage.
Total Rewards/Benefits
For decades, Bechtel has worked to inspire the next generation of employees and beyond! Because our teams face some of the world's toughest challenges, we offer robust benefits to ensure our people thrive. Whether it is advancing careers, delivering programs to enhance our culture, or providing time to recharge, Bechtel has the benefits to build a legacy of sustainable growth. Learn more at Bechtel Total Rewards
Diverse Teams Build The Extraordinary
As a global company, Bechtel has long been home to a vibrant multitude of nationalities, cultures, ethnicities, and life experiences. This diversity has made us a more trusted partner, more effective problem solvers and innovators, and a more attractive destination for leading talent.
We are committed to being a company where every colleague feels that they belong-where colleagues feel part of One Team, respected and rewarded for what they bring, supported in pursuing their goals, invested in our values and purpose, and treated equitably. Click here to learn more about the people who power our legacy.
Bechtel is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity and expression, age, national origin, disability, citizenship status (except as authorized by law), protected veteran status, genetic information, and any other characteristic protected by federal, state or local law. Applicants with a disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to [HIDDEN TEXT]","Bim, Revit, Computer Aided Design, Ms Office Tools, Navisworks, 3D modeling tools"
Senior Solution Architect - Data Governance Experience-Collibra,Cortex Consultants LLC,7-9 Years,,"Chennai, India",Login to check your skill match score,"Solution Architect
Exp-15+
Location-Multiple
Immediate to 15days
Skill-data gov,colibra,architect
Overview: The Senior Solution Architect - Data Governance Experience-Collibra plays a critical role in shaping our organization's data governance framework. This position requires an individual who has extensive experience with Collibra and a strong understanding of data governance principles. The architect will be pivotal in designing and implementing data governance solutions that enhance data quality, compliance, and usability across the enterprise. By leading initiatives that foster a culture of data stewardship, this role ensures that our data assets are effectively managed and leveraged to drive business outcomes. The Senior Solution Architect will collaborate closely with various stakeholders, including IT, compliance, and business units, to ensure alignment with the organization's strategic objectives. This position not only focuses on technical capabilities but also emphasizes the importance of change management and user adoption to ensure long-term success in our data governance journey.
Lead the architecture and implementation of Collibra to support data governance initiatives.
Develop a comprehensive data governance framework that aligns with organizational goals.
Engage with stakeholders to gather requirements and translate them into actionable solutions.
Ensure compliance with data regulations and industry standards during implementation.
Design and maintain data models that enhance data discoverability and usability.
Collaborate with data stewards and business units to promote a culture of data stewardship.
Conduct assessments of existing data management practices and recommend improvements.
Provide training and support for users to maximize the effectiveness of Collibra.
Facilitate workshops and meetings to drive consensus around data governance practices.
Monitor and report on data governance metrics to measure the success of initiatives.
Stay current with industry trends and advancements in data governance technology.
Support change management efforts to drive user adoption of data governance solutions.
Develop and implement data stewardship policies and procedures.
Work closely with IT to ensure seamless integration of Collibra with existing systems.
Serve as a subject matter expert for data governance best practices within the organization.
Required Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field.
At least 7 years of experience in data governance, data management, or a related area.
Proven experience as a Solution Architect, with a focus on data governance solutions.
Extensive hands-on experience with Collibra, including implementation and configuration.
Strong understanding of data governance principles, frameworks, and best practices.
Experience working with data models, metadata management, and data quality assessments.
Knowledge of regulatory compliance requirements such as GDPR, CCPA, or HIPAA.
Ability to engage and communicate effectively with stakeholders at all levels.
Strong analytical and problem-solving skills to address data-related challenges.
Proficient in project management methodologies, with experience leading cross-functional teams.
Experience in change management processes to facilitate user adoption.
Strong presentation skills with the ability to convey complex concepts to non-technical audiences.
Certification in data governance or relevant frameworks is a plus.
Experience with cloud-based data platforms and integration tools.
Ability to work in a fast-paced, dynamic environment with competing priorities.
Skills: regulatory compliance,project management,data stewardship,stakeholder engagement,data governance,metadata management,data quality assessments,data management,change management,collibra,data models,analytical thinking,solution architecture","data quality assessments, data models, project management, data stewardship, Regulatory Compliance, Data Management, Metadata Management, Collibra, Data Governance, change management"
Technical Architect - Data Analytics - Vice President,State Street,14-16 Years,,"Bengaluru, India",Login to check your skill match score,"Role Description
We are looking for an experienced Data Lead to drive our data strategy, architecture, and analytics initiatives. You will be responsible for managing data infrastructure, ensuring data quality, and enabling data-driven decision-making for Digital platform & products. This role requires a strong technical background, leadership skills, and a passion for leveraging data to create business impact.
Data Strategy & Governance
Define and implement data management strategies to align with business objectives.
Establish data governance policies and ensure compliance with industry standards (GDPR, CCPA, etc.).
Drive best practices for data security, privacy, and integrity.
Data Engineering & Architecture
Design and oversee scalable data pipelines, ETL processes, and data warehousing solutions.
Work with engineering teams to optimize database performance and storage.
Ensure the reliability and efficiency of data infrastructure, both on-premise and in the cloud.
Data Analytics & Insights
Lead data analytics initiatives to provide insights that drive business decisions.
Collaborate with cross-functional teams to develop dashboards and reports.
Promote data-driven decision-making culture across the organization.
Leadership & Collaboration
Manage and mentor a team of data engineers, analysts, and scientists.
Partner with product, marketing, finance, and operations teams to address data challenges.
Stay updated on the latest trends and technologies in data management and analytics.
Core/Must Have Skills
Bachelor's or Master's degree in Computer Science, Information Technology, Data Science, or a related field.
14+ years of experience in data engineering, analytics, or a similar role.
Strong expertise in SQL, Python, and data pipeline tools (Airflow, dbt, etc.).
Working experience on data technologies like Snowflake, DataBricks, Airflow, Kafka, Apache Airflow, Teradata, Hadoop, spark, Amazon redshift, etc
Knowledge of data modeling, data governance, and data integration techniques.
Strong problem-solving skills and ability to work in a fast-paced environment
Excellent leadership, communication, and stakeholder management skills.
Good To Have Skills
Experience with BI tools (Tableau, Power BI, Looker) and ML frameworks is a plus.
Work Schedule
On-Premise
Keywords (If any)
Why this role is important to us
Our technology function, Global Technology Services (GTS), is vital to State Street and is the key enabler for our business to deliver data and insights to our clients. We're driving the company's digital transformation and expanding business capabilities using industry best practices and advanced technologies such as cloud, artificial intelligence and robotics process automation.
We offer a collaborative environment where technology skills and innovation are valued in a global organization. We're looking for top technical talent to join our team and deliver creative technology solutions that help us become an end-to-end, next-generation financial services company.
Join us if you want to grow your technical skills, solve real problems and make your mark on our industry.
About State Street
What we do. State Street is one of the largest custodian banks, asset managers and asset intelligence companies in the world. From technology to product innovation, we're making our mark on the financial services industry. For more than two centuries, we've been helping our clients safeguard and steward the investments of millions of people. We provide investment servicing, data & analytics, investment research & trading and investment management to institutional clients.
Work, Live and Grow. We make all efforts to create a great work environment. Our benefits packages are competitive and comprehensive. Details vary by location, but you may expect generous medical care, insurance and savings plans, among other perks. You'll have access to flexible Work Programs to help you match your needs. And our wealth of development programs and educational support will help you reach your full potential.
Inclusion, Diversity and Social Responsibility. We truly believe our employees diverse backgrounds, experiences and perspectives are a powerful contributor to creating an inclusive environment where everyone can thrive and reach their maximum potential while adding value to both our organization and our clients. We warmly welcome candidates of diverse origin, background, ability, age, sexual orientation, gender identity and personality. Another fundamental value at State Street is active engagement with our communities around the world, both as a partner and a leader. You will have tools to help balance your professional and personal life, paid volunteer days, matching gift programs and access to employee networks that help you stay connected to what matters to you.
State Street is an equal opportunity and affirmative action employer.
Discover more at StateStreet.com/careers
State Street's Speak Up Line
Job ID: R-772025","Airflow, data integration techniques, Looker, Teradata, snowflake, dbt, Hadoop, Power Bi, Kafka, Tableau, Data Modeling, Sql, DataBricks, Amazon Redshift, Spark, Data Governance, Python"
Technical Architect - Data Analytics - Vice President,State Street Corporation,14-16 Years,,"Bengaluru, India",Banking/Accounting/Financial Services,"Role Description
We are looking for an experiencedData Leadto drive our data strategy, architecture, and analytics initiatives. You will be responsible for managing data infrastructure, ensuring data quality, and enabling data-driven decision-making for Digital platform & products. This role requires a strong technical background, leadership skills, and a passion for leveraging data to create business impact.
Data Strategy & Governance
Define and implement data management strategies to align with business objectives.
Establish data governance policies and ensure compliance with industry standards (GDPR, CCPA, etc.).
Drive best practices for data security, privacy, and integrity.
Data Engineering & Architecture
Design and oversee scalable data pipelines, ETL processes, and data warehousing solutions.
Work with engineering teams to optimize database performance and storage.
Ensure the reliability and efficiency of data infrastructure, both on-premise and in the cloud.
Data Analytics & Insights
Lead data analytics initiatives to provide insights that drive business decisions.
Collaborate with cross-functional teams to develop dashboards and reports.
Promote data-driven decision-making culture across the organization.
Leadership & Collaboration
Manage and mentor a team of data engineers, analysts, and scientists.
Partner with product, marketing, finance, and operations teams to address data challenges.
Stay updated on the latest trends and technologies in data management and analytics.
Core/Must have skills
Bachelor's or Master's degree in Computer Science, Information Technology, Data Science, or a related field.
14+ years of experience in data engineering, analytics, or a similar role.
Strong expertise in SQL, Python, and data pipeline tools (Airflow, dbt, etc.).
Working experience on data technologies like Snowflake, DataBricks, Airflow, Kafka, Apache Airflow, Teradata, Hadoop, spark, Amazon redshift, etc
Knowledge of data modeling, data governance, and data integration techniques.
Strong problem-solving skills and ability to work in a fast-paced environment
Excellent leadership, communication, and stakeholder management skills.
Good to have skills
Experience with BI tools (Tableau, Power BI, Looker) and ML frameworks is a plus.
Work Schedule
On-Premise
Keywords (If any)
Why this role is important to us
Our technology function, Global Technology Services (GTS), is vital to State Street and is the key enabler for our business to deliver data and insights to our clients. We're driving the company's digital transformation and expanding business capabilities using industry best practices and advanced technologies such as cloud, artificial intelligence and robotics process automation.
We offer a collaborative environment where technology skills and innovation are valued in a global organization. We're looking for top technical talent to join our team and deliver creative technology solutions that help us become an end-to-end, next-generation financial services company.
Join us if you want to grow your technical skills, solve real problems and make your mark on our industry.
About State Street
What we do. State Street is one of the largest custodian banks, asset managers and asset intelligence companies in the world. From technology to product innovation, we're making our mark on the financial services industry. For more than two centuries, we've been helping our clients safeguard and steward the investments of millions of people. We provide investment servicing, data & analytics, investment research & trading and investment management to institutional clients.
Work, Live and Grow. We make all efforts to create a great work environment. Our benefits packages are competitive and comprehensive. Details vary by location, but you may expect generous medical care, insurance and savings plans, among other perks. You'll have access to flexible Work Programs to help you match your needs. And our wealth of development programs and educational support will help you reach your full potential.
Inclusion, Diversity and Social Responsibility. We truly believe our employees diverse backgrounds, experiences and perspectives are a powerful contributor to creating an inclusive environment where everyone can thrive and reach their maximum potential while adding value to both our organization and our clients. We warmly welcome candidates of diverse origin, background, ability, age, sexual orientation, gender identity and personality. Another fundamental value at State Street is active engagement with our communities around the world, both as a partner and a leader. You will have tools to help balance your professional and personal life, paid volunteer days, matching gift programs and access to employee networks that help you stay connected to what matters to you.
State Street is an equal opportunity and affirmative action employer.
Discover more at StateStreet.com/careers","snowflake, Airflow, data integration techniques, dbt, Looker, Teradata, Spark, Sql, Apache Airflow, Data Modeling, Data Governance, Hadoop, Tableau, Kafka, Power Bi, Python, Bi Tools, Amazon Redshift, DataBricks"
IT Architect (Data Engineer),Medtronic,4-6 Years,,"Pune, India",Login to check your skill match score,"At Medtronic you can begin a life-long career of exploration and innovation, while helping champion healthcare access and equity for all. You'll lead with purpose, breaking down barriers to innovation in a more connected, compassionate world.
A Day in the Life
We're a mission-driven leader in medical technology and solutions with a legacy of integrity and innovation, join our new Minimed India Hub as Digital Engineer. We are working to improve how healthcare addresses the needs of more people, in more ways and in more places around the world. As a PySpark Data Engineer, you will be responsible for designing, developing, and maintaining data pipelines using PySpark. You will work closely with data scientists, analysts, and other stakeholders to ensure the efficient processing and analysis of large datasets, while handling complex transformations and aggregations.
Responsibilities may include the following and other duties may be assigned:
Design, develop, and maintain scalable and efficient ETL pipelines using PySpark.
Work with structured and unstructured data from various sources.
Optimize and tune PySpark applications for performance and scalability.
Collaborate with data scientists and analysts to understand data requirements, review Business Requirement documents and deliver high-quality datasets.
Implement data quality checks and ensure data integrity.
Monitor and troubleshoot data pipeline issues and ensure timely resolution.
Document technical specifications and maintain comprehensive documentation for data pipelines.
Stay up to date with the latest trends and technologies in big data and distributed computing.
Required Knowledge and Experience:
Bachelor's degree in computer science, Engineering, or a related field.
4-5 years of experience in data engineering, with a focus on PySpark.
Proficiency in Python and Spark, with strong coding and debugging skills.
Strong knowledge of SQL and experience with relational databases (e.g., PostgreSQL, MySQL, SQL Server).
Hands-on experience with cloud platforms such as AWS, Azure, or Google Cloud Platform (GCP).
Experience with data warehousing solutions like Redshift, Snowflake, Databricks or Google BigQuery.
Familiarity with data lake architectures and data storage solutions.
Experience with big data technologies such as Hadoop, Hive, and Kafka.
Excellent problem-solving skills and the ability to troubleshoot complex issues.
Strong communication and collaboration skills, with the ability to work effectively in a team environment.
Preferred Skills:
Experience with Databricks.
Experience with orchestration tools like Apache Airflow or AWS Step Functions.
Knowledge of machine learning workflows and experience working with data scientists.
Understanding of data security and governance best practices.
Familiarity with streaming data platforms and real-time data processing.
Knowledge of CI/CD pipelines and version control systems (e.g., Git).
Physical Job Requirements
The above statements are intended to describe the general nature and level of work being performed by employees assigned to this position, but they are not an exhaustive list of all the required responsibilities and skills of this position.
Benefits & Compensation
Medtronic offers a competitive Salary and flexible Benefits Package
A commitment to our employees lives at the core of our values. We recognize their contributions. They share in the success they help to create. We offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage.
This position is eligible for a short-term incentive called the Medtronic Incentive Plan (MIP).
About Medtronic
We lead global healthcare technology and boldly attack the most challenging health problems facing humanity by searching out and finding solutions.
Our Mission to alleviate pain, restore health, and extend life unites a global team of 95,000+ passionate people.
We are engineers at heart putting ambitious ideas to work to generate real solutions for real people. From the R&D lab, to the factory floor, to the conference room, every one of us experiments, creates, builds, improves and solves. We have the talent, diverse perspectives, and guts to engineer the extraordinary.
Learn more about our business, mission, and our commitment to diversity here","snowflake, Google BigQuery, AWS Step Functions, Git, Databricks, Sql, SQL Server, Apache Airflow, Hadoop, Pyspark, Kafka, AWS, MySQL, Hive, Python, Azure, PostgreSQL, Spark, Google Cloud Platform, Redshift"
Solutions Architect -Data/AI,Gruve,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Job Title: Solutions Architect- AI
Location: India- Pune/Bangalore
About the Company:
Gruve is an innovative software services startup dedicated to transforming enterprises to AI powerhouses. We specialize in cybersecurity, customer experience, cloud infrastructure, and advanced technologies such as Large Language Models (LLMs). Our mission is to assist our customers in their business strategies utilizing their data to make more intelligent decisions. As a well-funded early-stage startup, Gruve offers a dynamic environment with strong customer and partner networks.
Why Gruve:
At Gruve, we foster a culture of innovation, collaboration, and continuous learning. We are committed to building a diverse and inclusive workplace where everyone can thrive and contribute their best work. If you're passionate about technology and eager to make an impact, we'd love to hear from you.
Gruve is an equal opportunity employer. We welcome applicants from all backgrounds and thank all who apply; however, only those selected for an interview will be contacted.
Position summary:
We are seeking an experienced AI Solutions Architect to join our AI team. In this role, you will manage relationships with enterprise customers, gather and translate business requirements into AI solutions, and lead the design and implementation of LLM products. You will be the technical thought leader, guiding customers from prototype to production. This is an exciting opportunity with hands-on experience in building and deploying LLM-powered solutions.
Key Roles & Responsibilities:
Engage directly with enterprise customers to gather business and technical requirements in data and AI.
Design and implement scalable AI solutions leveraging LLMs.
Partner with cross-functional teams, including business stakeholders, data engineers, and software engineers, to deploy AI solutions.
Build hands-on prototypes and guide customers from proof-of-concept to production, ensuring timely delivery and business impact.
Develop best practices for AI solution delivery, including security and compliance measures for safe AI implementations.
Drive customer success by providing ongoing technical support and thought leadership in AI post-deployment.
Basic Qualifications:
8+ years of hands-on experience in software architecture.
Experience working as a Data Scientist or Analytics Consultant.
Expert in cloud platforms such as AWS or Azure.
Experience integrating AI solutions with enterprise systems.
Excellent communication skills and can engage with both technical teams and executive stakeholders effectively.
Can lead multiple concurrent engagements.
Have experience working with programming languages like Python.
Preferred Qualifications
Master's degree in computer science, Data Science, or a related field.
Experience with cloud-native architectures, CI/CD pipelines, and containerization (e.g., Docker, Kubernetes).
Relevant certifications in AI/ML technologies or cloud platforms (e.g., AWS, Azure, Google Cloud).
Experience with enterprise platforms such as Salesforce.","AWS, Kubernetes, Python, Azure, Docker"
Senior Solution Architect - Data Governance Experience-Collibra,Cortex Consultants LLC,7-9 Years,,"Pune, India",Login to check your skill match score,"Solution Architect
Exp-15+
Location-Multiple
Immediate to 15days
Skill-data gov,colibra,architect
Overview: The Senior Solution Architect - Data Governance Experience-Collibra plays a critical role in shaping our organization's data governance framework. This position requires an individual who has extensive experience with Collibra and a strong understanding of data governance principles. The architect will be pivotal in designing and implementing data governance solutions that enhance data quality, compliance, and usability across the enterprise. By leading initiatives that foster a culture of data stewardship, this role ensures that our data assets are effectively managed and leveraged to drive business outcomes. The Senior Solution Architect will collaborate closely with various stakeholders, including IT, compliance, and business units, to ensure alignment with the organization's strategic objectives. This position not only focuses on technical capabilities but also emphasizes the importance of change management and user adoption to ensure long-term success in our data governance journey.
Lead the architecture and implementation of Collibra to support data governance initiatives.
Develop a comprehensive data governance framework that aligns with organizational goals.
Engage with stakeholders to gather requirements and translate them into actionable solutions.
Ensure compliance with data regulations and industry standards during implementation.
Design and maintain data models that enhance data discoverability and usability.
Collaborate with data stewards and business units to promote a culture of data stewardship.
Conduct assessments of existing data management practices and recommend improvements.
Provide training and support for users to maximize the effectiveness of Collibra.
Facilitate workshops and meetings to drive consensus around data governance practices.
Monitor and report on data governance metrics to measure the success of initiatives.
Stay current with industry trends and advancements in data governance technology.
Support change management efforts to drive user adoption of data governance solutions.
Develop and implement data stewardship policies and procedures.
Work closely with IT to ensure seamless integration of Collibra with existing systems.
Serve as a subject matter expert for data governance best practices within the organization.
Required Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field.
At least 7 years of experience in data governance, data management, or a related area.
Proven experience as a Solution Architect, with a focus on data governance solutions.
Extensive hands-on experience with Collibra, including implementation and configuration.
Strong understanding of data governance principles, frameworks, and best practices.
Experience working with data models, metadata management, and data quality assessments.
Knowledge of regulatory compliance requirements such as GDPR, CCPA, or HIPAA.
Ability to engage and communicate effectively with stakeholders at all levels.
Strong analytical and problem-solving skills to address data-related challenges.
Proficient in project management methodologies, with experience leading cross-functional teams.
Experience in change management processes to facilitate user adoption.
Strong presentation skills with the ability to convey complex concepts to non-technical audiences.
Certification in data governance or relevant frameworks is a plus.
Experience with cloud-based data platforms and integration tools.
Ability to work in a fast-paced, dynamic environment with competing priorities.
Skills: regulatory compliance,project management,data stewardship,stakeholder engagement,data governance,metadata management,data quality assessments,data management,change management,collibra,data models,analytical thinking,solution architecture","data quality assessments, data models, project management, data stewardship, Regulatory Compliance, Data Management, Metadata Management, Collibra, Data Governance, change management"
Data & BI Solution Architect (Remote),Codvo.ai,10-12 Years,,"Pune, India",Login to check your skill match score,"Company Overview:
At Codvo, software and people transformations go together. We are a global empathy-led technology services company with a core DNA of product innovation and mature software engineering. We uphold the values of Respect, Fairness, Growth, Agility, and Inclusiveness in everything we do.
Job Description:
We are looking for a Data & BI Solution Architect to lead data analytics initiatives in the retail domain. The candidate should be skilled in data modeling, ETL, visualization, and big data technologies.
Key Responsibilities
Design and implement scalable data & BI architectures for retail analytics use cases.
Lead the development of data pipelines, ETL/ELT processes, and data integration across cloud platforms.
Define and enforce data governance, security, and compliance frameworks.
Collaborate with business stakeholders to build insightful dashboards and reports.
Ensure high-performance data delivery using modern big data and cloud-native technologies.
Required Skills & Experience
10+ years of hands-on experience in data engineering, business intelligence, and cloud analytics.
Strong proficiency in SQL, Python, and Apache Spark.
Proven experience with ETL tools like Informatica, Talend, or AWS Glue.
Expertise in BI tools such as Power BI, Tableau, and Looker.
Solid understanding and experience with cloud data platforms Snowflake, Amazon Redshift, or Google BigQuery.
Experience in retail or e-commerce analytics is a strong plus.
Exp-10+ Years Shift Timing- 2:30 PM -11:30 PM IST Location- remote","snowflake, Google BigQuery, Looker, Power Bi, Amazon Redshift, Apache Spark, Tableau, Python, Sql"
Sr Architect - Data Center,Bechtel Corporation,12-15 Years,,India,Login to check your skill match score,"Requisition ID: 282944
Relocation Authorized: National - Family
Telework Type: Full-Time Office/Project
Work Location: Various Bechtel Project Locations
Extraordinary Teams Building Inspiring Projects
Since 1898, we have helped customers complete more than 25,000 projects in 160 countries on all seven continents that have created jobs, grown economies, improved the resiliency of the world's infrastructure, increased access to energy, resources, and vital services, and made the world a safer, cleaner place.
Differentiated by the quality of our people and our relentless drive to deliver the most successful outcomes, we align our capabilities to our customers objectives to create a lasting positive impact. We serve the Infrastructure; Nuclear, Security & Environmental; Energy; Mining & Metals, and the Manufacturing and Technology markets. Our services span from initial planning and investment, through start-up and operations.
Core to Bechtel is our Vision, Values and Commitments . They are what we believe, what customers can expect, and how we deliver. Learn more about our extraordinary teams building inspiring projects in our Impact Report .
Bechtel India is a global operation that supports execution of projects and services around the world. Working seamlessly with business line home offices, project sites, customer organizations and suppliers, our teams have delivered more than 125 projects since our inception in 1994.
Our offices in Gurgaon, Vadodara and Chennai will grow significantly and sustainably with exciting career opportunities for both professionals and young graduates who are passionate about creating a cleaner, greener, and safer world; building transformational infrastructure; making decarbonization a reality; and protecting people and the environment.
Job Summary
Architect with more than 12 years of experience in Manufacturing and Infrastructure projects (with relevant experience in Data center) projects in a design office of repute. A candidate who has completed at least 1 data center project shall be given more weightage. Should have exposure to design office practices and familiarity with computer aided design and 3D modeling tools like Revit.
Major Responsibilities
Shall perform / supervise calculations, prepare material requisitions, service requisitions, code analysis, standard details, technical specifications and quantity take-offs and provide input for design drawings during all phases of projects.
Shall check / review the drawings prepared by the designers.
Shall assist in the development of basic layout drawings.
Shall lead conceptual studies and inter-disciplinary reviews
Shall create perspectives and presentation of design to client.
Provide technical training.
Perform feasibility studies for site development, building configuration, climate studies.
Education And Experience Requirements
Minimum 5-year degree in Architecture from an accredited college or university.
Preference will be given to the candidate with Master's degree.
Professional license from a recognized licensing board and/or LEED certification shall be of added advantage.
Experience of making Architectural presentation shall be an added advantage.
Level I: 12 -15 years of relevant work experience
Level II: 16 - 20 years of relevant work experience
Required Knowledge And Skills
Knowledge of architectural techniques and design principles with a basic knowledge of the types of data center, data hall layout, mechanical systems, electrical systems, and security systems needed to design a data center.
Knowledge of precedents and latest trends in architectural engineering, the principles and practices of related technical areas.
Knowledge of Engineering Procedures and design guides.
Thorough knowledge of the roles played by other engineering disciplines on projects.
Knowledge of regulatory Indian and International codes and standards, industry practices and design criteria pertinent to architectural engineering design, including fire life safety codes.
Knowledge of constructability and applicable standards and codes
Proficiency in the use of Revit, Navisworks and exposure to BIM is essential.
Proficient in selection of material from constructability and total installed cost perspective.
Skill in oral and written communication.
Should be proficient in using MS office tools.
Proved ability of managing a team of architects and designers will be an added advantage.
Previous experience of Data center design including administration building, process/ non process buildings and other ancillary facilities like maintenance building, guard house, substation, electrical buildings, pump house, etc.
Knowledge of master planning and fire life safety design guidelines
Good knowledge of faade design and material selection (including interior & exterior finishes)
Experience in working on EPC projects shall be an added advantage.
Total Rewards/Benefits
For decades, Bechtel has worked to inspire the next generation of employees and beyond! Because our teams face some of the world's toughest challenges, we offer robust benefits to ensure our people thrive. Whether it is advancing careers, delivering programs to enhance our culture, or providing time to recharge, Bechtel has the benefits to build a legacy of sustainable growth. Learn more at Bechtel Total Rewards
Diverse Teams Build The Extraordinary
As a global company, Bechtel has long been home to a vibrant multitude of nationalities, cultures, ethnicities, and life experiences. This diversity has made us a more trusted partner, more effective problem solvers and innovators, and a more attractive destination for leading talent.
We are committed to being a company where every colleague feels that they belong-where colleagues feel part of One Team, respected and rewarded for what they bring, supported in pursuing their goals, invested in our values and purpose, and treated equitably. Click here to learn more about the people who power our legacy.
Bechtel is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity and expression, age, national origin, disability, citizenship status (except as authorized by law), protected veteran status, genetic information, and any other characteristic protected by federal, state or local law. Applicants with a disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to [HIDDEN TEXT]","Ms Office Tools, Revit, Computer Aided Design, Bim, Navisworks, 3D modeling tools"
"Senior Manager, Data and Analytics Architect",MSD,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Senior Specialist, Data and Analytics Architect
THE OPPORTUNITY
Based in Hyderabad, join a global healthcare biopharma company and be part of a 130-year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.
Lead an Organization driven by digital technology and data-backed approaches that supports a diversified portfolio of prescription medicines, vaccines, and animal health products.
Drive innovation and execution excellence. Be the leaders who have a passion for using data, analytics, and insights to drive decision-making, which will allow us to tackle some of the world's greatest health threats.
Our Technology centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. AN integral part of our company IT operating model, Tech centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.
A focused group of leaders in each tech center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.
Role Overview
We are seeking a talented and motivated Technical Architect to join our Data and Analytics Strategy & Architecture team. Reporting to the Lead Architect, this mid-level Technical Architect role is critical in shaping the technical foundation of our cross-product architecture. The ideal candidate will focus on reference architecture, driving proofs of concept (POCs) and points of view (POVs), staying updated on industry trends, solving technical architecture issues, and enabling a robust data observability framework. The role will also emphasize enterprise data marketplaces and data catalogs to ensure data accessibility, governance, and usability.
This position will also focus on creating a customer-centric development environment that is resilient and easily adoptable by various user personas. The outcome of the cross-product integration will be improved efficiency and productivity through accelerated provisioning times and a seamless user experience, eliminating the need for interacting with multiple platforms and teams.
What Will You Do In The Role
Collaborate with product line teams to design and implement cohesive architecture solutions that enable cross-product integration, spanning ingestion, governance, analytics, and visualization.
Develop, maintain, and advocate for reusable reference architectures that align with organizational goals and industry standards.
Lead technical POCs and POVs to evaluate new technologies, tools, and methodologies, providing actionable recommendations.
Diagnose and resolve complex technical architecture issues, ensuring stability, scalability, and performance across platforms.
Implement and maintain frameworks to monitor data quality, lineage, and reliability across data pipelines.
Contribute to the design and implementation of an enterprise data marketplace to facilitate self-service data discovery, analytics, and consumption.
Oversee and extend the use of Collibra or similar tools to enhance metadata management, data governance, and cataloging across the enterprise.
Monitor emerging industry trends in data and analytics (e.g., AI/ML, data engineering, cloud platforms) and identify opportunities to incorporate them into our ecosystem.
Work closely with data engineers, data scientists, and other architects to ensure alignment with the enterprise architecture strategy.
Create and maintain technical documentation, including architecture diagrams, decision records, and POC/POV results.
What Should You Have
Strong experience with Databricks, Dataiku, Starburst and related data engineering/analytics platforms.
Proficiency in AWS cloud platforms and AWS Data and Analytics technologies
Knowledge of modern data architecture patterns like data Lakehouse, data mesh, or data fabric.
Hands-on experience with Collibra or similar data catalog tools for metadata management and governance.
Familiarity with data observability tools and frameworks to monitor data quality and reliability.
Experience contributing to or implementing enterprise data marketplaces, including facilitating self-service data access and analytics.
Exposure to designing and implementing scalable, distributed architectures.
Proven experience in diagnosing and resolving technical issues in complex systems.
Passion for exploring and implementing innovative tools and technologies in data and analytics.
35 years of total experience in data engineering, analytics, or architecture roles.
Hands-on experience with developing ETL pipelines with DBT, Matillion and Airflow.
Experience with data modeling, and data visualization tools (e.g., ThoughtSpot, Power BI).
Strong communication and collaboration skills.
Ability to work in a fast-paced, cross-functional environment.
Focus on continuous learning and professional growth.
Preferred Skills
Certification in Databricks, Dataiku, or a major cloud platform.
Experience with orchestration tools like Airflow or Prefect.
Understanding of AI/ML workflows and platforms.
Exposure to frameworks like Apache Spark or Kubernetes.
Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation
Who We Are
We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.
What We Look For
Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are intellectually curious, join usand start making your impact today.
Current Employees apply HERE
Current Contingent Workers apply HERE
Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.
Employee Status
Regular
Relocation
VISA Sponsorship
Travel Requirements
Flexible Work Arrangements
Hybrid
Shift
Valid Driving License
Hazardous Material(s)
Job Posting End Date
06/2/2025
A job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.
Requisition ID R345601","Dataiku, Starburst, AWS cloud platforms, Data observability tools, AWS Data and Analytics technologies, Collibra, Data Modeling, Databricks"
"Senior Manager, Data and Analytics Architect",MSD,5-8 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
R3
Senior Manager Data and Analytics Architect
The Opportunity
Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.
Be part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.
Drive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the world's greatest health threats.
Our Technology Centre's focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company's IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.
A focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.
Role Overview
We are seeking a highly motivated and hands-on Data & Analytics Architect to join our Strategy & Architecture team within CDNA. This mid-level role will play a critical part in designing scalable, reusable, and secure data and analytics solutions across the enterprise. You will work under the guidance of a senior architect and be directly involved in the implementation of architectural patterns, reference solutions, and technical best practices.
This is a highly technical role, ideal for someone who enjoys problem-solving, building frameworks, and working in a fast-paced, collaborative environment.
What Will You Do In This Role
Partner with senior architects to define and implement modern data architecture patterns and reusable frameworks.
Design and develop reference implementations for ingestion, transformation, governance, and analytics using tools such as Databricks (must-have), Informatica, AWS Glue, S3, Redshift, and DBT.
Contribute to the development of a consistent and governed semantic layer, ensuring alignment in business logic, definitions, and metrics across the enterprise.
Work closely with product line teams to ensure architectural compliance, scalability, and interoperability.
Build and optimize batch and real-time data pipelines, applying best practices in data modeling, transformation, and metadata management.
Contribute to architecture governance processes, participate in design reviews, and document architectural decisions.
Support mentoring of junior engineers and help foster a strong technical culture within the India-based team.
What Should You Have
Bachelor's degree in information technology, Computer Science or any Technology stream.
58 years of experience in data architecture, data engineering, or analytics solution delivery.
Proven hands-on experience with Databricks (must), Informatica, AWS data ecosystem (S3, Glue, Redshift, etc.), and DBT.
Solid understanding of semantic layer design, including canonical data models and standardized metric logic for enterprise reporting and analytics.
Proficient in SQL, Python, or Scala.
Strong grasp of data modeling techniques (relational, dimensional, NoSQL), ETL/ELT design, and streaming data frameworks.
Knowledge of data governance, data security, lineage, and compliance best practices.
Strong collaboration and communication skills across global and distributed teams.
Experience with Dataiku or similar data science/analytics platforms is a plus.
Exposure to AI/ML and GenAI use cases is advantageous.
Background in pharmaceutical, healthcare, or life sciences industries is preferred.
Familiarity with API design, data services, and event-driven architecture is beneficial.
Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.
Who We Are
We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.
What We Look For
Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join usand start making your impact today.
#HYDIT2025
Current Employees apply HERE
Current Contingent Workers apply HERE
Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.
Employee Status
Regular
Relocation
VISA Sponsorship
Travel Requirements
Flexible Work Arrangements
Hybrid
Shift
Valid Driving License
Hazardous Material(s)
Job Posting End Date
05/7/2025
A job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.
Requisition ID R341138","Dataiku, dbt, S3, Scala, AWS Glue, Databricks, Informatica, Redshift, Sql, Python"
"Senior Manager, Data and Analytics Architect",MSD,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Senior Specialist, Data and Analytics Architect
THE OPPORTUNITY
Based in Hyderabad, join a global healthcare biopharma company and be part of a 130-year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.
Lead an Organization driven by digital technology and data-backed approaches that supports a diversified portfolio of prescription medicines, vaccines, and animal health products.
Drive innovation and execution excellence. Be the leaders who have a passion for using data, analytics, and insights to drive decision-making, which will allow us to tackle some of the world's greatest health threats.
Our Technology centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. AN integral part of the our company IT operating model, Tech centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.
A focused group of leaders in each tech center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.
Role Overview
We are seeking a talented and motivated Technical Architect to join our Data and Analytics Strategy & Architecture team. Reporting to the Lead Architect, this mid-level Technical Architect role is critical in shaping the technical foundation of our cross-product architecture. The ideal candidate will focus on reference architecture, driving proofs of concept (POCs) and points of view (POVs), staying updated on industry trends, solving technical architecture issues, and enabling a robust data observability framework. The role will also emphasize enterprise data marketplaces and data catalogs to ensure data accessibility, governance, and usability.
This position will also focus on creating a customer-centric development environment that is resilient and easily adoptable by various user personas. The outcome of the cross-product integration will be improved efficiency and productivity through accelerated provisioning times and a seamless user experience, eliminating the need for interacting with multiple platforms and teams.
What Will You Do In The Role
Collaborate with product line teams to design and implement cohesive architecture solutions that enable cross-product integration, spanning ingestion, governance, analytics, and visualization.
Develop, maintain, and advocate for reusable reference architectures that align with organizational goals and industry standards.
Lead technical POCs and POVs to evaluate new technologies, tools, and methodologies, providing actionable recommendations.
Diagnose and resolve complex technical architecture issues, ensuring stability, scalability, and performance across platforms.
Implement and maintain frameworks to monitor data quality, lineage, and reliability across data pipelines.
Contribute to the design and implementation of an enterprise data marketplace to facilitate self-service data discovery, analytics, and consumption.
Oversee and extend the use of Collibra or similar tools to enhance metadata management, data governance, and cataloging across the enterprise.
Monitor emerging industry trends in data and analytics (e.g., AI/ML, data engineering, cloud platforms) and identify opportunities to incorporate them into our ecosystem.
Work closely with data engineers, data scientists, and other architects to ensure alignment with the enterprise architecture strategy.
Create and maintain technical documentation, including architecture diagrams, decision records, and POC/POV results.
What Should You Have
Strong experience with Databricks, Dataiku, Starburst and related data engineering/analytics platforms.
Proficiency in AWS cloud platforms and AWS Data and Analytics technologies
Knowledge of modern data architecture patterns like data Lakehouse, data mesh, or data fabric.
Hands-on experience with Collibra or similar data catalog tools for metadata management and governance.
Familiarity with data observability tools and frameworks to monitor data quality and reliability.
Experience contributing to or implementing enterprise data marketplaces, including facilitating self-service data access and analytics.
Exposure to designing and implementing scalable, distributed architectures.
Proven experience in diagnosing and resolving technical issues in complex systems.
Passion for exploring and implementing innovative tools and technologies in data and analytics.
35 years of total experience in data engineering, analytics, or architecture roles.
Hands-on experience with developing ETL pipelines with DBT, Matillion and Airflow.
Experience with data modeling, and data visualization tools (e.g., ThoughtSpot, Power BI).
Strong communication and collaboration skills.
Ability to work in a fast-paced, cross-functional environment.
Focus on continuous learning and professional growth.
Preferred Skills
Certification in Databricks, Dataiku, or a major cloud platform.
Experience with orchestration tools like Airflow or Prefect.
Understanding of AI/ML workflows and platforms.
Exposure to frameworks like Apache Spark or Kubernetes.
Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation
Who We Are
We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.
What We Look For
Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are intellectually curious, join usand start making your impact today.
Current Employees apply HERE
Current Contingent Workers apply HERE
Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.
Employee Status
Regular
Relocation
VISA Sponsorship
Travel Requirements
Flexible Work Arrangements
Hybrid
Shift
Valid Driving License
Hazardous Material(s)
Job Posting End Date
06/2/2025
A job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.
Requisition ID R345600","Dataiku, Starburst, AWS cloud platforms, Data observability tools, AWS Data and Analytics technologies, Collibra, Data Modeling, Databricks"
"Senior Manager, Data and Analytics Architect",MSD,3-5 Years,,"Hyderabad, India",Login to check your skill match score,"Job Description
Senior Specialist, Data and Analytics Architect
THE OPPORTUNITY
Based in Hyderabad, join a global healthcare biopharma company and be part of a 130-year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.
Lead an Organization driven by digital technology and data-backed approaches that supports a diversified portfolio of prescription medicines, vaccines, and animal health products.
Drive innovation and execution excellence. Be the leaders who have a passion for using data, analytics, and insights to drive decision-making, which will allow us to tackle some of the world's greatest health threats.
Our Technology centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. AN integral part of the our company IT operating model, Tech centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.
A focused group of leaders in each tech center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.
Role Overview
We are seeking a talented and motivated Technical Architect to join our Data and Analytics Strategy & Architecture team. Reporting to the Lead Architect, this mid-level Technical Architect role is critical in shaping the technical foundation of our cross-product architecture. The ideal candidate will focus on reference architecture, driving proofs of concept (POCs) and points of view (POVs), staying updated on industry trends, solving technical architecture issues, and enabling a robust data observability framework. The role will also emphasize enterprise data marketplaces and data catalogs to ensure data accessibility, governance, and usability.
This position will also focus on creating a customer-centric development environment that is resilient and easily adoptable by various user personas. The outcome of the cross-product integration will be improved efficiency and productivity through accelerated provisioning times and a seamless user experience, eliminating the need for interacting with multiple platforms and teams.
What Will You Do In The Role
Collaborate with product line teams to design and implement cohesive architecture solutions that enable cross-product integration, spanning ingestion, governance, analytics, and visualization.
Develop, maintain, and advocate for reusable reference architectures that align with organizational goals and industry standards.
Lead technical POCs and POVs to evaluate new technologies, tools, and methodologies, providing actionable recommendations.
Diagnose and resolve complex technical architecture issues, ensuring stability, scalability, and performance across platforms.
Implement and maintain frameworks to monitor data quality, lineage, and reliability across data pipelines.
Contribute to the design and implementation of an enterprise data marketplace to facilitate self-service data discovery, analytics, and consumption.
Oversee and extend the use of Collibra or similar tools to enhance metadata management, data governance, and cataloging across the enterprise.
Monitor emerging industry trends in data and analytics (e.g., AI/ML, data engineering, cloud platforms) and identify opportunities to incorporate them into our ecosystem.
Work closely with data engineers, data scientists, and other architects to ensure alignment with the enterprise architecture strategy.
Create and maintain technical documentation, including architecture diagrams, decision records, and POC/POV results.
What Should You Have
Strong experience with Databricks, Dataiku, Starburst and related data engineering/analytics platforms.
Proficiency in AWS cloud platforms and AWS Data and Analytics technologies
Knowledge of modern data architecture patterns like data Lakehouse, data mesh, or data fabric.
Hands-on experience with Collibra or similar data catalog tools for metadata management and governance.
Familiarity with data observability tools and frameworks to monitor data quality and reliability.
Experience contributing to or implementing enterprise data marketplaces, including facilitating self-service data access and analytics.
Exposure to designing and implementing scalable, distributed architectures.
Proven experience in diagnosing and resolving technical issues in complex systems.
Passion for exploring and implementing innovative tools and technologies in data and analytics.
35 years of total experience in data engineering, analytics, or architecture roles.
Hands-on experience with developing ETL pipelines with DBT, Matillion and Airflow.
Experience with data modeling, and data visualization tools (e.g., ThoughtSpot, Power BI).
Strong communication and collaboration skills.
Ability to work in a fast-paced, cross-functional environment.
Focus on continuous learning and professional growth.
Preferred Skills
Certification in Databricks, Dataiku, or a major cloud platform.
Experience with orchestration tools like Airflow or Prefect.
Understanding of AI/ML workflows and platforms.
Exposure to frameworks like Apache Spark or Kubernetes.
Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation
Who We Are
We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.
What We Look For
Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are intellectually curious, join usand start making your impact today.
Current Employees apply HERE
Current Contingent Workers apply HERE
Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.
Employee Status
Regular
Relocation
VISA Sponsorship
Travel Requirements
Flexible Work Arrangements
Hybrid
Shift
Valid Driving License
Hazardous Material(s)
Job Posting End Date
06/2/2025
A job posting is effective until 11 59 59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.
Requisition ID R345600","Dataiku, Starburst, AWS cloud platforms, Data observability tools, AWS Data and Analytics technologies, Collibra, Data Modeling, Databricks"
MDM Architect- Data Governance,Fractal,Fresher,,"Bengaluru, India",Login to check your skill match score,"Responsibilities
You will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management
Lead the end-to-end design, architecture, and implementation of MDM solutions
Define and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.
Collaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.
Design data models and hierarchies for Customer, Product, Vendor, and other master domains.
Develop and operationalize Data Governance frameworks aligned to business and compliance needs.
Enable data stewardship workflows, match & merge rules, and exception management.
Integrate MDM systems with upstream and downstream applications across the enterprise.
Lead workshops and training sessions on MDM and Data Governance for client teams.
Support RFPs, proposals, and client presentations with MDM/DG expertise
Assess, validate and implement MDM architecture for on-prem, cloud and hybrid environments with expertise in MDM solutions like Oracle MDM solutions (ERP, CDH, CDM), SAP MDG(S4/Hana, ERP), Informatica IDMC and other modern MDM solutions.
Develop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.
Collaborate with stakeholders to define and implement MDM strategies, standards, and operating models.
Develop and enforce end-to-end master data lifecycle processes including data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.
Collaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.
Support leadership in designing thought leadership, publish POV's/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.
Good to Have
Hands-on implementation experience with on-prem MDM and ERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG, SAP S4/HANA, Reltio, Stibo or hybrid deployments with other modern MDM, ERP and CRM platforms.
Strong knowledge of reference data management and data stewardship workflows.
Experience with ETL/ELT tools and integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).
Familiarity with data governance tools (e.g., Collibra, Alation, Informatica CDGC, etc.) and DQ tools (e.g., Informatica IDQ, CDQ, etc.).
Basic understanding of data governance best practices, data quality management, data privacy regulations
Familiarity with Agentic AI, machine learning, and analytics technologies
Knowledge of SQL, PL/SQl, Python, or any other database
Excellent communication and interpersonal skills to collaborate effectively with clients and internal teams.","S4 Hana ERP, Informatica CDI, Oracle MDM solutions, Informatica IDMC, Metadata Management, Pl Sql, Oracle Goldengate, Sql, Data Quality, Sap Mdg, Mulesoft, Python"
"Principal Solution Architect - Data Management, ETL, Data Integration (Presales)",Qlik,8-10 Years,,"Noida, India",Login to check your skill match score,"Description
What makes us Qlik
A Gartner Magic Quadrant Leader for 15 years in a row, Qlik transforms complex data landscapes into actionable insights, driving strategic business outcomes. Serving over 40,000 global customers, our portfolio leverages pervasive data quality and advanced AI/ML capabilities that lead to better decisions, faster.
We excel in integration and governance solutions that work with diverse data sources, and our real-time analytics uncover hidden patterns, empowering teams to address complex challenges and seize new opportunities.
The Senior Solution Architect Role
Are you ready to take the lead in shaping the future of data and analytics As a Senior Solution Architect, you'll be the go-to technical expert, guiding some of the largest customers and partners in the India region. You'll be at the forefront of demonstrating how cutting-edge data integration and analytics solutions can drive real business transformation. Collaborating closely with a dynamic Presales team in a flexible, agile environment, you'll have the opportunity to showcase your expertise while working with Sales, Marketing, R&D, Product, Consulting, and Customer Success teams. If you're looking for a role that is engaging, fast-paced, and full of opportunities to make an impact, this is it.
What makes this role interesting
Engage with high-profile customers and partners: Lead technical discussions and showcase innovative solutions to help organizations unlock the true power of their data.
Drive business success with cutting-edge technology: Leverage Qlik's next-generation data analytics and data integration platform to solve complex business challenges.
Be at the forefront of industry trends: Stay ahead of the game by keeping up with the latest advancements in data analytics, as well as the competitive landscape.
Collaborate with cross-functional teams: Work closely with internal teams and experts across Sales, Marketing, R&D, and Customer Success to build compelling solutions that resonate with customers.
Flexibility and agility: Thrive in an environment that values adaptability, innovation, and dynamic thinking.
Here's How You'll Be Making An Impact
Own the technical sales cycle: Become a trusted advisor by guiding customers through technical evaluations, ensuring a seamless journey from exploration to adoption.
Showcase innovation through tailored solutions: Deliver compelling presentations and custom demonstrations that address real customer needs and business challenges.
Prove value through successful Proof-of-Concepts: Help customers experience the true power of Qlik's platform by leading impactful proof-of-concept engagements.
Support business development efforts: Play a key role in driving regional revenue growth by supporting strategic sales initiatives and expanding Qlik's presence in the market.
Position solutions for long-term success: Communicate effectively with stakeholders at all levels, from technical teams to senior leadership, ensuring alignment on the value and impact of Qlik's solutions.
We're Looking For a Teammate With
At least 8 years of experience in a presales and/or consulting capacity
Strong Knowledge of Data Integration (ETL), Data Quality (DQ), Data Governance, iPaaS (APIs, micro services, Application Integration) and Big Data
Good to have working knowledge in Talend ETL tools
Excellent communication skills to the business as well as technical audience
Highly driven with strong interpersonal skills
Track record of developing relationships at technical, commercial, and executive levels throughout large enterprises
Ability to work independently and manage multiple complex opportunities.
Good experience in BI & analytics
Familiarity with cloud solutions like SaaS, PaaS and iPaaS
Good web development background (SQL, RDBMS, Java, HTML5, NET) is a plus.
Good understanding of Machine Learning tools and its usage such as Python/R
Travel Requirements
Willingness and ability to travel approximately 25 - 50%
Ability to travel internationally, if required
The location for this role is:
India Delhi
If you're passionate about helping businesses harness the full potential of their data and want to be part of a team that values expertise, innovation, and collaboration, this is your opportunity to make a real difference. Apply today!
More About Qlik And Who We Are
Find out more about life at Qlik on social: Instagram, LinkedIn, YouTube, and X/Twitter, and to see all other opportunities to join us and our values, check out our Careers Page.
What else do we offer
Genuine career progression pathways and mentoring programs
Culture of innovation, technology, collaboration, and openness
Flexible, diverse, and international work environment
Giving back is a huge part of our culture. Alongside an extra change the world day plus another for personal development, we also highly encourage participation in our Corporate Responsibility Employee Programs
If you need assistance applying for a role due to a disability, please submit your request via email to accessibilityta @ qlik.com. Any information you provide will be treated according to Qlik's Recruitment Privacy Notice. Qlik may only respond to emails related to accommodation requests.
Qlik is not accepting unsolicited assistance from search firms for this employment opportunity. Please, no phone calls or emails. All resumes submitted by search firms to any employee at Qlik via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Qlik. No fee will be paid in the event the candidate is hired by Qlik as a result of the referral or through other means.","Talend ETL tools, R, Html5, Sql, ipaas, Java, Application Integration, RDBMS, Machine Learning, Etl, Data Integration, Data Quality, Python, Data Governance, Apis, Big Data, Web Development"
"Manager, Data Science, Solution Architect",Cadbury,10-12 Years,,"Mumbai, India",Login to check your skill match score,"Job Description
Are You Ready to Make It Happen at Mondelz International
Join our Mission to Lead the Future of Snacking. Make It Uniquely Yours.
You work with business and IT stakeholders to support a future-state vision in terms of requirements, principles and models in a specific technology, process or function.
How you will contribute
You will work closely with the enterprise architecture team to chart technology roadmaps, standards, best practices and guiding principles, providing your subject matter expertise and technical capabilities to oversee specific applications in architecture forums and when participating in workshops for business function requirements. In collaboration with the enterprise architecture and solution teams, you will help evaluate specific applications with a goal to bring in new capabilities based on the strategic roadmap. You will also deliver seamless integration with the existing ecosystem and support project teams in implementing these new technologies, offer input regarding decisions on key technology purchases to align IT investments with business strategies while reducing risk and participate in technology product evaluation processes and Architecture Review Board governance for project solutions.
What you will bring
A desire to drive your future and accelerate your career. You will bring experience and knowledge in:
Defining and driving successful solution architecture strategy and standards
Components of holistic enterprise architecture
Teamwork, facilitation and negotiation
Prioritizing and introducing new data sources and tools to drive digital innovation
New information technologies and their application
Problem solving, analysis and communication
Governance, security, application life-cycle management and data privacy
Purpose of Role
The Solution Architect will provide end-to-end solution architecture guidance for data science initiatives. A successful candidate will be able to handle multiple projects at a time and drive the right technical architecture decisions for specific business problems. The candidate will also execute PoC/PoVs for emerging AI/ML technologies, support the strategic roadmap and define reusable patterns from which to govern project designs.
Main Responsibilities: -
Determine which technical architectures are appropriate for which models and solutions.
Recommend what technologies and associated configurations are best to solve business problems.
Define and document data science architectural patterns.
Ensure project compliance with architecture guidelines and processes.
Provide guidance and support to development teams during the implementation process.
Develop and implement processes to execute AI/ML workloads.
Configure and optimize the AI/ML systems for performance and scalability.
Stay up to date on the latest AI/ML features and best practices.
Integrating SAP BW with SAP S/4HANA and other data sources.
Review and sign off on high-level architecture designs.
Career Experiences Required & Role Implications
Bachelor's degree in computer science or related field of study.
10+ years of experience in a global company in data-related roles (5+ years in data science).
Strong proficiency in Databricks and analytical application frameworks (Dash, Shiny, React).
Experience with data engineering using common frameworks (Python, Spark, distributed SQL, NoSQL).
Experience leading complex solution designs in a multi-cloud environment.
Experience with a variety of analytics techniques: statistics, machine learning, optimization, and simulation.
Experience with software engineering practices and tools (design, testing, source code management, CI/CD).
Deep understanding of algorithms and tools for developing efficient data processing systems.
Within Country Relocation support available and for candidates voluntarily moving internationally some minimal support is offered through our Volunteer International Transfer Policy
Business Unit Summary
At Mondelz International, our purpose is to empower people to snack right by offering the right snack, for the right moment, made the right way. That means delivering a broad range of delicious, high-quality snacks that nourish life's moments, made with sustainable ingredients and packaging that consumers can feel good about.
We have a rich portfolio of strong brands globally and locally including many household names such as , and biscuits , and chocolate candy and gum. We are proud to hold the top position globally in biscuits, chocolate and candy and the second top position in gum.
Our 80,000 makers and bakers are located in more than80countries and we sell our products in over150countries around the world. Our people are energized for growth and critical to us living our purpose and values. We are a diverse community that can make things happen-and happen fast.
Mondelz International is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation or preference, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
Job Type
Regular
Technology Architecture
Technology & Digital","distributed SQL, AI ML technologies, Design, analytics techniques, Dash, CI CD, Statistics, Optimization, Shiny, source code management, Simulation, Databricks, Testing, React, Machine Learning, software engineering practices, Nosql, Python, Spark"
Data & Technical Architect,BNP Paribas India Solutions Private Limited,10-15 Years,,Mumbai,Banking,"Position Purpose
The CE&P AML IT team is in charge of AML Monitoring tools for CIB and all regions. AML Monitoring tools are mainly used by Financial Security Compliance and CIB ITO LoD1.
The purpose of the role is to seek a highly skilled and experienced AML Functional, Technical, and Data Architect with expertise in Actimize models and rules. The candidate will have a strong background in developing and optimizing database models on the AML Data Lake architecture, specifically focusing on Actimize. They will be responsible for designing, implementing, and maintaining data architecture solutions that effectively support our AML and compliance activities, with a specific emphasis on Actimize models and rules. Working closely with stakeholders, including AML analysts, data scientists, and IT teams, the candidate will ensure that the data architecture solutions align with business requirements and adhere to relevant regulations and policies, while also optimizing Actimize models and rules for enhanced detection and prevention of financial crimes. Key responsibilities will include analyzing system requirements, devising data migration strategies, and ensuring the efficient and secure storage of company information, with a focus on Actimize models and rules.
Responsibilities
Direct Responsibilities
Collaborate with stakeholders to understand AML functional requirements and translate them into Actimize solution design and architecture and Data requirements and translate them into data architecture solutions that support AML and compliance activities.Design and implement Technical and data architecture solutions on the AML Products andData Lake architecture, ensuring scalability, performance, and data integrity.Able to work independently with the Program Manager to understand business requirements and translate them to technical solutions in the applicationConfigure Actimize modules and components to meet specific AML use cases and workflows.Integrate Actimize with other systems and data sources to ensure seamless data flow and information exchange.Develop and optimize database models to effectively store and retrieve AML-related data, considering data volume, complexity, and reporting needs.Establish and enforce data quality and data governance processes to ensure the accuracy, completeness, and consistency of AML data.Implement data security and access control processes to protect sensitive AML information and ensure compliance with security standards and privacy regulations.Evaluate and propose the integration of new technologies and innovative solutions to enhance AML data management processes, such as advanced analytics, machine learning, or automation.
Contributing Responsibilities
Technical & Behavioral Competencies
Minimum of 10 years of experience as a Functional, Technical, and Data Architect, with a strong focus on AML and compliance, demonstrating a deep understanding of industry best practices and regulatory requirements.Extensive expertise in Actimize technical and functional architecture, with a proven track record of successfully implementing Actimize models and rules that align with specific line of business needs.Able to work independently with the Program Manager to understand business requirements and translate them to technical solutions in the application Demonstrated proficiency in developing and optimizing Actimize functional models and rules, as well as designing and optimizing database models on the AML Data Lake architecture.Strong experience in Data-Warehouse architectural design, providing efficient and effective solutions in the Compliance AML data domains.In-depth knowledge of AML and compliance regulations and policies, ensuring compliance with industry standards and legal requirements.Exceptional analytical and problem-solving skills, with the ability to identify and address complex issues related to AML and compliance architecture.Excellent communication and interpersonal skills, enabling effective collaboration with stakeholders at all levels of the organization.Ability to work both independently and as part of a team, demonstrating strong teamwork and collaboration skills.Strong project management skills, with the ability to effectively plan, prioritize, and execute projects within defined timelines and budgets.Good experience in technical analysis of n-tier applications with multiple integrations using object oriented, APIs & Microservices approaches.Very good understanding of principle behind various DevSecOps practices and working experience of industry standard toolsExperience with Agile methodology is a plus, showcasing adaptability and flexibility in project delivery.Good knowledge on front-end technologies preferably Angular.Knowledge on Software methodology practice Agile Methodology & SCRUM practices
Business Skills
IT / Business relation (Expert)
Compliance Financial Security (Proficient)
IT Skills: database
Transversal Skills
Ability to manage a project (Expert)
Analytical ability (expert)
Ability to understand, explain and support change (Expert)
Behaviors Skills
Ability to Deliver/Results driven(Expert)
Ability to collaborate (Expert)
Adaptability (Expert)
Personal Impact/Ability to influence (Proficient)
ooResilience (Proficient)
Specific Qualifications(if required)
Skills Referential
BehaviouralSkills:(Please select up to 4 skills)
Ability to deliver / Results driven
Adaptability
Personal Impact / Ability to influence
Resilience
Transversal Skills:(Please select up to 5 skills)
Ability to manage a project
Analytical Ability
Ability to understand, explain and support change
Ability to set up relevant performance indicators
Choose an item.
Education Level:
Bachelor Degree or equivalent
Experience Level
At least 10 years
Other/Specific Qualifications(if required)",Data Architect
Data Solution Architect,Purview India Consulting And Services Llp,10-20 Years,INR 20 - 30 LPA,"Hyderabad, Bengaluru","Information Technology, Information Services","In this role, you will:
Architect scalable and secure data solutions as required.
Take on general solution architecture tasks and deliverables as part of the wider Solution Architecture team where required.
Develop logical and physical data models to meet business requirements, contributing to conceptual models where required.
Collaborate with business analysts and application teams to gather and understand data requirements.
Create clear and effective data models, solutions and architectures, ensuring alignment with the overall data architecture and project goals.
Ensure that the data architecture and models support real-time, near-real-time and batch data processing where needed.
Ensure smooth data integration and flow across various platforms and systems, including integration of data sources into data refineries for analysis and reporting.
Provide training and support to other teams in understanding, using and implementing data models and architecture.
Ensure data models maintain high standards of data integrity, accuracy, and consistency.
Develop and implement best practices for data modelling, including naming conventions, documentation, and version control.
Create and maintain comprehensive documentation, including Solution Design Documents, entity-relationship diagrams and data dictionaries.
Ensure that architectures and models are reviewed and approved at the appropriate forums.
Requirements
To be successful in this role, you should meet the following requirements:
Demonstrable experience in solution architecture, data modelling and data architecture.
Demonstrable experience preparing and presenting architecture governance artefacts to design boards.
Experience providing feedback and technical knowledge to facilitate peer review of architectures.
Using architecture patterns to accelerate decisions and design.
Proficiency in data modelling tools such as ER Studio, ERWIN or Visual Paradigm.
Strong understanding of database systems, including relational (SQL) and non-relational (NoSQL) databases.
Familiarity with ETL processes and data integration tools.
Experience with cloud-based (GCP, AWS, Azure), on-prem and hybrid data architectures and solutions.
Strong analytical and problem-solving skills.
Excellent communication and documentation abilities.
Ability to collaborate with technical and non-technical teams.
In addition to the above, the following background experience would be highly beneficial:
Consultancy / customer facing experience within a software vendor or enterprise scale services environment.
Financial Services domain knowledge with practical experience of mobilizing technology to meet the market dynamics effecting the financial services domain (Risk / Cost / Customer).
A track record of successful refactoring / tech-debt remediation projects.
Exposure or experience of big data and big data technologies.
High Level & Holistic Capabilities sought:
Experience in digital and broader architectural transformation, applying solutions from experience, decomposing monolithic solutions and applying methods to manage technical debt.
Experience working with enterprise architectures, translating those into solution and data architectures.
Adherence to architecture governance practices and procedures.
Experience, knowledge and understanding of enterprise level software development.
Ability to participate in technical design and oversee all technology related issues of cloud native product development, promoting API reuse and managing down technical debt across the enterprise.","Data Modelling, Data Architecture, cloud, ELT"
Data Platform Architect & Data Engineer + AWS,GlobalLogic Inc,15-18 Years,,Pune,Software,"Skills Requirements: Experience 14+
Job responsibilities
AWS solution architect with experience in handling large databases( SQL, NoSQL Mongo, Redis), data warehouses, data lakes.
Work with teams to understand application requirements and provide guidance on data modeling and database design solutioning .
Architect multi-database solutions, integrating different databases and data processing systems.
Strong Communication, Problem solving skills and ability to work with client directly
Understand and align with company-wide data architecture strategies and best practices
Ensure data security, integrity, and compliance with Penske Standards ( work with the Data Engineering and Data Services teams)
Implement and maintain DB clusters, including replica sets and sharded clusters, ensuring data distribution and fault tolerance
work on database capacity planning and performance as per our application requirements
Conduct regular architecture reviews if needed
Develop, validate, and maintain disaster recovery and high availability strategies for the application SLAs
Translate technical concepts and patterns into business benefits for (Penske) management
Advise team on architectures, patterns, and strategies for making the best use of MongoDB
Proactively seek opportunities to support and mentor Penske team members and share standard methodologies.
Exposure to Designing High-Performance DB design with Highly Scalable, Available, and RPO/RTO Needs
Experience in Sharding/Partition Strategy etc.
Exposure to design and review the DB collections etc
What we offer
Culture of caring.At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you'll experience an inclusive culture of acceptance and belonging, where you'll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders.
Learning and development.We are committed to your continuous learning and development. You'll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally.
Interesting & meaningful work.GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you'll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what's possible and bring new solutions to market. In the process, you'll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today.
Balance and flexibility.We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way!
High-trust organization.We are a high-trust organization where integrity is key.By joining GlobalLogic, you're placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do.","Data Analyst, data engineering, AWS"
Data Visualization Architect  Power BI,Ifintalent Global Private Limited,5-6 Years,,Hyderabad,Information Services,"Job Summary:
We're looking for a talented and passionate Data Visualization Lead Engineer, who is responsible for
setup, maintenance and troubleshooting of Reporting Infrastructure built using Power BI Web
Services, Power BI Desktop and Mobile application. We are looking for a senior resource with close
to 5 6 years of hands-on experience in Power BI Administration who can exhibit exemplary prowess
in delivering end to end Reporting Solution and can take the accountability of running and
maintaining reporting operations. Also, we are focusing on as much automation possible to work with
a lean team, focusing more on quality than quantity. Applicant must have outstanding communication
skill as he/she would be needed to attend a lot of meeting with stakeholders and run regular stand ups.
Essential Responsibilities
Should exhibit expert skills in Power BI Administration
Azure Analysis Service Setup
Expertise with Power BI Desktop
Troubleshooting connectivity
Expertise in Power Automate and Power Query
Should be able to integrate Power BI with IM Tools like Microsoft Teams, Slack Channels
Extensive experience in Power BI DAX
Expertise in setting up and troubleshooting Data Gateways, On Premise and Cloud
Connectors
Extensive experience in Power BI integration to Snowflake and SQL Server
Extensive experience in migrating of reports from Tableau Server to Power BI Web services
Experience in migrating reports from Qlick Sense to Power BI
Should have had experience in managing User Licenses, Capacity Licenses etc.
Experience with Semantic Modelling Tools like AtScale etc. a big plus
Should be able to work with Client Side IT Teams to troubleshoot connectivity and
authentication issues.
Fast learner
Excellent communication skills and ability to articulate via verbal and written
communications
Advance working knowledge of Microsoft Excel
Educational Qualification/Experience
Bachelor's/Master's degree in Engineering/Computer Science/Mathematics/Operational
research/Statistics.
At least 5 6 years of experience of having Power BI Infrastructure
Expertise in delivering end to end reporting solutions",Power Bi
Data Engineer Architect,ACL Digital,10-12 Years,,"Pune, India",Login to check your skill match score,"Position: Data Engineer Architect/Data Architect
Minimum of 10 years of experience in data engineering or a related field, with a proven track record of leading large-scale data projects.
Extensive experience in designing and implementing data architectures, data pipelines, and ETL processes.
Prior experience in a leadership or management role within a data engineering team.
Technical Skills:
Expertise in data engineering tools and technologies such as SQL, Python, Spark, Hadoop, and cloud platforms (e.g., AWS, Azure, GCP).
Strong knowledge of data modeling, data warehousing, and data integration techniques.
Experience with big data technologies and frameworks is highly desirable.
Leadership & Communication:
Excellent leadership, mentoring, and team-building skills.
Strong strategic thinking and problem-solving abilities.
Exceptional communication and interpersonal skills, with the ability to interact effectively with senior leadership and other stakeholders.
Education:
Bachelor's degree in Computer Science, Engineering, Data Science, or a related field. A Master's degree or relevant certifications is preferred.
Why Join Us:
Competitive salary and benefits package.
Opportunity to lead a transformative data engineering practice within a dynamic and innovative organization.
Collaborative work environment with a focus on professional growth and development.
Access to cutting-edge technologies and resources.
Key Responsibilities:
Strategic Leadership:
Develop and implement a strategic vision for the data engineering practice aligned with the company's goals and objectives.
Drive the adoption of best practices in data engineering and ensure alignment with industry trends and emerging technologies.
Collaborate with senior leadership and other departments to integrate data engineering solutions into broader business strategies.
Team Management:
Lead, mentor, and grow a high-performing team of data engineers, fostering a culture of innovation, collaboration, and continuous improvement.
Oversee the recruitment, development, and performance management of data engineering talent.
Data Architecture & Engineering:
Design and implement scalable data architectures, ensuring data integration, quality, and security.
Oversee the development and optimization of data pipelines, ETL processes, and data storage solutions.
Ensure the effective use of data engineering tools, technologies, and frameworks.
Project Management:
Manage and prioritize multiple data engineering projects, ensuring timely and successful delivery.
Coordinate with project managers, data scientists, and other stakeholders to align project goals and deliverables.
Governance & Compliance:
Establish and enforce data governance policies and practices to ensure data integrity, security, and compliance with relevant regulations.
Monitor data quality metrics and implement strategies to address data issues.
Innovation & Continuous Improvement:
Stay abreast of industry trends, emerging technologies, and best practices in data engineering.
Drive continuous improvement initiatives to enhance data engineering processes and methodologies.","data integration techniques, Spark, Sql, AWS, Big Data Technologies, Data Modeling, Data Warehousing, Python, Azure, Gcp, Hadoop"
Big Data / Cloud Architect (Jan2025),Atgeir Solutions,10-12 Years,,"Pune, India",Login to check your skill match score,"Minimum Experience: 10 Years
We are looking for a Big Data / Cloud Architect to become part of Atgeir's Advanced Data Analytics team. The desired candidate is a Professional with proven track record of working on Big Data and Cloud Platforms.
Required Skills
Work closely with customers, understand customer requirements and render those as architectural models that will operate at large scale and high performance, and advise customers on how to run these architectural models on traditional Data Platforms (Hadoop Based) as well as Modern Data Platforms (Cloud Based).
Work alongside customers to build data management platforms using Open Source Technologies as well as Cloud Native services
Extract best-practice knowledge, reference architectures, and patterns from these engagements for sharing with Advanced Analytics Centre of Excellence (CoE) team at Atgeir.
Highly technical and analytical with 10 or more years of ETL and analytics systems development and deployment experience
Strong verbal and written communications skills are a must, as well as the ability to work effectively across internal and external organizations and virtual teams.
Ability to think understand complex business requirements and render them as prototype systems with quick turnaround time.
Implementation and tuning experience in the Big Data Ecosystem, (such as Hadoop, Spark, Presto, Hive), Database (such as Oracle, MySQL, PostgreSQL, MS SQL Server) and Data Warehouses (such as Redshift, Teradata, etc.)
Knowledge of foundation infrastructure requirements such as Networking, Storage, and Hardware Optimization with Hands-on experience with one of the clouds (GCP / Azure / AWS) and/or data cloud platforms (Databricks / Snowflake)
Proven hands-on experience with at least one programming language among (Python, Java. Go, Scala)
Willingness to work hands-on on the projects
Ability to lead and guide large teams
Architect level certification on one of the clouds will be an added advantage.","Go, Teradata, Cloud Platforms, snowflake, Java, Hadoop, Ms Sql Server, PostgreSQL, Scala, Big Data, Redshift, Hive, Gcp, Presto, MySQL, Spark, Databricks, Azure, Oracle, Python, AWS"
Master Data Management Architect,CONMED Corporation,5-7 Years,,India,Login to check your skill match score,"Master Data Management Architect
CONMED is a global medical technology company that specializes in the development and manufacturing of surgical devices and equipment. With a mission to empower healthcare professionals to deliver exceptional patient care, CONMED is dedicated to innovation, quality, and excellence in all aspects of our operations.
Role Overview:
The Master Data Management (MDM) Architect provides technical and administrative support for Master Data Management, focusing on improving data quality and aligning data governance to support data transformation. The role ensures consistent and accurate master data across the enterprise for better decision-making and operational efficiency. The ideal candidate is self-driven and adaptable, meticulously attentive, and brings relevant experience completing major data transformations.
This is a remote opportunity for people living in India.
Responsibilities:
Master Data Management
Lead MDM projects involving data cleansing, standardization, and migration to support digital transformation initiatives.
Collaborate with cross-functional teams across the organization to understand data needs, gather requirements, and ensure alignment with business transformation objectives.
Establish data quality metrics, monitor data quality issues, and implement corrective actions to maintain high data integrity throughout the transformation process.
Monitor and evaluate the effectiveness of MDM processes, identify opportunities for further optimization to support evolving business needs.
Manage on-premise application and administration of Syniti and/or Informatica MDM platform
Develop and enforce data governance policies.
Train staff on data management protocols.
Qualifications:
Bachelor's degree in Information Technology, Computer Science, Business Administration, or related field.
Experience with Syniti and/or Informatica MDM platform and SAP knowledge preferred. S4HANA.
5+ years of proven experience in managing MDM projects and implementing MDM solutions across large organizations.
Strong understanding of data governance principles, data quality best practices, and data cleansing techniques.
Strong analytical and problem-solving skills to identify data quality issues and develop solutions.
Excellent communication and stakeholder management skills to collaborate with cross-functional teams.
Self-driven and adaptable.
Preferred:
3-5 years of experience supporting ERP transformations
Key Competencies:
Technical Expertise: Experience with MDM projects, data governance principles, data quality best practices, and data cleansing techniques.
Analytical Skills: Strong problem-solving abilities to identify and address data quality issues.
Communication Skills: Excellent ability to collaborate with cross-functional teams and manage stakeholders.
Project Management: Proven experience in managing MDM projects and implementing solutions across large organizations.
Adaptability: Self-driven and adaptable to evolving business needs.
Platform Knowledge: Preferred experience with the Syniti platform and SAP.","Master Data Management, Data cleansing techniques, Data governance principles, Informatica MDM platform, Data quality best practices"
Lead Data Engineer- Java/Big Data/SQL/Architect-14+yrs,Visa,12-15 Years,,"Bengaluru, India",Login to check your skill match score,"Company Description
Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose to uplift everyone, everywhere by being the best way to pay and be paid.
Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.
Job Description
The Universal Data Catalog (UDC) / Visa Data Catalog (VDC) is a comprehensive metadata management platform designed to provide a centralized repository for all data-related information across the organization. The platform enables efficient data discovery, governance, and utilization by offering detailed metadata definitions for tables, columns, and other data assets. It supports various business units by improving data accessibility, enhancing data quality, and facilitating compliance with data governance policies. Our implementation leverages the open-source Datahub project, ensuring a robust, flexible, and scalable solution.
Key Responsibilities
Develop and maintain the Enterprise Data Catalog / Visa Data Catalog platform, ensuring it meets the organization's evolving needs.
Implement and manage metadata ingestion processes to ensure the catalog is up-to-date with the latest data definitions and business context.
Collaborate with data stewards, data owners, and other stakeholders to enrich metadata with business definitions, data lineage, and usage context.
Enhance the catalog's search and discovery capabilities to provide users with intuitive and efficient access to data assets.
Integrate the data catalog with other enterprise systems and tools to support data governance, data quality, and analytics initiatives.
Monitor and optimize the performance of the data catalog to ensure scalability and reliability.
Provide training and support to users on how to effectively leverage the data catalog for their data-related needs.
Actively collaborate with the open-source community to contribute to and leverage the latest advancements in the Datahub project.
Analyze industry best practices and keep the catalog functionality up-to-date with feature sets provided by the market, while focusing on Visa's scalability requirements.
Champion the adoption of open infrastructure solutions that are fit for purpose while keeping technology relevant.
Spend 70% of the time writing code in different languages, frameworks, and technology stacks.
This is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.
Qualifications
Basic Qualification
12 to 15 yrs+ of experience in architecture design and development of large-
scale data management platforms and data application with simple solutions
Must have extensive hands-on coding and designing skills on
Java/Python for backend
MVC (model-view-controller) for end-to-end development
SQL/NoSQL technology. Familiar with Databases like Oracle, DB2, SQL Server, etc.
Web Services (REST/ SOAP/gRPC)
React/Angular for front-end (UI front-end nice to have)
Expertise in design and management of complex data structures and data
processes
Expertise in efficiently leveraging the power of distributed big data systems,
including but not limited to Hadoop Hive, Spark, Kafka streaming, etc.
Deep knowledge and hands on experience on big data and cloud computing
technologies.
Strong service architecture and development experience with high
performance and scalability
Technical background in data with deep understanding of issues in multiple
areas such as data acquisition, ingestion and processing, data management,
distributed processing, and high availability is required.
Strong on driving for results and self-motivated, strong learning mindset, with
good understanding of related advanced/new technology. Keep up with the
technology development in the related areas in the industry, which could be
leveraged to enhance current architectures and build durable new ones.
Bachelor's degree in Computer Science or related technical discipline required.
Advanced degree is a plus.
Strong leadership and team player.
Strong skills on mentoring/growing junior people
Payment industry experience is a plus.
Preferred Qualification
Experience with ETL / ELT tools / applications
Experience with Apache NiFi and Apache Spark for processing large data sets
Experience on Elastic Search
Knowledge on Data Catalog tools
Experience in building Data Pipeline development tools
Experience with Data Governance and Data Quality tools
Additional Information
Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.","GRPC, Data Catalog tools, Data Quality tools, Kafka, ELT, Angular, Apache Nifi, Nosql, React, Oracle, Python, Java, Hadoop, SQL Server, Soap, Sql, REST, Hive, DB2, Spark, Elastic Search, Mvc, Web Services, Etl"
Data & AI Architect,ennVee India,12-15 Years,,"Chennai, India",Login to check your skill match score,"Back to Career Portal
Data & AI Architect
12 - 15 years of experience
Chennai
We're seeking an experienced Data & AI Architect to lead the design and implementation of large-scale data architectures on cloud platforms. The ideal candidate will have a strong background in data warehousing, big data, data analytics, machine learning, AI, and cloud data engineering.
Key Responsibilities
Collaborate with stakeholders to understand business requirements and develop solutions.
Design and implement scalable data architectures on cloud platforms.
Develop and maintain data models, data warehousing concepts, and data governance.
Implement data ingestion, pipeline, preparation, and orchestration solutions.
Apply machine learning algorithms and data analytics techniques.
Design and implement AI/ML models using popular frameworks.
Ensure data security, scalability, maintainability, and reliability.
Develop and maintain technical documentation.
Requirements
12-15 + years of experience in enterprise data architecture and implementation.
Strong expertise in cloud architecture.
Experience with AI/ML frameworks and libraries.
Experience with big data, data warehousing, and data analytics technologies.
Proficient in Python, Java, or similar programming languages.
Experience with data visualization tools (Power BI, Tableau, etc.,).
Strong understanding of data governance and security.
Knowledge of NoSQL databases (Cosmos DB, MongoDB, Cassandra, etc.,).
Strong understanding of data governance and security.","data visualization tools, NoSQL databases, AI ML frameworks and libraries, MongoDB, Cosmos DB, Java, Data Governance, Cloud Architecture, Tableau, Power Bi, Data Analytics, Cassandra, Data Warehousing, Python, Data Security"
Data Lead/Architect,DevOn,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"DevOn we are a leading provider of innovative technology solutions with a focus on data-driven decision-making, cloud computing, and advanced analytics. Our dynamic team is passionate about solving complex business problems through innovative technology and were looking for a skilled and motivated Data Engineer Lead to join us.
Role Overview:
As a Data Engineer Lead, you will be responsible for leading the design, development, and maintenance of data pipelines and ETL workflows, leveraging modern cloud technologies. You will work closely with cross-functional teams to ensure data availability, reliability, and scalability, enabling data-driven decision-making across the organization. This role requires a deep understanding of Python, PySpark, AWS Glue, RedShift, SQL, Jenkins, Bitbucket, EKS, and Airflow.
Key Responsibilities:
Lead the design and implementation of scalable data pipelines and ETL workflows in a cloud environment (primarily AWS).
Build and manage data ingestion, transformation, and storage frameworks using AWS Glue, PySpark, and RedShift.
Architect and optimize complex SQL queries for large datasets and ensure data integrity across systems.
Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and deliver high-quality data solutions.
Automate the end-to-end data pipeline process using Jenkins and Bitbucket, ensuring efficient CI/CD practices.
Optimize and manage data orchestration using Apache Airflow.
Provide technical leadership and mentorship to junior team members, ensuring best practices for data engineering are followed.
Work with AWS services such as RedShift, S3, Lambda, and EKS for deployment and management of data solutions.
Troubleshoot and resolve complex data pipeline issues, ensuring minimal downtime and high availability.
Participate in architecture and design reviews, providing input on technical solutions and improvements.
Continuously evaluate new tools and technologies to improve the efficiency and scalability of our data infrastructure.
Required Skills and Qualifications:
5+ years of professional experience in Data Engineering, with a proven track record of building scalable data pipelines and ETL workflows.
Strong expertise in Python for data processing and scripting.
Hands-on experience with PySpark for large-scale data processing.
In-depth knowledge of AWS Glue, RedShift, S3, and other AWS services.
Advanced proficiency in SQL for data manipulation and optimization.
Experience with Jenkins and Bitbucket for CI/CD automation.
Familiarity with EKS (Elastic Kubernetes Service) for containerized deployment of data applications.
Experience with Apache Airflow for data orchestration and workflow automation.
Strong problem-solving skills and the ability to debug complex issues in data workflows.
Excellent communication skills, with the ability to collaborate with cross-functional teams and explain complex technical concepts in a clear manner.
Ability to work in an Agile development environment, managing multiple priorities and delivering on tight deadlines.
Preferred Qualifications:
Experience with additional AWS services (e.g., Lambda, Redshift Spectrum, Athena).
Familiarity with Docker and container orchestration technologies like Kubernetes.
Knowledge of data modeling and data warehousing concepts.
Bachelors or Master&aposs degree in Computer Science, Engineering, or a related field.
Show more Show less","EKS, Apache Airflow, Jenkins, Bitbucket, Pyspark, AWS Glue, Redshift, Sql, Python"
Data Warehouse Architect,Anthology Inc,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Description
Data Warehouse Architect
Bangalore, India
The Opportunity:
Anthology delivers education and technology solutions so that students can reach their full potential and learning institutions thrive. Our mission is to empower educators and institutions with meaningful innovation that's simple and intelligent, inspiring student success and institutional growth.
The Power of Together is built on having a diverse and inclusive workforce. We are committed to making diversity, inclusion, and belonging a foundational part of our hiring practices and who we are as a company.
For more information about Anthology and our career opportunities, please visit www.anthology.com.
As a Data Warehouse Architect, you will act as the primary advocate of data modeling methodologies and data processing best practices to develop a clear technical vision for our Global Customer Success (GCS) data warehouse. You will be responsible for the leadership of overarching data architecture vision and coordination of all data warehouse efforts that affect data, with a focus on data moving throughout the GCS systems. This will be accomplished by mapping and documenting current data sources used throughout GCS, creating a data dictionary which captures the business meaning of the data used, and working with various teams to eliminate unnecessary steps and modify interfaces.
Primary responsibilities will include:
Documenting the flow and movement of data through Student Success including:
The frequency of movement
The source and destination of each step in the flow
Any transformation of data during the flow
Any aggregation and calculations of data in the flow
Working with business owners and application designers to identify and model integrative views and determine the quality-of-service requirements data currency, availability, response times and data volumes
Defining technical standards and guidelines for how to use architected databases, the technologies to be used for various purposes like data extraction, transformation and integration, and models of entities, objects, and processes. These guidelines will encourage the use of existing data stores where applicable as well as address security and quality
Ensuring that standards are up to date by investigating emerging technologies and new releases. This means working in conjunction with technology architects (where they exist). Participation in proof-of-concept projects and other projects that are early users of new technologies is required from time to time
Communicating the data architecture to the development community, IT operations, IT management at all levels, and business owners
Focusing on data quality by educating the business on its importance and involving and facilitating the work of business constituents on improvement programs including the assignment of data stewards which are a critical component
The Candidate:
Required skills/qualifications:
Bachelor's degree (Computer Science, Mathematics, Statistics, Industrial Engineering) or similar work experience
At least 10 years of experience in a directly related area, during which both professional and management capabilities have been clearly demonstrated
Expert in database technologies, such as SQL server database, Snowflake, etc. as well as data modeling, both logical and physical
Extensive experience working with enterprise applications like, Salesforce, Oracle Fusion and/or Peoplesoft
Proficiency in unstructured data such as YouTube, Facebook, Twitter, or LinkedIn
Expertise in establishing master data management
Mastery of multidimensional data modeling
Extremely strong analytical and problem-solving skills
Outstanding oral, written, and visual presentation skills
Substantial negotiating experience
Ability to thrive when working across internal functional areas in ambiguous situations
Fluency in written and spoken English
Preferred skills/qualifications:
Advanced degree in Applied Mathematics, Business Analytics, Statistics, Machine Learning, Computer Science, or related field
This job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required. Nothing in this job description restricts management's right to assign or reassign duties and responsibilities at any time.
Anthology is an equal employment opportunity/affirmative action employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, gender identity/expression, protected military/veteran status, or any other legally protected factor.","Salesforce, snowflake, multidimensional data modeling, master data management, Peoplesoft, Oracle Fusion, Data Modeling, unstructured data, SQL server"
Data Model Architect,Prodapt,Fresher,,"Chennai, India",Login to check your skill match score,"Overview
Prodapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.
Responsibilities
Deliverables
Design & Document - Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)
Design & Document Build Interface Speciation for Data Integration.
Activities
Data Architecture and Modeling:
Design and maintain conceptual, logical, and physical data models
Ensure scalability and adaptability of data models for future organizational needs.
Data Model P-S-R catalogs in the existing Catalogs,SOM,COM systems
CMDB Design and Management:
Architect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.
Define data governance standards and enforce data consistency across the CMDB.
Design data integrations between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).
Requirements
Good Communication skills.
Bachelors Degree.","CMDB Design, Data Architecture, Data Governance, Data Integration"
Data Warehouse Architect,Capital Numbers,5-7 Years,,"Gurugram, Gurugram, India",Login to check your skill match score,"We are seeking a highly skilled Database/Data Warehouse Architect with a strong background in Data Vault 2.0 methodology to join our team. The ideal candidate will have a minimum of 5 years of experience in database architecture, data modeling, and data integration, with proven expertise in designing and implementing enterprise-scale data warehouses.
Key Responsibilities:
Design and implement scalable and efficient Data Warehouse solutions using Data Vault 2.0 methodology.
Develop and maintain data models, including hubs, links, and satellites, ensuring data consistency and quality.
Collaborate with business stakeholders, data engineers, and analysts to define data integration and ETL frameworks.
Lead the architecture and implementation of end-to-end Data Vault 2.0 solutions.
Optimize data storage, retrieval, and performance using best practices in data architecture.
Work with Snowflake (preferred) or other cloud-based data platforms to develop and deploy data solutions.
Ensure compliance with data governance and security policies.
Required Qualifications:
5+ years of experience as a Database/Data Warehouse Architect.
Proven expertise in Data Vault 2.0 methodology and hands-on experience with at least one end-to-end Data Vault 2.0 project.
Strong understanding of data modeling, data integration, and ETL frameworks.
Experience with Snowflake or similar cloud-based data platforms (preferred).
Solid knowledge of SQL, data pipelines, and performance optimization techniques.
Excellent problem-solving skills and the ability to work in a fast-paced environment.
Strong communication and collaboration skills to work with cross-functional teams.
Preferred Qualifications:
Experience with cloud-based data solutions (AWS, Azure, or GCP).
Knowledge of Python, Scala, or other scripting languages for data processing.
Familiarity with BI tools and reporting frameworks","snowflake, reporting frameworks, ETL frameworks, data pipelines, performance optimization, Data Vault 2.0 methodology, cloud-based data solutions, Data Integration, Sql, Data Modeling, AWS, Bi Tools, Python, Azure, Gcp, Scala"
Data Modeller - Architect-10+ Years-Immediate,Cortex Consultants LLC,10-14 Years,,"Hyderabad, India",Login to check your skill match score,"Job Title: Data Modeller - Architect
Experience: 10 to 14 Years
Location: Chennai, Hyderabad, Bangalore, Pune, Delhi
Job Type: Permanent Role
Notice Period: Immediate Joiners Only
Job Description
We are looking for an experienced Data Modeller - Architect with 10 to 14 years of expertise in designing and architecting data models for large-scale enterprise systems. The ideal candidate will have in-depth experience in data architecture, data modeling, and building scalable solutions. This is a permanent role with immediate joiner requirements across multiple locations.
Key Responsibilities
Design and architect data models for complex data environments and large datasets
Develop high-level data architecture solutions and manage data integration across systems
Collaborate with stakeholders to understand business requirements and translate them into effective data models
Lead and mentor teams in the development and implementation of data modeling strategies
Optimize database performance, ensuring data integrity and efficiency
Define and enforce best practices for data management and modeling across the organization
Work closely with IT, data engineering, and analytics teams to align data architecture with business goals
Ensure the scalability, security, and performance of data solutions
Required Skills & Qualifications
10 to 14 years of experience in data modeling and architecture
Strong expertise in relational and non-relational databases, data warehousing, and cloud data platforms
Proficiency in designing data models for complex, high-volume systems
Hands-on experience with SQL, NoSQL, and big data technologies
Proven track record of leading data architecture initiatives and cross-functional teams
Experience in data integration, ETL processes, and data governance
Immediate joiners only
Skills: non-relational databases,etl,nosql,data technologies,data architecture,analytics solutions,software development,sql,cloud platforms,relational databases,modeling,cloud data platforms,etl processes,big data technologies,data integration,data governance,data modeling,data management,data,data warehousing","Relational Databases, ETL processes, non-relational databases, cloud data platforms, Data Warehousing, Data Architecture, Big Data Technologies, Data Modeling, Sql, Nosql, Data Governance, Data Integration"
Data / Solution Architect,Cynosure Corporate Solutions,Fresher,,"Chennai, India",Login to check your skill match score,"Responsibilities
Analyzing existing data sources
Expert understanding of data models and various 1 to many, many to many and other patterns, normalization and denormalization patterns and purposes
Profile data sources to reverse engineer data model and relationships between tables, identify key fields, and infer meaning of attributes
Meet with system owners to tie observations of data patterns with business processes and use cases that lead to those patterns
Conduct root cause analysis (RCA) to identify underlying issues and drive effective solutions.
Perform frequency distribution analysis to identify patterns and trends in the data.Architecting data strategy for Future State
Profile the data sources to identify anomalies, inconsistencies, and data quality issues.
Take ownership of the data, ensuring accuracy, completeness, and reliability.
Develop target data model that provides optimal long term functional opportunities
Design the right change data capture, audit strategy
Identify where reference tables are necessary
Design mapping tables / helper tables to support configurable ETL
Requirements:
General Attributes
Advanced sql skills, familiarity with a variety of DB technologies (oracle, sql server, etc)
Comfortable with AWS environment and databricks
Experience with ETL
Demonstrate curiosity and a relentless pursuit of understanding complex datasets.
Engage with various stakeholders, including clinicians, researchers, data scientists, and IT professionals, to gather requirements and ensure alignment.
Communicate effectively with stakeholders at all levels, translating technical concepts into clear and actionable insights.
Lead deep data analysis initiatives to extract meaningful insights and drive data-driven decision-making
Collaborate with cross-functional teams to develop and implement data models and solutions that meet business objectives","change data capture, Mapping tables, Databricks, Sql, AWS, Etl"
Data Model Architect,Prodapt,Fresher,,"Chennai, India",Login to check your skill match score,"Overview
Prodapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.
Responsibilities
Deliverables
Design & Document - Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)
Design & Document Build Interface Speciation for Data Integration.
Activities
Data Architecture and Modeling:
Design and maintain conceptual, logical, and physical data models
Ensure scalability and adaptability of data models for future organizational needs.
Data Model P-S-R catalogs in the existing Catalogs,SOM,COM systems
CMDB Design and Management:
Architect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.
Define data governance standards and enforce data consistency across the CMDB.
Design data integrations between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).
Requirements
Good Communication skills.
Bachelors Degree.","CMDB Design, Data Architecture, Data Governance, Data Integration"
Success Architect - Data Cloud,Salesforce,7-12 Years,,Hyderabad,Software,"Job description
At Salesforce, we are dedicated to fostering a diverse and inclusive workplace where individuals from all backgrounds are welcomed and valued. We believe that the unique perspectives and skills of diverse candidates greatly contribute to the success of our teams. As a Data Cloud Success Architect, you will play a crucial role in driving successful outcomes for our strategic customers by leveraging your technical expertise in data and analytics. You should have a keen interest in the emergence of AI and the role of Data in it s success.
Responsibilities:
Be a trusted Data Cloud subject-matter expert for the broader Success Architect organization, including how Data Cloud relates to the success of AI.
Engage with our Signature and Strategic customers to evaluate and recommend optimization strategies for technical architecture, dev/ops, performance, and solution design specific to Data Cloud.
Identify and evaluate capability gaps for standard product capability or identify creative Architect solutions through customization.
Facilitate and influence Executive stakeholders while aligning technology strategy to business value and ROI
Run playbooks aligned with our Success Architect engagement catalog, tailored to the unique needs and opportunities of Data Cloud customers.
Build strong relationships with both internal and external business partners, contributing to broader goals and growth.
Drive thought leadership through mentoring and knowledge sharing
Impact of the Role:
As a Data Cloud Success Architect, you will have a significant impact on our customers success and the growth of our organization. Your expertise and guidance will directly influence the technical architecture, performance optimization, and solution design strategies for our strategic customers, ensuring their success in leveraging Data Clouds powerful data capabilities. By driving customer satisfaction and delivering exceptional value, you will contribute to the overall growth and reputation of Salesforce as a leader in the industry.
Collaboration and Teamwork:
Collaboration is at the core of our success, and as a Data Cloud Success Architect, you will have the opportunity to work closely with diverse teams of professionals. You will collaborate with customers, colleagues, and partners to evaluate technical architecture, optimize performance, and design effective solutions. By fostering strong relationships and working collaboratively, you will contribute to the collective success of our teams and the achievement of our goals.
Basic Requirements:
Minimum 2 development/project implementation lifecycles of Salesforce Data Cloud/CDP
Minimum 8 years proven experience in enterprise consulting, including implementing enterprise software solutions in the Analytics/CDP spaces.
Demonstrated ability to analyze, design, and optimize business processes focusing on data integration architecture, with a focus on guiding customers through migration to and optimization of Data Cloud
Deep understanding of data modeling, integration architectures, and data governance best practices.
Excellent communication skills, and ability to work collaboratively with cross-functional teams from Developer to Executive
Ability to facilitate discussions and translate technical conceptssolutions into tangible business value and ROI for customers
Ability to demonstrate basic understanding and stay up-to-date with emerging data-related and AI technologies
Proactive and self-starting attitude with the ability to manage tasks independently while collaborating remotelywith customers and colleagues
Values the importance of Data Ethics and Privacy by ensuring that customer solutions adhere to relevant regulations and best practices in data security and privacy.
Preferred Requirements:
Experience in aconsulting implementationPartner for Data Cloud/CDP
Experience with large data such as Snowflake, Databricks, AWS, Google Cloud Storage/Big Query, Azure administration or architecture
Experience with Customer Data platforms such as Segment, Tealium, Adobe Experience Platform, Amperity, Treasure Data, Twilio Segment, Realtio, etc
Experience implementing Salesforce Clouds, Multi cloud scenarios - Sales, Service, Industries, Marketing or Commerce
Familiar with at least one of the Salesforce Architecture Domains: Integration, Access and Identity Management, Data Architecture, Sharing and Visibility, Declarative and APEX Development
Experience in programming languages such as Python, Java, .Net, SQL","Java, .NET, Python, Sql"
Data and Analytics Architect - L1,Wipro Limited,Fresher,,Delhi,IT/Computers - Software,"Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.
Job Description
Role Purpose
The purpose of the role is to define and develop Enterprise Data Structure along with Data Warehouse, Master Data, Integration and transaction processing with maintaining and strengthening the modelling standards and business information.
Do
1. Define and Develop Data Architecture that aids organization and clients in new/ existing deals
a. Partnering with business leadership (adopting the rationalization of the data value chain) to provide strategic, information-based recommendations to maximize the value of data and information assets, and protect the organization from disruptions while also embracing innovation
b. Assess the benefits and risks of data by using tools such as business capability models to create an data-centric view to quickly visualize what data matters most to the organization, based on the defined business strategy
c. Create data strategy and road maps for the Reference Data Architecture as required by the clients
d. Engage all the stakeholders to implement data governance models and ensure that the implementation is done based on every change request
e. Ensure that the data storage and database technologies are supported by the data management and infrastructure of the enterprise
f. Develop, communicate, support and monitor compliance with Data Modelling standards
g. Oversee and monitor all frameworks to manage data across organization
h. Provide insights for database storage and platform for ease of use and least manual work
i. Collaborate with vendors to ensure integrity, objectives and system configuration
j. Collaborate with functional & technical teams and clients to understand the implications of data architecture and maximize the value of information across the organization
k. Presenting data repository, objects, source systems along with data scenarios for the front end and back end usage
l. Define high-level data migration plans to transition the data from source to target system/ application addressing the gaps between the current and future state, typically in sync with the IT budgeting or other capital planning processes
m. Knowledge of all the Data service provider platforms and ensure end to end view.
n. Oversight all the data standards/ reference/ papers for proper governance
o. Promote, guard and guide the organization towards common semantics and the proper use of metadata
p. Collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure a common understanding, consistency, accuracy and control
q. Provide solution of RFPs received from clients and ensure overall implementation assurance
i. Develop a direction to manage the portfolio of all the databases including systems, shared infrastructure services in order to better match business outcome objectives
ii. Analyse technology environment, enterprise specifics, client requirements to set a collaboration solution for the big/small data
iii. Provide technical leadership to the implementation of custom solutions through thoughtful use of modern technology
iv. Define and understand current issues and problems and identify improvements
v. Evaluate and recommend solutions to integrate with overall technology ecosystem keeping consistency throughout
vi. Understand the root cause problem in integrating business and product units
vii. Validate the solution/ prototype from technology, cost structure and customer differentiation point of view
viii. Collaborating with sales and delivery leadership teams to identify future needs and requirements
ix. Tracks industry and application trends and relates these to planning current and future IT needs
2. Building enterprise technology environment for data architecture management
a. Develop, maintain and implement standard patterns for data layers, data stores, data hub & lake and data management processes
b. Evaluate all the implemented systems to determine their viability in terms of cost effectiveness
c. Collect all the structural and non-structural data from different places integrate all the data in one database form
d. Work through every stage of data processing: analysing, creating, physical data model designs, solutions and reports
e. Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance with industry best practices
f. Implement the best security practices across all the data bases based on the accessibility and technology
g. Strong understanding of activities within primary discipline such as Master Data Management (MDM), Metadata Management and Data Governance (DG)
h. Demonstrate strong experience in Conceptual, Logical and physical database architectures, design patterns, best practices and programming techniques around relational data modelling and data integration
3. Enable Delivery Teams by providing optimal delivery solutions/ frameworks
a. Build and maintain relationships with delivery and practice leadership teams and other key stakeholders to become a trusted advisor
b. Define database physical structure, functional capabilities, security, back-up and recovery specifications
c. Develops and establishes relevant technical, business process and overall support metrics (KPI/SLA) to drive results
d. Monitor system capabilities and performance by performing tests and configurations
e. Integrate new solutions and troubleshoot previously occurred errors
f. Manages multiple projects and accurately reports the status of all major assignments while adhering to all project management standards
g. Identify technical, process, structural risks and prepare a risk mitigation plan for all the projects
h. Ensure quality assurance of all the architecture or design decisions and provides technical mitigation support to the delivery teams
i. Recommend tools for reuse, automation for improved productivity and reduced cycle times
j. Help the support and integration team for better efficiency and client experience for ease of use by using AI methods.
k. Develops trust and builds effective working relationships through respectful, collaborative engagement across individual product teams
l. Ensures architecture principles and standards are consistently applied to all the projects
m. Ensure optimal Client Engagement
i. Support pre-sales team while presenting the entire solution design and its principles to the client
ii. Negotiate, manage and coordinate with the client teams to ensure all requirements are met
iii. Demonstrate thought leadership with strong technical capability in front of the client to win the confidence and act as a trusted advisor
Reinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",
IN-Manager_Azure and Databricks Architect_Data & Analytics_Advisory_Bangalore,PwC India,7-10 Years,,"Bengaluru, India",Login to check your skill match score,"Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data, Analytics & AI
Management Level
Manager
Job Description & Summary
At PwC, our people in business application consulting specialise in consulting services for a variety of business applications, helping clients optimise operational efficiency. These individuals analyse client needs, implement software solutions, and provide training and support for seamless integration and utilisation of business applications, enabling clients to achieve their strategic objectives.
As a business application consulting generalist at PwC, you will provide consulting services for a wide range of business applications. You will leverage a broad understanding of various software solutions to assist clients in optimising operational efficiency through analysis, implementation, training, and support.
Why PWC
At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us.
At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm's growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.
Job Description & Summary We are seeking an experienced Azure and Databricks Architect with 7 to 10 years of experience specializing in data services, data architecture, and data platforms. The ideal candidate will have a strong background in designing and implementing scalable data solutions on Azure, with a focus on Databricks.
Responsibilities
Architect and design end-to-end data solutions on Azure, with a focus on Databricks.
Lead data architecture initiatives, ensuring alignment with best practices and business objectives.
Collaborate with stakeholders to define data strategies, architectures, and roadmaps.
Migrate and transform data from Oracle to Azure Data Lake.
Ensure data solutions are secure, reliable, and scalable.
Provide technical leadership and mentorship to junior team members.
Mandatory Skill Sets
Extensive experience with Azure Data Services, including Azure Data Factory, Azure SQL Data Warehouse, and Synapse Analytics.
Deep expertise in Databricks, including Spark, Delta Lake.
Strong understanding of data architecture principles and best practices.
Proven track record of leading large-scale data projects and initiatives.
Hands-on experience with Oracle to Azure Data Lake migrations.
Excellent communication and collaboration skills.
Preferred Skill Sets
Azure Solutions Architect Expert certification Databricks Certification.
Databricks Certification.
Years Of Experience Required
7 to 10 years
Education Qualification
Graduate Engineer or Management Graduate
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required: Bachelor Degree
Degrees/Field Of Study Preferred
Certifications (if blank, certifications not specified)
Required Skills
Microsoft Azure
Optional Skills
Accepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Intellectual Curiosity, Learning Agility, Optimism, Performance Assessment + 21 more
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Not Specified
Available for Work Visa Sponsorship
No
Government Clearance Required
No
Job Posting End Date","data architecture principles, Azure SQL Data Warehouse, Synapse Analytics, Delta Lake, Azure Data Services, Oracle to Azure Data Lake migrations, Azure Data Factory, Spark, Databricks"
Data and Analytics - Architect,Schneider Electric,8-12 Years,,"Bengaluru, India",Login to check your skill match score,"Company: Schneider Electric
Department: Supply Chain Analytics
Job Role: Solution Architect
Location: Bangalore
Experience & Education: Around 8-12 years of experience
SUMMARY OF JOB:
1. Design data model or Expand existing data models with new features based on business feedback
2. Collaborate with the Global and Regional based data integration team to acquire expanded data from source systems
3. Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality
4. Acquiring data from primary or secondary data sources and maintaining databases
5. Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality
6. Work with the larger Growth Technology Systems team to help develop and uphold data governance policies and procedures to ensure standardized data naming, establish consistent data definitions and monitor overall data quality for assigned data entities
7. Filter and clean data by reviewing reports and performance indicators to locate and correct code problems
8. Analyze and improve performance on existing data models including query optimization, DAX optimization, and processing optimization (partitions).
9. Build and enhance SQL Server Analysis Services data models
10. Interpreting data, analyzing results using statistical techniques
11. Develop or assist in the development of SSRS, Excel, Tableau and Power BI reports
12. Develop required process documentation and adhere to security compliance
Qualifications
Primary Skills:
1. Good business analytical skills
2. Deep understanding of dimensional modeling, OLTP and OLAP database designs and implementation strategies
3. Data Warehousing concepts, architecture & schemas
4. Familiarity with ETL tools such as Microsoft SSIS/DTS, Data Stage, Informatica Power Center, Informatica Cloud
5. Experience in providing solutions for BI & DW environment
6. Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
7. Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS etc)
8. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
9. Good communication skills both written and verbal
10. Experience in Manufacturing sector
11. Familiarity with Agile methodologies such as Scrum
Preferred Knowledge:
1. Exposure to data visualization/BI
2. AWS
ESSENTIAL FUNCTIONS:
1. DW solutioning
2. Understanding business requirements, source the data, data model design
3. Assess Data Quality, Data Integration, Migration & Modelling
4. Handle end to end development
5. Frequent client interaction
Schedule: Full-timeReq: 00974M","OLTP and OLAP database designs, Data Warehousing Concepts, Xml, Javascript, Dax, Dimensional Modeling, Sql Server Analysis Services"
Associate Architect-Data Engineer & AI,Fission Computer Labs Private Limited,10-13 Years,,Hyderabad,Software,"Roles and Responsibilities
Associate Architect- Data Engineer is responsible for overseeing the design, development, and management of data infrastructure and pipelines within an organization. This role involves a mix of technical leadership, project management, and collaboration with other teams to ensure the efficient collection, storage, processing, and analysis of large datasets. The Lead Data Engineer typically manages a team of data engineers, architects, and analysts, ensuring that data workflows are scalable, reliable, and meet the business's requirements.
Responsibilities:
Lead the design, development, and maintenance of data pipelines and ETL processes
architect and implement scalable data solutions using Databricks and AWS.
Optimize data storage and retrieval systems using Rockset, Clickhouse, and CrateDB.
Develop and maintain data APIs using FastAPI.
Orchestrate and automate data workflows using Airflow.
Collaborate with data scientists and analysts to support their data needs.
Ensure data quality, security, and compliance across all data systems.
Mentor junior data engineers and promote best practices in data engineering.
Evaluate and implement new data technologies to improve the data infrastructure.
Participate in cross-functional projects and provide technical leadership.
Manage and optimize data storage solutions using AWS S3, implementing best practices for data lakes and data warehouses.
Implement and manage Databricks Unity Catalog for centralized data governance and access control across the organization.
Qualifications Required
Bachelor's or Master's degree in Computer Science, Engineering, or related field
10+ years of experience in data engineering, with at least 6 years in a lead role
Strong proficiency in Python, PySpark, and SQL
Extensive experience with Databricks and AWS cloud services
Hands-on experience with Airflow for workflow orchestration
Familiarity with FastAPI for building high-performance APIs
Experience with columnar databases like Rockset, Clickhouse, and CrateDB
Solid understanding of data modeling, data warehousing, and ETL processes
Experience with version control systems (e.g., Git) and CI/CD pipelines
Excellent problem-solving skills and ability to work in a fast-paced environment
Strong communication skills and ability to work effectively in cross-functional teams
Knowledge of data governance, security, and compliance best practices
Proficiency in designing and implementing data lake architectures using AWS S3
Experience with Databricks Unity Catalog or similar data governance and metadata
management tools
Skills and Experience Required
Preferred Qualifications:
Experience with real-time data processing and streaming technologies
Familiarity with machine learning workflows and MLOps
Certifications in Databricks, AWS
Experience implementing data mesh or data fabric architectures
Knowledge of data lineage and metadata management best practices
Tech Stack
Databricks, Python, PySpark, SQL, Airflow, FastAPI, AWS (S3, IAM, ECR, Lambda), Rockset, Clickhouse, CrateDB
Why you'll love working with us:
Opportunity to work on business challenges from top global clientele with high impact.
Vast opportunities for self-development, including online university access and sponsored certifications.
Sponsored Tech Talks, industry events & seminars to foster innovation and learning.
Generous benefits package including health insurance, retirement benefits, flexible work hours, and more.
Supportive work environment with forums to explore passions beyond work.
This role presents an exciting opportunity for a motivated individual to contribute to the development of cutting-edge solutions while advancing their career in a dynamic and collaborative environment.","Data Analysis, Machine Learning, Cloud Services, Databricks, Python, Pyspark, Data Pipeline"
"Senior Solutions Acceleration Architect, Data",Google Inc,10-15 Years,,"Gurugram, Bengaluru, Mumbai",Software,"Minimum qualifications:
Bachelor's degree in Computer Science, a related technical field, or equivalent practical experience.
10 years of experience in cloud computing, with a focus on data architecture, data analytics, and data engineering.
Experience with modern application development and DevOps practices, including CI/CD, containerization (Docker, Kubernetes), and infrastructure-as-code.
Experience in programming/scripting languages such as Python, with developing and deploying data solutions.
Experience in architecting and developing software or infrastructure for scalable, distributed systems.
Experience with database technologies (SQL, NoSQL), streaming data, and data warehousing solutions.
Preferred qualifications:
10 years of experience in cloud computing, with a focus on data architecture, data analytics, and data engineering, in a customer-facing or consulting role.
Experience in understanding customer requirements, breaking them down into actionable components, and designing technical architectures to meet those needs.
Experience managing stakeholder expectations and building consensus around complex technical projects.
Ability to analyze complex business problems and develop innovative technical solutions leveraging GenAI and data.
Ability to build rapid prototypes and proof-of-concepts to demonstrate innovative solutions.
Ability to communicate complex technical concepts effectively to both technical and non-technical audiences.
Responsibilities:
Lead the design, development, and iterative refinement of data-centric and AI-powered solutions on Google Cloud Platform (GCP), showcasing the potential of data and AI to address specific business needs.
Collaborate with customers to understand their business challenges, technical requirements, and objectives. Build strong, trusted relationships with customers and stakeholders.
Mentor and guide junior team members, fostering a culture of innovation and continuous learning.
Establish and promote innovative best practices and methodologies for data-driven solutions, actively contributing to industry thought leadership through publications, presentations, and community engagement.","Data Analytics, Application Development, data engineering, Data Architecture, Devops, Cloud Computing"
Solution Architect (Digital/Data practice),Reflections Info Systems,6-11 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Chennai",IT Management,"We are seeking a highly experiencedSolution Architect - Digitalto join our team. This role is pivotal in designing and implementing innovative digital solutions that meet both business objectives and customer needs.
The ideal candidate will possess a deep understanding of modern technologies, includingmicroservices architecture, mobile development (Android/iOS), data BI, backend systems, infrastructure, and DevOps.
Responsibilities include:
1. Architecture Design
Lead the design of end-to-end digital solutions that align with business objectives.
Develop and maintain architectural blueprints, design patterns, and best practices for digital transformation.
Ensure solutions are scalable, secure, and meet performance standards, with a strong focus on microservices architecture (preferably in Python).
2. Technical Leadership
Provide guidance and leadership to development teams throughout the project lifecycle.
Collaborate with business leaders, developers, and other architects to ensure alignment with business goals.
Stay updated on emerging technologies, trends, and best practices in digital architecture, mobile development, and DevOps.
3. Mobile and Backend Development
Design and implement solutions involving mobile native technologies forAndroid and iOS platforms.
Ensure smooth integration betweenfront-end and back-end systems, optimizing for performance, security, and scalability.
4. Data, BI, and 3rd Party Integrations
Lead the integration ofdata analyticsandbusiness intelligence (BI)solutions within the digital architecture.
Manage 3rd party integrations to enhance digital functionality.
Utilize BI tools to provide actionable insights for decision-making.
5. UI/UX and Customer Experience
Collaborate withUI/UX designersto create intuitive, seamless user experiences.
Ensure digital solutions align with customer expectations, enhancing engagement and satisfaction.
6. Infrastructure and DevOps
Oversee the design and management of infrastructure components to ensure they are robust, scalable, and secure.
ImplementDevOpspractices for efficient development, deployment, and maintenance.
7. Enterprise-Grade Software and Team Collaboration
Bring experience from working withenterprise-grade softwareto ensure the reliability and performance of solutions.
Foster a strong, collaborative team environment, promoting effective communication and knowledge sharing.
Act as a thought leader and trusted advisor, building strong relationships with customers and stakeholders.
8. Project Incubation and Delivery
Supportproject incubation, helping to prepare teams for successful project execution.
Review dashboards and reporting to ensure alignment with key deliverables and progress tracking.
Assist in addressing customer escalations and build customer confidence.
9. Productivity and Reusability
Create reusableassets, templates, and toolsto drive productivity improvements.
Monetize in-house developed accelerators and support key project implementations.
10. Mentoring and Training
Conductskills gap assessmentsand provide technology mentoring for team members.
Deliver training on key technologies and architectural best practices.
Primary Skills :
Bachelor s degreein computer science, Information Technology, Engineering, or a related field.
Minimum ofX yearsof experience insolution architecture, with a focus ondigital technologies.
Strong expertise inmicroservices architecture(preferably Python) andmobile native Android/iOS development.
Proficiency indata and BI tools,3rd party integrations,UI/UX design, andbackend technologies.
Solid understanding ofinfrastructure designandDevOps practices.
Experience withenterprise-grade software and products.
Excellent problem-solvingand decision-making skills, able to work under pressure.
Strong communicationand interpersonal skills, able to convey complex technical concepts to non-technical stakeholders.
Ability to buildstrong relationshipswith team members and customers, serving as a trusted advisor.
Help with project incubation and delivery preparedness
Create knowledge assets
Project progress reviews-Help the accounts deliver successfully by engaging in complex projects or projects in Red/Amber status
Provide solutioning for complex design requirements and problems
Review sufficiency of dashboards and status reporting
Align tech leads for key deliverables reviews
Help with addressing customer escalations and improve customer confidence
Assist AMs with account mining
Own up productivity improvements by creating re-usable assetstemplates, in-house developed tools/accelerators, monetize the accelerators
Support key project implementations
Skills gap assessment
Impart trainings on key technologies
Technology mentoring
Key Competencies
Analytical Thinking:Ability to analyze complex problems and design appropriate solutions.
Strategic Vision:Understands the long-term objectives and aligns digital solutions accordingly.
Collaboration:Strong team player, able to work effectively with cross-functional teams.
Innovation:Stays ahead of technological trends and integrates innovative solutions into architecture designs.
Adaptability:Quickly adapts to evolving business needs and emerging technologies.","Business intelligence, Analytical, Ios Development, Android, Front End, Python"
Presales Solutions Architect - Modern Data Center,AHEAD,5-10 Years,,Gurugram,Information Technology,"The AHEAD Modern Datacenter Specialist Solutions Engineer (SSE) will be focused on Datacenter technologies, includingstorage (block & file), compute, virtualization, and data protection. The SSE is considered a subject matter expert in these areas with responsibility for selling and designing complex solutions. The SSE is also considered an organizational thought leader for their specialty area.
Responsibilities:
Active participation in the AHEAD Modern Datacenter SSE team, advisory and delivery teams.
Working with the AHEAD partner management team and directly with selected strategic vendors to maintain technical and sales relationships.
Be a thought leader within the Solutions Engineering organization, responsible for designing, architecting, and at times consulting for AHEAD clients.
Build skills, relationships, and industry credibility across many segments to drive product, services, and consulting sales.
Support the sales teams during engagements by providing advice and solutions or proposals optimized to meet customer technical and business requirements.
Develop relationships with other Modern Datacenter SSE team members, leaders, and partners in support of sales objectives.
Strategize and execute technical sales calls.
Complete required pre-sales documentation (configurations, pricing, services, presentations, justifications, etc.) quickly and accurately.
Qualify sales opportunities in terms of customer technical requirements, decision-making process, and funding.
Present and market the design and value of proposed AHEAD solutions and business justification to customers, prospects, and AHEAD management.
Participate in the mentorship of entry-level team members.
Possess strong, detailed product/technology/industry knowledge.
Knowledge of job-associated software and applications.
Qualifications:
Expertise in Storage (Block & File), Compute, and Virtualization/VMware Cloud Foundation (VCF).
Data Protection experience with Dell, Rubrik, Commvault, etc.
Familiarity with Cisco UCS, Dell MX, PowerScale, PowerFlex and hyperconverged compute platforms (VxRail, Nutanix, Azure Stack HCI, OpenShift-V).
Understanding of Datacenter automation tools like Ansible.
Basic knowledge of cloud platforms like AWS and Azure.
VMware and Virtualization technology expertise.
Vendor experience with Dell, NetApp, Vast Data, Cisco, etc.
Professional communication, presentation, analytical, and problem-solving skills
Experience working as a technical lead in a pre-sales or sales campaign.
Ability to work under critical conditions and influence others to achieve results.
Strong interpersonal skills with excellent presentation skills.
Must be independent, self-motivated, a self-starter, and possess a good working
attitude.
Able to work well within a team and partner environment. Customer-focused and results-driven.
Other Skills:
Innovation Mindset: Ability to identify and leverage emerging Datacenter technologies to drive innovative solutions that meet evolving client needs.
Automation Expertise: Advanced understanding of automation frameworks and scripting to optimize Datacenter operations and reduce manual effort.
Problem Solving:Strong analytical skills with a focus on troubleshooting complex Datacenter environments, including storage, compute, and networking issues.
Consultative Approach:Ability to engage with clients as a trusted advisor, guiding them through the decision-making process with clear, strategic recommendations.
Collaboration Skills:Proven track record of working effectively across teams, including sales, engineering, and vendor partners, to deliver cohesive solutions.
Continuous Learning: Commitment to staying updated on the latest industry trends, certifications, and best practices related to Datacenter technologies.
Security Awareness:Knowledge of Datacenter security best practices, ensuring that proposed solutions adhere to compliance and regulatory standards.","Compute, VCF, VMware, Storage, Ansible, Data Protection"
Solution Architect (Digital/Data practice),Reflections Info Systems,10-14 Years,,"Thiruvananthapuram / Trivandrum, Bengaluru, Chennai",IT Management,"We are seeking a highly experiencedSolution Architect - Digitalto join our team. This role is pivotal in designing and implementing innovative digital solutions that meet both business objectives and customer needs.
The ideal candidate will possess a deep understanding of modern technologies, includingmicroservices architecture, mobile development (Android/iOS), data & BI, backend systems, infrastructure, and DevOps.
Responsibilities include:
1. Architecture Design
Lead the design of end-to-end digital solutions that align with business objectives.
Develop and maintain architectural blueprints, design patterns, and best practices for digital transformation.
Ensure solutions are scalable, secure, and meet performance standards, with a strong focus on microservices architecture (preferably in Python).
2. Technical Leadership
Provide guidance and leadership to development teams throughout the project lifecycle.
Collaborate with business leaders, developers, and other architects to ensure alignment with business goals.
Stay updated on emerging technologies, trends, and best practices in digital architecture, mobile development, and DevOps.
3. Mobile and Backend Development
Design and implement solutions involving mobile native technologies forAndroid and iOS platforms.
Ensure smooth integration betweenfront-end and back-end systems, optimizing for performance, security, and scalability.
4. Data, BI, and 3rd Party Integrations
Lead the integration ofdata analyticsandbusiness intelligence (BI)solutions within the digital architecture.
Manage 3rd party integrations to enhance digital functionality.
Utilize BI tools to provide actionable insights for decision-making.
5. UI/UX and Customer Experience
Collaborate withUI/UX designersto create intuitive, seamless user experiences.
Ensure digital solutions align with customer expectations, enhancing engagement and satisfaction.
6. Infrastructure and DevOps
Oversee the design and management of infrastructure components to ensure they are robust, scalable, and secure.
ImplementDevOpspractices for efficient development, deployment, and maintenance.
7. Enterprise-Grade Software and Team Collaboration
Bring experience from working withenterprise-grade softwareto ensure the reliability and performance of solutions.
Foster a strong, collaborative team environment, promoting effective communication and knowledge sharing.
Act as a thought leader and trusted advisor, building strong relationships with customers and stakeholders.
8. Project Incubation and Delivery
Supportproject incubation, helping to prepare teams for successful project execution.
Review dashboards and reporting to ensure alignment with key deliverables and progress tracking.
Assist in addressing customer escalations and build customer confidence.
9. Productivity and Reusability
Create reusableassets, templates, and toolsto drive productivity improvements.
Monetize in-house developed accelerators and support key project implementations.
10. Mentoring and Training
Conductskills gap assessmentsand provide technology mentoring for team members.
Deliver training on key technologies and architectural best practices.
Primary Skills :
Bachelor s degreein computer science, Information Technology, Engineering, or a related field.
Minimum of 10yearsof experience insolution architecture, with a focus ondigital technologies.
Strong expertise inmicroservices architecture(preferably Python) andmobile native Android/iOS development.
Proficiency indata and BI tools,3rd party integrations,UI/UX design, andbackend technologies.
Solid understanding ofinfrastructure designandDevOps practices.
Experience withenterprise-grade software and products.
Excellent problem-solvingand decision-making skills, able to work under pressure.
Strong communicationand interpersonal skills, able to convey complex technical concepts to non-technical stakeholders.
Ability to buildstrong relationshipswith team members and customers, serving as a trusted advisor.
Help with project incubation and delivery preparedness
Create knowledge assets
Project progress reviews-Help the accounts deliver successfully by engaging in complex projects or projects in Red/Amber status
Provide solutioning for complex design requirements and problems
Review sufficiency of dashboards and status reporting
Align tech leads for . This is to notify jobseekers that some fraudsters are promising jobs with Reflections Info Systems for a fee. Please note that no payment is ever sought for jobs in Reflections. We contact our candidates only through our official website or LinkedIn and all employment related mails are sent through the official HR email id. for any clarification/ alerts on this subject.","Business intelligence, Analytical, Ios Development, Android, Front End, Python"
Presales Solutions Architect - Modern Data Center,AHEAD,6-10 Years,,Gurugram,Information Technology,"Modern Datacenter Specialist Solutions Engineer (SSE) Datacenter Technologies
The AHEAD Modern Datacenter Specialist Solutions Engineer (SSE) will be focused on Datacenter technologies, including storage (block & file), compute, virtualization, and data protection. The SSE is considered a subject matter expert in these areas with responsibility for selling and designing complex solutions. The SSE is also considered an organizational thought leader for their specialty area.
Responsibilities
Active participation in the AHEAD Modern Datacenter SSE team, advisory and delivery teams
Working with the AHEAD partner management team and directly with selected strategic vendors to maintain technical and sales relationships
Be a thought leader within the Solutions Engineering organization, responsible for designing, architecting, and at times consulting for AHEAD clients
Build skills, relationships, and industry credibility across many segments to drive product, services, and consulting sales
Support the sales teams during engagements by providing advice and solutions or proposals optimized to meet customer technical and business requirements
Develop relationships with other Modern Datacenter SSE team members, leaders, and partners in support of sales objectives
Strategize and execute technical sales calls
Complete required pre-sales documentation (configurations, pricing, services, presentations, justifications, etc.) quickly and accurately
Qualify sales opportunities in terms of customer technical requirements, decision-making process, and funding
Present and market the design and value of proposed AHEAD solutions and business justification to customers, prospects, and AHEAD management
Participate in the mentorship of entry-level team members
Possess strong, detailed product/technology/industry knowledge
Knowledge of job-associated software and applications
Qualifications
Expertise in Storage (Block & File), Compute, and Virtualization/VMware Cloud Foundation (VCF)
Data Protection experience with Dell, Rubrik, Commvault, etc.
Familiarity with Cisco UCS, Dell MX, PowerScale, PowerFlex and hyperconverged compute platforms (VxRail, Nutanix, Azure Stack HCI, OpenShift-V)
Understanding of Datacenter automation tools like Ansible
Basic knowledge of cloud platforms like AWS and Azure
VMware and Virtualization technology expertise
Vendor experience with Dell, NetApp, Vast Data, Cisco, etc.
Professional communication, presentation, analytical, and problem-solving skills
Experience working as a technical lead in a pre-sales or sales campaign
Ability to work under critical conditions and influence others to achieve results
Strong interpersonal skills with excellent presentation skills
Must be independent, self-motivated, a self-starter, and possess a good working attitude
Able to work well within a team and partner environment
Customer-focused and results-driven
Other Skills
Innovation Mindset: Ability to identify and leverage emerging Datacenter technologies to drive innovative solutions that meet evolving client needs
Automation Expertise: Advanced understanding of automation frameworks and scripting to optimize Datacenter operations and reduce manual effort
Problem Solving: Strong analytical skills with a focus on troubleshooting complex Datacenter environments, including storage, compute, and networking issues
Consultative Approach: Ability to engage with clients as a trusted advisor, guiding them through the decision-making process with clear, strategic recommendations
Collaboration Skills
Proven track record of working effectively across teams, including sales, engineering, and vendor partners, to deliver cohesive solutions
Continuous Learning: Commitment to staying updated on the latest industry trends, certifications, and best practices related to Datacenter technologies
Security Awareness: Knowledge of Datacenter security best practices, ensuring that proposed solutions adhere to compliance and regulatory standards","Compute, vmware, virtualization, Storage, Dell, Ansible"
Data Engineer - Solution Architect,IOMETE,5-7 Years,,"Bengaluru, India",Login to check your skill match score,"About Us
IOMETE is the self-hosted data lakehouse platform for the age of AI and is pioneering a new approach to data management for complex enterprise data infrastructure environments with large data sets. While SaaS solutions may offer convenience for smaller organizations, larger enterprises often face one-size-fits-all-rigidity, vendor lock-in, data leaks and runaway costs. IOMETE provides a better alternative. No data ever leaves the customer's security boundary, providing enterprises full control over their data at significantly lower costs. Ideal for highly regulated industries like Financial Services, Healthcare, Government and Technology. Built with leading-edge technology like Apache Spark and Apache Iceberg,
Job Summary
At IOMETE, we're seeking a skilled Data Engineer Solution Architect to join our team and work closely with the support organization of one of our Fortune 50 customers. While you'll be on IOMETE's payroll, you will operate day-to-day as an integrated part of the customer's internal teamusing their systems, tools, and processes to provide exceptional support and drive user success.
In this role, you'll work directly with the customer's user organizationresolving support tickets, creating user-facing documentation, and delivering training to ensure smooth adoption and usage. You'll act as a technical expert and advisor, helping the customer navigate complex data workflows and get the most out of IOMETE's platform.
You will also collaborate closely with IOMETE's product and engineering teamsbringing back insights from the field to help prioritize bug fixes, shape feature development, and improve the overall customer experience.
This is a high-impact role that combines deep technical expertise with direct customer interactionideal for someone who enjoys working at the intersection of engineering and user success.
Qualifications
Strong proficiency in PySpark for distributed data processing and transformation.
Solid experience with SQL for querying and managing large datasets.
Hands-on experience with at least one modern data warehouse platform such as Snowflake, Databricks, or BigQuery.
Proficient in Python programming for data manipulation, automation, and building ETL pipelines.
Proven experience in designing, developing, and maintaining robust ETL (Extract, Transform, Load) workflows.
Familiarity with data modeling, data integration techniques, and performance optimization.
Ability to work with large-scale structured and unstructured data.
Experience with version control systems (e.g., Git) and CI/CD practices.
Knowledge of workflow orchestration tools (e.g., Airflow, Prefect, Dagster) is a plus.
Strong problem-solving and analytical skills with attention to detail.
Excellent communication skills and ability to collaborate with cross-functional teams.
Requirements
Location: For this role we are seeking candidates that are located in India only, preferably in the Bangalore area.
Minimum 5 years of relevant experience.
What We Offer
Exciting projects and challenges.
Opportunity to work with cutting-edge technology.
Collaborative and innovative work environment.
Competitive compensation and stock options.
This is a contracting role.
Compensation
Monthly compensation ranging from $4,000 to $6,000, commensurate with experience and qualifications.","workflow orchestration tools, Airflow, data integration techniques, Prefect, snowflake, Dagster, performance optimization, BigQuery, Pyspark, Data Modeling, Sql, Git, Version Control Systems, Databricks, Python, Etl"
Senior Solution Architect - Data Governance Experience-Collibra,Cortex Consultants LLC,15-17 Years,,"Bengaluru, India",Login to check your skill match score,"Solution Architect
Exp-15+
Location-Multiple
Immediate to 15days
Skill-data gov,colibra,architect
Overview: The Senior Solution Architect - Data Governance Experience-Collibra plays a critical role in shaping our organization's data governance framework. This position requires an individual who has extensive experience with Collibra and a strong understanding of data governance principles. The architect will be pivotal in designing and implementing data governance solutions that enhance data quality, compliance, and usability across the enterprise. By leading initiatives that foster a culture of data stewardship, this role ensures that our data assets are effectively managed and leveraged to drive business outcomes. The Senior Solution Architect will collaborate closely with various stakeholders, including IT, compliance, and business units, to ensure alignment with the organization's strategic objectives. This position not only focuses on technical capabilities but also emphasizes the importance of change management and user adoption to ensure long-term success in our data governance journey.
Lead the architecture and implementation of Collibra to support data governance initiatives.
Develop a comprehensive data governance framework that aligns with organizational goals.
Engage with stakeholders to gather requirements and translate them into actionable solutions.
Ensure compliance with data regulations and industry standards during implementation.
Design and maintain data models that enhance data discoverability and usability.
Collaborate with data stewards and business units to promote a culture of data stewardship.
Conduct assessments of existing data management practices and recommend improvements.
Provide training and support for users to maximize the effectiveness of Collibra.
Facilitate workshops and meetings to drive consensus around data governance practices.
Monitor and report on data governance metrics to measure the success of initiatives.
Stay current with industry trends and advancements in data governance technology.
Support change management efforts to drive user adoption of data governance solutions.
Develop and implement data stewardship policies and procedures.
Work closely with IT to ensure seamless integration of Collibra with existing systems.
Serve as a subject matter expert for data governance best practices within the organization.
Required Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field.
At least 7 years of experience in data governance, data management, or a related area.
Proven experience as a Solution Architect, with a focus on data governance solutions.
Extensive hands-on experience with Collibra, including implementation and configuration.
Strong understanding of data governance principles, frameworks, and best practices.
Experience working with data models, metadata management, and data quality assessments.
Knowledge of regulatory compliance requirements such as GDPR, CCPA, or HIPAA.
Ability to engage and communicate effectively with stakeholders at all levels.
Strong analytical and problem-solving skills to address data-related challenges.
Proficient in project management methodologies, with experience leading cross-functional teams.
Experience in change management processes to facilitate user adoption.
Strong presentation skills with the ability to convey complex concepts to non-technical audiences.
Certification in data governance or relevant frameworks is a plus.
Experience with cloud-based data platforms and integration tools.
Ability to work in a fast-paced, dynamic environment with competing priorities.
Skills: regulatory compliance,project management,data stewardship,stakeholder engagement,data governance,metadata management,data quality assessments,data management,change management,collibra,data models,analytical thinking,solution architecture","data quality assessments, Regulatory Compliance, data stewardship, Analytical Thinking, project management, Data Governance, Metadata Management, Collibra, solution architecture, Data Management, change management"
Principal Professional Services Architect (Data Loss Prevention),Zscaler,10-12 Years,,India,Login to check your skill match score,"About Zscaler
Serving thousands of enterprise customers around the world including 40% of Fortune 500 companies, Zscaler (NASDAQ: ZS) was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. As the operator of the world's largest security cloud, Zscaler accelerates digital transformation so enterprises can be more agile, efficient, resilient, and secure. The pioneering, AI-powered Zscaler Zero Trust Exchange platform, which is found in our SASE and SSE offerings, protects thousands of enterprise customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.
Named a Best Workplace in Technology by Fortune and others, Zscaler fosters an inclusive and supportive culture that is home to some of the brightest minds in the industry. If you thrive in an environment that is fast-paced and collaborative, and you are passionate about building and innovating for the greater good, come make your next move with Zscaler.
At Zscaler, our Customer Success Organization is a global, customer-focused team dedicated to delivering high-impact experiences and identifying innovative solutions. We leverage valuable data and research to provide expert, hands-on support starting from the implementation phase and beyond, ensuring customers achieve their goals and leverage our technology to its fullest potential. Together, we create a customer-centric culture that fosters success, adoption, and continuous growth.
Responsibilities
We're looking for an experienced Principal Professional Services Architect, Data Loss Prevention to join our Professional Services team. Reporting to the Senior Manager, Professional Services, you'll be responsible for:
Building long-term, trusted advisor relationships with customers to deliver tailored Data Protection solutions
Designing and executing solutions using Zscaler's suite of products, including DLP, CASB, Shadow IT, and SaaS integrations
Advocating customer needs with cross-functional teams to influence product development
Producing documentation, best practices, and demos to enable organizational growth
What We're Looking for (Minimum Qualifications)
10+ years of professional experience in network security, professional services, or DevOps
Minimum Bachelor's degree in Computer Science, Computer/Electrical Engineering, or related field
Must be proficient in Data Loss Prevention (DLP), CASB, and cloud security solutions
Proven ability with scripting languages, REST APIs, and Linux/Windows systems
What Will Make you Stand Out (Preferred Qualifications)
Certifications such as CISSP, CISM, or CCSP
Advanced expertise in network security architecture and SaaS integration
Relevant industry experience at leading networking security companies
At Zscaler, we believe that diversity drives innovation, productivity, and success. We are looking for individuals from all backgrounds and identities to join our team and contribute to our mission to make doing business seamless and secure. We are guided by these principles as we create a representative and impactful team, and a culture where everyone belongs. For more information on our commitments to Diversity, Equity, Inclusion, and Belonging, visit the Corporate Responsibility page of our website.
Benefits
Our Benefits program is one of the most important ways we support our employees. Zscaler proudly offers comprehensive and inclusive benefits to meet the diverse needs of our employees and their families throughout their life stages, including:
Various health plans
Time off plans for vacation and sick time
Parental leave options
Retirement options
Education reimbursement
In-office perks, and more!
By applying for this role, you adhere to applicable laws, regulations, and Zscaler policies, including those related to security and privacy standards and guidelines.
Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws.
See more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link.
Pay Transparency
Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, click here .
Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support.","CASB, Cloud Security Solutions, Windows Systems, Linux, Scripting Languages, Rest Apis"
Data Engineer(Azure/Architect),VidPro Consultancy Services,8-12 Years,,"Chennai, India",Login to check your skill match score,"Experience: 8-12 years
Location: Bangalore, Chennai, Delhi, Pune, Kolkata
Work Type : Full Time
Work Mode : Hybrid
Mandatory Skills: Python/ PySpark / Data Engineer/Azure Databricks/Azure Data Factory/ETL/SQL/Architect
Primary Roles And Responsibilities
Developing Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack
Ability to provide solutions that are forward-thinking in data engineering and analytics space
Collaborate with DW/BI leads to understand new ETL pipeline development requirements.
Triage issues to find gaps in existing pipelines and fix the issues
Work with business to understand the need in reporting layer and develop data model to fulfill reporting needs
Help joiner team members to resolve issues and technical challenges.
Drive technical discussion with client architect and team members
Orchestrate the data pipelines in scheduler via Airflow
Skills And Qualifications
Bachelor's and/or master's degree in computer science or equivalent experience.
Must have total 6+ yrs. of IT experience and 3+ years experience in Data warehouse/ETL projects.
Deep understanding of Star and Snowflake dimensional modelling.
Strong knowledge of Data Management principles
Good understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture
Should have hands-on experience in SQL, Python and Spark (PySpark)
Candidate must have experience in AWS/ Azure stack
Desirable to have ETL with batch and streaming (Kinesis).
Experience in building ETL / data warehouse transformation processes
Experience with Apache Kafka for use with streaming data / event-based data
Experience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)
Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
Experience working with structured and unstructured data including imaging & geospatial data.
Experience working in a Dev/Ops environment with tools such as Terraform, Circle CI, GIT.
Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot
Databricks Certified Data Engineer Associate/Professional Certification (Desirable).
Comfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects
Should have experience working in Agile methodology
Strong verbal and written communication skills.
Strong analytical and problem-solving skills with a high attention to detail.
Skills: etl,sql,projects,cassandra,mongodb,unix shell scripting,azure synapses,azure data factory,data engineering,azure datafactory,data lakes,azure,data management,hadoop,rdbms,pipelines,neo4j,circle ci,azure databricks,aws,git,data engineer,data,apache kafka,pyspark,terraform,skills,python,data warehouse,architects,pl/sql","Circle CI, Hadoop, Cassandra, Pyspark, Azure Databricks, Sql, Azure Data Factory, Terraform, Neo4j, Unix Shell Scripting, Apache Kafka, MongoDB, Python, AWS, Etl"
Associate Architect Data Historian,Merck Group,10-12 Years,,"Bengaluru, India",Login to check your skill match score,"Work Your Magic with us!
Ready to explore, break barriers, and discover more We know you've got big plans so do we! Our colleagues across the globe love innovating with science and technology to enrich people's lives with our solutions in Healthcare, Life Science, and Electronics. Together, we dream big and are passionate about caring for our rich mix of people, customers, patients, and planet. That's why we are always looking for curious minds that see themselves imagining the unimaginable with us.
Your role
In your role, you manage the technical implementation of the Merck AVEVA PI (former OSI PI) data historian programs.
You actively prepare architecture schemas and data flow diagrams, and you explain and defend the selected approach in respective discussions in front of infrastructure teams and architectural boards. You contribute to selection of implementation partners, and you actively manage external implementation teams and their delivery. You actively contribute to the optimization of the operating model for running programs and services. You provide implementation effort, cost estimation, resource planning and everything required from a work-package owner of a larger program. You act as first point of contact for technical implementation questions raised by our business stakeholders, as well as architectural guide to the implementation teams. Internally, you align and coordinate your activities with the service delivery managers and support them with the required compliance documentation. You manage the project to service hand-over, organize knowledge transfer and define training for the local IT service team members.
Who You Are
You have 10 + years of experience in responsible role in IT project delivery for global companies. You are technical expert for AVEVA PI (OSI PI) and you have a very good technical understanding of the surrounding technologies, such as SCADA, OPCs, Kepware, OEE and others. During your career you acted as functional team lead and technical instructor for an implementation team. You know how it feels to face technical issues in systems which operate 24/7 and you have resolved them in responsible position. You are assertive, being able to define and implement technical standards within a group of developers. You worked in the Healthcare industry, where you got used to comply to GxP guidelines and regulations. You strategically plan the future and express upcoming operative demands in front of your manager. Last not least, you are looking for a long-term engagement and an office-based local working place in a dynamic team with the perspective of growth and development.
What we offer: We are curious minds that come from a broad range of backgrounds, perspectives, and life experiences. We celebrate all dimensions of diversity and believe that it drives excellence and innovation, strengthening our ability to lead in science and technology. We are committed to creating access and opportunities for all to develop and grow at your own pace. Join us in building a culture of inclusion and belonging that impacts millions and empowers everyone to work their magic and champion human progress!
Apply now and become a part of our diverse team!","Oee, AVEVA PI, OPCs, Kepware, SCADA, Osi Pi"
Senior Architect - Data Integration Technology,WSAudiology,7-9 Years,,"Bengaluru, India",Login to check your skill match score,"Driven by the passion to improve quality of people's lives, WS Audiology continues to grow as market leader in the hearing aid industry. With our commitment to increase penetration in an underserved hearing care market, we want to accelerate our business transformation in order to reach more people, more effectively.
We are looking for a Data Integration Technology Architect for our newly created stream for IPASS. This role leads technically & Functionally our strategic project for data integration between applications using a Platform as a Service.
What you will do
Lead Technically & Functionally our strategic project for data integration between appplication using a Platform as a Service.
Collaborate with external vendors and developers, partner with internal stakeholders to create the best possible data product.
Analyze business requirements and prepare specifications for product development.
Create and design architecture and developer framework package for rollout.
Share Knowledge and create awareness around the new platform
Collaboration with Global IT functions on requirements and adoption of IPAAS & Management of external consultants.
What you bring
7+ years of relevant experience in relational DBs: SQL Server, Oracle, Mysql, NoSQL(Mongo DB etc)
Mulesoft or Workato experience preferred
Extensive knowledge on API driven Development
Extensive Cloud Knowledge (Azure, AWS , Google)
Extensive Knowledge on Streaming Applications such as Kafka / Flink
Extensive knowledge for Event-Driven Architecture vs API (REST, SOAP) for microservices.
Knowledge on DevOps & CI/CD Topics such as Containerization (Docker, Kubernetes), Jenkinks / Gitlab for CI/CD and versioning control.
Knowledge on data encryption, network protocols and general ACL for cloud integration.
University degree in Computer Science, Information Technology or similar streams.
Who we are
At WS Audiology, we provide innovative hearing aids and hearing health services.
Together with our 12,000 colleagues in 130 countries, we invite you to help unlock human potential by bringing back hearing for millions of people around the world.
With us, you will become part of a truly global company where we care for one another, welcome diversity and celebrate our successes.
Sounds wonderful We can't wait to hear from you.
WS Audiology is an equal-opportunity employer and committed to creating an inclusive employee experience for all. Regardless of race, color, religion, national origin, age, sex, gender, gender identity, gender expression, sexual orientation, marital status, medical condition, ancestry, disability, military or veteran status we firmly believe that our work is at its best when everyone feels free to be their most authentic self.","Event-Driven Architecture, Cloud Knowledge, relational DBs, Flink, Streaming Applications, CI CD, API driven Development, data encryption, API REST, Google, containerization, general ACL, Kafka, Microservices, Nosql, Docker, MySQL, Oracle, AWS, SQL Server, Mulesoft, Soap, Network Protocols, Workato, Devops, Gitlab, Azure, Kubernetes"
Architect - Data Center,Bechtel Corporation,6-8 Years,,India,Login to check your skill match score,"Requisition ID: 282943
Relocation Authorized: National - Family
Telework Type: Full-Time Office/Project
Work Location: Various Bechtel Project Locations
Extraordinary Teams Building Inspiring Projects
Since 1898, we have helped customers complete more than 25,000 projects in 160 countries on all seven continents that have created jobs, grown economies, improved the resiliency of the world's infrastructure, increased access to energy, resources, and vital services, and made the world a safer, cleaner place.
Differentiated by the quality of our people and our relentless drive to deliver the most successful outcomes, we align our capabilities to our customers objectives to create a lasting positive impact. We serve the Infrastructure; Nuclear, Security & Environmental; Energy; Mining & Metals, and the Manufacturing and Technology markets. Our services span from initial planning and investment, through start-up and operations.
Core to Bechtel is our Vision, Values and Commitments . They are what we believe, what customers can expect, and how we deliver. Learn more about our extraordinary teams building inspiring projects in our Impact Report .
Bechtel India is a global operation that supports execution of projects and services around the world. Working seamlessly with business line home offices, project sites, customer organizations and suppliers, our teams have delivered more than 125 projects since our inception in 1994.
Our offices in Gurgaon, Vadodara and Chennai will grow significantly and sustainably with exciting career opportunities for both professionals and young graduates who are passionate about creating a cleaner, greener, and safer world; building transformational infrastructure; making decarbonization a reality; and protecting people and the environment.
Job Summary
Architect with more than 6 years of experience in Manufacturing and Infrastructure projects (with relevant experience in Data center) projects in a design office of repute. A candidate who has completed at least 1 data center project shall be given more weightage. Should have exposure to design office practices and familiarity with computer aided design and 3D modeling tools like Revit.
Major Responsibilities
Shall perform / supervise calculations, prepare material requisitions, service requisitions, code analysis, standard details, technical specifications and quantity take-offs and provide input for design drawings during all phases of projects.
Shall check / review the drawings prepared by the designers.
Shall assist in the development of basic layout drawings.
Shall lead conceptual studies and inter-disciplinary reviews
Shall create perspectives and presentation of design to client.
Provide technical training.
Perform feasibility studies for site development, building configuration, climate studies.
Education And Experience Requirements
Minimum 5-year degree in Architecture from an accredited college or university.
Candidate with Master's degree is desirable.
Professional license from a recognized licensing board and/or LEED certification shall be of added advantage.
Experience of making Architectural presentation shall be an added advantage.
Level I: 6 - 8 years of relevant work experience
Level II: 8 - 10 years of relevant work experience
Required Knowledge And Skills
Knowledge of architectural techniques and design principles with a basic knowledge of the types of data center, data hall layout, mechanical systems, electrical systems, and security systems needed to design a data center.
Knowledge of precedents and latest trends in architectural engineering, the principles and practices of related technical areas.
Knowledge of Engineering Procedures and design guides.
Thorough knowledge of the roles played by other engineering disciplines on projects.
Knowledge of regulatory Indian and International codes and standards, industry practices and design criteria pertinent to architectural engineering design, including fire life safety codes.
Knowledge of constructability and applicable standards and codes
Proficiency in the use of Revit, Navisworks and exposure to BIM is essential.
Proficient in selection of material from constructability and total installed cost perspective.
Skill in oral and written communication.
Should be proficient in using MS office tools.
Proved ability of managing a team of architects and designers will be an added advantage.
Previous experience of Data center design including administration building, process/ non process buildings and other ancillary facilities like maintenance building, guard house, substation, electrical buildings, pump house, etc.
Knowledge of master planning and fire life safety design guidelines
Good knowledge of faade design and material selection (including interior & exterior finishes)
Experience in working on EPC projects shall be an added advantage.
Total Rewards/Benefits
For decades, Bechtel has worked to inspire the next generation of employees and beyond! Because our teams face some of the world's toughest challenges, we offer robust benefits to ensure our people thrive. Whether it is advancing careers, delivering programs to enhance our culture, or providing time to recharge, Bechtel has the benefits to build a legacy of sustainable growth. Learn more at Bechtel Total Rewards
Diverse Teams Build The Extraordinary
As a global company, Bechtel has long been home to a vibrant multitude of nationalities, cultures, ethnicities, and life experiences. This diversity has made us a more trusted partner, more effective problem solvers and innovators, and a more attractive destination for leading talent.
We are committed to being a company where every colleague feels that they belong-where colleagues feel part of One Team, respected and rewarded for what they bring, supported in pursuing their goals, invested in our values and purpose, and treated equitably. Click here to learn more about the people who power our legacy.
Bechtel is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity and expression, age, national origin, disability, citizenship status (except as authorized by law), protected veteran status, genetic information, and any other characteristic protected by federal, state or local law. Applicants with a disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to [HIDDEN TEXT]","Fire life safety codes, Bim, Security Systems, Mechanical Systems, Material Selection, Navisworks, Architectural techniques, Design Principles, Revit, Electrical Systems, data center design, Ms Office"
MDM Architect- Data Governance,Fractal,Fresher,,"Bengaluru, India",Login to check your skill match score,"Responsibilities
You will be part of the Data Governance team working on Data Strategy, Master Data Management, Data Quality, and Metadata management
Lead the end-to-end design, architecture, and implementation of MDM solutions
Define and implement Master Data Governance processes including data quality, data stewardship, metadata management, and business ownership.
Collaborate with business and IT stakeholders to define MDM requirements and translate them into scalable, high-performance technical solutions.
Design data models and hierarchies for Customer, Product, Vendor, and other master domains.
Develop and operationalize Data Governance frameworks aligned to business and compliance needs.
Enable data stewardship workflows, match & merge rules, and exception management.
Integrate MDM systems with upstream and downstream applications across the enterprise.
Lead workshops and training sessions on MDM and Data Governance for client teams.
Support RFPs, proposals, and client presentations with MDM/DG expertise
Assess, validate and implement MDM architecture for on-prem, cloud and hybrid environments with expertise in MDM solutions like Oracle MDM solutions (ERP, CDH, CDM), SAP MDG(S4/Hana, ERP), Informatica IDMC and other modern MDM solutions.
Develop integration frameworks for legacy and on-premise systems using Oracle GoldenGate, Informatica CDI/CAI, MuleSoft or other integration tools.
Collaborate with stakeholders to define and implement MDM strategies, standards, and operating models.
Develop and enforce end-to-end master data lifecycle processes including data onboarding, mastering, match & merge logic, survivorship rules, and downstream data syndication.
Collaborate with clients to understand their business objectives and data challenges and develop comprehensive data governance strategies aligned with their specific needs.
Support leadership in designing thought leadership, publish POV's/Articles/Blogs, establishing data governance policies, procedures, and frameworks that ensure the effective management, privacy, and security of data assets.
Good to Have
Hands-on implementation experience with on-prem MDM and ERP systems such as Oracle ERP, Oracle CDH/CDM, SAP MDG, SAP S4/HANA, Reltio, Stibo or hybrid deployments with other modern MDM, ERP and CRM platforms.
Strong knowledge of reference data management and data stewardship workflows.
Experience with ETL/ELT tools and integration platforms (Oracle ODI, GodenGate, MuleSoft, Informatica PowerCenter, etc.).
Familiarity with data governance tools (e.g., Collibra, Alation, Informatica CDGC, etc.) and DQ tools (e.g., Informatica IDQ, CDQ, etc.).
Basic understanding of data governance best practices, data quality management, data privacy regulations
Familiarity with Agentic AI, machine learning, and analytics technologies
Knowledge of SQL, PL/SQl, Python, or any other database
Excellent communication and interpersonal skills to collaborate effectively with clients and internal teams.","ELT tools, Informatica CDI, S4 Hana ERP, Data Governance frameworks, Oracle MDM solutions, Alation, Informatica IDMC, Data stewardship workflows, Informatica CDGC, Metadata Management, Collibra, Pl Sql, Oracle Goldengate, Sql, Data Quality, Informatica Idq, Sap Mdg, Mulesoft, Python"
Chief Architect - Data & AI,Orion Innovation,Fresher,,India,Login to check your skill match score,"Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.
We are seeking a dynamic and experienced leader for our Data Architecture and Data Science Practice. This role will be instrumental in shaping our organization's data strategy, driving innovation through advanced analytics, and ensuring robust data architecture to support our business objectives.
Responsibilities
Strategic Leadership: Develop and implement data strategies that align with organizational goals and objectives. Drive innovation and efficiency through the effective use of data.
Team Management: Lead and mentor a team of data architects, data engineers, and data scientists. Provide guidance and support to foster professional growth and collaboration within the team.
Data Architecture: Designing and maintaining scalable and efficient solutions to ensure data integrity, availability, and security across an organization's infrastructure. This includes translating business requirements into logical and physical data models, ensuring data is transformed correctly from source to target systems through detailed mapping, and converting business models into a comprehensive data platform. Data integration combines data from various sources into a unified view using ETL/ELT processes, data pipelines, and APIs, centralizing storage in data lakes and warehouses. An audit framework tracks and monitors data activities to ensure compliance and transparency, while scheduling and monitoring tools ensure data processes run smoothly and on time. Adhering to Service Level Agreements (SLAs) involves defining SLAs, tracking key metrics, managing incidents efficiently, and providing regular reports to stakeholders. This holistic approach supports business needs, ensures data quality, and maintains operational efficiency.
Advanced Analytics: Oversee the development and implementation of advanced analytics techniques, including machine learning, predictive modeling, and optimization algorithms. Drive the adoption of best practices and methodologies in data science.
Stakeholder Collaboration: Collaborate with stakeholders across the organization to understand business requirements and priorities. Translate business needs into data initiatives and deliver actionable insights to drive decision-making.
Technology Evaluation: Stay updated on emerging technologies and trends in data management and analytics. Evaluate new tools, platforms, and methodologies to enhance the organization's data capabilities.
Governance and Compliance: Establish and enforce data governance policies and procedures to ensure regulatory compliance, data privacy, and security. Implement best practices for data quality management and data lineage tracking.
Performance Monitoring: Define key performance indicators (KPIs) to measure the effectiveness of data practices. Monitor performance metrics, analyze trends, and identify areas for improvement.
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, Information Systems, or a related field.
Strong leadership experience in data architecture, data engineering, or data science.
In-depth knowledge of data architecture principles, data modeling techniques, and database technologies.
Proficiency in programming languages such as Python, R, SQL, etc.
Strong communication skills with the ability to translate technical concepts into business terms.
Experience working in a fast-paced environment and managing multiple priorities effectively
Orion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Candidate Privacy Policy
Orion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, Orion, we Or us) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (Notice) Explains
What information we collect during our application and recruitment process and why we collect it;
How we handle that information; and
How to access and update that information.
Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.","Data lakes, R, Data architecture principles, Data pipelines, Data modeling techniques, Apis, Optimization Algorithms, Sql, ELT, Database Technologies, Machine Learning, Predictive Modeling, Python, Etl"
Software Architect (Data Engineering),Velotio Technologies,7-9 Years,,"Pune, India",Login to check your skill match score,"About Velotio:
Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.
We are looking for an experienced Software Architect with deep expertise in Data Engineering and proficiency in Ruby on Rails (RoR). The ideal candidate will lead architectural decisions, optimize data pipelines, and ensure scalable, efficient, and high-performance data processing solutions. While Data Engineering skills are the primary focus, RoR experience would be a valuable addition.
Requirements
7+ years of experience in software development, with a focus on data engineering and cloud architectures
Strong experience with Snowflake, DBT, and Data Pipeline design
Expertise in Kafka, Argo, Kubernetes, and ML Flow
Deep understanding of AWS services, particularly Lambdas, Step Functions, Serverless, and SageMaker
Solid Python development skills for data engineering and backend services
Experience with Terraform for infrastructure automation
Strong architectural experience in designing scalable, fault-tolerant, and distributed systems
Nice-to-Have (RoR Focused) Skills:
Experience with Ruby on Rails (RoR) for API development and backend services
Familiarity with Sidekiq, MySQL, Redis, and performance tuning in a RoR environment
Understanding of CI/CD pipelines, DevOps best practices, and microservices architecture
Responsibilities :
Design and implement scalable, high-performance data architectures in cloud environments (AWS preferred)
Drive best practices for data pipeline design, integration, and orchestration using technologies like DBT, Kafka, Argo, and Kubernetes
Define and implement data governance, security, and compliance best practices
Provide technical guidance on integrating MLFlow, SageMaker, and other AI/ML solutions into data pipelines
Build and maintain data pipelines using Snowflake, DBT, Kafka, and AWS serverless services (Lambdas, Step Functions)
Ensure optimal performance, scalability, and cost-effectiveness of data workflows
Utilize Terraform for infrastructure as code to manage cloud environments efficiently
Work with Kubernetes for deploying, scaling, and managing applications
Develop and optimize data-driven applications leveraging Python
Collaborate with software teams to integrate data pipelines into RoR applications
Implement and optimize background job processing using Sidekiq, Redis, and MySQL
Ensure scalability, reliability, and maintainability of services
Benefits
Our Culture:
We have an autonomous and empowered work culture encouraging individuals to take ownership and grow quickly
Flat hierarchy with fast decision making and a startup-oriented get things done culture
A strong, fun & positive environment with regular celebrations of our success. We pride ourselves in creating an inclusive, diverse & authentic environment
At Velotio, we embrace diversity. Inclusion is a priority for us, and we are eager to foster an environment where everyone feels valued. We welcome applications regardless of ethnicity or cultural background, age, gender, nationality, religion, disability or sexual orientation.","Serverless, snowflake, ML Flow, SageMaker, Step Functions, DevOps best practices, Sidekiq, dbt, microservices architecture, Data Pipeline design, Lambdas, Argo, Aws Services, data engineering, Kafka, Redis, MySQL, Python, Kubernetes, Terraform"
Solutions Architect - Data Governance,Informatica,7-9 Years,,"Bengaluru, India",IT/Computers - Software,"Build Your Career at Informatica
We seek innovative thinkers who believe in the power of data to drive meaningful change. At Informatica, we welcome adventurous, work-from-anywhere minds eager to tackle the world's most complex challenges. Our employees are empowered to push their bold ideas forward, and we are united by a shared passion for using data to do the extraordinary for each other and the world.
Use a sales CRM to manage the sales pipeline and record information on prospects.Solutions Architect - Data Governance
We're looking for an Solutions Architect candidate with experience in IDMC Cloud Data Governance and Catalogue (CDGC and Metadata Command Centre), Cloud Marketplace, Cloud Data Quality and Profiling, Claire GPT, to join our team in Bangalore / Hybrid work schedule.
You will report to the Senior Manager, Customer Success Architect.
You will work in Pre-sales or Post-sales consulting role in Enterprise software solutions. Customer Success Management (CSM) organisation focusing on our Cloud-First Cloud-Native and Data 4.0 strategy.
Technology You'll Use
IDMC Cloud Data Governance and Catalog (CDGC and Metadata Command Centre), Cloud Marketplace, Cloud Data Quality and Profiling, Claire GPT
Your Role Responsibilities Here's What You'll Do
You would provide architecture and design, use cases solution, and solution implementation advice.
You would also work with our Professional Services team and have a seamless handoff for broader service engagements. Partner with Product/Engineering Teams to understand the best recommendations to design a solution OR provide comprehensive feedback to them to better align our product to customer needs.
Deliver compelling architectural blueprints, best practices, expert sessions, and scoped implementations to influence the strategic direction of customer adoption and lead customers through solution design for our SAAS products.
Collaborate with Customer Support and Engineering/Product Management.
Manage customer relationships, engaging them in value-added activities.
Experience using a CRM software system.
What We'd Like to See
Promote the value proposition in presentations and demos with prospects and customers.
Compose and publish external facing whitepapers, artefacts, case studies, architectural blueprints, blog posts, and articles for technical/industry publications.
Role Essentials
Bachelor's degree in computer engineering/Technology
7+ years of work experience.
Perks & Benefits
Comprehensive health, vision, and wellness benefits (Paid parental leave, adoption benefits, life insurance, disability insurance and 401k plan or international pension/retirement plans
Flexible time-off policy and hybrid working practices
Equity opportunities and an employee stock purchase program (ESPP)
Comprehensive Mental Health and Employee Assistance Program (EAP) benefit
Our DATA values are our north star and we are passionate about building and delivering solutions that accelerate data innovations. At Informatica, our employees are our greatest competitive advantage. So, if your experience aligns but doesn't exactly match every qualification, apply anyway. You may be exactly who we need to fuel our future with innovative ideas and a thriving culture.
Informatica (NYSE: INFA), a leader in enterprise AI-powered cloud data management, brings data and AI to life by empowering businesses to realize the transformative power of their most critical assets. We pioneered the Informatica Intelligent Data Management Cloud that manages data across any multi-cloud, hybrid system, democratizing data to advance business strategies. Customers in approximately 100 countries and more than 80 of the Fortune 100 rely on Informatica. . Connect with , , and . Informatica. Where data and AI come to life.","Profiling, Metadata Command Centre, crm software, Claire GPT, Cloud Marketplace, Catalogue CDGC, Cloud Data Quality, IDMC Cloud"
Solutions Architect - Data Integration,Informatica,7-9 Years,,"Bengaluru, India",IT/Computers - Software,"Build Your Career at Informatica
We seek innovative thinkers who believe in the power of data to drive meaningful change. At Informatica, we welcome adventurous, work-from-anywhere minds eager to tackle the world's most complex challenges. Our employees are empowered to push their bold ideas forward, and we are united by a shared passion for using data to do the extraordinary for each other and the world.
Solutions Architect - Data Integration
We're looking for an Solutions Architect candidate with experience in Cloud Data integration, Cloud Application Integration, Informatica PowerCenter to join our team in Bangalore., to join our team in Bangalore / Hybrid work schedule.
You will report to the Manager, Customer Success Architect.
You will work in Pre-sales or Post-sales consulting role in Enterprise software solutions. Customer Success Management (CSM) organization focusing on our Cloud-First Cloud-Native and Data 4.0 strategy.
Technology You'll Use
Cloud Data integration, Cloud Application Integration, Informatica PowerCenter
Your Role Responsibilities Here's What You'll Do
You would provide architecture and design, use cases solution, and solution implementation advice.
You would also work with our Professional Services team and have a seamless handoff for broader service engagements. Partner with Product/Engineering Teams to understand the best recommendations to design a solution OR provide comprehensive feedback to them to better align our product to customer needs.
Deliver compelling architectural blueprints, best practices, expert sessions, and scoped implementations to influence the strategic direction of customer adoption and lead customers through solution design for our SAAS products.
Collaborate with Customer Support and Engineering/Product Management.
Manage customer relationships, engaging them in value-added activities.
Experience using a CRM software system.
Use a sales CRM to manage the sales pipeline and record information on prospects.
What We'd Like to See
Promote the value proposition in presentations and demos with prospects and customers.
Compose and publish external facing whitepapers, artefacts, case studies, architectural blueprints, blog posts, and articles for technical/industry publications.
Role Essentials
Bachelor's degree in computer engineering/Technology
7+ years of work experience.
Perks & Benefits
Comprehensive health, vision, and wellness benefits (Paid parental leave, adoption benefits, life insurance, disability insurance and 401k plan or international pension/retirement plans
Flexible time-off policy and hybrid working practices
Equity opportunities and an employee stock purchase program (ESPP)
Comprehensive Mental Health and Employee Assistance Program (EAP) benefit
Our DATA values are our north star and we are passionate about building and delivering solutions that accelerate data innovations. At Informatica, our employees are our greatest competitive advantage. So, if your experience aligns but doesn't exactly match every qualification, apply anyway. You may be exactly who we need to fuel our future with innovative ideas and a thriving culture.
Informatica (NYSE: INFA), a leader in enterprise AI-powered cloud data management, brings data and AI to life by empowering businesses to realize the transformative power of their most critical assets. We pioneered the Informatica Intelligent Data Management Cloud that manages data across any multi-cloud, hybrid system, democratizing data to advance business strategies. Customers in approximately 100 countries and more than 80 of the Fortune 100 rely on Informatica. . Connect with , , and . Informatica. Where data and AI come to life.","Cloud Application Integration, crm software, Cloud Data integration, Informatica Powercenter"
"Senior Manager, Principal Solution Architect - Clinical Data Ecosystem",Bristol Myers Squibb,10-12 Years,,"Hyderabad, India",Login to check your skill match score,"Working with Us
Challenging. Meaningful. Life-changing. Those aren't words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You'll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.
Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more careers.bms.com/working-with-us .
Position Summary
At BMS, digital innovation and Information Technology are central to our vision of transforming patients lives through science. To accelerate our ability to serve patients around the world, we must unleash the power of technology. We are committed to being at the forefront of transforming the way medicine is made and delivered by harnessing the power of computer and data science, artificial intelligence, and other technologies to promote scientific discovery, faster decision making, and enhanced patient care.
Principal Solution Architect - Clinical Data Ecosystem will develop solution architectures for Clinical Data Ecosystem Drug Development business processes utilizing and other related technologies to help enable BMS to deliver medicines to patients faster. The incumbent will work closely with IT product teams including business stakeholders in Hyderabad and globally to analyze business and technology requirements and define, prototype, and scale automation solutions. As a Principal Solution Architect for Clinical Data Ecosystem IT based out of our BMS Hyderabad you are part of the Drug Development IT team that delivers, platform, data and analytics capabilities for GDD Global Biostatistics and Data Sciences, Clinical Data Management (Clinical Analytics, Site Selection, Feasibility, Real World Evidence). This position will require the incumbent to demonstrate effective teamwork, collaboration, and communication across IT and business stakeholders.
If you want an exciting and rewarding career that is meaningful, consider joining our diverse team!
Key Responsibilities
Work with Drug Development teams to identify opportunities for automation and recommend the appropriate technology solutions that meet their requirements
Develop and maintain architecture diagrams, technical specifications, and other documentation to ensure that technology solutions are scalable, secure, and maintainable
Developing data architecture strategy Working with stakeholders to define the overall strategy for the organization's data architecture. This includes defining data structures, metadata, data models, and data integration processes.
Partner with data scientists to develop GenAI and LLM-powered applications (Chat with Data), leveraging techniques like AI Agents, Agentic architectures, RAG, fine-tuning, and vector embeddings to deliver high-quality data discovery & consumption features.
Deep understanding of the GenAI Tech Stack, including its architecture, components, and capabilities.
Develop and Maintain CI/CD workflows and tools for efficient code and infrastructure deployment.
Defining data product strategy Working with stakeholders to define the overall strategy for the organization's data products. This involves understanding business goals, identifying data sources, and determining the appropriate technology stack.
Designing data products Creating conceptual, logical, and physical data models for the organization's data products. This includes defining data structures, schema, and data processing pipelines. Designing data systems Developing and implementing data systems that support the organization's business needs. This may involve working with various technologies, including data warehouses, data lakes, data marts, and data APIs.
Data modeling Developing logical and physical data models that reflect the organization's data needs. This includes defining data elements, data relationships, data flows, and data transformation rules.
Data governance Developing and implementing data governance policies and procedures to ensure that data is accurate, consistent, and secure. This includes defining data quality standards, data security protocols, and data privacy policies.
Collaborating with stakeholders Working closely with various stakeholders, including data scientists, software developers, business analysts, and project managers, to ensure that data is being used effectively and efficiently across the organization.
Staying up-to-date with industry trends Keeping up-to-date with the latest trends and advancements in data architecture and technology, and applying this knowledge to enhance the organization's data systems and processes.
Optimize data storage and retrieval to ensure efficient performance and scalability
Collaborate with data analysts and data scientists to understand their data needs and ensure that the data infrastructure supports their requirements
Implement and maintain security protocols to protect sensitive data
Closely partner with the Enterprise Data and Analytics Platform team, other functional data pods and Data Community lead to shape and adopt data and technology strategy.
Serves as the Subject Matter Expert on GDD Data & Analytics Solutions and build domain knowledge of the GDD specific area
Has End to End ownership mindset in driving initiatives through completion
Comfortable working in a fast-paced environment with minimal oversight
Mentors other team members effectively to unlock full potential
Prior experience working in an Agile/Product based environment
Provides strategic feedback to vendors on service delivery and balances workload with vendor teams
Qualifications & Experience
10-12 years of hands-on experience working on implementing, architecting and operating data capabilities and cutting-edge data solutions, preferably in a cloud environment. Breadth of experience in technology capabilities that span the full life cycle of data management including data lakehouses, master/reference data management, data quality and analytics/AI ML is needed.
Bachelor's degree in Computer Science, Information Technology, Life Sciences, or a related field. Advanced degree preferred.
Take Accountability and Ownership delivering business outcomes within time and budgets.
At least 7 years data engineering experience in designing and building complex data solutions
Hands-on experience developing and delivering data, ETL solutions with some of the technologies like AWS data services (Glue, Redshift, Athena, lakeformation, etc.), BI is a plus
Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.
In-depth expertise with data security, storage solutions, database virtualization and replication as well as other complex database/data technical tools and solutions.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Strong programming skills in languages such as Python, R, PySpark etc.
Experience with SQL and database technologies such as MySQL, PostgreSQL etc.
Experience with cloud-based data technologies such as AWS, Azure, or Google Cloud Platform
Strong analytical and problem-solving skills
Excellent communication and collaboration skills Functional knowledge in regulated environments or prior experience in Lifesciences Research and Development domain is a plus
Experience and expertise in establishing agile and product-oriented teams that work effectively with teams in US and other global BMS site.
Initiates challenging opportunities that build strong capabilities for self and team
Demonstrates a focus on improving processes, structures, and knowledge within the team. Leads in analyzing current states, deliver strong recommendations in understanding complexity in the environment, and the ability to execute to bring complex solutions to completion.
If you come across a role that intrigues you but doesn't perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.
Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as Transforming patients lives through science , every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.
On-site Protocol
BMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role
Site-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.
BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to [HIDDEN TEXT] . Visit careers.bms.com/ eeo -accessibility to access our complete Equal Employment Opportunity statement.
BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.
BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.
If you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information https //careers.bms.com/california-residents/
Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.","GenAI, data lakes, R, AWS data services, LLM-powered applications, data APIs, Data Security, Pyspark, PostgreSQL, Data Architecture, Data Modeling, Sql, data engineering, Etl Solutions, MySQL, Data Governance, data warehouses, Data Integration, Python"
Data Analytics Lead (Associate Architect),Toolify Private Limited,15-20 Years,,Bengaluru,"Recruiting, Staffing Agency","Data Analytics Lead (Associate Architect)
Location: Bengaluru, India
Experience 15+ Years
Joining- Immediate to 15 Days
Summary
Toolify is seeking a visionary Data Analytics Lead to guide and inspire our data delivery team supporting a capital markets client specializing in private markets and alternative investment strategies. This role requires a blend of technical expertise, leadership acumen, and domain experience in capital markets and private markets.
Key Responsibilities
Team Leadership & Delivery Excellence:
Lead a cross-functional team comprising data architects, analysts, business SMEs, and technologists to deliver high-impact data analytics solutions.
Define and enforce best practices for efficient, scalable, and high-quality delivery.
Inspire a culture of collaboration, accountability, and continuous improvement within the team.
Strategic Data Leadership:
Develop and execute a data strategy aligned with client business objectives, ensuring seamless integration of analytics into decision-making processes.
Collaborate with stakeholders to translate business needs into actionable data solutions, influencing strategic decisions.
Technical and Architectural Expertise:
Architect and oversee data platforms, including SQL Server, Snowflake, and Power BI, ensuring optimal performance, scalability, and governance.
Lead initiatives in Data Architecture, Data Modeling, and Data Warehouse (DWH) development, tailored to alternative investment strategies.
Evaluate emerging technologies, such as big data and advanced analytics tools, and recommend their integration into client solutions.
Champion data quality, integrity, and security, aligning with compliance standards in private equity and alternative investments.
Performance & Metrics:
Define and monitor KPIs to measure team performance and project success, ensuring timely delivery and measurable impact.
Collaborate with stakeholders to refine reporting, dashboarding, and visualization for decision support.
Governance & Compliance:
Establish robust data governance frameworks in partnership with client stakeholders.
Ensure adherence to regulatory requirements impacting private markets investments, including fund accounting and compliance
What's on offer
Competitive and above-market salary.
Flexible hybrid work schedule with tools for both office and remote productivity.
Hands-on exposure to cutting-edge technology and global financial markets.
Opportunity to collaborate directly with international teams in New York and London.
Candidate Profile
Experience:
15+ years of progressive experience in program or project management within the capital markets and financial services sectors.
Demonstrated expertise in SQL Server, Snowflake, Power BI, ETL processes, and Azure Cloud Data Platforms.
Hands-on experience with big data technologies and modern data architecture.
Proven track record in delivering projects emphasizing data quality, integrity, and accuracy.
Deep understanding of private markets, including areas such as private equity, private credit, CLOs, compliance, and regulations governing alternative investments.
Leadership & Collaboration:
Exceptional problem-solving skills and decision-making abilities in high-pressure, dynamic environments.
Experience leading multi-disciplinary teams to deliver large-scale data initiatives.
Strong client engagement and communication skills, fostering alignment and trust with stakeholders.
Preferred Certifications:
Relevant certifications (e.g., CFA, Snowflake Certified Architect, or Microsoft Power BI Certified).
Education
Bachelor's degree in computer science, IT, Finance, Economics, or a related discipline. Advanced degrees (MBA, MS in Computer Science, or related fields) preferred.","Architect, Snowflake Certified Architect, snowflake, Azure Cloud Data Platforms, CFA, Power Bi, SQL Server"
Architect - Enterprise Data Operations,Pepsico india,8-16 Years,,Hyderabad,Food and Beverage,"Responsibilities
Complete conceptual, logical and physical data models for any supported platform, including SQL Data Warehouse, EMR, Spark, DataBricks, Snowflake, Azure Synapse or other Cloud data warehousing technologies.
Governs data design/modeling documentation of metadata (business definitions of entities and attributes) and constructions database objects, for baseline and investment funded projects, as assigned.
Provides and/or supports data analysis, requirements gathering, solution development, and design reviews for enhancements to, or new, applications/reporting.
Supports assigned project contractors (both on- & off-shore), orienting new contractors to standards, best practices, and tools.
Contributes to project cost estimates, working with senior members of team to evaluate the size and complexity of the changes or new development.
Ensure physical and logical data models are designed with an extensible philosophy to support future, unknown use cases with minimal rework.
Develop a deep understanding of the business domain and enterprise technology inventory to craft a solution roadmap that achieves business objectives, maximizes reuse.
Partner with IT, data engineering and other teams to ensure the enterprise data model incorporates key dimensions needed for the proper management: business and financial policies, security, local-market regulatory rules, consumer privacy by design principles (PII management) and all linked across fundamental identity foundations.
Drive collaborative reviews of design, code, data, security features implementation performed by data engineers to drive data product development.
Assist with data planning, sourcing, collection, profiling, and transformation.
Create Source To Target Mappings for ETL and BI developers.
Show expertise for data at all levels: low-latency, relational, and unstructured data stores; analytical and data lakes; data str/cleansing.
Partner with the Data Governance team to standardize their classification of unstructured data into standard structures for data discovery and action by business customers and stakeholders.
Support data lineage and mapping of source system data to canonical data stores for research, analysis and productization.
Qualifications
8+ years of overall technology experience that includes at least 4+ years of data modeling and systems architecture.
3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
4+ years of experience developing enterprise data models.
Experience in building solutions in the retail or in the supply chain space.
Expertise in data modeling tools (ER/Studio, Erwin, IDM/ARDM models).
Experience with integration of multi cloud services (Azure) with on-premises technologies.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience with at least one MPP database technology such as Redshift, Synapse, Teradata or SnowFlake.
Experience with version control systems like Github and deployment & CI tools.
Experience with Azure Data Factory, Databricks and Azure Machine learning is a plus.
Experience of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).","snowflake, Data Operations, Data Warehousing, Data Analytics, Azure Databricks, Azure Machine Learning"
Architect - Fieldglass Data Analytics,Pepsico india,4-6 Years,,Hyderabad,Food and Beverage,"Responsibilities:
Lead team in the expansion new module of Fieldglass to roll out to all countries ensuring that all suppliers enroll and meet the expectations of program requirements
Lead the team to on time deployment of program expansion and meet project deadlines
Oversee the overall change management plan for effective communication to IT leadership and stakeholders globall
Responsible for leading Fieldglass implementations and maintain program excellence in all countries
Lead the managed service provider supporting PepsiCo IT's external labor management program is following Pepsico Policies, Procedures, Contractual commitments and other requirements
Report all results to Program lead and develop remediate plans
Lead the development and oversight of quarterly audits on the program to ensure policy and procedure adherence set forth in the program manual and ensure effective remediation plans are implemented where needed
Monitor, evaluate and report on all relevant service and supplier performance metrics including contract SLAs and KPIs and remediation plans where needed
Working with IT leadership and key PepsiCo stakeholders on program feedback to continuously improve the user experience
Develop governance structure needed for overall strategic goal of supplier consolidation and configuration changes to ensure global process consistency
Escalation point for all program related issues to support analyst
Lead and develop automation on existing processes for efficiency and cost savings
Manage all configuration changes globally to meet stakeholder needs
Dotted line manager experience with country analysts
Ensure MSP is meeting contractual commitments and cost savings goals and remediate when needed
Reporting, Analytics & Insights
Develop and execute productivity initiative to meet team's goals
Lead development and design of analytics dashboards used to present data to executive committees and it leadership
Qualifications
Qualifications
Bachelor's Degree wit h9- 11 years of IT experience within or interacting with IT or Master's Degree with
4-6 years demonstrated experience working with complex global commercial IT contracts and outsourcing constructs and agreements
4-6 years demonstrated experience providing support to global Information Technology groups and organizations
4-6 years demonstrated experience with supplier and/or client relationship management; including supplier performance and governance responsibilities
Ability to interact with key stakeholders and communicate persuasively in a multi-functional environment
Ability to deliver credible insights through work products and communications
Ability to organize and prioritize work and meet deadlines through excellent time management and strong organizational and problem-solving skills
Ability to work independently with little direction
Professional image and adherence to standards consistent with company policies and procedures
Excellent analytical skills and attention to detail
Excellent MS Office skills, in particular Excel, Word, and PowerPoint
Ability to take instructions readily and to formulate work plans that will provide the best results to achieve the intended goals","fieldglass, Ms Office Skills, Msp, Reporting, Data Analytics"
Architect - Enterprise Data Operations,Pepsico india,12-18 Years,,Gurugram,Food and Beverage,"Responsibilities
Responsibilities:Qualifications
Independently complete conceptual, logical and physical data models for any supported platform, including SQL Data Warehouse, EMR, Spark, Data Bricks, Snowflake, Azure Synapse or other Cloud data warehousing technologies.
Governs data design/modeling documentation of metadata (business definitions of entities and attributes) and constructions database objects, for baseline and investment funded projects, as assigned.
Provides and/or supports data analysis, requirements gathering, solution development, and design reviews for enhancements to, or new, applications/reporting.
Supports assigned project contractors (both on- & off-shore), orienting new contractors to standards, best practices, and tools.
Advocates existing Enterprise Data Design standards; assists in establishing and documenting new standards.
Contributes to project cost estimates, working with senior members of team to evaluate the size and complexity of the changes or new development.
Ensure physical and logical data models are designed with an extensible philosophy to support future, unknown use cases with minimal rework.
Develop a deep understanding of the business domain and enterprise technology inventory to craft a solution roadmap that achieves business objectives, maximizes reuse.
Partner with IT, data engineering and other teams to ensure the enterprise data model incorporates key dimensions needed for the proper management: business and financial policies, security, local-market regulatory rules, consumer privacy by design principles (PII management) and all linked across fundamental identity foundations.
Drive collaborative reviews of design, code, data, security features implementation performed by data engineers to drive data product development.
Assist with data planning, sourcing, collection, profiling, and transformation.
Create Source To Target Mappings for ETL and BI developers.
Show expertise for data at all levels: low-latency, relational, and unstructured data stores; analytical and data lakes; data streaming (consumption/production), data in-transit.
Develop reusable data models based on cloud-centric, code-first approaches to data management and cleansing.
Partner with the data science team to standardize their classification of unstructured data into standard structures for data discovery and action by business customers and stakeholders.
Support data lineage and mapping of source system data to canonical data stores for research, analysis and productization.
Qualifications:
12+ years of overall technology experience that includes at least 6+ years of data modelling and systems architecture.
6+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
6+ years of experience developing enterprise data models.
6+ years in cloud data engineering experience in at least one cloud (Azure, AWS, GCP).
6+ years of experience with building solutions in the retail or in the supply chain space.
Expertise in data modelling tools (ER/Studio, Erwin, IDM/ARDM models).
Fluent with Azure cloud services. Azure Certification is a plus.
Experience scaling and managing a team of 5+ data modelers
Experience with integration of multi cloud services with on-premises technologies.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience with at least one MPP database technology such as Redshift, Synapse, Teradata, or Snowflake.
Experience with version control systems like GitHub and deployment & CI tools.
Experience with Azure Data Factory, Databricks and Azure Machine learning is a plus.
Experience of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring, hiring and scaling data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Differentiating Competencies Required
Ability to work with virtual teams (remote work locations); lead team of technical resources (employees and contractors) based in multiple locations across geographies
Lead technical discussions, driving clarity of complex issues/requirements to build robust solutions
Strong communication skills to meet with business, understand sometimes ambiguous, needs, and translate to clear, aligned requirements
Able to work independently with business partners to understand requirements quickly, perform analysis and lead the design review sessions.
Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
Places the user in the center of decision making.
Teams up and collaborates for speed, agility, and innovation.
Experience with and embraces agile methodologies.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.","enterprise data management, Data Modeling, Data Lake, System Architectecture, Data Warehousing, Data Analytics, Azure Cloud Services"
Architect - Enterprise Data Operations,Pepsico india,12-18 Years,,Gurugram,Food and Beverage,"Responsibilities
Responsibilities:Qualifications
Independently complete conceptual, logical and physical data models for any supported platform, including SQL Data Warehouse, EMR, Spark, Data Bricks, Snowflake, Azure Synapse or other Cloud data warehousing technologies.
Governs data design/modeling documentation of metadata (business definitions of entities and attributes) and constructions database objects, for baseline and investment funded projects, as assigned.
Provides and/or supports data analysis, requirements gathering, solution development, and design reviews for enhancements to, or new, applications/reporting.
Supports assigned project contractors (both on- & off-shore), orienting new contractors to standards, best practices, and tools.
Advocates existing Enterprise Data Design standards; assists in establishing and documenting new standards.
Contributes to project cost estimates, working with senior members of team to evaluate the size and complexity of the changes or new development.
Ensure physical and logical data models are designed with an extensible philosophy to support future, unknown use cases with minimal rework.
Develop a deep understanding of the business domain and enterprise technology inventory to craft a solution roadmap that achieves business objectives, maximizes reuse.
Partner with IT, data engineering and other teams to ensure the enterprise data model incorporates key dimensions needed for the proper management: business and financial policies, security, local-market regulatory rules, consumer privacy by design principles (PII management) and all linked across fundamental identity foundations.
Drive collaborative reviews of design, code, data, security features implementation performed by data engineers to drive data product development.
Assist with data planning, sourcing, collection, profiling, and transformation.
Create Source To Target Mappings for ETL and BI developers.
Show expertise for data at all levels: low-latency, relational, and unstructured data stores; analytical and data lakes; data streaming (consumption/production), data in-transit.
Develop reusable data models based on cloud-centric, code-first approaches to data management and cleansing.
Partner with the data science team to standardize their classification of unstructured data into standard structures for data discovery and action by business customers and stakeholders.
Support data lineage and mapping of source system data to canonical data stores for research, analysis and productization.
Qualifications:
12+ years of overall technology experience that includes at least 6+ years of data modelling and systems architecture.
6+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.
6+ years of experience developing enterprise data models.
6+ years in cloud data engineering experience in at least one cloud (Azure, AWS, GCP).
6+ years of experience with building solutions in the retail or in the supply chain space.
Expertise in data modelling tools (ER/Studio, Erwin, IDM/ARDM models).
Fluent with Azure cloud services. Azure Certification is a plus.
Experience scaling and managing a team of 5+ data modelers
Experience with integration of multi cloud services with on-premises technologies.
Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.
Experience with at least one MPP database technology such as Redshift, Synapse, Teradata, or Snowflake.
Experience with version control systems like GitHub and deployment & CI tools.
Experience with Azure Data Factory, Databricks and Azure Machine learning is a plus.
Experience of metadata management, data lineage, and data glossaries is a plus.
Working knowledge of agile development, including DevOps and DataOps concepts.
Familiarity with business intelligence tools (such as PowerBI).
Skills, Abilities, Knowledge
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.
Proven track record of leading, mentoring, hiring and scaling data teams.
Strong change manager. Comfortable with change, especially that which arises through company growth.
Ability to understand and translate business requirements into data and technical requirements.
High degree of organization and ability to manage multiple, competing projects and priorities simultaneously.
Positive and flexible attitude to enable adjusting to different needs in an ever-changing environment.
Strong leadership, organizational and interpersonal skills; comfortable managing trade-offs.
Foster a team culture of accountability, communication, and self-management.
Proactively drives impact and engagement while bringing others along.
Consistently attain/exceed individual and team goals
Ability to lead others without direct authority in a matrixed environment.
Differentiating Competencies Required
Ability to work with virtual teams (remote work locations); lead team of technical resources (employees and contractors) based in multiple locations across geographies
Lead technical discussions, driving clarity of complex issues/requirements to build robust solutions
Strong communication skills to meet with business, understand sometimes ambiguous, needs, and translate to clear, aligned requirements
Able to work independently with business partners to understand requirements quickly, perform analysis and lead the design review sessions.
Highly influential and having the ability to educate challenging stakeholders on the role of data and its purpose in the business.
Places the user in the center of decision making.
Teams up and collaborates for speed, agility, and innovation.
Experience with and embraces agile methodologies.
Strong negotiation and decision-making skill.
Experience managing and working with globally distributed teams.","enterprise data management, Data Modeling, Data Lake, System Architectecture, Data Warehousing, Data Analytics, Azure Cloud Services"
Architect - Enterprise Data Operations,Pepsico india,4-14 Years,,Gurugram,Food and Beverage,"Responsibilities
Active contributor to code development in projects and services.
Collaborate with a cross-functional team of application developers, operations engineers, architects to understand complex product requirements and translate them into automated solutions that you build.
Collaborate with colleagues to support and improve architecture, systems, processes, standards and tools.
Lead technical discussions to ensure solutions are designed for successful deployment, security, and high availability in the cloud
Design, implement, and maintain server, storage, network, and security infrastructure as code.
Build reusable pipelines for application deployments.
Write and maintain code for automating the creation of scalable/resilient systems/infrastructure with a focus on immutability and containers.
Develop, implement, and test automated data backup and recovery, and disaster recovery procedures across multiple regions.
Write and maintain clear, concise documentation, runbooks and operational standards including infrastructure diagrams.
Assist development teams in the creation and understanding of automated application configurations.
Ensure all solutions are properly monitored and instrumented.
Troubleshoot and resolve complex issues in development, test and production environments.
Design and deploy scalable, highly available, and fault tolerant distributed systems.
Continuously identify, adopt, & refine best practices.
Qualifications
Bachelor's Degree in Cyber Security, Information Technology, Computer Science or related field or related practical experience.
4+ years of experience in Software and/or Infrastructure, with a desired 3+ years in a relevant cloud, Kubernetes, automation development, and/or orchestration positions.
2+ years of hands-on experience on Azure leveraging number PAAS services offered by the platform.
Requires excellent problem solving and analytic skills to effectively address the needs of customers, including experience handling problem escalations and notifications.
Experience working in GCP, AWS, PCF, Azure, or other cloud-based technologies.
Experience with Terraform, Ansible, Salt or similar automation tools are a benefit as we drive towards Infrastructure as Code (IaC).
Experience with SCM and DevOps tool suites; examples include Git, Sonar, Jenkins, Artifactory, HashiCorp Packer etc.
Experience with containers, docker, Kubernetes, serverless functions.
Experience with Linux (RHEL/CentOS) and Windows system administration.
Programming / Scripting background with knowledge of Python, PowerShell, Groovy.
Hands-on experience with Azure services (Proficiency with Azure DevOps, ARM Templates, Azure Policy, Azure CLI, Azure Rest API).
Experience provisioning, operating, monitoring, troubleshooting and maintaining systems running in the cloud.
Multi-year experience in application development and configuration automation.
Understanding of application, server, and network security.
Understanding of immutable infrastructure and infrastructure as code concepts.
Working knowledge of Agile/Scrum, experience leading continuous integration and continuous delivery concepts and frameworks.
Experience in Firewall and Load Balancing technology; Palo Alto, F5, Citrix Netscaler is a plus.
Cloud Certifications (Azure Solutions Architect, DevOps Engineer, or other cloud professional certifications) is a plus","enterprise data management, Gcp, Linux, Devops, Agile Scrum, Cloud Infrastructure, Python"
Architect - Salesforce Data Cloud,Pepsico india,3-7 Years,,Hyderabad,Food and Beverage,"Responsibilities
Oversee Salesforce Data Cloud environments across development, staging, and production.
Define best practices for environment setup, security, and governance.
Manage data pipelines, ingestion processes, and harmonization rules for efficient data flow.
Establish role-based access control (RBAC) to ensure data security and compliance.
Monitor data processing jobs, ingestion performance, and data harmonization.
Ensure compliance with GDPR, CCPA, and other data privacy regulations
Establish CI/CD pipelines using tools like Azure DevOps
Implement version control and automated deployment strategies for Data Cloud configurations
Define a data refresh strategy for lower environments to maintain consistency.
Qualifications
Mandatory Technical Skills
Extensive experience in setting up, maintaining, and troubleshooting CI/CD pipelines for Salesforce apps.
Strong knowledge of Azure DevOps tools and pipeline creation, with proficiency in automation scripting (primarily YAML, with additional languages as needed).
Hands-on experience with SFDX, Azure Repos, and automated release deployments for Salesforce.
Expertise in implementing GIT branching strategies using VS Code integrated with Salesforce CLI tool.
Mandatory Skills
Proficiency in Salesforce Data Cloud architecture and best practices.
Experience with data lake, Snowflake, or cloud-based data storage solutions.
Familiarity with OAuth, authentication mechanisms, and data security standards.
Salesforce Data Cloud Consultant Certification","Salesforce, snowflake, Cloud Architecture, Data Lake, Oauth, Yaml, Git, Cloud Data"
Data Management+DWH+Ananalytics+Architect exp+Synapse+ADF+ Data Migration,Coders Brain Technology Private Limited,12-16 Years,INR 20 - 30 LPA,"Bengaluru, Pune, Noida",Login to check your skill match score,"Job Description-
Position/Role- Data Management+DWH+Ananalytics+Architect exp+Synapse+ADF+ Data Migration
Experience: 12-16 yrs
Location Chennai/Bangalore/Mumbai/Pune/Noida
Notice Period- Immediate- 20 days
Rate- 30 lpa
Job Overview-
Experience: 12 to 15 years in Data Management, Data Warehousing, and Analytics.
Data Solutions: 4 to 5 years in architecting and implementing data solutions.
Azure Expertise: 3 years in implementing data solutions using Azure Data Factory and MS Synapse.
Azure Functions: Understanding of Azure Functions, including their operation and migration processes.
Programming: Proficiency in Python scripting and exposure to Java API development.
Azure Synapse Analytics: 1 to 2 years of experience is a plus.
ADF Integration: Experience in installing and configuring Azure Data Factory integration runtimes and linked services.
Big Data: Hands-on experience with big data platform tool selection
Data Migration: 2 years of experience in data migrations to Azure using Data Box or Data Migration Services.
Microsoft Fabric: Any commercial exposure to Fabric is highly desirable.","Ananalytics, Architect, exp, Synapse, Data Migration, Data Management, Dwh, Adf"
Data Platform and Infrastructure Architect,Arabelle Solutions,8-10 Years,,"Bengaluru, India",Login to check your skill match score,"Arabelle Solutions offers a broad portfolio of turbine island technologies and services that are used in more than a third of nuclear power plants globally helping customers across the world deliver reliable power as they transition to a lower-carbon future. The Arabelle steam turbine is the most advanced of its kind and the company provides turbine island lifecycle support solutions for all nuclear reactor types - improving power output, reducing environmental footprint, and lowering operational cost. Arabelle Solutions has around 3,300 employees across 16 countries and is a subsidiary of EDF Group.
At Arabelle Solutions, we're proud to design services and solutions that are generating power not just for today and tomorrow but for generations to come. You'll work alongside passionate bright minds. We offer a broad range of opportunities for those eager to build tomorrow's world. We believe a supportive culture is key to reach common goals. Diversity and an inclusive mindset makes us and our business stronger.
www.arabellesolutions.com
Data Platform And Infrastructure Architect
We are looking to onboard a Senior data platform architect as part of Arabelle Solutions Data & Analytics team to lead the architecture, administration and cost management of our data platforms. This role is a combination of technical leadership and hands-on technical expertise. The data platform architect will help define the overall data platform strategy, modernization efforts and will drive scalability, maintainability, cost management & monitoring initiatives.
Roles And Responsibilities
Primarily responsible for the design, development, administration, and maintenance of the data platforms
Strictly maintain access management, to ensure the integrity, security, and compliance of our data across all initiatives.
Build procedures and utilities to track performance and cost for all data engineering infrastructure, perform data platform administration, and enforce cost management best practices.
Investigate and incorporate modern data services and technologies to boost the functionality and efficiency of our data platform, in line with our strategic objectives.
Review and suggest enhancements to our data solutions with a focus on cost efficiency and ease of use, making our data platform scalable, accessible and beneficial to a wide range of user groups.
Work with the Data Architect to help build a next generation data architecture that is resilient to change, maintainable, and scalable
Ensure data platforms adheres to EU data security & privacy practices, laws and regulations
Lead data platform upgrades and also enable High Availability (HA) and Disaster Recovery (DR) capabilities for the data platforms
Manage Infrastructure and Platform Security & Vulnerability
Research, recommend and implement the appropriate data platform tools, software, applications, and systems to support data technology goals.
Design and manage data platform improvement and growth projects.
Execute platform configuration and performance tuning.
Identify inefficiencies and gaps in the current data platform and leverage solutions to ensure data standards
Diagnose and work with IT and data personnel to resolve platform access and performance issues.
Install and configure relevant components to ensure data platform access.
Monitor system details within the data platform, including platform storage and execution time, and implement efficiency improvements.
Acts as a team lead or project manager, directs the activities of matrixed project team members and contractors.
Minimum Qualifications Required
Bachelor's Degree from an accredited university in a relevant technical field including, but not limited to computer science, computer engineering, information systems, mathematics, or systems engineering.
Minimum 8 year(s) of experience in Data Platform architecture and managing multiple data platforms.
Technical experience with designing, building, installing, configuring, and supporting data platforms including Talend, HVR, AWS Aurora RDS PostgreSQL & Tableau.
Good understanding of Cloud Infrastructure and networking (AWS or Google)
Good knowledge of applicable data privacy practices, laws and regulations.
Exceptional analytical, conceptual, and problem-solving abilities.
Strong written and oral communication skills.
Demonstrated leadership ability in the data realm.
Strong presentation and interpersonal skills.
Experience working in a team-oriented, collaborative environment.
Demonstrated ability to interpret technical information and apply it to corporate goals.
This is an LPB (Lead Professional Band) Position.","Data Platform architecture, HVR, RDS, Networking, AWS Aurora, PostgreSQL, Tableau, Cloud Infrastructure, Talend"
Enterprise Architect - Airflow & Data Engineering,Quest Global,Fresher,,"Hyderabad, India",Login to check your skill match score,"Job Requirements
Overall Expectations
Application Architecture
High level and Low-level design
Code Quality responsibility
CI/CD Best practices
Hands-on & implementation
Specific Areas of experience:
IoT Experience
Scale
Data pipelines
MQTT
Data base experience
Partition
Sharding
Data lake
Technology
Java
Python
Kubernetes
Micro service Architecture
Work Experience
Overall Expectations
Application Architecture
High level and Low-level design
Code Quality responsibility
CI/CD Best practices
Hands-on & implementation
Specific Areas of experience:
IoT Experience
Scale
Data pipelines
MQTT
Data base experience
Partition
Sharding
Data lake
Technology
Java
Python
Kubernetes
Micro service Architecture","Sharding, Data base experience, CI CD Best practices, Micro service Architecture, Data pipelines, Hands-on implementation, IoT Experience, SCALE, Java, Partition, Mqtt, Application Architecture, Data Lake, Python, Kubernetes"
Presales Solutions Architect - Modern Data Center,AHEAD,Fresher,,"Gurugram, Gurugram, India",Login to check your skill match score,"AHEAD's Specialist Solutions Engineers are the technical experts that collaborate with our AHEAD account teams to help identify, qualify, and build solutions that support our customer's Digital Business transformation.
The AHEAD Modern Datacenter Specialist Solutions Engineer (SSE) will be focused on Datacenter technologies, including storage (block & file), compute, virtualization, and data protection . The SSE is considered a subject matter expert in these areas with responsibility for selling and designing complex solutions. The SSE is also considered an organizational thought leader for their specialty area.
Responsibilities
Active participation in the AHEAD Modern Datacenter SSE team, advisory and delivery teams.
Working with the AHEAD partner management team and directly with selected strategic vendors to maintain technical and sales relationships.
Be a thought leader within the Solutions Engineering organization, responsible for designing, architecting, and at times consulting for AHEAD clients.
Build skills, relationships, and industry credibility across many segments to drive product, services, and consulting sales.
Support the sales teams during engagements by providing advice and solutions or proposals optimized to meet customer technical and business requirements.
Develop relationships with other Modern Datacenter SSE team members, leaders, and partners in support of sales objectives.
Strategize and execute technical sales calls.
Complete required pre-sales documentation (configurations, pricing, services, presentations, justifications, etc.) quickly and accurately.
Qualify sales opportunities in terms of customer technical requirements, decision-making process, and funding.
Present and market the design and value of proposed AHEAD solutions and business justification to customers, prospects, and AHEAD management.
Participate in the mentorship of entry-level team members.
Possess strong, detailed product/technology/industry knowledge.
Knowledge of job-associated software and applications.
Qualifications
Expertise in Storage (Block & File), Compute, and Virtualization/VMware Cloud Foundation (VCF).
Data Protection experience with Dell, Rubrik, Commvault, etc.
Familiarity with Cisco UCS, Dell MX, PowerScale, PowerFlex and hyperconverged compute platforms (VxRail, Nutanix, Azure Stack HCI, OpenShift-V).
Understanding of Datacenter automation tools like Ansible.
Basic knowledge of cloud platforms like AWS and Azure.
VMware and Virtualization technology expertise.
Vendor experience with Dell, NetApp, Vast Data, Cisco, etc.
Professional communication, presentation, analytical, and problem-solving skills
Experience working as a technical lead in a pre-sales or sales campaign.
Ability to work under critical conditions and influence others to achieve results.
Strong interpersonal skills with excellent presentation skills.
Must be independent, self-motivated, a self-starter, and possess a good working
attitude.
Able to work well within a team and partner environment. Customer-focused and results-driven.
Other Skills
Innovation Mindset: Ability to identify and leverage emerging Datacenter technologies to drive innovative solutions that meet evolving client needs.
Automation Expertise: Advanced understanding of automation frameworks and scripting to optimize Datacenter operations and reduce manual effort.
Problem Solving: Strong analytical skills with a focus on troubleshooting complex Datacenter environments, including storage, compute, and networking issues.
Consultative Approach: Ability to engage with clients as a trusted advisor, guiding them through the decision-making process with clear, strategic recommendations.
Collaboration Skills: Proven track record of working effectively across teams, including sales, engineering, and vendor partners, to deliver cohesive solutions.
Continuous Learning: Commitment to staying updated on the latest industry trends, certifications, and best practices related to Datacenter technologies.
Security Awareness: Knowledge of Datacenter security best practices, ensuring that proposed solutions adhere to compliance and regulatory standards.","Azure Stack HCI, Storage Block File Compute, powerflex, VxRail, OpenShift-V, VMware and Virtualization technology, PowerScale, Dell MX, Cisco Ucs, Nutanix"
Generative AI Architect (Big Data & Java Expertise),eProSoft,8-10 Years,,"Hyderabad, India",Login to check your skill match score,"Generative AI Architect (Big Data & Java Expertise)
Hybrid role
Hyderabad, INDIA
We are seeking a skilled and innovative Generative AI Architect with a strong background in Big Data and Java to lead our AI-driven initiatives. The ideal candidate will design and implement cutting-edge generative AI models and systems that leverage large datasets, while ensuring seamless integration with Java-based platforms. This role requires a deep understanding of AI/ML technologies, data architecture, and software engineering to drive the next generation of AI-powered solutions.
Key Responsibilities:
Lead the design and architecture of generative AI models and systems that utilize large-scale data. Collaborate with data scientists, ML engineers, and product teams to create scalable and efficient AI-driven platforms.
Leverage Big Data technologies (Hadoop, Spark, etc.) to handle massive datasets and ensure efficient data ingestion, processing, and storage for AI applications.
Architect, develop, and integrate AI models into Java-based enterprise applications, ensuring smooth operation within existing systems.
Work with teams to develop generative AI models (e.g., GPT, GANs) and optimize them for performance, scalability, and business needs. Ensure models are trained on relevant datasets and continuously updated.
Collaborate with cross-functional teams, including data engineering, software development, and business stakeholders, to drive AI initiatives. Mentor junior engineers and provide guidance on AI and Big Data architecture.
Architect and deploy AI systems in cloud environments (AWS, GCP, Azure) and ensure robust infrastructure for AI model deployment and scalability.
Stay updated with the latest trends and advancements in AI, Big Data, and Java technologies, applying this knowledge to enhance systems and processes.
Required Qualifications:
8+ years of experience in software engineering, AI, or data architecture roles.
Proven experience in designing and deploying AI models, particularly generative AI (e.g., GPT, GANs, VAEs).
Strong background in Big Data technologies such as Hadoop, Spark, Kafka, and NoSQL databases.
Extensive hands-on experience with Java, including multithreading, JVM tuning, and integration with large-scale systems.
Experience with cloud platforms (AWS, GCP, Azure) for AI model deployment and scaling.
Expertise in machine learning frameworks (TensorFlow, PyTorch, Keras).
Strong understanding of data pipelines, ETL processes, and distributed systems.
Proficiency in AI algorithms and techniques (e.g., NLP, computer vision, deep learning).
Knowledge of microservices architecture, RESTful APIs, and containerization (Docker, Kubernetes).
Bachelor's or Master's degree in Computer Science, Engineering, Data Science, or a related field.
Please submit your resume on LinkedIn (or) you can share your resume with [HIDDEN TEXT]","microservices architecture, NoSQL databases, Generative AI, ETL processes, Keras, Java, Tensorflow, Hadoop, Kafka, Deep Learning, AWS, Pytorch, Kubernetes, Azure, Docker, Gcp, Big Data, Computer Vision, Nlp, Spark, Restful Apis"
