{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30553,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update -y\n!apt-get install -y \\\nlibglib2.0-0 \\\nlibnss3 \\\nlibdbus-glib-1-2 \\\nlibgconf-2-4 \\\nlibfontconfig1 \\\nlibvulkan1 \\\ngconf2-common \\\nlibwayland-server0 \\\nlibgbm1 \\\nudev \\\nlibu2f-udev \n!apt --fix-broken install -y  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -P /tmp https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chrome-linux64.zip\n!unzip /tmp/chrome-linux64.zip -d /usr/bin/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/usr/bin/chrome-linux64/chrome --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -P /tmp https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chromedriver-linux64.zip\n!unzip /tmp/chromedriver-linux64.zip -d /usr/bin/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/usr/bin/chromedriver-linux64/chromedriver --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install -y python3-selenium\n!pip install selenium==3.141.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyGithub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom github import Github\nimport datetime\nimport smtplib\nfrom email.message import EmailMessage\nimport pytz\nimport pandas as pd\nimport json\nimport github\nfrom kaggle_secrets import UserSecretsClient","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize Selenium WebDriver","metadata":{}},{"cell_type":"code","source":"# Add configurable options\ndef add_driver_options(options):\n    chrome_options = Options()\n    for opt in options:\n        chrome_options.add_argument(opt)\n    return chrome_options\n\n# Function to get WebDriver object\ndef get_driver() -> webdriver.Chrome:\n    driver_config = {\n        \"options\": [\n            \"--headless\",\n            \"--no-sandbox\",\n            \"--start-fullscreen\",\n            \"--allow-insecure-localhost\",\n            \"--disable-dev-shm-usage\",\n            \"user-agent=Chrome/116.0.5845.96\"\n        ],\n    }\n    CHROME_BINARY_LOCATION = \"/usr/bin/chrome-linux64/chrome\"\n    CHROMEDRIVER_BINARY_LOCATION = \"/usr/bin/chromedriver-linux64/chromedriver\"\n    options = add_driver_options(driver_config[\"options\"])\n    options.binary_location = CHROME_BINARY_LOCATION\n    driver = webdriver.Chrome(\n        executable_path=CHROMEDRIVER_BINARY_LOCATION,\n        options=options)\n    return driver","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to scrap job details and return data as a DataFrame\ndef get_source_data(job_role_lst: list, cnt: int) -> pd.DataFrame:\n    fnl_lst = []\n    key = 1\n\n    # Iterating through job roles\n    for job_role in job_role_lst:\n        lst = job_role.split(' ')\n        str_query = \"+\".join(lst)\n        page = f'https://www.foundit.in/srp/results?query=\"{str_query}\"'\n        url = page.encode('ascii', 'ignore').decode('unicode_escape')\n\n        driver = get_driver()\n        driver.get(url)\n        count = 0\n\n        # Scraping job details\n        while count <= cnt:\n            x_pth = \"/html\\\n                    /body\\\n                    /div[@id='srpThemeDefault']\\\n                    /div[@class='srpContainer']\\\n                    /div[@id='srpContent']\\\n                    /div[@class='srpCardContainer']\\\n                    /div[@class='srpResultCard']\\\n                    /div\"\n            elements = (driver.find_elements(By.XPATH, x_pth))\n\n            for element in elements:\n                try:\n                    job_dict = {}\n                    job_title = element.find_element(By.CLASS_NAME, \"jobTitle\").text\n                    company_name = element.find_element(By.CLASS_NAME, \"companyName\").text\n                    skills_str = ''\n\n                    # Extracting skills\n                    for i in element.find_elements(By.CLASS_NAME, \"skillTitle\"):\n                        skill = i.text\n                        if skill != '':\n                            skills_str += skill + ','\n\n                    sub_element = element.find_element(By.CLASS_NAME, \"cardBody\")\n                    job_type_str = sub_element.find_element(By.XPATH, \"div[1]/div[@class='details']\").text\n                    location_str = sub_element.find_element(By.XPATH, \"div[2]/div[@class='details']\").text\n                    experience_str = sub_element.find_element(By.XPATH, \"div[3]/div[@class='details']\").text\n\n                    # Storing job details in dictionary\n                    job_dict['key'] = key\n                    job_dict['job_role'] = job_role\n                    job_dict['job_title'] = job_title\n                    job_dict['company_name'] = company_name\n                    job_dict['skills'] = skills_str[:-1]\n                    job_dict['job_type'] = job_type_str\n                    job_dict['location'] = location_str\n                    job_dict['experience'] = experience_str\n                    fnl_lst.append(job_dict)\n                    count += 1\n                    key += 1\n\n                    if count == cnt:\n                        break\n                except:\n                    pass\n\n            try:\n                element.find_element(By.CLASS_NAME, \"mqfisrp-right-arrow\").click()\n            except:\n                break\n\n        driver.quit()\n\n    # Converting list of dictionaries to DataFrame\n    df = pd.DataFrame(fnl_lst)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get GitHub repository object\ndef get_repository() -> github.Repository.Repository:\n    access_token = UserSecretsClient().get_secret('access_token')\n    repo_str = UserSecretsClient().get_secret('repo_str')\n    g = Github(access_token)\n    repo = g.get_repo(repo_str)\n    return repo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to upload DataFrame as CSV file to GitHub\ndef upload_dataframe_to_github(df: pd.DataFrame, folder: str):\n    branch = 'master'\n    repo = get_repository()\n    csv_content = df.to_csv(index=False)\n\n    # Uploading CSV file to GitHub\n    if folder in ['source', 'consumption']:\n        IST = pytz.timezone('Asia/Kolkata')\n        dt_tm = str(datetime.datetime.now(IST))[:19].replace(' ', '-')\n        file_str = f'{dt_tm}.csv'\n        repo.create_file(folder+'/'+file_str, 'upload '+folder+' level data', csv_content, branch=branch)\n        print(folder+' layer file uploaded!')\n    elif folder == 'reference':\n        file_str = 'indian_cities.csv'\n        try:\n            repo.create_file(folder+'/'+file_str, 'create indian cities data', csv_content, branch=branch)\n            print(folder+' layer file uploaded!')\n        except:\n            file = repo.get_contents(folder+'/'+file_str)\n            repo.update_file(folder+'/'+file_str,'update indian cities data',csv_content,file.sha)\n            print(folder+' layer file uploaded!')\n    else:\n        raise Exception(\"folder is either 'source' or 'consumption' or 'reference'!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to send status via email\ndef sendMail(reciever_id: str, exception=None):\n    # Configuration\n    sender_id = UserSecretsClient().get_secret('sender_id')\n    pass_word = UserSecretsClient().get_secret('pass_word')\n    git_link = UserSecretsClient().get_secret('git_link')\n    IST = pytz.timezone('Asia/Kolkata')\n    time_now = datetime.datetime.now(IST)\n\n    # Creating email message\n    message = \"STATUS:\\n\"\n    mail = EmailMessage()\n    mail['From'] = sender_id\n    mail['To'] = reciever_id\n\n    # Handling status based on exception\n    if exception is None:\n        mail['Subject'] = \"Extraction done \" + str(time_now.strftime(\"at %H:%M:%S, on %d/%m/%Y\"))\n        message += \"Data extraction, cleanup, and loading done successfully \" + str(time_now.strftime(\"at %H:%M:%S, on %d %B %Y (%A)\")) + \".\\n\"\n        message += \"Files uploaded at \" + git_link + \".\\n\"\n    else:\n        mail['Subject'] = \"Exception occured \" + str(time_now.strftime(\"at %H:%M:%S, on %d/%m/%Y\"))\n        message += \"During the process of data extraction an exception occured.\\n\"\n        message += \"Exception: \" + str(exception) + \".\\n\"\n        message += \"Visit https://www.kaggle.com/ to resolve the issue.\\n\"\n\n    message += \"\\nSent from kaggle notebook.\"\n    mail.set_content(message)\n\n    # Sending email\n    server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n    server.starttls()\n    server.login(sender_id, pass_word)\n    server.send_message(mail)\n    server.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get cities in India as a DataFrame through web scraping\ndef get_cities_indian() -> pd.DataFrame:\n    cities = []\n    page = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-India-2033033'\n    url = page.encode('ascii', 'ignore').decode('unicode_escape')\n    driver = get_driver()\n    driver.get(url)\n    div_element = driver.find_element(By.CLASS_NAME, 'reading-channel')\n    lists = div_element.find_elements(By.TAG_NAME, 'li')\n\n    # Extracting city names\n    for list in lists:\n        txt = list.find_element(By.TAG_NAME, 'a').text.lower()\n        cities.append(txt)\n\n    df = pd.DataFrame(cities, columns=[\"city\"])\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scraping and loading source data\ndef scrap_and_load():\n    # List of job roles\n    job_roles = [\n        \"Data Engineer\",\n        \"Data Analyst\",\n        \"Data Architect\",\n        \"Data Scientist\",\n        \"Machine Learning Engineer\"\n    ]\n    # Get source data (job postings)\n    df = get_source_data(job_role_lst=job_roles, cnt=500)\n    print('web scraping done!')\n    # Upload data to GitHub\n    upload_dataframe_to_github(df=df, folder='source')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to scrap Indian cities and update reference file in GitHub repo\ndef update_cities_data():\n    # Get list of indian cities as dataframe\n    df = get_cities_indian()\n    # Upload dataframe as csv file in GitHub repo\n    upload_dataframe_to_github(df=df, folder='reference')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through the files and get the most recent one\ndef get_new_file(folder:str) -> str:\n    if folder not in ['source', 'consumption', 'target', 'reference']:\n        raise Exception(\"folder is either 'source' or 'consumption' or 'target' or 'reference'\")\n\n    repo = get_repository()\n    contents = repo.get_contents(folder)\n    path = ''\n\n    # Finding the most recent file\n    for c in contents:\n        if path < c.path:\n            path = c.path\n\n    return path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get file path\ndef get_file_path(folder:str, path_type:str) -> str:\n    github_url = 'https://github.com'\n    repo_str = UserSecretsClient().get_secret('repo_str')\n    branch = 'master'\n    file_path = get_new_file(folder=folder)\n    file_url = github_url + '/' + repo_str + '/blob/' + branch + '/' + file_path\n\n    # Returning URL based on path type\n    if path_type == 'url':\n        return file_url\n    elif path_type == 'raw':\n        return file_url.replace('github', 'raw.githubusercontent').replace('blob/', '')\n    else:\n        raise Exception(\"path_type is either 'url' or 'raw'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to convert reference cities CSV file to list\ndef cities_lst() -> list:\n    ref_raw_path = get_file_path(folder='reference', path_type='raw')\n    df = pd.read_csv(ref_raw_path)\n    lst = df.city.tolist()\n    return lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to extract cities from location string passed as parameter\ndef parse_location(location_str: str) -> str:\n    location = [i.strip().lower() for i in location_str.replace('/', ',').split(\",\")]\n    loc_lst = []\n\n    for loc in location:\n        if loc in cities:\n            loc_lst.append(loc)\n\n    lst = list(set(loc_lst)) # Remove duplicates\n    result_string = ','.join(lst)\n    return result_string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to parse experience string passed and return required experience integer value\ndef parse_experience(experience_str: str) -> int:\n    experience_str = experience_str.strip().lower()\n\n    if '-' in experience_str:\n        lst = [int(i) for i in experience_str.split(' ')[0].split('-')]\n        res = sum(lst) // len(lst)\n    else:\n        if 'fresher' in experience_str:\n            res = 0\n        else:\n            res = int(experience_str.split(' ')[0])\n\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process skills and return skills list\ndef process_skills(skill_str: str) -> str:\n    skill_lst = skill_str.lower().split(',')\n    job_roles = [\n        \"data engineer\",\n        \"data analyst\",\n        \"data architect\",\n        \"data scientist\",\n        \"machine learning engineer\"\n    ]\n\n    for i in job_roles:\n        for j in skill_lst:\n            if i in j:\n                skill_lst.remove(j)\n\n    result_string = ','.join(skill_lst)\n    return result_string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to parse job type string passed and return a list of expected values\ndef parse_job_type(job_type_str: str) -> str:\n    job_type_str = job_type_str.lower()\n    type_lst = []\n\n    if 'full' in job_type_str:\n        type_lst.append('full time')\n    if 'home' in job_type_str:\n        type_lst.append('work from home')\n    if 'contract' in job_type_str:\n        type_lst.append('contract job')\n    if 'remote' in job_type_str:\n        type_lst.append('remote job')\n    if 'part' in job_type_str:\n        type_lst.append('part time')\n\n    result_string = ','.join(type_lst)\n    return result_string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that replaces NaN with values based on columns\ndef replace_nan(df: pd.DataFrame) -> pd.DataFrame:\n    nan_values = {\n        'job_role' : '',\n        'job_title' : '',\n        'company_name' : '',\n        'experience' : 0,\n        'location' : '',\n        'skills' : '',\n        'job_type' : ''\n    }\n\n    for col, val in nan_values.items():\n        df[col] = df[col].fillna(val)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to clean data\ndef clean_data(df: pd.DataFrame) -> pd.DataFrame:\n    df = replace_nan(df)\n    df['job_role'] = df['job_role'].str.lower()\n    df['job_title'] = df['job_title'].str.lower()\n    df['company_name'] = df['company_name'].str.lower()\n    df['experience'] = df.apply(lambda x : parse_experience(x['experience']), axis=1)\n    df['location'] = df.apply(lambda x : parse_location(x['location']), axis=1)\n    df['skills'] = df.apply(lambda x : process_skills(x['skills']), axis=1)\n    df['job_type'] = df.apply(lambda x : parse_job_type(x['job_type']), axis=1)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to upload dictionary as JSON file to GitHub\ndef upload_dict_to_github(data_dict: dict, folder: str):\n    if folder == 'target':\n        branch = 'master'\n        repo = get_repository()\n        IST = pytz.timezone('Asia/Kolkata')\n        dt_tm = str(datetime.datetime.now(IST))[:19].replace(' ', '-')\n        file_str = f'{dt_tm}.json'\n        content=json.dumps(data_dict, indent=4)\n        repo.create_file(folder+'/'+file_str, 'upload target data', content, branch=branch)\n        print(folder+' layer file uploaded!')\n    else:\n        raise Exception(\"folder should be 'target'!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that cleans source dataset and uploads consumption file\ndef upload_consumption_file():\n    # Read recent source file into DataFrame\n    raw_path = get_file_path(folder='source', path_type='raw')\n    df_raw = pd.read_csv(raw_path)\n    # Clean DataFrame\n    df_con = clean_data(df_raw)\n    # Upload file\n    upload_dataframe_to_github(df_con, folder='consumption')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to sort dictionary based on the values in descending order and return first n key-value pairs\ndef sort_dict_by_value(dictionary: dict, descending=True, n=None) -> dict:\n    sorted_dict = dict(sorted(dictionary.items(), key=lambda item: item[1], reverse=descending))\n    if n is not None:\n        sorted_dict = dict(list(sorted_dict.items())[:n])\n    return sorted_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that aggregates consumption dataset and returns a dictionary\ndef roll_up_data(df_con: pd.DataFrame) -> dict:\n    job_roles = [\n        \"data engineer\",\n        \"data analyst\",\n        \"data architect\",\n        \"data scientist\",\n        \"machine learning engineer\"\n    ]\n\n    df_con = replace_nan(df_con)\n    final_dict = {}\n\n    for job_role in job_roles:\n        final_dict[job_role] = {\n            'company_dict' : {},\n            'skill_dict' : {},\n            'location_dict' : {},\n            'job_type_dict' : {},\n            'experience_dict' : {\n                'entry level' : 0,\n                'mid level': 0,\n                'senior level': 0\n            }\n        }\n\n    for index, row in df_con.iterrows():\n        job_role = None\n\n        for column, value in row.items():\n            if column == 'job_role':\n                job_role = value\n\n            if column == 'company_name' and value != '':\n                try:\n                    final_dict.get(job_role).get('company_dict')[value] += 1\n                except:\n                    final_dict.get(job_role).get('company_dict')[value] = 1\n            elif column == 'skills':\n                for s in value.split(','):\n                    if s != '':\n                        try:\n                            final_dict.get(job_role).get('skill_dict')[s] += 1\n                        except:\n                            final_dict.get(job_role).get('skill_dict')[s] = 1\n            elif column == 'location':\n                for l in value.split(','):\n                    if l != '':\n                        try:\n                            final_dict.get(job_role).get('location_dict')[l] += 1\n                        except:\n                            final_dict.get(job_role).get('location_dict')[l] = 1\n            elif column == 'job_type':\n                for j in value.split(','):\n                    if j != '':\n                        try:\n                            final_dict.get(job_role).get('job_type_dict')[j] += 1\n                        except:\n                            final_dict.get(job_role).get('job_type_dict')[j] = 1\n            elif column == 'experience':\n                if value < 3:\n                    final_dict.get(job_role).get('experience_dict')['entry level'] += 1\n                elif value >=3 and value <= 5:\n                    final_dict.get(job_role).get('experience_dict')['mid level'] += 1\n                else:\n                    final_dict.get(job_role).get('experience_dict')['senior level'] += 1\n\n    for k1, v1 in final_dict.items():\n        for k2 in v1.keys():\n            if k2 in ['company_dict', 'skill_dict', 'location_dict']:\n                final_dict[k1][k2] = sort_dict_by_value(final_dict[k1][k2], descending=True, n=10)\n\n    return final_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that aggregates consumption dataset and uploads target file\ndef upload_target_file():\n    # Read recent consumption file into DataFrame\n    con_path = get_file_path(folder='consumption', path_type='raw')\n    df_con = pd.read_csv(con_path)\n    # Aggregate data\n    fnl_dict = roll_up_data(df_con)\n    # Upload file\n    upload_dict_to_github(fnl_dict, folder='target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Entry point of the script\nif __name__ == '__main__':\n    # My email id\n    reciever_id = UserSecretsClient().get_secret('reciever_id')\n    # Try-catch block to handle exceptions and send mail\n    try:\n        # Upload cities data in GitHub\n        update_cities_data()\n        # Indian cities list\n        cities = cities_lst()\n        # Scrap and load source data\n        scrap_and_load()\n        # Upload consumption file\n        upload_consumption_file()\n        # Upload target file\n        upload_target_file()   \n        # Send mail if process succeeded\n        sendMail(reciever_id)\n    except Exception as e:\n        print('exception occured!')\n        print(e)\n        # Send mail if exception occurred\n        sendMail(reciever_id, exception=e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}