{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30553,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# Update package list silently\n!apt-get update -qq > /dev/null 2>&1\n# Install required libraries silently\n!apt-get install -qq \\\nlibglib2.0-0 \\\nlibnss3 \\\nlibdbus-glib-1-2 \\\nlibgconf-2-4 \\\nlibfontconfig1 \\\nlibvulkan1 \\\ngconf2-common \\\nlibwayland-server0 \\\nlibgbm1 \\\nudev \\\nlibu2f-udev > /dev/null 2>&1\n# Fix broken installs if any\n!apt --fix-broken install -y > /dev/null 2>&1\n# Download Chrome binary\n!wget -P /tmp https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chrome-linux64.zip > /dev/null 2>&1\n# Unzip Chrome to /usr/bin\n!unzip /tmp/chrome-linux64.zip -d /usr/bin/ > /dev/null 2>&1\n# Download ChromeDriver\n!wget -P /tmp https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chromedriver-linux64.zip > /dev/null 2>&1\n# Unzip ChromeDriver to /usr/bin\n!unzip /tmp/chromedriver-linux64.zip -d /usr/bin/ > /dev/null 2>&1\n# Install Selenium Python bindings\n!apt install -y python3-selenium > /dev/null 2>&1\n# Install specific Selenium version\n!pip install selenium==3.141.0 > /dev/null 2>&1\n# Install PyGithub\n!pip install PyGithub > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T19:01:26.752097Z","iopub.execute_input":"2025-06-11T19:01:26.752644Z","iopub.status.idle":"2025-06-11T19:02:46.105546Z","shell.execute_reply.started":"2025-06-11T19:01:26.752588Z","shell.execute_reply":"2025-06-11T19:02:46.103851Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# Standard library imports\nimport csv\nimport datetime\nimport html\nimport json\nimport os\nimport re\nimport shutil\nimport smtplib\nimport time\nimport urllib.parse\nfrom email.message import EmailMessage\n\n# Third-party imports\nimport pandas as pd\nimport pytz\nimport github\nfrom github.GithubException import GithubException\nfrom kaggle_secrets import UserSecretsClient\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.common.exceptions import TimeoutException,NoSuchElementException,StaleElementReferenceException,InvalidSelectorException,WebDriverException,InvalidArgumentException","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T19:02:46.108075Z","iopub.execute_input":"2025-06-11T19:02:46.108443Z","iopub.status.idle":"2025-06-11T19:02:46.724356Z","shell.execute_reply.started":"2025-06-11T19:02:46.108411Z","shell.execute_reply":"2025-06-11T19:02:46.723294Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def add_driver_options(options):\n    chrome_options = Options()\n    for opt in options:\n        chrome_options.add_argument(opt)\n    return chrome_options\n\n\ndef get_driver() -> webdriver.Chrome:\n    driver_config = {\n        \"options\": [\n            \"--headless\",\n            \"--no-sandbox\",\n            \"--start-fullscreen\",\n            \"--allow-insecure-localhost\",\n            \"--disable-dev-shm-usage\",\n            \"user-agent=Chrome/116.0.5845.96\"\n        ],\n    }\n    CHROME_BINARY_LOCATION = \"/usr/bin/chrome-linux64/chrome\"\n    CHROMEDRIVER_BINARY_LOCATION = \"/usr/bin/chromedriver-linux64/chromedriver\"\n    options = add_driver_options(driver_config[\"options\"])\n    options.binary_location = CHROME_BINARY_LOCATION\n    driver = webdriver.Chrome(executable_path=CHROMEDRIVER_BINARY_LOCATION,options=options)\n    return driver","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T19:02:46.725595Z","iopub.execute_input":"2025-06-11T19:02:46.726033Z","iopub.status.idle":"2025-06-11T19:02:46.733255Z","shell.execute_reply.started":"2025-06-11T19:02:46.726003Z","shell.execute_reply":"2025-06-11T19:02:46.732034Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def get_repository()->github.Repository.Repository:\n    access_token = UserSecretsClient().get_secret('access_token')\n    repo_str = UserSecretsClient().get_secret('repo_str')\n    g = github.Github(access_token)\n    repo = g.get_repo(repo_str)\n    return repo\n\n\ndef get_new_file(folder:str,branch:str='master')->str:\n    repo = get_repository()\n    contents = repo.get_contents(folder,ref=branch)\n    path = ''\n    for c in contents:\n        if path < c.path:\n            path = c.path\n    return path\n\n\ndef get_file_path(folder:str,path_type:str,branch:str='master')->str:\n    github_url = 'https://github.com'\n    repo_str = UserSecretsClient().get_secret('repo_str')\n    file_path = get_new_file(folder=folder,branch=branch)\n    file_url = github_url+'/'+repo_str+'/blob/'+branch+'/'+file_path\n    if path_type == 'url':\n        return file_url\n    elif path_type == 'raw':\n        return file_url.replace('github','raw.githubusercontent').replace('blob/','')\n    else:\n        raise Exception(\"path_type is either 'url' or 'raw'!\")\n\n\ndef clean_text_for_csv(text:str)->str:\n    if not isinstance(text,str):\n        return text\n    cleaned_text = urllib.parse.unquote(text)\n    cleaned_text = html.unescape(cleaned_text)\n    cleaned_text = cleaned_text.replace('\\\\','\\\\\\\\') \n    cleaned_text = cleaned_text.replace('\\n','\\\\n')\n    cleaned_text = cleaned_text.replace('\\t','\\\\t') \n    cleaned_text = cleaned_text.replace('\\r','\\\\r') \n    cleaned_text = cleaned_text.strip()\n    return cleaned_text\n    \n\ndef save_lst_as_csv(data:list[dict],folder:str,filename:str,branch:str='master'):\n    cleaned_data = [\n        {k: clean_text_for_csv(v) for k,v in row.items()} for row in data\n    ]\n    df = pd.DataFrame(cleaned_data)\n    repo = get_repository()\n    csv_content = df.to_csv(index=False)\n    file_pth = folder+'/'+filename\n    try:\n        repo.create_file(file_pth,\"create source file\",csv_content,branch=branch)\n        print(f\"{file_pth} created!\")\n    except:\n        file = repo.get_contents(file_pth,ref=branch)\n        full_raw_pth = get_file_path(folder=folder,path_type='raw',branch=branch)\n        existing_df = pd.read_csv(full_raw_pth)\n        updated_df = pd.concat([existing_df,df],ignore_index=True)\n        updated_csv_content = updated_df.to_csv(index=False)\n        repo.update_file(file_pth,'update source file',updated_csv_content,file.sha,branch=branch)\n        print(f\"{file_pth} updated!\")\n\n\ndef check_folder_exists(folder:str,branch:str='master')->bool:\n    try:\n        repo = get_repository()\n        contents = repo.get_contents(folder,ref=branch)\n        return True\n    except GithubException as e:\n        if e.status == 404:\n            return False\n        else:\n            raise e \n\n\ndef check_file_exists_in_folder(folder:str,filename:str,branch:str='master')->bool:\n    try:\n        repo = get_repository()\n        contents = repo.get_contents(folder,ref=branch)\n        for content_file in contents:\n            if content_file.name == filename:\n                return True\n        return False\n    except GithubException as e:\n        if e.status == 404:\n            return False\n        else:\n            raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T19:02:46.735249Z","iopub.execute_input":"2025-06-11T19:02:46.735593Z","iopub.status.idle":"2025-06-11T19:02:46.755499Z","shell.execute_reply.started":"2025-06-11T19:02:46.735538Z","shell.execute_reply":"2025-06-11T19:02:46.754318Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def get_days_filter_for_current_month():\n    today = datetime.datetime.today()\n    days_passed = today.day\n    available_days = [30,15,7,3,1]  \n    for days in available_days:\n        if days <= days_passed:\n            return str(days)\n    return \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T19:02:46.757367Z","iopub.execute_input":"2025-06-11T19:02:46.757774Z","iopub.status.idle":"2025-06-11T19:02:46.777090Z","shell.execute_reply.started":"2025-06-11T19:02:46.757736Z","shell.execute_reply":"2025-06-11T19:02:46.775793Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def scrapeNakuri(job_role:str,driver,is_full_load:bool):\n    job_lst = []\n    query = \"-\".join(job_role.lower().split())\n    base_url = f\"https://www.naukri.com/{query}-jobs?jobAge=1\"\n    if is_full_load:\n        base_url = f\"https://www.naukri.com/{query}-jobs?jobAge={get_days_filter_for_current_month()}\"\n    wait1,wait2 = WebDriverWait(driver,10),WebDriverWait(driver,30)\n    driver.get(base_url)\n    # print(base_url)\n    try:\n        xpth = \"/html/body/div/div/main/div[1]/div[1]/div[1]/div[@class='styles_nrc__heading__dLYOD']\"\n        wait1.until(EC.presence_of_all_elements_located((By.XPATH,xpth)))\n        no_results_text = driver.find_element(By.XPATH,xpth).text\n        print(f\"{no_results_text} for {job_role}!\")\n        return []\n    except NoSuchElementException:\n        pass\n    except TimeoutException:\n        pass\n    except Exception as e:\n        # print(e)\n        return []\n    job_links = []\n    page_count = 0\n    xpth1 = \"/html/body/div/div/main/div[1]/div[2]/\"\n    while page_count < 26:\n        wait2.until(EC.presence_of_all_elements_located((By.XPATH,f\"{xpth1}div[2]/div/div[@class='srp-jobtuple-wrapper']\")))\n        elements = driver.find_elements(By.XPATH,f\"{xpth1}div[2]/div/div[@class='srp-jobtuple-wrapper']\")\n        # print(len(elements))\n        for div_element in elements:\n            link = div_element.find_element(By.CLASS_NAME,\"title \").get_attribute(\"href\")\n            job_links.append(link)\n            # print(link)\n        # print()\n        try:\n            next_page = wait2.until(EC.element_to_be_clickable((By.XPATH,f\"{xpth1}div[3]/div/a[2]\")))\n            next_url = next_page.get_attribute(\"href\")+\"?jobAge=1\"\n            driver.get(next_url)\n            page_count += 1\n            # print(page_count,end=' ')\n        except:\n            break\n    # print()\n    job_links = job_links[:500]\n    xpth2 = \"/html/body/div/div/main/div[@class='styles_jdc__content__EZJMQ ']/div[1]/\"\n    for url in job_links:\n        try:  \n            driver.get(url)\n            driver.refresh()\n            wait2.until(EC.presence_of_element_located((By.XPATH,xpth2+\"section[1]/div[1]/div[1]/header/h1\")))\n            job_title = driver.find_element(By.XPATH,xpth2+\"section[1]/div[1]/div[1]/header/h1\").text\n            company = driver.find_element(By.XPATH,xpth2+\"section[1]/div[1]/div[1]/div/a\").text\n            experience = driver.find_element(By.XPATH,xpth2+\"section[1]/div[1]/div[2]/div[1]/div[1]/span\").text\n            salary = driver.find_element(By.XPATH,xpth2+\"section[1]/div[1]/div[2]/div[1]/div[2]/span\").text\n            locations = [loc.text for loc in driver.find_elements(By.XPATH,xpth2+\"section[1]/div[1]/div[2]/div[@class='styles_jhc__loc___Du2H']/span/a\")]\n            description = driver.find_element(By.XPATH,xpth2+\"section[2]/div[2]/div[@class='styles_JDC__dang-inner-html__h0K4t']\").text\n            industry = driver.find_element(By.XPATH,xpth2+\"section[2]/div[2]/div[2]/div[2]\").text\n            department = driver.find_element(By.XPATH,xpth2+\"section[2]/div[2]/div[2]/div[3]\").text\n            employment_type = driver.find_element(By.XPATH,xpth2+\"section[2]/div[2]/div[2]/div[4]\").text\n            skills = [skill.text for skill in driver.find_elements(By.XPATH,xpth2+\"section[2]/div[3]/div/a[@target='_blank']\")]\n            job_dict = {\n                'job_title': job_title,\n                'company': company,\n                'experience': experience,\n                'salary': salary,\n                'locations': locations,\n                'description': description,\n                'industry': industry,\n                'department': department,\n                'employment_type': employment_type,\n                'skills': skills,\n                'scraped_at': datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d %H:%M:%S\")\n            }\n            job_lst.append(job_dict)\n            # print(url)\n        except:\n            pass\n    return job_lst","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-06-11T19:36:28.072452Z","iopub.execute_input":"2025-06-11T19:36:28.073580Z","iopub.status.idle":"2025-06-11T19:36:28.082529Z","shell.execute_reply.started":"2025-06-11T19:36:28.073528Z","shell.execute_reply":"2025-06-11T19:36:28.080643Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def scrapeFoundit(job_role:str,driver,is_full_load:bool):\n    query = \"+\".join(job_role.split())\n    base_url = f\"https://www.foundit.in/srp/results?query={query}&jobFreshness=1\"\n    if is_full_load:\n        base_url = f\"https://www.foundit.in/srp/results?query={query}&jobFreshness={get_days_filter_for_current_month()}\"\n    url = base_url.encode(\"ascii\",\"ignore\").decode(\"unicode_escape\")\n    wait = WebDriverWait(driver,30)\n    # print(url)\n    driver.get(url)\n    try:\n        no_results_text = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div[6]/div/div/div[2]/div[1]/div[2]/p[1]\").text\n        print(f\"{no_results_text} {job_role}!\")\n        return []\n    except NoSuchElementException:\n        pass\n    except Exception as e:\n        # print(e)\n        return []\n    job_list = []\n    count = 0\n    while True:\n        try:\n            driver.refresh()\n            base_xpath = \"/html/body/div[@id='srpThemeDefault']/div[@class='srpContainer']/div[@id='srpContent']\"\n            card_container_xpath = f\"{base_xpath}/div[@class='srpCardContainer']/div[@class='srpResultCard']\"\n            wait.until(EC.presence_of_element_located((By.XPATH,card_container_xpath+\"/div[@class='srpCardsWrapper']\")))\n            job_cards = driver.find_elements(By.XPATH,card_container_xpath+\"/div[@class='srpCardsWrapper']\")\n            for card in job_cards:\n                card.click()\n                try:\n                    try:\n                        accept_cookies = driver.find_element(By.XPATH,'/html/body/div[5]/div/div/div[3]/button')\n                        accept_cookies.click()\n                    except:\n                        pass\n                    detail_xpath = f\"{base_xpath}/div[@class='srpJdContainer']\"\n                    detail_element = wait.until(EC.presence_of_element_located((By.XPATH,detail_xpath)))\n                    job_data = {}\n                    job_data['job_title'] = detail_element.find_element(By.CLASS_NAME,\"jdTitle\").text\n                    job_data['company_name'] = detail_element.find_element(By.CLASS_NAME,\"jdCompanyName\").text\n                    highlights = detail_element.find_elements(By.CLASS_NAME,\"highlightsRow\")\n                    job_data['experience'] = highlights[0].find_element(By.XPATH,\"./div[1]\").text\n                    try:\n                        job_data['salary'] = highlights[0].find_element(By.XPATH,\"./div[2]\").text\n                    except Exception:\n                        job_data['salary'] = None\n                    job_data['location'] = highlights[1].text\n                    job_data['industry'] = highlights[2].text\n                    job_data['job_description'] = detail_element.find_element(By.CLASS_NAME,\"jobDescInfoNew\").text\n                    job_data['skills'] = [skill.text for skill in detail_element.find_elements(By.CLASS_NAME,\"pillItem\")]\n                    job_data['scraped_at'] = datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d %H:%M:%S\")\n                    job_list.append(job_data)\n                    count += 1\n                    # print(count,end=' ')\n                    if count == 500: \n                        # print()\n                        return job_list\n                except Exception as e:\n                    # print(e)\n                    pass\n            pagination = driver.find_element(By.XPATH,card_container_xpath+\"/div[@class='pagination']\")\n            pagination.find_element(By.CLASS_NAME,\"mqfisrp-right-arrow\").click()\n        except Exception as e:\n            # print(e)\n            # print()\n            return job_list\n    # print()\n    return job_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:05:39.536557Z","iopub.execute_input":"2025-06-11T20:05:39.537041Z","iopub.status.idle":"2025-06-11T20:05:39.554337Z","shell.execute_reply.started":"2025-06-11T20:05:39.537007Z","shell.execute_reply":"2025-06-11T20:05:39.553067Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"job_roles = [\n    \"Data Engineer\",\n    \"Data Analyst\",\n    \"Data Architect\",\n    \"Data Scientist\",\n    \"Machine Learning Engineer\"\n]\nbranch = \"test\"\ndriver = get_driver()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:09:50.265383Z","iopub.execute_input":"2025-06-11T20:09:50.266593Z","iopub.status.idle":"2025-06-11T20:09:51.549574Z","shell.execute_reply.started":"2025-06-11T20:09:50.266549Z","shell.execute_reply":"2025-06-11T20:09:51.548111Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"overall_start_time = time.time()\nprint(\"--- Scraping Nakuri Started\\n\")\ndriver = get_driver()\nfor role in job_roles:\n    print(\"Job-role: \"+role)\n    folder = f\"Source/Nakuri/{role.replace(' ','')}\"\n    yrmn_str = str(datetime.datetime.now(pytz.timezone('Asia/Kolkata')))[:7].replace(' ','-')\n    csv_file_str = f\"{yrmn_str}.csv\"\n    is_full_load = not check_file_exists_in_folder(folder=folder,filename=csv_file_str,branch=branch)\n    lst = scrapeNakuri(role,driver,is_full_load)\n    print(\"Number of jobs extracted: \"+str(len(lst)))\n    if lst:\n        save_lst_as_csv(data=lst,folder=folder,filename=csv_file_str,branch=branch)\n    print()\noverall_end_time = time.time()\ntotal_time = overall_end_time - overall_start_time\ntotal_time_minutes = total_time / 60\ntotal_time_hours = total_time_minutes / 60\nprint(f\"\"\"Overall time taken: {total_time:.2f} seconds (or) {total_time_minutes:.2f} minutes (or) {total_time_hours:.2f} hours.\"\"\")\nprint(\"\\n--- Scraping Nakuri Ended\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:09:51.551942Z","iopub.execute_input":"2025-06-11T20:09:51.552284Z","iopub.status.idle":"2025-06-11T20:09:51.558110Z","shell.execute_reply.started":"2025-06-11T20:09:51.552249Z","shell.execute_reply":"2025-06-11T20:09:51.556845Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"overall_start_time = time.time()\nprint(\"--- Scraping Foundit Started\\n\")\nfor role in job_roles:\n    print(\"Job-role: \"+role)\n    folder = f\"Source/Foundit/{role.replace(' ','')}\"\n    yrmn_str = str(datetime.datetime.now(pytz.timezone('Asia/Kolkata')))[:7].replace(' ','-')\n    csv_file_str = f\"{yrmn_str}.csv\"\n    is_full_load = not check_file_exists_in_folder(folder=folder,filename=csv_file_str,branch=branch)\n    lst = scrapeFoundit(role,driver,is_full_load)\n    print(\"Number of jobs extracted: \"+str(len(lst)))\n    if lst:\n        save_lst_as_csv(data=lst,folder=folder,filename=csv_file_str,branch=branch)\n    print()\noverall_end_time = time.time()\ntotal_time = overall_end_time - overall_start_time\ntotal_time_minutes = total_time / 60\ntotal_time_hours = total_time_minutes / 60\nprint(f\"\"\"Overall time taken: {total_time:.2f} seconds (or) {total_time_minutes:.2f} minutes (or) {total_time_hours:.2f} hours.\"\"\")\nprint(\"\\n--- Scraping Foundit Ended\")\n\ndriver.quit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T20:11:37.800961Z","iopub.execute_input":"2025-06-11T20:11:37.801373Z","iopub.status.idle":"2025-06-11T20:11:37.809349Z","shell.execute_reply.started":"2025-06-11T20:11:37.801344Z","shell.execute_reply":"2025-06-11T20:11:37.807187Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"---","metadata":{}}]}